[
{"website": "Avenuecode", "title": "How to SOLID", "author": ["Magnum Fonseca"], "link": "https://blog.avenuecode.com/how-to-solid", "abstract": "SOLID is an acronym that represents five pillars of object-oriented programming and code design theorized by Uncle Bob (A.K.A Robert C. Martin) around the 2000s. All developers should have a clear understanding of these pillars for developing software properly to avoid bad design. C aring about your code and how other engineers (or your future self) will read it and see its quality is undoubtedly the mission of any engineer who really cares about their product and their colleagues. Use SOLID best practices to reduce code complexity, reduce the coupling between classes, increase the separation of responsibilities, and delimit the relationships between them. These are simple ways to improve code quality. A good design reveals intent, provides insights into requirements analysis, is adaptable, tolerates the evolution of business needs, and also accepts new technologies without undue cost. But an understandable code doesn’t just happen! You have to craft it, you have to clean it, and you have to make a professional commitment. Professional commitment? Make it your responsibility to craft code that: delivers business value, is clean, is tested, and is simple. I’ll try to explain SOLID principles in the simplest way so that it’s easy for everyone to utilize. In other words, one class should serve only one purpose. This does not presume that each class should have only one method, but they should all relate directly to the responsibility of the class. Let's look at this Employee class as an example: How many reasons does it have to change? Let's assume Employee A wants to change the rule to the business rule of calculateSalary and Employee B wants to change the format of printReport -  this will cause a merge conflict. When responsibilities are mixed in the same source file, collisions become more frequent. Then Employee B adds a new Report. The report looks pretty good. Business users are happy. However, during the next payroll cycle, all paychecks have the wrong amount. When responsibilities are co-located, accidental breakage becomes more likely. HOW TO IDENTIFY VIOLATIONS OF SRP Classes that violate SRP are: In other words, we are able to change or extend the behavior of a system without changing any existing code. When we must change a module, we must also re-test, re-release, and re-deploy that module, and when we change old, working code, we often break it. Software entities (classes, modules, functions, etc.) should be extendable without actually changing the contents of the class we're extending. If we follow this principle closely enough, it is possible to modify the behavior of our code without ever touching a piece of the original code. A change should be low cost and low risk. Given the class: This code is messy and is obscuring its content. A good module should read like a story. But if we're not careful, that story can be obscured by little decisions, odd flags, or strange function calls. Let's figure out what went wrong here. HOW TO IDENTIFY VIOLATIONS OF OCP: SWITCHING ON TYPE Let's look at a solution to these issues: This code tells its own story. It doesn’t need comments, and there are few obscuring details. The Client object has hidden most details behind an abstract interface, and this is what Object Oriented Design is all about. The revised code is: OCP is not an excuse for doing more than is necessary. Abstraction is expensive, and you should only use it when there is an imminent risk involved. In general, it's smart to prove to yourself that it will be worth the cost. By creating appropriate abstractions, we can produce modules that are open for extension but closed for modification. Inheritance is a relationship between a base class and its derived classes, each derived class, and all clients of its base class. Here is the original formulation: \"If for each object o1 of type S there is an object o2 of type T such that for all programs P defined in terms of T , the behavior of P is unchanged when o1 is substituted for o2 then S is a subtype of T \" (Liskov substitution principle). In other words, instances of a derived class must be usable through the interface of its base class without clients of the base class being able to tell the difference. Subtypes must be substitutable for their base types. LSP can also be described as a counter-example of Duck Test: “If it looks like a duck, quacks like a duck, but needs batteries — you probably have the wrong abstraction.” Let's look at how to recognize LSP violations. HOW TO IDENTIFY VIOLATIONS OF LSP In the example above , there are no apparent OCP or LSP violations. This principle encompasses the following points: LSP is all about how it behaves-as-a and not how it is-a . Inheritance != IS-A A client should never be forced to implement an interface that it does not use; clients shouldn’t be forced to depend on methods they do not use. Look at the fat interfaces below: In the example above, methods are grouped for ease of implementation, and there are more methods than any one client needs. The problem is Phantom Dependency! A change on behalf of one client impacts all clients, and a new client impacts all existing clients. In general, an interface should be small, but sometimes a class with a fat interface is difficult to split. The good news is that you can fake it, as in the example below: NOTES ON ISP ClientGateway still has a fat interface, but clients don’t know this because all dependencies point AWAY from it, phantom dependency has been eliminated, and changing existing interfaces or adding new interfaces does not impact other interfaces. Therefore, other clients are not impacted. Entities must depend on abstractions, not on concretions. This principle states that the high level module must not depend on the low level module, but they should depend on abstractions. The example below is a poor implementation that does not follow the Dependency Inversion Principle: Sales depends upon all derivatives of Client, but only because it creates them. It only calls methods in Client. In procedural designs, source code dependencies flow with control. This makes it easy to add functions to existing data structures without changing the data structures. In object-oriented design, design dependencies can be inverted so that source code dependencies flow against control. SOLUTION: ABSTRACT FACTORY [GoF] In the example below, dependencies are inverted, and Sales no longer depends on derivatives of Client. This is a better implementation: S OLID might seem to be a handful at first, but with constant usage and adherence to its guidelines, it becomes a part of you and your code. Remember that these are principles, not rules. The principles should be applied on a case by case basis when it makes sense. Are the SOLID principles applicable to Functional Programming? Of course! Functional programmers want to separate their code to avoid crosstalk between responsibilities and users. They want to minimize the number of modules affected by a change.", "date": "2020-9-16"},
{"website": "Avenuecode", "title": "Android Basics - Consuming Web Services", "author": ["Samuel Filizzola"], "link": "https://blog.avenuecode.com/android-basics-consuming-web-services", "abstract": "In this series, Android Basics, I will guide you through the topics that you'll need to know in order to integrate this network into your apps. Introduction Hey there! If you're embarking on the journey of learning all things Android, make sure that you give this a read first. It's the first article in our series on \"Android Basics\" here at Avenue Code Snippets. In this article, I will make a comparison to the native connection methods suggested by Google and Retrofit , using the Star Wars API as an example of remote data. Let's begin with a simple question: why would you want to connect your application to a remote service in the first place? Here are some possible reasons: It's always helpful to begin by defining your objectives. My goal in this exercise is to consume the data from the API by making a simple list of the Star Wars characters. I will make two simple branches of the same project on GitHub , both using a MainActivity with a RecyclerView to show the character data. Later, in another post, I will go more in depth regarding RecyclerViews and Adapters. In order to use a native Android connection, we have to use an AsyncTask , which is the method Google suggests for multi-threading inside an app. The AsyncTask consists of an interface that has two methods that have to be implemented. The first is used to do things in a background thread (doInBackground), and the second warns the main thread when the background processing is done (onPostExecute). The other methods are optional, and you can check them out in the link above. The following steps will help you in achieving your end goal: Now, let's code! I've decided to make the AsyncTask in a different file from the Activity. So, I've used an interface that gives me access to the data anywhere I decide to use the Task. Its structure should look like this: When making a connection inside any app, you should make it in a different thread from the Main Thread. I used the method recommended by Google, which is to make Http connections: HttpURLConnection . Therefore, I used the doInBackground method to execute behind the scenes: Here, I used GSON to parse the data directly from the server to an Object that I have mapped to have the same structure as JSON from the server. More information on GSON and how to implement serialization/deserialization can be found here . Later, you can delve even more into this process by checking the GitHub from this project. In this sample, I have made the JsonResult model that follows the specification of the JSON on the server: As I said in Step One, by using an interface, I was able to find a way to allow the data to go from the task to anywhere within the project. Excellent! So, in my activities that have the list, I just use the interface when executing the task: You can check out that particular branch of this project to see how I've made it with just the Android AsyncTask and simple HttpURLConnection. Branch with Android AsyncTask Since Retrofit is a library, we have to load it into our project using Gradle. In the build.gradle file, we add: We have to tell Retrofit which serialization/deserialization method we want to use. Therefore, we have to use GSON, which is why we have that second line. On the Retrofit site, there are six common libraries for use, so feel free to adapt them to your needs. We can now use this in our project! Now the easy part: I've decided to make a static class so I can easily access Retrofit from anywhere in the code. So, I've made the interface, which is the file that has all the methods of the API I will utilize: In this case, I only have one method right now. In the future, of course, I can have more, such as the search method or the method to return specific data of only one character. Now, back to the static class, which I can invoke from anywhere in the code: Retrofit uses the Call method to make requests from the API. It uses Callbacks to make the communication between the API and the App. The Call instances can be executed either synchronously or asynchronously, and the Callbacks will be executed on the main thread. So: In this case, I chose the asynchronous method .enqueue to make the call from the API. I could use the synchronous .execute; however, the asynchronous method was a better fit in this case. For more information on Retrofit methods, you can check out this Retrofit javadoc . And that's how you use Retrofit. Branch with Retrofit In every project, we often have to use different methods when approaching the same topic. For networking in Android, I think 90% of the time Retrofit is an excellent choice for the network layer in your application, simply because it lessens the amount of hard work and saves a lot of time with communication between the app and backend.", "date": "2017-8-17"},
{"website": "Avenuecode", "title": "Project Management: The Art of Kung Fu", "author": ["William Hom"], "link": "https://blog.avenuecode.com/project-management-the-art-of-kung-fu", "abstract": "Having lived and breathed project management for the past 20 years at various clientele sites, I've come to believe that many of the principles of martial arts can be applied directly to project management.  In this blog, I will discuss how I was able to use these principles to successfully navigate a client environment from day one. To quote the legendary Kung Fu master, Bruce Lee... Image courtesy of Quote Ambition I admit that I am not a martial arts expert, nor do I have a black belt, but from what I can deduct from the many hours I've spent watching Bruce Lee’s \"Enter the Dragon\" and \"Fist of Fury,\" martial arts does require you to develop a basic foundation and discipline (think timelines, budgets, and scope) before moving on to the next levels of form, function, and finesse. This becomes your trademark and style that sets you apart from the others. But first... Image courtesy of Quote Ambition Before setting foot in a client site, be sure to... It goes without saying that whatever tools you use to convey timelines, budgets, statuses, and scope, you need to make them simple and easy to understand so that your conclusions become “aha” moments for your audience. Common tools (not an exhaustive list) that I use on a daily basis include: What I have listed below may seem basic, but surprisingly, many folks have a difficult time following these guidelines. These are practices that clients often notice set you apart from others. Before we jump into the specifics, this quote strikes at the heart of what needs to be done on day one. Image courtesy of Quote Ambition I would probably rank day one as the most stressful day since basic tools for fostering productivity--such as meetings, emails, VPNs, and MS Project--are not yet set up on the client site. For example, there may not even be a client-issued laptop when you step into the office on day one. Treat this as a challenge, and try to discover how you can get up to speed while waiting for access. Image courtesy of Catholica Omnia Laptop - It isn't unusual to walk into a client site on day one to discover that you don't have a laptop set up for your use. Make sure to bring your personal laptop or request a laptop from your company so you can be productive. Email set up - You would think that this is normally set up before you get on the client site.  You’d be surprised that this is not the case. If there is no client email set up because the paperwork is in progress, give out your company email. Access to video conferencing  - If you are not able to gain access to your client’s video conferencing, sign up for a free online meeting trial from Cisco Webex or Zoom. If possible, borrow from someone. Image courtesy of Quote Ambition Be creative in how you get things done For instance, one of my clients did not have an online Kanban board or Jira scrum board set up for me to conduct my daily scrum meeting because they could not get me a Jira account. Rather than be the victim, I went old school and pieced together several large sheets of post-it easel pads and stuck it on the wall with my scrum team assignments, and with a permanent marker and blue masking tape, I was able to conduct a daily scrum. The client was impressed with my out-of-the-box thinking and can-do mentality. Picturize (yes, this is actually a word) It’s so nice when someone takes the initiative to do this. Personally, I draw many boxes and bullet points on the board because I need to convey my understanding of things quickly and validate whether my understanding is correct. You'll get a lot of attention and feedback when you draw simplistic diagrams and engage your team members. Your team members will start taking pictures of what was created, and not only will you get recognition and respect, but you will also get a graphic that can be used in future presentations. Send regular updates Project managers should send regular updates about their projects to management and to the team.  Status updates should include a summary, with any alerts/blockers, burndown charts, etc. I’ve also seen dashboards with bug counts and number of build failures submitted by the team and regression testing pass/fail metrics. That's it for the basics! Next week, we'll talk about why and how project management needs to be re-invented.", "date": "2019-4-17"},
{"website": "Avenuecode", "title": "AC Spotlight - Maryann Menendez", "author": ["Manoela Vieira"], "link": "https://blog.avenuecode.com/ac-spotlight-maryann-menendez", "abstract": "Maryann Menendez, Global Director of SME Marketing at SAP, discusses digital marketing trends and opportunities for small and medium-sized businesses during and after COVID-19. Avenue Code : Tell us about your personal career path. What inspired you to pursue marketing within the tech industry, and how did you get to where you are today? Maryann Menendez: I started university as an accounting major, but since it didn’t hold my interest, I made the move into another area and graduated with a BA in Business, double major in Marketing and International Business. After working in sales, I joined PC Magazine Latin America. Since then my roles have been in an array of marketing-related capacities in the tech industry: PR, advertising, channel marketing, field marketing, etc. I’ve always loved technological innovation and interacting with people in different cultures and countries, so I worked for Latin America operations when living in Miami and San Diego. I’ve held a global role since joining SAP almost 10 years ago. AC: On a personal level, tell us why being a STEM4Girls Advocate is important to you. What do companies gain when they have women in tech leadership roles, and how can they promote and support women in these roles? MM: I’m a STEM4Girls advocate because I was great at math but was counseled toward a wrong career path in high school. Instead of being encouraged to become an engineer, I was told I would make a great accountant, which wasn’t the right fit for me. I made it back into a STEM environment, but I realize that other girls interested in science and math might be unaware of the right opportunities for them. Companies with women in tech leadership roles have the advantage of a more well-represented team in terms of IQ and emotional intelligence. A good way to encourage women to pursue these leadership roles is what we’ve done for men for ages: find someone with potential and talent, mentor them, and provide them with opportunities for growth, innovation, and leadership. Most importantly, women need internal advocates who know how to position their strengths within the company for new and more challenging leadership opportunities. AC: With the advent of COVID-19, what are the biggest opportunities and trends we’ll see in the tech sector? MM: We’ve been talking about the importance of digital transformation for years. This is a great opportunity for tech companies to show businesses of all sizes how tech can help them move forward. As for trends, the tech sector itself has been moving every in-person experience (event, training, meeting, etc.) online. AC: How can companies use technology to position themselves for success after COVID-19? Do you foresee business operations “returning to normal,” or will there be a “new normal”? MM: Given the uncertainty about when businesses will reopen 100%, companies need to use technology to position themselves for success today, ASAP, and not wait for “after COVID-19.”  This means they need to consider how they can change in-person touchpoints to a different experience while still providing their value-add services/products. Retailers have adapted by delivering via retail-by-mail and retail-to-go, and restaurants have moved to pick-up and delivery. Entrepreneurs and small businesses in different sectors can adapt too. A music teacher, for example, can teach students virtually using a desktop communication platform or phone apps. Businesses must have continuity plans in order to find a path forward. Whether because of a pandemic, natural disaster, or one of many other scenarios, companies need to have a Plan B when business operations cannot continue as normal. AC: How should COVID-19 change the way we market? MM: While nothing can replace a face-to-face conversation, meeting, etc., we will see more online calls, events, meetings, and trainings. During the last few months, companies have realized there are a lot of operations that can be carried on remotely. The same is true for marketing: we need to think about changing the experience to one that suits the needs of our prospects and customers in today’s reality. This applies to every cycle of the buyer’s journey. Maryann Menendez speaking at SAP headquarters in Germany. Image courtesy of Maryann Menendez. AC: As the Global Director of SME Marketing, has COVID-19 forced you to tailor marketing strategies more specifically to the needs of each geography, or has it united your efforts and strategies? MM: Our overall strategy and goals are united, but the way that strategy is implemented can vary from region to region depending on the needs of our customers and prospects. It has, however, united our global team and all the different regions in sharing ideas and best practices with cross-functional team on thinking digital experience or digital first. AC: What trends will we see in the next five years for digital marketing? MM: Artificial intelligence, remote work, and augmented reality. Whether it be chatbots on a website or information gathering, the use of AI will be accelerated. A rise in working from home is something we’ve seen already. Augmented reality will help marketers create a better customer experience. AC: You’ve spoken about how necessary interdepartmental cohesion is for enterprise organizations. How can marketing creatively support other key areas like business development? MM: Marketing and business development, aka sales, need to work together harmoniously. Business development teams understand which solutions, products, or services provide the most value to a target market/industry/demographic. It’s up to marketing to know which digital channels and tactics are best for reaching those target markets. If there's a new solution/product/service, marketing should do A/B testing to help business development meet their goals. AC: What do you do to keep up with innovations in best marketing practices? MM: I read, search online, subscribe to several online marketing publications, participate in industry events, and keep up with various social groups online. I also read blogs by entrepreneurs and business leaders who have experienced successes and failures so that I can learn how they have converted challenges into opportunities. There’s more than one way to look at things. It helps when we are open to new approaches. AC: Was there a defining moment for you personally that made you into the industry leader you are today? MM: There have been several. First, I was asked by the owners of a small business to represent them in conversations with business lawyers about opening an office in a foreign country. My bosses picked me because I knew the language and culture of that country, and most of all because they trusted me and my knowledge of their business model. Second, I was personally commended as a model for regional marketers by both the President/CEO and CMO of another company. Finally, I have been asked numerous times to present in front of business partners and colleagues around the world. When you are asked to share your insights, expertise, and opinions, that’s a sign! AC: Thanks for speaking with us today, Maryann! It’s been great to hear your perspective on pursuing digital transformation for marketing initiatives during COVID-19 and beyond.", "date": "2020-7-24"},
{"website": "Avenuecode", "title": "Agile and Continuous Touch-points", "author": ["Rik Itzkowitz"], "link": "https://blog.avenuecode.com/agile-and-continuous-touch-points", "abstract": "The best teachers I have ever had made their finest points outside of the typical every-day classroom lesson. A long time ago, my fourth grade school teacher wanted to teach us a valuable life lesson. He whispered five sentences into the ear of a student in the first row. She then whispered the sentences to the next kid and this went around and around until the very last student had the information. I recall being somewhere in the middle of this long whispering chain. Not surprisingly, the information had completely changed, and also, this being grade school, the results were very amusing. The teacher was trying to show how rumors and hear-say can quickly change what once might have been truthful information into something you would never trust as fact. At the time, I was more worried about what might have been packed for me at lunch, or passing my next spelling test, so I haven’t thought about this for years. Before AGILE development became hip, the process to create software involved getting some kind of requirements at the beginning. At the end, we would all bite our lips while showing our product to those paying for it. All of the technical concerns of scalability, separation of responsibilities, using the right tools, none of this matters if the business isn’t getting the product they actually wanted. When things go wrong at this point, its very bad. Money has been spent, trust disappears, and the project is in serious trouble. Let’s be honest, even the most diligent business analyst isn’t going to have every detail about the requirements. Sometimes, conditions change beyond anyone’s control. In fact, I was recently on a year long project where in the final month, a whole slew of new requirements rolled in because of foreign regulatory changes. We must accept the fact that requirement will change. As requirements change, so too will the approach and technical solve. AGILE came along with a significant change in how we approach product development. We still have a notion of initial requirements, but the inception only covers a starting view of the project and often excludes fine detail. Instead of 1 large cycle, we have many smaller iterations or sprints. The units of work played inside these iterations are called “stories”. Each story would be implemented by a developer, validated and finally signed-off by the business and architect. We can measure the performance, or velocity of the team, by how many story points have been completed. This was an important change because instead of setting yourself up for a major disappointment, there are many cycles. There are now many decision points to change scope and to some degree, the technical approaches. This does not mean you can re-invent the application every iteration, but it gives us more flexibility. Now let’s think about this problem again, only this time, with-in a single iteration. For any particular story we play, we may think we know everything at the start, but often don’t. The business analyst writes a story by themselves, hands it to the team and sits back and expects the result to be exactly what is expected. It all makes perfect sense when you are the one person writing the story. Too often, this ends in disaster. The developer picks up the story and thinks one thing. A QA picks it up next, possibly thinking something different. We hope at sign-off what we have is what we expect. If it’s not, the story gets moved back. Just like in my fourth grade lesson, information at one end of a chain does not resemble what we end up with. A story implementation can work out, even in-spite of bad communication, but we are missing out on an opportunity to shape the product we want. You may have stories that get moved back at sign-off. If the business is not active in the sign-off of stories, you may notice new stories covering the same ground. How this manifests itself from group to group may differ, but there are ways to measure this phenomenon. We need meaningful touch points with-in the sprint itself. Without this, the work done for the whole iteration can often be disappointing. This begins by the business analyst and technical team writing the story together. In particular, the acceptance criteria of the story are the confluence of the requester and responder of the work defined. I am not taking about a mini-inception or any additional amount of time to be spent. Simply, that the time spent communicating in isolation be substituted with some collaboration. As the architect, in helping to write the story, I include sections for design, implementation and testing considerations. This helps align the approach for developers, QA’s and forces us to answer the needed questions up front. Some teams include a “desk check”. This is another touch-point just before the QA would start validating. At this time, the business can get a preview of how something works, and the architect can see that it was implemented as expected. The final touch-point is the sign-off. Ideally the QA that validated the story gives a sign-off demo to the business and the architect. If we have the other touch-points with-in the iteration in place, the sign-off should go fairly quickly and easily. The product will be better because the design will accommodate knowledge of the future. The developers now have clarity in the stories they pick up and how it fits in with the overall design. The QA’s can validate the story with greater understanding of what actually should have been developed. Sign-offs become a formality and now we have moved our work to done. We thought that when we substituted a single touch-point for many touch points, our problems were solved. What we continue to learn is that what we must have is continuous touch-points to shape product development effectively. I suppose if I had to go back to the fourth grade, and re-live that moment again, I could stand up and ask for a huddle or a sign-off session. I probably should be careful though, my teacher had one or more conversations with my parents over my velocity.", "date": "2016-9-28"},
{"website": "Avenuecode", "title": "Your Roadmap to a Machine Learning Career", "author": ["Frederico Caram"], "link": "https://blog.avenuecode.com/your-roadmap-to-a-machine-learning-career", "abstract": "On any given day, a data scientist is a mathematician, a statistician, a computer programmer, and an analyst equipped with a diverse and wide-ranging skillset, balancing knowledge in different computer programming languages with advanced experience in data mining and visualization. Machine Learning (ML) is an ever-changing, multidisciplinary field, so staying up to date on the constantly-evolving algorithms, techniques and models is crucial. For this reason, the most important trait for any aspiring ML engineer is the ability to be a self-learner. While this roadmap provides guidance for anyone who wants to start a career in the field of ML, it's important to keep in mind that we highly encourage readers to explore other courses and materials beyond those recommended below. It's unnecessary to explain why learning a programming language is an absolute must for anyone wanting to start a career in ML. But while many programming languages provide frameworks to work with ML, Python and R have the richest and most constantly growing ecosystems. Python and R have been competing as the primary languages for ML for a while, but Python has been gaining momentum and is currently the most used language in ML . Since it has an easier syntax and a smoother learning curve, we recommend Python as a starting language. 1. It's easy to find Python courses on the internet. If you're willing to pay, one of the best for beginners is the course offered by DataCamp . 2. Codecademy also offers a friendly introductory course , but again, you will have to pay if you want to access more advanced topics. 3. Learn Python offers a thorough course , and the best part is that it's free. 4. We also highly recommend the book Learn Python the Hard Way : It's important to note that even though Python is the most broadly adopted language (and in my opinion should be the first language you learn), R is another very good asset to have. It's particularly powerful for exploratory analysis and data visualization, but it also has strong frameworks for ML and Statistics. DataCamp and Leada offer great courses on R. Now that you know the basics of your programming language of choice, it's time to build your other basic skills: Algorithms : It's essential for a machine learning engineer to know the existing algorithm families and the involved trade-offs, from the basic gradient descent and linear regression to the more advanced deep learning algorithms and reinforcement learning models. A great place to start is Andrew Ng’s machine learning course . Even though this course is based on Matlab code, students are free to use any programming language of their choice. The next step is Andrew's deep learning specialization course , which is an up-to-date course that covers the main deep learning models and their basic usages. The course is made in Python and provides nice Jupyter Notebooks and quizzes to support your learning. If you are not willing to pay for the course (which, even though it is expensive, is highly recommended), the videos are also available on Youtube . If you already have a DataCamp subscription, you'll have access to a few courses that cover some basic algorithms, which can be helpful in reinforcing basic skills and comprehension. Elements of AI also provides some free online courses that cover the basics of Machine Learning in a very approachable manner. And if you're interested in digging a little bit further into Natural Language Processing (NLP), there is a good NLP course provided by Dan Jurafsky--the class itself is no longer available via Coursera, but you can still find course lecture slides on his Stanford Website , as well as in his excellent book, co-authored by James H. Martin, entitled Speech and Language Processing . While we highly recommend that you take these particular courses, there are plenty of other useful classes that you can find in the most popular online learning sites, such as Coursera , Udacity and Udemy . Additionally, though we consider online courses the best way to get started in the ML field, there are also many good introductory books, such as Machine Learning for Hackers , Machine Learning: An Algorithmic Perspective and Data Mining: Practical Machine Learning Tools and Techniques, Third Edition . Mathematics: Most Machine Learning models rely on some level of mathematics to accomplish training and optimization. Because of this, it's critical for ML engineers to have calculus skills and at least a basic understanding of how ML algorithms work and how their hyperparameters affect their behavior. It's also important to understand mathematical notation and to be able to understand, reproduce, improve and apply algorithms found in the literature. So if your math skills are a little bit rusty, we highly encourage you to go back to basic calculus and linear algebra. Khan Academy provides some very good courses on both subjects for free. While math is an important tool to carry in your ML tool belt, don’t feel discouraged if you have a hard time with it. These struggles actually give you a great opportunity to improve, and Hackernoon provides an excellent post on this very concept. Statistics: There are many controversies regarding the differences between Statistical Learning and Machine Learning, but while there are many similarities and very subtle differences between the two, it's a fact that it is not possible to do Machine Learning without using statistics to some extent. It's critical for ML engineers to have a deep knowledge of probability. If you feel like your probability skills are lacking, Khan Academy provides good introductory courses on this subject as well. It is also important to have a strong working knowledge of inferential statistics. You can find good courses on Coursera, such as the one created by Duke University . Finally, knowing Bayesian Statistics proves helpful as well. Courses for this can again be found via Coursera, such as the one provided by UCSC . There are some good books on this subject to supplement your learning, among which is The Elements of Statistical Learning . Now that you know basic theory, it's time to apply your knowledge. The best way to do this is to use existing datasets from well known use cases. Python sklearn framework provides a datasets package with many “toy” datasets that can be used for training and learning purposes. R also provides a package with many famous datasets that can be used for training. Another good place to find datasets for many different problems and domains is Kaggle . The main advantage of applying your knowledge in these training environments is that, while you're creating a machine learning tool from the ground up, you still don’t have to worry about data handling, which can be a very time-consuming step, letting you focus more on technique. Since most of these datasets are famous, it's also easy to find internet examples that can provide some guidance on how to get started. Exchanging knowledge is a great way to broaden your own knowledge base. So now that you have enough knowledge to talk about basic concepts and to engage in ML conversations, do it. Talking about ML with other people can help you see things from new perspectives, raise questions that you haven’t thought about before and find new applications and approaches that you haven’t considered. Meetups and conferences are great networking venues since a lot of people with different knowledge bases and skills join these events. These are great ways get in contact with other beginners who can share helpful materials and courses, as well as more experienced professionals who have already been where you are and can help you develop further. Joining competitions is a wonderful way to start getting serious. While competitions don't provide guidance on the models you have to use or how to train them, as with the existing datasets mentioned in step 3, they do provide clean and ready-to-use data, which saves you the trouble of going after data yourself and doing all the cleaning and handling needed to make it usable. Some competitions also provide forums where users can share their discoveries. These are excellent resources that can help you find your way to better performing models. The most famous competition website is Kaggle . In addition to competitions, it also provides many useful materials that give aspiring data scientists an opportunity to test theirs skills. But even though Kaggle is one of the main resources for ML engineers, there are many other sites where you can join competitions, such as Driven Data , CrowdAnalytix and Data Science Challenge. Steps 1-5 help us get a feeling for what it's like to work in machine learning and data science, but working on a real problem is much more challenging than it seems at first sight. Working on a real-world problem is an incomparable experience. Through this experience, you will face problems you haven't encountered before, like dealing with dirty or missing data, having to handle huge, unstructured datasets or working with real-time data. Even if you don't have a job that gives you this kind of opportunity, you can try to repeat other people's work by searching for a blog post or academic paper that you find interesting and trying to reproduce it all by yourself. Don’t fool yourself--this isn’t an easy job, and it will require a lot of hard work, but it is worth your time. Now you've gotten an idea of what it's like to work on a real-world problem and you know how frustrating it can be since sometimes we aren’t able to get the results we want. But don’t get flustered--you’re just getting started, and there is still a long road ahead. And don’t forget: your self-learning proclivity is your most valuable asset for this job. Machine learning and data science, like most of the technology-related fields, is constantly evolving. So keep studying, and always keep your skills sharp! Sources: \"Roadmap: How to Learn Machine Learning in 6 Months\" by Zachariah Miller via Medium \"The Missing Roadmap to Self-Study Machine Learning\" by Dr. Jason Brownlee via Machine Learning Mastery \"Best Machine Learning Resources for Getting Started\" by Dr. Jason Brownlee via Machine Learning Mastery", "date": "2018-10-10"},
{"website": "Avenuecode", "title": "AC Spotlight - Jest Sidloski", "author": ["Alfredo Moro"], "link": "https://blog.avenuecode.com/ac-spotlight-jest-sidloski", "abstract": "(The following article is property of Avenue Code, LLC, and was originally published with permission at Retail Insider on December 13th, 2020 .) Jest Dempsey Sidloski , VP of Marketing, Customer Experience & eCommerce at Peavey Industries LP/Peavey Mart, discusses how his company is emphasizing community care as it adapts to COVID-19 and merges its brands. Avenue Code: Tell us about your personal career path. How did you get to where you are today? Jest Dempsey Sidloski: I started working at Peavey Mart during high school, which means I’ve spent more than half of my life working for Peavey Industries LP. Interestingly, I went to college for Emergency Medical Services but accepted an invitation to join Peavey’s Manager trainee program during the recession of 2008. At 21, I had 16 staff members and was running a multi-million dollar a year store. I successfully grew sales, improved employee culture, and was able to help impact my community with my team's support. I was then promoted into the Home Office as the company's first corporate Training & Development Manager, where I created an LMS system, and developed a program for finding and training new talent within the company. I also accepted an interim role as District Manager for 10 stores, including our flagship home office store in Red Deer, Alberta. After this, I was promoted to Director of Customer Experience, where I helped define customer journey maps, improve customer experience, manage the complaints call center, and update customer-facing policies. From there, I was promoted to Director of Marketing, which incorporated our customer experience department. In 2020, I was promoted to Vice President of Marketing, Customer Experience in eCommerce. My current role includes flyer, eCommerce, social media, videography, magazines, community sponsorships, partnerships, branding & public relations, customer service, and call centres. AC: What are you personally most passionate about in your career? JS: A passion for client happiness, customer experience, and sales attracted me to retail initially. Today, my passion still starts and ends with good customer experience, whether for external customers (buyers) or internal customers (employees). I try to constantly improve experiences, systems, and software. As a former President of the Suicide Information Center, 100 Men, and The Women’s Outreach, I am very oriented toward charity and service. Peavey Industries allows me to exercise that same passion through company-sponsored philanthropic endeavors while still having time to focus on my family and home life. AC: You’ve spoken of COVID-19 as a positive force for accelerated change. How have your digital initiatives related to eCommerce and marketing pivoted since COVID-19? JS: We were one of the first retailers in Canada to pull our flyers from distribution to improve consumer safety. This led to an instant eCommerce growth strategy, which included adding more products and offers online as well as facilitating a seamless curbside pickup experience. We learned that our consumer was eager to explore our business digitally, which allowed us to put more time and energy into creating better workflows, tighter eCommerce timelines, and new rules to make it easier for customers to shop with us according to their preferences. When we felt it was safe, we re-introduced both traditional and digital flyers and launched a new, comprehensive digital strategy. We also launched an education portal for our consumers in just 2 weeks. This was in response to an incredible shift in consumer behavior and purchasing habits as people began growing their own food, raising animals, and enjoying nature in their own backyards. C onnectedtotheland.info works with Canadians from coast to coast to develop educational blogs, live stream education, articles, how-to’s, and even a podcast that made it to #3 on the Canadian podcast charts under home and garden. This initiative fits within our marketing strategy, dubbed “doing retail different.” AC: What are the most important factors in sustaining and growing retail and consumer brands through the pandemic? JS: My keywords this year are transparency and integrity. These two words speak to everything we’ve done as an organization to sustain and grow through the pandemic. We didn’t have all the answers or all the experience customers were looking for at the onset of the pandemic. We had to transparently tell our customers what we could do today while letting them know what we were working on for tomorrow. We listened to feedback from our staff and our customers and were able to pivot and quickly react to expectations. This was extremely important in letting customers know how we planned to continue to support them and their essential needs. Transparency is at our core - we had daily updates from our president at the onset of the pandemic. With transparency came teamwork I’ve never seen before - we had constant meetings with department stakeholders from Human Resources, Health & Safety, Store Operations, and our supply chain to ensure we were all on the same page. That teamwork allowed marketing to address its aging website infrastructure and sign on to build a new website, which we hope to launch in March of 2021. AC: Which strategies did Peavey Industries LP implement ahead of its larger competitors, and how was this possible? JS: The pandemic was declared on March 11, 2020. On March 20, 2020 we launched and marketed our curbside pickup options. Later on we launched drive through pickup and payment options. Peavey Industries LP brands sell a significant amount of growing supplies, including soils for consumer gardens, and we were quickly able to implement a drive through, load up, and pay system for customers to purchase large quantities of these items without ever leaving their vehicles. This was made possible by technology like mobile payment systems. AC: Peavey Industries LP is recognized for its strong internal culture and brand reputation. How has your leadership style adapted during COVID-19, and what remains constant? JS: The pressure on every single one of our retail employees was incredible in 2020. From balancing consumer expectations to constantly changing rules to new consumer shopping patterns to trying to keep a smile on our faces, it’s safe to say everyone experienced new levels of stress. COVID-19 did one thing: it brought our culture even closer together. One of the greatest things a business can do is listen, both to its customers and to its employees. We ensured we heard as many voices as possible through the pandemic. While we faced criticism for certain decisions, we also received praise, and that led to ensuring we continued to be transparent with each other and with our employees and customers on what we were doing and why we felt it was best for everyone. During the pandemic, I had to throw the leadership books in the garbage, because they simply weren’t relevant. I had to ensure I was not only a manager, but also an ear of support. My employees were afraid, nervous, eager, and exhausted. So was I. My walls came down, and I had moments of utter joy and utter emotional exhaustion. Empathy remained consistent and should be at the forefront of any challenge. Very few people alive today have experienced a pandemic before, and it’s okay to not know how to feel, to be vulnerable and afraid. Empathy allowed us to react calmly, to do what we felt was best, and to admit when we failed. Through an empathy lens, anything is possible. AC: Tell us about Peavey Industries LP’s decision to merge its brands this year. What opportunities are in store, what challenges are you facing, and how are you addressing those challenges? JS: One thing COVID taught us early on was that if we were going to be disruptive, now was the time. Peavey Industries LP purchased TSC Stores in 2016 and operated with two major banners in Canada. Both brands shared a similar history: they both launched only one year apart in the 60’s, both were originally American-owned companies bought by Canadians, and both offered similar segments of consumer products. With our eCommerce growth in 2020, a consumer market share increase, and the merging of our systems and technology, we still had two separate major corporate brands. (We also have a regional MainStreet Hardware brand and a primarily dealer-owned ACE Canada brand.) For Peavey Mart and TSC Stores, it meant one team doing double the work. We had 2 flyer programs, 2 websites, 2 social media accounts for each platform, 2 brand stories, and 2 different logos. But that’s about all that separated the brands since the acquisition. Peavey Industries LP brands. Image courtesy of Peavey Industries LP. The company has spent the last 4 years aligning product selection, pricing, customer service, systems, and even rewards programs. So it made absolute sense to continue our path of positive distribution in the Canadian retail landscape and to rename our TSC Stores to Peavey Mart - a brand that has supported rural communities since 1967. With this change comes better marketing strategies, more philanthropic opportunities, better culture alignment, and, most importantly, we can proudly say we work for the same company that’s aligned to create a unique retail experience in Canada. AC: What is the key to successful strategic partnerships? JS: For me, it comes down to reputable engagement. Everything we do at Peavey Industries is based on being authentic, real, and transparent with our customers, suppliers, and employees. Successful strategic partnerships are based on alignment in these areas. AC: How have both COVID-19 and your decision to merge brands driven changes related to in-store experiences? JS: “We want our customers to love us” is a phrase we use in the executive group and with important strategic pillars in our company. From new policies to the way our consumers shop with us to our employee service training to rewards and promotional programs, we must always ask ourselves: will this make our customers love us? This opens a door to ensure we continue to listen with intention and heart and to make changes to do what people need us to do. We are a retailer, but that doesn’t mean we can’t also be a community builder and a proud Canadian partner trying to do things a little more personally. AC: Thanks for your time today, Jest! It’s a pleasure to hear how Peavey Industries is caring for its community as it continues to adapt.", "date": "2021-3-24"},
{"website": "Avenuecode", "title": "AC Spotlight - Eduardo Matos", "author": ["Andressa Lopes"], "link": "https://blog.avenuecode.com/ac-spotlight-eduardo-matos", "abstract": "Eduardo Matos, CTO at GetNinjas, speaks with us about implementing and maintaining Agile practices through rapid growth and global expansion. Avenue Code: I’d like to start by thanking you for the opportunity to talk, and the first thing I’d like to discuss is your professional career. How did you become the CTO of GetNinjas? Eduardo Matos: Thank you for the opportunity! I discovered GetNinjas through various events and people who worked there. About 16 years ago, I started my career teaching HTML and CCS at the Weaving Union and subsequently at various companies in São Paulo. Along with some partners, I had a consultancy catered to clients who saw opportunities to deliver higher quality solutions over a longer term; usually clients want short-term solutions, but we developed systems that offered more in the long run. During this time, I developed sales and management skills that have helped me in the 3 plus years I’ve been at GetNinjas. I started here as an expert frontend developer, became a team tech lead, then took on the interim CTO role, and eventually become the permanent CTO. AC: We know GetNinjas is a success today. To what do you attribute the success of the company? EM: GetNinjas allows entrepreneurs to have an extra or even primary source of income. Given the market scenario and unemployment difficulties people face, GetNinjas offers a very needed and flexible form of income. So the social impact is huge. For me, this is one of our success factors. It’s amazing to know that we impact people’s lives in a tangible way. AC: What are the biggest challenges of being a CTO in a scenario where technology changes every day? Often it seems like we start studying a technology, and suddenly we have an even newer version of what we’re learning. What do you do to keep up with these changes? EM: It's really a challenge. Here, one of our values ​​is “prioritize and simplify.” This value helps us to avoid being too attached to a particular solution and to focus on “why” instead of “how.” This reduces anxiety about using new technologies. At the same time, we try to use the right tools at the right time. Of course, there are cases where we take a gamble on some technology and realize it doesn’t work, but this is normal. It’s part of learning. We have an architectural decision-making process where we involve experts to make decisions that are complex, using community and market assessments. It’s important for us to stay aware of what’s happening with technology, because it impacts our business in some way. If we don’t evolve, we don’t find professionals who want to develop new technologies. AC: Related to this, how do you prepare your platform for the future when it comes to changes in consumer behavior? We know that consumer behavior changes frequently, and you run a platform that depends heavily on consumer engagement. EM: Today we follow some Agile lean software development methodologies, so we have short interactions to figure out consumer pain points as quickly as possible. Again, this process is not perfect. It's easy to say, but it's very hard to apply, especially in software development. I’m a programmer, and I know that when you start creating a solution, you become attached to it. You want to do it the best you can, with all the perfect scenarios. Our job here is always to make people aware that someone is using this platform and we need to solve these pain points as soon as possible. It's not a matter of delivering too much at a super fast rate, it’s about time-to-market. If we waste time, our consumers will look for another platform or another way to solve these problems. AC: Have you seen any significant changes in consumer behavior since you arrived at GetNinjas? EM: Consumer behavior varies a lot depending on the situation in Brazil. We realize that some political decisions or market decisions directly influence our company. Beyond this, one particularity of our business is that the peaks for new professionals joining our network and for people placing new orders is at the beginning and end of each year. AC: Could you name some of the biggest challenges your team faces in implementing strategic decisions? EM: Our biggest challenge comes from the fact that we have a horizontal platform. We have so many professionals using our platform, from masons to day laborers to accountants and more. Our biggest challenge is creating a platform that serves everyone’s needs. To that end, we run several research projects and stay data driven. We have a well-structured data lake, and we do a lot of A/B testing and market research. Today we have five people on our research team, and they run everything very quickly. They’ve adopted everything from interviewing people on the street to crafting surveys to conducting research within our applications. The challenge is always creating a platform that meets the needs of so many unique services with ease of usability. AC: How do Agile methodologies impact your business? EM: GetNinjas was built with Agile methodologies from the start. We used Scrum eight years ago when the company was founded to try to improve our internal processes, workflow, and especially to gain visibility into our deliverables. Over the last two years, we've moved a little to Kanban to understand the flow of things and seize opportunities faster. We have an Agile Coach who helps us, and we’re hiring more Agile experts because we see that agility is about business even more than it is about software development. It’s easy to talk about agility in the context of software development, but today we’re focusing on business agility - controlling our portfolio, seeing if we have overlapping decisions between teams, seeing if we’re meeting market demands, etc. A well-defined strategy reflects directly and positively on the operational part of development. If operational and strategic decisions are not aligned, things never work well. AC: We know that the company was born because of the CEO's difficulty finding a painter. Many companies arise from real needs we have on a daily basis. Today, GetNinjas is the largest service hiring application in Latin America. As a CTO, what are your biggest concerns with this rapid growth? EM: We have been operating in Mexico for 10 months now, and we plan to expand to other countries as well. One of our concerns is that our platform, in addition to catering to so many industries, must now also cater to different countries. So our concern today is a lot about scaling our applications to different countries and getting them ready to handle linguistic differences, cultural differences, etc. For example, service names here in Brazil may be different elsewhere. Our applications already have a microservices structure; we have monitoring, alarms, metric panels, etc. We have a very robust structure to make this scale happen in multiple countries. But it is still a challenge for us to control demand, especially when considering how to roll out new features for different countries and how to keep everything in line with the same business strategy. AC: I’m sure this growth mandates changes in structure and technology. How has this impacted your team’s internal culture? EM: When we started talking about Agile development coming into business, it had a very positive impact on the company. At first it was a little difficult for everyone to understand these changes, but gradually everyone has recognized their value. In the wider market, we go through times of ups and downs in agility. In the beginning, when the concept was new, people adopted Agile practices more than Agile values, which meant that practices could quickly become process obligation. We went through this journey at GetNinjas too, so today we’re restoring an understanding of Agile values, even in development. Sometimes teams are very focused on the solution, and we’re trying to expand the focus to purpose and company goals in the medium and long term. If everyone in the company understands this, it’s easier to show the value of changing practices, even if it’s painful or even if it means abandoning projects that no longer serve the bigger purpose. AC: How do you see Agile methodologies as a tool for evolution in the IT market? EM: Today, I think I see the need to look at Agile values. When we look at the Agile principles created 18 years ago, we see that we’re still trying to solve the same problems today. The challenge for Agile communities in general is to try to show again the values ​​of collaboration, of engaging the consumer in the whole process of development, and of interaction between people. We have an event now, Agile Brazil, and one of the people who signed the original Agile manifesto recently spoke there about the heart of Agile. He talked a lot about collaboration. So for me, I think it’s important to focus on the value of collaboration--not just operational collaboration, but collaboration between all business areas (development, research service, etc.). The biggest challenge today is trying to get these values ​​back into product development. AC: Thanks for chatting with us today, Eduardo! It’s great to have your perspective on maintaining Agile through rapid growth.", "date": "2020-1-1"},
{"website": "Avenuecode", "title": "Defining the Differences Between Physical and Logical Database Administration", "author": ["Guilherme Teixeira"], "link": "https://blog.avenuecode.com/defining-the-differences-between-physical-and-logical-database-administration", "abstract": "A Database Management System (DBMS) is a software that enables users to create and maintain a database. It facilitates the process of defining, constructing, and manipulating databases for various applications. Commonly known examples include DB2, Oracle, Sybase, and Ingres. A DBMS requires a basic architecture and structure to handle the process and files related to the database software, which means that a physical and logical architecture of an Operational System must exist. A basic database and instance structure is based on physical files and logical files/structure. The physical files represent the real files generated by the DBMS and written directly to the Operational System disk or Storage Area. The logical files/structure are handled by the DBMS and have reference to the physical files. Basically, a database has a main concept of tablespaces (or database files), which contain the information about all database objects like tables, indexes, procedures, etc. The DBMS is responsible for handling this logical structure of database objects and saving them on physical files. A database administrator (DBA) is an IT professional responsible for managing and maintaining a DBMS and taking care of databases. In some cases, DBAs have special authorization, which gives them the ability to do almost everything in the database subsystem as well as the responsibility of taking care of all of the databases. Inside the database administration role, there are two specific and distinct sub-categories: physical DBAs, also known as system DBAs, and logical DBAs. It is true that in some cases we have a DBA working in both physical and logical administration, but we need to remember that each role has its own distinct responsibilities. A logical database administrator is responsible for generating the database model and tables design, defining the model of the database objects, and creating procedures and functions for the application and its users. It's unusual to see a logical DBA taking care of DBMS installation and upgrades, applying version fixes, verifying the database CPU and memory consumption, or addressing anything related to database backup strategies. The logical DBA usually works closely with the application owner and developers to define the structure of the database model. The logical database model is a set of conceptual tools used to describe data and their relationship and restriction of consistency and integrity (i.e. Entity-relationship model and Relational model). Entity–relationship model example from Lucidchart This part of the database design is extremely important since the logical DBA and application developers generate the model with all tables and the definitions and relationships between them. This model contains all tables and the specification of each one, including all columns for each table with their respective datatypes and lengths. This logical model has the basic information about how the data will be logically stored inside the DBMS. With the model in hand, the logical DBA can provide an average for the size of the rows in each table, which is valuable information considering that the data will be saved and retrieved from a disk/storage - the most expensive operation in a system. As mentioned before, a DBMS is a software that requires an Operating System. The physical DBA is the professional responsible for installing, upgrading, migrating, and maintaining the DBMS. To be a physical DBA, you don't need to have a deep understanding of the purpose of the database or what exactly the application and users are storing inside the tables - the business purpose of the database is a subject that doesn't directly concern the physical DBA. Other responsibilities of a physical DBA include authorizing access to database and database objects, monitoring database usage, acquiring software and hardware resources as needed, monitoring and tuning database performance, designing backup and recovery strategies, and defining/modifying the physical organization structure. Once the DBMS is installed and ready to be used - with new instances and the database already created - it's time to start thinking about the other part of the physical database architecture: the physical data model. The physical data model provides a low-level concept that describes the details of how data is stored on the computer. The physical model is extremely important since it supports the detailed functionality of the DBMS, including how it will save and retrieve data from the disk/storage. DBMS Architecture Sample -  Image courtesy of IBM LabChats Oracle® and IBM DB2® Architecture As depicted in the image above, this is where the physical and logical DB worlds meet. Based on the logical model, the physical DBA can build an improved physical model. The average size of the table rows, the large objects (LOBs), indexes, and constraints will drive the DBA to build a physical model that focuses on the performance and improvement of I/O operations, calculating how many tablespaces will be required and what will be stored in each one - usually separated by normal data types, large objects, and indexes - and the memory allocation size (buffer pools) to better fit the data retrieved from the tablespaces. You can think about all of this information in this way: the database is a box. Everything inside this box is the responsibility of the logical DBA (tables, procedures, functions, etc.). Everything outside the box that keeps the box up and running is the responsibility of the physical DBA (database storage configuration, DBMS health, memory and disk utilization improvement, etc.). This article is meant to give an overview of a DBMS and to offer a more defined understanding of the responsibilities of physical and logical DBAs and the relationship between them. It is true that DBA roles are not limited to the above-mentioned tasks. There are a thousand other activities related to database administration and database manipulation - like ETL, KDD process, and Business Intelligence--but this overview is meant to clarify any ambiguities about the database and the responsibilities of logical and physical DBAs.", "date": "2018-10-3"},
{"website": "Avenuecode", "title": "How to Beef Up Your Security Using OAuth2 External Provider In Mule", "author": ["Anupam Gogoi"], "link": "https://blog.avenuecode.com/using-oauth2-external-provider-in-mule", "abstract": "The main goal of choosing an Authentication Protocol is to avoid the heavy lifting on development side and to make it easy on the API users with regards to consuming APIs. With OAuth2, the user authenticates as a virtual user with the same credentials they normally use to access the web app. Many developers will find that with OAuth2, their application security will be strengthened and their workload lightened. In this tutorial, I'm going to explain how to apply OAuth2 policy (using external provider) for managing clients in Anypoint Platform. OAuth2 is the industry-standard protocol for authorization. It enables applications to get limited access to resources on an HTTP service, such as Facebook, GitHub, Twitter, etc. The framework describes a number of grants for a client application to acquire an access token, which can be used to authenticate a request to an API endpoint. OAuth2 specification describes five grants for acquiring an access token: - Authorization code grant - Implicit grant - Resource owner credentials grant - Client credentials grant - Refresh token grant For more details, please check the link: https :// oauth . net / 2 / Mule supports various security policies such as Client ID enforcement, HTTP Basic authentication, Spring Authentication, LDAP security manager, etc. to name a few. Most importantly, it supports OAuth2. As per Mule documentation, it supports two providers out-of-the box. They are: - OpenAM versions 11 or 12 - PingFederate These providers can be configured in the Access Management section of Anypoint Platform. Alternatively, one can use the OAuth 2.0 Access Token Enforcement using External Provider policy to secure an API in Anypoint Platform. In this article, I am going to describe how to create an OAuth2 External Module with grant type Client credentials and apply the module against a simple API. Mule documentation provides an example of how to use External OAuth Provider for client authentication; however, the documentation does not fully explain the internal workings of what goes on under the hood. Therefore, in this article, I will try to explain some internal processes as well as mount an External OAuth2 Provider. The article will be divided into the following parts: -Creating a simple API in API Manager -Implementation of the API and publishing it to Cloudhub -Creating an External OAuth2 Provider -Creating Proxy and Portal for the API as well as applying Security Policy -Testing OAuth2 I have created a very simple API named employee (version v1) in the API Manager. The code snippet is as shown below: Now, implement the API in a project. I am using the simplest implementation for this article because it's not our main concern. Here is the simple implementation of the API: As per the implementation, if we hit http :// localhost : 8081 / api / employees we should receive the result in local. Let's suppose that after deploying it to Cloudhub the service is available at: http://employees-service.cloudhub.io/api/employees Creating the External OAuth2 Provider This is the most exciting part! Here is the complete source code . The code is extremely simple, but needs some explanation. Try running any Mule project, and you should observe this log output: Internally, there is an agent class APIPlatformClientCoreExtension that looks for two property values ( client_id and client_secret ) to connect to the Anypoint API Manager. So, what are client_id and client_secret ? These are simply the credentials of your Anypoint Platfrom. Just browse to the organization link of your Anypoint Platform account, and you should see this infomation: You can provide these credentials in the mule-app.properties of your project as shown below: That's it! Now run the project and you should see the following logs: This means that now your application can connect directly to the API Manager. Awesome, right? To create an OAuth2 provider, you must have the Enterprise Security module installed in your Anypoint Studio. Please read the first part of this article about how to install the security module and create a maven project in order to use its security components. The OAuth2 provider that I have created is very simple, as shown below: The most important part is the configuration of the OAuth provider module: From the diagram, it's clear that the Access token url is oauth/token and Authorization url is oauth/authorize, but is that enough? Of course not! So, what's the complete url for the Access token then? Let's dig into this. Let's look at the xml configuration: In the oauth2-provider:config element, we have defined listenerConfig-ref so that it refers to the HTTP listener which is localhost and with the port 8081. So, our Access token url will be: http :// localhost : 8081 / oauth / token ?<params> (We will go to the params soon!) You can see that the clientStore-ref element of the oauth2-provider configuration is pointing to: That's it. It's the simplest External OAuth2 provider. Now let's test our OAuth2 provider. Run the project and in Postman, fire the request with the following request params to http :// localhost : 8081 / oauth / token : grant_type =client_credentials client_id =any client_id registered in your Anypoint platform. We simply send our Anypoint Platform client_id. client_secret =client_secret, related to the client_id, registered in your Anypoint platform. We simply send our Anypoint Platform client_secret. And in return you get the access token. Awesome! Now let's look into the code again: If you look into the Operation of the OAuth2 provider, you can see that we have defined it as Validate . Now, look into the HTTP Listener path: So, to validate the access token retrieved in the previous section we must make a request to: http :// localhost : 8081 / app / validate ? access _ token = <access token received> Here is the response: This means that our token is valid. Now, let's deploy the provider in Cloudhub: Now, let's fire the same thing to the following url: That's it! Our Provider is working perfectly, so let's validate the token and fire the request to: http :// provider . cloudhub . io / app / validate ? access _ token = <obtained in the previous step> You should get a positive response. Create Proxy Once our API definition and implementation are in hand, we can create proxy for the API. Now what is API Proxy ? In short, API Proxy is a layer that sits above our API implementation. It serves as kind of shield to protect our API implementation. Through API Proxy, one can govern the APIs by applying various security policies, SLA tiers, etc. Here's how you can create the API Proxy: Click your API and its version (employee, v1) so you can see the API Status block. Then, click the configure endpoint link and configure the proxy. Here, the most important thing is the Implementation URI that you point towards the actual implementation of the API. In our case, it will be http :// employees - service . cloudhub . io / api , which we have implemented and deployed in Cloudhub as described in the previous section. Then configure the proxy: The proxy url will be made public for the users. In our case it is: http :// employee - proxy . cloudhub . io / employees . After defining the proxy, let's create a portal for our API. Browse the API, click its version, and create a basic portal. Here is a snapshot of my portal: Browse the API and its version, and click it. In the left hand side pane, click the Policies link and apply the External OAuth2 policy, as shown below: In the Access token validation ur,l please enter: http :// provider . cloudhub . io / app / validate Now, let's try to access: http :// employees - proxy . cloudhub . io / employees You shoud get this response: With the client_id and client_secret received, you can now get the access token: Finally, let's access our employees' service: You can now successfully access the service using the OAuth token! In this tutorial, I have shown how to apply OAuth2 to manage the Clients using an External Provider. If you encounter problems while configuring the OAuth2 provider, set the http.port=8081 in the init.properties file and if you want to know more about our MuleSoft solutions, don't hesitate to contact us and learn why leading enterprise organizations are turning to Avenue Code as the preferred systems integrator for MuleSoft Anypoint Platform.", "date": "2017-10-3"},
{"website": "Avenuecode", "title": "AC Spotlight - Amanda Willinger", "author": ["Holly Camponez"], "link": "https://blog.avenuecode.com/ac-spotlight-amanda-willinger", "abstract": "We were honored to catch up with Amanda Willinger, Vice President, PR, Digital & E-Commerce at LAGOS.  Amanda has an inspiring career path, beginning with her origins in the art world and rising through the ranks to become VP, Digital & E-Commerce for LAGOS, a designer fine jewelry brand. AC: Amanda, you have an impressive career path spanning multiple verticals within the retail industry. Can you tell us a little bit about how you got where you are today? AW: In my experience, it’s important to follow your passion and work hard to achieve success. Of course, good timing and good luck don’t hurt either! I started my career in the art world because it was a real passion of mine. Initially, I had planned to go to law school, but eventually decided to continue to pursue my lasting interest in art and jewelry, and was fortunate enough to gain experience while at Christie’s and Sotheby’s in subject matters as diverse as impressionist art, modern art, jewelry, and memorabilia. From there it was a natural transition to jewelry - I’ve always been fascinated by the craft and creativity that go into it, as well as the meaning it tends to carry. Ask a woman about her jewelry, and there’s a moment or story there. AC: During the course of your career, you’ve moved through several roles, from production, marketing, creative services, and now leading the digital and e-commerce efforts for LAGOS. What have you noticed about how shopper behavior is changing? AW: We are living in a unique time with a radically shifting retail landscape. At the same time that physical stores are closing and challenged to evolve their business models, Amazon is capturing 25% of all US retail sales and more than half of all online sales. With rapid delivery available even for luxury items, how can traditional luxury retailers compete? Frankly, I love being part of the retail landscape at such an interesting and dynamic time. In general, luxury is not ahead of the curve when it comes to innovation in the digital realm. The opportunities are there - we can be even more innovative with our brand, gain even more market share. And the timing is right for our consumer - women have learned that jewelry is not something you have to wait for a man to buy for you. Women buy jewelry for themselves like they would handbags or shoes. A few pieces from LAGOS. Image via LAGOS. AC: LAGOS’ founder, Steven Lagos, has been quoted in Forbes as saying that strength and integrity are the values that make the LAGOS brand so successful. How do you see those values informing the way the company operates? AW: LAGOS has been in business for 40 years, and we have people who have been here since the beginning. I’ve never worked with a company that has, across the board, such high morale and dedication. Integrity impacts everything that we do, starting from the senior level all the way down. From Steven, our founder, to Chris Cullen, our President and CEO, there is a ripple effect to every person in the company. Steve famously challenges anybody to call up our customer service and talk to anyone - you will never walk away disappointed. Things go wrong, of course! That’s part of running a business. But if you can turn it around in a way that creates an elegant solution for your customers, this fosters loyalty. Our goal is always to keep our customer at the center of everything we do. AC: What are your personal values? What is the most important ingredient for success in today’s retail environment? AW: I have a quote on my wall from Winston Churchill: “A pessimist sees the difficulty in every opportunity, and an optimist sees the opportunity in every difficulty.” That quote pretty much sums up what I think is important. For me, it’s about embracing the basics and keeping your eye on the ball. It’s important to have a plan and assemble a team with the right mix of diligence, integrity, and high-level execution. What I really love about this company is that we’re small enough to make good ideas happen, no matter where the idea comes from as long as it’s supported by a solid plan. For instance, we were able to completely rebuild and replatform the website, which was not an easy prospect. It was a huge endeavor that affected the entire company - and I had complete support to do so, from operations to marketing to customer service. It’s a complex task, and I don’t take anything for granted - any sale or customer. Creating and developing the digital business has been a great joy for me. Since we redesigned the site, we’ve seen seven times our previous sales volume. AC: In your role as VP of Digital & Ecommerce at LAGOS, what is the most important thing you hope to bring to the company? AW: Gestalt theory states that when there is a unified whole comprised of many parts, the sum of the parts becomes much better as a whole. This applies to what I do, because it’s not just digital - it’s also product design, marketing, customer service, etc. It’s our overall value proposition - and our commitment to deliver an exceptional product experience to our customers. It’s never done! That’s what I love about LAGOS. We have a huge opportunity for growth, and right now our biggest opportunity is brand awareness. AC: Amanda, it was a pleasure hearing from you regarding LAGOS’ efforts on staying ahead of the luxury retail curve. It sounds like you guys are primarily focused on becoming even more innovative with the brand and gaining more market share. The focus on fostering customer loyalty and maintaining the integrity of all aspects of the brand is inspiring. We look forward to watching LAGOS grow exponentially! This article is property of Avenue Code, LLC, and was originally published with permission at Total Retail .", "date": "2017-10-26"},
{"website": "Avenuecode", "title": "AC Spotlight - Audrey Gauthier", "author": ["Alfredo Moro"], "link": "https://blog.avenuecode.com/ac-spotlight-audrey-gauthier", "abstract": "Audrey Gauthier , VP of Marketing and E-Commerce at Little Burgundy, highlights the changing priorities of the Canadian customer and how brands are adapting to deliver. Avenue Code: Tell us about your career path. How did you get to where you are today, and what drew you to Genesco? Audrey Gauthier: After completing my Master’s in Business Administration at Sherbrooke University, I started my career at La Vie en Rose, and I feel in love with the creativity and endless possibilities of e-commerce. I then transitioned to ALDO, where I rapidly assumed new omnichannel management responsibilities for in-store marketing in addition to managing digital optimization for e-commerce. From there, I became VP of Marketing and E-Commerce at Little Burgundy and stayed with the company when it was acquired by Genesco. AC: What was the impact of COVID-19 on Little Burgundy? How did your brick and mortar strategy change? How did your digital strategy adapt? AG: Business-wise, it’s been very challenging. We’re selling non-essential products and goods, so even though our e-commerce is running effectively, the closure of our stores had a big impact on the overall business. That said, Little Burgundy and Genesco as a whole have been able to navigate this challenge well and use it to fast forward our omnichannel strategy. For example, we’re currently transitioning to a new CRM platform, and our IT team is working to enable curbside pickup. It’s been a tremendous year for e-commerce transformation, as we’re investing in new tech that will benefit our long-term growth. AC: What have you learned about the Canadian consumer and how their preferences and needs have changed during COVID-19? AG: We’ve done a lot of research on the Canadian consumer, and we found that the pandemic significantly impacted everyone’s state of mind. We’ve all been exposed to an increase in anxiety on multiple levels, not only due to social isolation, but also due to job loss and monetary stresses. This resulted in several factors for businesses: The consumer is more concerned with sustainable and recyclable materials as part of an evolving social consciousness. Buying priorities are also determined by a desire to support local brands, as well as the practical need for high product availability. Consumers also want to support businesses that show value transparency and an embrace of diversity. And it goes without saying that the consumer is moving more toward digital-first than ever before. AC: What are the keys to creating a successful e-commerce strategy and omnichannel experience? AG: It’s essential to be very consumer-centric and improve our omnichannel experience. Of course we need to attract customers to our e-commerce platforms, but we also need to give a lot of thought to the after-purchase process to ensure speed and accuracy in everything from shipping to returns and reimbursements. AC: What are your current and upcoming innovation projects? AG: We’re utilizing new technology to gain a single view of the customer by integrating our systems for various online and in-person touchpoints. System integration is key to improving the omnichannel customer experience. AC: What trends are you seeing in e-commerce as a whole right now? AG: Curbside pickup is a big one. Many stores, ours included, had a process in place for ordering items from sister stores for pick up, but now there’s a need to ensure inventory is available at any given location almost immediately. Related to this, same-day delivery is becoming the norm, which is forcing e-commerce companies to evolve logistically. Finally, investing in the after-purchase experience is huge: because online purchases are the norm now, potential buyers want to read customer reviews to feel confident in their purchases, which means companies need to exert effort in delivering a positive experience. AC: What is the key to successful strategic partnerships? AG : Clear communication, setting solid KPIs, and trust. We saw all of these magnified during this past year, as there was greater flexibility for payment timelines and much more sharing of resources and ideas. I personally believe that trust has to be built over time. AC: How would you like to see Little Burgundy’s marketing and e-commerce division develop this year? AG: I see a lot of opportunities for us to increase our market share, improve our omnichannel approach, and attend to our customer journey to differentiate ourselves from other players. We also have room to grow our brand awareness throughout Canada. Finally, I see Little Burgundy becoming a more predominant e-commerce player in Canada for multibrand footwear, bags and accessories. AC: On a personal note, how have your own work-life balance and management style adapted since COVID-19? What new challenges and opportunities have opened for you during the pandemic? AG: It’s been a really good year for me personally, even with all of the challenges. Gaining back commute and travel time has allowed me to adapt my personal routine, focus on running goals, and improve my management style. Now, I’m able to get more visibility on everything my team is doing. Instead of looking only at results, I have more time to be able to support better my team in their processes and strengthen their methodologies. One woman on my team actually commented that our regular Zoom meetings have “humanized” her colleagues because she has the chance to see them in their personal environments. AC: Thanks for your time today, Audrey! It’s been great to get your perspective on how COVID-19 has changed the priorities of customers and what that means for e-commerce businesses.", "date": "2021-5-11"},
{"website": "Avenuecode", "title": "How Important is Company Culture to Project Management?", "author": ["William Hom"], "link": "https://blog.avenuecode.com/how-important-is-company-culture-to-project-management", "abstract": "Understanding and navigating company culture is an important skill to develop as a project manager in order to work collaboratively with your team and with management. You may be great at putting together project plans, running efficient stand-ups, and facilitating team meetings, but if you are not able to adapt to the company culture early on and be accepted by your team, then it's difficult to gain credibility, respect, and traction to get things accomplished. Sometimes, you may feel like a victim in a finger pointing culture. Rather than playing the victim and not getting things done because you feel you work in a dysfunctional or toxic culture, establish yourself as someone who can quickly get up to speed without much direction or guidance. Be independent. Don’t push processes on the team in the name of best practices. That may not go over well. Be open to suggestions and ideas of what works well in that organization and what team members want to see changed. They are your true sounding board. The important thing to remember is that you are there to get things moving, together with the team, in a forward direction; it is your responsibility to complete projects on time and on budget with minimal interruptions, noise, and disruptions outside the team. Image courtesy of Shutterstock I’ve worked in different company environments, ranging from start-up e-commerce companies to large telecom companies and large retail e-commerce companies, and they all have one thing in common: their culture changes over time. The culture that you adapt to on the first day on the client site will change, and so will the company's people and management. Stay focused on basic project management principles, and always exhibit a positive, can-do attitude, listening carefully and understanding ideas others are trying to convey to you. Be open to criticism, own the issues, and do your best to ensure that people stay positive and are having fun doing their jobs. Be the social coordinator. Inject fun into the project team by keeping things light and simple. As an example, I brought in Nerf toys, which people initially thought was a little strange, but then they got used to shooting hoops outside their cubes. I’ve also brought in many treats, such as chips, candies, and cookies to a common area. If you can develop a reputation as someone who is easy to work with, a thought partner, a team player, and a reliable resource to get things accomplished in any role assignments while making things fun as a project manager, program manager, or scrum master, your position will be solidified in the company culture. This poetic quotation from Bruce Lee speaks to the resourcefulness needed to succeed in different types of environments. Image courtesy of KeepInspiring.Me There are many phases and flavors of company culture that exist in an organization and even within various departments of the same organization. As I mentioned earlier, company cultures are dynamic and change (dramatically or subtly) over time. Below I've described an example of this growth and broken it down into three phases based on my experience of working with a retail e-commerce company over an 8-year period. (My experience at this company was not isolated--I have encountered the same cycle at another e-commerce company, where I was part of both the \"Prosperity/Startup Phase\" and, after approximately 7 years, a transition into the \"Reset/Transitional Phase,\" which eventually grew into the \"Opportunity/Mature Phase.\") When I joined, the company was pursuing big plans for expansion and enhancement. There was a sense of urgency and momentum typical of a startup environment. From my very first day with this company, I felt a sense of ownership and camaraderie within the team. The culture was defined by people who were motivated, energized, positive, and cordial to one another. Management emphasized that there was no place to blame or fault others, and it was great to see people dedicated and willing to take on risks and to appreciate differing viewpoints. There were social gatherings held outside the workplace after hours, which formed a basis of trust and friendship and enabled a collaborative culture. Over the course of approximately 7 years, however, the culture changed, and so did its people. From my perspective, the culture seemed to change from a startup to a slower growth, “mature” company where revenue growth was not accelerating as fast. This is what I call the Reset Phase. The Reset is a subtle, transitional phase, which in my experience takes place over the course of a year where there are budgeting and departmental reorganizations. As the initial management team begins to leave the organization to pursue other opportunities, new leadership takes over and focuses on the fiscal discipline and operational expenses of the company, accelerating the reset. The new leadership looks at ways to re-invent their business model to boost revenue growth while at the same time managing shrinking capital and expense budgets. That is when the culture starts changing, and people need to adapt to the direction set by the new leadership team. As the company shifts towards developing a strategy to revive the business, restructuring departments and the churn of people leaving (which can be good or bad) becomes prevalent throughout the organization, and hence moral may suffer. As the frequency of the restructuring and churn increases, there are several opportunities for you to take advantage of this phase, where alignment to the new business strategy allows you to make a difference and stand out during the transition. This is what I call the Opportunity Phase, since introducing ways to cut costs and implement process efficiencies to help stabilize churn will be what sets you apart. For example: Culture change will occur. The biggest takeaways are to always stick to the basics, to go the extra yard in picking up more projects, and to make sure you have confidence in your ability to execute and stay focused.", "date": "2019-5-1"},
{"website": "Avenuecode", "title": "Domain-Driven Design & Onion Architecture", "author": ["Diego Souza Rodrigues"], "link": "https://blog.avenuecode.com/domain-driven-design-and-onion-architecture", "abstract": "Today, we'll briefly introduce the basic concepts of Domain-Driven Design and Onion Architecture and highlight some advantages of bringing these two approaches together. First, we have to answer the following question: what is domain? DOMAIN Domain is a sphere of knowledge. It refers to the business knowledge that our software is trying to model. So what is Domain-Driven Design? DOMAIN-DRIVEN DESIGN DDD is an approach where the main focus is on real business values more than on technologies, frameworks, etc. Concepts and technological details are important, but they are secondary. In order to have a clear understanding of the business and its domain, it's essential to take a collaborative approach involving both technical experts and domain experts, who should both use the same language for effective communication. Another important point is reducing complexity by using object-oriented design and design patterns to avoid reinventing the wheel. Domain Experts are the people who know the business rules. They may be accounting managers, marketing specialists, cooks, waiters, etc. They may or may not have any knowledge in technology. Technical Experts are the people responsible for translating the domain experts' knowledge into software. For this communication to be effective, it's important to use a common language between technical experts and domain experts. We call this common language: Ubiquitous Language, which should be used in all forms of communication, from meetings and documentation all the way to source code, becoming the domain model implemented in the code. BOUNDED CONTEXT When using ubiquitous language, each term (though similar) should have only one meaning. Unlike human language, where words may mean different things depending on context, software does not handle ambiguity well. To handle this, DDD requires that each language belongs to one application context. This context is called Bounded Context . It defines a scope where a ubiquitous language can be used freely. Outside it, terms may have other meanings. Since in DDD the language is the model, and vice versa, each Bounded Context defines the application scope of a specific model. That is, a model is only valid within its own Bounded Context, as shown below: THREE-TIER ARCHITECTURE When we use Domain-Driven Design, we can use different architectures. There is, for example, the traditional three-tier architecture. These layers can be split up into smaller layers. The basic idea is to have the presentation layer at the top, the business/domain layer i n the middle, and the data access layer at the bottom. Onion Architecture is based on the inversion of control principle. It's composed of multiple concentric layers interfacing with each other towards the core. This architecture doesn't depend on the data layer, as in traditional multi-layer architectures, but rather on domain models . We can find some Domain-Driven Design concepts present in the Onion Architecture domain layer, but it's important to point out that DDD and Onion Architecture are not necessarily the same thing. We can use Onion without using DDD. The above drawing is simplified. There are other representations of Onion Architecture that feature additional layers, which can be subdivided into smaller layers. But Onion Architecture usually consists of four layers: infrastructure, application services, domain, and the core. Domain-Driven Design also has a service concept that is slightly different from the concept of an application service. It may cause confusion and force us to always qualify with \"I'm referring to the application service,\" or \"I mean the domain service.\" So it's simpler to name the application service layer API , which also better describes what that layer represents. The arrow pointing from the infrastructure layer to the core layer represents the fact that each layer can see the underlying layers, but the innermost layers have no visibility or knowledge of the outer layers. CORE When we use Onion Architecture, we start with the central layer, the core. We keep all domain objects that have business value in the core. We should never include technological concepts like database, REST, or SQL. The core layer, being the central layer, doesn't even know that your domain, API, and infrastructure exist. DOMAIN The Domain layer is where all the business rules belong. By controlling the domain through the API, and inserting all business logic within the domain, we have a portable application. API We use the API to communicate with the domain. To do so, we must expose only immutable objects, preventing misuse of the API to gain domain access. If we return mutable objects through the API, people using the code could gain access to domain components we might not intend to expose. Although the API has access to the Domain and Core, it doesn't know anything about the Infrastructure. INFRASTRUCTURE This is the outermost layer. It includes adapters for databases, UI, external services like RESTful, etc. It has access to all areas of the API, Domain, and Core, though generally most operations that access the Domain must do so through the API. The exception to this would be something like domain interfaces that contain infrastructure implementations. Domain-Driven Design gives us a more realistic approach to defining what really has business value. With Onion Architecture, we achieve a good level of decoupling and can abstract the technology specifications that are secondary to the business. Infrastructure abstraction makes it easier to adapt and adopt new technologies that best meet application requirements.", "date": "2019-9-11"},
{"website": "Avenuecode", "title": "Using Common Project in Mule", "author": ["Anupam Gogoi"], "link": "https://blog.avenuecode.com/why-use-domain-project", "abstract": "Mule has the concept of a Domain Project to share common resources. However, per the documentation, there are some limitations when using a Domain Project to share resources. Here are the main limitations as per the documentation: Defining flows, subflows, or any message processors as shared resources is not supported. To share a WMQ connector as a shared resource, you need to remove the mule-transport-wmq-ee-.jar file from the $MULE HOME/lib/mule/per-app/ directory and remove all the native jars for WMQ connectivity in your application’s $MULE HOME/apps//lib/ directory. Place all these JARs in the $MULE_HOME/domains//lib/ folder instead. Adding properties in the configuration files of applications that use shared resources can result in issues, as these properties are shared with other apps in the domain and there may be conflicts. You can instead set environment variables. The following connectors and related specifications can be specified as shared resources at this time: HTTP/HTTPS (both endpoints and connectors). VM. TLS Context elements. JMS. JMS Caching Connection Factory. Database. WMQ. JBoss Transaction Manager. Bitronix Transaction Manager. The main power of Domain project really comes into play at the time of Deployment (Mule Runtime). Along with sharing common configurations among the projects using the Domain project, it can also be used to share common libraries among the projects. (I will discuss how to tweak with Domain Project to reduce the Mule deployment time in another article). In general, though, sharing common flows like exception handling, etc. is not a great idea in Domain Project. Mule has an out-of-the-box concept where every flow is treated as a Spring Bean . So, my argument is: why don't we create a project and put all the common flows, resources (HTTP, VM, etc.), and connections and export this as JAR and use it as a dependency in other projects and load the flows as Spring Beans? In this experiment, I will first create a project with shared resources ( my-common-flows ). For the sake of simplicity, I am creating the following shared resources. One HTTP listener. Common exception handling strategy. Most importantly, I am exporting this project as a JAR file instead of the conventional ZIP . Here is the source code of the project. In this project, I have two flows only: This is an empty flow without any flow elements. It only contains an HTTP Listener configuration. This configuration file contains a single flow called common-exception-handling . Also, I have a simple property file with a simple property as shown below. Most importantly, I am not using the ZIP packaging format in the pom.xml file. Instead, I am packaging it as a JAR . Please check this part of the pom.xml file as shown below: Now we are going to use this common resource in a project named my-domain-test . The source code is available here . So, first of all, we will modify the pom.xml file of the project and insert dependency of the common resource project we have created above. Here is a part of the pom.xml : That’s it. Now we can use the shared resources. Here in this project we have a simple flow where I am using the following shared resources, HTTP Listener (declared in connections.xml of the my-common-flows project). Common exception handling (declared in my-common-flows.xml of the my-common-flows project). A shared property (declared in the init-DEV.properties of the my-common-flows project). Here is the simple flow diagram: The most important part in the configuration file my-domain-test.xml is the following: When importing the shared resources in other projects, Anypoint Studio occasionally fails to load the resources. To solve this, just close and open the project. The above discussion shows how easily the common flows and configurations can be shared using Common Project. Thus we can eliminate the limitations of Domain project when it comes to sharing common flows and configurations. However, for sharing common libraries among various projects in Deployment scenarios (Mule Runtime)  under the same domain , Domian Project plays a great role. Let's get in touch about how Avenue Code can guide you as the preferred systems integrator for MuleSoft Anypoint Platform!", "date": "2017-4-19"},
{"website": "Avenuecode", "title": "How Java Garbage Collection Works and Why You Should Care - Part 1", "author": ["Roseane Silva"], "link": "https://blog.avenuecode.com/how-java-garbage-collection-works-and-why-you-should-care-part-1", "abstract": "Developers are well aware that there's a lot going on behind the scenes when it comes to Java, one of which is the famous Garbage Collector (GC). Although its existence is well known, the way it works is not so clear to most (human) developers. Java Garbage Collection is the process responsible for automatic memory management. This process is needed because when Java programs are executed, they allocate objects in the heap. Eventually, some of these objects are no longer needed, so they must be deleted in order to free up memory. Knowing how this process works can help you create better code lines, since application performance is closely related to memory allocation and deallocation. In  languages such as C and C++, developers used to work with the free function and delete operand, respectively, in order to do GC manually (explicit management). There are, however, some problems related to this approach, such as dangling references: when a memory space is deallocated but the object that was allocated in it is still being referenced, a space leak occurs. A space leak is when an object is not being referenced any more but the memory that was allocated is not released. These are the most common problems. It is important to point out that each Java Virtual Machine (JVM) can have its own GC implementation as long as it respects the JVM specifications. The most commonly used JVM is HotSpot, by Oracle, and this is what we'll use today to explain GC. All HotSpot GCs follow the same basic rules: One important point to discuss is memory fragmentation. This occurs when memory is freed up and small pieces of memory are released in many areas; it can happen when there is not enough continuous space to allocate new objects. One way to avoid this is compaction, where memory can be compacted after objects are deleted. This puts remaining objects on a continuous block at the beginning of the heap, which improves the way new objects are sequentially stored after this initial block. Marking all objects is an inefficient process since the number of allocated objects just increases. Since most of these objects are short-lived, we have the JVM Generations concept, which is when a heap is broken up into smaller parts or generations. The JVM Generations concept is an \"age strategy\" where objects are classified by age. Classifications include: young generation (divided into eden and survivor spaces, as shown below), old generation, and permanent generation. Figure 1: Garbage Collection process. Serial A serial collector takes care of garbage collection using a single thread. This is helpful because there is no communication overhead between threads. It can’t, however, take advantage of multiprocessor hardware (though it can be useful for small data sets). Parallel Parallel collectors use multiple threads in order to take care of garbage collection. This is the main difference between parallel and serial collectors. Parallel collectors are ideal for medium-to-large datasets on multiprocessor hardware. One important note is that parallel compaction enables the parallel collector to perform major collections in parallel, otherwise it will be performed in one single thread by default. CMS (Concurrent Mark Sweep) CMS is for applications that can afford sharing processor resources with the GC and that prefer shorter GC pauses (the time GC takes to recover space that is no longer in use). It uses the same algorithm as parallel collector. The main GC process is multithreaded, but it runs simultaneously with the application process. As a quick heads up, the CMS collector is deprecated as of JDK 9. G1 (Garbage First) G1 is intended for multiprocessors with large amounts of memory. It is parallel and concurrent and can achieve high throughput. G1 has gained popularity, and since it is more  efficient, it will replace CMS. One of the differences that makes G1 a better solution than CMS is that it's a compacting GC: \"G1 compacts sufficiently to completely avoid the use of fine-grained free lists for allocation, and instead relies on regions\" ( Oracle ). This feature eliminates some possible problems with fragmentation. With G1, the heap is partitioned into equal heap regions, and the same roles (such as eden, survivor, and old generations) can be applied to these regions, but there is not a fixed size for them, which translates into flexibility, as shown below: Figure 2. G1 Garbage Collection allocation. Image courtesy of Oracle . The Z Garbage Collector This is a scalable, low-latency, fully concurrent GC. It doesn’t stop the application threads at all and can perform all the hard and expensive work concurrently. This is possible because of some new techniques like colored 64-bit references, load barriers, relocation, and remapping. It was introduced with JDK 11 and is intended for applications with large heaps that require low latency. Today we covered the basics of Java Garbage Collection and took a look at the various collectors Java makes available. Understanding how each collector works will help you navigate the best solutions for your applications. Next week, we'll take a look at how to select the best garbage collector based on the scope and parameters of your project, as well as best practices for utilizing these tools. Stay tuned! For references and further reading about Java Garbage Collection, please see the following articles: MANDIC. Java Garbage Collection: melhores práticas, tutoriais e muito mais . Accessed: Dec 4, 2019. THE URBAN PENGUIN. JAVA Object Lifecycle, de-referencing and garbage collection . Accessed: Dec 4, 2019. ORACLEa. Java Garbage Collection Basics . Accessed: Dec 4, 2019. GEEKS FOR GEEKS. Garbage Collection in Java . Accessed: Dec 4, 2019. ORACLEb. HotSpot Virtual Machine Garbage Collection Tuning Guide . Accessed: Dec 4, 2019. ORACLEc. Getting Started with the G1 Garbage Collector . Accessed: Dec 18, 2019. DEVMEDIA. Introdução ao Java Garbage Collection . Accessed: Dec 23, 2019.", "date": "2020-8-12"},
{"website": "Avenuecode", "title": "AC Spotlight - Stephane D'Astous", "author": ["Anna Vander Wall"], "link": "https://blog.avenuecode.com/ac-spotlight-stephane-dastous", "abstract": "Stephane D’Astous, General Manager at Novaquark Montréal, speaks with us about digital transformation trends and bridging the gap between technical talent and end users in the gaming industry. Avenue Code: Tell us about your educational and career journeys. How did you get to where you are today? Stephane D’Astous: My story starts with my father, a famous Montréal architect. When I was a child, 3D CAD software didn’t exist, so my dad built small-scale replicas of everything he designed. This inspired me to employ my creativity to build something concrete, so I studied industrial design, which taught me a lot about how to navigate and unite the sometimes disparate fields of creative art and technical talent, end user and product manufacturer. After this, I decided to pursue an MBA, which required a lot of effort and sacrifice: I took night classes for 4 years while working 50 hours a week in the aeronautics industry at CAE and also raising 2 children. Finishing the degree was a defining moment for me, because I felt like I was equipped to be a great manager, and several doors opened for me. I worked six more years in aeronautics, where I learned a lot about excellent project management and how to serve major clients, before I was offered a job in the video game industry at UbiSoft. I had to decide if I should follow my head or my heart: my head told me that my aeronautics career was safe, stable, and respectable, but I had hit a bit of glass ceiling. The gaming industry, on the other hand, was immature and unknown at the time, but my heart was drawn toward its energy, passion, and creativity, and I felt I had something to contribute to its development. It was a high risk/high reward situation, and I went for it. The move not only allowed me to do what I love - bridging the gap between technical expertise and artistic creativity - but it also allowed me to produce complex, code-based projects for entertainment purposes rather than practical needs. AC: What drew you to joining the Novaquark team in particular? SD: Montréal is full of studios that want to expand to North America, so there were opportunities to join many companies, but I wanted to do something innovative; a middle-of-the road product wasn’t going to attract me. When I met Jean-Christophe Baillie, Novaquark’s Founder and President, I understood that his product was extremely ambitious and maybe even disruptive. His 2016 kickstarter campaign was met with phenomenal support, and he surpassed his objectives. But so far, he was in Paris only. He wanted to utilize Montréal’s talent to transfer the prototype into a product. Novaquark’s values are very important to me, particularly in terms of what the company brings to Montréal. Here we have almost all types of games, from single-player to multiplayer to VR, but before Novaquark, we didn’t have any MMO (massively multiplayer online) games. Other players in the Montréal industry are creating conventional, well-crafted products, but Novaquark is doing something different and innovative. AC: What are the unique challenges and opportunities for you in managing Novaquark’s Montréal office? Have you adapted strategy to fit geography? SD: Before you can add value to something, you have to thoroughly understand the project’s trajectory and where it is in the process. I realized that Novaquark’s product was quite advanced in some areas and behind in others. In business, there are always three important factors: money, time, and quality. For Novaquark, time is the biggest factor, because our product is due to be released this summer. So from the time I joined the team to the time of the product launch, I had only a year and half to recruit a team of specialists and add value. Paris and Montréal have well-known connections and a shared language, so I assumed cultural values would be relatively well aligned. The French love a good argument; afterward, they go back to work like nothing happened. For us North Americans, there’s more of an understanding that if a senior/lead person decides something, we all get behind it and make it happen. While some of these cultural differences have been challenging, our Paris and Montréal studios complement each other in that Paris brings specific technical talent and our Montréal team is more sensitive to the look and feel of the game experience. The Paris team is creating a highly-performant back-end, and Montréal is raising the bar on the UX/UI side. AC: What do you do to stay abreast of innovations in tools and technologies? SD: I try to preserve my lunch hour for reading articles, consulting TEDx, and taking courses on lynda.com, Khan Academy, and Udemy. I’m not a programmer, but I need to understand coding basics to be effective in my role, and these resources are perfect, bite-sized ways for me to understand potential challenges. I believe you have to guard a regular window during your work hours to self-educate, because after hours, it just doesn’t happen. In my job, I bridge tech and art, and I think the most important component of doing this well is keeping things simple and focusing on the human element. We need to focus on users and consumers, because we’re making a game for everyone, not just for developers. I also use a lot of methodologies like Agile because it keeps everyone on the same page, allowing developers and artists to walk through the entire process together and spot departmental discrepancies early and often. Very often, managers operate from a reactive mode and think that one meeting can solve big issues, but actually, you need consistency and recurring meetings to maintain a unified vision. AC: You have 20 years of experience in multiple sectors of the digital industry. What changes have you seen in the gaming industry and wider entertainment industry in the last five years? SD: This is a head scratching question and very fun to consider! I’ve seen a huge increase in the ease of product accessibility. Before, you had to be sitting in front of a PC that was connected to services via cables. Now, we have a huge amount of mobile platforms connected to WiFi and to the cloud for superior streaming and accessibility. Everything is dematerialized, and this happened much faster than I anticipated. This also affects user ownership. We don’t walk into physical stores to buy physical games; in fact, we rarely even buy products to download anymore. Instead of downloading music on iTunes, for example, we rent it on Spotify. Netflix was a very early player here, and almost everyone is moving in this direction. Subscription-based businesses are the new norm. For the gaming industry in particular, a huge change is that game makers now have to offer a free-to-play aspect. If they don’t, their product might not do as well. AC: Which trends do you see emerging in the next few years? How will this influence the wider tech marketplace? SD: AI and deep learning have been big buzzwords in the last few years. There’s massive potential here, but we don’t know yet to what depths this capability will affect us on a daily basis. Montréal is known as an AI hub, but we’re still in an early phase with this technology, and we’re trying to find the right applications for it. Another big trend is the Internet of Things. Obviously this already exists, but I think there’s a phase two coming that will offer even greater connectivity of common objects. Quantum computing is another trend. This is beyond my expertise, but it’s certainly going to change the way we try to solve big problems. There’s also a lot more potential for VR/AR/MR. Everyone was very excited about these fields at first, but they’ve existed for a long time without coming to fruition. Perhaps this is because it’s a flashy, impressive technology that hasn't been mastered for a practical purpose yet. Finally, digital ethics and privacy are becoming an increasingly important issue. With so many objects connected to each other and to outside data warehouses, information is everywhere and privacy doesn’t exist in the same way it used to. AC: What has been a highlight/shining moment for you in the last few years? Was there a moment that you knew you were on the right track? SD: There were three moments for me, two in the multimedia industry and one in the video game industry. When I was hired by Moment Factory, they were known for entertainment events rather than for permanent projects. I was responsible for creating the business unit for permanent projects, and we won an international open bid for a major digital installment at the world-famous Changi Airport in Singapore. Similarly, at Float4, we won several industry prizes for our innovative digital project at the City Walk shopping mall in Dubai. More recently, Novaquark has also received a lot of recognition. Even though our product hasn’t been released yet, we’ve shown demos to gaming veterans from Google, Epic, and Valve, and they have all confirmed that this product is potentially disruptive. To have this kind of validation even before the product is finished is very encouraging, because it means we’re on the right track. AC: What was your biggest “Aha!” moment in the last 6 months as it relates to strategy/management? SD: Some months ago, Novaquark had to make the classic “make or buy” decision for a software tool. The engineering team wanted to build everything internally so that they could control the quality of the code, even though there were proven commercial solutions on the market. After a long debate, I was able to persuade our engineers to buy an existing product. The results were big savings on time and money. Instead of building the product from scratch, we just needed to build a way for it to work with our product. The decision we made seems logical in retrospect because it brought great benefits, but at the time, it was heavily disputed. AC: Do you have any closing advice for our readers? SD: Especially for the younger generation, I want to say, “be resilient.” A lot of young managers don’t like to fail; they want to climb the ladder rapidly and without sacrifice. But especially in high risk industries, you can't be perfectly shielded. If you believe in what you do, you need to roll up your sleeves and make a difference. If I hadn’t been resilient, I wouldn’t have achieved so many things in my career that have required great effort. AC: Thanks for your insights today, Stephane! It’s inspiring to hear about your passion for pursuing the gaming industry when it was immature and to hear about the innovative ideas you’re still pursuing at Novaquark today!", "date": "2020-3-11"},
{"website": "Avenuecode", "title": "An Overall Introduction to Oracle Commerce Cloud", "author": ["Arlindo Neto"], "link": "https://blog.avenuecode.com/an-overall-introduction-to-oracle-commerce-cloud", "abstract": "When the concept of EDI (electronic data interchange) was first introduced in the 1960s , the stage was set for commerce to develop into the online shopping reality we have today. Now, e-commerce is so embedded in our society that we cannot imagine living without it. E-commerce today is not only a reality; it is also a need for a globalized world where trade cannot be constrained by border lines and distances that were before considered insurmountable. With e-commerce, we can sell and buy anything, from anywhere, with a minimum of human interaction. All this is thanks mostly to electronic commerce platforms provided by software industry giants like Oracle and its Commerce Cloud solution. When companies started to use the internet as a means for B2C transactions, their store websites were implemented from scratch. Even all the infrastructure needed to support transactions and inventory management (including an admin console and a report dashboard) were created individually for each company. It goes without saying that this model made breaking into the online market extremely costly since doing so required a full set of software engineers and architects to create and maintain each online system. For larger companies, this was an excellent arrangement. They had the budget for such innovative investments and could reap benefits from the long-term ROIs. Smaller companies, however, were unable to compete in the online market until solution providers entered the scene. Companies like Oracle saw this new market need and realized that every web store was made from a common set of modules. In other words, because of the similarities between online platforms, a single platform could be provided and customized by any business, whatever its size or online history, allowing more companies to compete in the digital world-wide market. Today, the efforts to create a customizable platform have evolved into a solution called Oracle Commerce Cloud, which allows companies to launch fully operational online stores with minimal efforts. OCC provides the following features for administrators to use in the creation of their online stores: In this module, stakeholders can enter and display individual products. They can also create: In this module, the company's marketing department can manage three tools to help convert/retain customers: Additionally, this module can be used to create rules for stacked promotions and to define their audiences. One of the most important features any general platform must provide is the ability for its users to customize their online store with their brand logo, colors, legal information, and general disclaimers. To meet this need, OCC provides a full set of tools for the creation of layouts, themes, widgets, and components. A query system is one of the best ways to sell items in your online store. It allows customers to search for and find products available in the company's inventory. It must, however, be \"smart\" enough to understand what customers are querying by the keywords used, even when their search terms do not exactly match product names or descriptions (or whatever field you could set as searchable). OCC handles these possible obstacles using three strategies: In order to know whether or not business strategies are working well, managers must have a way to track how customers are using the web store and what is and isn't generating revenue (and how much). All of this information can be filtered and customized within the reports module. In this module, a manager or stakeholder can set, among other things, shipping methods, payment types, gateways and allowed billing countries, and taxes and social information for the store. This module also allows users to set up external APIs so that other systems may be used for internal events (such as order placements) or when information is requested from an outside application (such as an existing system already in use by the company). The inclusion of external APIs allows data requests at any point, making it easier to migrate the current web store architecture without too much impact on the business. Even though we have just scratched the surface of the capabilities of OCC, this brief walk-through provides an introduction to the possibilities this powerful solution makes accessible. Solutions like Oracle Commerce Cloud give business decision makers a way to leverage revenue without increasing the complexity of their company's operations, or at least keeping costs at the same level. If you're interested in learning more about the unique tools provided through Oracle Commerce Cloud, we encourage you to check back again soon for related articles! Leading enterprise organizations are turning to Avenue Code as the preferred systems integrator for Oracle Commerce due to our focus and expertise with complex, large scale omnichannel systems. Our team of experts are able to build engaging multichannel applications that will boost loyalty and increase sales. Interested? Submit this form and someone will be in touch shortly.", "date": "2018-4-11"},
{"website": "Avenuecode", "title": "Creating Custom Directives in Vue.js", "author": ["Eduardo Lucas Paoli"], "link": "https://blog.avenuecode.com/creating-custom-directives-in-vue.js", "abstract": "New to Vue.js? Let's talk about how to create custom directives. Vue has some default directives, like v-for, v-if, and v-on. Vue also allows you to create custom directives to alter data from some selected element and create a good shared code. This article will give a basic explanation of how to create custom directives using Vue.js. Directives can be considered as an extension of html; they allow you to add a behavior to the element. As mentioned above, the directive's code can be shared with other html elements or components. Directives in Vue can be created using Vue.directive(‘directive-name’, {}) . The directive is similar to a component, where the first parameter is the directive identifier and the second is the object: Note that our directive uses the prefix ‘v’. Every directive needs this prefix, because without it, Vue cannot identify a directive. Custom directives also have some hooks that we can use: bind - Called once when attached to the element. inserted - Called when the element is inserted into the parent node. update - Called after bind, when the initial value is modified. The new value becomes the argument. unbind - Called once when the policy is unlinked from the element. All these hooks are optional. Below, we'll give an example of a custom directive structure with some hook functions: In our example, we will use the hooks bind and inserted , with el and binding objects. These objects have some arguments that we use more than others, and these include value and arg . (If you want to know more about arguments, I recommended reading about Directive Hook Arguments .) Now that we have an overview of how to create custom directives and hook functions, we will provide an example by creating a custom directive that changes a text color. In the last example, the color is fixed, but sometimes, you can add colors dynamically, as follows: In our next example, the directive is receiving an object with positions to align the text. Custom directives can also receive arguments. Let's change our last code and add some arguments. Now our directive is receiving dynamic arguments. All directives that were created before are considered global directives, meaning they could be accessed in every component in an application. Custom directives created inside a component, however, are locally directive, meaning they can only be used within the component in which they are registered. Note that the input created outside the directive doesn’t receive the focus behavior. As shown above, custom directives are easy to create and quite simple to understand. Custom directives can help us create shared and reusable code. Sometimes, we need to create simple code that can be used in many components, and custom directives can help with this. If you want to know more about custom directives, I recommended reading about Directives In Depth . Happy coding!!!", "date": "2019-11-20"},
{"website": "Avenuecode", "title": "Callback Hell, Promises, and Async/Await", "author": ["Bruno Dias"], "link": "https://blog.avenuecode.com/callback-hell-promises-and-async/await", "abstract": "Asynchronous JavaScript, which is JavaScript that uses callbacks, promises, and async/await, helps with functions that take time to return some value or to produce some result. This article gives a basic explanation of how callbacks, promises, and async/await work, and it also helps anyone who has struggled with unreadable callbacks. A callback function is usually used as a parameter to another function. The function that receives the callback function as a parameter is normally fetching data from a database, downloading a file, making an API request, or completing some other task that could block the code thread for a notable amount of time. Imagine, for example, that you are going to download an image. The download takes about 2 seconds, and you don't want your program to stop on this line of execution as you wait for the download to finish. A callback allows you to keep running other functions and \"call back\" the image after the download is complete. Let's look at an example using the setTimeout function to simulate the image download. (The colors on the right are the logs of the codes.) As we can see, the second() function only runs after 2 seconds because of the callback function on the setTimeout function. While the download is happening, the executions of the first() function continue. Image courtesy of callbackhell As we can see, this code is hard to understand and maintain, and it isn't scalable. This is what we call callback hell. (There's even a site called callbackhell that explains this subject in detail.) In modern JavaScript, we have two approaches that \"promise\" to end this suffering. These are called \"promises\" and \"async/await.\" A promise is an object that represents something that will be available in the future. In programming, this \"something\" is values. Promises propose that instead of waiting for the value we want (e.g. the image download), we receive something that represents the value in that instant so that we can \"get on with our lives\" and then at some point go back and use the value generated by this promise. Promises are based on time events and have some states that classify these events: We can produce or consume promises. When we produce a promise, we create a new promise and send a result using that promise. When we consume a promise, we use callback functions for the fulfilled and rejected states of that promise. In the example above, we are producing a promise that looks for the IDs of employees. You may notice that the promise object receives a callback function that accepts two arguments: resolve and reject. This callback is called an executor function, and it is called immediately when the promise is created. The executor function informs the promise of whether or not the event was successful. If it was successful, the function \"resolve\" is called. If it was unsuccessful, the \"reject\" function is called. Now let's consume our promise. For this, there are two methods that we will use: then () and catch (). All promises inherit these two methods. The then () method from the returned object allows us to add an event handler when the promise reaches the state of fulfilled, which means its success state, thus returning the result of the function, \"resolve,\" called by the executor function. So what we do is pass a callback function that will handle the returned data. This callback function receives an argument--in our example \"IDs.\" The \"IDs\" argument is a result of the promise, and the return of our function that receives \"IDs\" resolves our Array. The catch method works the same way, but the expected return is from the \"reject\" function. Well then, the basic principle of the promises to end callback hell are: Promises have a great advantage (if used correctly) called chaining. With chaining, we can simply add a new then () method after a then (). This gives us greater control over our chain of resolved promises. But we have to be careful that the spell does not come back against the sorcerer, for there is a promise callback hell. Imagine a case where we are authenticating a user (merely illustrative): Can you identify in the consumption of this promise the pattern of a callback hell? We have one call within another, and if more entries are added, this code may become unreadable and difficult to maintain. So it's always good to research and keep in mind good implementation practices for promises. For example, in some cases, we can solve all the promises at once: But just use Promise.all () when the order of the promises is not relevant. We can also serialize the promises in sequence: This shorthand syntax works because the then () method returns a promise. Here's a helpful article detailing good practices to follow. Async /await is another alternative for consuming promises, and it was implemented in ES8, or ES2017. Async/await is a new way of writing promises that are based on asynchronous code but make asynchronous code look and behave more like synchronous code. This is where the magic happens. So here's what happened in the code above: we created an async function using the async keyword before the function. This means that this function is asynchronous, which means it basically runs in the background. So what's happening in this async function? An async function may have one or more await expressions, so what we did was to consume our promises with the expression \"await,\" which returns the result of our resolved function called through the promise. This is basically a top-down way of handling asynchronous cases since it runs in the background. One important thing to note is that an async function returns a promise. So to return the values of the getEmployee () function, we need to use the then () method, thus returning the expected value--that's why it is important to know the concept and understand how promises work as well. That's all you need to know as an intro to callbacks, promises, and async/await! Let me know how it goes for you!", "date": "2019-5-29"},
{"website": "Avenuecode", "title": "Your Guide to Organization-Wide Digital Maturity, Pt. 1", "author": ["Alexander Carvalho"], "link": "https://blog.avenuecode.com/your-guide-to-organization-wide-digital-maturity-pt.-1", "abstract": "Why are some companies thriving in our post-pandemic economy while others fail? There is one key differentiator: today's winners are tech-driven. Time and again, we see that successful companies already have - or are consciously evolving toward - a high level of digital maturity that allows a reinvention of their business model, its processes, and its products/services. This article is the first in a series of five blogs that present step-by-step guidelines to help organizations add business value by increasing digital maturity in three key areas: business strategy, design services, and digital evolution enablement. Today, we'll define what we mean by digital maturity and explain why it is increasingly indispensable to economic success. “Going digital” is frequently regarded as a new phenomenon, but a quick look at history shows that it has been key to business success for more than 50 years. Companies that are driving digital transformation thrive, while companies that fail to evolve suffer fatal repercussions (see Kodak, Blockbuster, and Xerox , among others). As technological capabilities increase, so does industry disruption. Improved connectivity, processing, and storage result in improved efficiencies, better consumer experience, and valuable innovations in products and services. In short, technology has radically changed societal habits, creating a much more demanding consumer market and accelerating marketplace competition. The importance of digital maturity is only accentuated in today’s globally uncertain economic climate. Now more than ever, tech-driven companies are poised to survive, thrive, and outperform competitors. Many companies survived the global shelter-at-home protocol because they already had a level of digital maturity that allowed for a reinvention of their business models, processes, and products/services. Organizations that were not already focusing on digital maturity before the COVID-19 pandemic, on the other hand, had to chase digital knowledge and evolve in weeks in order to win a race that is necessary for business survival. Generally speaking, there are three scenarios that mandate organization-wide digital transformation: 1) the business strategy is dependent on digital implementations and innovations; 2) one team within the organization is successfully utilizing the Agile methodology, design thinking, and other best practices, demonstrating their value and the need to replicate this model throughout the organization as a whole; 3) the customer demands digitization of services/products. Digital evolution must encompass an organization’s mindset, processes, technologies, and best practices, permeating the business from strategy to execution. Digital maturity hinges on evolving digitally in three key areas: aligning business strategy and digital initiatives; utilizing design services to improve processes, provide a better consumer experience, and identify new opportunities; and digital evolution enablement related to people and organization, data and analytics, and technology, all accelerated by Coach by Emergence. Next week, we'll discuss how to create a mature business strategy to support your digital evolution. Don't want to wait? Download our free whitepaper for your complete guide to organization-wide digital maturity. How Blockbuster, Kodak and Xerox Really Failed (It’s Not What You Think) , Greg Satell. Inc.com. Image and ideas based on materials from the “ Digital Transformation ” course offered by Boston Consulting Group, University of Virginia. Coursera. Accessed 9.16.2020.", "date": "2020-12-2"},
{"website": "Avenuecode", "title": "The Retail Ops Weekly #6", "author": ["Ivan Dwyer"], "link": "https://blog.avenuecode.com/retail-ops-weekly-6", "abstract": "It was another action packed week, but I must say that the most exciting topic for me was boring – boring infrastructure that is. As the container standards debate continues on, many have been quick to point out that the outcomes that we’re driving towards should be focused on the end user, not the technology itself. When it comes to enabling the business, never underestimate the power of boring. A notable by-product of the digital transformation era is that IT is no longer considered a cost center. CIOs are tasked with getting the most out of technology investments, which means delivering results to the business. Dan O'Shea explains Why more and more retailers are outsourcing their IT [Retail Dive] Speaking of technology investments, more is not always better. It’s about understanding the consumer and putting forth the best, most personal experience. Doug Stevens shares some best practices and harsh truths for the modern retailer. How to Give Retail Customers Something Unique, Memorable, And Valuable [Incredibly] Process improvements require constant communication, but in highly technical roles, we can often get stuck 'in the weeds'. Jen Krieger shares advice for teams. Coffee Shop DevOps: How to use feedback loops to get smarter [opensource] Implementing a culture of change within a distributed enterprise organization is a daunting task that can backfire if not handled properly. Margo Cronin provides practical advice for incorporating a DevOps mindset within the constructs of familiarity. Respect Your Organisational Monolith [InfoQ] Netflix pioneered the microservices architecture, and has continued to improve their complex backend. From their Tech Blog, Katharina Probst and Justin Becker dive into how they plan to split the API service into two, with the implications in doing so. Always nice to see such levels of transparency. Engineering Trade-Offs and The Netflix API Re-Architecture [Netflix] On the topic of boring, Bob Wise shares thoughts on Docker’s role in the ecosystem, and how he believes the Cloud Native Computing Foundation should govern container standards. An Ode to Boring: Creating Open and Stable Container World [Medium] When app categories become crowded as the food delivery services have grown to be (at least in San Francisco), the distinguishing factors often come down to the app UX itself. Nikki Gilliand compares two such apps in detail. UberEats vs. Deliveroo: A comparison of the app user experience [Econsultancy] The folks over at UsabilityGeek did us all a service by compiling a list of top brands’ UI/UX guidelines. Aside from a useful reference point, it’s interesting to browse through and get a peek into how many brands perceive themselves. Official Usability, User Experience And User Interface Guidelines From Companies [UsabilityGeek]", "date": "2016-9-7"},
{"website": "Avenuecode", "title": "How to Create a Router in Angular", "author": ["Mauricio Correa"], "link": "https://blog.avenuecode.com/how-to-create-a-router-in-angular", "abstract": "If you've journeyed with us over the past several months, you know the basics of how to use Angular 2+ , and you also know how to add some flare to your Angular projects by creating custom directives . So what else do you need to know before shipping your own Angular web application to production? You need to know how to use Routers. Most modern web apps are Single Page Applications (SPAs). In other words, they're responsive websites with a fixed header and footer and dynamic content in the middle. To use SPAs well, you must know how to use Angular's new Router. We'll show you how to use a Router in this step-by-step tutorial, starting with creating a Router for the root component, which centralizes all routes declared in the main module (app.module.ts). (Clone the sample app covered in the article or download our demo here . ) First, use Angular CLI to create a new app with routing as follows: You can also add the –routing parameter when creating new modules for your app: Angular CLI will then: All direct routes should be rendered right below the router-outlet tag once they're visited. This is the code inside app-routing.module.ts In order to create app routes, you need to add an object to the routes array declared at the beginning of the file. This object should contain the following keys and values: To add a component to the object above, we need to first create a component using the Angular CLI as usual: Now modify 'app-routing.module.ts' to point to the recently created component: Setting the path as an empty string will trigger the WelcomePageComponent at the base URL. In this case, Angular CLI’s default dev server base URL would be: http://localhost:4200 If you were to inspect the page in order to see where the component has been rendered, this would be the output: Other routes on the same level would make angular remove <app-welcome-page></app-welcome-page> and append another component tag in its place. Before we can navigate between two different routes, we need to create a new component with: Next, add a file named grocery-list-routing.ts to the component you just created. (Note that we’re not creating a routing module this time, so it’s not necessary to add the ‘.module’ naming convention to the file.) This file should contain the route array for that specific component: This is a similar file to the one that’s created by the CLI at first Run, with the slight difference that, instead of creating a Routing Module, it only exposes an array of routes added to a constant. This constant should then be imported into app-routing.module.ts and then added to the main route array. An easy way to do this is to use the ES6 spread operator (three dots), like so: In this example, the routes added to the groceryListRoutes constant will be prepended to the global routes array. Now that you've created a second component, it’s time to show how to navigate between them. Go to app.component.html and add two list items with anchor tags inside the existing, unordered list, and instead of href, add the routerLink attribute: An empty routerLink attribute value will take us to the welcome page, and groceries will take us to the grocery list page. If you want to change a portion of the screen and still create meaningful URL Routes without refreshing the page, you can use children routes. To do this, simply add another array of routes to a children attribute inside the route array you want to augment: Note that ‘ :name’ is the URL parameter identifier; this will be replaced by the parameter to be sent to the child route itself. In order to create a link to that child route, the name parameter must be appended to the routerLink attribute: So if we were to click on the anchor tag, 'See banana details,' the value 'banana' would be passed as a parameter to the child route. If we want to get the value that has been passed as ' :name,' we need to inject the ActivatedRoute to the component, like so: Now, in order to get the parameter value, we need to subscribe to the route parameter’s Observable. This must be done as soon as the component has been loaded. (A more in-depth example has been implemented in the code demo below.) We can also create auth guards with the Angular Router. Auth guards serve as an additional step between the route activation and the actual rendering of the component. An auth service can be added to ‘canActivate’ or ‘canDeactivate’ inside the route object as follows: This generates a guard service that will be activated once the route path is triggered. The guard service has to implement the canActivate/canDeactivate method. The code inside this method can be customized to validate the authorization information about the user and return true if the user is properly authenticated. Angular’s Router library exposes many different features, such as capturing events related to navigation start and/or navigation end and snapshots of the Activated Route, which you can subscribe to listen for changes. To see all of the topics we covered today in action, be sure to download the demo . Now that you know how to use Angular's Router to allow users to navigate between views, you're ready to use Angular to create your own web app. Useful links: https://angular.io/guide/router https://github.com/angular/angular-cli/blob/master/docs/documentation/stories/routing.md http://jasonwatmore.com/post/2016/12/08/angular-2-redirect-to-previous-url-after-login-with-auth-guard", "date": "2018-6-27"},
{"website": "Avenuecode", "title": "Tutorial: Firebase + Android", "author": ["André Servidoni"], "link": "https://blog.avenuecode.com/realtime-chats-with-firebase-in-android", "abstract": "Firebase is a customizable mobile platform with several common features such as authentication. For those just getting started with the platform, I've put together a brief tutorial on how to integrate with an Android application. We will start with a introduction to Firebase, including some of its components. For the purposes of this article, it is assumed that you, the reader, is already familiar with the basic components of Android applications. If you're not familiar, or just want to brush up on your knowledge, I'd recommend taking a look at this article on Android Basics . Once we've established what Firebase is and how it works, we'll apply it to build an Android app solution. So, let's get started! In this article we will talk about how to use the r eal-time database and authentication for the Android chat that we will build. To use the authentication provider, you need to enable it in the Firebase console. Go to the Sign-in Method page in the Firebase Authentication section to enable Email/Password sign-in, and any other identity providers you want for your app. After that you can include the library in your gradle file: Let's create a example user so we can test the login after we implement it. Navigate to the Users tab and click the Add User button. For the real-time database we don't need to \"turn it on\" in the console. Just add the library dependency in your gradle file: To finish, let's change the rules of the database so we can read and write it. Just set both to true , as shown in the example below. (For more information about database rules, please see the links in the references section at the end of this article.) The MainActivity will implement this callback, and all fragments in the attachment process will get a reference from this callback. In this second step, we will create the login and create account fragments. Afterwards, we will integrate with the Firebase authentication tool. The screens will be very simple to implement. The login form will contain two EditTexts (User email and password) and two buttons(Sign in and Create account) - one to do the login, and the second to navigate to our create account fragment. The CreateAccount form will contain two EditTexts, like the login, and one button to create the account Now that we have the layouts, let's create the fragments. One important thing to remember is to attach the ActivityCallback from MainActivity to both fragments. We can use two methods of fragment to do so: onAttach and onDetach With the first one, onAttach , we will cast the context to the callback: And with the second one, onDetach , we will set the callback reference to null, (to avoid needless leaks with a reference in memory). Now, let's go back to the MainActivity to add the code to open the login fragment when we  first  enter the application. We can also implement the content of the callbacks for opening the Create Account and Logout, since we already have both of these fragments. The login fragment will be added in the onCreate() method: To open the Create Account, go inside the openCreateAccount() method: And for the Logout, we will call the login fragment again. It will be called in the chat fragment, in Step 3 of this article: Go back to the login fragment.  You can call the callback methods as button actions so we can begin to \"link\" the flow of the app: The method above called attemptLogin() will be used in the next section, so don't worry about it for now! OK, we now have all the basic setup - time for the fun part! We're now ready to integrate with the Firebase Authentication tool ! The authentication tool provides several methods that we can use to manage Users in our application. We will use two of them for our purposes: To use them we need to have a reference of the FirebaseAuth object. It's implementing the single pattern, so we just need to get an instance of this. Let's declare the object in the fragment class: We can also get the instance of it in the onCreateView method while the fragment is being created: Obviously, this needs to be done in both fragments, Login and CreateAccount! Now that we have the instance from FirebaseAuth object, in the CreateAccount screen we can call the createUserWithEmailAndPassword in the click listener of the button. This method accepts two String parameters, email and password, so just retrieve from the EditTexts in the UI and send. But how can we test whether the operation was completed successfully or with an error? Using the previous method, return a Task . In this object, you can set an OnCompleteListener to determine whether the oepration was successful. The task object has another two listeners: OnFailureListener and OnCompleteListener . The first one is called only when an operation is not successful, and the second one, every time any operation is executed. For our purposes, we will only use the OnCompleteListener . When the onComplete method is called, we need to verify whether the task was successfully executed and do the logic. When the user account is created, the Firebase does the login automatically, so we don't need to do the login call again. Instead, we'll simply call the callback method mCallback.openChat() to navigate to the chat screen. This will be called in as an action of the Create Account button in the fragment. The final code will be something like this: Now, to login an existing user, we'll use almost exactly the same steps as we did when creating a new account. The only difference is which method of FirebaseAuth we'll use: The parameters are the same as what we see from the UI screen (email and password). The difference here is that instead of receiving a task instance onSuccess callback,  we will only receive the AuthResult object that contains the information about the logged user. So to verify if the action was or was not successful, we need to add the OnFailureListener and do the logic there. For example, we can show a dialog or a toast to the user alerting them to the failed login. Remember the method attemptLogin() we created earlier? Now it's time to implement it, ensuring tht the call for the signIn method is inside it: Now let's build the final (and most important) part of the app 😁 The layout will be composed by a RecyclerView, with every message shown as an item in the list so you can customize the rows. We'll also include one EditText to receive the input from the user. For the item row, we'll create a CardView layout with two TextViews, one for the username and the other for the message itself. Now that we have the layouts, we can create the fragment and the adapter for the screen. Don't forgot to attach the ActivityCallback from MainActivity to both fragments (again using the two fragment methods of onAttach and onDetach ). Another thing we will use is the OptionsMenu of the fragment. This is where we will insert the \"logout\" action, calling the logout callback. It will look like this: Note: the creation of the adapter will not be covered here. If you need some help or would like more information about it, you can check out this link with documentation and examples. Now that all fragments, the adapter, and layout are set up, it's time to integrate with the Firebase. In this fragment, we will need to have the FirebaseAuth instance to logout the user, the DatabaseReference, which serves as the reference of the database to write/read the chat messages, and a FirebaseDatabase to get the specific DatabaseReference from. Let's setup all the objects that we will use. For this example I created a helper method to do this, which we will call in the OnCreate: Before we begin to write and read data from the database, let's create a model that will represent the structure of a message item in the database. Basically, our JSON format will be a list of message \"objects\" that will contain the name of the user(email), id of the message and the message itself. It will be something like this: So, for our project, let's create a class called ChatData with three String fields. One detail: don't forget to add an empty constructor and public getters and setters for the class variables. This will be important when we go to save and read the values from our database. For more information about how to structure the JSON, consult this link . Now that we have our model, let's read/write the values in our database! The FirebaseDatabase gives us several methods and a listener to verify things such as when the entire database has been updated, when a specific child was updated, etc. In this case, we will get the updates for all changes, so you can add more people to the chat and create a room for talks, for example. So for this, we add the ValueEventListener to the DatabaseReference . It contains two methods: one for success, onDataChange , that will return the snapshot of the database, and one for error, onCancelled , which is called in the event that this listener failed at the server due to security and/or Firebase Database rules. A DataSnapshot instance contains all the data from a firebase database. Any time you read Database data, you receive the data as a datasnapshot. It will be returned in the listener methods that you attach to the DatabaseReference : Essentially, these are efficiently-generated immutable copies of the data from a firebase database. They can't be modified, and will never change. Now that we know how to listen for data changes, let's go back to our chat app and handle the dataSnapshot of the callback. To do this we need to parse all the items from the snapshot response.  We can use a for loop, as follows: The writing operations are done with the setValue() method to save to a specified reference, replacing any existing data at that path. You can write the following types in the database: For the custom objects, the class that defines it must have a default constructor that takes no arguments and and public getters for the properties to be assigned. The contents of your object are automatically mapped to child. Using a Java object also typically makes your code more readable and easier to maintain. So in our case, ChatData will be our custom object 😁. Now in our app let's add the setValue when the user clicks on the action button of the keyboard: In the code above we get the reference from the input EditText and set the EditorListener . With the action method, we get the child from the database reference object and set the content with our custom object. We use the child as a timestamp so we can have all the information changed in the chat. You can use the username as child so that every time you update the database, the specific child (if already exists) will be updated. That’s it! Now we have a real-time chat app in Android using the Firebase platform! As we can see, this excellent solution from Google provides us with an easy-to-implement platform for use with our apps, that requires minimum time implementing the backend. The complete project is in my Github, feel free to use it and share your feedback! Github Project The documentation of Firebase is very complete and can be found in the following links", "date": "2017-5-31"},
{"website": "Avenuecode", "title": "Git 102", "author": ["Mahezer Melo"], "link": "https://blog.avenuecode.com/git-102", "abstract": "We all know how to do git add. , git commit, and git push . Some of us even let our IDEs do it for us. But is that all there is to it? Git has been one of my passions for a long time, and that's because of how flexible and powerful it can be, when used correctly. There are three main areas of interest for me in Git, and I'd like you to invite you to explore these with me today: This is surely the simplest area, but it is by no means the least important. Even though this is not really a Git CLI command, it should definitely be on your mind when starting a project. Most (if not all) Git repositories providers (e.g. Github, GitLab, Bitbucket) interpret the README.md file of your project as the documentation homepage of the project. And it is crucial that your README is updated and that it is so simple that even people who don't know how to code can set up the repository on their machines. Here is a list of questions your README should answer: BUT WHY? The reason you should be worried about making your README file as good as possible is that it will take your onboarding time to a minimum no matter the seniority level of the newcomer. Plus, it helps the newcomer get up to speed faster! Finally, a good README will require good project documentation, which will reduce knowledge silos in your team. COMMIT MESSAGES The second step for Developer Experience is related to commit messages. It can be really frustrating and time consuming when you're trying to figure out when a bug was introduced and all you have in the commit messages is gibberish and it's not clear where one feature ends and another begins. Here are a few tips: Your git should tell the story of how the project was developed. Some people use the ID from a Kanban card as a prefix to a commit, and others simply write a nice, descriptive text regarding the changes. There is no right way to do it, really. There is, however, a wrong way to do it. I know, sometimes we get frustrated and tired, and we commit messages such as \"testing fix\", \"plz work now\", \"foo bar\", \"simple test,\" etc. However, this is not how the commit messages should be on a normal basis. As a rule of thumb, I like to use Tim Pope's git commit message pattern as a guideline for my messages. Also, you can check out the git rebase --interactive command when you feel your commit messages are too messy or you need to transform a bunch of testing commits into one single working commit. We've all been there: you git checkout one week's worth of work on a Friday at 5PM. But no more! There is a chance that you'll be able to recover your files. Enter your knight in shining armor: git fsck . Git fsck is responsible for scanning the project and finding any dangling files that are not reachable by any node in the git tree. Git creates a dangling blob the moment you run git add on your file. This file will be attached to the git tree once you run git commit , and it will no longer be dangling. So, if you did run git add on your file, and you checked it out afterward, you would be able to see it by running the following command: git fsck --lost-found This will return a list of all the dangling files in the project, such as: Once you have that hash code, you can use another one of Git's beautiful lifesavers: git cat-file . It works just like the cat command on your terminal (if you don't know what that does, it prints the content of a given file), but for any git blobs. It's syntax runs something like this: git cat-file <type> <hash> . Sometimes you will happen to have a dangling tree instead of a dangling blob (such as when you git checkout a folder). This is what the <type> parameter is for. But since you can't recover your files in bulk, the dangling trees will only be useful to remember what the files were that were inside that folder. Most of the time, your <type> parameter will be \"blob\". Here is an example of the full first commit, new change, checkout, and recovery of a file: From here you can print the contents of the file to a new file using the terminal itself, such as: git cat-file blob \"your-hash\" > \"new-filename\" (or you can simply copy and paste from the terminal into a file - it works either way). I know, from here on, it's a bit of manual work depending on how many files you lost, but hey, at least you won't have to think about how you created that complicated logic again. Besides, it shouldn't take you more than 2 hours to recover everything, and you can always create a script to automate your recovery. Last but not least, let's talk about flexibility. I will cover this topic in two parts: recording and retrieving. Retrieving First off, let's tackle retrieving since this is the most common use case. Imagine there's a bug you solved in a branch that will not be merged with the rest of the code. Deleting the branch will also delete the bugfix, and you can't make a branch from that branch because there's a bunch of code that cannot be used over there. What do you do? Delete the branch and code the bugfix all over again? Copy the files for the bugfix outside of the project folder and then paste it back when you switch to another branch? You don't need to do that. Git gives you a tool specifically for handling cases where you only need a single commit from a branch. The name is git cherry-pick . Here's how you use it: Done! The commit is now on another branch, without the hassle of a file switcheroo that might overwrite any code that's already being done in the new branch. The bugfix will be added as a simple change that was committed like any regular code. Recording Now, this is all fun and games, but you can't do that when you create those mile long commits that create three different features at once. That's when you use git's flexibility in the way you create commits. First off, a good commit should always leave the project working. I like to do that even on feature branches, with the exception of when you're asking for someone's help on some particular issue, but then you git rebase it after the issue is solved (see the Developer Experience section). The ideal commit should have exactly what's needed for a feature or a bugfix to work. Nothing more, nothing less. And you might think: \"What if I have both feature changes and bug fixes in a single file?\" Worry no more, my friend! Git has got you covered with git add --patch . The --patch flag in your well-known git add allows you to add only a portion of the changes made in a file. That way, you can create one commit for your bugfix, and another one for your feature. And that concludes our tour of some of Git's not-so-well-known tools. This is just the tip of the iceberg when it comes to Git. I am still learning it to this day. As Donald Rumsfeld said : \"There are known knowns. There are things we know we know. And we also know there are known unknowns. That is to say we know there are some things we do not know. But there are also unknown unknowns, the ones we don't know we don't know\". I believe that a lot of growth can come out of the things we know we don't know very well. That way, you have a very clear list of things you need to do in order to grow as a developer. And I hope I just added Git to your list. Do you know any other tools that are super useful? Add them in the comments section! Do you know any friends who would benefit from this article? Share it with them! Git documentation - git-scm.com/docs", "date": "2021-3-31"},
{"website": "Avenuecode", "title": "AC Spotlight: Mark Cooper", "author": ["Alfredo Moro"], "link": "https://blog.avenuecode.com/ac-spotlight-mark-cooper", "abstract": "Mark Cooper , VP of Technology at NAV CANADA, shares how air traffic control leaders are evolving from technology-dependent to technology-enabled. Avenue Code: Tell us about your career path. How did you get to where you are today? Mark Cooper: I’ve always had a passion for aviation, and I never doubted I’d end up working in the industry. Having said that, my career started at sea, working for a company called GEC-Marconi in the UK writing code for the Royal Navy command and control systems. I was fresh out of University, and going on sea trials on a Type 42 destroyer within weeks of starting work was an experience I’ll never forget. From GEC I moved to Lockheed Martin and was the Chief Engineer of the UK high-level airspace control center. This was my first paid interest in aviation, and it educated me quickly on the complexity involved in air traffic management. I spent eighteen very enjoyable years at Lockheed Martin and thought I would be there for life, but Lockheed Martin decided to focus on defense, and the IT business moved over to a company called Leidos. At that point, I decided it was time for a change and joined Deloitte as an Aviation Technology Partner. Deloitte is recognized as an audit and accounting firm, but the scale and depth of their technology business really surprised me. I spent three exceptional years working with some of the smartest people I’ve ever met, after which I moved from the UK to Canada. AC: What drew you to joining NAV CANADA? MC: It certainly wasn’t the Canadian winters! What was so exciting about NAV CANADA was the opportunity to transform one of the world-leading Air Navigation Service Providers (ANSP). Since I had worked as a supplier to ANSPs, I knew that NAV CANADA was very innovative. NAV CANADA is invested in leading aviation technology companies like Aireon and Searidge and was the first ANSP to privatize. Since joining, I’ve realized that the strength of NAV CANADA is firmly in its employees, and I’ve been very impressed by their skills and their passion for the company. Despite the difficult time aviation has undergone recently, the people have always remained professional, and their commitment to help others above themselves never ceases to amaze me. AC: What challenges and opportunities have arisen for NAV CANADA post COVID-19? MC: COVID-19 has devastated the aviation industry globally. Since NAV CANADA is privatized, our income is generated by charging flights for our services, so we have experienced a significant drop in revenue. This has led to strict cost control measures, workforce reductions, and prioritizing only essential, safety-critical services. It’s all too easy to hide behind the pandemic, but at NAV CANADA, we’re doing the opposite - we’re using technology to reinvent the company, positioning ourselves to emerge stronger when air traffic resumes. Pre-pandemic, the NAV CANADA culture was very much office-based. Within days, we had to transition around 2,000 people to a remote work model (excepting live operational systems personnel), launch a remote software development environment, and start a migration to Microsoft Teams. AC: Which digital initiatives are you prioritizing in 2021? MC: Air Traffic Management always lags behind the mainstream industry, and rightly so, as we have far more stringent standards to adhere to and the testing is obviously extensive and time consuming. That said we absolutely need to pick up the pace and use technology to transform the operational services we provide.  At NAV CANADA we are actively exploring technologies such as automated testing to reduce the time taken to deploy products and upgrades. We have started putting plans in place to shift NAV CANADA from being technology-dependent to technology-enabled. Up until today, every ANSP globally has used technology to assist the human in the loop through safety checks or enabling capacity increases. Nothing changed fundamentally for decades because it didn’t need to. The pandemic has shown us that we must change. We must work with airlines and airports to enable a more efficient operation and continue to focus on improving safety. This shift toward being a technology-enabled company comes from something called Trajectory Based Operations (TBO). TBO is not a new concept, and in fact, ICAO (International Civil Aviation Agency) mandates that TBO be implemented by all ANSPs over the next decade. TBO is a paradigm shift the industry must make to utilize the exponential potential technology can play in our future operations. Fundamentally, TBO shifts us from an individual air traffic controller actively separating traffic for the duration of the flight to minimal interaction, provided the flight complies with an agreed contract between the airline and the ANSP. This results in gains for everyone: air traffic control becomes safer and less labor-intensive, airlines see fuel efficiency gains, passengers travel more efficient routes, and the environment is less polluted. Mark Cooper, photo courtesy of NAV CANADA. AC: How does data play into your overall innovation strategy? MC: Data is essential to our future operations. We collect a lot of data, and our goal is to use it to make evidence-based decisions using advanced data analytics and simulation engineering. It goes without saying that air traffic is incredibly complicated and there are hundreds of potential outcomes to every situation, so using data to generate insights and augment the user with decision support is an area of great interest to us. Essential to TBO is the collaboration with airlines and airports to deliver a full integration of flight information, which gives us all a single source of truth from which to make the optimal decision. AC: What trends do you see within the aviation industry as a whole? MC: The pace of change in aviation is now rapidly accelerating, because new entrants to aviation (such as drones) and the evolution of Unmanned Traffic Management are forcing the industry to evolve to safely adopt new airspace system users. Digital hubs also promise much in terms of innovation. The ability to take several air traffic control towers and manage them using a digital solution from one central location facilitates service improvements and helps improve both operational flexibility and resilience. Finally, you can’t talk about trends without talking about AI. AI certainly has a role to play in the future of aviation services, but due to the safety-critical nature of core operational systems, the role of AI in the near future will be in enabling the operation, not performing the operation. AC: How can executives ensure their tech investments yield a high ROI? MC: It’s important to remember that technology is an enabler. To achieve a high ROI, it must solve a real business problem. Also, the processes and controls we have around our investments are critical to maximizing returns. Without organizational buy-in and cultural alignment, technology will not deliver the anticipated ROI. Now more than ever, people need to be told why technological changes are being implemented and to understand the vision for the organization. AC: What is the key to successful strategic partnerships? MC: To be successful, a strategic partnership must be mutually beneficial. “Profit” tends to be a dirty word to customers, but it’s absolutely essential that both parties are motivated and successful. Whilst working at Lockheed Martin, I was lucky enough to be involved with the Institute of Collaborative Working (ICW) in London, which pioneered a partnering standard that ultimately made its way to be ISO44001. This standard sets a great foundation for strategic partnerships in part by defining an exit strategy for partnership termination. It sounds counterintuitive, but having an exit plan drives deeper understanding of objectives, improves interactions, and ultimately strengthens the relationship. AC: What are you personally most passionate about in your career? MC: That’s simple, it’s my team. I enjoy nothing more than seeing my employees be the best that they can be. Many of the people who work for me are more capable than I am, and I fully appreciate that my role is to help them continue to grow. AC: Thanks for your time today, Mark. It’s been fascinating to hear about how NAV CANADA and the aviation industry as a whole are evolving to be technology-enabled.", "date": "2021-2-9"},
{"website": "Avenuecode", "title": "Consultants: The Out-of-the-Box Perspective Your Team May Need", "author": ["Mirna Silva"], "link": "https://blog.avenuecode.com/consultants-the-out-of-the-box-perspective-your-team-may-need", "abstract": "Assuming that your team has been working on similar projects and using the same technologies for a while, it's normal to develop set views of how certain issues should be addressed. Many companies automatically start to make decisions based on what feels comfortable or what expends less energy instead of objectively analyzing processes. If your team hasn't gotten out of its comfort zone recently, it may be time to start thinking outside the box. Of course, it's completely fine to preserve routine processes because you want to save your team unnecessary effort or because you know how to do certain projects with your eyes closed. We all recognize, however, that the success of any given project goes way beyond these considerations. Performance, maintenance, reusability, and other technical considerations should always be assessed as well. But changing the entire ecosystem of your team is not easy, and when you're in the trenches on a daily basis, it's hard to assess which particular processes are efficient and which would benefit from adjustment. Because of this, we recommend working with an outside consultant. If you want to know how your team can implement new strategies to improve productivity and ingenuity, keep reading. A consultant is an outside partner who works closely with your company or team, offering advice on which technologies increase efficiency and how to implement them in order to best meet your business goals or overcome project challenges. While a consultant's role is always tailored to a company's particular needs, their main purpose is to provide guidance and technical expertise. There are three types of consultants: expert consultants, project-as-demand consultants, and augment staff consultants. When a company or team faces an obstacle they've never encountered and don’t have enough knowledge to solve, it's probably time to call the cavalry. In this  scenario, the company should get in touch with a consulting agency that can quickly provide expert solutions. The agency will likely conduct a couple studies and analyze processes to identify obstacles and present solutions for the client to implement. Project as demand is the most common consulting scenario. Here, a company hires a consulting agency (or a single consultant) to independently take on a project from start to finish. For example, a company might hire an IT agency to develop their software, with the understanding that the contract concludes once the software is delivered. In a scenario where a client doesn't need guidance and already has solid processes in place, consultants enter the scenario as experienced partners to help deliver a project on time.  In the scenario, the consultant is hired as a contractor and operates as part of the team, taking on the same responsibilities and duties as other employees. This is a wonderful solution for companies that don't want to spend too much time interviewing, hiring, and training new employees. Besides their technical expertise and business acumen, consultants have the ability to think on their feet and diplomatically resolve all sorts of issues and conflicts. Having someone with this profile on your team contributes greatly toward swift business goal achievements. Additionally, consultants must keep up with industry trends in order to stay at the top of their game. Because they enter all sorts of business scenarios, consultants are continuously learning about the best technologies available on the market, their best uses, and the best implementation for each context. Since consultants are not tied to a single company, they also have the opportunity to work on several different project types during their careers. The higher a consultant's seniority, the greater professional experiences they've collected over the years. Such a diverse background enables them to wisely make important decisions during project development and to solve multiple obstacles. Hiring experienced professionals saves your company time, effort, and money since they can quickly guide you toward the best solution. Best of all, consultants are true team players. They work well with all sorts of teams, and because their future success depends on your satisfaction, they do everything in their power to provide your company with excellent solutions while respecting each employee. At Avenue Code, we put a lot of time and effort into handpicking the best IT professionals so that we can always provide our clients with the best consulting experience. If you'd like to take your company one step further, we would love to show you how Avenue Code can support your unique business needs.", "date": "2018-7-4"},
{"website": "Avenuecode", "title": "AC Spotlight - Elisa Frenz", "author": ["Andressa Lopes"], "link": "https://blog.avenuecode.com/ac-spotlight-elisa-frenz", "abstract": "Elisa Frenz , Founder and Co-CEO at SYNERGISTAS, shares her vision for creating a global healthcare model that is both sustainable and highly accessible. Avenue Code: Elisa, you have an amazing background and now you are the founder and Co-CEO at SYNERGISTAS, which bridges health challenges and sustainable health innovation. How did you cultivate the skills to prepare you for this Herculean initiative? Elisa Frenz: My undergraduate degree was in Intercultural European and American studies, which involved looking at the world through an interdisciplinary lens that encompassed economics, culture, language, etc. I find this skill extremely helpful in my work today as I continue to build bridges between various organizational cultures, company profiles, mindsets, and industries globally. My career requires me to be a world transcender who translates and aligns goals for different parties. I also had the opportunity to move from Berlin to Barcelona, where I completed my Master’s Degree in Business Management. This allowed me to explore more the Spanish language and culture, including the deep dive into the Latin American culture and its diversity. I fell in love with the culture, and this experience still inspires my work today. I entered healthcare in 2007 while still in Spain. While I didn’t have a medical or scientific background, multilingual skills were in high demand. I started by coordinating clinical trials, which involved managing multiple organizations like the investigators running the clinical trials, the pharmaceutical companies, and the service providers. It’s in my nature to be a problem solver and identify opportunities to create change and impact. One of these projects was to expand an academy that introduced Life Science graduates into the clinical trial process. I did this after moving back to Berlin, where I developed new business strategies and healthcare concepts that could be scaled across the world. This meant looking into Asia and the US as well as Europe. My approach is very hands-on rather than theoretical, which has allowed me to learn quickly. AC: What inspired you to become an entrepreneur? EF: I founded my first startup to disrupt the way communication is handled between the corporate companies sponsoring clinical trials and the people actually participating in those trials. To pharmaceutical companies, people are the most important assets in introducing a new drug into the market, so I found it problematic that people are treated like subject data instead of humans. I fully understand the need for certain processes and efficiency KPIs, but I believe that the process and technology can and should be much more relational. For example, I worked in a consultancy capacity to help digitize processes for running tumor board sessions. I discovered that different experts from different disciplines didn’t have the same information about patients, and I wanted to change this. It’s extremely hard to drive change in this space since experts have established long-standing processes, and I learned a lot about the challenges of digital implementation and how to bridge gaps between various hospital stakeholders and the companies developing medical software. During this time, I built health ecosystems within German-speaking regions. I found that in discussions on global health, there was interest in Asia, Africa, Europe, and the US, but Latin America was severely underrepresented, primarily because of a lack of funding. This didn’t change until COVID-19 created a need for a truly global conversation. AC: Part of your work at SYNERGISTAS involves fostering this global conversation. How would you explain the mission of SYNERGISTAS, and how is it being received? EF: SYNERGISTAS is all about bridging expertise to offer something very tangible. This is a fascinating space, because there are so many small changes that can be made to create an enormous impact. But our focus isn’t talking about digital health tools; we want to address global challenges related to the climate and sustainable development, and COVID-19 gave everyone a push in the right direction. Our goals are so big that we rely on strong partners. We work in a collaborative mode with people and organizations who also recognize the need for change. So far, we’re finding that the spirit we’re transmitting is quite engaging for traditional industries. Our perspective brings joy as we’re introducing innovative concepts and companies. On one hand, we’re forming partnerships to create a solution space, and on the other hand, we’re making use of these solutions as an answer to societal challenges. In my view, the sympathetic and creative partnerships we’re forming are a breakthrough in our ability to achieve our goals. AC: You have great passion and a clear purpose when it comes to healthcare. How do you see this market and how do you want to contribute? What are the biggest challenges? EF: It’s very hard to effect transformation in a setting that is extremely traditional. It’s a pity, because it limits the creativity of finding solutions, and that’s partly why I’m interested in looking into countries outside Europe: I feel there’s more willingness to explore new possibilities. In Germany, there has been limited progressive thinking in healthcare because of the thought that we have what we want and there’s no need to change. COVID-19 showed us that we do need to change. We need agility - agility for insurance, clinics, and healthcare providers - and we don’t have it. At SYNERGISTAS, we’re not presenting ourselves as an external provider offering a defined solution. Instead, we’re taking time to talk, understand problems, make connections, and ultimately create a space to problem solve and recreate processes and systems. I believe we need to begin by co-creating a common understanding of the problem before we can co-create solutions. Because of this, we’re open to partnering and networking with a variety of companies in a variety of industries and geographies provided there’s a shared vision. It’s a simple concept, yet it’s groundbreaking. AC: What are the major differences that you see, with regard to healthcare, between Europe and Latin America? EF: In Europe, the healthcare systems are rooted in solidarity, but in Latin America, there’s a strong private sector. The differences between poor and rich are tremendous in Latin America. I believe that technology and digitalization allow us to reduce and maybe eliminate this differentiation. Technology allows us to reduce the cost of healthcare, making healthcare more accessible. We’re currently working on bringing and equipping mobile health stations to serve vulnerable communities in Mexico, which is a passion project for SYNERGISTAS. We believe access to healthcare services should be available to everyone. AC: We know that SYNERGISTAS connects people around the world to promote healthcare development in regions like Latin America. How do you see partnerships in this scenario? EF: The most fundamental element is thinking similarly in terms of values and people. I look at partnerships as a way to extend our knowledge and increase capacities. For example, I don’t understand all of the technicalities of technologies, but I lend a strong application perspective. AC: What is your vision for the future of SYNERGISTAS? EF: We believe in collaboration and co-creation, which can be difficult for organizations to seek on their own. We want to bring in our expertise, digitalize it, and empower people to implement it themselves with guidance in certain areas like finding partners, bridging cultural perspectives, and creating the human touch around technology. Our ultimate goal is to touch as many lives as possible to increase the wellbeing of the planet, giving people the ability to have a good life.  This requires work, awareness, and engagement. And it requires new systems. We need to stop thinking that organizations with a purpose do work pro bono: we need sustainable projects backed by sustainable financials. Without money or monetary incentives, it’s very difficult to make change happen. For example, in Germany, perfectly good healthcare equipment is disposed of regularly. We want to change the waste model so that we’re sending this equipment to countries that need it, but it’s often cheaper for corporations to just throw the equipment away. This is one area we’re working on changing. AC: As a successful female entrepreneur working in healthcare and technology, what advice would you give to young women just beginning their careers? EF: Never hold back. Don’t assume that the idea you have is too big. Surround yourself with the right people, because being in the wrong environment can be very limiting. Speak your ideas, collaborate with people who have a similar vision, and experiment with what works and what doesn’t. That’s how you find your personal niche. Next to having a good community, I would also say that finding a solid mentor will help you jump two steps at a time when you’re ready. AC: Thank you for your time today, Elisa. It’s inspiring to hear about your tenacity and progress in creating a global conversation around improving healthcare systems.", "date": "2021-3-18"},
{"website": "Avenuecode", "title": "Saving Time with Design Patterns in Kotlin", "author": ["Fabio Zoroastro"], "link": "https://blog.avenuecode.com/saving-time-with-design-patterns-in-kotlin", "abstract": "Hey Java programmer, feeling challenged? We've got some design patterns for your OOP architecture that we'll implement in Java and Kotlin to help you out. Design patterns are well known in software engineering as repeatable solutions to common problems. There are problems, regardless of the language with which you're building your software program, that you will run into in every project you work on. In most cases, design patterns will help you address such problems with established solutions. A common question that comes up with object-oriented products is when to use composition and when to use inheritance. Let's start by reminding ourselves of the difference between the two through these helpful quotations from Design Patterns: Elements of Reusable Object-Oriented Software : \"The two most common techniques for reusing functionality in object-oriented systems are class inheritance and object composition.\" \"Object composition is an alternative to class inheritance. Here, new functionality is obtained by assembling or composing objects to get more complex functionality... This style of reuse is called black-box reuse, because no internal details of objects are visible. Objects appear only as 'black boxes.'\" \"Object composition is defined dynamically at run-time through objects acquiring references to other objects. Composition requires objects to respect each others' interfaces, which in turn requires carefully designed interfaces that don't stop you from using one object with many others. But there is a payoff. Because objects are accessed solely through their interfaces, we don't break encapsulation. Any object can be replaced at run-time by another as long as it has the same type. Moreover, because an object's implementation will be written in terms of object interfaces, there are substantially fewer implementation dependencies.\" \"Ideally, you shouldn't have to create new components to achieve reuse. You should be able to get all the functionality you need just by assembling existing components through object composition. But this is rarely the case, because the set of available components is never quite rich enough in practice. Reuse by inheritance makes it easier to make new components that can be composed with old ones. Inheritance and object composition thus work together.\" To summarize, use composition when you feel that doing so will create a polymorphic behavior and code reuse. Evaluate the benefits and drawbacks of each design pattern applied to your architecture design and go with the option that gives you the best trade-off. With the brief background above, let's take a look at implementations for both composition and inheritance in Java and Kotlin. Before jumping into the code, let's imagine a scenario where the following business criteria are requested (AC stands for acceptance criteria): AC 1: A retail organization has its brand(s). Let's expose brand's name; AC 2: After we've developed AC1,  we are asked to expose brand's profit for a specific yea r ; AC 3: After we've developed AC1 and AC2, we are told that the created brand should be part of the GreenCompany group. A brand (interface) exposing its name was created, as well as AvenueCodeKids (class) returning its name. With some code changes, we now have the profit calculation in place. Later on down the road, AvenueCodeKids became a Green Company, and then we implemented its functionality accordingly. Imagine how large and complex this can get if we keep adding interfaces to AvenueCodeKids. We should Keep It Simple .  Let's try a composition implementation now. Nothing new at this point, right? Keep going. Now you can see that you have separated the profit calculation from the brand ecosystem, which has many advantages . Does this sound more complicated? It might, but it gives flexibility and loose coupling, which guarantees maturity to your software. Look at AvenueCodeKids now. Look at how decoupled it got. AvenueCodeKids is much cleaner and ready to be adapted now, don't you think? Imagine more business logic like \"ProfitCalculator\" or \"GreenCompany\" added to your class. Following this approach, your code will be more \"S.O.L.I.D.\" Feel like using the Delegate design pattern? Try to improve the above snippet using Lombok Delegate. Alright. That looks great. What about code in Kotlin now? Let's implement the same business criteria specified above. (For those who are unfamiliar with Kotlin , it's \" an OSS statically typed programming language that targets the JVM, Android, JavaScript and Native. It’s developed by JetBrains . The project started in 2010 and was open source from very early on. The first official 1.0 release was in February 2016.\") The syntax is not that different. Above, we have three interfaces and one class (AvenueCodeKids) inheriting all of them. It looks just like a case of Delegate Design Pattern usage, except it uses \" by \" as a keyword, and indeed it is! There's no unique solution/language/design pattern for software development. Design patterns are a big topic in software engineering. You will notice that the more seniority you get, the more design patterns you need to know. There are so many approaches and design pattern combinations that we can use when refactoring or even creating code from scratch. By the way, did you notice that our final version of Composition Java code was 13 lines and Kotlin was just 3? If you're interested in further reading, I recommend the following articles and books that I used as references: Kotlinlang Martin Fowler's Design Patterns Design Patterns: Elements of Reusable Object-Oriented Software", "date": "2019-6-26"},
{"website": "Avenuecode", "title": "Lessons We Learned From Agile Trends 2017", "author": ["Sthefanie Mingall"], "link": "https://blog.avenuecode.com/lessons-we-learned-from-agile-trends-2017", "abstract": "Agile Trends is an event that aims to push the boundaries of the Software Industry with modern themes, trends and real-world use cases. And of course, Avenue Code was there to sponsor it. In addition to sponsoring, we sent some of our best and brightest to learn from the speakers and fellow attendees. Here's what we learned. Vinicius Kairala - Usually, companies try to implement a winning strategy to transform all their data knowledge into a profitable model using select practices based on Advanced Analytics, Big Data and Data Science. Despite the well-deserved hype around these terms, many of the resulting practices can be overly complex and are not a good fit for every enterprise, depending on the variables. Marcos Nyssens and Daniel Scalli Fonseca of IBM gave a fantastic talk during Agile Trends on transforming Advanced Analytics into real money in large corporations. Here are my takeaways: First of all, you need to know what problem you are trying to solve. Asking yourself whether you HAVE a problem is a good place to start in order to pinpoint what you need to tackle. An extremely common mistake many companies make is collecting massive amounts of data without knowing what they'll do with it. This approach nearly always jeopardizes the quality of the information extracted. Secondly, you should whether your company's data is significant enough to merit analysis. Again, not all data is going to be a useful source of knowledge. In fact, the wrong data can increase complexity and muddy the waters when it comes time for analysis. So, what's the best way to begin modeling and gathering big data? The best way to begin working with analytics is to work closely with data experts. These specialists will know not just how to extract data, but also understand its significance. This means they will be able to advise you on the meaning of the information available, and help you to process it in a way that is truly beneficial for your company. Last but not least, gathering good data by itself is not sufficient to succeed. You must know how to absorb and process this information. Two good practices that companies should follow when dealing with data are: 1) Have clear metrics to monitor progress and define whether data science is efficiently solving the previously identified problem, and 2) Commit to short cycles of analysis in order to ensure quick feedback about progress and forecast future results. Mariane Ferroni - In the Agile community, we talk about communication all the time, but the importance really cannot be overstated. As much as projects are disparate and differ in the specifics, you will always find someone who has experienced or is in the middle of a similar situation to yours. Exchanging experiences and ideas throughout the lifecycle of a project is not only critical to avoiding pitfalls, but is also fundamental to discover new ideas and share best practices. Michel Duarte - The approach of the customer to the delivery team is a recurring theme of the methodology, but this year I heard multiple speakers greatly reinforcing the approach of the design and development team,  with a focus on mutual learning and more effective delivery of value. Regardless of how simple or complex the scope of the project, the agilists at this year's conference reinforced the importance of Sprint 0, Pre-Game, Inception, the pre-project phase to align all the participants of the team and establish the primary objectives. It is important to reinforce the participation of top corporate executives in agile events. Passing the information to those who already know the methodology is easy, but without the support of top management, progress is hampered and the real benefits of the methodology are imparied. Implementing agile methodology cannot be a forced process. Getting buy-in from all participants is absolutely critical, which can often take time. It's also important to acknowledge that you will always need to adapt to your company's existing internal processes. Ricardo Luiz- The application of behavioral economics in information architecture can guide us to different decisions depending on the way the information is presented (more on this here ). The Spotify model has served as the inspiration for the deployment of agile at scale, but large corporations like Itaú often encounter difficulties in replicating it. When implanting a model, we also need to adapt our culture and needs to this model in order to achieve continuous improvement. Luis Alves - For high performance teams, certain stages are always necessary (many choose to follow the Tuckman model: forming, storming, norming, and performing). However, although the stages are common to all high performing teams, there's no single recipe for success within them. Instead, several techniques are available such as the RACI matrix and Feedback Canvas for help at each stage. Were you in attendance at Agile Trends São Paulo? What were your thoughts? Let us know in the comments!", "date": "2017-5-3"},
{"website": "Avenuecode", "title": "AC Spotlight - David Kelleher", "author": ["Anna Vander Wall"], "link": "https://blog.avenuecode.com/ac-spotlight-david-kelleher", "abstract": "David Kelleher, Digital Factory Leader at Braskem, shares his personal career journey and his philosophy on pioneering digital transformation. Avenue Code: Tell us about your personal career path. How did you become the Digital Factory Leader at Braskem? David Kelleher: Before coming to Brazil, I worked in operations and technology at Ireland’s biggest bank. Since my wife is Brazilian, we traveled to Brazil regularly for summer holidays and eventually decided to move there. I didn’t have a job lined up, but I knew everything would work out. Sure enough, a job landed on the table with a Brazilian FinTech startup that implemented the acquiring payments platform in local servers for Transbank, the only acquiring bank in Chile. As the Product Lead, I led developers and teams to deliver a complex application that allowed users to create their own payment application on a proprietary XML database system able to run on any type of payment terminal. This was a very exciting and innovative idea for the payments market, and it showed me how much I loved working with product. After this, I worked as a Product Manager on a new payment vertical for PagSeguro. This meant relocating to São Paulo, which I can only describe as a new sensory experience. I had to learn a new language and a new city while temporarily relocating without my wife and our young children - so the new job was the easy part! But I loved it. At this time, I decided to change my LinkedIn profile settings to show that I was open to recruiters. I wasn’t looking for something new, but I wanted to make myself available to grow and find new opportunities. Eventually, I received a call from Braskem about leading their Digital Factory. I met the entire team and had about six interviews over five months - each of them casual and open conversations - before I officially joined the team in March 2019. AC: Can you tell us a bit about the role of the Digital Factory team at Braskem? DK: These days, everyone is transforming digitally, and Braskem is leading the way within the chemical industry. The Digital Factory is responsible for the development of the digital solutions in-house. We're Braskem's Digital muscle, specially focused on machine learning and data science. AC: Does your team own every aspect of digital transformation, including building, testing, and implementing? DK: We have a four-phase methodology: 1) when an idea arises, it goes through a pre-framing process; 2) once approved, it goes to framing, which involves determining which resources are required, hiring vendors, etc.; 3) the digital transformation team builds an MVP; 4) the product proves its value for the business and moves to industrialization. For example, one of our bigger projects is to use machine learning to help with the same predictive quality work that’s done in the lab today. There’s a very complex process for creating polymers, and every few hours, our lab team members have to take samples, analyze them, and adjust product specifications to ensure that the quality of our products meets our clients’ needs. During this assessment period, we’re losing money. So we implemented a machine learning solution with sensors to collect variables to determine product quality, shortening the period without product quality information from four hours to fifteen minutes. Our product operator can then quickly make adjustments to ensure everything is on spec, thus avoiding waste. This solution is currently an MVP, and we’re deciding how to scale it up. The Digital Factory team is also responsible for helping to implement new ways of working, like an Agile culture. One of the biggest drivers of digital transformation isn’t tech - it’s how you work and how you treat the people who are doing the work. If your people aren’t comfortable, happy, excited, and motivated, they’re not going to be able to produce quality work. So my team promotes new ways of working and trains other departments in the same mindset. This is also why we have Avenue Code’s scrum masters and support teams helping us with our digital transformation. The good news is that people naturally adapt to these new methods because they reflect a healthier way of working. AC: There are a lot of companies trying to develop their own digital factories to promote different ways of thinking and working, but changing culture is often challenging. How do you promote this innovative digital mindset at Braskem? DK: We work with an Enablement Team, which is responsible for stakeholder engagement, trainings, and workshops, and we host a lot of activities to cast a company-wide vision. As a global company, we have to work hard to ensure that each location has access to the same workshop opportunities. Additionally, we have a Digital Portfolio team and a Change Management team. These are critical, because as much as people may want to change, change is hard and takes time since it mandates a new way of thinking. The Innovation Team is also key in mapping the principle pain points for our business and our clients. They help us to source the greatest and latest in technology and trends to help us do our best work in the creation space. This team is also primarily responsible for mapping vendors like Avenue Code to support us. Niels Pflaeging says, “Culture is like a shadow. You cannot change it, but it changes all the time.” As leaders, we have these amazing goals and a vision about how to transform our company, and then we come to work and have lots of emails and many meetings, so we have to think about what’s the smallest step that we can take at any one moment to reach our end goal. It’s not going to happen in a leap; it happens in baby steps. Change is a journey, and over time, the steps we take are cumulative. AC: What do you think is the next step for innovation and technology that Braskem could take to increase business? DK: It’s going to be something innovative. The whole goal of digital transformation is to remain competitive and to be a step ahead. Since Braskem is very focused on sustainability (e.g. our I’m Green Polyethylene and our efforts around Circular Economy), I think Braskem’s next step could involve using innovative tech solutions to achieve even greater sustainability. For example, last year we partnered with a client to create a monomaterial stand-up pouch packaging solution made with only one type of polymer. Perhaps our next step is helping other clients become more sustainable with their products as well. AC: Can you personally help Braskem develop in any of these directions, or is that up to Braskem’s industrial team? DK: One unique thing I like about Braskem is that I will meet people who have worked here for 20 years in 15 different roles. If I discovered a new product or a new way of working and was able to prove its value, I could make that product or method my job. By allowing this flexibility, Braskem lets its people maximize the value they bring to the company. AC: You have had a lot of global experience. Can you share your thoughts on where you see a culture that enables innovation? How would you compare Europe, Brazil, and America? DK: Speaking from personal experience, it really comes down to the people who are involved, because the overall work environment is being redefined right now. People used to talk about working 9am-6pm six days a week, and now they’re talking about how tech might enable more flexibility and reduce work week hours. We’re in a stage of experimentation with culture change and digital transformation because nobody really knows what's going to work and what the future will hold. AC: With work environments shifting, what is the key to strong leadership right now, and how do you encourage your people to be self-motivated? DK: As you know, I’m a parent, and every day I teach my kids about how to recognize and put names to their emotions. Most people never learn this. When they get stressed or angry, there’s an underlying emotion, and they don’t know what it is. I don’t really see my own team as different from my kids - on the inside, everyone is a child. I don’t treat them like kids, but I like to interact with the understanding that there is an emotion behind every action, and realizing this makes us more self aware. On a fundamental level, this is how I lead my team: they are people first, and work is secondary. What we see in people is their output, or what they produce, but underneath, there is a whole world of complex emotions. If all of the underlying emotions are off balance, we can’t produce anything valuable. It’s always important to remember that people are people. This also means attending to my team members’ goals, so I try to create a culture of continuous professional development. We all work for someone, but at the end of the day, as Andy Grove said, “Your career is your business, and you are its CEO.” People say a work-life balance is important. I disagree. I think a life balance is important, and work is part of life. AC: Thanks for sharing your insights, David. It’s been a pleasure!", "date": "2020-3-4"},
{"website": "Avenuecode", "title": "Dear Roadmap, I’m Not Breaking Up with You!", "author": ["Ramon Alves"], "link": "https://blog.avenuecode.com/dear-roadmap-im-not-breaking-up-with-you", "abstract": "Your product roadmap is a powerful tool for showing  where you want to take your product and how you plan to achieve your goals. Creating an effective roadmap, however, is not an easy task, especially in an Agile environment where circumstances and ideas rapidly change. There is no definitive cookbook for creating--and, more importantly, maintaining--product roadmaps. Because of this, most product leaders end up feeling like this: From: Product Roadmaps Relaunched , by Bruce McCarthy and others authors. If these sentiments look familiar to you, you may be thinking “I don’t want to create a roadmap because it's too painful to maintain.” I'm sorry to say there is no magic solution to this problem. Just as your process and product are likely to adjust based on multiple feedback sessions, so also your roadmap will change to reflect evolving ideas. It's very important to remember, however, that roadmaps play a crucial role in product development and should not be discarded. Product leaders are the main channel for internal, and often for external, company communication. Using product roadmaps to improve communication can have a huge effect on internal product development alignment and external product buy-in. Today, let's examine a few ways to effectively create and maintain your product roadmap. In a nutshell, product roadmaps show how you want to achieve your product vision. There is no rule that you must follow to create your product roadmap, but it's important to consider your audience and your goals. Ask yourself: People tend to expect that product roadmaps will present a feature-by-feature wish list to communicate expected deliverables and deadlines, but you also want to use it to excite readers about your goals. How do you achieve both? A short answer is: Don’t deliver features. Deliver results. Use your product roadmap to show how your users and stakeholders will benefit from each delivery. Why is it important to focus on results rather than features? Well, there are constant challenges when developing products, so remember that: We often hear clients and users say, “I want this” and “I want that.” As a product owner, it's essential to understand the core of the issue: sometimes, what clients and users say they want is not what they need. So start by carefully considering the \"why\" behind their requests. This will guide you to deliver satisfying results for your clients and users. Let’s look at an example of how to create a successful product roadmap. Suppose you are working on an e-commerce project and you want to find other ways to promote your products and sales through your own customers. You may decide to build an integration with an existing photo-sharing application so that your output will be something like: Instagram integration. With this output in mind, think about the \"why:\" Why does Instagram integration fit your business model and how will that help you achieve your final goal? Asking \"why\" is how you discover your outcome. In this scenario, your outcome might be: “Make it easier for users to promote our product and thereby boost sales.” While it's important to start by ideating an output and then asking \"why\" to discover your outcome, remember that no rule is set in stone: you may end up in a situation where your product roadmap needs to include a feature as well as a result, and that's okay. The goal, however, is to always ensure your roadmap effectively communicates desired results in a specific yet flexible manner. Have you started creating your own product roadmap? Are you facing too many maintenance challenges? Did you focus on communicating results? Did you think about the \"whys\"? Feel free to share your thoughts and questions in the comments below!", "date": "2018-3-21"},
{"website": "Avenuecode", "title": "How Machine Learning Applications Save Retailers Time and Money - Part 1", "author": ["Hossein Javedani Sadaei"], "link": "https://blog.avenuecode.com/how-machine-learning-applications-save-retailers-time-and-money", "abstract": "As retail companies continue to compete to increase market shares, business owners are seeking to implement technologies that they believe will  boost their revenues. Because of the many technological systems in use, it’s impossible to analyze all accessible data generated by different resources, such as mobile phones, websites, social media, forums, emails, etc. Fortunately, most businesses, including retailers, now understand that Machine Learning (ML) solutions can solve many of their problems and grow their businesses. If ML solutions are modeled properly, businesses can improve their processes to increase customer engagement by analyzing purchase data, inventory and stock information, competitor data, and customer experiences and information, e.g., their browsing history, their date and time information, questions they might have, the final products they purchase, their rates and reviews about products, and their posts on social media. In this way, ML methods can be used in many applications related to various industries. In fact, applications of ML are almost unlimited when it comes to retail. ML systems can enhance retail process performance by providing feedback related to marketing and product placement, customer recommendations, demand anticipation, anomaly detection, purchasing habits, search systems, etc. This article introduces the most influential machine learning applications used by retailers. The subsequent blog in this series will discuss alternative applications and the future of ML in retail settings. Stocking and Inventory Businesses that offer goods and products to customers must deal with inventory, which is either sourced from other manufacturers or produced by the business itself. The use of smart inventory systems directly influences supply chain efficiency. Managing raw materials and resources for production, packing products, and shipping orders all depends on knowing what’s in stock. The problem, though, is that the above-mentioned processes are extremely complex, especially in large-scale retail companies that service many territories. Moreover, other outside factors, such as supplier efficiency, service provider performance, and even adverse weather, impact inventory management. Therefore, managing retail stock in a smart way necessitates the use of ML models to interpret complex internal and external data. This has led to a lot of research on ML applications. 1,2 Recommendation Systems (RS) Product recommendation systems (RS) for sales is one of the most commonly and successfully applied uses of ML technologies because it increases both sales and customer satisfaction. RS formulate and present personalized offerings to customers, increasing purchase and growing business profits, thereby making them extremely useful in e-commerce. Additionally, RS can help customers find items relevant to their previously purchased goods. 4 Check out our previous article that discusses this topic more in depth here . 5 Smart Manufacturing Every retailer has the potential to employ ML methods to enhance performance by utilizing the insights that predictive analyzing provides. In fact, machine learning methodologies can provide solutions for daily retail problems, enabling manufacturers to optimize and update processes quickly instead of spending months experimenting with other approaches. ML methods  optimize manufacturing processes by: Payment Services Banks and other payment service providers already employ ML systems extensively, especially for credit card transaction monitoring. As a matter of fact, ML algorithms play an important role in the real-time authorization of transactions. ML models used in payment transactions enable companies to: Dynamic Pricing One of the primary uses of ML systems is to determine optimal dynamic pricing algorithms.  There are many algorithms that make optimal pricing decisions in near real time, helping businesses increase revenues and profits. In this case, the aim of the dynamic pricing algorithm in retail websites is to decide whether to display the standard price or the discount price for specific users in real time. Recently, there has been an increased adoption of dynamic pricing policies in retail and other industries where sellers can store inventory. Some of the factors that contribute  to dynamic pricing include the increased availability of demand data, the ease of changing prices with new technologies, and the availability of decision-support tools for analyzing demand data. The references that follow feature some of the research that has been conducted on this topic. 15,16,17 Improved Promotions ML algorithms are widely used for analyzing the results of promotions and improving sales in retail companies. Promotions planning is an important consideration in increasing retail profits. ML technologies can reasonably predict future promotion sales figures based on past promotion results, provided sufficient data exists. This allows retailers to evaluate the success of new promotion campaigns before launching them. For more information, see the following resources. 19,20 Using Machine Learning to Improve Campaign Performance The risk that retailers face when launching marketing campaigns with unknown returns can be reduced by using updated and less complex technologies. For retailers, the key to resolving this problem is the use of clear retail marketing analytics methodologies to track and predict the results of marketing campaigns. Creating accurate predictions, however, depends on employing ML algorithms with proper metrics and KPIs. Retailers should implement tools and products that let them use all their data to provide alerting, predictive analytics and machine learning capabilities. This approach allows companies to make faster and more successful decisions 21 . Understanding Buying Habits of Customers Buying habits are the tendencies customers have when purchasing products and services. These tendencies come from a variety of different factors, many of which seem obvious or unimportant. When examining buying habits, ML evaluates data related to  both physical and mental patterns to build a model of prediction. It’s important to note that one of the biggest assumptions of this model is that customers with the same demographic information have the same buying habits. For example, algorithms might assume that young women will buy more colorful clothes than their elders. These assumptions generate automatic customer segmentation that personalizes presentation and advertising displays for customers within particular demographics. Today we discussed some of the most commonly employed machine learning applications and provided examples of each. Each case is supported by valuable references for curious readers seeking more detailed information. Our upcoming blog will discuss related applications as well as the future trends of ML implementations. Interested in learning more? We got you. Download our free whitepaper, \"Tame Your Big, Wild Data With A Robust Forecasting Method\". 1 \"Case-based reinforcement learning for dynamic inventory control in a ....\" http://www.sciencedirect.com/science/article/pii/S0957417408005034 . Accessed 2 Jan. 2018. 2 \"Application of machine learning techniques for supply chain demand ....\" 29 Jan. 2007, https://pdfs.semanticscholar.org/1edf/af279f01d6ea3469b20c57cff6fa69dec219.pdf . Accessed 2 Jan. 2018. 3 \"Amazon to open London hub | London Evening Standard.\" 23 Jul. 2012, https://www.standard.co.uk/business/business-news/amazon-to-open-london-hub-7965332.html . Accessed 5 Jan. 2018. 4 \"Recommender systems in e-commerce - IEEE Conference Publication.\" http://ieeexplore.ieee.org/abstract/document/6935763/ . Accessed 2 Jan. 2018. 5 \"How to Build A Recommender System In Less Than 1 Hour.\" 30 Nov. 2017, http://blog.avenuecode.com/how-to-build-a-recommender-system-in-less-than-1-hour . Accessed 5 Jan. 2018. 6 \"Netflix Recommendations: Beyond the 5 stars (Part 1) - Netflix TechBlog.\" 5 Apr. 2012, http://techblog.netflix.com/2012/04/netflix-recommendations-beyond-5-stars.html . Accessed 5 Jan. 2018. 7 \"Dynamic Manufacturing: Creating the Learning Organization: Robert H ....\" https://www.amazon.com/Dynamic-Manufacturing-Creating-Learning-Organization/dp/0029142113 . Accessed 3 Jan. 2018. 8 \"Optimizing Production Manufacturing Using Reinforcement Learning.\" https://pdfs.semanticscholar.org/7d1c/132a797ae06ac5342f7274cc82bdc4419e0d.pdf . Accessed 3 Jan. 2018. 9 \"Intelligent scheduling with machine learning capabilities : the ....\" 3 Aug. 1998, https://www.ideals.illinois.edu/bitstream/handle/2142/29867/intelligentsched1639shaw.pdf?sequence=2 . Accessed 3 Jan. 2018. 10 \"Application of machine learning techniques for supply chain demand ....\" 29 Jan. 2007, https://pdfs.semanticscholar.org/1edf/af279f01d6ea3469b20c57cff6fa69dec219.pdf . Accessed 3 Jan. 2018. 11 \"Netflix Recommendations: Beyond the 5 stars (Part 1) - Netflix TechBlog.\" 5 Apr. 2012, http://techblog.netflix.com/2012/04/netflix-recommendations-beyond-5-stars.html . Accessed 5 Jan. 2018. 12 \"Machine-learning algorithms for credit-card applications.\" https://academic.oup.com/imaman/article-pdf/4/1/43/6764690/4-1-43.pdf . Accessed 3 Jan. 2018. 13 \"CARDWATCH: a neural network based database mining system for ....\" http://ieeexplore.ieee.org/document/618940/ . Accessed 3 Jan. 2018. 14 \"Patent US5893902 - Voice recognition bill payment system with ....\" 13 Apr. 1999, http://www.google.com/patents/US5893902 . Accessed 3 Jan. 2018. 15 \"Dynamic Pricing in the Presence of Inventory Considerations ....\" http://www2.isye.gatech.edu/people/faculty/Pinar_Keskinocak/dynamic-pricing.pdf . Accessed 3 Jan. 2018. 16 \"Dynamic Pricing in Retail Gasoline Markets - University of California ....\" 8 Apr. 2003, https://are.berkeley.edu/~sberto/boren_shep.pdf . Accessed 3 Jan. 2018. 17 \"Dynamic Pricing on the Internet: Importance and Implications - jstor.\" https://www.jstor.org/stable/27750982 . Accessed 3 Jan. 2018. 18 \"Dynamic pricing - Lynda.com .\" 22 Jul. 2017, https://www.lynda.com/Data-Science-tutorials/Dynamic-pricing/573130/622265-4.html . Accessed 5 Jan. 2018. 19 \"Can Uncertainty Improve Promotions? | Journal of Marketing Research.\" http://journals.ama.org/doi/abs/10.1509/jmkr.47.6.1070 . Accessed 3 Jan. 2018. 20 \"In‐store trade promotions ‐ profit or loss? | Journal of Consumer ....\" http://www.emeraldinsight.com/doi/abs/10.1108/07363769610115401 . Accessed 3 Jan. 2018. 21 \"The Role of Ad Likability in Predicting an Ad's Campaign Performance ....\" 4 Mar. 2013, http://www.tandfonline.com/doi/abs/10.2753/JOA0091-3367370207 . Accessed 3 Jan. 2018. 22 \"Why Do Customers Buy? How to Identify Customer Buying Habits ....\" 16 Jun. 2016, https://thrivehive.com/why-do-customers-buy-how-to-identify-customer-buying-habits/ . Accessed 3 Jan. 2018. 23 \"8 Millennial Car Buying Habits Your Dealership Needs to ... - AutoRaptor.\" 6 Mar. 2017, https://www.autoraptor.com/millennial-car-buying-habits-dealership-needs-know/ . Accessed 5 Jan. 2018.", "date": "2018-2-7"},
{"website": "Avenuecode", "title": "How Kafka Enables Real-Time Stream Processing, Part 1", "author": ["André Melo"], "link": "https://blog.avenuecode.com/how-kafka-enables-real-time-stream-processing-part-1", "abstract": "Apache Kafka is probably primarily known as a messaging middleware with a more flexible structure than queues, but it also empowers teams with its lower entry barrier for real-time data pipelines. So-called stream applications using Kafka are scalable, are fault tolerant, and display strong ordering and delivery guarantees, besides having extensive integration for moving data in and out of Kafka clusters. Rather than having to carefully design, maintain, and test hand-rolled, data-intensive solutions, some applications can leverage Kafka Streams to do the heavy work. But let’s not get ahead of ourselves. For the first of this two-part Snippets series, we’ll focus on basic stream processing operations in order to introduce some of the challenges involved. Basic knowledge of Kafka helps, but it’s not required; we’ll focus on a high-level view of streams and discuss how they are represented in Kafka only when necessary. First, the main abstraction in stream applications are, well, streams: a sequential and potentially unbound source of data. For instance, we could have a stream containing all the posts on a social network: We can perform operations on streams. A simple one would be applying a filter to each record, producing another stream as a result. Suppose we have a stream of reactions to posts and we want to filter only the positive ones: Another basic operation is transforming messages into a new output stream; in the following diagram, we are counting the characters inside each post to produce a stream of integers: Quite intuitive, right? Time to move to something more complex. Instead of counting the characters inside a post, we are going to perform an aggregation: counting the number of posts by author. Notice that this aggregation updates and produces a new count every time a new post arrives. So instead of performing the aggregation every time we need this metric, as is usually done when using relational databases, a count in stream applications is a continuous reduction working only with the previous count and the record that has just arrived. In Kafka Streams, the result of this aggregation is not actually a stream, but we’ll talk about that in a while. What is important right now is to notice that this is a stateful operation: to update the count once a new post arrives, we need the previous count by the same author. That's why it makes sense to change each record's key to the post's author, as shown in the previous diagram, allowing us to quickly get the previous count. Notice that since an author can take a long time to post something new, we'd need some sort of durable storage to persist each count. Keep this in mind, but let's move to another common stateful operation: joining streams. Suppose we need to find out whether post length and reaction are correlated. One way to start would be joining each reaction in the reaction's stream to its post. In a simple scenario, we’d have something like this: This is a good starting point, but joining streams like this might not be possible. Remember, the stream is an unbound source of data, so there’s no telling when (or if) we’re going to get a new reaction to Alice’s post. To join both streams, we’d have to either keep posts around for an infinite amount of time, waiting for the next reaction, or, every time we get a reaction, we’d have to traverse the post stream looking for the post to join--which would hardly be called real-time stream processing. Instead, in order to keep the data manageable, stream joins are performed within user-defined time windows. We perform the join only for posts and reactions that fall inside the same window, so metrics computed over this result would also be windowed (e.g. “based on the first 5 minutes after a post, we’ve seen that larger posts have X% fewer reactions”). Notice, however, that in this case, a bunch of future reactions that fall outside the time window will not be joined and so won’t be part of our metric. For instance, notice how Alice’s reaction to Bob’s post falls outside its time window. Actually, for anything computed on top of the join result, it would look like Bob’s post has no reactions at all. We could extend the time window, but that would only minimize the problem, not solve it. What would be nice is if it was possible to perform a quick lookup by the post key for each incoming reaction in order to join the post and reaction without any time windows involved. Enter tables. Tables are different data structures available in Kafka Streams. For a given key, they store only the last value seen, just like tables in relational databases. The image below shows the difference between storing records in streams and tables: In Kafka Streams, the join semantics for streams and tables (in that order) are exactly what we need. Every time a new reaction arrives, Kafka will look up its post in the posts table and join the tuples. Under these semantics, joining reactions and posts would result in the following stream: With this, we have an idea of what’s involved in stream processing: how streams work, the operations available, the complexity associated with stateful operations, and so on. That's great starting knowledge, but in preparation for the next part of this blog series, let's start going deeper. For instance, how are streams and tables represented inside Kafka? Well, it turns out that the same topics involved in plain Kafka are used to represent streams. If you're not familiar with them, you can think of topics as append-only lists, where each topic consumer keeps tabs of its position on the list: This naturally maps to a stream representation, right? New data is always inserted in the end of the topic, and consumers continuously read data from some point onward. Tables are also backed by topics, but they'll usually have log compaction enabled, meaning that for a given record key, Kafka might store only the last value received (which naturally won't be a problem for tables). Keep in mind, however, that consumers are not limited to moving forward one position at a time. In fact, they have full control of their offset in a topic, being able to move back and forth however they see fit. This will be important when we talk about recovery in real-time stream processing applications! Besides discussing recovery in part two of this series, we’ll also cover other challenges involved in real-time stream processing, why they matter, and how Kafka solves them. Until then, drop a comment if you have any questions or feedback. See you in the next snippet!", "date": "2019-7-17"},
{"website": "Avenuecode", "title": "How to Lazy-Load ES2015 Modules in the Browser", "author": ["Tiago Garcia"], "link": "https://blog.avenuecode.com/lazy-loading-es2015-modules-in-the-browser", "abstract": "Over the last few years, developers have been relentlessly moving their server-side sites to the client-side on the premise that the page performance would be improved. At the same time, ES2015 is already production-ready through transpilers such as Babel . Now you no longer need to fight the AMD vs CommonJS war - described on my article The mind-boggling universe of JavaScript Module strategies - since you can simply write ES2015 modules and have them transpiled and delivered to the browser, while supporting your existing CommonJS or AMD modules. This article demonstrates how to load ES2015 modules synchronously (during the page load) and asynchronously (performing lazy-loading) using System.js . When developing JavaScript code to be executed on the browser, you always have to decide WHEN you want it to be executed. There is always some chunk of code that must run during the page load , as for instance the structural setup of an SPA using frameworks such as Angular, Ember, Backbone or React. Such code must be referenced on the main HTML document returned to the browser after a page request, most likely through one or more <script> tags. On the other hand, you might have some more chunks of code from features that should only be executed if certain triggering conditions happen. Classical examples are: This way, for a given feature like those ones above, if its triggering condition never happens, its chunk of code won't ever be executed. Hence, that chunk of code is definitely not needed during the page load and it can be deferred. In order to defer it, you simply need to leave that code out of the chunk of code which gets downloaded and executed during the page load - so that it would only be downloaded and executed on demand, when its triggering condition happens for the first time. This approach of asynchronously loading deferred code, or lazy-loading , plays an important role in improving the page performance, in terms of reducing the page load time and the Speed Index . In order to learn more about the Speed Index and the performance impacts on Page load vs Lazy-loading, check out my article: Leveling up: Simple Steps to Optimize the Critical Rendering Path . The AMD standard was created for asynchronous module loading on the browser, being one of the first successful alternatives to the spaghetti mess known as global JavaScript files scattered around your page. According to the Require.js documentation : The AMD format comes from wanting a module format that was better than today’s “write a bunch of script tags with implicit dependencies that you have to manually order” and something that was easy to use directly in the browser. It is based on empowering the Module Design Pattern with a module loader, dependency injection, alias resolution and asynchronous capabilities. One of it main usages is to perform lazy-loading of modules. Despite being a formidable idea, it brings some inherent complexity: namely, the need to understand runtime module timelines, which was previously unnecessary. This means that developers need to know when each asynchronous module is expected to do its work. By failing to understand this, developers got into situations that may work sometimes and may not work some other times, due to race conditions, which can be quite difficult to debug. Because of such things, AMD lost quite a bit of its momentum and traction, unfortunately. In order to learn more about AMD pitfalls, check out Moving Past RequireJS . Before we go any further, let's go over ES2015 modules. If you are already familiar with them, here's a quick refresher. Modules have been finally adopted as an official part of the JavaScript language in ES2015. They are powerful yet simple to grasp, standing on the shoulders of the CommonJS modules giants. Basically, a ES2015 module will live in its own file. All its \"globals\" variables will be scoped to just this file. Modules can export data and also import other modules. Export a ES2015 module's interface through the keyword export before each item you want to export (a variable, function or class). In the following example, we are exporting Dog and Wolf : Let's see how to import this module in a Mocha/Chai unit test, using the syntax import <object> from <path> . As for <object> we can pick which elements we want to import - something called named imports . We can then decide to just import expect from chai as well as Dog and Wolf from Zoo . By the way, this syntax of named imports resembles another handy ES2015 feature - destructuring objects . If you only have one item to export, you can use export default to export your item as an object instead of exporting a container object with your item inside: Importing default modules is simpler, as object destructuring is no longer needed. You can simply directly import the item from the module. In order to learn more about ES2015 modules, check out Exploring ES6 book — Modules . As surprising as it may be, ES2015 doesn't actually have a module loader specification. There was a popular proposal for a dynamic module loader - es6-module-loader - which inspired System.js . This proposal has been retreated, but there is both a new Loader spec in the works by WhatWG , and the Dynamic Import spec by Domenic Denicola . Nevertheless, System.js is today one of the most frequently used module loader implementations which support ES2015. It supports ES2015, AMD, CommonJS and global scripts in the browser and NodeJS. It provides an asynchronous module loader (to pair with Require.js) and ES2015 transpiling through Babel , Traceur or Typescript . System.js implements asynchronous module loading using a Promises-based API. This is a very powerful and flexible approach, since promises can be chained and combined: so for instance, if you want to load multiple modules in parallel, you can use Promises.all and just fire your listener when all the promises have been resolved. Lastly, the Dynamic Import spec is getting a lot of traction and has been incorporated on Webpack 2. You can check out how it's going to work on Webpack 2's guide for Code splitting with ES2015 . It's also inspired in System.js so the transition would be quite simple. In order to illustrate the loading of modules in both synchronous and asynchronous fashion I've created a sample project, which will synchronously load our Cat module during the page load, and lazy-load the Zoo module once the user clicks on a button. The code is available on my Github project lazy-load-es2015-systemjs . Let's have a look at the main chunk of code which is loaded during the page load, our main.js . First, notice how it performs synchronous loading of Cat through import . After that, it creates an instance of Cat , invokes its method meow() and append the result to the DOM: Lastly, notice the asynchronous loading of Zoo through System.import('zoo') , and finally, the instances of Dog and Wolf invoking their method bark() and again appending the results to the DOM: When the page is first loaded, the only modules which are loaded are Cat and Main : Once the user clicks on the button, the Zoo module is then loaded: Mastering the art of keeping the page load close to the minimal necessary and lazy-loading deferrable modules can definitely improve your page performance. AMD and CommonJS paved the way for ES2015 modules, which are available to you right now via transpilers. You can start loading your ES2015 modules with System.js, or with the Dynamic Imports spec over Webpack 2, while the official solution is not yet released. For more information on this, check out the presentation Lazy Loading ES2015 modules in the browser given by the author at conferences such as: Front End Design Conference (St. Petersburg, FL), DevCon5 (New York, NY) and Abstractions (Pittsburgh, PA).", "date": "2016-9-21"},
{"website": "Avenuecode", "title": "The First Peer-to-Peer Extraordinary Women in Tech Global Conference is Here to Amplify Women’s Voices in IT", "author": ["Manoela Vieira"], "link": "https://blog.avenuecode.com/the-first-peer-to-peer-extraordinary-women-in-tech-global-conference-is-here-to-amplify-womens-voices-in-it", "abstract": "Avenue Code is pleased to announce the first-ever Extraordinary Women in Tech global conference on December 9 - 10, 2021, where women from all over the world are invited for an unparalleled lineup of peer-to-peer networking opportunities, personal branding career workshops, interviews with angel investors and mentors, and exclusive insights on the latest advances in AI. The conference will feature an impressive lineup of female executive speakers from Microsoft, Oracle, Salesforce, American Express, and Facebook, in addition to tech evangelist Tannya Jajal, professional services EMEA, resource manager, METNA, VMware, in Dubai and startup and women’s influencer Mali Baum, CEO and founder, WLOUNGE, Berlin. Attendees can customize their experience with sessions on innovation, technology, and leadership. Sensitive topics on aging, balancing motherhood, and appearance will also be addressed. The conference will then conclude with a celebration of new friendships and learnings at an exquisite gala. \"This conference brings together the brightest female minds to advance and celebrate the movement of women leadership in technology,” says Ulyana Zilbermints, vice president, client success, Avenue Code, and conference curator. “As an immigrant, a mother, and a female executive in the tech sector, I experienced first-hand the numerous challenges that are inherent to our segment. As time went by in my 20’s, I found the constant need to establish technical credibility within 10 seconds or less, and in my 30’s having to be at my very best all the time and still falling short from male counterparts. As I’m reaching 40 in today’s world, I can finally breathe and embrace the authentic and companioned way I lead my team.” Zilbermints, a recognized mentor for women seeking to advance their careers, has been featured on the cover of Russian Time Magazine as an example of a woman who has successfully balanced her life as a Silicon Valley tech executive, a business owner, and a mother to 7-year-old twins. “It’s not just about women entering IT, it’s also about helping them advance,” says Extraordinary Women in Tech panelist Fariba Rawhani, CIO, Teranet Inc. “It’s very important that we women support one another and send the message that it’s okay to be successful and ambitious; it’s okay to want to expand our horizons and do more. Always remember that women have a sisterhood.” The Extraordinary Women in Tech global conference will take place (in person) at the InterContinental Hotel in San Francisco, where attendees will enjoy the unique opportunity to fast track their careers by meeting one on one with a strong support system of female leaders from around the globe. The conference will feature the following: Inspired by a company culture that promotes talent diversity, Avenue Code’s majority female leadership team formed a committee to give women opportunities to cultivate and receive recognition for their tech talent through hackathons, inclusion advocacy, and most recently, the Extraordinary Women in Tech webinar series turned global conference. What began as a webinar series quickly became a community for tech leaders in more than 23 countries, with over 60% of participants representing managers, directors, executives, and C-levels. “Our panelists and guests grew so connected through these talks that we wanted to contribute in a bigger way by giving all women in tech a platform to build stronger relationships and fast track their careers,” explains Zilbermints. The Extraordinary Women in Tech talk show was created to spotlight female tech leaders from the US, Brazil, Canada, Germany, Mexico, the Middle East, and beyond. Prior webinar topics included: In the words of Extraordinary Women in Tech panelist Virginia Roda, director of technology and business consulting at KPMG Argentina, “...the future is technological and … we have the possibility today, since we are more inclusive, to design it together. This future has a gender perspective, and that includes us. You have to get in, and you have to be part of it. If we want it to include us, we have to get involved.” Avenue Code’s Extraordinary Women in Tech global conference is here to give women a way to be involved and to amplify women’s voices in IT. Avenue Code is only selling 300 tickets to create an intimate environment where valuable connections thrive. Sponsorship packages and tickets are now available! \"Authenticity leads to happiness. Happiness changes over time. Society has a definition of what women are, but if we are to be authentic to be happy, then that definition has to be something that is authentically created by us, and we need a space to evolve it.\" - Ke r r y Joel, vice p r e siden t of Digi t al Expe r ience, E W I T w ebina r panelis t", "date": "2021-5-12"},
{"website": "Avenuecode", "title": "How to Use the Serverless Function with a Telegram Chatbot", "author": ["Cristiano Piccin"], "link": "https://blog.avenuecode.com/how-to-use-the-serverless-function-with-a-telegram-chatbot", "abstract": "In this post, we will briefly introduce Serverless architecture and discuss how it is an optimal solution for handling event-driven processing. Serverless is an architectural paradigm in which either a function or a microservice can be hosted on a cloud server without the server running continuously; it is an event-driven approach where each execution starts, runs a small process to meet the request, and stops at the end. Some of its defining characteristics are: To explain how Serverless architecture works, let's consider an example where we create a Serverless function that calculates BMI (Body Mass Index) through a Telegram chatbot. Passing aleatory commands to the bot, we will receive the right command syntax to write in order to tell the bot to calculate BMI. When the user enters his weight and height information, the bot will convey this to the S erverless function; the function will then calculate the BMI and send the result back to the user, as depicted in the chart below: Chatbot flow With this \"big picture\" chart in mind, let's examine the individual components involved in this process. First, we have created a Telegram chatbot following these steps . The bot's function is to receive a request and pass it along either through a pre-configured webhook or through a pooling model, which makes background requests for the chatbot server. In our case, we are using the webhook mechanism. This means that when a message is sent, the Telegram chatbot server will redirect the request to a specific endpoint already configured in the webhook. This request then goes with the chatbot payload so that our FaaS can receive and process the message. Setting webhook For this example, we have used Google Functions as our FaaS provider, which requires a valid Google Account in Google Cloud. We access Google Cloud through the dashboard panel, where we click \"Main Menu\" - \"Cloud Functions.\" Dashboard We then create a function called \"function-bot,\" exposing an HTTP endpoint, but we can also choose other trigger types, such as pub/sub or Cloud storage buckets. When creating a function, users can specify the memory size to be allocated as well as the code that will be the main execution function. The code must be in Node.js at this time and can be put into an inline editor, zip package upload, or Cloud repository. The function follows the Express.js framework standards, receiving a request and response as input parameters. The Serverless function code that receives the chatbot webhook request operates as follows: The above code illustrates how, after this BMI processing, we must call the Telegram chatbot URL with the BMI level as the payload message. Once all configurations have been completed, we can publish our function. At this point, our system is able to execute through the HTTP endpoint. Since we have completed all the steps to execute our chatbot, we can access the bot, which we named \" myhealthybot,\" through the Telegram interface. We can start the chat by listing weight and height information. The chatbot response is instantaneous. In the example above, we've created a simple chatbot to illustrate how quickly and simply you can create Serverless functions, which are scalable and affordable options for handling asynchronous processing. The market at large is utilizing tools like Serverless more and more, so be sure to take some time to explore all the uses of this architecture for your own business. Cheers! https://core.telegram.org/bots/api https://martinfowler.com/articles/serverless.html https://aws.amazon.com/lambda/ https://azure.microsoft.com/en-us/services/functions/ https://cloud.google.com/functions/", "date": "2018-3-14"},
{"website": "Avenuecode", "title": "Beyond Agile - Product-Driven Companies", "author": ["Eduardo Bruno Silva"], "link": "https://blog.avenuecode.com/product-driven-companies", "abstract": "", "date": "2018-11-28"},
{"website": "Avenuecode", "title": "Takeaways from RubyConf São Paulo 2019", "author": ["Rafael Silva"], "link": "https://blog.avenuecode.com/takeaways-from-rubyconf-s%C3%A3o-paulo-2019", "abstract": "Locaweb, one of the largest web hosting companies in Brazil, hosted this year's RubyConf at Universidade Anhembi Morumbi in the heart of São Paulo's booming Vila Olímpia area. Our Avenue Coders couldn't miss it! Avenue Code developers Rafael Silva and Magnum Fonseca at RubyConf Brasil Over the years, RubyConf Brasil has matured and gained a lot of traction. With two days of activities and talks from a variety of São Paulo companies, this event shows that Ruby’s adoption in Brazil is still going strong. This year, RubyConf welcomed a lot of new programmers and companies that are adopting Ruby. Several startups are using Ruby to power their innovation, and to steer that innovation into a brighter future, RubyConf brought several interesting topics, from new core language features to architecture to metrics to LGBTQI + challenges and accessibility discussions. For more experienced developers, Lucas Uyezu's (Shopify) opening talk introduced the new static type checkers in Ruby 3 and how to use tools like Sorbet to start incorporating type checking into apps. \"Static type checkers and type annotations can help you find implicit concepts and code smells in your code .\" - Lucas Uyezu Lucas and his team have spent months implementing type checking in their project and, in his talk, he explained challenges and improvements of this new Ruby implementation. As for new companies and teams, starting a new web application can be quite challenging given the amount of alternatives and choices available. Since version 0.1 of Rails was released fourteen years ago, several similar web frameworks have appeared and many developers wonder if Rails is still a good option in 2019. Thiago Pradi (Trusted Health) showed in his talk that it is! His talk addressed modern web application architecture issues (Integration with JavaScript frameworks, WebSockets, API format, performance), as well as the new features just released in Rails 6. RubyConf Brasil 2019 showed us that the Ruby community continuously adopts best practices, which we saw through several talks about DDD (Domain Driven Design), upgrading Rails, code review, composition, and inheritance. In his talk “Domain Driven Rails,” Gabriel Ferreira (Locaweb), showcased how applying DDD concepts when redesigning one of their major customer-facing tools helped them solve several issues and deliver a higher quality product on time that was easier to maintain. This conference showed that there are many startups and big companies that are using Ruby and Ruby on Rails nowadays. We had proof that Ruby can fit your business model as Ludmila Pontremolez (CTO - Zippi) showed in her talk how Sidekiq powered up fintech Square's Payroll of filings and payments for all U.S. states. Ludmila Pontremolez, CTO of Zippi, speaking at RubyConf Brasil 2019 Thanks to its intuitive, simple, and readable syntax, Ruby is great for startups and internal tools and results in much higher productivity. Faster development means a quicker time to market, which is important for startups with a limited budget. This way, more money can be invested in the development of additional features. Rails 6.0 was released this year, and it’s packed with many features that both smaller and bigger applications will benefit from, as it seems that many improvements related to speed and scalability were introduced in Rails 6. Two new additions to the Rails family are Action Mailbox and Action Text, which came straight out of Basecamp. In December 2020, as a Christmas gift, we'll receive a lot of improvements with Ruby 3.0, which will be 3 times faster. Big companies such as Airbnb, GitHub, SlideShare and Zendesk are using Ruby on Rails, which shows how great it can be! This blog was coauthored by Avenue Coders Rafael Silva, Software Engineer and Ruby Mentor, and Magnum Fonseca, Software Engineer.", "date": "2019-12-11"},
{"website": "Avenuecode", "title": "How Software Development Principles Can Help You Be a Better Leader", "author": ["Marcio Viegas"], "link": "https://blog.avenuecode.com/how-software-development-principles-can-help-you-be-a-better-leader", "abstract": "You influence people and you manipulate bits. If you do it right, they will deliver what you expect. Even though there is a vast difference in how leadership and development work - and you can presume that based on the disparity of meaning between \"influence\" and \"manipulation\" - some software development principles serve as a great guide to better leadership. \"Tell-Don't-Ask\" is a principle in object-oriented programming that reminds us that rather than asking an object for data and acting on that data, we should instead tell an object what to do. Changing the vocabulary to a more declarative approach, the lesson is don't micro-manage data, and don't remove responsibility from your team. Tell people what you expect from them, share known data, and trust that they will do what they know how to do. In the end, delegation mandates trust. \"Don't repeat yourself (DRY)\" is a principle aimed at reducing repetition in software patterns, replacing it with abstractions. A well-defined level of abstraction is fundamental for decoupling responsibilities, defining ownership, and avoiding duplicate work while making sure the vital information is shared between interested parties. A leader should set expectations, define interfaces, deal only with essential information, and not waste everyone's time (WET). \"You Aren't Gonna Need It\" is the principle that helps us avoid complexities that arise from adding functionalities you think you may need in the future. As a leader, your time to respond to problems is often more important than your ability to avoid them, primarily because you can't anticipate all issues. Together with the \"Keep It Simple, Stupid\" principle - universal among software developers but initially noted by the Navy - the message here is to be productive with your time and others' time. If you keep adding processes and tools to prevent future problems, those premature optimizations may become the problems you will be trying to solve. Single source of truth (SSOT) in software development is the practice of structuring information so that it is stored and managed centrally, avoiding overlaps and duplication. As a leader, you will face situations where normalized data is a must because you won't have time for redundancies. As you deal with higher expectations and riskier decisions, information is critical, and the only thing worse than no data is the wrong data. So learn to manage your sources so that information is reliable and available at all times, keeping a single source of truth. Write your commands accurately, and software is always going to deliver as expected. On the other hand, people might consider your input, and maybe they will provide what you need, but the result depends on complex variables, including a diversity of personalities. The most beneficial thing you can and should do is be empathic and actively listen to everyone. Collectively, through the art of soliciting feedback, you will create the best way to achieve the expected result with mastery.", "date": "2020-4-15"},
{"website": "Avenuecode", "title": "Google Cloud AI Product Review, Part 1", "author": ["Hossein Javedani Sadaei"], "link": "https://blog.avenuecode.com/google-cloud-ai-product-review-part-1", "abstract": "Nowadays, using the cloud to develop artificial intelligence apps is common for medium and large-sized companies, and compared to other cloud service providers, Google has a big share of the market. Here at Avenue Code, it's been a while since our data science team adopted Google as one of the main service providers for its data science projects. Over time, we've gained some insights on Google Cloud's AI products and tools, and we'd like to share our experience with our readers. This two-part blog will introduce these tools, present comparison studies, and provide useful learning resources. Let's start with Cloud AutoML. In general, AutoML was created for algorithm selection, hyperparameter tuning of models, iterative modeling, and model assessment. Lately, at the Cloud NEXT 2018 conference, Google AutoML was released in its beta version . This service dramatically decreases the steps required when training and tuning a machine learning method. Image courtesy of Google Cloud Cloud AutoML beta is a good example of a machine learning product that allows developers with minimal machine learning expertise to still train high-quality methods particular to their business requirements by leveraging Google’s transfer learning. AutoML consists of three main modules: AutoML Vision, AutoML Natural Language, and AutoML Translation. AutoML Vision AutoML Vision, a graphical drag-and-drop engine that allows users to leverage Google’s cloud computing backend to train self-learning object identification and image discovery patterns, is retiring alpha and opening beta to the public. One customer for this product is Disney. Disney has been using Cloud AutoML so that its clients can find and shop products related to specific Disney characters. Image courtesy of Analytics Vidhya AutoML Natural Language This API allows more people to build custom machine learning models to group content into a custom set of categories. Custom machine learning models for classifying content are beneficial when the pre-defined classes are available. It is also possible to use the AutoML Natural Language UI to upload training data and train and test a custom model. Image courtesy of Google Cloud AutoML Translation Lately, Google is keeping a competitive edge in the neural machine translation (NMT) race with AutoML Translation, a new iteration of its AutoML machine learning series of products. AutoML actually supports Google’s cloud platform clients to use their own training data to customize and train domain-specific machine learning methods. There are many languages that have already been supported by this API, which can be found here . Even though Google AutoML is one of the most successful services in this domain, there are other companies and startups that have been working on providing similar services. The following table provides a short list of these organizations. More information and details can be found here . Image courtesy of Applied AI A tensor processing unit (TPU) is an AI accelerator application-specific integrated circuit (ASIC) produced by Google especially for neural network machine learning. The tensor processing unit was released in 2016 at Google I/O, when the organization announced that the TPU had already been used inside their data centers for over a year .  Cloud TPUs are a group of hardware accelerators that Google produced and optimized especially to speed up machine learning workloads for training and reasoning programmed with TensorFlow. Cloud TPUs are meant to give the best representation per dollar for targeted TensorFlow workloads and to enable machine learning engineers and researchers to go through iterations more quickly. The following are the main features of this product: Although the first TPU focused on efficiently running machine-learning models for tasks like language translation, AlphaGo Go strategy, and search and image recognition, the more intensive task of training these models was done separately on top-end GPUs and CPUs. TPU2 is intended to both train and run machine learning models and cut out the GPU/CPU bottleneck. The authors of the RiseML blog measured throughput in terms of images per second on synthetic data (i.e. with training data created on the fly at various batch sizes), and here's what they found: Image courtesy of RiseML Blog Also, the following analysis of cost efficiency outlines TPU2's considerable advantages: Image courtesy of RiseML Blog With this pricing, Cloud TPU is the clear winner. The Google Cloud Machine Learning Engine is a large-scale machine learning model that covers a broad set of ML. It is not a SaaS program that you can just upload data to and start using. It is integrated with other Google Cloud data platform products, such as Cloud Storage, Cloud Dataflow, and Cloud Datalab. In short, this service is a computing platform which by itself does not do ML jobs; developers should implement models by writing codes. The main competitor of the Cloud ML Engine in the market is SageMaker from Amazon, so let's compare the two: Both AWS and Google Cloud provide Jupyter notebook, with backend running on a cloud VM that has pre-installed machine learning frameworks and cloud services. There are some differences between the two, however. ML Engine supports only Tensorflow, scikit-learn and XGBoost frameworks. On SageMaker, you can use MXNet and your custom libraries as well. You get new versions of Tensorflow on ML Engine weeks to months before you get them on SageMaker. If you plan to use automatic hyperparameter optimization, this works better on ML Engine , both in terms of results and time. Other comparisons are detailed in the  table below: Major companies that have adopted Google ML Engine include Airbus, Home Depot, Snapchat, Evernote, Niantic, Telus, Accenture, and Pivotal. The following links provide some important resources for learning more about Google Cloud ML Engine: Google has announced a beta version of BigQuery ML, an innovative software that allows users to develop some machine learning methods inside the Google BigQuery Cloud data warehouse using SQL commands. BigQuery ML lets data scientists and data analysts create and apply ML models on scale-structured or semi-structured data directly inside BigQuery, using simple SQL, in just a few seconds. The main features of BigQuery ML are: This technology, however, is restricted in the number of models that it can support. So far, BigQuery ML supports only two types of models: linear regression models that forecast numerical values, such as stock price prediction, and binary logistic regression models that can be applied to classify an email as spam and do other relatively simple tasks in data sets. BigQuery ML may not satisfy data scientists who prefer to use additional tools and methods to build models. On the other hand, BigQuery ML does make it possible for the users of data analytics with a good knowledge of SQL and a poor understanding of other advanced ML models to start developing models without having to acquire new languages and also to use further analytics tools to solve some simple problems (see the example below). Image courtesy of Medium The main advantage of BigQuery ML is that it directly integrates machine learning aptitudes into a data warehouse's SQL interface. In this blog, we reviewed some of the main Google Cloud AI products, including Cloud AutoML, Cloud TPU, Cloud ML Engine, and BigQuery ML. We also compared these products with similar services from Google competitors. In the next blog, we'll examine additional Google Cloud AI tools.", "date": "2018-12-12"},
{"website": "Avenuecode", "title": "How to Build a Simple Validation Mechanism for JAVA Spring MVC REST Endpoint Parameters", "author": ["Fábio Suarez"], "link": "https://blog.avenuecode.com/how-to-build-a-simple-validation-mechanism-for-java-spring-mvc-rest-endpoint-parameters", "abstract": "Over the years I have seen several approaches for validating inputs (@RequestParam, @RequestBody, etc.) for REST endpoints. Many of these approaches are pretty manual and require explicitly calling a validation mechanism. But is there a way to automate this process while writing as little code as possible, still validating all inputs, and responding with friendly messages to consumers? In this Snippet, I will present a minimalistic approach for handling this issue by using javax.validation.constraints combined with Spring MVC @RestController, @ControllerAdvice, and @ExceptionHandler. To follow our examples, you will need: Starting with the controllers, we need to tell Spring MVC that we want to check for constraint validators at the method calls. In order to do so, simply annotate your controller with @Validated from the org.springframework.validation package: Now let's take a look at an example payload (getters and setters omitted for better presentation), whose fields we want to receive and validate as a request body for an endpoint: To enable the validation mechanism on the request body, the ExampleController method using the Payload should have the @Valid annotation, as pictured below: When we call this endpoint with an invalid @RequestBody , we should get a MethodArgumentNotValidException, which contains all the information we need  to construct a friendly message to the consumer. But what if we want to validate and retrieve the parameters from a GET request without the Payload or any other DTO to encapsulate the request parameters? The validation framework has us covered here as well. You can still validate the method arguments with the same annotations used in the Payload 's fields: In this case, if it receives some invalid parameters, we should get a ConstraintValidationException that also contains all the information we need  to construct a friendly message to the consumer. In order to give the API the expected contract rules,  with the HTTP status and message that we want to expose for consumers, we should declare a ControllerAdvice bean that contains the methods annotated with @ExceptionHandler , which will handle the exceptions properly: Next, you can create your message resolver and handle each violation with all the details provided by the exceptions themselves. Here is a simple example which just joins the default messages for each parameter/field while also using their names to compose a friendly message: With this mechanism in place, we can now think about creating custom annotations for when we need to validate specific business scenarios. In this example, we're creating a constraint to validate a phone number using a regex validator. In order to do this, we need to create an @interface which defines our new constraint: Then, we implement our PhoneNumberValidator as requested, overriding the isValid method from the ConstraintValidator class: Using this is quite simple, as with the other constraint annotations: If you need to validate some complex logic using multiple fields inside a Payload , you can still use @Assert annotations for private validation methods with custom messages. Here is an example: You may have seen another approach for validating parameters inside a RequestBody that involves adding a BindingResult parameter to the controller method to avoid the MethodArgumentNotValidException and to allow you to handle the error through a manual check: Although this works well for requests that have a parameter annotated with @RequestBody , you should be aware that it doesn't work for RequestParam -annotated parameters. These latter parameters are mostly used on GET requests, like the one we used as an example in this Snippet, which have the annotations directly on the method arguments. As we can see, this is a very simple approach that saves time by avoiding the usual manual or restrict validation mechanisms, as well as some resulting boilerplate code. Do you have additional t houghts? Do you use some other validation mechanism in your projects? Let me know in the comments!", "date": "2018-7-18"},
{"website": "Avenuecode", "title": "The Retail Ops Weekly #3", "author": ["Ivan Dwyer"], "link": "https://blog.avenuecode.com/retail-ops-weekly-3", "abstract": "The Rio Olympics are in full swing, giving the world an inside view of Brazilian culture and history. For those of us watching on TV, the feeling of saudade is present. As is quickly becoming customary, I've pulled together another set of relevant thought pieces, news stories, and tips & tricks from the past week. Enjoy! Industry Speaking of the Olympics, many retailers are seeing strong spikes in apparel sales, including Fanatics who says they're up 50% compared to the 2012 London Olympics. E-Retailers Go For the Summer Olympics Gold [Internet Retailer] After a week of speculation, it seems as if the deal will go through between Walmart and Jet.com, which would be the largest acquisition of an e-commerce company ever. Stated to be announced today, potentially before this email arrives. Walmart is buying Jet.com for $3 billion and will announce the deal on Monday [Recode] Agile is a practice where you only get out what you put into it. Gary Spillman gives fantastic (and entertaining) advice on how best to extract value from retros. Don’t Look Back in Anger: Retrospectives, Software Development and How Your Team Can Improve [BazaarVoice] As more companies look to DevOps practices as a way to deliver software quicker and more effectively, it’s important to look at the whole picture, not just individual teams. What happens when product and service teams need to collaborate with each other? App dev silos are DevOps killers: Start by tearing them down [TechBeacon] Many of us who follow the container ecosystem closely witnessed quite the debate on Twitter last week. The tl;dr of it boils down to the right balance between industry standards and continued innovation. Joab Jackson breaks it down. Container Format Dispute on Twitter Shows Disparities Between Docker and the Community [The New Stack] Despite the clear benefits, nobody claimed microservices to be an easier path towards developing large scale applications. If we're meant to be strict with our bounded contexts, what do we do when we actually want to share data between services? Christian Posta dives in using the often used airline ticketing example. The Hardest Part About Microservices: Your Data [Christian Posta] Designers can benefit from optimization just as developers do. Duval Nicolas shares some personal advice around creating a shared library of components for rapid iterations and consistent output. Dare I say DRY design? Stop designing interfaces, Start designing experiences [Medium] Your weekly reminder that user experience is about just that - the experience. Nik Page digs into the business implications of UI/UX, with a few retail cases. UX Strategy – It Is All About The Experience [Usability Geek]", "date": "2016-8-29"},
{"website": "Avenuecode", "title": "Improving Javascript Performance with Event Delegation", "author": ["Rafael Guedes"], "link": "https://blog.avenuecode.com/improving-javascript-performance-with-event-delegation", "abstract": "Events are the most common way to promote interactions between the UI and application layers on web applications. Be it a click, a focus or a scroll, webapps are filled with them, bringing sites to life, enhancing user experience and promoting friendly interfaces to the end user. As much as they are helpful, event handlers are costly. Sometimes very costly . Let's use a very simple situation which you have surely already encountered on one of your applications. Assuming you have a list with a variable number of items and you want to set a click event, you might have implemented something like this: let rings = ['elves', 'dwarves', 'men']; let rule = (evt) => { let race = evt.target.textContent; console.log(`you rule the ${race}`); }; let drawRings = (rings) => { let ringList = document.querySelector('#ringList'); rings.forEach((ring) => { let ringEl = document.createElement('li'); ringEl.innerText = ring; ringEl.addEventListener('click', rule); ringList.appendChild(ringEl); }); }; drawRings(rings); See live demo Note: appendChild() , as any DOM manipulation operation, is slow. Using it on a loop is not recommended for a large amount of items (you might want to use Document Fragment instead). We are going with append just for the sake of simplicity. Although this code works, we are binding one handler for each element. That means we are allocating space in memory for every single item being displayed. Now imagine a commerce site listing hundreds of products and each one of them having their own set of click , hover , and other events, and it's easy to see where we are heading. Our goal is to have as few event handlers as possible, and Javascript has in its fundamentals the right tools to help us with that. The answer is event delegation. But first, let's take a look at some concepts. The way events work on Javascript is very simple. It's fired from the innermost captured element, and then bubbled up to outside elements. Also, event handlers are stacked so they will be executed one at a time from the captured (inner) to their last parent (usually document). The code below illustrates that behavior: let clickHandler = (evt) => { let tagName = evt.currentTarget.tagName; console.log(`executing handler from ${tagName}`); }; document.body.addEventListener('click', clickHandler); let outer = document.querySelector('.outer'), text = document.querySelector('.text'), inner = document.querySelector('.inner'); outer.addEventListener('click', clickHandler); text.addEventListener('click', clickHandler); inner.addEventListener('click', clickHandler); See live demo So... If the event is fired to outer elements, that means we don't need to assign our handler to each one of the inner ones, right? Right! But we aren't we missing a final piece? We already know a click event was triggered, but our handler needs to identify the source element so it can act properly and individually. Thankfully, Javascript makes it easier by keeping a reference of target object: var target = evt.target; // Returns the Node object that originally fired the event The target property of an event object ensures that we will always have a reference for the element that dispatched the event we are listening to. Its value is the same all the way up the chain, enabling us to provide specific behavior and handlers. That's what we call event delegation . We need just one single handler to take care of all child nodes and that will save us a lot of memory space and processing time. One handler to rule them all! let rings = ['elves', 'dwarves', 'men']; let ruleThemAll = (evt) => { let race = evt.target.textContent; alert(`You rule ${race} \\nI rule them all`); }; let oneRing = document.querySelector('#oneRing'); oneRing.addEventListener('click', ruleThemAll); let drawRings = (rings) => { rings.forEach((ring) => { let ringEl = document.createElement('li'); ringEl.innerText = ring; oneRing.appendChild(ringEl); }); }; drawRings(rings); See live demo Now we are going somewhere. Our handler is bound once (and only once) to the parent <ul> element, and will take care of all its children. As you can see in the example above, it works perfectly well with dynamic content so you won't need to worry about that either. No. Not at all. Let's say you go that way and listen to the click event on the document . The first problem is a gigantic function that will need to compare every possible target to redirect to the correct handler. That means multiple accesses and comparisons that are also costly, and depending on their amount, it can mean an actual decrease in performance rather than improving it. That by itself is already a no-go argument but there are a couple of items on this equation to be considered: Scalability and Maintainability . Both of them tend to be worse using a document-level approach, since a monolithic structure is less flexi ble to begin with, and tends to grow even more inflexible. The best solution is always to delegate them to the most meaningful outer container so you can gain the most from bubbling without worrying about whether that's the one you want to execute. As we can see, event delegation not only promotes an efficient way to improve the speed of your web application, but also makes code more elegant by abstracting event-binding from the element creation level. The implementation itself is very simple as it's just a matter of moving the handler up on the DOM tree. Keep that in mind next time you use Javascript events :). Happy coding!", "date": "2017-1-5"},
{"website": "Avenuecode", "title": "The Retail Ops Weekly #5", "author": ["Ivan Dwyer"], "link": "https://blog.avenuecode.com/retail-ops-weekly-5", "abstract": "A key theme from many of the Q2 earnings reports this past week is that the shift from physical to digital is imminent. As a result, executives are looking for more innovative ways to bridge the gap. One notable trend is that underperforming stores are being closed down in favor of further investments across digital channels to improve the customer experience. While these shifts come with an air of change, the end result will be a win-win for retailers and consumers alike. In that spirit, many of the articles curated from the past week follow along with the theme of digital transformation. Enjoy! Expanding channels to reach more customers is a key practice across any digital transformation initiative. Stibo Systems conducted a study to learn more and found that the primary motivations are in creating a relevant and consistent customer experience across all channels. Moving From Omnichannel to Digital Transformation [Retail TouchPoints] Many digital retailers are building intelligent systems that know our styles, preferences, and even moods in order to tailor the shopping experience accordingly. While that sure sounds like the future, is this a realistic goal? Artemis Berry takes a candid look at the meanings behind personalization. How 'relevant' is personalization? [Medium] Digital transformation is not just about the technology, it’s just as important to adapt company culture to match. Chris Cancialosi tells the story of a 150 year old bank that successfully undertook a culture change using DevOps practices. How Culture Change Fuels Digital Transformation: Lessons From Westpac New Zealand [Forbes] Recruiting is a two way street, with individuals and companies both putting their best foot forward. This can be difficult for many companies who may not have a known, visible brand. Dave Fecak tackles the challenge for companies competing with the big guys for engineering talent. 'I Want To Work With Good People' and Engineering Brands [DZone] The Java ecosystem has had numerous changes of the crown over the years, but it’s felt a bit fragmented in recent times. After witnessing a glaring number top tier enterprises at the SpringOne platform talk about developer empowerment, James Governor is declaring a clear leader. Spring Won Platform: the Big Switch just happened [RedMonk] Sure, it’s easy to point fingers and chuckle at outdated infrastructure practices resulting in the recent Delta outage, but that’s missing the point. Michael W. Cooper shares some best practices for dealing with an outage from his own experience at T-Mobile. Outages happen: 4 ways to prevail over your next big system failure [TechBeacon] Understanding how users interact with products is a science, and often comes down to psychology. Cesar Bejarano puts forth the factors that facilitate discoverability; those moments when things just make sense. Human Centered Design & The 6 Fundamental Principles of Interaction Between Products and Users [uxdesign.cc] As more and more purchases happen from mobile devices, it is crucial that native apps and mobile optimized websites enable a clear shopping experience. Nick Babich shares a few practical tips for the ever important Product Detail view. Mobile eCommerce: Product Details [UX Planet]", "date": "2016-9-5"},
{"website": "Avenuecode", "title": "Use Java EE JSON to Avoid Mapping Complex DTOs", "author": ["Felipe Moraes"], "link": "https://blog.avenuecode.com/use-java-ee-json-to-avoid-mapping-complex-dtos", "abstract": "(This article was first published on LinkedIn .) When developing applications, you’ve probably faced a situation in which you need to integrate with third-party APIs, either to improve a service that provides users with data or to add a new functionality. Many software engineers solve this by mapping complex DTOs, but there's a much easier method available. The traditional method for integrating third-party APIs with any given application is as follows: What many software engineers don't realize is that this process is unnecessarily time consuming, which compromises the overall business value of the development project. On the other hand, using Java EE JsonObject and JsonPointer is much more time effective. Let's compare the two approaches with the following example. Imagine that you work for a company whose main app recommends beverages to clients.  (I've created a simple application to mimic this app; you can download the source code here .) As you can see, the app simply suggests beverages randomly to clients when they make calls to this URL . The following options are available: \"Coffee,\" \"Beer,\" \"Hot Chocolate,\" \"Smoothie,\" and \"Tea.\" So far, our app is a huge success (after all, who wouldn’t want an app that randomly recommends beverages?), but the marketing team thinks the app's advertising would be more effective if beverage suggestions were made based on current temperatures in each user's general location. To add this new feature to the app, we decide to use this weather API . If you take a look at the documentation of this API, you will see that it returns the current temperature (and ancillary data) based on the location selected. Here is an example: By analyzing this data, we discover that we only need the field “temp” that is inside the “main” JSON object. With this knowledge in hand, we can proceed to integrate the third-party API with our main app. Based on the JSON above, your solution may be to map all the data with some Java classes, make the API call, and marshal the response into your classes so that you can pick the current temperature from the info. This methodology is depicted in the diagram below: This diagram perfectly illustrates how some \"simple\" mapping can turn into a somewhat complex class hierarchy full of terms whose meanings we don't know and that don't have any bearing on our purpose (the beverage app). We simply want to know the current temperatures and correlate them to different drinks, but instead we've increased the complexity of our domain and consumed valuable time trying to understand and map several terms into our system. At this point, you may be wondering how to access the information you need without mapping the whole object graph. Java EE and the JSON-P API (JSR 374) answer this question in the form of JsonObject and JsonPointer. First, we will create a new class called TemperatureService inside our control package (if you are wondering why I chose such names for the packages, view this video ) with the following code inside it: The code above calls the weather API and maps its return to a JsonObject (from json-p spec). We can use this to create a JsonPointer to the field we want inside the JSON (something like an XPath for XML). Then we can easily use this pointer to extract the data we need from the JSON without creating any class inside the domain, thereby saving hours of mapping classes to values that don't contribute at all to our app. You can test the API using the following URLs: ( Beverage suggestions that don’t consider temperature ). ( Beverage suggestions that do consider temperature ). At Avenue Code, we love finding and sharing time-saving methodologies for developing quality products, and we're always looking for new ways to learn. So tell us what you think of this approach in the comments below--how do JsonObject and JsonPointer work for you? Do you see advantages we haven't pointed out? Any reasons you'd prefer to continue using DTOs? We look forward to hearing from you!", "date": "2018-8-8"},
{"website": "Avenuecode", "title": "MongoDB 4.2: Transactions Are in the House", "author": ["Henrique Elias"], "link": "https://blog.avenuecode.com/transactions-are-in-the-house", "abstract": "If you've used MongoDB or been a part of users' conversations about MongoDB, you'll be aware that, while it's one of the most successful NoSQL databases available, it has faced a lot of criticism for not supporting multi-document transactions. Now, that's about to change. Original SQL databases were built to manage data and stream processing in a relational database. In the late 2000s, the first NoSQL databases were developed t o deal with some limitations of SQL relational databases related to multi-structured data, horizontal scalability, and quickly evolving schemas. In this context, 10gen software company began developing MongoDB in 2007 as a service product, and then in 2009, the company shifted to an open source development model that offered commercial support and other services. In 2013, 10gen changed its name to MongoDB Inc. Now, MongoDB is one of the most successful NoSQL databases out there. Despite the fact that MongoDB is a very successful example of an NoSQL database, one the biggest criticisms it faces is that it does not allow ACID (atomicity, consistency, isolation, durability) transactions when it comes to multi-document transactions, though it does support ACID transactions at the document level. It is important to note here that most NoSQL databases, not just MongoDB, lack true ACID transactions, which were more a function of SQL databases. Some users think that transactions have a great impact on their applications and thus choose to continue using SQL databases rather than NoSQL databases. From my own experience of working with MongoDB, modeling data appropriately will, in many scenarios, minimize the need for multi-document transactions, and the denormalized data model will continue to be optimal for data and use cases. But as many NoSQL database users continue to maintain, there are certain scenarios where multi-document transactions can make your application less complex and more reliable. These users will be happy to hear that MongoDB supports multi-document transactions in MongoDB v4.0. This development has been in progress for a long time. Let's take a look at what lead to this development. Due to the original proposal of MongoDB, their first DB engine, MMAPv1, wasn’t built to support transactions. In December of 2014, however, MongoDB acquired WiredTiger, which was their first step toward including transactions. This acquisition meant that MMAPv1 was replaced by WiredTiger as the default DB engine in MongoDB v3.0. As shown below, WiredTiger performs well in the following categories in most use-cases, whereas MMAPv1’s design makes it suitable for specific, specialized cases. Most importantly, WiredTiger also enabled MongoDB to begin building support for multi-document transactions. This didn't happen immediately since it was impossible for MongoDB to implement all the changes they planned at once. After 3 years, however, MongoDB will soon release v4.2, which will support transactions in a sharded deployment. MongoDB Evolution Path to Transaction When a developer faces a new paradigm, like a shift from SQL to NoSQL databases, he can use it to help his work processes or he can have one of two negative responses: either the developer resists new challenges (a defensive reaction to change) and thinks that the new paradigm does not work for his case, or else he uses the new paradigm in the wrong way.  I have seen some cases where a developer was using MongoDB as a database for an application where the data was too related to be used in a non-relationship database, which meant he had to increase his code's complexity to ensure the consistency of the relationships. In another case, the developer was trying to create his data model, using an NoSQL document-oriented database,  by thinking about it in terms of an SQL relationship database. In some cases, this approach can work, but the best approach is to have a whole new perspective about the application and understand how it makes the most sense to build the model relationships, when necessary, using inner documents or soft constraints (identifiers). In this case also, if the developer used the new paradigm in the way it was intended to be used, his experience would have been much more positive. This is what happened to some developers who tried to use MongoDB: the first MongoDB version releases faced many critiques because they didn't have certain features that developers were used to having with SQL databases, such as join operations and transactions; there were also complaints about its Map/Reduce performance. Some users didn't like the new paradigm of the NoSQL document-oriented database in general. It's important for developers to realize that 1) sometimes adapting to a new model can bring positive change and 2) MongoDB has listened to user critiques and has worked hard to resolve them. MongoDB has evolved a lot in the last years for the better. Not only will it support multi-document transactions, but it also has other features, such as the Aggregate pipeline, which allows users to aggregate data in a flexible way that performs better than Map/Reduce. While MongoDB can still improve, it's certainly headed in the right direction, and many developers can benefit from using it. I believe that transactions will make MongoDB the best choice of a wide range of new applications -- apps that require “all-or-nothing” execution. MongoDB's new version release will allow developers to evolve their current applications to the transactional paradigm, or at least give them the option to choose whether or not they want to use transactions.", "date": "2018-8-15"},
{"website": "Avenuecode", "title": "Let’s not do a Retrospective. We’re too busy right now!", "author": ["Ramon Alves"], "link": "https://blog.avenuecode.com/lets-not-do-a-retrospective-were-too-busy-right-now", "abstract": "The retrospective ceremony is an amazing opportunity for teams to become more Agile by learning from the past and trying new approaches. By so doing, teams can consistently achieve better results in overcoming project obstacles. However, it's often put aside due the pending work the team still needs to create, or because they see no value doing it. The shortest description I can give of a retrospective ceremony is this: a team looks back to examine the project and results, with the end goal of learning from their experiences to improve. The retrospective is, perhaps, the most valuable source of ideas to improve methods and processes. It is an opportunity for team members to come together and discuss obstacles facing the group, where each person is asked to share ideas and insights based on their own perspective and professional experience. However, how often have you heard that the team is too busy to do a retrospective? With the sprint about to end, and project deadlines right around the corner, it's all too common for agile teams to sacrifice the retrospective in the interest of saving time to finish the pending work - or at least trying to finish it. But if the team doesn’t get together to discuss why they are working hard yet struggling to accomplish the sprint goal, they may never identify the blocker or blockers. And if you don’t identify the obstacles for your team and generate solutions for overcoming them, how can you decide what to do differently next time? If you are facing a situation like this, just add a note in your retro board: “Never skip the retro again. It's critical to learn and avoid making the same mistakes over and over”. Problem solved, right? Except that until you start actually conducting retrospectives, you won't have the opportunity to discuss with your team and find out whether they agree. We need to learn from the previous sprints, ask questions and discuss how well or how poorly we are performing against our stated objectives. Based on that conversation, the team can then decide what improvements they will implement in the next sprint. Think of it as a feedback session to facilitate continuous improvement. Ask why some things worked and others didn’t - why the team went down one path and not another. Why is this important? For one, it helps the team to gain a better understanding of the situation as it stands and the problems they are investigating. Once you understand the problems, identifying targeted solutions comes much more easily. The communication is of critical importance in a retrospective. All participants need to feel comfortable enough to ask questions and suggest insights in a safe, transparent, and trust-filled environment. Icebreaker games can facilitate personal interactions and help participants get to know each other better - and, most importantly, enhance the communication within the team. At this point, the goal is to encourage participative communication from the entire team. Keep in mind, the retrospective is not a social talk and the goal is not to point fingers and find fault for project failures. Rather, the goal is to improve the team's performance in the next sprint, namely by finding ways to complete stories more effectively and efficiently. For all topics - or at least the most important ones (e.g. top 10) - the agreements should be made by the entire team. Just like Scrum, employee seniority doesn’t matter here. The outcome will be something similar to an action list that defines what will be done in the next sprint to improve the team's performance. Holding a retrospective will not ensure that your next sprint goals will all be accomplished. It's not a cure-all for every problem faced by your team. However, it will ensure that your team is trying new approaches to overcoming obstacles both old and new - a critical component of Agile philosophy. Let's say they are unable to deliver all the committed work in the next sprint. In the next retrospective, maybe they will discover that they are committing to more work than they can realistically do. Or, maybe they will realize that using another tool or changing their software process will save time and make them more productive. If the team is already able to deliver all the business value they committed in the beginning of the sprint, can't we cut out retrospectives until we run into problems? Please, don’t go this way. If the team is easily able to deliver everything, consider using the retrospective as a time to discuss committing to more work, or testing a new tool to prevent bugs. This is how your team becomes a high-performance team. Personally, I believe there is always room to improve. No matter how well you and your team are performing, you can always make changes and increase your results. Time to start thinking through what suggestions you'd like to bring to your team in the next Retrospective!", "date": "2016-10-19"},
{"website": "Avenuecode", "title": "AHP - A Group Decision Making Technique", "author": ["Fábio Suarez"], "link": "https://blog.avenuecode.com/ahp-a-group-decision-making-technique", "abstract": "Have you ever found your team or group in a situation where you had to make a complex decision? Maybe you came to an impasse, unable to decide between two or more options, or maybe you selected one and still doubted whether you had evaluated all possible pros and cons. Well, what if I told you that there's a way to easily use a little math and psychology to organize and rank your options to determine the best solution ? What if I further told you that these techniques can be applied to a wide variety of decisions, like selecting which framework you should use to create your new application, or even deciding which dog you should adopt? Decision theory is the study of decisions and their consequences. The subject is not a very unified one. There are many different ways to theorize about decisions, and therefore there are also many different research traditions. Modern decision theory has developed since the middle of the 20th century through contributions from several academic disciplines. Although it is now clearly an academic subject in its own right, decision theory is typically pursued by researchers who identify themselves as economists, statisticians, psychologists, political and social scientists, or philosophers. There is some division of labor among these disciplines. For instance, a political scientist is likely to study voting rules and other aspects of collective decision-making, a psychologist the behavior of individuals making decisions, and a philosopher the requirements for rationality in decisions. When looking into how organizations usually make decisions, we notice a constant desire to have clear, objective, and mathematical criteria. However, decision making is, in its totality, a cognitive and mental process of selecting one out of the highest possible number of adequate options based on tangible and intangible criteria, and these criteria do not even necessarily have to be justified by the decision makers. There are various ways of dividing the decision making process, but to simplify, we can split it into 5 basic steps: Multi-Criteria Decision-Making Methods provide ways for us to evaluate the multiple, often conflicting criteria involved in decision making . It's typical to have conflicting criteria when evaluating options, e.g. in an aggregated value decision, where cost considerations are often in conflict with quality considerations. There are plenty of known MCDM Methods. Let's look at some of the most popular: Image courtesy of researchgate ELECTRE (ELimination Et Choix Traduisant la REalite) and PROMETHEE (Preference Ranking Organization METHod for Enrichment of Evaluations) are the main methods used in the French family of theory decision studies. These methods allow decision makers to select the best choice with the biggest advantage and the least conflict between various criteria. Different versions of both methods have been developed, but all methods are based on the same fundamental concepts; they just differ operationally and according to the type of decision being made . The main idea is the proper utilization of “outranking relations.” They enable decision makers to model a decision process by using coordination indices. These indices are concordance and discordance matrices. The decision maker uses concordance and discordance indices to analyze outranking relations among different alternatives and to choose the best alternative using CRISP data. Grey Theory's decision making techniques are highly mathematical. This MCDM Method is based on fuzzy logic and is used to address problems whose variables are partly known and partly unknown, being defined as \"insufficient data\" and \"weak knowledge.\" The idea is to have a large amount of input data and to examine it in an interactional manner. TOPSIS (Technique for Order of Preference by Similarity to Ideal Solution) is a method of compensatory aggregation that compares a set of alternatives by identifying weights for each criterion, normalizing scores for each criterion, and calculating the geometric distance between each alternative and the ideal solution, which is the best score in each criterion. An assumption of TOPSIS is that the criteria are monotonically increasing or decreasing. Finally, we have AHP, one of the most widely used MCDM Methods. We'll focus on AHP today since it's mathematically simple compared to the other methods listed above and since it allows us to compare criteria without previously assigning numeric metrics to them. Here's how AHP can be used to easily make a complex decision. Developed by Thomas L. Saaty in the 1970s, AHP, or Analytic Hierarchy Process, is a technique for group decision making wherein the problem is decomposed into a hierarchy of more easily comprehensible sub-problems, each of which can be analyzed independently. Once the hierarchy is built, the decision makers evaluate various alternatives by comparing them to each other two at a time. In making the comparisons, the decision makers can use concrete data about the elements, but they typically use their own judgments about the elements' relative meaning and importance. In fact, the premise that human judgments can be used in performing evaluations is essential to the AHP. The AHP then converts these evaluations into numerical values that can be processed and compared over the entire range of the problem. A numerical weight is derived for each element of the hierarchy, allowing diverse and often incommensurable elements to be compared to one another in a rational and consistent way. In the final step of the process, numerical priorities are calculated for each of the decision alternatives. These numbers represent each alternative's relative ability to achieve the decision goal and voila, you have your rational decision! Let's go through a simple, existent example published on Wikipedia to understand how the AHP really works. Imagine that we need to select a new CEO for a traditional, big company. The board of directors has selected three possible candidates (Tom, Dick and Harry), but they are struggling to decide who will be the new CEO since some BOD members aren't sure which skills should be prioritized in this process. According to the AHP, the first step is to define a hierarchy of sub-criteria that we can use to compare the options. In this case, the board decided that the four main criteria should be: Experience, Education, Charisma, and Age. Having defined the criteria, we now need to start comparing the options pairwise. In order to do that, we are going to create a WCM, Weight Comparison Matrix, for each criterion. This will measure the relative degree to which each option of the pair accomplishes this specific criterion. The WCM is a square NxN matrix, where N is the number of options that we have, in this case 3. The values of this matrix should be filled by comparing the column number option with the line number option in the given criterion, and these values should be defined using the fundamental scale for pairwise comparisons. After filling the matrix, we sum the values in the line for a given option and divide it by the sum of the values of the whole matrix, defining a priority for this given option relative to this specific criterion. There are a few online calculators that can do this for us after we define the pairwise comparison values. These calculators usually provide an inconsistency factor based on the filled values, indicating how inconsistent the comparison structure of the matrix is based on the fundamental scale for pairwise comparisons. The following images break down each stage of this process: After defining the WCM of each option for each criterion, we now need to calculate the importance between the criteria themselves. We are going to use the same pairwise comparison method, but using a CxC WCM, where C is the number of criteria. Now for our final step, we are going to calculate the criterion weight by multiplying each criterion priority by the corresponding option priority. Then, we sum all the weighted criteria by option, having a final score for each option over the main goal. Based on the board's choice of decision criteria and their judgment about the relative importance of each one, Dick, with a priority of 0.492, is by far the most suitable candidate. Tom, with a priority of 0.358, is second, and Harry, at 0.149, is third. Not only do our board members now have a logical decision, but also, because they have used the AHP, it is easy for them to trace their thinking and to justify the steps that led to their decision. If they have second thoughts about the final outcome, they can revisit the process and make changes if needed. They can also, if they deem it helpful, reveal the details of their process to their consultants, candidates, stakeholders, or to anyone else who might be concerned with the decision. So next time you and your team aren't sure which option will best accomplish its goal, give the Analytic Hierarchy Process a try!", "date": "2019-2-13"},
{"website": "Avenuecode", "title": "Avenue Code Expands to Germany", "author": ["Anna Vander Wall"], "link": "https://blog.avenuecode.com/avenue-code-expands-to-germany", "abstract": "Avenue Code opens its European innovation lab to help enterprise organizations accelerate digital evolution and pivot toward a flexible, distributed model. Avenue Code has just opened its doors in Berlin. Based in San Francisco, with delivery centers in Brazil, Montréal, New York City, and the Netherlands, Avenue Code already serves several iconic brands in Europe. After expanding to the Netherlands at the end of 2019, Berlin was the obvious destination for 2020. Avenue Code’s Founder and Chief Strategy Officer, Zeo Solomon, explained that Berlin emerged as the best choice for several reasons: “Germany is not only the European Union’s largest economy; it’s also one of the world’s largest and most stable trading countries, offering a secure, highly developed political and economic framework. Recently, Germany has been cultivating a diversified workforce, nurturing innovations, and providing economical and financial incentives to foreign investors.” Berlin is attracting several promising startups and top global talent, confirming its reputation as Europe’s primary innovation and technology hub. “Our new Berlin office enables us to continue staying in front of innovation, cultivating ideas from Europe and attracting top talent,” stated Ulyana Zilbermints, Global VP of Business Development. Solomon emphasized Avenue Code’s innovation culture fit with Berlin, explaining that the consultancy chose Berlin “for its vibrant and dynamic culture that has attracted some of the brightest technological minds from all around the world.” Over the next year, the consultancy hopes to add 100 new jobs at its Berlin office. Avenue Code’s clients include Fortune 100 companies in the United States, Brazil, Canada, and Europe, where the firm serves diverse industries like healthcare, FinTech, automotive, and retail. Zilbermints expressed that “Berlin is a strategic location to expand our offerings, transitioning healthcare to virtual reception/telehealth, optimizing banking and payment systems, and converting retail to virtual malls.” Rick Lefevere, Regional Business Development Manager for Europe, added that “Opening our Germany office is the logical step in expanding Avenue Code Europe, offering digital transformation solutions that help companies adapt and thrive in these challenging times.” Avenue Code’s European innovation office is positioned to help clients in multiple verticals adapt to the new realities of a post-COVID economy by driving digital evolution and adopting a distributed model, all while bringing new jobs to Berlin.", "date": "2020-8-26"},
{"website": "Avenuecode", "title": "7 Tips to Stay Productive While Working from Home", "author": ["Manoela Vieira"], "link": "https://blog.avenuecode.com/7-tips-to-stay-productive-while-working-from-home", "abstract": "", "date": "2020-3-19"},
{"website": "Avenuecode", "title": "AC Spotlight - Justina Peixoto", "author": ["Andressa Lopes"], "link": "https://blog.avenuecode.com/ac-spotlight-justina-peixoto", "abstract": "Justina Peixoto , CIO at 123Milhas, highlights innovations in the company’s digital journey and shares her non-traditional career path to becoming a tech leader. Avenue Code: Tell us about your personal career path. How did you get to where you are today? Justina Peixoto : My story is very interesting because I didn't start in the technology area. My trajectory started when I decided to take a technical course in chemistry! Since there was a limited enrollment for the chemistry course, however, a friend of mine recommended a technical course at Cotemig that opened the door to my first job with computers. I worked as a newspaper designer for a while, which helped me pay for college, and after that I got my first opportunity as a developer. I then joined Art, a group at 123Milhas. I started my journey at 123 as a PHP developer, worked for almost two and a half years as a developer, and then migrated to DevOps. I also acted as a project leader and finally became CIO. AC: Did you consciously cultivate soft skills to help you make this transition from a technical area to an upper-level management position? JP: Becoming CIO was an incredible opportunity for me to develop my soft skills. I have always been very shy, and working in a predominantly male environment has been challenging and enriching. I was able to learn how to impose myself and put my ideas on the table. At 123Milhas, merit is highly valued, so regardless of gender etc., technical capacity is the most relevant consideration in hiring. AC : What are the biggest challenges related to being a female leader in a predominantly male environment? What advice would you give to women who aspire to be technical leaders? JP : I have never had a hard time working with male colleagues and have not experienced the challenges other women have, but perhaps I never made room to hear them either. I was always very focused on what I believed in and thought I was competent to do. We have to believe more in our abilities. If the people around us or the company we’re in don’t share the same values, it may be time to change environments. That said, I believe practices can be improved in any scenario as long as they are not rooted in outdated values. AC : Could you tell us more about 123Milhas’s digital journey? What were the biggest challenges? JP : Six years ago, we issued plane tickets manually. The first robot we built to automate the ticketing process was controversial because some people thought that the automation would eliminate jobs. We eventually demystified that belief by showing that technology is in favor of people and not against them since it frees us to focus on excellence in assisting customers. Automation has also helped us create several validation processes that guarantee the best prices within the closed market for plane tickets that are usually purchased using miles. AC : Does 123Milhas have an area dedicated to innovation? How do you foster a culture of innovation throughout the company? JP : When I joined the company, the team was very small. After about 3 or 4 years, we started an expansion process and now have clearly defined roles for each person and each team. I realized that there was an imminent need to assemble a DevOps team, but because It was difficult to find this expertise in the market, we trained internal talent to manage this area. Another important point is that we are moving toward innovation in user experience as well as in technology. We recently hired some designers to improve the user experience as a whole. On the technical side, we are also promoting different ways for project leaders to think about constant implementation improvements. Innovation is linked to technology, but it’s also important to constantly review processes. AC : Travel industries have been greatly affected by the pandemic. In your opinion, what is the role of technology in mitigating this crisis? JP : We’re actually working on process issues that do not directly involve technology. I am closely monitoring our APP processes to learn more about the bottlenecks our service team faces. I am also focused on working with new products that may in the future improve our range of resources input, which will sustain the company going forward. 123Milhas decided to hire rather than reduce its number of employees through this crisis; we are working hard to improve and expand our offerings so that we are ready when things start to improve. AC : How does 123Milhas handle strategic partnerships? JP : We have several partners that help us broaden our range of services, gain competitive advantage, and improve our ROI.  We see strategic partnerships as very important. In 2019, we implemented the Agile methodology, and for that reason, we now search for partners with the same business model to ensure project continuity and culture fit. AC: Is your team entirely internal, or do you partner with consultants as well? JP: We have a team of 40 people internally, and we also work with an outsourced team. We have been in the market for over 10 years, but our newest product - 123Milhas - is less than 4 years old and has enabled us to grow very fast. Because of that growth, we’re now hiring consultants to help us with human resources, important processes, and technical areas. AC : What is something you are proud to have accomplished in your career? JP : I am very proud to have worked on an automation project to optimize a process that involved a team of 100 people. The outcome of the project was that a process that used to take 8 hours took just 1 minute after the optimization. AC: Thanks for sharing your personal career story, as well as all the work you’re doing to strengthen the digital journey at 123Milhas. It’s been a pleasure, Justina!", "date": "2020-10-28"},
{"website": "Avenuecode", "title": "The Shared Responsibility of the Product Backlog", "author": ["Rafael Bandeira de Melo"], "link": "https://blog.avenuecode.com/the-shared-responsibility-of-the-product-backlog", "abstract": "Who is responsible for the product backlog? For many Agilists, there is only one answer - the Product Owner (PO), and by definition, that's correct! The PO is in fact the only one accountable for the product backlog, and they are the person in charge of keeping it organized and prioritized. As a business representative, it's up to them to work hard so that the backlog reflects the product vision, leveraging high priority features, release dates, defects, and even tech debts. However, the PO naturally has a myopic vision of the technical challenges due to his or her business mindset. There are scenarios where a PO often ends up deprioritizing technical activities due to \"business urgencies”. This scenario is aggravated in large corporations that usually apply Scrum of Scrums (SoS) with common goals and deadlines across components for the same product. Among the possible solutions that can help mitigate this problem, one that seems to be widely applied by companies is the sharing of backlog maintenance with a technically skilled professional. The Scrum Guide defines the PO role as “responsible for maximizing the value of the product resulting from work of the Development Team”, which means acting on different areas and activities such as: prioritizing user stories based on business value, effort, and urgency - as well as groom user stories in order to ensure that all scenarios and details are sufficient enough for sizing and implementation. Along with operational activities, there’s a constant challenge to manage the tension between the business' desire for new features to help enhance the user’s process, and product foundational upgrades along with the need for technical debt reduction. So, how would the mechanics of shared responsibility work? Check out these scenarios below: Choosing a Development Team Member to Help and Facilitate Technical Understanding This is usually the first option when applying shared responsibilities. It shouldn't be difficult to identify a team member that stands out due to their technical background and communication skills - this person will most likely be the go-to person for the PO when they need clarification around a technical subject. In this case, the PO will partner on demand with this member, inviting them to prioritization sessions to give more visibility to the technical aspects of the product. This strategy is frequently utilized as a dry-run to learn more about shared responsibility, and can be adjusted as needed. Creating a Product Owner Team (POT) This next option is slightly more structured. In this scenario, a team is created consisting of professionals with diverse and complementary skills (usually a PO, Tech Lead, Business Analyst, and QA Lead) who are all responsible for ensuring that the backlog reflects both business and technical needs. This team will basically be composed of key members who will collectively be accountable for the backlog prioritization - which could be considered a better, holistic approach. However, this approach goes against an explicit definition in Scrum Guide that states that the PO role must never be a committee. That's because a committee brings complexity to the decision-making process, making it bureaucratic and necessitating that everyone's ideas are considered. Creating a Technical Product Owner (TPO) Role Often mistaken with the POT, this role does not involve a committee or group, rather a single professional with technical skills, such as an Architect or Tech Lead with the goal of advising and assisting the PO in technical matters only. The TPO works closely with the PO assisting the general prioritization process by providing technical inputs considering architectural dependencies, impacts, and blockers. The PO may even delegate the technical part of the backlog to the TPO, but it’s important that they do work together to ensure that both sides are familiar with the \"other side” and are running towards a unique and common goal. Together they have a better and balanced backlog due to a more realistic and sustainable prioritization. On a daily basis, the TPO would be responsible for presenting any technical content to the team, signing off on technical cards, and being the overall technical reference throughout sprint planning. In Conclusion Each case has its own nuances, and each company and project may have a different fit or work better under different circumstances. I’ve witnessed startups with high performance projects achieve amazing delivery rates where a single and different developer would partner with a PO on each rotating sprint. Same concept, different flavor. I've also worked on many large-scale projects with different formats, and the most effective model applied was utilizing the TPO role. Having a TPO role drastically improves the planning phase as well as the product itself. In a few companies with these projects that I've worked on in the past, there wasn't an acronym/title for the role, but the role formed naturally after the PO partnered with the Architect. Even when PO has a technical background, I believe that delegating the technical aspect is the best way to go because it will allow the PO to focus on business like activities. A good rule of thumb is to consider your project and team structure. Talk to everyone to get their agreement and commitment. You can always improve this process over time - after all, it's Agile!", "date": "2017-12-27"},
{"website": "Avenuecode", "title": "Becoming a PMI Agile Certified Practitioner", "author": ["Ludmila Roizenbruch Paiva"], "link": "https://blog.avenuecode.com/becoming-a-pmi-agile-certified-practitioner", "abstract": "So you're ready to take your agile practice to the next level. You're interested in the Project Management Institute's Agile Certified Practitioner program, but...PMI and agile together? That's right. In 2011, the Project Management Institute - the worldwide most widely recognized organization in project management expertise and certification programs - launched a new certification which is still raising eyebrows in the agile community. We're talking about the PMI - ACP (Agile Certified Practitioner). The suspicion stems mainly from the fact that PMI, and in fact the whole project management discipline, have a historical association with traditional waterfall plan-driven methodologies - considered bureaucratic, old-fashioned and ineffective by the agile community. Actually, words such as “agile”, “incremental”, “light” or “lean” have historically rarely been mentioned by PMI references. However, with the PMI-ACP launch, PMI has thrown itself into the center of the agile conversation and has taken a big step towards making itself relevant to the community of developers, PM's, and organizations that have migrated to agile methodologies. Additionally, being a PMI-ACP has become a great differentiator for professionals working with agile methodologies, who eagerly embrace complexity and thrive on rapid response times. Further, there are approximately only 11.800 certified PMI-ACP professionals worldwide, in contrast to approximately 723.000 PMP certifications - which shows how it can be a really special asset for agile adopters` career. The first question you should ask about the PMI-ACP is: why should I take it instead of PSM I (from Scrum.org), CSM (from Scrum Alliance) or other agile certifications? I can give you two good reasons: first, because these other certifications obviously focus on SCRUM, while the PMI-ACP tests a broader set of methodologies and practices, including XP (Extreme Programming), Kanban, lean, TDD (Test-Driven Development) and others. Second, because PMI is far more widely recognized than other agile organizations, and the PMI-ACP certification process requires not only a complex exam, but also fulfilling a specific set of requirements (I'll touch on these later), which are not required for the other certification - making it a more selective, strict and solid title. Now that you have the reasons to consider taking PMI-ACP, let's talk about its requirements. Firstly, it's important to know that not everyone is eligible to take the exam. Since its goal is not to introduce agile practice to newcomers, but to recognize someone's existing expertise, these four items are required (besides basic educational background): All these requirements are verified through an online application, completed by the candidate via the PMI's website. In order to test the information's veracity, all applications are subject to an audit. But only a percentage are even selected to go on to this next step. If your application is selected, you'll be notified to provide evidential documentation and submit it to PMI. Once eligibility is approved, the candidate pays a fee and has an one-year period to take the exam. It's possible to take the exam up to three times in this period, should your first attempt prove unsuccessful. The exam is electronic and takes place at a PMI testing center. It's important to highlight that it`s not necessary to be a PMP to take the PMI-ACP exam. All details concerning eligibility and the certification process can be found on the PMI website (www.pmi.org). Let's talk about the PMI-ACP exam content. It's composed of 120 multiple choice questions, from which 20 are not scored, because they are considered “pretest” questions for officially integrating the exam later. However, the candidate has no way of knowing which questions these are. Candidates have 3 hours to answer all the questions, which means an average of one and a half minute per question (you don't have to use all the available time, of course). The questions are divided into seven disciplines, randomly distributed throughout the exam. According to PMI , the disciplines and their respective test item percentages are: The questions are selected randomly from a database, which means that each exam is produced by a different composition of them. It is important to say that some questions are situational - that is, fictional scenarios in which the candidate should choose the best option, not necessarily the only true option. Some questions are more direct and will take you no more than a few seconds, but others require more complex reasoning in order to test the candidate's agile mindset and even interpersonal skills. Another special thing about this certification is that, as a truly agile experience, there’s no such thing as a documented Book of Knowledge, as there is for all other PMI certifications. The candidate study should include, besides the required training, referred articles and books. Finally, to keep your PMI-ACP certification, you must earn 30 PDUs (Professional Development Units) along a three-year cycle. These can be obtained through participation in agile events, trainings, studies, podcasts, working as an agile professional, writing articles, and giving back knowledge to the agile community. The PDUs are reported on the PMI website and are also subject to evidential auditing. So, now you know a little bit more about PMI-ACP. If you are interested in managing agile teams and conducting projects using agile approaches, this certification is worth considering as a differentiator. It will help you learn more about agile framework and become a truly outstanding agile leader. Take your career to the next level - it will be worth it! You can find a lot more information on the PMI website . Good luck on your PMI-ACP path!", "date": "2016-11-16"},
{"website": "Avenuecode", "title": "React Context API", "author": ["Bruno Dias"], "link": "https://blog.avenuecode.com/react-context-api", "abstract": "Let's talk about the React Context API that React 16.3 added. An \"old\" API existed, but people avoid using it now since documentation recommends the new version. So what's great about React Context API? Well, the definition that the documentation provides for the context API makes the benefits pretty clear: \"Context provides a way to pass data through the component tree without having to pass props down manually at every level.\" With pure React (using props objects), we need to store our data in our parent component in the tree to pass down the data to the child component. This chain of data can be very confusing if we have a lot of nested components. So the most common use case is to isolate the data in a place and make the components access it when needed. Redux is the most popular tool to deal with this case, and the new Context API works in essentially the same manner. Let me show you an example. Let's create a React project with ```create-react-app```. In the app.js file, we will create two components, App and Car. On the App component, we will create an object with some initial state in it, and our goal will be to pass this state to the Car component. Let's say we have a component chain with our Car component inside Factory and Factory inside App. The Car component needs to access the state stored in App. So I have to pass the data from App to Factory and then from Factory to Car. As I mentioned earlier, this can become problematic if we have a lot of nesting: This is the kind of situation where the Context API can make our lives easier. If we are going to use Context API, we need two things: a Provider and a Consumer. A Provider accepts a prop value, which can be an object, functions, or any type of data. You can assume it is an object containing your data and actions you want to work with. The Consumer is the object that makes the data stored on the Provider available to the component. Let's look at an example: First of all, we have to create a Context and a Provider component where we are going to store our data. On the MyProvider component, we are setting a state object and a render function. This function must return a Context Provider Object, which, in this case, is MyContext. As shown above, the Provider will wrap our application because we are going to use it in the App component as a store, and then we can access the data from it later. At this point, the data we set for our Provider is hard coded on the prop value, the string,  'My context value'. We are passing this data to all the components that are wrapped inside the Provider, which are all its children. So we can go back to our App component and wrap our code. In this way, any child inside that Provider can access the data stored in it. You may notice that I removed the prop from the Factory component.This is because now we are going to consume our data directly from our Context to the Car component. And how can we do this? In the Car component, we have to create a Consumer to grab the data from our Provider. The Consumer requires a function to be passed as a child, and the child of a Consumer is always a function, so we need to indicate this to React by using curly braces. In the function, we receive the context value as an argument. This value is what you pass in the MyProvider props, and then you can return what you want, e.g. a <p> tag. Now let's look at the actual result on the screen: You can see our Provider wrapping all of the components and our Consumer just on the Car component. If we go back to our Provider, the prop value is hardcoded \"My context value.\"  How do we pass our state object to this props? Actually, it's very simple: we pass it as an object and mount this object as we please, with functions, objects, and any type of data. And then we consume it like this: So, as we see in the examples above, the benefit of using Context API is avoiding all the processes you have to go through to get and pass data to parts of the React Component tree. I hope this helps you understand a little bit about how Context API works. If you want to access the code repository, here it is . Thanks for reading!", "date": "2019-6-5"},
{"website": "Avenuecode", "title": "How To Build Headless Browser Testing and Integrate With Jenkins", "author": ["Douglas Matsumoto"], "link": "https://blog.avenuecode.com/how-to-build-headless-browser-testing-and-integrate-with-jenkins", "abstract": "Google recently announced that the ChromeDriver now has support for headless mode, but what exactly is a headless browser? When and why should we use it? How does it differ from the standard web browsers? This article will demonstrate the use of the new headless mode on Chrome and introduce you to one of the fundamental practices of modern software development: automated tests in a  Continuous Integration environment. We will walk through a quick tutorial on how to use the new ChromeDriver in headless mode as well as how to integrate it with Jenkins in order to run the tests. Headless browsers are browsers like any other but without a graphical user interface (GUI). The GUI allows us humans to visualize an aesthetic representation of the page source code (usually in HTML, CSS, and Javascript). Headless browsers fetch the page the same way standard browsers do, and can render the page (screenshot), and save it as an image file if requested. Although headless browsers don’t render the page for users, they are not faster than standard browsers. The main advantages of using them are that these browsers are more lightweight and therefore require less computational resources, such as memory and processing. They are ideal to use in environments that don't have a GUI: for example, a Continuous Integration server. First of all, we will need to download a version of ChromeDriver that supports the headless mode. In this article I am going to use version 2.3, which you can download for your operating system here . Now, we will create a small application to run an automated test with Selenium using the ChromeDriver's headless mode. I will start a project using Maven and list the dependencies and plugins we will need to build a basic script to automate a simple test. Here’s my pom.xml: And here is our main script: At line 23, we set the location of our driver (the one you downloaded for your operating system in section 2). At line 24, we instantiate the ChromeOptions object which contains all the options for your driver. You can view it as a sort of “configuration” parameter for Chrome. The most important detail to notice is at line 25, where we tell the driver to use the headless mode by passing the argument “--headless”. Line 27 instantiates the ChromeDriver receiving your options object and fetching the Avenue Code website at line 28. What happens from line 30 to 37 is quite simple: we test whether the website title matches our String in line 38. One of the most important sections are lines 33 through 35. At line 33, we create a new file named “srcFile” and reference it to the screenshot taken of the page by the driver. Then we copy this file to the directory specified at line 35. So every time we need to take a screenshot from a page, we have to repeat the process of lines 33 and 35. For cleaner and more concise code, you can create a method to repeat this process and receive the file name as a parameter. To build and run the test from the terminal we can type: mvn clean verify This will execute all your tests inside the test source directory (src/test/java). This works thanks to the “ mvn -surefire-plugin” we added into our pom.xml. And here are the test results: And we can check the screenshot taken by the driver at the “screenshot” directory of the project. Now, it’s important to have this project in a source control repository if you want to integrate it with Jenkins in the next section. Continuous Integration (CI) is a term we are hearing more and more often and is a requirement for most modern software development. Here’s a definition of CI by John Ferguson Smart: “Continuous Integration, in its simplest form, involves a tool that monitors your version control system for changes. Whenever a change is detected, this tool automatically compiles and tests your application. If something goes wrong, the tool immediately notifies the developers so that they can fix the issue immediately.” This process makes software development more reliable and less prone to bugs by providing fast feedback. We also usually run regression and integration tests, check code quality, and provide metrics for every build. But we have to keep in mind that Continuous Integration is more like a practice than certain tools: the entire team has to learn how to think the “Continuous Integration way”, and needs to understand all the pipeline steps from development through deployment to keep the CI server reliable. Now that you know what a CI server should provide, it is time to create a simple Job that will run our automated tests from the first part of the tutorial. Jenkins is a Java application, so you will need a Java Development Kit ( JDK ) installation of at least version 5.0. To check whether you have an installation of Java, you can type the following command in your terminal: You can install Jenkins by accessing the website and downloading the installer for your operational system. Once installed, it will start an instance of Jenkins in your machine and you will be able to access it by opening your browser and typing the following URL: http://localhost:8080 Another way to run Jenkins is to download the Jenkins binary file as a Generic Java Package. This file is distributed as a WAR file so you can use it to run independently of your operational system and without an installation process. After downloading the WAR file, go to the directory that contains the “jenkins.war” and type the following command from your terminal to start Jenkins at the port 8080: $ java -jar jenkins.war If this is the first time you start Jenkins, it will ask you for a password. You can find the password on Windows where it is stored at “C:/Program Files (x86) /Jenkins/secrets/initialAdminPassword”. On my Linux, it was stored at “/.jenkins/secrets/initialAdminPassword”. It is important to note that this will be the default password for your “admin” user unless you change it afterwards. Depending on the version of Jenkins you installed, it will suggest that you download and install a few plugins when you open it for the first time, and you can either install the recommended plugins or do it later as we will in the next section. Here is what my Jenkins homepage looks like: That’s it! If you see the Jenkins homepage, that means you are almost ready to create your first job. In order to get the best of Jenkins, we will need a few plugins to incorporate into our jobs. To install plugins you must first go to the “Manage Jenkins” section on the left side panel and then click on “Manage Plugins”. This page is where you can install new plugins by clicking on the “Available” tab and then on the “Installed” tab to see the plugins you already have. For this tutorial we will need the following plugins: We will also need to implement some configurations in order for the plugins to work correctly. In order to configure the plugins, we go to “Manage Jenkins” and then to “Global Tool Configuration”. You should see the “JDK” and “Git” sections in this page. Let’s first set our JDK parameters. After clicking on “Add JDK”, the section will expand and display a few fields to which you can assign any name you want. For this example, I named them “JDK 1.8\". Now for the JDK itself, you have two options: to provide the PATH of your JDK installation in your machine or let Jenkins automatically download it for you. If you choose the automatic option, all you need to do is provide the version you want Jenkins to use. Here is what my JDK configuration looks like: Configuring Git is almost the same process. You can either specify a name and the path to your executable Git or choose to let Jenkins automatically download it. Here is what my Git configuration looks like: With everything set, we can now create our job to run the automated test using our headless driver. On the Jenkins homepage, click on “New Item” on the left sidebar. Then, choose a name for your job and select the “Freestyle project”. On the next page we will configure our job. Let’s start by configuring the “Source Code Management” section. Since we are using Git for version control we can select the “Git” option. Next, we can add the repository URL into the “Repository URL” field and specify the branch you want to clone from. In my case, I used the master branch. Here is my source code configuration: In the “Build” section, we tell Jenkins how it should build our project after cloning the repository. Here, we should put the same command we used to run our tests locally in the first part of this article. If you are on a Mac or a Linux you can add the “Execute shell” for your build step. If you are on Windows, use the “Execute Windows batch command”. Now, type “mvn clean test” into the command field and save your job. Here is my Build section configuration: With everything set, we can now run the job! Go to the Jenkins dashboard and you will see your newly created job. Now you just need to click on the small clock with the green arrow on the right side to start the build. The first time might take a few minutes because it will download the code from the repository as well as all the needed dependencies. Here are the results on the console output of Jenkins (which should be similar to the output you had running locally in your machine). Note that I omitted most of the parts of the log and left the important lines where Jenkins clones the repository as well as builds and runs the project : If there are any changes to the code, the job will retrieve them from the repository every time you run it. You can also navigate from the Jenkins UI (localhost:8080) through the project folder “workspace” and look for the folder “screenshots” where we saved the screenshot taken by the Chrome webdriver. It’s important to note that Jenkins doesn’t work only for Java. You can use it in any programming language you want. All you have to do is set the right shell command you use to run your project, and it should work without any problem. References Bidelman, Eric. (2017, April). Getting Started with Headless Chrome. Retrieved from https://developers.google.com/web/updates/2017/04/headless-chrome Sangaline, Evan. (2017, April 14). Running Selenium With The New Headless Chrome. Retrieved from https://intoli.com/blog/running-selenium-with-headless-chrome Smart, J. Ferguson. Jenkins: The Definitive Guide. Sebastopol: O'Reilly Media, 2011.", "date": "2017-8-3"},
{"website": "Avenuecode", "title": "City of Digital Angels -  How LA's Hottest Brands Are Embracing the Digital Revolution", "author": ["Holly Camponez"], "link": "https://blog.avenuecode.com/city-of-digital-angels-how-las-hottest-brands-are-embracing-the-digital-revolution", "abstract": "Last week we headed down the coast to LA to hear from top retailers and brands at #LAECOM, hosted by IC Summit. While the panelists were astonishingly diverse, ranging from social media darling Snapchat to insurance providers like Farmers, and e-comm startups peddling everything from athleisure to home goods, some consistent themes emerged. Image courtesy of IC Summits. Every panel, regardless of topic, touched on machine learning and artificial intelligence. Interestingly, the consensus was far from positive. While panelists expressed some excitement about the potential for personalization, recommender systems, and sentiment analysis, the resounding sentiment was quite different: suspicion. Tanya Hersh, VP of Marketing at Hollar, expressed her hesitation to put complete faith in the “mysterious algorithms within the black box.” As machine learning is increasingly consumable in the form of SaaS products - including from several exhibitors at the Summit - marketers are increasingly suspicious of the science behind the pretty dashboards and buzzwords du jour. Image courtesy of technologyreview.com . Our take: Machine learning has enormous potential to impact everything from demand forecasting to sentiment analysis (see our newest whitepaper for more), but buyers should beware: not every data science product is all it's cracked up to be. Don’t be afraid to ask the tough questions when investing in this area. The shift to mobile is no secret - in fact, the dead horse has been beaten at every industry conference in existence over the last several years. So what was there to discuss at the IC Summit? Panelists commented on the fact that in 2017 for the first time, the bulk of their transactions occurred via mobile. While some contended that shoppers will always prefer to make some purchases via desktop, others, like William Bater, Director of E-commerce at Tenzo Tea, shared that 80% of Tenzo’s transactions happen via mobile and expressed a conviction that Gen Z will be increasingly comfortable shopping and transacting via mobile. Our take: Any way you slice it, a solid mobile strategy has never been more important. If you’re not sure where to begin, a good IT consultancy can help you determine when and how to introduce mobile, or how to optimize your existing strategy. Panelists were agreed that voice and video are still underutilized as means of customer acquisition and engagement, but a clear direction or philosophy was missing from the conversation - although Snapchat’s Dani Van de Sande shared some compelling new features from Snapchat’s self-serve advertising platform to utilize organic video as a means of engagement. By and large, the feeling was that while by voice and video are powerful tools, attribution is difficult to track and the potential is largely untapped. Image courtesy of Snapchat. Our take: In the ever-important distinction between noise and trends, video and voice technology are here to stay. Look for both of these technologies to become more prevalent, and start having the discussions around attribution tracking and user behavior with your tech teams now. Speed continues to gain importance as a key factor in driving consumer behavior. Panel moderator Jonathan Bradbury, Director of Global eCommerce from Munchkin, cited research pointing to the fact that 45% of people expect a website to load in 2 seconds or less. 52% claim that quick page speeds are important, and there is a significant drop in conversion caused by delays of even 1 second. Panelists alluded to the 90-second checkout test - any process that’s longer will result in a dropoff in conversions, regardless of the channel. One of the panelists joked that a human’s attention span is now shorter than that of the average goldfish, and when it comes to purchasing behaviors, it’s hard to argue with that assessment. Image courtesy of PetMD. Our take: The discussion about speed isn’t new, by any means. While there are numerous speed tests available, it isn’t always easy to identify just how to go about improving your site or app performance. If you need help pinpointing the low-hanging fruit that will take you from slow and unsteady to quick and reliable, this is a good place to begin. The digital revolution is far from over - if anything, it’s accelerating. Ecommerce as a whole can benefit from looking at changes in technology as the new normal, rather than a one-time transformation from brick-and-mortar to digital transaction.", "date": "2018-2-23"},
{"website": "Avenuecode", "title": "Hadoop and Hadoop MapReduce for Dummies", "author": ["Paulo Rezende"], "link": "https://blog.avenuecode.com/hadoop-and-hadoop-mapreduce-for-dummies", "abstract": "Simply defining Hadoop is a difficult task. Since Hadoop doesn’t fit in other common software categories such as operational systems, databases, message queues, etc., defining it usually requires long sentences. However, long sentences would prevent this introduction from achieving its purpose, which is to explain Hadoop and its' MapReduce framework to those who know absolutely nothing about it. So, how do we explain something without describing what it is? By describing what it does and how it works. What does Hadoop do? Most developers work their entire life without creating a truly distributed application. They create applications that interact with distributed databases, distributed message brokers, and maybe even distributed application servers, but their applications are not distributed ones. Hadoop gives developers the power to create distributed applications and execute distributed processes, but it does so by running itself in a completely distributed model. Why is my application not a distributed one? The same concept may have lots of different definitions. A distributed application, in this context, means taking an input or a request, and executing distributed processes to generate an output or response. For example, a Rest API application may run over a distributed application server, and can be deployed across many physical servers (or many virtual machines). It can communicate with distributed databases and any other distributed system, but when a request comes, the code that handles that request executes on a single server or process. Why do I need to run distributed processes? Usually, developers don’t need to think about distributed processes. They rarely write code that handles more than a couple of messages, thousands of records, or more than one file at a time regardless of the number of rows in a table, the number of messages in a topic, or the number of files on filesystem. But, what if there is a requirement where you need to process an entire table or all the files you have on a filesystem? Distributed processes become the solution when the volume of data that has to be processed cannot be handled by a single process executing on a single thread running on a single server. Are you talking about Big Data? Maybe. Notice that most companies who use some sort of computer system to support their operations already accumulate huge volumes of information from those systems. Regardless of their business segment, they may have millions of operations and transactions registered somewhere. However, they process these operations and transactions on an individual level. Their systems cannot extract valuable information from all the accumulated information because they would not be capable of processing all the data. How does Hadoop handle huge volumes of data? In order to understand how Hadoop processes data, it is important to understand how it stores data. The answer is the Hadoop Distributed Filesystem, or simply HDFS, which is a cluster of servers running HDFS  that store the data that will be processed by Hadoop. If the volume of data increases, the solution is to add more servers to the cluster. A HDFS cluster can literally have thousands of nodes. The strategy used by Hadoop to allow developers to create processes using this huge volume of data is to distribute and execute code provided by developers across all nodes. Each node will execute the code, only processing the data stored on that same node. Notice how this is completely different from other programming models. On Hadoop, data is not transported to where the code will be executed. Instead, the code is transported to and executed where the data is stored. How do I create distributed applications with Hadoop? You can do so by using Hadoop MapReduce. This framework provides developers the ability to create code that will be executed on HDFS nodes. Basically, developers need to implement two classes when implementing MapReduce. As the framework’s name suggests, there will be a mapper class and a reducer class. Mapper will be executed on all HDFS nodes, no exceptions. There are more details involved, but the basic idea is that Hadoop will read HDFS, look for the files expected by the mapper, and invoke it by passing the content of the file. Mapper will then process, filter out, sort the data on file, and forward it to reducer. Reducer can perform additional operations on data received from mapper, but the essential function is that the output generated by reducer will be saved to HDFS on new files. Reducers also execute on nodes, but since the data generated by the mapper phase is considerably smaller than the original data stored on HDFS, only a few instances of reducers may be required during the execution of a MapReduce, instances that will only run on some nodes of the cluster. Once the processed result is saved to HDFS, external applications can access HDFS and provides users with the information they are looking for. Can you give a real and practical example? Yes. Let's take a look at a retail company with a couple of different systems managing information for about a million products. Each piece of information (description, images, pricing, inventory, etc.) about each product is successfully managed by each of those different systems. All these systems use some sort of relational database, but there is no system or database centralizing all the information regarding products. This company will need to summarize all the information regarding all of its products multiple times per day and send it to some partners via a text file. The approach that most of developers would choose is to create a single application that connects to all databases, reads records from necessary tables, and saves them onto a file. Notice that this application would need to group information about the products before saving it all onto the file. The problem is that millions of records are stored by each database and they all need to be used to create the output file. Reading these records one by one is not a viable solution as it would take hours to complete. Furthermore, files need to be generated with up-to-date information and sent to partners every 15 minutes. Using memory to store all the records is also not a viable solution because the amount of memory required would be huge and likely not provided by any server. Therefore, this company used Hadoop to solve its issues. First, they utilized processes connecting to databases and copying information from them to HDFS. These processes were spread over hundreds of nodes in the Hadoop cluster. Each process only reads certain records from tables, not all of them. When these processes finished, thousands of files containing product information were saved and distributed across the HDFS nodes. However, this information was not yet summarized. It was merely a copy of what was stored by original databases. In order to create the ideal file expected by partners, other processes were executed to read the files stored on HDFS, group the information about products, and save everything onto a single file. Again, these processes were not run on single servers, instead, they were run on the hundreds of servers in the Hadoop cluster. By running on this distributed model, files only took 5 minutes to be generated, as opposed to the expected 15 minutes, and contained summarized and up-to-date information about millions of products. Is MapReduce the only solution? No, not anymore. On Hadoop 1, MapReduce was not only a framework provided to developers for writing distributed applications, but also the underlying mechanism used to distribute and execute these applications across Hadoop nodes. On Hadoop 2, these two tasks were split. MapReduce still exists as a framework, but managing distributed applications is now a task executed by YARN. YARN enabled the creation of other frameworks to run on Hadoop and extract information from HDFS. A very common alternative to MapReduce is Spark. Basically with Spark, you process data creating a pipeline of operations. Data can be processed in many steps and unlike MapReduce, you are not limited to only two processing phases.  Another option would be Tez, which is intended to replace MapReduce. There are plenty of other tools available for use as well such as HBase which is a database for Hadoop. There's also Sqoop which makes it easier to transfer data between relational databases and Hadoop. Pig also offers a high-level language to write programs that analyze data stored in Hadoop. These are just some of the options from a much longer list. What should I do next? If you want to learn more about Hadoop, here are my recommendations: Try to get familiar with HDFS. Hadoop documentation explains how to install and use it . Installation and configuration are good starting steps that will help you to learn more about HDFS works; however, if you wish to skip these steps and expedite the process, Cloudera and HortonWorks are two companies that provide virtual machines with everything already set. Just choose one, download it, and start playing around with the HDFS commands. You can also implement your own MapReduce. The tutorial provided by Apache is a good starting point, but also try to write your own mapper and reducer from scratch utilizing a scenario from your business where it could function a solution.", "date": "2017-9-7"},
{"website": "Avenuecode", "title": "What You Need to Know about JShell", "author": ["Daniel Vilas-Boas"], "link": "https://blog.avenuecode.com/what-you-need-to-know-about-jshell", "abstract": "One of the tools that Java developers wanted to see in the development kit was a tool that had been present in other programming languages (ipython, erb) for a long time - an interactive shell. Thankfully, this interactive Java Shell tool, known as JShell, was added in the JDK 9 release (September 2017) and has since become one of its most acclaimed features. JShell is also known as Read-Evaluate-Print Loop (REPL). The name is self-explanatory: the tool reads user input, evaluates the expressions, and prints the outputs in a continuous loop, usually from the command line (terminal). One of the big advantages of JShell is that it gives you the ability to quickly evaluate expressions, testing pieces of code one at a time interactively without having to use Java's regular compile flow. The regular process would involve: writing a class with a main method, compiling it, checking whether there are any compile errors, and then running the compiled class to get the output. On the other hand, JShell allows developers to test individual statements, functions, method variations, and unfamiliar libraries almost instantaneously. Therefore, building software with known behavior becomes much faster. In order to install JShell on your local machine, you will need to install JDK 9 or any subsequently released version. Note that Oracle currently does not support Java 9 or 10 anymore, because they were not meant to be LTS (long-term supported) versions. So if you visit the official download page , these versions are not available for download. Let's proceed by installing either Java 11 or 14.  This can be done manually by downloading from their website or by using your OS package manager (brew, apt-get). As a side note, if you need to have another version of Java installed on your machine, you can switch the versions using JEnv , SDKMAN , or Jabba, or by manually setting the JAVA_HOME environment variable. After the installation is complete, we are ready to use JShell. Starting and stopping it is pretty straightforward: ➜  ~ jshell |  Welcome to JShell -- Version 13.0.2 |  For an introduction type: /help intro jshell> /exit |  Goodbye Note: If you want more details on each operation, start JShell in verbose mode (-v) VARIABLES, METHODS, AND CLASSES: jshell> int x = 10; x ==> 10 |  created variable x : int jshell> double y = 20; y ==> 20.0 |  replaced variable y : double jshell> x + y $7 ==> 30.0 |  created scratch variable $7 : double You may have observed that when evaluating expressions, JShell creates a scratch variable and assigns the value to it ($7); that variable can be used later by simply typing its name. Another difference that you may have observed is that it's not necessary to add a semicolon at the end of statements - JShell will automatically add it for you. If you're using Java 10+ (which you probably are), you can also take advantage of the new `var` resource. This will allow you to create variables that will have their types inferred according to the value passed. jshell> var anotherString = \"Daniel\" anotherString ==> \"Daniel\" |  created variable anotherString : String jshell> var myNumber = 10 myNumber ==> 10 |  created variable myNumber : int jshell> var anotherNumber = 10d anotherNumber ==> 10.0 |  created variable anotherNumber : double To declare functions, follow the same structure as above: jshell> String reverse(String myString){ ...> return new StringBuilder(myString).reverse().toString(); ...> } |  created method reverse(String) jshell> reverse(\"MONDAY\") $23 ==> \"YADNOM\" |  created scratch variable $23 : String You can replace variables and methods by simply declaring them again. Also, take advantage of JShell's auto-complete by typing <tab> as many times you want. This works for Java native packages, display signatures, documentation, and even data inserted by the user in the current session. Another very useful resource is the ability to declare methods and classes with contained values that were not declared before. This resource is also called Forward Reference. jshell> BigDecimal taxOverSalary(BigDecimal salary){ ...> return salary.multiply(COUNTRY_MONTHLY_TAX_RATE);} |  created method taxOverSalary(BigDecimal), however, it cannot be invoked until variable COUNTRY_MONTHLY_TAX_RATE is declared |    update overwrote method taxOverSalary(BigDecimal) jshell> taxOverSalary(new BigDecimal(\"1000\")); |  attempted to call method taxOverSalary(BigDecimal) which cannot be invoked until variable COUNTRY_MONTHLY_TAX_RATE is declared jshell> BigDecimal COUNTRY_MONTHLY_TAX_RATE = new BigDecimal(\"0.001\") COUNTRY_MONTHLY_TAX_RATE ==> 0.001 |  created variable COUNTRY_MONTHLY_TAX_RATE : BigDecimal |    update modified method taxOverSalary(BigDecimal) jshell> taxOverSalary(new BigDecimal(\"1000\")); $37 ==> 1.000 |  created scratch variable $37 : BigDecimal One important point here is that if you update the type of a variable later on, that will break the method at runtime. You can only view whether a method may cause future errors in verbose mode. To create classes, follow the same examples from above: jshell> class Pet { ...> ...> public String name; ...> private int age; ...> ...> public void setAge(int age){ ...>     this.age = age; ...> } ...> public int getAge(){ ...> return this.age; ...> ...> }} <press tab again to see documentation> jshell> p. equals(       getAge() getClass()    hashCode() name notify()      notifyAll() setAge(       toString() wait( jshell> p.setAge(10) jshell> p.name = \"Toddy\" $63 ==> \"Toddy\" |  created scratch variable $63 : String In JShell, there are some native commands that will help you a lot during development: Most JShell shortcuts are the same resources available in several other interactive shells. To take advantage of using external libraries, download the jar files and place them in /opt/libs, then add the folder to your classpath in one of the following ways: 1. Add the libraries to the classpath: jshell --class-path /opt/libs/guava-19.0.jar:/opt/libs/commons-lang3-3.4.jar 2. Change evaluation context within the current session: jshell> /env -class-path /opt/libs/commons-lang3-3.4.jar:/opt/libs/guava-19.0.jar After following one of the steps above, you will be able to import third-party libraries as is: jshell> import org.apache.commons.lang3.StringUtils As you may have observed, Oracle has put a lot of effort into building a very complete interactive shell that provides a lot of resources. It should be used mostly for fast prototyping and experimentation, since it provides instantaneous feedback without the need for compilation (which may take some time depending on the project size). It does not replace IDEs - these should actually be used together. I also think JShell is a great tool for educational purposes, as it is easy to use and helps people who are starting to learn programming or Java. In my personal opinion, JShell has two main target audiences: 1) People who are learning programming logic or Java - it is a fantastic tool due to its simplicity and how easy it is to use. 2) Most experienced developers - might need some time to get the hang of it, but after gaining proficiency with combining commands and using auto-complete with the preferred editor side by side, it will boost productivity. I also think that it could be easier to import external libraries ( you can try this alternative ) and that editing big portions of code within the shell might not be helpful since it works very much like emacs, which is not unanimous among developers. For further reading and resources, check out these articles: Oracle documentation JDK downloads Jenv trouble shooting Jenv configuration", "date": "2020-6-10"},
{"website": "Avenuecode", "title": "Omnichannel is Dead. Long live Omnichannel!", "author": ["Holly Camponez"], "link": "https://blog.avenuecode.com/why-were-done-talking-about-omnichannel", "abstract": "For years now, the evolution of how people shop, plan, and behave with the advent of new technologies has been a hot topic. I don’t know about you, but I’ve read approximately 10000000000 articles about how brick and mortar is dead, and at least 100000000 more about how retailers must “go omnichannel” and “adapt to the millennial shopper”, or else go the way of the buffalo. At Avenue Code , though, we have a vested interest in how to actualize these buzzy, elusive imperatives for our clients. Last week we headed to Shoptalk in Las Vegas to hear, straight from the horse’s mouth (the horse in this instance being over 200 top retailers and brands), what’s happening in the world of e-commerce. So what did we learn? Something is happening. Visualize the ideation, development, and actual implementation of technology as a slinky.* Before it ever makes a real step, each spiral has to be set in motion and push the next one on and on until finally, the center of gravity shifts and the whole thing moves into action (or it goes crazy and gets away from you altogether - see below). In e-commerce, this has historically translated to years of talk and hypotheticals before the user experience ever changes. But guess what? The slinky is moving! We’re finally seeing actual changes from a user perspective. Multiple brands shared in concrete detail how they’re already owning the store of the future. Zack Overton, GM and VP of Customer Experience at Samsung Electronics, spoke about Samsung 837 , a purely experiential, non-retail digital playground. “The future of retail,” he told us, “is not on its way. It’s underway”. Likewise, the endless aisle concept has come to fruition, and the folks over at Oak Labs are developing a touch screen fitting room mirror that can be used to change the lighting, check availability of alternative colors and sizes, and even buy items directly from your phone. The ROI of these innovations is staggering. Healey Cypher, Founder and CEO of Oak Labs, reported that visitors to these stores spend 40% less time in the fitting room, yet spend an average 59% more money. We’ve seen virtual reality and AI touted for years as the next step in the evolution of retail. So what’s making it stick this time around? After years of trying, we finally have both the data and the analytics to make sense of it. Retailers and e-tailers are no longer having to make merchandising, marketing, or experiential decisions based on gut feelings. Eric Colson, Chief Algorithm Officer at StitchFix, shared how this informs the customer experience and, in turn, shopper behavior: “[At StitchFix], the entire business model is predicated on getting the algorithm right”. In a subscription-based business with personalization at its core, data is at the heart of every decision. Marry that data with world-class stylists who understand the human element, he explained, and you have a multi-million dollar brand. With the advances made in analytics comes a shift in how products are sold. The critical thing here is that the data comes from listening to what consumers are truly saying they want, and acting to address their pain points - not from gimmicky futuristic technology for technology's sake.  And, importantly, the solutions are not one size fits all. For example, augmented reality is now being leveraged to directly address a common problem for shoppers in the market to purchase furniture - how will a given piece translate from the showroom to their own home? John Strain, EVP and CTO for Williams-Sonoma, explained how the WSI Pottery Barn brand recently announced a partnership with Google to let consumers virtually explore furniture, paint colors, and more . As a result, shoppers can hold up  a phone or tablet in their living room and see the piece of furnture superimposed at scale. Houzz and Sephora are already running apps that match shoppers with furniture recommendations based on their Pinterest favorites, or let them virtually try out an entirely new makeup look, followed with product recommendations. In a word, VR and AI are succeeding now because for the first time, they’re being applied in the service of what consumers actually want . It sounds simplistic, but think about it - how many annoying ads have you seen because “people like you” were interested? Retailers and brands are getting smart about the fact that personalization is not about groups - it’s about individuals. As Google put it in their slogan for the brand-new Google Assistant, “We made this [Google Search] for everyone. We’re making this [ Google Assistant ] just for you”. Omnichannel used to hold a monopoly on what it meant to effectively reach consumers. While it’s not going anywhere, people have gotten a bad taste in their mouths from the term. As Strain put it, “We don’t love the term omnichannel. It feels like the lowest common denominator - like you’re trying to be all things to all people. At this point, we’d rather talk about optimizing multi-channel, and playing to the strengths of each channel”. So what does that mean when it's put into action? It means, in essence, removing the barriers to a unified shopping experience for consumers. Shoppers don't want to choose between a bricks and mortar experience or a desktop experience or a mobile experience. They want the freedom to move seamlessly between channels, and the successful retailers are those who let consumers forget about channel decisions altogether. Need help implementing your consumer-centric strategy? Westfield, the international shopping mall giant, takes that notion a step further. Admirably, Westfield (which operates thousands of shopping centers and outlets globally) has been investing in the future of commerce for years with its Westfield Labs division, based out of San Francisco. Westfield Labs has recently been re-branded as Westfield Retail Solutions , and continues to bridge the physical/digital gap to meet shoppers where they are. Speaking at Shoptalk, EVP of Product Engineering and CTO Mike Blandina put it unequivocally: “Omnichannel is dead. The consumer IS the channel”. With mobile continuing to take expanding market share of all purchases globally, the lines between physical and digital are increasingly blurred. Omnichannel isn’t dead, and it isn’t going anywhere. But what it means for retailers and consumers has changed. To be successful, an omnichannel strategy cannot simply mean having an app, responsive design, and a presence on social media. At this point, it’s about reimagining e-commerce from a consumer-centric perspective, and letting emerging technologies follow shopper behaviors - not the other way around. We can’t know exactly what e-commerce will look like in the coming years. But we’ll keep listening, and keep working behind the scenes for our clients to stay on the forefront of innovation. Were you at Shoptalk last week? What were your takeaways? Let us know in the comments! *Hat tip to our CEO, Chase Hill, for the slinky metaphor!", "date": "2017-3-29"},
{"website": "Avenuecode", "title": "Tutorial: Building a WebApp using Swift Part 2", "author": ["Guilherme Teixeira"], "link": "https://blog.avenuecode.com/tutorial-building-a-webapp-using-swift-part-2-2/", "abstract": "In the first part of our Tutorial, we learned how to setup and run our Vapor environment, as well as how to configure MySQL for use. If you missed Part 1, you can read it here ! Now, in Part 2, we will create our data Model and learn how to develop the necessary basic data operations that will be used on our Pet App. So, let's get started! Model is the base protocol for any of your application's models, especially those you want to persist. Using Model subclasses, we can get objects that represent your information, and all the complexity involved in create, delete, update and fetch commands will be resolved! All we have to do is specify the fields, make it conform to ResponseRepresentable protocol, and create preparations. First of all, let's create a new file named Pet.swift inside Models folder. The first step is to open this file and import: import Vapor import Foundation Now, we declare our class as Model protocol, like this: final class Pet: Model { } Let's create a few properties that will be the persisted columns on our database, not forgetting to id properties that are required when using MySQL: var id: Node? var name: String var age: String var owner: String var lastVisit:String var specie:String var picture: String var exists: Bool = false Now we implement the 3 needed methods: init(name: String, age:String, owner:String, lastVisit:String, specie:String, picture:String) { self.id = nil self.name = name self.age = age self.owner = owner self.lastVisit = lastVisit self.specie = specie self.picture = picture } init(node: Node, in context: Context) throws { id = try node.extract(\"id\") name = try node.extract(\"name\") age = try node.extract(\"age\") owner = try node.extract(\"owner\") lastVisit = try node.extract(\"lastVisit\") specie = try node.extract(\"specie\") picture = try node.extract(\"picture\") } func makeNode(context: Context) throws -> Node { return try Node(node: [ \"id\": id, \"name\": name, \"age\" : age, \"owner\": owner, \"lastVisit\": lastVisit, \"specie\" : specie, \"picture\" : picture ]) } The first initializer is the one we will use to create our new persisted information. The second and the third methods are necessary to conform to ResponseRepresentable. With that, we have the information in Node object format, which is used by Fluent to store and retrieve information, and as an added bonus we get facilities like the ability to transform objects in JSON format. Lastly, to complete our class, we have to implement two more methods that will handle the database tables. The first is the prepare method, responsible for creating new tables when they are not yet extent, and the second method is revert , used to delete the entire table: static func prepare(_ database: Database) throws { try database.create(\"pets\") { pet in pet.id() pet.string(\"name\") pet.string(\"age\") pet.string(\"owner\") pet.string(\"lastVisit\") pet.string(\"specie\") pet.string(\"picture\") } } static func revert(_ database: Database) throws { try database.delete(\"pets\") } Now, our Model class is done. We only need to declare our preparations together with our Droplet creation on Main.swift , like this: let drop = Droplet( preparations:[Pet.self], providers:[VaporMySQL.Provider.self] ) Now that our Model is done, we can start our database operations. In computer programming, CRUD is the Acronym for the four basic persistent storage functions: Create , Read , Update and Delete . For now, our server CRUD requests will be answered with JSON response, and User Interface will be the theme of Part 3 of this Tutorial. Let's open Main.swift , register new routes, and practice what we'll need to do to get the CRUD operations for our Pet App: drop.post(\"pet\") { request in guard let name:String = request.data[\"name\"]?.string, let age:String = request.data[\"age\"]?.string, let owner:String = request.data[\"owner\"]?.string, let nextVisit:String = request.data[\"nextVisit\"]?.string, let specie:String = request.data[\"specie\"]?.string, let picture:String = request.data[\"picture\"]?.string else { throw Abort.badRequest } var myPet:Pet = Pet(name: name, age: age, owner: owner, lastVisit: nextVisit, specie: specie, picture: picture) try myPet.save() return try JSON(node: Pet.all().makeNode()) } In this first snippet, we register a POST method for an endpoint \"pet\" , and safely store the needed parameters using Swift's guard let . If some of these parameters are missing, then we throw a Bad Request message. Then, with all parameters correct, we create our Pet instance and save through myPet.save() . Guess what??? Our Pet is saved!!! And to finish, we simply return all the entries on our Pet table in JSON format. drop.put(\"pet\") { request in guard let identifier:String = request.data[\"id\"]?.string, let name:String = request.data[\"name\"]?.string, let age:String = request.data[\"age\"]?.string, let owner:String = request.data[\"owner\"]?.string, let lastVisit:String = request.data[\"lastVisit\"]?.string, let specie:String = request.data[\"specie\"]?.string, let picture:String = request.data[\"picture\"]?.string else { throw Abort.badRequest } guard var myPet:Pet = try Pet.find(identifier) else { throw Abort.notFound } myPet.name = name myPet.age = age myPet.owner = owner myPet.lastVisit = lastVisit myPet.specie = specie myPet.picture = picture try myPet.save() return try JSON(node: Pet.all().makeNode()) } The first half of the Update route is pretty similar to Insert. The primary difference is that for updates, we use PUT method instead, and also send the id to identify which entry we will update. After checking all parameters, we'll try to fetch an existent Pet. If we do not find it, then we respond throwing Not Found. With the Pet object loaded, we simply need to update with new information, and again, invoke the save method. Finally, we respond again with all Pet entries. drop.delete(\"pet\", String.self) { request , identifier in guard var myPet:Pet = try Pet.find(identifier) else { throw Abort.notFound } try myPet.delete() return try JSON(node: Pet.all().makeNode()) } Our deletion route also uses the same endpoint, but with DELETE method. In this route we explore a new possibility. Instead of passing id as a parameter, we use a type safe routing parameter String.self , accessible via the second closure parameter - in this case, identifier . It makes our code cleaner, safer and avoids some validations, once this handler alone will be executed when a valid string arrives. To proceed with our deletion logic, we fetch the Pet with specified id using Pet.find(identifier) . With the object in hand, we only need to call delete() and it'll be gone! Our Pet app will demand two reading methods: One that will fetch all Pets, and other that will fetch specific Pets: drop.get(\"pet\") { request in return try JSON(node: Pet.all().makeNode()) } drop.get(\"pet\", String.self) { request , identifier in guard var myPet:Pet = try Pet.find(identifier) else { throw Abort.notFound } return try JSON(node: myPet.makeNode()) } The first route is a GET method that simply points to pet endpoint. As in previously presented examples, it will return load all Pets in Node format through Pet.all().makeNode() and return as JSON using the JSON(...) call . The second route, as the delete route, has a type safe routing parameter that will be accessed through the second closure parameter identifier . It tries to fetch the specific Pet through Pet.find(identifier) . If nothing is found, it throws Not Found. With the loaded Pet, we return its JSON representation in the end of the method. I can't believe it! Part 2 is ending!!! Don't be sad! Part 3 is coming! In this part of the Tutorial, we learned how to create our Model, do the basic operations over it, and also create simple and beautiful routes to the same endpoint that do operations according to the HTTP methods and parameters. It opens a myriad of options to be explored! In the next part we will learn how to build the page design to allow user interaction through the browser, using Leaf for templating and Skeleton to make page more beautiful. We also will modify our two GET routes to return pages. Thank you for reading and see you in Part 3!", "date": "2017-1-25"},
{"website": "Avenuecode", "title": "DevOps Practice", "author": ["Raj Thapar"], "link": "https://blog.avenuecode.com/devops-practice", "abstract": "DevOps has become the practice du jour among innovative IT departments. But what does it look like day to day? And how did it evolve over time? Below, I'll explore some tools and platforms that help enhance your DevOps practice. I came across the term \"DevOps\" about a couple of years ago when the client I work for started using Chef for deployment of applications. It was a significant step towards automating the deployment process. Chef allows you to write automation code in human-readable Ruby scripts. It standardizes the deployment process by policies. It can scale infrastructure quickly by just adding a new \"node\" to the Chef cluster and running Chef client on it. A \"node\" is a machine - physical, virtual, cloud - that is under management by Chef server. A question that came to my mind was what is DevOps, and how does automation by Chef falls under that description? Loosely defined by Wikipedia as \"a culture, movement or practice that emphasizes the collaboration and communication of both software developers and other information technology (IT) professionals while automating the process of software delivery\", DevOps seeks to create a process by which software can be quickly and effectively developed, tested, and released. Chef definitely allowed testing and releasing software by making the deployment process fast, reliable and repeatable. Although during the transition phase the scripts are written by Chef experts, over time developers can contribute by reviewing the scripts and even writing them. The scripts become a kind of contract between the developer infrastructure requirements and the infrastructure implementation. A lot has changed in the DevOps field over the years. Provisioning of infrastructure has always been a slow process, and hence a bottleneck for quick delivery of software. Virtual machines helped a lot in this regard by providing flexibility of running different operating system on the same physical resources. The companies do not have to go out and buy new physical machines every time a need arises, but can configure their existing resources instead. This cuts down the release and delivery time. With the emergence and mainstream acceptance of cloud, organizations can deliver software even faster. IaaS (Infrastructure as a Service) clouds offer virtual machines as a service. PaaS (Platform as a Service) offers a development environment to developers. SaaS (Software as a Service) offers applications as a service. Organizations have an option to use third-party cloud services on a pay-as-you-go cost or they can deploy cloud services in house (on-premise). It has become easy to setup a build/deploy pipeline using cloud services. Building software is as simple as provisioning a machine on cloud and running an CI (Continuous Integration) server like Jenkins on it. Deploying software is as simple as pushing the published artifacts from build step to the cloud. Spring Boot comes with embedded tomcat server so it cuts down the deployment time. The client I work for has started to use a separate build/deploy pipeline for individual projects. This cuts down the investment on proprietary CI servers and decouples the problems associated with using the centralized CI server. The pipeline can be set up within a few hours once the process is tried a couple of times. That is remarkable improvement over the existing practices. It requires collaboration of infrastructure team and developers - infrastructure manages and provides the resources that developers can use. It is correctly a good DevOps practice.", "date": "2016-10-5"},
{"website": "Avenuecode", "title": "Understanding the Basics of Neural Networks", "author": ["Anderson Alves de Sá"], "link": "https://blog.avenuecode.com/understanding-the-basics-of-neural-networks", "abstract": "For some years now, machine learning has been one of the most celebrated and explored areas of technological development. In particular, deep learning and neural networks have fascinating potential. To illustrate how neural networks function and to demonstrate their success, we'll explain a naive implementation of a simple neural network using Java. The goal of deep learning is, in an abstract way, to emulate some basic principles of the biology that makes up the human brain. Deep learning models are made of neural networks composed of nodes (which may be compared to the brain's neurons, which are simple processing structures) and the connections between nodes (which may be compared to the brain's synapses). While each neuron in the human brain can have about a trillion connections, artificial networks usually work with smaller quantities of nodes and connections. Even so, they have an impressive ability to process many different problems that range from natural language approximation to image recognition. Regression functions are at the core of neural networks. They are the math that helps neural networks \"learn\" through data training. For example, regression functions can help the network recognize a dog in a picture. These mathematical functions train the network to weight its parameters and move forward to a more precise classification of its training data until it is adjusted to be exposed to new data. While anyone interested in deep learning needs to know how these mathematical functions allow neural networks to classify and forecast data trends, the functions are complex and can be a barrier those exploring the field. Today, we'll examine a naive implementation of a simple neural network using Java to show how NNs work. Although neural networks are usually implemented with the mathematical model using matrices operations, we'll try to explain their basic functionality by describing their conceptual representations. The first class in the neural network model is the node, and it is analogous to a neuron. There are three components that constitute a node: the weights, the activation function, and the bias. Weights allow values to be adjusted through training sets so the neural network can learn and find patterns in new and unknown data. From: Wikipedia The activation function defines the output of the node given the inputs it receives, and the bias serves as a trainable constant value in the function. The image above shows a representation of the inputs through the activation function, where, if the inputs reach a certain threshold, they will be passed along to some extent to other nodes in the network. The second class in the neural network model is the layer. Each layer in a neural network is comprised of a set of nodes, making it a multi-layer neural network. The layers in a neural network serve three different purposes. First, there is the input layer, which is responsible for inputting data into the NN. Next, there is the output layer, which gives the results of data processed through the NN. Finally, there are hidden layers, which reside between the input and output layers. There can be any number of hidden layers in the NN, and adjusting the NN to include the optimal number of hidden layers and nodes is crucial when working with more complex data. As depicted below, the network itself is constituted of methods for training and testing: Without training, our neural network is as good as a random guess. Our test network has thousands of weights and biases that can be adjusted to improve its accuracy. By changing a single weight, the network can perform better or worse against a training data set. If we repeat this process for several iterations and keep only the improvements, the network will gradually improve as a whole. With this basic understanding of the components of this neural network, we can run some tests using the famous Iris dataset . The Iris dataset is comprised of three different classes, each of which refers to a type of Iris plant. Each line in the dataset contains an instance of a plant, with values corresponding to the characteristics of the specific plant. The test method is as follows: one test run yielded 99.4% accuracy for the training data and 95.1% for the test data set. (If you want to learn more, you can find the repository with the complete project here .) With the implementation of basic Neural Network concepts, it’s already possible to get good results from pattern recognition challenges, like the Iris data set, with great efficiency. Even though there is a lot of room for improvement within the field of deep learning, results like these make it clear why neural networks have garnered great attention within the broader field of machine learning and why we at Avenue Code are excited to continue pioneering machine learning possibilities. (This article was co-authored with Luis Felipe Talvik, Software Engineer at Avenue Code.)", "date": "2018-6-13"},
{"website": "Avenuecode", "title": "A Developer's Perspective On Angular 2+", "author": ["Mauricio Correa"], "link": "https://blog.avenuecode.com/angular-2-should-i-learn-it", "abstract": "Should I learn Angular 2+?  You may have asked yourself this question several times, but you've probably stayed on the safe side, and most likely stuck to your preferred framework or plain vanilla javascript and ES6. Now that the frameworks you’ve been hearing about have finally been released and are heavily used by the community, you may be seriously debating whether you should switch over to MEA(2+)N, React + Relay + GraphQL, or Vue.js. Choosing Your Path If you're looking for reasons to learn Angular 2+, this is the article for you. Although Angular may have its flaws, it can also be pretty flexible, which means you shouldn’t be afraid to take that leap towards new technology. If you’re new to Angular, there are three concepts you should be aware of before jumping into the rest of this article: If you're already familiar with these sections, you can go ahead and jump to Section 2. One-way and two-way data-binding: This relates to how your class (controller in Angular 1.x) communicates with your view (HTML template). If you want to monitor changes made to a given input field, you're probably going to use the ngModel attribute. The ngModel attribute is a directive that will track the value, user interaction, and validation status of the input control and keep the view in sync with the model. HTML Template and Typescript Class: Whenever the user types something into the input field, the value inside the ES6 Class will also be changed. This will only happen because we’re using the banana-in-a-box syntax [()], which will ship the change event in the input field alongside the value that has been typed. Providers: The typical way to traverse data between the Angular application and some external resource is by using providers - you’re going to be using something called service. First, you need to make it injectable since most classes need to be injected when programming using Angular 2, whether through the App module, or by using the regular ES6 module import. You might also want to use Angular’s implementation of XMLHttpRequests, the Http module, since it provides you with an ES6 Observable - but make sure you declare it in your constructor as well. This will return an Observable object to your class, which you can then subscribe to and manipulate in order to display proper information for the user. Declarations: Instead of creating controllers and element directives as you previously would have in Angular 1.x, you’re now going to use Components , along with directives and pipes (previously known as filters). In order to use a component, you must first decorate your class with a Typescript decorator. By decorating your ES6 class with a Typescript decorator, you’re telling it that it should have a selector , a template , and when applicable, a set of services . We can now use the service we’ve previously created to get user data: We’ve injected the PresidentService into the PresidentComponent by importing it into the file and then adding it to the decorator key array called providers . Next, all you need to do is create an instance of that service inside the class's constructor and then use the method that has been created. This method will return a response, that can be parsed as a json object through the .json() method. In this example, we assume our API is returning a json object with a key named ' users ', which contains an array of user strings. Now that you know how to handle Angular 2+, it's time to get to know some of the advantages and disadvantages of the framework. ES6 and Typescript: Angular 2+ takes advantage of ES6 and Typescript, which helps to increase the potential of writing clean and well-organized code. Typescript offers the option to assign types to your variables and functions. This allows coders to avoid mistakes while coding, especially when using a code linter that prevents the code from being compiled, or a powerful IDE that warns you of code errors prior to saving the file and triggering the compilation. This makes you more aware of what's going to be sent and injected to each function or class. You can also create your own Types and add their own constructors to them. T his will prevent errors, such as a variable being undefined once the page has been loaded, since the constructor is executed first. You'll also know exactly what property your object will have once you use it inside your code. It's not mandatory to use Typed variables all the time, or even type every single identifier in your code, but typing methods and variables will certainly make the lives of other developers easier. They let developers know at the first glance exactly what a method will return when executed. No More Digest Cycles: The digest cycle has been replaced, which means checking fields for changes doesn't work the same way as it did with Angular 1.x. Angular 2+ has introduced a module called ChangeDetector which uses reactive programming to monitor changes when application state changes are perceived by Angular ChangeDetector. Each component has its own ChangeDetector, which makes the process of checking DOM elements for changes less resource-consuming. The change detection is performed from top to bottom - the same way the data flows - and all components are linked through the use of a tree. Changes can be marked so that change verification occurs only when it has to be executed by using the ChangeDetector ref, thus making the Change Detection process look something like this: Image Source: Thoughtram Angular CLI Generator: The one tool that makes it all easier to develop using Angular2+ is the Angular CLI generator. It's an ember.js fork and is made to automate the generation of Angular 2 Components, Services, Directives, and Pipes. Angular CLI directs the programmer towards a more gradual learning curve, especially when it comes to transpiling code and creating dynamic stylesheets, since it does not force the programmer to learn how to work with Webpack's loaders. Another great advantage of using Angular CLI is the possibility of creating an automated, streamlined folder structure when dealing with angular modules. Code stubs will already be created with a single command, thus making code easier to work with without the need to know Angular2+/Typescript syntax by heart. You can create an angular component simply by typing: ng is the keyword used to execute any Angular CLI related command g is shorthand for generate c is shorthand for component Angular CLI will take care of creating the folder with the necessary files and link the component to the App module, the Angular 2 main module file where all direct dependencies are declared, alongside the spec file for unit testing, which can be run via command line through Karma. Unit tests are automatically configured, and their initial stubs can also be found inside the aforementioned folder. This spec file can then be augmented  and tests can also be run with Angular CLI: Isolated Component Scope Angular components have an isolated scope, and data inside its classes are enclosed and visible only to its template. In order to access data from other components, you'll need to define an Input Output flow or inherit properties and methods through class inheritance. The example above shows both the PoliticianComponent and Politician model being inherited by the SenatorComponent and Senator model. Derived classes have access to default and protected methods of their super classes, and if methods and properties aren't overidden, they will be passed along to the derived class. Creating specific CSS rules for components is going to be much less verbose since the generated CSS gets transformed into a code block, and gets inserted into the head section of your rendered HTML once the page loads. The component CSS will then be scoped through the use of dynamically generated attributes whenever the page loads. This is done automatically whenever a class is decorated with Component, but can also be disabled if you think the generated HTML is too cluttered. Communication Through @Input @Output: The main method for sharing reactive data between components in Angular 2+ is through the use of the @Input and @Output modules. They provide a mechanism that allows any form of data to be passed around components. In order to pass data to child components, you must use the @Input module and declare the object both on the child component tag inside the parent component template, and inside the component.ts of the child component. In the example above, we can see that the property \"hobbies\" that is being passed to the child component via @Input, is of Array<string> type. It will then be passed along to the child components, and any changes made to it inside the parent component will also be reflected in the child component. On the other hand, in order to propagate changes from the child component property to its parent component, it is necessary to import the @Output module, which is usually used to create an EventEmitter that will notify the parent component of any changes made to that specific listener. The parent component must then have a handler attached to that EventEmitter in order to be able to update corresponding values inside the parent component. This can be especially useful for sending onChange events to the parent components. As mentioned earlier in Section 1, the banana-in-a-box syntax will deliver both the value and the corresponding generated event of a given HTML element when used. When using an input, the ngModel will bind the value that has been typed to the property, but not the change event itself. This means that data will only be sent from the data source to the view target once, and typing into the input field will not yield changes to the model. In this case, aside from updating the model, we want the model change event to be emitted back to its parent component. In order to do this, we need to use the ngModelChange event property. Whenever something is typed into the input field, the ngModelChange event will be triggered and emit the NewHobbyType. Then, all that has to be done is use the previously created newHobbyTyped @Output property to emit the change event into the input field and parent component. Easy Filtering With Pipes: Filtering in Angular 2+ is just as easy as it was in Angular 1.x. Since you will mostly be using them on your template to format data, filters are now called pipes. You can always build your own filters easily, but Angular 2 core comes with built in filters such as json i18nPlural. The json pipe will convert values using JSON.stringify. This is useful for debugging while developing, and allows you to make requests to a certain endpoint while displaying data the way it is delivered from the response object. Content rendered after adding the json pipe: In order to build your own filter, you need to create a pipe.  Angular CLI provides an easy way of creating Pipes inside your project folder structure: This will generate a .pipe.ts file in which you can then modify its transform method into something you might want to present in the HTML template. You can then use the recently created Pipe on a given expression. In this case, we've bound the return value of the Pipe to the innerHTML attribute since .bold() generates an HTML element. Using double curly braces to transclude data into the markup would result in rendering the string itself. Debugging: Debugging an Angular 2+ application has been made easy through the use of Angular CLI. It automatically generates source maps for the ts files, which means a folder with the source code will be available when using it in development mode. That way, it's possible to add breakpoints and inspect code without having to fiddle with transpiled javascript files. Folder structure created by webpack showing source code for the entire application. As shown in the picture above, Angular CLI uses webpack under the hood to create the appropriate loaders in order to provide source maps for the source files. When debugging, it's possible to watch typed variables and interact with class instances to inspect property values, as with plain javascript. Bonus - Augury, A Useful Tool For Real-Time Debugging: Augury is a really useful tool that comes in handy when programming an Angular 2+ application. It's a Chrome Extension that allows components and values to be inspected and modified on the fly. It also verifies useful information, such as the ChangeDetection methods, for each component and the module injections that are handled through the app. In the picture above, it's possible to see how we've examined the current values of all properties of a given component as well as its dependencies. In the picture above, we can see the Injector Graph, how the component tree has been generated, and its providers. In this case, the PresidentComponent needs the PresidentService to be injected into it in order for it to have access to its methods and properties. In this case, we can see that the SenatorHobbiesComponent is a child component of the SenatorComponent, and its creation is triggered by the SenatorComponent . Also, if we're using Angular Router , we'd also be able to inspect the Router Tree to check all available routes. In this case, we have a root outlet for first level routes and two more levels of child routes. As you can see, even the route object can be checked. If you want to fiddle with the extension right away and get real inputs, you can follow this link: https://augury.angular.io/demo/ Conclusion That's about it! If after reading this article, you feel that you've learned something about Angular 2+, then it's served its purpose. If you'd like to improve your Angular 2+ skills, the Official Style Guide is a great way to start - link below. It contains very detailed, instructive tutorials as well as very thorough documentation. Here are some other links for you to delve into: Don't hesitate to try new things and keep exploring. The best way to learn is by putting your hands on actual code. Good Luck!", "date": "2017-10-5"},
{"website": "Avenuecode", "title": "Why Metrics Matter in Product Design", "author": ["Jessica Toyota"], "link": "https://blog.avenuecode.com/why-metrics-matter-in-product-design", "abstract": "What makes a successful product design, and at what stage should metrics be defined? Let's dive into why, when, and how product metrics should be set. The success of a new feature or product depends on a complex combination of multiple factors, and achieving the expected outcome is never a guarantee. Even when we are confident that we delivered the best design work, we can't consider it a success if our team doesn't have clear metrics to assess the results of a project. Of course, the need to measure results is not a new idea, but designers frequently overlook how crucial it is to define those metrics at the beginning of any job. More than a tool to evaluate results at the end of a project, success metrics guide the focus of research efforts, increase the speed of decision making, and help the team stay aligned. One common pitfall of agile teams is to come up with creative solutions that are innovative and exciting but do not address a real need or respond to a real problem. For example, we might have a great design for a new onboarding flow for a music app. When we spend time thinking about how we will measure the success of this new feature, we are forced to make a connection between what we are implementing and the business goals, such as acquiring new users, converting users from the free plan to a premium plan, or reducing churn. We are also forced to face the fact that the change we are considering might not affect the business goals in a significant way. Recognizing that disconnect allows for an early course correction. Once the team establishes that a new feature is worthwhile, designers are free to start exploring possible solutions, different user experiences, and potential ways to support user goals. When choosing the best way forward between multiple options, we want subjectivity out of the way as much as possible. This process is faster when the whole team uses the same criterion: which solution will more effectively affect the success metrics selected for the project. In our previous example of the new onboarding flow, we might create a simple onboarding flow to make it easier for new users to sign up, or we might include more steps to gather information on the user's music taste to provide better recommendations and increase usage. After the project, having a set of success metrics is necessary to determine if the investment on the project was worth it, to iterate on the design, or to go in a completely different direction. In the end, design is about making intentional decisions, and success metrics can be a valuable input to guide those decisions, if they are properly selected. Design success metrics can also be useful as predictors of more strategic business metrics. The most common business indicators are \"lagging indicators,\" or indicators that measure results after the fact. For example, a team will know if they hit their quarterly sales goal at the end of the quarter. Before that, they can measure \"leading indicators,\" such as reduction in cart abandonment, or percentage of returning customers. One of the main reasons why not all teams use metrics to evaluate projects more consistently is the fact that selecting the right set of metrics requires some thoughtful consideration. Let's imagine the re-design of the payment flow for an e-commerce app motivated by the business goal of increasing sales. The first thing to keep in mind is that some subjective aspects, for example \"ease-of-use,\" can still be evaluated with objective measurements like \"time to complete an order.\" The same team might want to evaluate how the new design for the payment flow communicates trust, something hard to quantify. In those cases, the appropriate measure might be subjective, such as user feedback. The lack of hard numbers is not a reason not to measure success. Another consideration when selecting a metric is finding an indicator for which you already have a baseline to compare: measuring the \"percentage of users who successfully complete a task\" is only helpful if this data was already being captured before the launch of a new design. If defining success metrics is not part of the culture and the design process in your team or organization, you can get started by familiarizing yourself with the strategic business goals of your organization and how they are being measured; investigating which product metrics are already available to you (click-through rates, Net Promoter Score, conversion); determining if any of those metrics are appropriate to measure the success of the design you are working on, and if not, if they can be reliable proxies; and then starting a conversation with your stakeholders. Everyone should care about success metrics, and there's no one better to lead this process than product designers! How do you measure success for product design at your organization? Leave a comment and let me know!", "date": "2021-3-3"},
{"website": "Avenuecode", "title": "How to Pass the OCP Java SE 11 Certification", "author": ["Rafael Romão"], "link": "https://blog.avenuecode.com/how-to-pass-the-ocp-java-se-11-certification", "abstract": "I've been asked to share some recommendations on how to pass the Java SE 11 Certification , so I decided to write this article to pass along the tips that helped me. In the beginning of 2019, I decided to make a big change in my career and migrate to Java after using .NET for more than 15 years. The reason was simple: .NET and C# are great, even more so after .NET Core 3, but market opportunities are still scarce when compared to Java, so I came to the conclusion that the best move for me at the time was to move on. My goal was ambitious: to be as good in Java and its ecosystem as I was at the time in C# and .NET by the end of 2020. With that goal in mind, I prepared a learning path that would end in two certifications: OCP Java SE 11 Developer and Pivotal Spring Framework 5. To accomplish this goal, I had to study and practice a lot, and here are the tips I can give you based on my experience: Thanks for reading, and best of luck! This post was updated on October 13th to reflect the changes in the Oracle exams. *This content was originally posted on Medium .", "date": "2020-10-21"},
{"website": "Avenuecode", "title": "Object Creation in JavaScript", "author": ["Rafael Bicalho"], "link": "https://blog.avenuecode.com/object-creation-in-javascript", "abstract": "In our last article, we talked about Prototype Inheritance in Javascript . Today, we’ll take a look at the different ways to create objects in JavaScript and the resulting prototype chain for each method. Almost all objects in Javascript are inherited from Object , which sits on top of the prototype chain. The prototype of Object.prototype is null: The simplest way to create an object is to use the object literal notation. The created object has Object.prototype as its prototype. The prototype of Object.prototype is null , so the prototype chain is: Sometimes you'll want to create multiple objects with the same properties. In JavaScript, constructor functions are functions used to create objects when called with the new keyword. The first letter of a constructor function is capitalized by convention. So, if we create a constructor called Person , the prototype of the objects created will be Person.prototype . The resulting prototype chain is: Because the previous structure looks so similar to classes in other languages, people expected it to behave like real classes, and that was not the case. So in an attempt to make object creation more intuitive in Javascript, ECMAScript 5 introduced Object.create , a static method that creates a new object from an existing object. The prototype of this object will be the first argument passed to the method. The resulting prototype chain is: As an option for using constructor functions, ES2015 introduced a cleaner and more convenient syntax by adding the class keyword (along with others like: super, extends, and constructor), but only as a syntactical sugar. JavaScript remains a prototype-based language and does not provide a class implementation of class-based languages like Java or C++. Here again, the prototype chain is: If you want to dive deeper into Javascript object creation, I highly recommend reading all about new operator , classes , and Object.create() . Happy coding!", "date": "2019-3-13"},
{"website": "Avenuecode", "title": "Spring WebFlux: A Quick Start", "author": ["Leonardo Silva"], "link": "https://blog.avenuecode.com/spring-webflux-a-quick-start", "abstract": "A common problem with the traditional blocking API style is what we call backpressure. Simply put, backpressure is when clients or consumers overwhelm the server's capacity to process requests. This creates the need for efficient hardware and software scaling. Fortunately, Spring WebFlux offers a solution. We always want to improve our servers' capacity to process requests, preferably without additional hardware costs. Programming reactively and asynchronously allows us to improve our API's ability to scale vertically with no need for additional nodes in the cluster. As of September 2017, Spring 5.0 went GA, and one of its most exciting new features is its support for building reactive APIs through the new spring-webflux module. Spring WebFlux provides us with the resources to build a reactive API in a declarative rather than imperative manner. Its two main programming models can be classified into annotated controllers - based on the same annotations from the spring-web module - and functional endpoints, which use lambda expressions and functional style. Today we'll walk through the steps of building a simple microservice to illustrate the main features of the spring-webflux module using functional endpoints, followed by integration tests to validate the Restful API. Make sure you have Java version 1.8 or later. We are going to use MongoDB as our database because Spring provides an out-of-the-box, reactive MongoDB repository implementation for us to use. You can get MongoDB using brew with a simple command: $ brew install mongodb To start up the server, simply type: $ mongod Next, let's set up our database connection. Fortunately, doing so is as simple as adding the following line to your application.properties file. Now we need to create a POJO to represent the entity we want to save. We are going to use a simple Book.java class as our model. Our DB repository is implemented the same way a traditional JPA repository is. We just need to create an interface that extends the Spring repository implementation. For our purposes, we will be using the ReactiveMongoRepository interface, which already contains all the features we need to make our API fully reactive. Now let's start developing our REST controller. Our controller contains 5 endpoints. We will walk through each one of these to illustrate how reactive concepts are implemented. This endpoint adds an entry to our MongoDB database. Notice that we are returning a Mono object that wraps the entity that was created. If you look into the Mono class definition, you will see that it extends a class called Publisher : a Publisher is a reactive stream that may send data to one or more subscribers. The subscribers may use the information as it becomes available--that is, they react as the publisher provides information. This endpoint is responsible for returning a Publisher that hands over all the book instances we have created in our DB. The main difference between this endpoint and the previous one regarding reactivity is the Publisher implementation we return. We have used a Mono to return the single instance we have just saved, and now we are using a Flux to get all the instances we have created until this moment. That's the main difference between a Flux and a Mono : a Mono can omit 0 to 1 instances of the parameterized item, while a Flux can omit 0 to n instances. We now query our books collection to return a single item specified by its primary key: the ISBN. Notice that, in this endpoint, we are wrapping a ResponseEntity object into a Mono object. We are using a Mono publisher because we want to return a single instance, and we are using the ResponseEntity object to enable us to return HTTP status 404 in case the book doesn't exist in our application domain. This endpoint is also useful to show how WebFlux is compatible with a traditional HTTP implementation. For this endpoint we do something slightly different from the GET /books endpoint: we add a delay time between each item to simulate processing time. This is done through the method call delayElements(Duration.ofMillis(1000)) . The publisher will wait for 1 second before addressing each item in the stream. Notice that the media type we produce is also different. We are producing an application/stream+json media instead of a simple JSON. That is useful for telling our client that we are returning a stream instead of a regular response entity. The traditional web client implementation for consuming REST APIs in Spring is the RestTemplate class. With the new reactive framework, we also get a reactive REST client implementation: the WebClient class. For writing our integration tests, we are going to auto-inject the implementation of WebClient for testing purposes, which is WebTestClient . Its fluent interface allows us to do method chaining and assert the results in a more declarative way. Observe that all the pre- and post-test operations in the DB are using the block() method to avoid synchronization problems. Let's explore the /booksWithDelay endpoint with curl. First, we need to build our API. From inside the project directory, execute: $ gradle build When the previous command is finished, start up the web server with: $ gradle bootRun Check in the console to see when the application is done starting. After it's done, we can start sending requests to our endpoints. Let's create two books in the DB: $ curl -X POST http://localhost:8080/books -H 'content-type: application/json' -d '{ \"isbn\" : \"978-0451518651\", \"title\" : \"1984\", \"publicationDate\" : \"1949-06-08\"}' $ curl -X POST http :// localhost : 8080 / books -H 'content-type: application/json' -d '{\"isbn\" : \" ‎0-330-25864-8\", \"title\" : \"The Hitchhiker'\\''s Guide to the Galaxy\", \"publicationDate\" : \"1979-10-12\"}' Now, let's retrieve them by using the GET /booksWithDelay : $ curl -X GET http://localhost:8080/booksWithDelay {\"isbn\":\"978-0451518651\",\"title\":\"1984\",\"publicationDate\":\"1949-06-08\"} {\"isbn\":\" ‎0-330-25864-8\",\"title\":\"The Hitchhiker's Guide to the Galaxy\",\"publicationDate\":\"1979-10-12\"} You should notice that the books are being provided with a 1s delay between each other. That's because we added that delay on purpose to illustrate the reactive concept. We could use subscribers to this publisher to process information as it becomes available rather than having to wait for all items to become available. Now that we've learned a little more about reactive programming with Spring, it's important to make a couple closing remarks. If you try to debug a functional reactive API, you are going to find out that debugging is much harder when you use functional style. That's something to account for, depending on how complex your application is. Also, keep in mind that if you already use Spring MVC, there's no need to completely switch to WebFlux. Both styles work together without any problems, so you can build reactive controllers that coexist with your MVC controllers. I hope you have enjoyed learning something new! WebFlux Framework, Part 5. The Web Web on Reactive Stack Spring Framework Guru", "date": "2018-10-24"},
{"website": "Avenuecode", "title": "How to Prevent Memory Leaks", "author": ["Rodrigo Novaes"], "link": "https://blog.avenuecode.com/how-to-prevent-memory-leaks", "abstract": "Resource management is one of the most challenging tasks involved in software development. Even with out-of-the-box solutions like garbage collection (GC), there are still several ways to cause defects through poor resource allocation/deallocation. In memory handling, virtual machines and operating systems have evolved to aid developers in writing safer programs, avoiding leaks and data corruption. But what other options do we have when we can't hire help from an assistant? This article presents a mental experiment that gives the reader a different approach to memory management without using out-of-the-box features. Keywords: C++. Smart Pointers. RAII. Memory Management. Resource Acquisition. I want to propose a mental experiment: Imagine yourself as a parent in a house . A few years ago, you moved to the OS country, met someone with whom you fell in love, and then decided to build a life together. In addition to managing your house chores, you are a very renowned software engineer whose love for his/her work has reached your children: you have two beautiful kids named Stack and Heap. Stack and Heap are similar in several ways. For instance, they both enjoy playing with the same toys, have the same taste in TV shows and comic books, and are avid fans of One Piece. However, they also have some important differences: while Heap is very adaptive and eager to learn new things, Stack can be quite inflexible and short-tempered. On the other hand, Stack is very obedient and organized, while Heap can't even get her toys without being ordered to. This can cause a lot of trouble sometimes, like last week when you tripped on a Spiderman action figure and almost fell to the ground. Now, a house requires a lot of effort to work properly. You and your partner have divided all tasks efficiently, but regardless of this, managing Stack and Heap causes an overload in your daily activities. It would be easier if Heap was more self-managed, just like Stack, who always knows where his toys are. In fact, his organization skills are so impressive that he also knows where Heap keeps her toys. This may have something to do with the fact that Heap is short and depends on Stack to grab toys in the wardrobe for her. Anyway, the problem starts when Stack loses track of a toy that Heap is playing with. Now he isn't able to get the toys together and has to depend on his parents to organize the house. The problem intensifies when you, the parent, also forget to order Heap to get her toys! Now you may inadvertently trip on any number of action figures in the future. Over the years, you've made some attempts to help Heap become more organized. You tried to hire an assistant named GC, who aided you in house chores and kept an eye on Heap. However, GC has a very tight schedule: the house has several tasks to be fulfilled, like cooking, washing, cleaning, paying bills, and more. Imagine the time loss GC will have when interrupting all these activities in order to collect Heap's stuff around the house. Moreover, it's quite complicated to determine when GC will have the opportunity to interrupt all the tasks and clean up after Heap. It can happen now or later; you never know. Well, certainly many houses have their assistants, and they have grown smarter and more efficient over time: they can do more than one task at once and still collect the unorganized children's toys, but it doesn't diminish the cost of doing it. Trust me: hiring GC is still expensive, even though it's practical. Time passes, and your household problems have taken their toll. You often trip on things that Heap leaves behind, like that one time when you stumbled and fell on your TV, smashing it to pieces. You concluded that having Heap play freely is dangerous, and you decided to visit a child therapist. Dr. Boost Stroustrup, PhD., suggested you take advantage of Stack's organization skills. Since Heap can't reach her toys in the wardrobe, meaning that someone always has to pick them up for her, you could ask Stack to keep track of Heap's steps while they play. Therefore, whenever Heap leaves a toy behind, Stack will always make sure to bring it back to the wardrobe, freeing the unwanted items from the floor of your house. You decide to give Dr. Stroustrup's suggestions a try, and they work wonders. Stack and Heap love playing together, and since Stack knows each one of Heap's steps, your house is as clean as possible! Also, your kids have a bond that's way stronger than before, since Stack is a lot more  involved in Heap's life. You and your partner are now looking forward to seeing how your children will grow. Will your family move to a better place, enjoying the fact that the interactions between Stack and Heap have improved your conditions? Will you have a well-deserved holiday trip to Nlogonia now that your bank account is fatter without GC's costs and expenses? There are plenty of options, but of course, they depend on your opportunities. Analogies are an art and they serve a purpose: helping us to understand a concept. In this analogy, the house was a computer process, the children were the Stack and Heap memory areas, and the parent was you, the programmer. Other concepts were explored as relevant pieces of the story, such as Garbage Collector (GC), a program responsible for collecting heap-allocated areas, and Resource Acquisition is Initialization (RAII), a concept that assigns the ownership of a heap-allocated object to a stack-allocated object [1][2]. In this article, we will explore the sibling approach: Stack playing together with Heap to achieve resource-safety [2]. This is an instance of the RAII idiom [2], which has been standardized in C++'s libraries since C++11 and implemented under the well-known name of smart pointers [3]. After reading this article, it's expected that the reader will: The author also recommends that you read about GC, a solution embedded into Java Virtual Machines (JVMs) for dynamic memory management [4]. There are clear and well-written articles referenced in this blog that will help you [5][6]. C++ has been in the market for a while. Unlike some languages, like Java, every resource allocated in a C/C++ program needs to be returned to the operating system (OS) manually [4]. This was considered a tedious operation, since every unreleased resource is considered a leak and leaks can lead to critical problems [1]. Some examples of these are [2]: However, C/C++ has a powerful feature that allows one big enhancement in memory management: static object allocation [1][4]. Unlike Java, whose JVM allocates every object in the Heap, hence using implicit dynamic object allocation, C++ allows the programmer to create objects in the Stack. Also, Stroustrup's language has operators dedicated to an object's allocation and deallocation. The first operator is well-known across object-oriented (OO) programming languages, often referred to as constructor . The second operator is a destructor and is called whenever an object is freed in the Heap memory area. The code snippet in Listing 1 shows an example using C++. Listing 1 – An implementation of an integer wrapper class. The resource acquisition is done whenever an i nteger object is created in Stack (refer to the main function). When main 's scope is finished after the return statement, all Stack objects are collected automatically. Hence the i nteger 's destructor is called, deallocating its Heap-allocated member. As stated in Listing 1, the key concept for understanding RAII is resource ownership . If a class C has one or more members whose type is a reference (a.k.a. pointers), a static instance of C will be automatically collected from the Stack by calling C 's destructor, meaning that all of its dynamic-allocated members can be freed as long as their respective destructors are properly invoked [1][2]. In the analogy at the beginning of this article, Stack and Heap's toys are resources , or in other words, objects . When you applied the suggestion of asking for Stack's help, you actually made Stack own Heap's resources. Since Stack owns his and Heap's resources, he will always know when and how he should collect his and his sister's toys. However, as a programmer, you still need to tell a class C how its objects should be destructed – especially those Heap-allocated member objects – and that does not provide the same comfort as hiring GC. That's why we should take a look at the following section. If you have come to terms with resource ownership , then you're more than ready to learn about smart pointers . In Listing 1, you saw what would be a small reference to a Java's integer wrapper class, which is merely a container for an integer value stored in the heap. In C++, heap-allocated objects need to be explicitly deallocated by the program, meaning that the responsibility lies on you: the programmer. However, we can make use of RAII to delegate this responsibility to Stack-allocated objects instead. Prior to C++11, the Boost C++ libraries distributed headers containing classes we call smart pointers , which are nothing more than the same integer wrapper class from Listing 1 enhanced by the template design pattern [7][8]. From C++11 onward, the C++ standard library implemented its own version of smart pointers , which has been maintained and has evolved since then as part of the <memory> header [3]. Please consider Listing 2 as an enhancement of Listing 1, where we now use a smart pointer as the integer resource's owner. Listing 2 – An implementation of an integer wrapper class. The resource acquisition is done by the smart pointer unique_ptr inside i nteger . Since all objects are allocated in Stack, any dynamic allocation will be automatically released by the implicit call to the objects' destructors. In other words, i nteger 's destructor will be implicitly called, as well as unique_ptr 's. Hence, the Heap-allocated value will be freed after main 's termination. Some might say that the perfect design of an owner is of one that does not know anything about the resource it owns [2]. In this context, it might seem that our analogy fails: Stack is Heap's brother, so it's more than likely that he'll know about his sister's toys. However, imagine that Stack and Heap will grow into two marvelous teenagers and, at some point, Heap will get into the habit of logging her daily events into a diary. Stack's help will still be necessary to grab Heap's diary at the wardrobe (resource acquisition), but he's so obedient and polite that he'll never look into the contents. Stack respects Heap's privacy as much as smart pointers respect their resources' encapsulation. From what was observed, it's reasonable to affirm that garbage collection is not required to achieve high-level memory management. RAII is a feature that avoids all of the main leaks stated at the beginning of this article, with the benefit of adding low overload to objects' allocation and deallocation [2]. Moreover, having garbage collection doesn't imply full resource deallocation, since resources can be collected from several sources besides memory, like streams and sockets. Even Java, with JVM's out-of-the-box memory management, requires a closer look at how programmers handle their resources [9]. With RAII, however, it's possible to delegate any kind of deallocation to stack-allocated objects, meaning that destructors can close streams, sockets, and perhaps even Heap's wardrobe. In this article, we explored the concepts involving RAII using an analogy with household chores. All implementations were done using C++11, whose smart pointers implementations were retrieved directly from the memory header. In addition, the smart pointer implementation used was a unique_ptr , whose features are [3]: There are other types of smart pointers , though. They are shared_ptr , weak_ptr and auto_ptr (which has been deprecated since C++14 and removed as of C++17). Each one has its own features and appropriate use cases; I recommend this as further reading [3]. I hope that this article offers helpful reflections for you. It might be said that there are several ways in which programming is similar to parenthood; you have to educate your program to be humble, smart, effective, and efficient. Even though it fails sometimes, it's important to never give up on it and to never stop doing what you can so that it'll turn into something great in the future. The curious thing is that I don't have any children. If I did, I'd make sure to introduce them to One Piece. [1] MICROSOFT. Object lifetime and resource management (RAII). Microsoft Documentations, Nov. 19th 2019. Available on: < https://docs.microsoft.com/en-us/cpp/cpp/object-lifetime-and-resource-management-modern-cpp?view=msvc-160 >. Last accessed: Nov. 26th 2020. [2] B. Stroustrup, H. Sutter, G. dos Reis. A brief introduction to C++’s model for type and resource-safety. Dec. 2015. Available on: < https://www.stroustrup.com/resource-model.pdf >. Last accessed: Nov. 26th 2020. [3] CPP Reference. Standard library header <memory>. Apr. 1st 2020. Available on: < https://en.cppreference.com/w/cpp/header/memory >. Last accessed: Nov. 26th 2020. [4] Oracle. Java Garbage Collection Basics. Available on: < https://www.oracle.com/webfolder/technetwork/tutorials/obe/java/gc01/index.html >. Last accessed: Nov 26th 2020. [5] R. Silva. How Java Garbage Collection Works and Why You Should Care - Part I. Aug. 12th 2020. Available on: < https://blog.avenuecode.com/how-java-garbage-collection-works-and-why-you-should-care-part-1 >. Last accessed: Nov 26th 2020. [6] R. Silva. How Java Garbage Collection Works and Why You Should Care - Part II. Available on: < https://blog.avenuecode.com/how-java-garbage-collection-works-and-why-you-should-care-part-2 >. Last accessed: Nov 26th 2020. [7] Boost C++ Libraries. Smart Pointers. 2002. Available on: < https://www.boost.org/doc/libs/1_63_0/libs/smart_ptr/smart_ptr.htm >. Last accessed: Nov 27th 2020. [8] C. Caballero. Design Patterns: Template Method. Sep. 24th 2019. Available on: < https://medium.com/better-programming/design-patterns-template-method-5400dde7bb72 >. Last accessed: Nov 27th 2020. [9] Baeldung. Understanding Memory Leaks in Java. Feb. 12th 2020. Available on: < https://www.baeldung.com/java-memory-leaks >. Last accessed: Nov 27th 2020.", "date": "2021-1-13"},
{"website": "Avenuecode", "title": "How Kafka Enables Real-Time Stream Processing - Part 2", "author": ["André Melo"], "link": "https://blog.avenuecode.com/how-kafka-enables-real-time-stream-processing-part-2", "abstract": "Welcome to the second part of our Snippets series on real-time stream processing using Apache Kafka! To recap, in part one we introduced stream processing and discussed some of the challenges involved, like the stateful nature of aggregations and joins. We’ll discuss the impact of such stateful operations in a while, but first, let’s delve deeper into one key aspect of real-time stream processing: performance. Of course, throughput will probably be the first thing that comes to mind when you think about real-time stream processing. As in other distributed systems, the best way to achieve this is by horizontally scaling stream processing to a large number of nodes. In Kafka, that’s what partitions are for: streams can be split into partitions (think shards in databases), and each record will be assigned to a partition based on a user-defined key. Behind the scenes, Kafka will distribute those partitions to brokers inside the Kafka cluster. That takes care of scaling the backend, but one key thing to note is that Kafka Streams applications do not run inside the Kafka cluster; they just leverage default APIs to read and write to Kafka in very clever ways. Fortunately, scaling your stream processing application is easy: just run another instance of your application, and it’ll automatically join the taskforce to process incoming data. At any point in time, you can have as many active instances as there are partitions in the input streams your application is processing--and that’s one of the reasons why figuring out how many partitions to use is so important. But let’s back up a bit. Why is it so easy to scale the stream processing application? Answering this question will give us more insight into how Kafka Streams work. The thing is, Kafka keeps track of all partitions in each input stream used by your application. As soon as an instance joins or leaves the processing efforts, Kafka will redistribute those partitions to the remaining instances. The default policy allocates one partition to a single instance/thread of your application. Kafka Streams will leverage threads for parallelism as well. And since we’re talking about exclusive access to a partition, the bulk of the work will be done coordination-free. The best kind of concurrency is none at all ;). Finally, partitions naturally help with application availability since we won’t have a single point of failure, which brings us to the next problem to address in a real-time data pipeline. If you think about it, delays due to failures mean, from a business perspective, that we’re not achieving real-time stream processing at all. Fault tolerance is equally important, if not more important, due to the costs associated with diagnosis and recovery. In the previous section, we went over partitions and mentioned how they can help with fault tolerance. That isn't enough, however: we can lose access to a considerable amount of data if a partition goes down. That's why Kafka also allows partitions to be replicated for the sole purpose of having a fallback (i.e. you can't read or write directly to them). If Kafka detects a partition leader is down, it’ll pick one in-sync replica and promote it to leader. Do keep in mind that, by default, a partition leader will not wait for replicas to acknowledge writes , so there’ll be data loss if it fails after locally committing a write but before background synchronization happens. To enable durable writes, one has to force waiting for acknowledgments from a certain number of replicas before the write is considered done. That will, however, increase write latency. Moving to a broader level, when it comes to High Availability, Kafka ships with MirrorMaker, an open source tool for replication. If an entire Kafka Cluster goes down, you can use a fallback cluster in another region that MirrorMaker kept in sync. There are also enterprise solutions available that focus on active-active replication, meaning geographically distributed clusters are also available for writes. In this scenario, one could also distribute the streaming application itself and expect low latency around the globe since it could use the nearby cluster for writes. Kafka has fault tolerance, but if our application goes down, how would that affect the real-time SLA? Leaving fallback clusters aside, storage is extremely important to tackle this problem. But why would storage be such a big deal in real-time stream processing? Well, do you recall from the first part how we have stateful operations? And not only can failures happen, but the application is elastic, so instances can also come and go out of nowhere? Those two together mean that if for whatever reason we need to migrate processing from one instance to another, we lose precious time already spent performing joins and aggregations (which, by the way, are usually the most expensive operations in Kafka Streams). One way to solve this would be to store in-progress, stateful operations in centralized, durable storage. That way, if some other instance of our application needs to take over some aggregation, it can use the central storage to start where the previous one left off. Luckily, Kafka provides the durable storage we want. First, Kafka is able to scale no matter how much data you store on it. Also, as we’ve just mentioned, writes can be made durable by forcing producers to await for replica acknowledgment. That, together with the fact that we can read data from any point in a topic, means Kafka provides the durability and flexibility we need. For performance reasons, Kafka Streams also maintains local storage (by default the file system) that synchronizes changes to an internal topic our application will create. Keep in mind that Kafka, not the local storage, is the source of truth for stateful operations. If a single instance or even the entire stream application goes down catastrophically, losing all the local data, recovery instances can replay the internal topic to reconstruct their local state and resume processing. Recovering local state can take some time depending on the application, so once again our real-time SLA would be affected. To fix this, you can configure local storage replication so that if processing fails at one node, another one replicating its local storage can assume it. We still have to monitor and manage application uptime, but notice how the hard problem of performatively dealing with shared mutable state is solved by Kafka. Recovering from Bugs This is important to discuss, given the fact that topics are append-only. If a buggy application writes an incorrect record, it's impossible to delete it, so how can we fix this? Let's use an example: Suppose we have an application aggregating some data to compute physical inventory quantity. At some point, an unanticipated edge case causes us to produce an impossible negative value. We can't delete that value, but remember our brief discussion of Kafka topics in the first part and how consumers have full control of their offsets? We can recover by fixing the calculation in our application and then making it move input topic offsets back to reprocess records. The next time our application picks the values that caused negative quantities, it'll produce the correct output. An elegant fix, but there's more to it: we need to clear local storage and we need to move to the end of every intermediate topic Kafka creates. That way, when we reprocess our input, the correct values will come after the invalid ones. Seems like a lot, no? Luckily, Kafka ships with a tool that makes things easier . It'll take care of pretty much everything, except removing local state and making sure downstream applications outside Kafka can deal with the temporary, invalid values. That last one is really up to us since whether we need to fix those applications or not will depend on their semantics. Now that we’ve covered fault tolerance and recovery, it makes sense to talk about correctness. It turns out it’s not as easy as expected, and we’ll see why in the next couple sections. No discussion about delivery and ordering guarantees is complete without this disclaimer : If you’re lucky, your streaming application will be okay with duplicated records, in which case the default at-least-once delivery guarantee Kafka provides will work just fine. However, there are a bunch of common aggregations that will produce incorrect values in the face of duplicated records, like computing counts, sums, averages and so on. In that case, it's best to switch to exactly-once-delivery, but some care must be taken. First, there’s a performance degradation associated with the stronger delivery guarantee; it should be slight, but it’s important to test the impact in your real-time data pipeline. Second, exactly-once-delivery requires coordination between all readers and writers involved, even external databases. The best way to achieve that is to make sure all processing happens inside Kafka, from the moment you read your inputs in the stream application all the way to inserting data in an external database. If your streaming application depends on external systems outside the Kafka cluster, you’ll have to take extra care to make sure those systems' outputs will not cause the guarantee to be lost . Also, if you’re, say, writing the results of your real-time data pipeline to a relational database, you have to do so in a manner that writes to the database are kept in sync with your application position at any input stream. That is, your application pulls data from a bunch of input streams and then filters, maps and reduces it, but if in the end the external database transaction fails, the application has to make sure it doesn’t advance its position in any of the original input streams or the output streams created on top of them. Sound tricky? That’s why the recommended approach is to use a Kafka Connector that supports exactly-once-delivery as well. Why try solving this hard problem by yourself if someone else already did it for you, right? Not only will the connector take care of moving the data in and out of external storage, but it also performs the required coordination to achieve exactly-once-delivery. As with delivery guarantee, ordering in stream processing applications can range from unnecessary to explicitly required in order to compute aggregations correctly. Kafka does provide an ordering guarantee, but it's at the partition level and only if there’s a single writer for that partition (e.g. a single application instance). A single application instance is an unlikely scenario since we need the parallelism to achieve real-time stream processing. In that case, ordering is still achievable, but it will be an application responsibility. For instance, by using the lower level Processor API inside Kafka Streams, you can buffer records locally and then write the outputs one at a time in any order you wish. Of course, it doesn’t make sense to sort an infinite amount of data, so this will require a windowed operation. Another complicating factor is that the default Kafka configuration can produce records out of order, no matter the partition, if it has to retry sending a record (which it will, assuming default configuration). If a stream application first sends record A, followed by record B, we can get the order wrong in an output topic in the event that Kafka resends A due to a timeout. The configuration can be changed (see max.in.flight.requests.per.connection here ), but it disables sending parallel write requests, so there’ll be a performance impact to measure. Finally, it’s worth mentioning that order is based on offsets, the monotonically increasing number that keeps track of a stream application position in a stream. Order is not based on timestamps. Kafka is a great tool that enables teams to do their own real-time stream processing with a low-entry barrier. A lower entry barrier, actually ;). I hope this Snippet managed to answer a few questions, but know that there are still a lot of important features, APIs, and tools that we haven’t covered: the Producer and Consumer API, the Connectors, KSQL, or just using Kafka as a messaging middleware. If this has spiked your interest in Kafka, I encourage you to learn more about what it can do. In the meantime, feel free to ask any questions and to give your feedback in the comments.", "date": "2019-7-24"},
{"website": "Avenuecode", "title": "BrowserSync - A Tool to Streamline Manual testing", "author": ["Renata Andrade"], "link": "https://blog.avenuecode.com/how-browsersync-helps-your-manual-testing", "abstract": "For many, manual cross-browser testing takes a lot of time and is a boring, repetitive job. Here is a practical example of how BrowserSync helps us test different browsers at the same time without any programming . In this blog, w e're going to set it up and explore some good examples of where it's utilized. Here we go! What does it do? BrowserSync is an npm package that has the power to synchronize file changes and interactions across multiple browsers and devices. By \"synchronizing file changes and interactions across multiple browsers and devices\", that means almost everything you do in one browser is watched and performs on other browsers and devices that you are connected to via BrowserSync. You can open your IE, Chrome, and Safari on an iPhone at the same time, do whatever you need to do on one of them, and watch the same action be executed on the other browsers. You don't have to develop or maintain any steps because it's not an automation tool; it's a tool for manual testing, therefore, you can just 'plug and play'. This is a big help when you have to check responsive designs, scrolling, clickable links, or images, etc. Awesome, right? This is all possible because of the injection of an asynchronous script tag (<script async>...</script>) right after the <body> tag during the initial request. By now, we understand that the <body> tag must be present in order for this process to work. How do we set it up? It has an extremely simple setup. Let's check it out. Since it is a Node.js package, you might want to go here and make sure you have it installed. Assuming the previous step has been completed, just input: And that's it, we're all set! Now what? There are 2 ways to use it: 1. You have the project source running on your local machine. If this is the case, just run it inside the project folder (it requires the css folder with the .css files inside). For this option, you can also have the LiveReload working simultaneously. You can go ahead and implement a css change and watch it re-load instantaneously on your browser without having to refresh. 2. You have the project source running on somewhere else For this, go ahead and run: Replace `[URL]` with your project URL. Example: browser-sync start --proxy https://www.avenuecode.com/ --files \"css/*.css, *.html\" Open at least 2 browsers/devices on `https://localhost:3000` and have fun! It works well with scroll, c licks (using xpath), field fill, Browserstack, devices under the same network, and tunnel for devices out of the network. It doesn't work with mouse hover, and I have also found that it loses connection far too often with Safari. More Features Besides InteractionSync, it has some other awesome features: UI, Network Throttle, and URL history To open the BrowserSync UI, just access http://localhost:3001 on your browser. On the UI, you can see all the URLs that you can connect to BrowserSync , manage the Sync options such as mirroring clicks, see the URL history, add plugins, debug remotely, and set the Network Throttle. Take the time to explore these features! Conclusion It has been extremely helpful working with BrowserSync because it allows me to do all my tasks at once. I'd love to hear your feedback on this tool. How does it work for you? Does it help with efficiency? Let's chat and see how we can further improve this tool.", "date": "2017-8-22"},
{"website": "Avenuecode", "title": "How to Reduce Discovery Phase Time During Digital Transformation", "author": ["Moacir Otavio Di Pietro"], "link": "https://blog.avenuecode.com/how-to-reduce-discovery-phase-time-during-digital-transformation", "abstract": "For large organizations, discovering whether or not a product or idea will be successful takes an average of 89 days. But with the right techniques , you can ideate and validate in just 5 days. Whenever we talk about the ability of companies to adapt their products according to market needs, the word \"agile\" is one of the first we call to mind. Agile methodologies help companies launch new products incrementally, at the same time adapting their chain of internal processes to support the maintenance services of their portfolio. Business agility is also a characteristic of companies with a high capacity to generate innovation and gain space in the market. Nowadays, the term \"Corporate Darwinism\" is used to describe companies with the capacity to remain relevant in the market, or else to become an endangered species. Larger companies that are in the process of digital transformation still have complex internal processes that are vital to their value delivery chain, though mature processes are very bureaucratic in many circumstances and full of interdependencies and chains of approval. In these companies, which have business areas with high knowledge of the market and which map new opportunities for their consumers, it is essential that there is speed in the process of modeling new ideas so that an idea is transformed as quickly as possible into a minimum viable product (MVP) and its market traction can be validated. An MVP is the fastest and cheapest way possible to validate low-risk hypotheses. This is where businesses face many difficulties. The processes mentioned above are complex and require maturity, and in larger companies, ideas are subject to time-consuming approval chains. Getting the product or service ideation phase right is essential to a company's ability to deliver something new in time to capture a new market opportunity. For example, producing a new product requires stipulating a business case with a coherent idea model that is aligned with all business areas involved. Everything needs to be defined up front, including the financial resources and technical teams involved in creating the product, the amount of time that will be spent to start development, and when the work will begin. This is a phase that we will call Discovery . In larger companies, the discovery phase requires numerous alignment and approval meetings, and often the time spent on approval means that after the approval process is completed,  the innovation idea may already be obsolete. Real cases show that this stage can take an average of 89 days before the project is approved. We can see that the time to market is compromised and the company misses several opportunities because they don't have agile processes. Imagine a massive transatlantic ship - the crew wants to change direction downwind, but because the ship is so large, the crew is so large, and communication takes so much time, the crew is unable to perform the maneuver in time to obtain the benefit of a favorable current. In this scenario, a smaller and more agile boat can do better, just as small companies often move faster. A factor that contributes to this slow pace is that many of the professionals assigned to work on these projects have to manage their routine duties in addition to the new project, and the project, however important it is, is often not seen as a priority since people will continue to be assessed in the same way based on their normal work routine. To create an initial product design that takes into account proposals and ideas from the business side and technical teams, exercises and specific group dynamics are recommended to generate constructive discussions in a safe environment. In this way, a single product view is generated to guide teams at the beginning of development. By providing collaborative workshops and applying Design Thinking practices, teams can build a more coherent view of the MVP in just 5 days, with initial cost and time estimates included. In other words, when the entire team is focused and collaborating together, it's possible to reduce the discovery phase from 89 days to 5 days. This is because ideas are voiced and doubts are solved in real time. When key players aren't focused, the process takes an average of 89 days due to the unavailability of key players for the initial survey since whoever conducts the survey has to find the right people and connect their ideas separately. Workshops help to get everyone in the same room at the same time to expedite the process. Beyond ensuring engagement, workshops are valuable because they allow for several techniques that stimulate the creation of ideas to be developed as product functionalities. All approaches favor the incremental development process, where new discoveries identified throughout the process are incorporated into the product's backlog and prioritized according to the value they provide. Let's look at some examples of workshops that make it possible to reduce the discovery stage : Design Sprint - A set of exercises that make it possible to create and test a prototype in 5 days, following a set of brainstorming steps such as mapping, sketching, decision making, prototype creation, and testing. Design sprints are suitable for teams that need to validate ideas through analysis, testing prototypes, and experimentation for product modeling. Story Mapping - A technique for building products that, through the visualization of cards containing the needs of users, enables the creation of proposals for product features. The technique can be used by any team that wants to start creating a new product, but it is recommended for teams already familiar with the agile mindset of incremental development. Lean Inception - A workshop containing a sequence of dynamics for mapping objectives, identifying personas representing characteristics of product users, and suggesting functionalities through brainstorming exercises. Lean inceptions are suitable when business teams and technical teams are working together to better understand the user's needs. It is important to note that these discovery and ideation-focused sessions generate many benefits , even if their results are simply discovering that the project is not worth creating, that it may eventually cost twice as much as expected, or even that the company does not have the resources necessary for its development. These discoveries are a success because in 3 to 5 days you will know if it's worth moving forward with an idea. As important as knowing where to go is knowing where not to go! How long has the product ideation stage taken in your company? Share your experience in the comments. Note: this post was co-written by Michel Duarte Correa .", "date": "2021-2-17"},
{"website": "Avenuecode", "title": "AC Spotlight - Jason McNary", "author": ["Ulyana Zilbermints"], "link": "https://blog.avenuecode.com/ac-spotlight-jason-mcnary", "abstract": "(The following article is property of Avenue Code, LLC, and was originally published with permission at Total Retail on January 12th, 2021 .) Jason McNary , Global Board Member and CEO, Americas at UNOde50, gives us a pulse on global trends in jewelry retail. Avenue Code: Tell us about your personal career path. How did you get to where you are today? Jason McNary: I started my career with Abercrombie & Fitch, working in multiple locations as I progressively advanced into leadership roles. I then joined BCBG Max Azria, where I worked in retail and then in wholesale before finding my way back to retail again. I went on to work for Calypso St. Barth as the SVP of Retail. In this role, I managed everything related to retail and commercial, including stores, international expansion, construction, operation, and retail HR. I was then recruited to work at Intropia, a global company based out of Madrid, where I acted as President of the Americas market. From there, I became President of North and South America at agnès b., a French fashion brand, where I developed the business in Canada and the States from a wholesale and digital perspective. I was most recently recruited to UNOde50 as CEO for their American market. AC: What challenges and opportunities have arisen for UNOde50 post COVID-19? JM: At a high level, we were impacted in three ways: we had to reevaluate our financial positioning, to decide how to maximize our liquidity, and to protect the safety of our employees and consumers. From a technical standpoint, we’ve had to look at our 3-year plan and figure out how to get to year 3 in year 1. We are investing heavily in paid social media marketing, and we’re optimizing our e-commerce platform by elevating customization offerings and implementing virtual styling. We’re also focused on expansion into Latin America and South America, where we see a big white space for marketplaces. The pieces of the Renaissance capsule are inspired by the works of artists such as Da Vinci from the Renaissance period. This capsule belongs to the FW20 collection \"Faces\" of UNOde50. Image courtesy of UNOde50. AC: You’ve spoken about the importance of businesses challenging themselves to live as if they’re always in a pandemic in order to accelerate growth. How do you personally create that mindset for yourself and your team? JM: This mentality starts in our executive strategy meetings. What COVID-19 did for my team is accelerate initiatives we had tabled due to budgeting. We had to reexamine the business payoff and act quickly on whatever helped take our company to the next level. In short, the pandemic created urgency for our strategy and helped us lean on one another. At every executive meeting, we keep two empty chairs at the table. These chairs represent the consumer and the employee. We keep these two personas at the forefront of our strategy decisions as we continue to ask ourselves how to propel the business and what we would do differently if the pandemic or another disruption was again imminent. AC: With more and more people working remotely, we’ve seen a trend toward casual fashion. How has this affected the jewelry industry? JM: Before COVID-19, bracelets were our best seller. Now, because people are collaborating through video meetings, we’ve seen a decline in bracelet purchases and an increase in earring purchases. Necklaces remain popular. We’re also seeing a surge in customization orders, especially among millennials and Gen Z, as more and more customers want to be involved in the design process. Limited edition necklace from the India collection. Image courtesy of UNOde50. AC: What trends do you see within DTC as a whole? JM: I see that brands are trying to take control of their consumer and of how the brand is represented, so it will be interesting to see what happens with the wholesale model. Before the pandemic, wholesale was about 40% of our revenue for the American market. That has changed, of course, but I do believe the wholesale model will stay relevant to some degree. We’ll also see the use of marketplaces becoming even more important in 2020 and beyond. Brands will be asking how to use marketplaces as a storefront for the consumer, how to use them as part of external theming, and how to form strategic partnerships, always keeping the consumer at the forefront. AC: How do you recreate a personalized purchasing experience online for jewelry? JM: I believe that the digital experience will evolve significantly in jewelry. For example, we’re talking with vendors about adding a virtual styling option so our customers can try jewelry on through the use of their cameras. This same technology will also become relevant offline as we work to safely reopen physical stores. Because jewelry is an intimate purchase, it was behind the apparel industry as relates to this use of AI, but now we have an open road to innovate. AC: We’ve seen a trend toward influencers creating their own product lines. Do you think traditional jewelry companies have to evolve and add more value to stay competitive? JM: I think this trend is opening a door for strong collaborations between influencers and brands. While I do see the industry moving away from big celebrity collaborations, I think organic, long-term collaborations are being created when an influencer naturally loves a brand and there’s alignment in values and aesthetics. AC: What is the key to successful strategic B2B partnerships? JM: I look for common ground where both parties are interested in solving the same problem and bringing more joy to the consumer. I also look for transparency in communication on both sides. Finally, I look for brand and consumer engagement alignment. I want my partner to relate to my brand and my consumers. AC: What is your perspective on leadership diversity? What are the best practices for organizations seeking to develop career pathways and promote talent? JM: First and foremost, I look for diversity to be a natural part of culture. There’s always room to elevate diversity, but for us at UNOde50, it isn’t a forced focus - it’s already natural in our hiring and organizational practices. With so much social unrest happening in the US, we were fortunate to have that diversity already built into our organization. We feel very positive about our internal processes, so we don’t feel the need to talk about them externally. People sense that value as they shop and interact with us. AC: What advice would you give to emerging entrepreneurs? JM: I may not have said this a year ago, but my advice today is this: take care of yourself. Whether you’re taking time to exercise or to relax with your family, practicing health is important to success. AC: Thank you for your insights today, Jason. It’s been fascinating to hear your observations on the jewelry industry post COVID-19.", "date": "2021-3-11"},
{"website": "Avenuecode", "title": "Your 60-Second Intro to GKE", "author": ["Andre Menezes"], "link": "https://blog.avenuecode.com/your-60-second-intro-to-gke", "abstract": "Google Kubernetes Engine (GKE) is a Google Cloud solution for container orchestration. With GKE, we can manage the Kubernetes ecosystem and all routines related to deploying, managing, and monitoring our applications. There are several advantages to using GKE instead of a manually-configured Kubernetes cluster. In this post, we'll discuss three of our favorite GKE benefits. You can discover more by consulting Google's official documentation . Obviously, you may need to pay to use some additional features, but the benefits to your organization are well worth the investment. If your workloads exceed the maximum resource allocation on the cluster, GKE provides a new node for you to maintain the disponibility of your applications. When the workloads return to the real state, the additional node is automatically removed. We have the cluster auto scale functionality, but what happens if one node fails or needs manual intervention for maintenance?  Enter the node repair feature! This feature looks for problems with your Kubernetes nodes and resolves issues, automatically replacing failed nodes with new nodes. And if you need to upgrade your cluster, you can do so on the fly with the automatic upgrade feature that guarantees security, reliability, and zero downtime. You don't need a comment about the importance of logs, monitoring, and metrics to analyze the behavior of your application and improve it, right? GKE provides native integration with the Stackdriver for analyzing your application logs; all you have to do is enable logging and monitoring on your cluster. Google Kubernetes Engine saves time and makes it easier for teams to ensure agility, reliability, and security in all operations. In our next post, we will show you how to install and configure a GKE cluster and use some features. Meanwhile, you can find out more about how GCP can help you achieve your goals by clicking below to learn what the Google + Avenue Code partnership can do for you.", "date": "2019-6-14"},
{"website": "Avenuecode", "title": "Best Practices for Machine Learning with GCP", "author": ["Hossein Javedani Sadaei"], "link": "https://blog.avenuecode.com/best-practices-for-machine-learning-with-gcp", "abstract": "In this blog, I am going to explain some of the best practices for building a machine learning system in Google Cloud Platform.  We'll start by showing how to understand and formulate the problem and end with tips for training and deploying the model. We'll also discuss required GCP tools, data processing in GCP, and data validation. One of the first steps in developing any ML problem is understanding what you’d like the ML model to produce. What are the inputs for the model and, more importantly, what is the result? A developer needs to determine and define all related metrics for success with the ML system. One thing that will help a lot is to review some heuristics for that problem. By this, I mean you should consider how you would solve the problem if you weren't using machine learning; then, think about to what extent and in which ways your model will be better than that heuristic. The next step is to formulate the problem in the simplest way possible. Simple problem formulation is always easier to reason about and implement. Later, when you build the complete model, it's easy to add more complexity to increase accuracy. Provide a list of the data you want the machine learning model to accept. Work to find the resources where each input comes from, and evaluate how much work it will be to acquire a data pipeline using GCP to construct each column of a row. It is better to first concentrate on easily obtainable inputs. Before we go deeper, let's talk about the general form of a machine learning workflow. In general, any machine learning workflow in the cloud consists of the following steps: We will use this workflow and determine the best way to follow it in GCP. At the beginning of any cloud project, you need to decide who will do what and to what extent clients need to have access to resources. Google Cloud Platform provides resource containers, such as organizations, folders, and projects, that enable you to group and hierarchically organize other GCP resources. For machine learning resource management, it's good to define a project for each particular ML model. This makes it easier to empower and use all GCP services, including managing APIs, enabling billing, adding and removing collaborators, and managing permissions for GCP resources. For example, now you are easily able to create a model version with the trained model you previously uploaded to Cloud Storage. To ingest data for building machine learning models, there are some GCP and third-party tools available. In fact, the tools you use entirely depend on the data type and the source of data. Sometimes there are APIs on the data provider side that can be used for data ingestion. In this case, one option is to spin up some of the following resources -- Compute Engine, Kubernetes, or App Engine -- to acquire data. If the data comes in real-time, Cloud Pub/Sub seems to be a very good option; for example, you can use Google transfer appliance to transfer a huge amount of data. To import data from another service cloud provider, it's possible to use Cloud Storage Transfer Service. Also, you can employ GCP Online Transfer to use your network to move data to Google Cloud Storage. As part of the data life cycle for building a machine learning model, we need to think of where data should be stored. Normally, the first place data will be stored is either Google Cloud Storage or Big Query. For stream data, the sync can be BigQuery as well, since it's able to work with this kind of data. Raw data cannot be utilized for machine learning development purposes. It must be processed. In GCP, once we transfer data to Google Cloud Storage or BigQuery, data is obtainable via some applications and tools for processing as follows: The data that you can use in your training must obey the following rules to run on the Google AI Platform: VERIFICATION AND VALIDATION The method of cleansing, enhancing, and transforming your data can add significant changes to it, some of which might not be expected. So there are some techniques for verifying your dataset, from start to finish, for your data wrangling efforts. You need to think of the following: SELECTING PLATFORM AND RUNTIME VERSIONS After cleaning the data and placing it in proper storage, it's time to start building a machine learning model. AI Platform from GCP runs your training job on computing resources in the cloud. You can train a built-in algorithm against your dataset without writing a training application; you can design a training application to run on AI Platform as well. Before I continue, I should note that Cloud ML Engine is now a part of AI Platform. So you can scale up model training by utilizing the Cloud ML Engine training service or AI platform in a serverless environment within GCP. AI Platform promotes popular ML frameworks. It also presents built-in tools to help you understand your models and efficiently explain them to business users. AI Platform delivers the power and flexibility of TensorFlow, scikit-learn, and XGBoost to the cloud. Along with native support for modern frameworks like TensorFlow, you can arrange any other framework running on Cloud ML Engine. In this case, just upload a Docker container with your training program, and Cloud ML Engine will put it to work on Google's infrastructure. You can use AI Platform to train your machine learning models by applying the resources of Google Cloud Platform. Besides this, you can host your trained models on AI Platform so that you can post prediction requests and manage your models and jobs using the GCP services. DEVELOP AND DEPLOY YOUR ML MODEL WITH AI PLATFORM For training with huge datasets, the best practice is to run the model as a distributed TensorFlow job with AI Platform so you can designate multiple machines in a training cluster. To accelerate the training time, it's better to train with GPUs or TPUs. GPUs and TPUs are designed to perform mathematically intense operations at high speed. Before running your training application with AI platform, you must package your application, along with any supplementary dependencies you require, and upload the package to a Cloud Storage bucket that your Google Cloud Platform project can reach. AI Platform then presents model training. You must give your training job a name. One of the best techniques is to define a base name for all jobs connected with a given model and then append a date/time string. This practice makes it simple to sort lists of jobs by name because all jobs for a model are then arranged together in ascending order. When operating a training job on AI Platform, you must also define the number and types of machines you need. To make the process easier, you can choose from a set of predefined cluster blueprints called scale tiers. Alternatively, you can pick a custom tier and specify the machine types yourself. One very significant step in developing a machine learning model is hyperparameter tuning. If you need to use hyperparameter tuning, you must add configuration details when you build your training job. However, the best practice is to directly use HyperTune. By using this service, you can deliver better outcomes more quickly by automatically tuning deep learning hyperparameters. Data scientists frequently manage thousands of tuning experiments on the cloud. HyperTune saves many hours of tiresome and error-prone work. GCP applies regions to set the location of computing resources. If you store your training dataset on Cloud Storage, you should run your training job in the same region as the Cloud Storage bucket you're using for the training data. If you need to run your job in a separate region from your data bucket, your job may become longer. You can define the output directory for your job by arranging a job directory when you configure the job. When you submit the job, AI Platform verifies the directory so that you can adjust any problems before the job runs. You are required to account for the --job-dir argument in your application. Take the argument value when you parse your other parameters and apply it while saving your application's output. One good practice for applications is to output data, including checkpoints through training and a saved model. You can output other data as required by your application. It's easiest to save your output files to a Cloud Storage bucket in the same GCP project as your training job. GCP VMs may be restarted occasionally. You should ensure that your training job is resilient to these restarts by saving model checkpoints frequently and by configuring your job to restore the most up-to-date checkpoint. You normally save model checkpoints in the Cloud Storage path that you specify with the --job-dir argument in the gcloud ai-platform jobs submit training command. The TensorFlow Estimator API executes checkpoint functionality for you. If your model is wrapped in an Estimator, you do not need to worry about restart events on your VMs, so using Estimator API is a much better approach. There are also some good practices for deploying the models. For instance, if you are deploying a custom prediction method, upload any additional model artifacts to your model directory as well. The total file size of your model directory must be 250 MB or less; if you create subsequent versions of your model, put each one into its own separate directory within your Cloud Storage bucket. I hope this blog gave you some great tips for building a machine learning model in GCP. Do you have any other recommendations? Let us know in the comments below how you're implementing your ML model in GCP! For references and further reading, check out: Google Cloud Google Cloud AI Platform Cloud Dataflow documentation Cloud Dataproc documentation Interacting with BigQuery Deploying models Cloud ML Engine is now a part of AI Platform", "date": "2019-9-18"},
{"website": "Avenuecode", "title": "How to Implement the File Upload Process In A REST API", "author": ["Thiago Santana"], "link": "https://blog.avenuecode.com/how-to-implements-the-file-upload-process-in-a-rest-api", "abstract": "File-sharing is one of the most elementary ways to perform system integration. In the context of Web applications, we refer to the process where a user sends data/files from a local computer to a remote computer as upload. Sometimes we need to expose an upload operation in our REST API that allows us to transmit: The catch is, we want all this information to arrive to the server in the same request. Sounds like a challenge, doesn't it? The purpose of this article is to demonstrate a strategy, among several existing ones, that allows us to implement this integration scenario using resources provided by the Mule components. To provide some context, this strategy was inspired by the architecture of a legacy project that I needed to maintain. It was aimed at helping to execute the transmission of binary files in SOAP Web Services without the use of MTOM. When we don't intend to use MTOM with SOAP, where the files are transmitted as MIME attachments of the payload, the implementations of the protocol convert the contents of the file to be transmitted and the result of this conversion is a binary string in the Base64 format. After the conversion, the String is deposited inside an XML tag, which is a part of the Body content of the payload being transmitted to the remote server as shown below. The advantage of using this strategy is that we're now able to transmit this tag in the same payload which may contain other information, such as the data needed to process business rules, or better yet, the meta information of the file. Since all the necessary information is in the same payload, the process of reading and/or parsing the message is largely simplified. This is all handled by the service in a single request, which is very important for the processing economy and feature maximization. It means we don't have to worry about the maintenance and/or management of transactions handled by the service. We have achieved Stateless behavior. This is very desirable because it conforms to one of the great features of the HTTP protocol (which was designed to be Stateless) and allows the transaction to be completely processed in one request. Therefore, it won't be necessary to request additional information for the service consumer, plus, we can still have part of the treatment process done internally and asynchronously without prolonging the client response time. Finally, since there's no need to request additional information regarding the transaction in progress, it provides server resource savings as there will be no need for further processing. When making use of any interface, we should consider the fact that consumers usually expect to find availability , simplicity, and stability . Defining a RAML contract is one of the ways that we can establish guidelines that favor the construction of a REST API that intends to offer simplicity and stability . Just as we do with WSDL s, when we define and make a RAML available, we give our consumer a better chance to prepare for it as well as identify difficulties and provide suggestions for improvements for future versions of the contract. Using ready-made infrastructures, such as the Mulesoft API Design cloud platform, the consumer can also test this RAML by generating an endpoint with mock information provided in the RAML itself, allowing the consumer to test the interface in its consuming part. In order to implement the proposed scenario above, we need to define the RAML contract containing the POST operation for the resource \"file\" like so: For a resource like this, we can have the following HTTP request example: Below, I explain the different flows that make up the Mule project of the REST API, which is running as Runtime Mule Server 3.8.1 EE. Main Flow: It consists of an HTTP inbound endpoint configured to handle requests on port 8081, an APIkit Router, and a Reference Exception Strategy. Post Flow: The APIkit Route sends HTTP Post requests to this stream. It consists of two triggers (Flow References) for the buildFileData stream and filesSplitter, a DataWeave to prepare the response that is returned to the consumer, and a Logger. Build FilesData Flow: This flow declares both the FilesContent variables (a java HashMap) and FilesData (a simple String var). In this flow, you can find a ForEach that traverses the inboundAttachment of the current message (a multipart-formData) to separate the FileContent [] from the filesData. There's also a component that can be used to remove the rootMessage variable generated during the interaction of the ForEach. Lastly, there is the DataWeave that performs the merge of the original FilesData Payload with the FilesContent data. This is done to simplify the activity of passing binary data within a JSON for the consumer. Files Splitter Flow: This flow converts the received JSON FilesData into a built-in Mule Java object. This object is partitioned with each part representing a file that will be sent to the outbound VM component where each file is processed separately. The Collection Aggregator component is responsible for gathering the enriched payloads in the same structure. Upload File Flow: As suggested by the documentation, we can use an inbound VM to handle requests originating from a Message Splitter. Within this flow, each file is preprocessed to be sent to Amazon S3 by the Create Object operation. The pre-process consists of converting the Base64 binary String from the \"fileContent\" field of the payload into a BiteArray. The result of this conversion (here performed by the base64-decoder component) is stored in a variable. The Create Object component is then satisfied with this variable along with the file and bucket names. Because this operation doesn't give a usable response back, we add a call to the Get Object component that allows us to retrieve data from the file that was in storage. Lastly, with the help of a DW, the data received at the beginning of this flow is enriched with the FileSize and HttpURI data that was received in the response of the S3 Get Object operation. Exception Mapping Flow: Mule automatically generates this flow when we create a project, providing some RAML file to the APIKit. It offers some exception treatments handled automatically by APIKit. Most simple REST API tests, especially those involving GET operations, can easily be done by command line utilities such as cURL. Stress tests can be generated with the help of the JMeter tool as well. Tests can also be performed on newer versions of SoapUI. I opted to perform the tests with the help of the Postman tool (an extension for the Google Chrome browser). We can submit more than one fileContent[] as form param, but only the indexes reported as being used in the JSON passed in the form param \"filesData\" will be processed by the API. All others will be ignored. If no exceptions occur, we will then receive a JSON containing the processed data and HTTP status 201. And that's it folks. I want to hear your thoughts on this tutorial and any other techniques you would like to discuss or share. Please comment below! https://docs.mulesoft.com/mule-user-guide/v/3.6/splitter-flow-control-reference#example-splitting-and-aggregating-with-asynchronous-flows https://stackoverflow.com/a/15572342 https://raml.org/ https://docs.mulesoft.com/mule-user-guide/v/3.8/amazon-s3-connector http://jmeter.apache.org/ https://www.tutorialspoint.com/jmeter/ https://curl.haxx.se/docs/httpscripting.html https://www.soapui.org/rest-testing/getting-started.html https://objectpartners.com/2016/07/12/working-with-the-postman-rest-client/", "date": "2017-10-12"},
{"website": "Avenuecode", "title": "How to Create a REST Service on Google AppEngine Using GO - Part 2", "author": ["Pedro Costa"], "link": "https://blog.avenuecode.com/how-to-create-a-rest-service-on-google-appengine-using-go-part-2", "abstract": "This article is part two of a series of articles on GO development on Google Cloud. Read part one here ! Now that we've covered the basics of project creation on Google App Engine, it's time to get a REST service up and running on it. In order to accomplish that, we will be using the following tools: In this post, I’ll be covering all the steps needed to get one resource up and running on GAE. We’ll cover the datastore service creation as well as route handling. The next part of this will cover how to write unit tests for the services. Usually the unit tests should be written before the code, but I want to give a complete perspective of the code before showing how to write the tests. In order to prevent this post from becoming too long, I’ll just cover one small entity creation called User . The project structure will be as follows: The very first step is to write the datastore wrapper: First, we need to define the user package and import some other packages that will be used to persist data on datastore and add log onto AppEngine. Then, we need to define the name of the User index as well as a generic error message. Datastore uses indexes instead of tables, so, think of this index as the User table . Last but not least, we need to define what the User entity will look like. It will have 2 properties (Name and Email), both strings, and the json: property, which will be the name of those properties when marshalling/parsing the JSON object. The next step is to add the actual code that will handle the requests, and save/retrieve the data on the User index. This code is a basic CRUD abstraction and is pretty self-explanatory. The only thing we need to keep in mind is that exported methods MUST start with a Capital Character in order to be visible from other packages. If you want to add a private method to this file, all you have to do is add a func with a lowercase first letter. I like using Gin to route my services. It’s not mandatory, but it helps simplify the code. We will be adding routes on the main.go file. AppEngine uses the main() func, and because of that, the entry point for your service needs to be the init() func. In this file, we’re basically routing all requests to the gin router, and adding handlers for the /users endpoint. You can add middleware to this router object, like authentication, CORS support, and so on. The final code that needs to be added is the one responsible for handling the requests. I’ll add a file user.go i n the controllers folder, and the file will be as so: Pretty straightforward, right? We’re basically parsing the payload JSON to a User object and passing it to the right datastore service method. We just finished writing all the necessary code to get the service running. The project structure should be looking like this: The app.yaml is pretty much the same as the one used in the first post: Once again, in order to run your service locally, all you need to do is: And to deploy it on your GAE project: Datastore provides a powerful no-SQL database with high scalabily and reliabilty and it's quite simple to wrap up on a service. I hope that after reading this post you're excited to write your next REST service on Google AppEngine!", "date": "2017-12-6"},
{"website": "Avenuecode", "title": "Takeaways from Agile Trends 2019", "author": ["Milena Santos"], "link": "https://blog.avenuecode.com/takeaways-from-agile-trends-2019", "abstract": "", "date": "2019-4-29"},
{"website": "Avenuecode", "title": "Creating a Virtual Machine with a Web Server in Google Cloud", "author": ["Douglas Augusto"], "link": "https://blog.avenuecode.com/creating-a-virtual-machine-with-a-web-server-in-google-cloud", "abstract": "Web servers are used to host websites and make them available to users through HTTP protocol requests. It is common for these web servers to be created within virtual machines in the cloud. In this post, we will provide a step-by-step guide on how to create a virtual machine and web server inside Google Cloud. Google Compute Engine delivers virtual machines running on Google's innovative data centers and global fiber networks. Compute Engine resources are created in regions or zones. A region is a geographical location where you can run your resources. Each region has one or more zones. For example, the us-central1 region denotes the region in the central United States that has the zones us-central1-a, us-central1-b, us-central1-c, and us-central1-d. Resources that live in a zone are referred to as zonal resources. You can create a new virtual machine through the Cloud Console or the gcloud command line. In the GCP Console, on the top left of the screen, select Navigation menu > Compute Engine > VM Instances: To create a new instance, click Create: To create a virtual machine, there are several parameters that must be filled. The main ones are: Name, Region, Zone, Machine Type, Boot Disk, and Firewall. To create a virtual machine through the command line in gcloud, all the same parameters must be passed, as shown in the example below: gcloud compute instances create [name] --machine-type [machine-type] --zone [zone] First, open an SSH client with the virtual machine. You can perform this operation directly from the list of virtual machines in the Cloud Console, as shown in the image below: After accessing the machine's SSH client, run the following commands: sudo su - // get root access apt-get update // update the OS apt-get install nginx -y // install NGINX ps auwx | grep nginx // check if NGINX is running If everything is correct, you may access your web server simply by entering the external IP of the machine, as shown in the image below: When accessing the link, you will see the default page of your server: Creating virtual machines to run your own web server is an easy task and a quick process. Among the key benefits of Compute Engine is its ability to configure the virtual machine with custom resources since the platform is not limited to predefined virtual machine configurations.", "date": "2019-5-22"},
{"website": "Avenuecode", "title": "How to Build Microservices with Helidon", "author": ["Pankaj Bharambe"], "link": "https://blog.avenuecode.com/how-to-build-microservices-with-helidon", "abstract": "This is your practical guide to Oracle's new open source framework, Project Helidon , a collection of Java libraries designed for creating microservices-based applications. Originally named J4C (Java for Cloud), Helidon was designed to be simple and fast. It has two versions: Helidon SE and Helidon MP . Helidon SE features three core APIs to create a microservice -- a web server, configuration, and security -- for building microservices-based applications. An application server is not required. Helidon MP supports the MicroProfile 1.1 specification for building microservices-based applications. In this snippet, we will mostly concentrate on Helidon SE. Helidon requires Java 11 (or newer) and Maven. Helidon has a small, functional style API that is reactive, simple, and transparent; an application server is not required. A Helidon microservice is a Java SE application that starts a tinny HTTP server from the main method. To start webServer API, we need to add the required Maven Dependency to the pom.xml Inspired by NodeJS and other Java frameworks, Helidon's web server component is an asynchronous and reactive API that runs on top of Netty. The WebServer interface provides basic server lifecycle and monitoring enhanced by configuration, routing, error handling, and building metrics and health endpoints. See the startServer() method below: First, we need to build an instance of the Routing interface that serves as an HTTP request-response handler with routing rules: see createRouting() . In this example, in createRouting() we have registered Metric, Health, and context path /greet. We use GreetService to greet the user. The information below is from GreetService.java: After building the Routing Interface, we need to start the server. Helidon provides a functional interface for start and shutdown methods for servers. Let’s build and run our server application with Maven: The command above will help build the application, and you'll be able to see the logs below on your terminal. After a Successful Build, run the server by invoking jar: When the server starts, you should see the following in your terminal window: You can see the server is started on 65015, one of the available random ports. Run the curl: You should see: The configuration component, Config, loads and processes configuration properties in key/value format. By default, configuration properties will be read from a defined application.properties or application.yaml file placed in the /src/main/resources directory. The following code example demonstrates how to use Config and builds upon the previous example by reading an applications.yaml file to specify a port on which to start the web server. The greeting subnode defines the server response that we hard-coded in the previous example. The port subnode defines port 8080 for the web server to use upon startup. Let’s update our startServer() method to take advantage of the configuration we just defined: First, we need to build an instance of the Config interface by invoking its create() method to read our configuration file. The get(String key) method, provided by Config, returns a node, or a specific subnode, from the configuration file specified by key. For example, config.get(\"server\") will return the content under the server node. Next, we create an instance of ServerConfiguration , providing immutable web server information. This is possible when we invoke its create() method by passing in the statement, config.get(\"server\"). The web server is created as in the previous example, except we use a config() method that accepts the config instance. We can now build and run this version of our web server application using the same Maven and Java commands. Execute the same curl command: You should see: The Security class provides support for authentication, authorization, and auditing. A number of security providers for use in Helidon applications have been implemented. There are three ways security may be built into a Helidon application: from a builder; by configuration; or a hybrid of the first two. The following code example demonstrates how to build an instance of Security , use Config to obtain user authentication (with encrypted password) and display the server time. Helidon provides built-in support for health and metrics endpoints. Health: Metrics in Prometheus Format : Metrics in JSON Format : Helidon provides quick start examples to demonstrate the differences between Helidon SE and Helidon MP. The following Maven and Java commands will generate and package the Helidon SE example to create a REST service using Helidon's web server. In today's Snippet, we provided an introduction to Helidon, the new Java microservice framework that Oracle recently open sourced. Helidon is lightweight, simple, and fast, and more and more developers are excited about its capabilities. How have you used Helidon for your projects? Tell us in the comments below! Project Helidon Helidon Javadocs Maven Java 11", "date": "2021-4-14"},
{"website": "Avenuecode", "title": "Safely Working with Multiple Git Providers", "author": ["Rafael Romão"], "link": "https://blog.avenuecode.com/safely-working-with-multiple-git-providers", "abstract": "Developers who work with more than one Git provider need to be aware of this common pitfall and know how to avoid it. Most developers who use Git on a daily basis are used to the commands below: What they do is to write the user name and email that must be used on your commits to a configuration file, ~/.gitconfig . After the commands above, the file will look like this: And when you issue the command below, your commit will have the name and email configured in this file in the author property: You can explicitly set the author in a git commit like this, though: And Git will ask you for this information the first time you try to commit without having this .gitconfig file configured: Run: Now suppose that, like me, you work as a contractor consultant responsible for pushing changes to private repositories from client organizations. In this case, you will have to use an email provided by this client organization, which will not be the same one you use in your public GitHub account, for example, and most of the time these organizations will have a confidentiality contract with you or your employer that requests you to not disclose any information about what you are working on or who you are working for. That said, what would happen if you first configure your .gitconfig like this: And then, in your free time, you use the same computer to work on a public GitHub project and push some changes there? Yes, I’ve heard of that happening. The G it commit worked without any errors or warnings, and the client email ended up in a public GitHub repository. And it turns out the client had a scan policy that found it there and sent an alert to the person who pushed the commit with the wrong email. But why did that happen? The reason that the wrong email was accepted without any authentication error is that, in this case, the authentication happened through an SSH key , so a username and password is not requested to push. This is the best way to authenticate to Git, but if the same SSH key is used by multiple Git providers, or if multiple keys are configured but the mapping between the SSH key and Git provider is not done properly, this kind of mistake can happen. First, configure multiple SSH keys, one for each Git provider: GENERATING AND REGISTERING THE SSH KEYS In summary, follow the instructions given on this page and give different file names for each key. Here I will demonstrate how to do it for two GitHub providers, the public one and the enterprise one, as an example. Copy the SSH public key to the clipboard: Create a new SSH Key on GitHub and paste the pub key there: Now I repeat the process with the GitHub enterprise account provided by my client. Copy the SSH public key to the clipboard: The address to register a new SSH Key in the GitHub enterprise is the same, but with a different base URL. Mapping each key to a different Git provider is how we tell Git which key to use to authenticate on each Git provider. This will allow us to git push without being prompted for an authentication, but this will not avoid the wrong email problem. But we will get there soon. To map different keys to different Git providers, we need to edit the file ~/.ssh/config with entries like these: At this point, if we do a git config --global user.email \"myemail@myclient.com\" and leave it this way, we are doomed. All our commits to our public GitHub will be pushed with our client’s email. The solution to this problem is actually quite simple. We just need to create different .gitconfig files, register our user information in there, and assign each one to a different folder hierarchy. In my case, I have a folder named ~/Projects where I keep all my Git projects. In there, I have the folders Client , Company, and Personal . Client folder contains all my, well, client projects, while in the Company folder I keep my Avenue Code internal projects, and in the Personal folder I keep the ones you can find in my public GitHub. An important detail here is that I named the Client folder as Client and not with the name of my client. This way, I have another guarantee that I will not accidentally leak the name of my client in case I want to push my Git configurations to a public Git repository, for example. Now, in each of these three folders, I will create a file named .gitconfig with contents like the following: But that is not all. There is one final step. I need to inform G it about these files, and I can do that by editing my ~/.gitconfi g file like this: This will tell G it to get the missing configs, in this case the user.email , from the referenced files. Also, since there is no email informed here, if we try to g it commit anything in a folder hierarchy that is not mapped, Git will present that message informing us that it does not know which user email to use. It is important that we do not inform a global user.email after that, since we might end up having to clone a Git repository outside these mapped folders, and if there is no global user.email , we will not accidentally use it. That is what I have for today. If you work with more than one Git provider, I strongly recommend that you review your Git configs and ensure you will not have this kind of problem. Sadly, when this happens, we usually do not notice until a few months have passed. If you have any questions or other recommendations on this topic, leave us a comment below.", "date": "2020-10-14"},
{"website": "Avenuecode", "title": "AC Spotlight - Carl Boutet", "author": ["Anna Vander Wall"], "link": "https://blog.avenuecode.com/ac-spotlight-carl-boutet", "abstract": "(The following article is property of Avenue Code, LLC, and was originally published with permission at TotalRetail on August 9, 2019 .) Retail strategist Carl Boutet discusses the cultural and technological transformations retailers must make to create personalization and consistency across physical and digital platforms. Avenue Code: ​Tell us about your career path and what led you to retail. Carl Boutet: ​My formal retail career path started while I was at university working for the head office of Canadian Tire, then for a marketing agency. That was followed by my true “retail schooling.” For nearly 10 years, I was in the trenches opening and running my own shops inside Costco for Costco Canada in 65 locations, where I implemented a new wave of retail business with the advent of mobile phones and other telecommunication and technology products. From there, I began my journey of strategy consulting. I spent over six years with a co-op of over 600 Canadian independent retailers in home furnishings, appliances, and electronics. More recently, I founded StudioRx , a retail strategy and technology advisory collective that goes beyond advising retailers to include Cloudraker/Altavia marketing group, McGill University’s Bensadoun School of Retail Management, several data and analytic technology companies, trade associations, startups, and innovation and research labs. I stepped away at times to work in design, technology, and project management, but the energy in retail is addictive and always pulled me back. What you do to engage consumers is critical, and there’s almost always instant gratification because the feedback loop is fast. You know immediately how your strategies are effective/ineffective, and can use that information to adapt for the next consumer. I’m not saying you should look at immediate results only — it’s important to have a longer horizon view too — but it’s a rare industry where you get such immediacy. That said, the amount of continuous change in consumer/retailer engagement makes strategizing highly complex. AC: ​You’ve spoken a lot about blurring the lines between physical and digital commerce. What prompted this line of thought? CB: ​Previously, there were a lot of silos between the digital team, the merchandising team, the operations team, etc. It’s become more and more apparent that this nonharmonized view is detrimental both to the organization and to the consumer. There’s a tendency to think of physical and digital arenas as separate, but the two increasingly intersect. The digital intersects with the physical (think virtual reality and augmented reality), and the physical intersects with the digital (think of consumers who are on phones while in stores). We can’t think of these as separate channels. AC: ​What lessons have retailers learned the hard way? CB: ​ It’s destructive to treat the same customer differently depending on the channel. Personalization is of course important, and the way you engage should evolve based on the channel, but consistency is absolutely key for brand strength. I’m still the same customer regardless of whether I’m walking through the store or scrolling my phone at home. Let’s take a practical example to illustrate the downside of personalization for different channels: You buy something online, and when you visit the store shortly after, you see the same item you purchased online, but listed at a lower price. Retailers can refund the item, but this has to happen in-store, which means the customer has to jump through hoops. AC: ​What drives new ways of consumer engagement? CB: ​Engagement is consumer driven. The elephant in the room is always Amazon.com. It has been disruptive and created havoc, but its customer obsession is clear. Amazon has driven every other retailer to look more carefully at how it treats its customers. Consumers are very generous in giving retailers a lot of valuable data, and retailers should be giving back something that’s more meaningful. A good first step would be getting away from generic marketing. Retailers are targeting huge, cookie cutter demographics like “millennial” when they should be targeting “Anna.” We have the technology to do this, so the fact that we aren’t just reflects laziness. AC: ​What’s the blocker? If we have the technology, why aren’t retailers doing this? CB : ​The blocker is often cultural, as it requires the data science team, the merchandising team, the customer care team, the marketing team, etc., to work very closely together. This is where the idea of silos is destructive — there’s a culture of finger pointing where nobody takes responsibility. You can ramp up server speed way faster than you can ramp up culture. The notorious “digital transformation” era we’re in is really more about cultural transformation than technical transformation because companies can’t embrace new technology without embracing cultural change. Beyond company culture, tech needs to be developed that has a longer line of sight about consumer behavior than the typical CRM. There’s also a talent issue — retailers are really fighting to find talent, and this isn’t easy, especially at scale. Another blocker is financial. The suggestions above require big investments, and the return on investment path isn’t always as clear as executives would like it to be. There’s a fear of failure in most traditional retail environments that isn’t conducive to innovation. If we don’t see ROI quickly, we get nervous and pull the plug. AC: ​What are the biggest challenges for enterprise companies? CB: ​Speed and urgency. Companies say they can’t move fast enough, but they simply can’t wait until something is perfect to roll it out. They need to make releases faster and iterate from there. This doesn’t necessarily mean rolling something out to 5,000 stores at the same time, but it does mean doing concept stores. This is why there’s retail research and innovation at McGill. Having a longer horizon view works where there are iterative strategies. Having an iterative mentality isn't always easy. Many firms are focused on tactics and strategy, not operations, though retailers like Walmart are taking good first steps by making acqui-hires like Jet.com and Art.com. AC: ​What are the blind spots for retailers when they consider cultural change? CB: ​We need to broaden our focus beyond transactional data to include contextual consumer data. This goes back to my frustration around placing consumers in demographic buckets. Even a single consumer changes over time and won’t want to buy the same things he or she bought two years or even six months prior. Companies should be paying attention to behavioral and contextual clues. For example, if you buy a guitar on Amazon, you’ll receive recommendations for guitar strings and picks, whereas if you buy a guitar on AliExpress, you’ll get recommendations for local guitar teachers. Retailers need to think about physical and digital, product and experience all at the same time. We also have to redefine retail to include elements of hospitality, entertainment and community, all in a single brand experience, like Nike partnering shoe buyers with local running clubs, or the Yorkdale Mall’s Restoration Hardware store that’s also a restaurant. These are opportunities to engage and grow brand appreciation. Any retailer can do that in a cost-conscious way if it's creative. AC:​ We hear a lot about creating a consistent brand presence across multiple physical/digital channels, but what I still see missing is a truly seamless, agnostic experience that would take you from your desktop to your mobile device to your physical location. I keep hearing whispers about this, but nothing substantive is in place yet. What's the hurdle retailers are facing to get there? CB: ​It’s a huge challenge to get brands to play nicely together because they’re naturally competitive, but it’s slowly happening. I was recently riding Lyft, and my driver asked about the purpose of my trip. I was going to the Eaton Center in Toronto, and my driver mapped out a curated route through the mall to match my goals. Lyft was obviously partnering with the mall operator! Partnerships are growing, but the challenge comes when you try to integrate platforms. We’re not quite at the point where we can integrate apps for Lyft, Nike, Starbucks, and so on. We’ve got a lot of vision. Now we need the execution. It’s an exciting time to be in digital transformation — retailers are ready to work! AC: Thanks for your insight on the cultural and technological changes companies must adopt in order to thrive as consumer-driven retail continues to evolve!", "date": "2019-8-14"},
{"website": "Avenuecode", "title": "Testing With Karate Knocks", "author": ["Vinicius Piedade"], "link": "https://blog.avenuecode.com/testing-with-karate-knocks", "abstract": "Testing is never enough nowadays, and implementing integration tests can be a bit “boring”. So, why not utilize karate knocks to speed up your web service tests and make your life easier? Here's a brief introduction to the karate framework. This article was originally published on viniciuspiedade.me and is republished here with permission from the author. Karate enables you to script a sequence of calls to any kind of web-service and assert that the responses are as expected. It makes it very easy to build complex request payloads, traverse data within the responses, and chain data from responses into the next request. Karate’s payload validation engine can perform a ‘smart compare’ of two JSON or XML documents without being affected by white-space or the order in which data-elements actually appear, and you can opt to ignore fields that you choose. Since Karate is built on top of Cucumber-JVM , you can run tests and generate reports like any standard Java project. But instead of Java – you write tests in a language designed to make dealing with HTTP, JSON, or XML simple. Ok, now it's time for the code! We’ll spin up a local server with a json-server acting as our target web-service. Create your awesome database as a json file: Now, let the json-server spin-up a web-service automagically : npm install json-server -g json-server mydb.json Now we have our awesome service to perform a few tests using the powerful karate. To start coding, we’ll need: Let’s create a new project with a karate maven archetype: mvn archetype:generate \\ -DarchetypeGroupId=com.intuit.karate \\ -DarchetypeArtifactId=karate-archetype \\ -DarchetypeVersion=0.6.1 \\ -DgroupId=cme.viniciuspiedade \\ -DartifactId=myproject-qa Let’s make a small change on the pom.xml, adding entries to the testResources tag: Now we have a basic karate structure! We’ll use the project structure conventions based on Karate-Demo , but do a few things differently. First, your project classpath structure must consider the src/test/resources folder as a source folder because all karate config files are stored inside this folder, and they need to be at the root path of your test-classes like this: Here, you can see an example of the power of the karate framework: With a single *.feature file using a Gherkin-like language, we're able to define a test against the ‘/posts’ endpoint, performing a POST with a post payload defined on the request body, asserting that it will return the HTTP status 200 and that the response body matches the expected post specification. Karate will interpret your Gherkin file with the power of Cucumber and its built-in StepDefinitions behind the scenes, as well as prepare request calls with all available http methods, deal with json, xml contents, and http headers, and make assertions with responses using its powerful jsonPath matcher and a lot more. Karate provides a flexible way to run your feature files because it will run all files from above your current Java Test class recursively (including subfolders/packages). Based on this, we’ll organize our features on consumer and functionality hierarchy like below: Let’s understand more about this project structure. In the root package, we have our consumer folders - one folder for each consumer, and inside the root of each consumer folder, we’ll have a Consumer Junit Test class responsible for running all tests for its consumer organized into functionalities (each functionality with a separated folder) from the Maven command line. Inside each functionality, we have one Junit class file, but with a Runner suffix and one or more feature files. That means that you can run all the features of the functionality of a specific consumer simply through your local IDE (eg. eclipse), running the *Runner.java class as a Junit Test class. Run all features as a regressive test: mvn test Run all features of the consumer1: mvn test -Dtest=consumer1.Consumer1Test Run all features of consumer1 posts functinality: mvn test -Dtest=consumer1.posts.PostsCRUDRunner Now, we’ll make a few improvements to the karate config files in order to make it eaiery to use in our projects. Below, you’ll see all the steps necessary to configure karate using the Buscapé convention: karate-config.js Our karate-config.js file will look like this: With this config file, we're defining a different baseUrl based on an environment variable called karate.env. If it’s filled with homolog value, all requests will hit our fake homolog server ( http :// homolog . myserver . com ), and if no value was found for karate.env, it will then look to our local server - in this case, our json-server. We are then able to provide a different mock server port, or leave it as default 3000. Let’s code our first feature of Consumer1: Post C.R.U.D. $ mvn test You can specify the mock server port with -Dmock.server.port argument: $ mvn test -DargLine=”-Dmock.server.port=8080″ $ mvn test -DargLine=”-Dkarate.env=homolog” That’s it! Karate is an awesome project where you can easily create automated integration test pipelines inside your current CI/CD stack. Check out this project on my GitHub here , and enjoy!", "date": "2018-1-10"},
{"website": "Avenuecode", "title": "Parallax on the web", "author": ["Eduardo Silva"], "link": "https://blog.avenuecode.com/parallax-on-the-web", "abstract": "Those who enjoy surfing the internet and admire the beauty in websites have almost definitely noticed the introduction of a new design concept: the parallax. The parallax itself is a technique that explores component independent movement, creating astonishing 3D visual effects in the page. This effect relies heavily on the scrolling event, where each component can move at different velocities - during the scrolling - in order to grab users’ attention and direct it to a certain area/information. But what types of parallaxes exist? Why and where should we use them? These are a few questions we'll try to answer in this article. The parallax effect is actually not really new. According to Wikipedia , \"the technique grew out of the multiple camera used in traditional animation since 1930s\". Its popularity, though, came from the 2D computer graphics in the 1980s, which brought the illusion of a character being moving along with a background scenario as seen in the picture below: Figure 1: Parallax effect for 2D games ( source ) In this example, the mountains, sky and clouds in the background are moving more slowly than the small boy running in the grass. This effect creates the illusion in our eyes that our character is actually running and moving toward. But when was the parallax effect first introduced to the web? According to Carla Dawson , the web first saw a parallax in 2011, when Ian Coyle created the very first parallax website for Nike, entitled \"Nike Better World\" ( This article presents a case study of the website) . This event shone a spotlight on parallax. Web designers started to consider the technique, which became a trend in 2013 onwards, according to the graphic below: Figure 2: Graph showing parallax use in websites tendency. ( source ) As mentioned earlier, the parallax is used to draw users’ attention to an area or concept on the website. Its strong appeal for motion is often used to stress some important information, such as a product in clearance, for example. Figure 3: Scrolling down the page to review the suits discounts In Figure 3, as the user scrolls down the page, the promotional discount information appears in a way that can't be missed. It's a huge alert to attract the customer's attention to the product on sale. It is also often used to keep the user interested in a whole company/product landing page, since it forces the user to scroll the page down to the bottom by piquing his curiosity. A third example is for sites that are supposedly trying to tell you a story. An amazing storytelling example is flattvrealism . The user is always motivated to scroll down to the bottom so he knows the end of the story. Parallaxes are great, but of course there always setbacks we should be aware of. Parallaxes can have the following cons: Depending on the parallax we are trying to create, the complexity level can be very tricky, since we are dealing with scrolling events. Knowing exactly when and where to trigger the action is not always easy, especially for the storytelling examples, such as http://www.flatvsrealism.com/ . Besides, if the parallax is not well designed, it can cause the website to be confusing and unclear to the user. Another point to consider is performance. Depending on the project, the loading time can become a real issue because of the time to load so many images. Besides, we have to consider we are always listening to the scroll event, which may increase the use of CPU and memory, which is not always good when dealing with, for example, mobile devices. Lastly, since the idea is to keep scrolling, the tendency is to keep developing single page websites, which can be a problem for SEO tools, since the number of pages can vote up your website in Google searches. We should always be aware of these 3 considerations when we decide to make use of the technique in our websites. Performance and SEO are always important priorities. Since the parallaxes rely on the scroll event, many kind of effects can be applied. In this article, we'll cover the 4 most commonly used cases and come up with examples for each one. The parallax names seen in this article are not official. The idea was to come up with easy-to-memorize names that define with words what the parallax does. Here is the list: Probably the most popular parallax on the web. It is mainly used on landing pages, and creates the effect of layers being overlapped. It is reminiscent of a letter being pulled out/pushed into an envelope. Figure 4: Header Parallax in action In the Figure 4, we can see one example of the Header Parallax in action. The hero area, which is the darker portion in the page is being covered during scrolling by the white section below it. It's a great effect and relatively easy to achieve if compared with other types. The reveal parallax is another example extensively used over the internet. It consists of revealing the content only when the user scrolls down to that point. Normally, that area will be hidden and only when the user get up to a specified point, the content will show up. An example can be seen below: Figure 5: Revealing parallax example In the Figure 5, we can see that the footer (the black area) is not shown by default. As soon as the user scrolls down to a specified area, the footer starts to show up. It's the same illusion of opening the curtains from a theater to delight the crowd with a play. In this case, the web designer wants to maintain the user focused on the items the website has to offer. It also communicates to the user the idea of continuation. The website tries to say: \"there are more items to come. Keep up scrolling, but check out my items\". Items in this case could be products, talks, lessons or whatever is important to the business group. Below, we have a example from the TEDxPortland website: Figure 6: Lazy loading images parallax In Figure 6, the TED talks are being shown as the user scrolls. As mentioned earlier, it gives the idea of continuation. There are more items (talks) to come. The talks are what we want you to be focused on. Keep scrolling! The landing parallax consists in drop content in the page as the user scrolls. The difference between the landing and the lazy loading parallaxes is that the way the components are displayed in the page. In the lazy loading method, we are only displaying the elements in a fixed position. The elements are basically fading in/fading out. However, in the landing parallax, the elements are moving from an origin position and dropping to an end position as the scrolling goes on. The effect can be seen in Figure 7: Figure 7: Landing parallax in action Here the elements don't have a fixed position where they will only show up. They need to come from a direction and move toward a destination. It's one of the most appealing methods and it can cause a very good impression, depending on the animation's complexity level. The example above was taken from a storytelling parallax website. It can be reached through the url http://www.albinotonnina.com/ . Here the the website's owner tries to retain the user’s attention on his resumé by making use of several interesting animations controlled by the scroll event. Great examples of parallaxes can be easily found over the internet. Each website shows great work and user experience. It's worth to take some inspiration before starting our own project. Below is a list of selected amazing parallax websites: http://www.flatvsrealism.com/ http://www.albinotonnina.com/ https://www.spotify.com/br/ http://antonandirene.com/ http://time4.online/ Over the years, many techniques and resources came to make the web experience more and more friendly and effective. As we can see, parallax is very important for creating incredible visual effects capable of maintaining user focus on what is most important in a website. The importance of doing that is to increase the conversion rate no matter what kind of business the website has to offer so the company/professional can always grow. In a subsequent article we'll explore each one of the parallaxes types described in practice. We'll develop a website using all the examples. Stay tuned!", "date": "2016-12-28"},
{"website": "Avenuecode", "title": "Reasons To Use Private Mode When Using LocalStorage and SessionStorage", "author": ["Mark Carlson"], "link": "https://blog.avenuecode.com/reasons-to-use-private-mode-when-using-localstorage-and-sessionstorage", "abstract": "This article was originally published here in July 2016 and is republished here with the author's permission. So you write some code, validate the feature, send it off to QA, and move on to something else. The last thing you expect is to have a QA engineer come back and say that your code doesn't work. When that happens, you begin the process of identifying the differences (or the singular difference) between the tester's system and yours. I opened the site on my phone, it works. I open the site on his phone, it doesn't work. We both have iPhones. Both have the same iOS version. Both have the same browser. My links work, his don't. Go figure. Wait, why is his border black and mine is white? Ahh, he's in private browsing mode. Well that shouldn't make any difference, should it? Links are links, right? So I open an incognito tab, go to the site, and sure enough my links don't work either. Who would've guessed? So, I hook up my phone to my computer to open my dev tools and start scouring the console log for errors. This one looks unusual: Turns out I had added a script to save the open navigation tabs to sessionStorage, so subsequent page views would display the open tabs when the user opened the hamburger menu. Safari's mobile browser not only disables localStorage and sessionStorage in private mode, but also throws a fatal error! The script stops running. Degredation is hardly graceful. In this case, the links did not respond to tap events. The problem here was pretty easy to fix. Test for sessionStorage and use a try/catch to silently ignore the error: Lesson learned. Always test your code in private mode on a mobile device when using localStorage or sessionStorage", "date": "2016-11-30"},
{"website": "Avenuecode", "title": "Your Introduction to Multiple Objective Optimization", "author": ["Eduardo Salinas"], "link": "https://blog.avenuecode.com/your-introduction-to-multiple-objective-optimization", "abstract": "We consider optimization every day of our lives. We always want to take the route that will get us to work the fastest, accomplish as many tasks as possible on time based on priority, and so on. Optimization means finding the solution (or solutions) for a problem that produces a result that cannot be further improved. Today, we'll introduce multiple objective optimization. We refer to numerical optimization when we talk about improving the result of a mathematical problem. This is used in engineering, biology, economics, and aerospace design, to name a few. Think about the challenge of modelling the wing of an airplane to minimize resistance and maximize lift. An ideal solution would probably cheapen costs for both the airline and the customers. While this article does not intend to solve such a complex problem (entire theses have been devoted to solving airfoil design), it tries to give a brief introduction to numerical optimization without going too much into the technical side of things. In addition, while we use Swift to generate the example functions, no previous knowledge of Swift is required to understand the concepts we'll cover. Let’s start with a basic optimization example. Say we were given the problem of finding the minimum of the function x^2 (or x times x); we could say that the best solution would be when x = 0, since 0^2 = 0. We know that there is no other solution (for real numbers) that would yield a number less than 0. We can easily see this when we plot this function: In Single Objective Optimization, we would search for the best solution for only one problem at a time. But what happens if we want to find the best solution for more than one problem? We then enter in a field called Multiple Objective Optimization, or MOO for short. In MOO, we usually want to either minimize or maximize multiple functions simultaneously. However, these functions are often in conflict with one another. An improved solution for one function often means a worse solution for another function. In such cases, we are left with a group of optimal solutions. This group, or set of solutions, is referred to as the Pareto set, while the result of evaluating the Pareto set in the function is called the Pareto front. (I won't go into details regarding the mathematical definition of the Pareto set, since this is out of scope for the purpose of this post. The original article can be found here ). To illustrate MOO, I've created a small function that takes in a range of values and a given function. We then use this range to evaluate the given function and create a set of CGPoints to add into our plot, as shown below: Now that we have our plotting function, we can illustrate a simple example. Consider the following two functions: The first function is a simple quadratic function. The second function is also quadratic. The only difference is that we are flipping the y-values and shifting the result up by a fixed amount. If we were to plot our two functions, we'd see something like the image below. Notice the intersection between the two functions. Here, the functions are in conflict since decreasing our first function increases the value of the second function. There are a number of approaches to obtain the Pareto set for MOO problems, including scalarization techniques, line search methods, and evolutionary algorithms. Due to the scope of this post, I’ll only show an example of a scalarization method. This method will serve as an introductory point for the subject, since more advanced methods involve evolutionary algorithms and a deeper understanding of numerical optimization. These methods aim to reduce the MOO problem into a single objective one and then find the solution to the problem. Weighted Sum Method The Weighted Sum Method was proposed by Lotfi Zadeh. The idea behind this method is to associate a weight, or scalar value, to each function and then combine the functions into one. So if we have the two functions f(x) and g(x) , we could reduce this problem to: h(x) = w₁f(x) + w₂g(x) , where w₁ and w₂ are scalar values, and where w₁+w₂ = 1. Going back to our original problem, we could set these scalar values to, say, 0.5 each. This means that we value both functions equally, and thus, the resulting solution would be: So if we wanted to get the smallest possible value on this function, we'd have: Since we merged both functions into one, what we get is a single optimization problem, which means a single optimal solution for the given set of weights. This is, in fact, the main drawback of scalarization methods. We can, however, have an even distribution of weights, which would result in a set of optimal solutions, but that still doesn't guarantee an even distribution of points in the Pareto front. Let's try a different example now. Suppose we have the following functions: Using the Weighted Sum method, we can obtain the Pareto set using the given range. We can then use this function to plot the Pareto front, which shows the set of optimal solutions when we change the value of the weights. We have to take into account the following things, though: As you can see, MOO is a complex field, and what's shown here is just the tip of it. Over the course of the years, new techniques and algorithms have been developed to try to solve MOO problems while reducing the computational effort and time it takes to analyze them. We’ll always see numerical optimization in our everyday lives, even if we don’t realize it. But now, every time you see a SpaceX rocket launch, you can think to yourself: someone had to solve a MOO problem by minimizing the fuel spent while maximizing the lift. I hope you enjoyed this article, even if it was not as Swift-y as you might have expected!", "date": "2019-10-23"},
{"website": "Avenuecode", "title": "AC Spotlight - Sandro Tavares", "author": ["Andressa Lopes"], "link": "https://blog.avenuecode.com/ac-spotlight-sandro-tavares", "abstract": "Sandro Tavares, IT Manager at Midea Carrier, shares his perspective on management, business operations, and digital transformation in times of crisis. Avenue Code: Tell us how you became the IT Manager at Midea Carrier. Sandro Tavares: I had a long and interesting career in IT before joining Midea Carrier. I started my journey studying Computer Science and working as an intern at Eletrosul. As soon as I graduated, I was invited to work at Portobello, where I grew from Junior Analyst to CIO. After this, I decided to apply for a master’s degree in logistics while working as a market consultant at Bunge. My responsibilities included redesigning processes and managing projects related to the logistics integration of Bunge in Brazil, as well as coordinating Bunge’s IT infrastructure projects. I then became IT Manager at Karsten, where I reviewed IT governance and guided the ERP selection process. In April 2009, I was invited to manage IT for Tigre Group, which had 5 companies in Brazil and 9 companies in Latin America and the USA. This was without a doubt my biggest challenge, as I had the opportunity to restructure the strategic IT plan and also direct our SAP solution implementation. This took 2 years and a team of more than 300 people, including employees and consultants, and it resulted in a new level of IT sophistication and management control. At the end of 2014, I was invited to take on another new challenge, this time at Midea Carrier. My focus here has been leveraging the Midea Carrier business model that was already very successful in China and replicating it in Brazil and throughout Latin America. And so begins my great adventure at Midea Carrier! AC: Technology develops constantly. What are the biggest challenges in orchestrating a development team, and how do you achieve positive results? ST: One of the biggest challenges is to be prepared to face the most adverse scenarios. Throughout my career, I have experienced many circumstances. I have worked in companies focused on investments and technology that generated positive financial growth, allowing us to focus on innovation and automation; but I have also seen periods of financial instability where companies had to rethink their entire strategy and adjust their budget to the reality of the moment. In this case, the focus was on maintaining operations and supporting the business. Strategy changes like these affect a team's motivation significantly. Being part of an IT area and not seeing innovation projects can be demotivating. One piece of advice that I always give is that it’s important to tell the team where the company is. A well-informed team may be upset about a situation, but they will never be negatively surprised. Another major challenge is ensuring that a team is constantly updating their technological expertise in addition to gaining closer knowledge of business goals. An important strategy is to actively stay in contact with the entire team through tools like one-on-one meetings. AC: Can you tell us more about how Midea Carrier views sustainability? ST: Midea Carrier believes that there is no real development without a commitment to the environment. For this reason, we have a pioneering strategy in the adoption of policies and actions to reduce damage and preserve natural resources like water and energy. In 2010, Midea was ranked one of the top 100 green companies in China, while Carrier was one of the first companies in the world to set targets for reducing energy consumption and greenhouse gas emissions. Between 2006 and 2010, factories reduced water use by 30%, and they also reduced greenhouse gas emissions. The joint venture of Midea Carrier maintains the same principles that Midea and Carrier hold individually. AC: We saw an article in which Midea Carrier donated air conditioning to the COVID-19 combat center. Can you tell us about how you see the pandemic affecting your sector? ST: Solidarity has always been important in the world we live in, and it is even more important at a time like this. On the one hand, we have an economic impact on the market, but we cannot forget the social impact. There was a time when 100% of our team was working remotely, but now we are gradually returning, following safety policies. Regarding the donation, I reiterate the words of Midea Carrier's Marketing Director, Simone Camargo: “Midea Carrier is committed to collaborating to fight the disease in Brazil. With this donation, we hope to contribute to the rapid recovery of patients, mainly for the control of temperature and humidity, helping to ensure the comfort of the medical team and hospitalized people, in addition to minimizing the proliferation of infections.” AC: How do you see the future of business after the pandemic? Will processes remain the same? ST: I am absolutely sure that processes will change. This was not the first crisis, and it will not be the last. The difference is that the crisis brought on by the pandemic came as an avalanche, at a very high speed. It will not be easy for the world to return to normal. This will have a big impact on how people get back to work. Another point that will change is the face-to-face component of business: the world will get used to virtual meetings and events. Since we see that this new dynamic works very well, it will be difficult to return to the old model. Much is said about the \"new normal,\" and I believe that we will have positive takeaways from this moment that will be used for future improvements. AC: How has Midea Carrier faced the challenge of digital and Agile transformation? Is that part of the company's culture? ST: Midea Carrier has invested considerably in digital transformation, understanding that it adds value to our products and services. Projects like analytics, IoT, artificial intelligence, and others are on our radar and are already generating good results for the business. Our emphasis on product innovation is apparent in several models available in the market. In the production process, we are focused on implementing concepts from Industry 4.0. We expect to invest even more in digital transformation in conjunction with Carrier and Midea, which have considerable investments in the USA and China. AC: What is the culture of Midea Carrier in relation to partnerships? ST: We have a very strong model of partnerships and outsourcing. Various logistics, commercial, IT, and other operations are carried out by partners. We understand that the company needs to focus on core business and outsource other operations to reduce costs and improve services. In the IT area, we have a considerable outsourcing model, where services such as our datacenter, service desk, telecom management, developments, user support, and more are carried out by partner companies, with excellent results. AC: Could you tell us something you did that you are very proud of? ST: I am very proud of everything I have built in my career, as I have always acted with motivation and with pleasure in doing a good job. I believe that I have left a great legacy in every company where I had the opportunity to work. The feeling of leaving the places where I worked with my head held high and knowing that I did my best is formidable. This is my biggest motivation: making my mark, making a difference, and being remembered. AC: Thank you for your time today, Sandro! It’s a pleasure to hear about your career journey and your work at Midea Carrier.", "date": "2020-9-30"},
{"website": "Avenuecode", "title": "AC Spotlight - James Gregson", "author": ["Alfredo Moro"], "link": "https://blog.avenuecode.com/ac-spotlight-james-gregson", "abstract": "(The following article is property of Avenue Code, LLC, and was originally published with permission at Total Retail on December 14th, 2020. ) James Gregson , digital creative director at LEGO Group , reminds marketers of the importance of social media fundamentals in a rapidly changing economic and social climate. Avenue Code: Tell us about your personal career path. How did you get to where you are today? James Gregson: As someone who was not academically oriented, my career path was non-traditional. Thanks to some strong parental guidance, I attended Syracuse University and learned, at a basic level, what I didn’t want to do. I studied advertising design but quickly switched to computer animation with a minor in painting. Hardly transferable skills, but my takeaway was that I was going to be more successful doing something I enjoyed. I started my professional career by working in the PR agency world,  in the early days of digital communications and bloggers. Right around the financial crisis of 2008, I decided to start my own consultancy. I did everything from designing websites to core social media support, eventually working for LEGO as a consultant, which led to a full time job. AC: Has your role at LEGO evolved? JG: I was originally hired as a senior manager to operationalize the global to local social media setup. My core competency is creativity, which allowed me to ideate new solutions to diverse challenges related to operations, processes, workflows, etc. My role has since evolved and is now 100% creative, as I lead the internal creative agency America’s team responsible for organic social content. AC: What are you personally most passionate about in your career? JG: I find joy and an outlet for my creativity in problem solving. Social media gives you the opportunity to be very close to the data, empowering you to identify insights and produce solutions that receive immediate feedback. Problem solving at an operational level is interesting, but it’s less reactive than social. AC: How do you use data to drive your marketing strategy at LEGO? JG: Data on content performance is the key to successful creative optimization. Content must deliver at or above objectives. That’s the beauty of social - you can be very data and insight driven. Instinct may inform social strategy, but data defines right and wrong. AC: Why do companies fail at social? JG: We fail when we over complicate things and deviate from fundamentals like strong audience-first content. The best content is not always the content that costs the most money to produce. Some of the brand’s top social content literally took minutes to create. One challenge is that social media has become so pervasive that everyone within the organization wants to use it to broadcast their message, but there needs to be a prioritization and ROI assessment for every message. This is why it’s important to respect the expertise of social media teams who understand content performance. AC: What new challenges and opportunities has COVID-19 created for LEGO’s marketing strategy and for retail marketing as a whole? JG: We saw a huge increase in new adults buying LEGO products. While COVID-19 has devastated entire industries, two takeaways are clear. First, if you didn’t have a digital e-commerce platform before COVID-19, you’re probably struggling, no matter which industry you’re in. Second, the impact of COVID-19 on physical events is tremendously challenging for everyone. User habits have shifted dramatically, and marketers have to review and revise where and how they’re reaching their target audience. AC: What are your thoughts on corporate involvement in social hot topics like climate change and Black Lives Matter? JG: I’m lucky to work at a company whose core value proposition is being purpose driven. Everything from our sustainability goals to campaigns like LEGO Replay reflects that LEGO is about more than selling products. Purpose-driven marketing isn’t new, but what has evolved in 2020 is that if a company says they are going to do something, they have to follow through and deliver on those intentions. They must also get feedback from every demographic and psychographic within the business to ensure their intention doesn’t reflect a singular mindset. Conscious, purpose-driven marketing is table stakes now. AC: What has been a highlight for you in the last few years? Was there a moment, either for you personally or for LEGO, that you knew you were on the right track? JG: My highlight is having a career I love, because I know how difficult that is to achieve. As a child, I was a huge fan of LEGOs, so it’s incredible to be contributing to LEGO now by moving the brand into new areas. AC: Thanks for sharing your insights on best social media marketing practices, James!", "date": "2021-3-9"},
{"website": "Avenuecode", "title": "Design Thinking: Beyond the Buzzword", "author": ["Holly Camponez"], "link": "https://blog.avenuecode.com/design-thinking-beyond-the-buzzword", "abstract": "Design Thinking . We’ve all heard it, we all nod along when it’s mentioned, and we’re all guilty of tossing it into our conversation as if it were some particularly zesty salad topping capable of transforming the humblest romaine-and-vinaigrette of ideas into a gloriously robust, stand-alone feast of strategic business plans. But what actually is it, and why do we need it? The truth is that Design Thinking, roughly defined as a user-centric philosophy and set of principles, has rightly earned its buzzword status due to the immense value it’s been proven to generate for businesses across the spectrum of industries, geographies, target markets, and cultures. Just like Agile Methodology , there’s a reason it’s invoked at every turn - in a nutshell, because it works. Like all buzzwords and bywords, however, there comes a point at which the meaning is blurred and obscured. There are hundreds of articles readily available to explain, define, and sing the praises of design thinking, so this one doesn’t aim to add to the melee. Instead, at Avenue Code’s .Design division , we’ve decided that 2021 is the year of Design Being , and we’re focusing on how to get these principles into action to add business value for our clients. Below, you’ll find 4 practical applications of design thinking , along with a brief explanation of how they work. The only thing worse than a vague concept is a workshop about a vague concept, right? Well...not really. Our Design Thinking workshops are a whirlwind 1-3 days that gently force a cross-section of your company’s leadership from the C-Suite down to the humblest scrum master together to participate in hands-on activities and exercises to generate a value map for your product or service. Using design thinking principles, we tackle a concrete business challenge your company faces, and in the course of the workshop, we’ll facilitate discussions that generate assets including the following: When was the last time you felt that all relevant decision-makers in your organization were aligned on who you were creating something for, aware of technical dependencies and potential blockers, and agreed on what success would look like? If the answer is never, don’t feel bad - you’re far from alone. But also, don’t put off talking to us about a design thinking workshop, because this is a powerful tool that can set a project in motion in less than a week. Chances are you’ve heard of this methodology from Google that gives a whole new meaning to “fail fast, fail often.” Design sprints have become their own kind of trend, with endless variations and adaptations. At Avenue Code, we stay pretty close to the tried-and-true 5-day process, and we've become adept at running it remotely (thanks, COVID). Here’s how it looks : Imagine your company’s executive, operational, technical, marketing, sales, and product teams coming together for five full days dedicated to framing a business problem, collaboratively ideating on how to solve it, and then building, testing, and validating that solution idea. Often, teams who on day one can’t even agree on what the problem is or if it exists at all, end day 3 mutually agreed on not only the problem itself, but also the likeliest solution for it. Which is great! But as design thinkers, we have to continually come back to the users, so the most critical part of a design sprint consists of prototyping and testing the solution and gathering actionable data from actual users. The idea is that by the end of the week, you have not only agreement on the problem and a host of ideas for solutions, but also a go or no-go criteria for the particular solution being test-driven in the design sprint. The average cost of an IT project is somewhere around $167M , with most taking several years to complete ( Harvard Business Review ). It’s a huge risk to undertake a project anywhere near that expensive and time-consuming without some kind of validation as to whether or not it will actually solve the problem as intended. Design Sprints do the trick admirably and are well worth the small investment of time. Imagine this scenario: your mobile application has rave reviews, 5 stars in the Apple store, and record download numbers. But your customer service team reports high customer turnover, and the rate of referral-based customer acquisitions has been dropping over the last quarters. Something isn’t adding up. While it may sound like something out of a business school mystery novel, it’s actually not all that uncommon. Most businesses tend to operate in silos and focus on the metrics surrounding individual customer touchpoints - mobile applications, web interfaces, call center satisfaction, etc. But customers, whether individual consumers or businesses, don’t see your company like that. They’re looking more broadly at the overall experience your company provides - or in design terms, the customer journey. Service design is the discipline of examining your customers’ journeys from the moment they glimpse your logo to the final day of their contract and everything in between. Service design isn’t new - in fact, it’s been around since the '80s. For whatever reason, it’s gone out of vogue, but we’ve found that it’s still one of the most powerful and effective disciplines to uncover institutional blind spots, increase customer loyalty, and significantly grow your NPS . We generally run an initial service design engagement with 2 experienced consultants for 8-12 weeks depending on the complexity of your scenario. At the end, you’ll have a holistic view of your customers’ journey(s) through their eyes, as well as a deeper understanding of where gaps exist between their expectations and the touchpoints as they currently exist. You’ll also understand where there are dependencies and how to create seamless experiences for your customers while reducing friction internally. We highly recommend that our clients continue to invest in service design, because the payoff is always there. So as part of our engagement, we can also help to build out a team of service designers to keep the practice going long after the initial engagement ends. So, you’re sold on design thinking principles as a way to inform strategy, and you want to ensure that your own design organization is equipped to implement the types of practices we’ve been discussing above. Not sure where to start? Let’s get really meta for a minute here - one of our most popular design service offerings is about designing how you design within your organization! That means everything from interviewing various groups to identifying key pain points and opportunity areas to helping you define your mission and vision, creating success metrics, cultivating a design culture, and collaboratively building the roadmap to get there. Because this offering is totally customized for each client , the focus may look a little different depending on the outcomes of the situation analysis. But what it always has in common is that it’s built to set you up for success as a design thinking-driven organization in the future, with efficient design processes, a working design system, a thriving design culture, and a clear mission, vision, and roadmap. Design Thinking as a philosophy has hundreds of applications. As we at Avenue Code’s .Design division continue to explore our commitment to Design Being, we’re always uncovering new ways of engaging with our clients. We hope that you enjoyed reading about a few of our more popular service offerings , and we’d love to hear about what’s worked well at your organization! Please leave your thoughts below and don’t hesitate to reach out if you’d like to talk about these or other types of services we offer.", "date": "2021-1-20"},
{"website": "Avenuecode", "title": "Microservices Architecture As A Large-Scale Refactoring Tool", "author": ["Rafael Romão"], "link": "https://blog.avenuecode.com/microservices-architecture-as-a-large-scale-refactoring-tool", "abstract": "It's understandable that the same software would not be able to run in different environments, which is why software architecture has evolved along with software. This evolution is a direct result of the continuous change occurring in the world. As businesses and markets have changed, the loads that software systems are able to handle have had to steadily develop as well. In this context, Microservices Architecture is a way of organizing components of a software system that allows one to avoid many of the common problems a monolithic enterprise application can present today. Monoliths are single process, server side applications that preserve all the business logic. In the best case scenario, they may split this logic into large services, each one entangled with many responsibilities. There are several reasons why an application becomes a monolith, and it's important to understand them in order to grasp why they tend to remain monolithic. It may seem obvious, but the first reason why applications become monoliths is due to the lack of separation of concerns in the design of the application. In most applications, the business logic of different parts of the application complement one another in some way, whether they share the same database or are developed and maintained by the same team. Therefore, it is natural that they end up as an entangled mass of code that is difficult to separate. It's no easy feat to implement the separation of concerns. On a basic level, it requires a lot of work to design the software components independently of one another. Principles like GRASP and SOLID can help, but they have to be put into practice by the development team and then continuously enforced by an architect or tech lead. On a higher level, the architecture of the whole system must be carefully thought out in order to allow the different software components to be separately developed, deployed, and maintained. If this effort is not prioritized, the separation may flounder due to lack of funding. If the components of the software are physically separated and independently deployed, they will have to exchange data between them. This is usually slower than having the data shared in the memory of a single software process, but today we have faster computers, faster networks, and the cloud, so this concern is not as prevalent as it was in the past. Historically, however, it has led -and still leads - many developers to avoid splitting a software into separate processes. Another common concern is code duplication. If the code is split among different programs, part of the code will end up being duplicated - something that should be avoided whenever possible. The same code must then be modified in different places when the logic changes. Although there are options to move this shared code into a separate library, it can easily become another maintenance burden on the developer's shoulders. Finally, separate software components without separate databases won't be enough to avoid the creation of a monolith. The main point of entanglement in a software system is typically the database, and splitting the database usually implies data duplication. It goes against many of the principles we have learned in our database classes. What about normalization and transaction consistency? With all of the above obstacles and concerns in mind, why is it necessary to modernize monolithic applications? The main reason for modernizing monolithic applications is that software systems need to be able to respond to change. They need to adapt to and evolve with the business, because the more time we spend changing a system to conform to a new business requirement, the more revenue is potentially lost. So, what should we do if we can’t respond to change in a monolithic application? Should we throw it out and rebuild from scratch? The big problem with monolithic applications is that a change in one place usually requires changes in other places as well. It's not uncommon for these changes to occur in areas completely unrelated to the area that was modified. After each change, the application has to be tested again in order to ensure that everything is functioning correctly as well as to check whether or not the whole system needs to be re-deployed. Two excellent resources that detail what a well-distributed enterprise application consists of are the Heroku 12 Factor App Specification and The Reactive Manifesto. At the same time, we can’t just throw our legacy system away. If we have a functioning monolith, as bad as it might be, we probably can’t afford to rebuild something that most likely took years to become as valuable as it is now. The solution I propose is to rebuild parts of the monolith as Microservices applications, allowing them to co-exist and integrate with the remainder of the legacy monolith. Here are some of the advantages we can obtain with this approach: In a monolithic application, we are usually tied to the technologies used during the beginning of the application's development. Evolution is possible up to a point, but even then we are still tied to runtimes, frameworks, and languages. Consequently, we are often equally dependent on teams who are familiar with those technologies. With new and refactored functionalities hosted in separate services, we have a greater guarantee that if something was implemented incorrectly in a certain service, we would only need to fix that specific service. The error would also be easier to rewrite from scratch if hosted in separate services than if it were part of a monolith. This modularity also allows us to deploy it independently from the rest of the system. We can host it in an environment like Kubernetes , for example, and have it autoscale . Software systems eventually fail - an unfortunate fact that cannot be ignored. If we have a huge monolith that is responsible for everything, a small failure could cause the entire system to shut down. Even with a fallback plan, the error could be propagated into the fallback plan - or at the very least, cause some downtime. But by utilizing independent services, we can count on the robustness of our system in case of failure. We are able to structure our system so that only the service that broke will need to be re-started. Requests to that service can be redirected to other instances, or queued up for later processing. A Microservice is a component of a software system that can be developed and maintained independently. A Microservice can accomplish a single job successfully, but the integration of multiple Microservices is usually necessary in order to reach the end goal. The term Microservices was introduced to highlight the fact that it’s focused on a single responsibility or service. The main difference between a Microservice and a traditional Web Service is the fact that a Microservice is small with regards to the functionality it provides. While a Web Service can expose several endpoints to perform different kinds of computations, a Microservice will usually only expose a few endpoints for a single kind of operation. For example, we can have an Identity and Access Web Service responsible for all identity and access operations, from authenticating a user to fulfilling a user profile. With Microservices, by contrast, we can have an independent Microservice solely for authentication and authorization purposes while another Microservice can be developed to manage user profiles and registration. It might seem futile to separate these two services, but if we consider the fact that we might receive several authentication and authorization requests throughout the day, but only a few new user registrations, the independent scalability feature of Microservices can become a significantly beneficial aspect. Using Microservices to refactor large scale enterprise applications is not simple, but it is a practical alternative to working with a brittle monolith unable to quickly assimilate to simple changes. In order to refactor a monolith into a set of Microservices, we have to find seams in the code where we can move the functionality to a separate module. Usually, this can be done by replacing a local function call with a remote call using a tool such as a REST operation. Whenever the system calls this functionality, it will call the separate Microservice instead of the old legacy code. Once the system is not dependent upon that legacy code, it can be removed, resulting in a slightly thinner monolith. New features are also added to the system from time to time, which creates a good opportunity to delegate new functionalities to new Microservices, as opposed to inflating the monolith with more code. Some years ago, I had the opportunity to work on the process of migrating a huge monolith to the cloud . The monolith itself remained relatively the same, but was moved from a local datacenter to a cloud environment. Most of its new functionalities are now provided by separate services running in this same environment. However, they now utilize technologies that were not available decades ago when the first version of this software went into production. Here are some important aspects that we need to take into consideration when using Microservices to refactor a monolith: The first thing to consider is that the refactoring must be incremental. We should not refactor the whole system and promote it to production when it is done. Instead, we should use the opportunities we have to refactor small pieces of the system, since this approach allows us to limit risk. Ideally, we should start with a new functionality, then build it as a separate Microservice so we can put it into production and observe. The integration of Microservices with the rest of the system is also important. The preferred method would be to use REST calls from inside the monolith, but the less preferable method of database integration could also be a valid start if the monolith proves to be difficult to change. Another crucial aspect is data management. A majority of the time, splitting a monolith into Microservices equates to splitting the database that this monolith consumes. The ideal solution would be to avoid sharing the same database amongst different applications to avoid data duplication. Attaining consistency means the ability to have multiple copies of the same data that can sometimes be inconsistent, but will eventually become consistent again. At a first glance, one of the reasons a monolith becomes a monolith is because it's harder to maintain separate multiple services as opposed to a single one. It's definitely a challenge to build and keep multiple Microservices healthy. In order to do so, architectural governance is essential. It's not possible to simply build and deploy without considering all the nuances of the Microservices themselves. Microservices afford great adaptability, along with the ability for software to evolve along with business and technology, but it's not an easy process. The final aspect I’d like to point out is the cost/benefit analysis.. Before choosing a functionality of the monolith to be rebuilt as a Microservice, we need to carefully evaluate whether or not we really need to do so, or if it's better to just leave it untouched within the legacy and look for other things to refactor. Building a new Microservice also means incurring all the risks of building software without the visible benefits in comparison, so it must be undertaken judiciously. By using a Microservices Architecture, we can build large scale, modular software systems with each component having a single responsibility. These systems can be developed by small, independent teams, which allows us to use the best technology for the job. Furthermore, each component can be independently deployed and scaled so that if a certain module needs to handle a heavier load, the system can count on multiple components to work in parallel. It's also easier to improve or even rewrite a single component without affecting the other components of the system, and to adopt new technologies as they become available. Traditional monolithic applications are brittle and difficult to change, but we can combine them with Microservices to replace each single functionality of the monolith. This will allow us to gradually rebuild the system utilizing a modern and sustainable architecture, allowing it to evolve alongside the business and technology. This strategy will allow the the system to evolve while minimizing the costs and risks. This gives us the possibility to try out new technologies, new approaches to old problems, and new data management options. Want a deeper look at how Microservices Architecture can refactor your monolith? I explore the subject in greater detail in my whitepaper - give it a read and let me know what you think!", "date": "2017-9-28"},
{"website": "Avenuecode", "title": "Is Behavior Driven Development Right For You and Your Team?", "author": ["Lucas Schmoeller"], "link": "https://blog.avenuecode.com/a-software-engineers-thoughts-on-behavior-driven-development", "abstract": "Behavior Driven Development, also known as BDD, is a software development process that can be utilized to help your team deliver value and build more reliable software. As you may have guessed, BDD incorporates various aspects from Agile and lean practices, and more importantly, TDD and DDD . Just like in TDD, BDD uses tests to help specify, design, and verify application code, therefore reducing the the defect count and improving code maintainability. But what makes BDD different from TDD? BDD differs from a simple unit test because its tests focus on the actual behavior of the system based on predefined scenarios, making the tests easier to understand and more meaningful. This results in the production of higher quality code. So, why should you start implementing BDD? Here's a quick list of reasons why: So, should you use BDD? Maybe! It depends on the team's knowledge, the company culture, and the team's openness to change. However, also keep in mind that you can always apply certain BDD practices to improve your quality of work!", "date": "2017-10-31"},
{"website": "Avenuecode", "title": "Avenue Code Named Global Leader in Tech and Culture", "author": ["Anna Vander Wall"], "link": "https://blog.avenuecode.com/avenue-code-named-global-leader-in-tech-and-culture", "abstract": "Avenue Code wins awards and emerges as a tech leader enabling the global workplace evolution. Global software consultancy Avenue Code was recently recognized with three awards for the technological and cultural growth that led the company to assume a new level of innovation leadership during 2020’s worldwide workplace evolution. The consultancy was distinguished as a Largest San Francisco Tech Employer by San Francisco Business Times, a Best Place to Work by Built In San Francisco, and a Best Company to Work For by Great Place to Work Minas Gerais in Brazil. Founder and CSO Zeo Solomon made it clear that these accomplishments signify a social responsibility: “Once the world began its dreadful quarantine, we realized the importance of our role in enabling the new work paradigm. Our technology provided the backbone that allowed the world to continue operating in a much-needed distributed model. We were honored to be a part of the solution and worked tirelessly in fulfilling our role in the pandemic.” As part of that solution, Avenue Code added delivery centers in Amsterdam and Berlin to supplement its distributed workforce throughout the US, Canada, and Brazil. The consultancy’s technical acumen is supported by a strong internal culture that prioritizes connections among team members globally. Danielle Borges, Global HR Director, explained that “Working remotely introduced a new challenge for our culture. We overcame this by becoming even more connected through online events and training sessions, weekly virtual stand-ups, global fun days, and our peer-to-peer Buddy Program, achieving healthy growth without any negative impact on our beloved culture.” Avenue Code executives affirm that this focus on a relationship-building culture is what enabled the consultancy to form strong relationships externally, cultivating many new clients worldwide. Avenue Code ranked Best Place to Work by Built In SF Avenue Code will continue to assume leadership in technology and culture in 2021. Recognizing a gap in the marketplace for seamlessly adaptive global management platforms, the consultancy plans to release its first PSA product suite in 2021. Avenue Code is also heavily investing in diversity initiatives such as the Extraordinary Women in Tech talk show and upcoming peer-to-peer conference. “Working with tech executives for more than 15 years, I recognized that less than 5% are females,” said Ulyana Zilbermints, Global VP of Business Development. “In 2020, Avenue Code started a movement to spotlight female tech leaders and amplify their voices. As a global technology company, we’re proud to provide female leaders a space where they can connect with other women all over the world.” Official announcements on Avenue Code’s new product suite and global Extraordinary Women in Tech conference will be released soon.", "date": "2021-3-30"},
{"website": "Avenuecode", "title": "AC Spotlight - Adrien Nussenbaum", "author": ["Holly Camponez"], "link": "https://blog.avenuecode.com/ac-spotlight-adrien-nussenbaum", "abstract": "This week we got to talk with Adrien Nussenbaum, co-founder of Mirakl. Mirakl enables retailers and brands to launch their own online marketplace. They help companies quickly offer more products, learn more from customers about what they want, and consequently sell more. The Mirakl Marketplace Platform automates the hard things about marketplace management on an API-based solution, trusted by over 100 customers. Avenue Code : Adrien, thanks so much for speaking with us today! With your work at Mirakl, it’s clear that you have a pulse on trends within commerce. What are some of the biggest changes you have observed in consumer behavior in the last 5 years? Adrien Nussenbaum : I think that beyond all the transformation technology has brought to retail - virtual assistants, personalization, etc. - any answer to this question would be totally incomplete if it didn’t start by recognizing the fact that Amazon has completely redefined retail. Despite some great innovation we’ve seen from individual retailers, the reality is that Amazon alone took 53% of all online sales in the US last year. They have massive market share, and to put it simply, they’ve become the source of all truth for shopping. If you’re looking for something, you know you can find it at Amazon, and you know you’ll get fast delivery. All of this is really possible because of the marketplace of 3rd party sellers Amazon has built, all of whom can fulfill orders and supply catalogs. The key change here is to a model where instead of driving, walking into a mall, and finding interesting products, the consumer is going to find their purchase via a distribution system online. Avenue Code : You started Mirakl just over five years ago. Can you tell us about how it fits into the new model, and what gaps you hoped to fill? Adrien Nussenbaum : Sure! When we started Mirakl, we were keenly aware of the risk for retailers of falling short. There are so many critical elements nowadays to capture and maintain consumer interest. If you don’t have the right selection online, an extensive selection, consumers who prepare for their shopping experience online and don’t see what they’re looking for will not take the steps to show up physically. They lose interest with the brand. We realized that the kind of situations in which retailers are missing sales and falling short of converting visitors is largely due to a shortage of product selection and complementary products. The flip side is that this is also a huge opportunity for retailers. If you can craft a product selection and a product universe that satisfies consumers, you can go leaps and bounds further. At the end of the day, this is accessible to retailers via a marketplace strategy. The creation of Mirakl was driven by the fact that in some previous work with a retailer on the edge of bankruptcy, we had been able to save them through the implementation of a marketplace strategy. We knew that by creating a platform that enables exposure to additional business, we could enable other retailers to take advantage of the same opportunities. Adrien presenting on a marketplace solution for retailers. Avenue Code : What’s the biggest obstacle you face at Mirakl? Adrien Nussenbaum : Whenever you sell real innovation, whether it’s a technology that redefines the world, as for instance, electric cars, or whether it’s a technology that drives new ways of doing business, such as what Mirakl does, the biggest obstacle to overcome is always fear. At the end of the day, you sell to human beings who have a job and don’t want to lose it. So when you’re selling disruption, it’s a lot tougher than selling technology that simply modernizes an existing profit center. The fear on our part is that people will be reluctant to take the risk. Even 5 years into our journey as a company, a huge part of what we do every single week is look for new people to evangelize, to reassure, and to educate on the larger trend of what’s going on in their industry. But the good news is that this can be overcome. We face our clients’ fears head on by doing our homework, and putting their needs in parallel with our own, and mostly by showing with concrete results the value we bring to the table. We have very immersive workthroughs that we deliver to our prospects that can help them project and sell. Obviously the peer referencing helps a lot too. Avenue Code : What is the key to success in today’s retail environment? Adrien Nussenbaum : It’s realizing that retail has fundamentally changed. Consumers used to come to you, and now you need to be where they are, and you need to be appealing to them. The power has shifted to the consumer, which changes everything. I look at it from the point of view of what marketplaces bring. Marketplaces are the only economy where it is viable to bring the breadth of products, the pricing, the service level, and the responsiveness that consumers expect. Of course, there are more implications. But the most important takeaway for me is that if you don’t have a very unique value proposition, it’s incredibly difficult to succeed in a commoditized industry. This applies to houseware, sporting goods, electronics...no matter what sector you’re in. Avenue Code : You just announced your new marketplace platform for services earlier this year. What excites you about the new step? How will it change the way we do business? Adrian Nussenbaum : This new offering really falls directly into the vision we have for how to make retailers more relevant. More and more, customers don’t necessarily want to shop for products - they want to shop for solutions. It can be anything. One hour delivery, the overall experience, etc. So having a platform for services makes sense because that’s a fairly critical component of offering comprehensive solutions for consumers. For instance, let’s say you’re in the market for a camera. This a totally commoditized market - you can easily shop around and research and find the best camera at the best pricepoint. But now imagine there’s a retailer who can offer you bundled services - you get your camera, but you also get access to a cloud-based photo printing service, photography lessons, and an overview tutorial. Suddenly, it makes a lot more sense to buy from that retailer than from another. Avenue Code : Fascinating stuff, Adrien. It’s clear that Mirakl is on the front lines of making innovation a reality for many retailers. We’ll look forward to seeing more from you folks soon as the products and services categories are brought together as full solutions!", "date": "2017-8-8"},
{"website": "Avenuecode", "title": "Reactive Streams And Microservices - A Case Study", "author": ["Mauro Moura"], "link": "https://blog.avenuecode.com/reactive-streams-and-microservices-a-case-study", "abstract": "In this post, I will go over a real example of how developers can use Reactive Streams to build more responsive Microservices. I will also conduct a progressive analysis on the steps that need to be taken in order to make the change from an application using Spring MVC to one using Spring WebFlux and RxJava, as well as how to apply reactive programming and work with streams of data. “Reactive Streams is an initiative to provide a standard for asynchronous stream processing with non-blocking back pressure.” reactivestreams.org1 While most developers are familiar with the concept of stream processing, the understanding of back pressure is essential to Reactive Streams. When developing message driven systems, it is common to use a \"push\" relation between consumer and producer. The producer sends messages to the consumer as soon as they are available, and if the consumer is not able to process these messages in real time,  they are stored in a  buffer. One point of concern with these types of systems is dealing with the excess amount of messages in the buffer when the producer is faster than the consumer. In real systems, neglecting this scenario often leads to slow response times for clients, excessive resource consumption, and even downtimes. Back Pressure solves this problem by using a 'pull' relation between the producer and consumer. Now, the consumer informs the producer on how much data it can process, and the producer only sends the amount of data requested by the consumer. Reactive Streams provides a standard interface for this relationship between consumers and producers of data. In Java 9, Reactive Streams is accessible through a set of interfaces in the class Flow2 at the package java.util.concurrent. When an instance of Subscriber is passed on to a Publisher, it will receive a call on its method, onSubscribe(Subscription), but it will not start receiving any events yet. Items will only be received by the Subscriber when it calls the method request(long) in its Subscription, signaling a demand for new items. The Subscriber can receive data through three distinct methods: - Subscriber.onNext() is invoked with the next item and can be called n times, with n being the long value passed on the method request(long) of its Subscription. - Subscriber.onError() is invoked when an error occurred while processing the stream. - Subscriber.onComplete() is invoked when there is no further data to be processed. In the case of onError() and onComplete(), no new data will be emitted, even if the method request(long) is called again. Developers can implement these interfaces directly into their projects but should be aware that the implementation of back pressure in asynchronous systems is a complex task3. Instead, they should use some of the available implementations of Reactive Streams like RxJava4, Reactor5, and Akka Streams6.  The examples in this article were created using RxJava. RxJava is one of the implementations of the Reactive Streams API in the JVM. It is a very lightweight library composed of a comprehensive set of operators, allowing the manipulation and composition of streams using a fluent API. RxJava also implements the Reactive Extensions7 API, an interface for asynchronous programming using the Observable Streams pattern available in many different languages. Spring WebFlux8 is a framework that provides better support for reactive programming for the Spring Framework 5, offering developers the ability to create servers and clients that support HTTP and WebSockets using a reactive stack. The Microservice is part of an e-commerce platform. All the information concerning the products sold online is stored in a Cassandra9 Cluster. Cassandra is a distributed  NoSQL database, generally used when scalability, high availability, and performance are needed when working with large amounts of data. The task of the Microservice is to receive a HTTP GET request with a list of product identifiers in the format of UUIDs, and return all product information to each corresponding UUID stored in Cassandra. During the development of the Microservice, changes will be made in order to make it more responsive, reducing the time it takes for the list of products to be made available to the client. Using SpringMVC, the code is pretty simple. First, the domain class Product is defined: Product.java: The Product class is a simple POJO representing the products stored in Cassandra. The annotation @Table maps this class to a table called products. The  @PrimaryKey annotation indicates that Cassandra stored the attribute id as a column named product_id used as the primary key of the table products. ProductRepository.java: The interface ProductRepository extends the interface CassandraRepository10 from the module Spring Data Cassandra11. This interface already has commonly used methods to access data in a Cassandra database like count( ), findAll( ) and findOneById( UUID ). ProductService.java: The class ProductService just calls the method findAllById( List<UUID>) from the bean productRepository and returns to the List of Products found in the database. ProductController.java: The ProductController exposes the HTTP GET endpoint that receives the list of UUIDs as a web request parameter, and returns a List of Products in the body of the HTTP response in the JSON format. The image below shows a simplified timeline of the application when a request is received: Now, the Microservice will be updated to use Spring WebFlux instead of Spring MVC. After adding the dependencies in the classpath, the first change needed is in the class ProductService: ProductService.java: ProductService now returns a  Publisher12, from the Reactive Streams API, instead of a List of Products. Flowable is the class in RxJava that implements the  Publisher interface. The method Flowable.fromIterable(Iterable) from RxJava creates a Flowable from a class that extends Iterable, in this case, a List. This method call will create a stream that will emit all the Iterable items to its Subscribers. RxJava offers a broad range of utility methods to create streams from  other classes like Array, Future, and Callable. ProductController.java: Spring WebFlux allows the ProductController to return a Publisher and handles the subscription to the returned stream. The Microservice is now using a reactive stack, but some improvements can be made to reduce the application response time. Cassandra is unique in that queries using the IN clause with a set of keys tend to perform poorly when compared to the use of separate queries by individual keys13. When using the IN clause, the node that receives the query will be responsible for coordinating alongside all the nodes that have data related to the query before returning the result to the application. Querying for the keys individually turns out to be more efficient, especially if using a TokenAware14 Java client that already knows which node contains the data by the primary key. A code change is necessary to reflect this optimization: ProductService.java: The method findById(UUID) from the interface ProductRepository returns an Optional, a container for values that may or may not be null. The Flowable.fromIterable(Iterable) method is still used to create the stream, but now with the  UUIDs provided as a request parameter as the source. From now on, RxJava fluent API can be used to apply modifications to the stream and manipulate the data. The first call is to the method map, which transforms the stream. The map function is used to query the database for every UUID in the stream, converting a stream of UUIDs into a stream of Optional<Product>. The method filter will only let items that pass a given condition to propagate further into the stream. Any empty Optional returned from the query is filtered, and the map method is used again to convert a stream of Optional<Product>  into a stream of Product objects. The timeline can now be updated to represent what is executed during a request to the Microservice: While the Microservice is now more efficient in getting the data from Cassandra, there is still room for improvement. The service makes the queries to the database in a sequential manner, but what if it executes the queries simultaneously instead? Considering a database that is under a normal load, the response time can be further reduced. That is where the concept of processing asynchronous data comes into play, using the RxJava API to make the requests to the database in parallel and coordinate the retrieval of data for the client. For every UUID, the Microservice will make an asynchronous query to the database, and when the queries are completed, merge their results and send the response to the client. First, another repository is created, called ProductAsyncRepository: ProductAsyncRepository.java: The productRepository will now use AsyncCassandraTemplate, a utility class from Spring to assist in the creation of asynchronous queries for Cassandra. The method findOneById( UUID )  returns a ListenableFuture, a class from Spring that extends java.concurrent.Future, a representation of an asynchronous computation. This change allows us to make the query to the database without blocking the main thread. To retrieve the result later, a call must be made to the Future.get() method, which will return the List of Products that matched the query. Next, the class ProductService is modified: ProductService.java: Flowable.fromIterable( List<UUID> ) still creates a list of UUIDs, but now the stream uses the operator flatMap instead of map. flatMap() receives a function that has the current stream item  as an  input and returns as a stream. The emissions of the streams returned are then merged and returned as a new stream. The stream is created with the method Flowable.fromFuture(productAsyncRepository.findOneById(id), Schedulers.io()). The first parameter is the result of the recently created repository method, and the second parameter is a Scheduler,  which is a utility class that abstracts concurrency in RxJava. By passing Schedulers.io() to the method, RxJava will then free the current thread and use a thread pool designed especially from IO-bound work to wait for the Future to be completed. The last operator in the stream is another flatMap, which converts the arrays of the streams in a stream of its items. Now, the timeline can be updated to match the changes made to the application: Throughout the process, it was possible to identify the changes in the code and behavior of the Microservice. The use of Reactive Stream increased the service responsiveness keeping the code readable. Although this was a simple example that only had a few RxJava operators and no error handling or back pressure, it gives a good idea of the ways one can use Reactive Streams in a Microservice. The final section will address FAQs that developers usually have about the subject. This is by far the most commonly asked question. The answer is always the same: of course, you can! A solution using only core Java would be easier to digest for someone without a background in functional programming. The only dependency of RxJava is the reactive-streams API since it implements everything using only core Java. Experience tells us that developing responsive applications is not always as simple as the example above. It is unlikely that responsiveness is your application’s only need. Scalability and high availability are also necessary. Usually, there is more than one query in the database as well as many requests to other services. When taking those factors into account, the application will need to use thread pools, Executors, and Future objects. The blend of all these technologies in big projects tends to create messy and disorganized code. This causes you to neglect performance; instead, you’re pre-occupied with worry over whether the next commit will break the system. Tests are much more difficult to implement now because you have so many variables that tuning your application has more to do with instinct than with logic. With the use of reactive-streams and RxJava, you get an implementation of Reactive Streams with many compositional patterns, testing utilities, and an open source community to help you achieve responsiveness, resilience, and scalability throughout all layers of your Microservice. That is typically matter of personal choice as well as an analysis of the requirements of the product you are trying to build and how comfortable your team is with the technology. The fact that Reactive Extensions are available in many different languages and that the Java version is very lightweight and can be used alongside other frameworks are points to consider. However, Reactor and Akka Streams are also great tools. Therefore, it is recommended that you experiment with both of them to see which one you are most comfortable using. The RxJava learning curve can be a little steep, especially if you do not have a background in functional programming and knowledge of functional operators. Debugging is more complicated because now you have RxJava operators wrapping your method calls, and you are using method references and lambdas. You will struggle with your IDE a bit in the beginning until you can determine the parameters and the return type of your streams. Like every technology, RxJava and Reactive Streams are not always the best tools to use to solve every problem. They are an elegant way to work with asynchronous streams of data, but you should definitely analyze your system requirements to determine whether it is a good fit for your problem. Citations 1 http://www.reactive-streams.org 2http://download.java.net/java/jdk9/docs/api/java/util/concurrent/Flow.html 3https://github.com/ReactiveX/RxJava/wiki/Implementing-custom-operators-(draft) 4https://github.com/ReactiveX/RxJava 5https://projectreactor.io 6http://doc.akka.io/docs/akka/current/java/stream/ 7http://reactivex.io 8https://docs.spring.io/spring-framework/docs/5.0.0.BUILD-SNAPSHOT/spring-framework-reference/html/web-reactive.html 9http://cassandra.apache.org 10https://docs.spring.io/springdata/cassandra/docs/current/api/org/springframework/data/cassandra/repository/CassandraRepository.html 11https://projects.spring.io/spring-data-cassandra/ 12 This example uses Java 8, so the Publisher is the one available in https://github.com/reactive-streams/reactive-streams-jvm/tree/v1.0.1 13https://www.datastax.com/dev/blog/java-driver-async-queries , Case study: multi-partition query, a.k.a. \"client-side SELECT...IN\" 14 http://docs.datastax.com/en/drivers/java/2.1/com/datastax/driver/core/policies/TokenAwarePolicy.html", "date": "2017-9-14"},
{"website": "Avenuecode", "title": "Understanding the Underlying Principles of Smart Contracts", "author": ["Rodrigo Sekimoto"], "link": "https://blog.avenuecode.com/understanding-the-underlying-principles-of-smart-contracts", "abstract": "In my last post, I gave a brief introduction on Blockchains , and how it's all the hype because of its ability to completely eliminate the middle man during transactions - all you need are the two interested parties and a few lines of code to help validate the trade. These seemingly simple lines of code can actually be deceptively complex. If they have the ability to seal the deal between the two parties, why not refer to them as Smart Contracts? The basic idea of Smart Contracts is that the code generated by the developers will attempt to validate the transaction, note that I say try, because this code was generated by humans, which means it's probably not completely bug-free. With a Smart Contract, certain conditions are pre-defined, and the contract can only be triggered and start a transaction if that set of conditions are met. Basically, if X happens, then so does Y - a pretty straight forward set of instructions. The concept of Smart Contracts was first conceive d in 1993 by the computer scientist Nick Szabo, and he described it as follows : \" New institutions, and new ways to formalize the relationships that make up these institutions, are now made possible by the digital revolution. I call these new contracts \"smart\", because they are far more functional than their inanimate paper-based ancestors. No use of artificial intelligence is implied. A smart contract is a set of promises, specified in digital form, including protocols within which the parties perform on these promises. \" Smart contracts vary from one another, but the big unifying factor is that all these contracts are autonomous, and can be triggered by different sources, but will always process the transaction the same way as previously defined. The following code, written in Solidity, was taken from the example provided by Ethereum : contract MyToken { /* This creates an array with all balances */ mapping (address => uint256) public balanceOf; /* Initializes contract with initial supply tokens to the creator of the contract */ function MyToken (uint256 initialSupply) { balanceOf[msg.sender] = initialSupply; // Give the creator all initial tokens } /* Send coins */ function transfer (address _to, uint256 _value) { require (balanceOf[msg.sender] >= _value); // Check if the sender has enough require (balanceOf[_to] + _value >= balanceOf[_to]); // Check for overflows balanceOf[msg.sender] -= _value; // Subtract from the sender balanceOf[_to] += _value; // Add the same to the recipient } } A mapping is an associative array, where you associate addresses with balances. The addresses are in the basic hexadecimal ethereum format, while the balances are integers. function MyToken ( uint256 initialSupply ) { balanceOf[msg.sender] = initialSupply; // Give the creator all initial tokens } This first function creates all initial tokens for this contract. This is a special, startup function that runs once and once only when the contract is first uploaded to the network. So far, you've got a contract with an initial supply: /* Send coins */ function transfer (address _to, uint256 _value) { require (balanceOf[msg.sender] >= _value); // Check if the sender has enough require (balanceOf[_to] + _value >= balanceOf[_to]); // Check for overflows balanceOf[msg.sender] -= _value; // Subtract from the sender balanceOf[_to] += _value; // Add the same to the recipient } And now, you have a function that transfers tokens from one account to another given the condition that the sender has a high enough balance. Your smart contract can have multiple different triggers that start a transaction, ranging from a specific date, to the temperature of a room. If you'd like to see what Solidity and a Smart Contracts look like in the Ethereum network, make sure you check this out, or, if you'd like to try it out on the Ethereum test network, then you can follow this very detailed tutorial by Pete Humiston . In this article, we used Ethereum as an example, but the underlying principle of Smart Contracts can be applied to other blockchains that allow the usage of such features such as RSK sidechain, NEO, ADA, and many others. Smart Contracts can be autonomous, and you can even make Smart Contracts to support other Smart Contracts. So, hopefully I've been able to help clarify the enigma that is Smart Contracts. I'm curious to know what your thoughts on Smart Contracts are. Leave them in the comments below!", "date": "2018-1-31"},
{"website": "Avenuecode", "title": "Camel and Wicket Playing Together", "author": ["Luis Felipe Volpato Alves"], "link": "https://blog.avenuecode.com/camel-and-wicket-playing-together", "abstract": "Imagine this scenario: you're responsible for integrating your company’s system with an old system from one of your clients. Every day, just when you think you've achieved it, the client’s system crashes and you have no idea why. You are so disappointed that you don’t even want to get a drink with your friends anymore, you just want to stay home thinking of ways to solve the problem. Well, that was my story - until I found Apache Camel and Apache Wicket. Apache Camel is a lightweight integration framework. Sometimes, the facing problem is not just about integration, but about showing the user results as well. Apache Wicket is a web component-based framework on which you can write web pages very easily using only java and html. In this article I will show you how I used these two amazing tools to solve my problem, get my life back on track and start drinking beer again. Really. They're that good. Getting started To start, I'll use Maven to manage the dependencies. Creating an application in Wicket using Maven is pretty easy. I just needed to find the correct archetype ( this page helps a lot), and I’m ready. Running this command will create a new project for me, and I can import it into my favorite IDE as a Maven project: I selected the options following this picture: This step could take a while, since it will download some dependencies. To test the recently created application, I ran the com.avenuecode.Start class as a Java application and access http://localhost:8080 in my browser. The result was: Ok, I have a web application working! Now it’s time to integrate it with Camel. To use the best of the two frameworks, I’ll need a third and well-known framework: Spring. Wicket came with its own Spring version, but I faced some compatibility issues here, so I just excluded this dependencies and used a compatible version. In this example, using Wicket version 8.0.0-M2 and Camel version 2.16.0 , I’ll use Spring version 4.3.3.RELEASE . Adding Spring dependency and removing the Spring itself in pom.xml (I need this dependency because Wicket contains specific code for Spring, we just need another version): To add the dependencies for Spring: Camel dependencies: To better organize our pom.xml , we’ll add these properties: Saving pom.xml will organize the dependencies in the IDE. In case this doesn’t happen automatically, you should update the project inside the IDE). A good way to see if it's working is to run the com.avenuecode.Start class again and access http://localhost:8080 . I saw the same page, so I’m good. The dependencies are fine, but I needed to do a few more steps to really use Spring in Wicket. First, I needed to create the file applicationContext.xml in WEB-INF folder. This is what it looks like: The content for applicationContext.xml is something like this: I needed to add this content to web.xml as well: Last step is to “turn on” Spring in our Wicket application, adding this line in init() method in com.avenuecode.WicketApplication : Now, how can I tell if it’s working? As I added com.avenuecode.bean in the component-scan, Spring will search in this package for Spring beans. So I can create one bean to test it: And then change the com.avenuecode.HomePage to use this bean: Note that HelloBean is annotated with @SpringBean . This is the annotation that Wicket uses to use the beans in Spring. Now let’s check back and see if it’s working. I ran the com.avenuecode.Start , and looked at the bottom of the page. It’s showing \"Hello, Spring!\" instead of the Wicket version, and Spring and Wicket are working well together! Ok, Wicket and Spring are running. Now what about Camel? To setup Camel, I needed to add a camel-context.xml file in the same folder as applicationContext.xml , WEB-INF . In this file, I’ll tell Camel where the folder is to scan that contains the routes to execute. Here is an example: To configure Camel in Spring, I just added <import resource=\"camel-context.xml\" /> into applicationContext.xml . To see everything working (at least to see it working on the console), I also changed the root level from WARN to INFO inside log4j2.xml , like this: The last step was to create a Camel route to move from a specific directory to another one. Running com.avenuecode.Start again, I could see Camel in action. Then, I moved a file to the directory src/data . The file was moved to target/messages , and the console displayed the message: Message received! . File in src/data folder: File should be moved to target/messages : And the console: The problem is pretty similar to what I did before now. The system receives a file in an ftp server and needs to move it to a local folder to be processed. But to make it easier to show, I’ll continue working with local folders, using src/in and target/out . I’ll create another route to process this file, and store the status in memory so I can show it. To store the status, I can simply create a Spring bean: And the route: Running com.avenuecode.Start showed in the console what’s happened. If I put a file inside data/in , it will show “Message processed!” and the status will be changed in memory. But I can’t show the console to the user, right? I used our initial page, just changing some code in com.avenuecode.HomePage : And the HomePage.html file: Running com.avenuecode.Start and accessing http://localhost:8080 showed this page: The status UNDEFINED is because the system hasn't processed any files yet. I can create an example file, called file1.txt with random content and move to src/in , and access http://localhost:8080 again. After moving to src/in , I can check the status. The file was received successfully and I didn't have any errors! Creating a file2.txt with some error content, I could check the whole power of the integration! And now I can check if there are any errors in the integration whenever I want! :D It didn't take much effort to integrate Camel and Wicket. It’s a simple integration with fantastic results! Try it yourself! This was just one example, but hopefully it conveyed an idea of just how useful these tools can be. The code used here is available in Github for further reference. Enjoy! :-)", "date": "2016-12-14"},
{"website": "Avenuecode", "title": "Simple Product Creation in Oracle Commerce Cloud", "author": ["Marcelo Guimarães"], "link": "https://blog.avenuecode.com/simple-product-creation-in-oracle-commerce-cloud", "abstract": "An online catalog organizes your products, SKUs, and collections in a manner reflective of the way users navigate to them in your store. The store contains only one catalog, but that catalog can contain any number of collections, which can contain any number of products and associated SKUs. Does a simple solution exist for creating so many entries? Today, we'll take a look at how easy Oracle Commerce Cloud makes it to create a product with multiple SKUs. ( SKU stands for \"stock keeping unit\" and represents a purchasable instance of a product in your store.) Our example product is a sneaker that comes in different sizes and colors with prices that vary according to color. We'll also customize the entry so that, when users select the color they want, the proper image will be displayed. Let's begin by defining the product type within our catalog: Now that we've added \"Sneakers\" as a product, let's discuss how to create the product itself with its various SKUs. The product we created will be displayed on our site as shown below. Notice that the price range of the SKUs is displayed ($45.00- $60.00). In the product page, when a buyer selects a color option, the corresponding image will appear. Only when the buyer selects a color and size will the \"Add to Cart\" option become available. Note too that when the \"Inventory Count\" of a product is zero, an \"Out of Stock\" message is displayed and the \"Add to Cart\" button becomes unavailable. This is a simple, manual way of creating products with different properties using Oracle Commerce Cloud . It's important to note that the platform offers several functionalities to automate the process of creating products and updating inventory, especially through spreadsheet imports. As this article evinces, OCC is a solution that’s highly-functional out of the box while simultaneously enabling a high level of customization. As many e-commerce companies continue to transform to remain competitive within an ever-evolving technological landscape, Avenue Code has become a preferred systems integrator for premier platforms like Oracle Commerce Cloud due to our expertise with complex, large-scale omnichannel systems. We believe in educating our clients on the unique functionalities that have given them their world-class reputation.", "date": "2018-4-18"},
{"website": "Avenuecode", "title": "A UI Engineer's Thoughts On Data Visualization Tools", "author": ["Alysson Ferreira"], "link": "https://blog.avenuecode.com/a-ui-engineers-thoughts-on-data-visualization-tools", "abstract": "In this new era of information, there's an increasing need to understand the latest trends quickly and efficiently, which means there's also a need for meaningful sources of trustworthy information. This is where data visualization comes in. Data visualization is the art of displaying information by combining the beauty of imagery with the conciseness of statistics, which allows us to organize complex data into convenient graphical representations. In simple terms, data visualization is the art of translating complex data into meaningful information. In order for a piece of art to be beautiful, it needs to have some structure. The same is true for data visualization. One requirement of data visualization is ensuring that the data is well-cleaned and formatted. With that in mind, let's move on to the next section. Typically before using data visualization software, it's necessary to clean data. What does that mean? It means that in order to format the data, it must be standardized so that it can be properly processed. For instance, in your data, you might have terms like \"San Francisco\", \"San Fran\", or \"The City By the Bay\", which would be treated as different entries when they're actually referring to the same city. Therefore, data cleaning is an important step for prepping data and accurately shaping it into standard formats so that it can be correctly analyzed and produce meaningful results. Here's a list of some data cleaning tools that can help with this: DataWrangler was created by Stanford University's visualization group, and is a web-based data wrangling service that offers an interactive method for data cleaning and transformation. This means you can clean your data using their web platform and transform messy, real-world data into organized data tables with accuracy. This project has been completed and no longer actively supported, but is still available for use. From this project, a commercial venture called Trifacta Wrangler was created . Although still in beta, Trifecta Wrangler is a desktop software that delivers the same results, but with more features. Its free license allows users to import from local CSV, Json, text, and Excel files. Learn more: There are many materials, such as video tutorials, that can be found on the DataWrangler webpage as well as from Trifecta's resource page . Drake is more than a simple data cleansing tool. It's a data workflow tool that's able to manage and organize the whole process of data manipulation, starting from cleaning, all the way to delivering data to be consumed by visualization software. For instance, it allows you to create workflows to process data that could automate the following steps: -defining file locations for input and output -converting data files into proper extension formats -unifying multiple sources into a single one -ensuring that every new data file change is reflected on the single unified file, as well as documenting it properly on a Wiki Learn more: T here are extensive documents, and even a walk-through video , o n the Drake main webpage . Formerly known as Google Refine, OpenRefine is also another tool that offers data cleansing, transformation, and augmentation from data lying on the web. It offers the ability to perform advanced operations that have their own expression language. It's an open project that has been actively supported on GitHub and features many built-in algorithms that help find text items that can be grouped together. Once your data is imported, these cell-clustering algorithms can be found by selecting 'edit cells', then 'cluster', and then 'edit' . Once an option is chosen, you can decide whether to accept or reject the suggestions. Besides cleaning, OpenRefine also offers analysis tools such as filtering and sorting. Check out the project main webpage to see helpful tutorials, a discussion list, and comprehensive documentation. There are also some demo screencasts , and even a book called Using OpenRefine . Together, these provide a great introduction to the tool. There are a plethora of free data visualization tools out there on the web, but I've narrowed them down to tools that are free and/or supported by an active community. Let's take a look at these useful and free tools that empower us with data visualization benefits: D3.js is one of the first data visualization tools that comes to mind when talking about free, open-source alternatives. It's a javascript based library for creating web visualization and displays the results on the web page. However, with great power comes great responsibility. D3.js is extremely powerful and flexible, because it allows you to build amazing things with it, but as a trade-off, it's not the easiest tool to use, so you might need to spend some time going through the helpful library documentation. The g ood thing is that D3.js has extensive documentation, as well as a myriad of nice use-cases based on D3.js as examples. Here is a cool example that shows the intricate constellation map of directors and their movie stars. Learn more: You can find out more about D3.js on its official webpage , and here is the code source on GitHub . This powerful library definitely had to be included on the list. Matplotlib is a Python-based plotting library that's capable of producing a multi-platform variety of quality figures in different formats. It can be used with Python code, J upyter Notebook , IPython Shell , web applications, and toolkits. Its goal is to empower the user with strong graphical generating capabilities. Common visualizations can be generated with ease - plots, histograms, bar charts, spectra, scatterplots, etc. can all be achieved with just a few lines of code. Here are some examples in a thumbnail gallery . Complex visualizations can also be achieved with this tool since it's capable of generating 2D, and even 3D plots using the mplot3d toolkit. H ere are some examples of plots this tool can generate . There's also a 2.0 version of Matplotlib that allows users create similar or more advanced visualizations, but with more ease than the former 1.0 version.  A step-by-step user guide for this version can be found here . Learn more: There are many examples, plugins, and toolkits available - including third-party libraries that have higher level plotting capabilities. H ere are some animation examples. Another open source Javascript-based library is Chart.js . It's a small, easy to use library that delivers customizable, interactive, and flexible charts. It only has eight chart types, which can be a drawback if you're looking for something more sophisticated; however, the types offered are very commonly used and can serve most cases. In the 2.0 version, it's possible to mix and combine different chart types. All charts are rendered on an HTML canvas, and have a flat and responsive design, which allows for adaptability and perfect scaling. Charts can also be animated in the 2.0 version, which means they can support user interaction. This is a perfect tool for small projects, as well as those starting to become familiar with data visualization development. Learn more : Here, you can find the tool source code and documentation . You can even find a list of popular extensions and plugins , including code integration with Angular, React, Django, Vue.js, and Java. R is a language and environment for statistical computing and graphics, and is also a GNU project . It's really powerful because it's highly extensible and provides a wide variety of statistical and graphical techniques including linear and nonlinear modeling, time-series analysis, and classification and clustering algorithms. It can easily be used to produce good-quality plots, including mathematical symbols and formulae. There are thousands of add-ons on the web, and it can be used for creating mappings, dashboards, and interactive web visualizations. T he downside of R is that it has a text-only interface run from command line, which is a restriction not all users are comfortable with. It also requires some knowledge of statistics in order to be properly used. Another interesting alternative is Google's tool for data visualization called Google Charts. It provides a simple interface, but is also interactive and flexible enough to allow for adaptations to your data presentation requirements. It provides a visualization API containing many chart options for multipurpose data presentations, including diagrams, maps, tables, timelines, and some others charts. Some cool features are that it can be easily embedded into your spreadsheet and website, and it also easily allows for icon creation, such as this Google-o-meter . Its documentation is also really complete and intuitive, which is a plus. The drawback is that you don't have access to the underlying code, which may result in a lack of flexibility that would be required in certain cases. FusionCharts Suite XT , which is built with Javascript, offers one of the biggest arsenals of data visualization elements, composed of a total of 90+ types of charts and widgets, as well as 1000+ different maps. Although listed for developers, they come with smart defaults, which means you can quickly build really descriptive charts in a short amount of time. Their charts are also highly customizable supported on all major browsers. It offers extensive documentation , as well as a complete API reference - here, you can find many live code samples for inspiration. The drawback is that it's not a cheap software to purchase, but you can obtain a free trial version with no feature restrictions which can also be installed as npm and bower packages. To learn more, you can find the project documentation here . Power BI is a business intelligence general platform from Microsoft that tries to offer a complete data workflow, from data cleaning to data visualization, without requiring any development knowledge. Its web service includes support for streaming data and scheduled data updates, and i t's designed for robust data analysis as well as offers a drag-and-drop interface for creating visualizations, reports, and dashboards. It also has extensive database connectivity, thus making it flexible enough to connect with many types of databases. It offers an experience similar to Microsoft Excel, so if you're an Excel user, you'll pick it up pretty quickly. It has a desktop version that's free to use, and users can use its web service for publishing public visualizations, with a capacity limited to 1GB per user. Here are some of the drawbacks. First, it's not easily customizable and is considered to be a bit limited compared to other tools. Also, its desktop version is limited for Windows users since the web browser allows any OS, but doesn't allow private visualizations on the free account. Finally, if you require many different use cases and user profiles that require customization, Power BI is not the best solution. However, PowerBI is still under development, and there are new features being added monthly. Built on top of D3.js and stack.gl , Plotly.js is a web service based visualization tool that allows the user to easily create interactive data visualization charts. It ships many different chart types such as statistical graphs, SVG maps, density plots, and even 3D charts. The charts are fully customizable, and they are declaratively described as JSON objects, which is what makes it universal and browser-based. The charts support zoom, pan, hover, and click interactions as well. A free account includes support for an unlimited amount of public files; however, it only supports one private file. If you want to learn more, feel free to go on to their help center . This tool from Google offers one of the simplest ways to generate data visualizations from many different formats. Users can select many options for generating visual reports, such as maps, tables, line charts, bar graphs, scatter plots, pie charts, and many others. Easy to create and easy to share, Fusion Tables offers a fast way for non-techies to quickly create visualizations and spread them over the web. It also includes support for geographic information system (GIS) functions to analyze data by geolocation. On the other hand, it offers very limited support for visualization, so more advanced users won't get the most out of this tool. If you're interested, there's a 3-minute tutorial on how to create a map using Fusion tables as well as a Help Center and a visualization example gallery . Qlik Sense is a multipurpose BI tool that's capable of performing both data cleaning and data visualization, which allows the creation of reports as well as dashboards designed for business demands. It has a responsive interface, which makes it mobile and tablet friendly. It doesn't require technical skills which makes it easy to use, and has a fairly robust engine which allows it to generate visual content efficiently. It can be fully customized, offering a complete set of standard APIs for building rich applications, and has the ability to create custom dashboards which enable the user to centralize information from multiple sources. One cool aspect is that is has a pretty wide data integration range such as with REST, Salesforce, Apache Hive, and many other data types. Besides robust data integration, it also has data cleaning and transformation tools which eliminates the need for external tools for data transformation. It has a free Windows-based desktop version, which is designed for personal use, and also has a free cloud-based solution, Qlik Sense Cloud, which is free for sharing with up to five users. More on that can be found in these tutorial videos . Tableau public is a free service that allows users to publish interactive data visualizations on the web. It offers good drag-and-drop support for data visualization that can be easily created without any development expertise. It's customizable and offers the ability to create interactive dashboards that combine multiple visualizations in a single view, called 'viz'. This centralizes the information, which can then be easily shared and embedded onto a web page. You can easily perform data calculations within their application, and a free public account has up to 10GB of storage. The drawback here is that your data will be public, so if your data is sensitive, you should resort to another free option. Here's an FAQ with answers to common questions. Ok! So for the readers that made it all the way to the end, here's the cherry on top. Let's make some predictions on data visualization fields based on our experience and expertise. One possible motive is developing software that can help automate the data visualization procedure. This means users can generate more complex visualizations with less effort. As we're entering a new, digital age , business users want more than simple line and bar charts. They're eager for more complex, robust visualizations as opposed to long bodies of text. With that said, another possibility in the near future of data visualization, is interactive media. Nowadays, we can already see some interaction with filters combined with DOM events that allow the end user to modify to some degree. In the future, however, the expectation is not just to be able to filter through sensitive information, but also to be able to target detailed points of interest with little effort. Finally, we expect the outcome of more visualization tools that allow us to make meaningful insights from Big Data . T hese novel tools must have more power in order to generate reports for non-technical workforces on the top of big data. These data visualization tools must be capable of making it easier for employees to rapidly determine new things in the ever-increasing datasets. Machlis, Sharon. “ 22 free tools for data visualization and analysis. ” Computerworld, 25 May 2017. Hoppe, Geoff. \" 22 Free and Open Source Data Visualization Tools to Grow Your Business. \" Clapterra blog, 7 June 2017. Sharma, Nishith. \" The 14 best data visualization tools. \" Design & Dev, The Next Web, 21 April 2015. Bierly, Melissa. \" 10 Useful Python Data Visualization Libraries for Any Discipline \" . Mode, 8 June 2016.", "date": "2017-11-2"},
{"website": "Avenuecode", "title": "How to Break into Machine Learning: A Non-Traditional Approach", "author": ["João Moráveis"], "link": "https://blog.avenuecode.com/how-to-break-into-machine-learning-a-non-traditional-approach", "abstract": "In the last 2 or 3 years, you may have heard that \"data is the new currency.\" Or perhaps you've read news about the increased usage of artificial intelligence to create self-driving cars, supplement customer support, and improve product recommendation systems based on the last purchased item or the customer's shopping profile. Machine learning has found its way into the market, and many companies are using it to improve or develop new products. But one topic that we don't hear much about is, \"How do I get started in a machine learning career?\" \"Are there pre-requirements, or can I just jump in and base my ML studies on documentation?\" If you have asked yourself these questions, then keep reading! Today we're going to talk about how to break into the vast field of machine learning. Machine learning was born as a study branch within artificial intelligence so researchers didn't have to tell a machine the rules on how to behave. Instead, a machine would learn from the data itself by identifying patterns and determining its own behavior. Even though this field started as an applied subject within artificial intelligence, it proved to be very effective and promising, so researchers started applying it to many different problems. Beyond identifying and extracting information based on a researcher's ideas drawn from visible data and the relationships between them, researchers could also develop algorithms that could identify hidden patterns and use them to gain insights from the data. Following this movement, researchers from many universities started to collect data from different domains, from flower characteristics to car models to illnesses such as brain cancer, in order to develop and create new models. (A model is a math function that states how a piece of information relates to other information.) As a result, we now have many frameworks that abstract these functions (algorithms) and can be applied to any collection of data. These frameworks include Scikit-learn and TensorFlow. As you can see from this context, machine learning by nature has a strong academic root, hence all its concepts are based on academics. Because of this, people interested in learning about this field and pursuing it as a career path usually assume that they need solid academic knowledge based in math and/or a deep understanding of research methods. Most articles would set a study guideline as follows: To be honest, this knowledge would make your life easier, but it is not required since you can study and acquire this knowledge as you go through each topic. Many people faced with the program above might just give up, perhaps because they don't love math enough to pursue each subject deeply or because the concepts might seem too abstract.  Some people like me learn better when seeing all of the concepts in action. In this case, I recommend learning through hands-on examples. I would like to suggest a different guideline to get started with machine learning and remain motivated each step of the way: All that is stated above is my point of view, and the path presented shows how I got into machine learning and started to understand how to solve ML problems. But I want to encourage everyone to define their own best method. My guideline is just a small push, given that when you start reading about a topic, you will be led to other topics you'll need to study. Online courses and/or technical books are good resources to rely on because the authors present the content with a fluid and pragmatic methodology, building basic knowledge first before addressing advanced topics. I hope this article helps develop your interest in exploring this promising and amazing field. And if you have already started studying, I hope this article motivates you to continue your journey!", "date": "2020-5-20"},
{"website": "Avenuecode", "title": "Why Go?", "author": ["João Henrique Machado Silva"], "link": "https://blog.avenuecode.com/why-go", "abstract": "It was Todd Mcleod that said something like this… \"The more I learn the harder it gets to ask questions, the more I ask questions the more I learn\" If you're looking for a scalable, light on the page open source programming language, there are a few great options out there. But one that often goes comparatively unnoticed is Go, first released in 2012. Here, I'll explore some of the attributes and benefits of the language and provide a few resources to learn more. Before I actually start to talk about Go, I want to talk about its credentials, and specifically, its creators' credentials. Go was created by three guys called Rob Pike, Robert Griesemer and Ken Thompson. Let's take a look at what these guys had already accomplished before creating Go: Rob Pike , who for me is like the Yoda of the computer programming world today (I'm a big fan), is best known for his work at Bell Labs , where he was a member of the Unix team and was involved in the creation of the Plan 9 from Bell Labs and Inferno operating systems, as well as the Limbo programming language . Oh! And he also helped create a little something called UTF-8, which you may have heard of! Robert Griesemer had an early interest in programming languages and ended up studying in Switzerland under the creator of Pascal, a language which many people say had a direct influence on Go. He also had about 15 years of experience working with C++, but his biggest qualification, in my opinion, is that he was recruited by Rob and Ken to join the original Go Team at Google. Ken Thompson , this guy is out of this world. If Yoda had a brother, it would be Ken. He spent most of his career at Bell Labs , and he was solely responsible for designing and implementing the original Unix operating system. He also invented the B programming language , the direct predecessor to the C programming language , and was one of the creators and early developers of the Plan 9 operating systems. You read that right - he invented the programming language that came before C. Anyway, this trio worked at Google when Go was originally created. One thing that I like to point it out is that Go is one of a few languages created after computers had more then one core. And Go makes exceptional use of that! Go is a typed language that is compiled to machine code. And unlike other new languages that have relied on LLVM, such as Swift and Rust, Go is built from the ground up. It is not dependent on layers like virtual machines, etc. to run. Additionally, Go has “ goroutines ”, that are much more efficient and cheaper to create than threads. \"Go was born out of frustration with existing languages and environments for systems programming. Programming had become too difficult and the choice of languages was partly to blame. One had to choose either efficient compilation, efficient execution, or ease of programming; all three were not available in the same mainstream language.\" https://golang.org/doc/faq Simplicity was a key motivation and a requirement for the design from the beginning. If you are serious about the language, you should definitely read this paper by Rob Pike. In Go, compile times are fast - making the immediacy of running a program feel like a scripting language, which is another win for Go when it comes to performance. Another great feature is that it does not suffer from continuous language changes. In fact, a crucial premise in Go is to have no change in the language. If you come from mobile development, for example, and worked with Swift, you know what it is like to have a language that continuously changes. The Go Standard library is very good. For example, the net/http library is used directly to write servers and there is no need to use layer upon layer of frameworks to write, such as in Ruby or Javascript. \"Concurrency is not parallelism.\", Go Proverbs Concurrency is a huge part of Go! Go has built-in support for concurrency with Goroutines and Channels, making it a natural choice for writing back-end systems with messaging, caching and scalability requirements. This is particularly attractive to programmers coming from single threaded languages like Ruby, Python and others. It's not that those languages do not support multithreaded applications, but they need some type of implementation to actually support it. Rob Pike has an extremely elucidating talk on concurrency that you can watch called Concurrency is no Parallelism . Go compiles everything into a statically linked binary file, so no dependencies are required and you're left with only a single binary to deploy. This simplifies deployment significantly. You don’t have to worry about whether your users have Ruby, or for that matter, the JVM installed. Nor do you need to worry about a dependency being upgraded since the entire program is compiled into a single binary. Sweet, right?! Well, besides Google? Here are some: Docker , Heroku , Netflix , Uber , Hashicorp , New Relic , New York Times … just to name a few. There is a long list of companies using Go nowadays, and it's only growing. Here is a official list of companies using it around the world. Note that this list is not exhaustive - I personally came name a few companies using Go that do not appear on this list. One really good place to begin is the Go Tour . It does a great job teaching the basics and helps you develop some fluency, since you learn by playing around in the Go Playground . The playground is like an online code editor, where you can run small chunks of code, save, and share with unique links. For reference we have the Effective Go guide. And of course there is the Go Language specification , but until you're familiar with the basics, you may find it to be a bit dense at first. Those are all good places to start, but if you/re ready to go the distance and invest, I recommend the Code School course on Go Language, which is great to get you started. There is also a course on Udemy by Todd Mcleod called Learn How To Code: Google’s Go (golang) Programming Language which I think does a nice job. I can also recommend about of books: Fans of Go (called Gophers, by the way) describe Go as having the expressiveness of dynamic languages like Python or Ruby, with the performance of compiled languages like C or C++. In conclusion, Go is a performatic and simple language, with concurrency naturally built in, deployment as easy as can be, created by some of the best minds in computer programming today. It's also supported by Google and huge community of open source developers with huge momentum, and has been growing faster and faster every day. So really, the question you should be asking yourself isn't \"Why learn go\", but rather, \"Why aren't we already using it?\".", "date": "2016-11-23"},
{"website": "Avenuecode", "title": "AC Spotlight: Brent Fawcett", "author": ["Alfredo Moro"], "link": "https://blog.avenuecode.com/ac-spotlight-brent-fawcett", "abstract": "Brent Fawcett , Head of Information Technology at Robert B. Somerville Co. Limited, discusses the critical role of IT and data governance in the oil, gas, and energy infrastructure sector. Avenue Code: Tell us about your career path. How did you get to where you are today? Brent Fawcett: I entered IT when computer science was emerging in schools. Fortran was the programming language of the day, which we had to submit using punch cards. My first personal computer was a Commodore 64. I spent many hours on BBSs (Bulletin Board Systems) that I accessed via “dial-up” over a 300 Baud Modem. I muddled around writing code in BASIC, but my career began in the hospitality industry. I was always technologically inclined, and I remember being fascinated by point of sale (POS) systems and inventory processes. It wasn’t until I started having children that I decided to formalize my education, which led to senior roles as a networking specialist. My first role was at Nortel, and the progressively senior roles were at Harley-Davidson, Canadian Tire, DAVIS & HENDERSON, and Hydro One, which transmits about 97% of Ontario’s electricity. From there, I became an IT Operations Manager at Alectra before taking my current role as Head of IT at Robert B. Somerville. In my present position, I manage all IT operations and can make a strategic impact on business operations. AC: What are the major responsibilities of an IT department in the oil, gas, and energy infrastructure sector? BF: At Hydro One and Alectra, my primary concern was safety for workers and reliability for consumers since I was responsible for the infrastructure that the critical applications leveraged to controlled electrical transmission and distribution. Delays due to technical failures have a huge impact on project costs, so at Robert B. Somerville, I’m responsible for making sure that end users have high-functioning apps with relevant data easily available. I’m also helping to transition the company to a remote work model, which is challenging for an industry that so heavily relies on travel and is slow to transition to newer technologies. AC: How has COVID-19 affected Robert B. Somerville Co. Limited’s operations? BF: There was no question of cancelling our current contracts, but the big challenge was that, while our revenue remained relatively the same, our operational costs rose exponentially. We had high PPE costs and logistical challenges related to social distancing regulations. Before COVID-19, we already had a virtual environment utilizing Citrix and the Microsoft 365 Ecosystem (Teams, SharePoint Online, OneDrive, etc.) The infrastructure was there, but we had to quickly educate everyone in our company to transition effectively. Additionally, we had partners that had no virtual infrastructure in place, so we were called on to help them pivot as well. Moving our preferred method of engagement to our existing help desk system was also important. AC: As Head of IT, what changes are you making to help the company adapt and sustain itself through the pandemic? BF: One of my biggest focuses is business enterprise risk management. I’m helping the company assess which IT operations are critical for the business, including applications and data, confidentiality, integrity, and availability. We’re creating a security work stream and developing policies to protect data, which is even more important now that remote work means less control and increased vulnerability to malware, phishing, etc. AC: Where do you see the biggest payoffs for your industry when it comes to investing in IT? BF: Data analytics is critical. We’ve collected huge amounts of data for repetitive tasks, and now it’s time to visualize that data and convert it into actionable business payoffs. For example, we have a lot of expensive equipment without great oversight, so we need to use technology like embedded IoT to predict equipment failure, track equipment location, etc. This is an enormous undertaking that takes time, but it’s an important investment. AC: What’s next for Robert B. Somerville Co. Limited in 2021 and beyond? BF: From a business perspective, the demand for oil and gas remains high, so we’ll continue building pipelines for government and big businesses. We also plan to grow our utilities business into new verticals, especially in Ontario. From an IT perspective, we want to establish an intranet portal for information sharing, onboarding, external payment portals, etc. AC: What roles will data, machine learning, and automation play in your upcoming initiatives? BF: Like everyone else, we’re gathering massive amounts of data. Now we need to understand how to convert that data into efficiency gains with automation. For example, we sometimes onboard 500 people in a month, and we need to automate that process so it’s less time intensive for managers. The same is true for security. We need to find a way to analyze data in our event management system and automate the process of identifying and sending alerts for malicious activity. Enabling data visualization is a process that requires specialists. AC: What is the key to successful strategic partnerships in the IT area? BF: Both parties need to be transparent and understand that it’s not a vendor relationship but partnership management. Anything other than win-win is always lose-lose. All of my partners have certain competencies and certain areas that aren’t fortes. Being candid about strengths and weaknesses saves time and money. AC: What are you personally most passionate about in your career? BF: I’m an information junkie, so I’m always challenging myself by investigating new technologies I learn about through vendors, mentors, and peers. Outside of technology, I’m very invested in the lives of my adult children. I’m also personally passionate about health and wellness and have become a triathlon athlete. Whether it’s tech or fitness, I like to push boundaries and be uncomfortable, because that’s where I grow. AC: Thanks for your time today, Brent. It’s been great to learn about your personal career and hear your perspective on data visibility within the oil, gas, and energy sector.", "date": "2021-1-6"},
{"website": "Avenuecode", "title": "AC Spotlight - Kristy Wieber", "author": ["Anna Vander Wall"], "link": "https://blog.avenuecode.com/ac-spotlight-kristy-wieber", "abstract": "(The following article is property of Avenue Code, LLC, and was originally published with permission at Total Retail on August 15, 2019 .) Kristy Wieber, co-founder and president of Rent frock Repeat , explains the innovative clothing rental model that uniquely answers consumer needs while benefiting apparel brands. Avenue Code: Tell us about your personal career path. What brought you to where you are today? Kristy Wieber: I met Lisa Owen, co-founder of Rent frock Repeat, when we were both working for a different company. We were frustrated at being in a male-dominated work culture that wasn’t open to innovation, and we wanted to start our own company but didn’t know what it would be. I went on to become vice president of marketing for a different digital agency and eventually left that job as well. Later, Lisa and I were both invited to a wedding. We didn’t want to spend hundreds of dollars on dresses we would wear only once, so we tried to rent but couldn’t get dresses shipped to Canada, which gave us the idea. A year later, Lisa and I were running Rent frock Repeat out of Lisa’s basement. AC: How does the Rent frock Repeat business model work, and how is it evolving? KW: We began by renting only special events dresses, but customers kept coming to us asking to rent blazers, suits, blouses, skirts, coats, etc. That’s when we realized there was a bigger pain point to solve. Our big focus right now is moving from eight years of special events dress rentals to a monthly subscription rental service for all-occasion wear. What retailers don’t realize is that they're already renting clothes without profit — one out of five women buys clothes, posts pictures of herself wearing them, and then returns the items, all for social media. In this sense, we aim to provide a solution both for retailers and for women who don’t have time to shop or who have Marie Kondo’d their closets and are left with too few options. Humans have an innate desire to want new things, and Rent frock Repeat offers an innovative solution. Our business model also allows customers to receive a curated closet and experience more of the fashion marketplace, getting introduced to not only established brands, but emerging designers as well. AC: Tell us more about your relationship with clothing brands and what differentiates Rent frock Repeat from other retailers. KW: We’re striving to partner with brands on designing products that look great and bear up after many wears. We want to be sustainable and get away from fast fashion. Rather than having a price per item, we will offer two options: one, customers can get four pieces for $99 a month and swap out one piece once or, two, they can get six pieces for $199 and swap all of them. Shipping is included, and we take care of dry cleaning. We’re also very flexible, so members can pause their subscription at any time. If you look at your clothing budget for the year, these prices are totally manageable considering the amount most people spend on clothing that sits in their closets without being worn. Research shows that, on average, we have 136 pieces of clothing in our closet, but only regularly wear 27. A service like this offers an opportunity for variety without commitment or clutter. However, because members have the chance to wear their rental pieces for a month, if they do decide they love it and want to keep it, they can buy items from us at a discounted rate. They understand if they really love it before buying it. Customers can pick their own items, but if they don’t know what they want, they can answer a 66-question style profile that enables stylists to pick items for them. Our stylists will also work with artificial intelligence to continually learn and improve. This is also one way we’re going against the grain of regular retail purchasing — we're basing our clothing buys on real-time customer data, whereas most retailers make decisions based on past data. AC: What is it that gives customers the patience, confidence and trust to fill out a 66-question style profile? KW: Currently, we’re testing this offering with 5,000 customers before rolling it out unlimited. We’re already at capacity with this test, and we have 2,000 customers on our waitlist who have filled out their style profiles without knowing costs and without having used our stylist service. We’ve earned that trust through eight years of excellent service. AC: What goes into building consumer loyalty for a brand such as yours? How are you transforming the retail industry? KW: When we first launched the company, we had no intention of having showrooms. However, because we offered special events dresses, women wanted to try them on. This led to hiring a stylist, offering fittings, and asking questions so that we weren’t just pulling dress after dress. It completely changed our relationship with our customers so that we really understand who they are and what they need. Now, we can get it right without a physical fitting because we’re asking so many more questions, and there’s better technology available. For instance, we can do pop-ups and take correct measurements, but we still have stylists on hand not only to answer questions, but also to review what's going in a member’s box before it leaves the warehouse. Our stylists have built incredible relationships with our members, to the point where customers would just call their stylist and say, “I need something for this weekend, can you pick it out for me?” They're honest and will help customers step outside of their comfort zone. It comes down to listening to the customer instead of trying to sell something. We’re always trying to surprise and delight our customers. Rent frock Repeat isn't just a transaction, it’s a sharing experience. We want to transform that experience of opening your closet and being depressed, which 10 percent of women currently are. Getting dressed is like putting on armor. Your outfit can make you feel either confident or like you don't even want to go out the door. In some sense, clothing can make the woman. Rent frock Repeat’s new subscription service offerings. Credit: Rent frock Repeat AC: What do you look for in your relationships with retailers? KW: We’re looking for retailers and designers that truly want to be our partners. We know there’s going to be a new way to consume, and that’s the sharing and service economy. Therefore, we’re considering how we can help both our customers and our partners. Our retail partners are able to learn something because in the beginning, we will share the dashboard so brands can see which items, colors, sizes, etc., are the most requested. We know the brands our customers want, but we also want to introduce emerging brands Canadians may not have seen before. When we started eight years ago, we had to use an out-of-the-box solution and customize it for dress rentals. It was robust, but couldn’t keep up with our quickly changing company. Now, our tech team is working on agile systems that can adapt as we do. We want the experience to be effortless not only for us and our customers, but for our stylists, retailers and vendors. AC: What has been a highlight for you in the last few years? Was there a moment, either for you personally or for Rent frock Repeat, when you knew you were on the right track? KW: There have been several big moments. We won the Canada Post E-commerce Innovation Awards twice. But honestly, it’s the little encouragements from our customers over the last eight years that have confirmed we’re doing something right. A woman emailed us the other day about our subscription service, saying, “This is brilliant. I can’t wait. Please hurry up!” It’s these moments that are so exciting and make us feel like a community vs. a company with customers. AC: What's the key to success in today’s retail environment? KW: You have to see around the corner and anticipate where our society and market are headed. You can’t just do what everyone else has done for the last 50 years. Consumers are driving the industry much more than retailers and vendors ever have. AC: What are the unique challenges you see in the Canadian market? KW: Canada is smaller in terms of population, and we don’t have as many shipping options as retailers in the States, so there are challenges getting items to customers in a timely manner. We’re looking for ways to improve this. I also think Canadians are more reticent and cautious with new ideas. But when they try our services, they become our biggest champions. Our road map is that we have a white-label platform that other companies can use to rent their products so they don’t have to recreate the wheel each time. In the States, American Eagle and Ann Taylor are already renting apparel. AC: What does Rent frock Repeat do to stay abreast of innovations in tools and technologies? KW: When we launched eight years ago, it was too soon in a sense. We were ahead of our time. It took a lot of educating the customer on how rental works. We’re always asking ourselves if we’re doing what’s right for the customer or for ourselves. Focusing on the former leads to innovation. We keep an eye on what’s going on in the industry because we want to learn and see how we can be better. We’ve never rested on our laurels. We like to find people who know what they’re talking about and ask for their help. We know we don’t know everything, and 95 percent of the time, people are so willing to help! AC: Thanks for your refreshing perspective on apparel rental through an innovative, customer-first focus. We look forward to checking in with Rent frock Repeat again once your new offering is fully launched!", "date": "2019-11-6"},
{"website": "Avenuecode", "title": "How to Use Data Science to Assess Public Opinion", "author": ["Marcio Viegas"], "link": "https://blog.avenuecode.com/how-to-use-data-science-to-assess-public-opinion", "abstract": "On a pre-COVID-19 Friday night, I decided to stay in and study data science instead of attending a public celebration. I  started with the \"Complete Data Science Training: Mathematics, Statistics, Python, Advanced Statistics in Python ,\" and then decided to look into Google Cloud Platform as well. On Saturday, I woke up and decided to skip the festival again, preferring instead to study data science and GCP. Coincidentally, I had been wondering how many other locals felt like they would rather stay in than join the celebration, and I decided to utilize my data science studies to gain a better understanding of general public opinion. My first and only idea, based on some previous work developed by my teammates at Avenue Code, was to capture Tweets using the Filte red Stream provided by Twitter, then analyze the content's feeling using GCP Natural Language API. I quickly designed some architecture that was simple but would do the job flawlessly. Besides the two services mentioned, I also used GCP Pub/Sub and GCP Firestore, as shown below: I could have used GCP Functions to avoid running code directly in my machine. I guess I thought about that when I was designing the solution - go all serverless, right? - but the time taken to learn one more service  wasn't worth the pay off in this POC . Almost 10k Tweets later, I plotted the information using ChartJS . The x-axis represents the feeling of the Tweet. Dots closer to the number 1 represent a positive feeling, and dots closer to the number -1 represent the opposite. The y-axis represents the intensity of the discourse. The higher the number, the more intense the feeling is. The result was simple and effective: Since this was a purely experimental assessment, I didn't spend too much time analyzing the results. It's a well-known phrase that: \"worse than no data is data without context.\" The circumstances in which I obtained the information are so unique that we could end up in Simpson's Paradox . That said, the premise of my study was relatively simple and had the potential to yield effective results. The code is provided in this repository within the MIT License.", "date": "2020-9-9"},
{"website": "Avenuecode", "title": "AC Spotlight - Aaron Gette", "author": ["Holly Camponez"], "link": "https://blog.avenuecode.com/ac-spotlight-aaron-gette", "abstract": "Aaron Gette, CIO of the The Bay Club, caught our interest through his integration of a people-led approach with his tech-centric executive role. Being an innovator and a major player at The Bay Club, a San Francisco institution, we knew he would have some unique insights into the world of tech. Avenue Code : When you were a child, who did you want to be when you grew up? I have a feeling that only a few CIO's know that they are going to go down that path. Do you agree? Why did you become a CIO? Aaron Gette: As cliché as it sounds, I've always wanted to be an astronaut and a pilot. I was captivated by space travel - still am, actually. So, where does that line up with becoming a CIO? Good question. For those of us who were introduced to the personal computer in the 1980's, it's hard not to find yourself in a tech-related job today. That initial intrigue developed into a deep curiosity that led me down the programming path early on in my life. While other kids were venturing off to horseback riding or sports camps over the summer, I attended the YMCA coding camps in the 80's. Although my love for tech  has only grown over the years, my desire to code has faded. Over time, I've been able to build rapport with peers, customers, and investors that has helped open the door to my first CIO gig. The last 4 years as a CIO have been an amazing journey for me, and more than anything, I enjoy empowering those around me to be great as well as mentoring those on my team to achieve the success they desire in their career. Avenue Code: What made you choose an executive role at The Bay Club as the next step in your career? Aaron Gette: The Bay Club has been an institution in San Francisco for over 40 years. It represented a challenge that was unique because it was transforming into a hospitality organization, and I was excited to be a major player in that transition. We as technology leaders have a real responsibility with all the resources and technological advances at our disposal, to drive businesses to new heights. Using technology, we can create new competitive advantages through innovation and the acceleration of market opportunities that may have never been realized before. I'm always excited for a new challenge, and taking The Bay Club into the future while defining a new hospitality category around Lifestyle was exactly what I was looking for. The iconic Bay Club in San Francisco. Image via The Bay Club . Avenue Code: What expectations did you have for the CIO role, and what opened up for you once you were in the role? Aaron Gette: My expectations were in line with the vision and playbook of the CEO. Matthew Stevens has done a phenomenal job with taking The Bay Club into new and uncharted territory. We have designed the cloud architecture and infrastructure to support our rapid business expansion and provided the tools to drive decisions that keep us ahead of all the competition. Having a true sense of trust allows the team to operate autonomously while keeping the technology strategy intertwined with the business strategy. I've been able to learn the depth and breadth of a business that covers so many different aspects all while creating a community across business units and teams that drives a more collaborative environment. We have really just begun, and I'm excited to take this brand outside of California. The newest addition to The Bay Club - the luxurious Manhattan Country Club. Image courtesy of The Bay Club. Avenue Code: IT is transforming and evolving. Do you feel the CIO role is changing at the same speed? What have you observed as an innovative leader who is part of  the new wave of CIO's? Aaron Gette: The modern CIO role covers so much these days and I think it's most important for the CIO to constantly be innovating. Not only do we have to understand the digital landscape internally, but also that which defines so many other businesses outside of our own front door. I think it's critical for a successful CIO to build a great peer network and get involved in the leadership community in order to challenge themselves to find new ways to lift their team and business to exceed expectations. The ability to understand the business so that you can align your goals will create both cost cutting measures and drive top line revenues. Avenue Code: If you were to interview someone for the CIO role, what are some things you would look for? In other words, what are the critical factors that must be in place in order to be successful in this role? Aaron Gette: It starts with integrity, and holding yourself accountable. There are great successes and soul crushing failures, but we have to find ways to learn and be better the next time around. The intrinsic characteristic of a great leader, not a boss or a manager, is the ability to empower those around you to be better, which requires trust and mentorship. A great CIO can instill a sense of culture that goes beyond those who are in front of you daily. For many of us, we have distributed workforces that also have to feel like a part of the team in order to truly contribute towards the success of the team. This means rolling up your sleeves and participating in the right ways to foster the culture you want, since culture is a critical element in enacting change. To me, success in this role is really about finding the pieces that can be commoditized, and investing in those that will bring about the change your business needs. It's also important to be able to connect and interact with all your customers, both internal and external. After all, there is no substitute for fantastic customer service. Avenue Code: You're an active speaker at several conferences and events. Are you hoping for the audiences you address to see this role in a new way? Where does your passion to share your story with the audience come from? Aaron Gette: With what I have been able to accomplish in my career, it has certainly afforded me the opportunities to speak and share with other IT leaders. Every time I'm up there, I hope to connect with my peers and share the evolution of the CIO role as I've experienced and envision it to be. Change is the only constant, and if you're able to harness the power of change to define the role that ignites the business and feeds your growth, then you are altering the way the world sees the CIO. We are a unique and quintessential piece of the C suite as we challenge the way both boards and investors see our abilities to inspire the business. My passion to share my experiences with others come from those trials and tribulations and I hope to encourage them to step out and take risks. Their success alters the legacy of the business they represent as well as improves the community of IT leaders yet to come. Avenue Code: What is the next thing you'd like to accomplish as an innovative leader? Aaron Gette: The future is bright with so many amazing emerging technologies. I'm most excited to step out into the direct digital customer interactions and experiences we can now provide. I've recently written articles on my views of AI, IoT, and AR and their impact on creating unique customer experiences that are real game changers in almost any industry. I cannot think of another time when we, as IT leaders, have had so many options at our disposal. It can be challenging and easy to get lost in the hype, so finding practical ways to initiate and bring these technologies into your business is your best first step. Establish relationships with partners and push to get phased adoptions that will build to the digital experience your customers want and business needs in order to drive engagement and wallet share. We're excited for the future of digital engagement here at The Bay Club and what we will soon be delivering to our customer base. Avenue Code: This was really enlightening, Aaron. It was great hearing about how you first got into tech and then transitioned into this CIO role as well as where you see the CIO role going in the future. You offered some great advice on how CIO's can foster teamwork and a collaborative culture in order to push innovation within the company. Thanks for your time!", "date": "2017-9-19"},
{"website": "Avenuecode", "title": "AC Spotlight - Kamelia Aryafar", "author": ["Anna Vander Wall"], "link": "https://blog.avenuecode.com/ac-spotlight-kamelia-aryafar", "abstract": "Kamelia Aryafar explains why keeping data science, ML, and AI front and center is essential to the long-term success of e-commerce companies in every vertical. Avenue Code: Tell us about your personal career path. What inspired you to pursue a career in computer science and machine learning, and how did you get to where you are today? Was there anything surprising that you learned about yourself and your abilities along the way? Kamelia Aryafar: I think I have been incredibly lucky to have had support along the way in different steps of my journey in addition to the passion for computer science. I have been drawn to computers ever since I programmed Hangman on my computer at the age of six. I never considered pursuing anything else - I was obsessed with exploring the capabilities of computers, which naturally led to AI. AI is one of the most impressive inventions in human history, and the possibilities are endless. What I’ve learned about myself is also what I tell other people who are pursuing careers in ML/AI - patience is required. It can be so frustrating to spend hours hunting for a tiny bug that creates a massive problem, but that’s what it takes to optimize constantly. AC: As a Board Member of Persian Women in Tech and the author of several compelling articles , you’re a well-respected advocate for encouraging women to pursue STEM careers, especially at the executive level. What do organizations gain when they embrace a balanced leadership model? KA: Women add a much needed perspective because they view products from a different angle. When women aren’t involved, product design misses crucial considerations. For example, there was a time when the automotive industry was only testing product safety features for the male body. In STEM fields, women excel. In fact, the first generation of computer scientists were women, but now there are more males than females in STEM. The ratio should be more balanced at all levels, including at the executive level and in board rooms. But we are making progress. The more we talk about and normalize women and under-represented groups in STEM, the better it will get. I should note that having a balanced leadership model is valuable outside STEM as well. AC: Why is it vital for enterprise organizations to add a CAO role to their executive team? KA: Every company is trying to extract value from their data and use it to personalize product recommendations to drive sales. Because it’s relatively new, there’s an executive-level leadership position for every essential business area except for AI and ML. Several companies are beginning to add this role, and that’s what closes the gap between excitement over the potential of these technologies and implementations that lead to actionable insights. AC: What are the biggest challenges of walking into a company where this role isn’t already established? KA: Expectation management. AI and ML are tools for a long-term game and don’t often generate immediate results. It takes time and patience to go through all iterations. Change management is also crucial, as implementing this role necessitates a departure from historic business strategies. AC: Organizationally, can ML and data analytics teams operate relatively independently from other departments, or does there need to be a wider, cross-organizational cultural shift in order to utilize these teams to their fullest potential? KA: Eventually, the paradigm needs to be highly cross-functional and collaborative. When I talk to companies at the infancy stage of establishing an AI or data team, it makes sense to centralize it because you want to establish the pipeline, infrastructure, and organizational alignment on prioritization. But down the line, AI needs to be heavily matrixed into the organization so that its value can be added into every product team. AC: How does the scale of data change an organization’s ability to utilize it effectively? KA: Sparse data equals less accurate models. Having diverse, large scale data is better because it lets you create more generalized models, but there are other challenges related to storage, cost, data utilization, and infrastructure. Algorithms that were built for smaller-scale data are not applicable anymore, so it’s really important to take advantage of data by thinking through architecture and system design from the outset. A lot of companies are struggling right now because they have siloed, fragmented data in legacy systems. They have to create a data lake before they can create a meaningful machine learning model. It’s relevant to note that we are generating data exponentially, and most of the data in the world has been created in the last few years. AC: What are the biggest opportunities in harnessing the power of data for recommender systems and personalization? KA: There’s a big emphasis, especially in e-commerce, on creating personalized recommender systems that present products aligned with each consumer’s style and preferences. These machine learning algorithms are based on demographics, purchase history, searches, etc. and are designed to make the world smaller for us so that we can quickly find relevant products. In brick and mortar stores, we find what we need and walk out. But in e-commerce, no one has the time or patience to sort through millions of products to find what they’re looking for. The potential for this technology is massive and extremely helpful for buyers and sellers alike. That being said, there are two things we need to pay attention to as an ML community. The first is privacy. We need to consider the ethics of AI and what kind of data we’re extracting for our personalization systems. This leads to explainable AI. A lot of companies are focused on using AI models for personalization that explain to the consumer what data was used to generate the ad. For instance, many ads now include tags that say, “Why am I seeing this ad?” This is something we need to pay more attention to. The second consideration is bias and lack of diversity. For recommender systems, it’s easy to create models without enough diversity. There’s a danger of confining consumers in an information filter where they see more and more of the same product and nothing else. It’s challenging to maintain personalization while creating enough exposure to new products and growing along with the consumer. There are different ways to grow with users. For example, if you’re shopping for mid-century modern furniture, we don’t want to box you into this product profile exclusively, so we inject new styles into recommendations while maintaining relevancy to your interests. AC: What is the future of ML and AI in e-commerce? KA: E-commerce is one of the biggest consumers of AI. As we shift increasingly toward online shopping, AI will also increase in importance. Personalization is essential for helping consumers find what they want and convert quickly. AI and ML are also key for marketing spend optimization, as well as helping e-commerce platforms identify and customize offerings for a clear target audience. Historically, a lot of companies have used CRM to define static clusters of consumers and segments to pursue. But with ML and AI, you can personalize to particular consumers and create a one-on-one relationship as opposed to a cluster-based relationship. AC: What do you think is the key to success in terms of strategic partnerships? KA: The successful partnerships I’ve been a part of have all had an aligned vision, goal, and roadmap very early in the process. The unsuccessful partnerships I’ve seen are those that are trying to explore a vision together. So the first question I ask when I go to review meetings with new technology vendors is “what is the goal?” AC: What do you do to stay abreast of innovations in tools and technologies? KA: I read as many research papers as I can and volunteer to serve on several review boards for conferences. It’s a fantastic way to stay current on ever-evolving technology, because to review someone’s work, you have to understand it deeply. I also give back to the community by publishing my own work. When you’re writing a research paper, whitepaper, or blog post, you generally start with a survey to understand the current state of the world before adding your own contribution. I also connect with other researchers so that I can hear about their work and ideas, both by attending conferences and joining social media platforms, which are great at building these communities for AI researchers! AC: Thanks, Kamelia, for sharing your expertise on the potential of AI and ML in e-commerce, as well as your inspiring vision for women in IT leadership roles!", "date": "2020-8-5"},
{"website": "Avenuecode", "title": "AC Spotlight - Eric Tam", "author": ["Holly Camponez"], "link": "https://blog.avenuecode.com/ac-spotlight-eric-tam", "abstract": "AC: Tell us about your personal career path. What brought you to where you are today? Eric Tam: My father always asked me what I wanted to do when I grew up, but all I knew was that I didn’t want to be an architect like he was. Little did I know that software architecture would be in my future! Right out of college, I gained a data perspective as a market analyst for APL. Soon after I ended up managing marketing collateral for PG&E. When the internet first emerged, my friend and I started our own digital agency, Red Eye, when I was just 27. Three years in, we had 30 employees and were acquired! In some ways, we were in the right place at the right time, but we also built our own luck. Whereas most companies were just trying to create electronic brochures, we focused on back-end development and led e-commerce trends by advising clients to sell online, which seemed crazy at the time. When the bubble burst in ’01, I left my role with the acquiring company. Over the next few years, I became the CEO/COO for three different startups, generating funding for each. Eventually, I moved on to work at Fluid as the senior vice president of development and delivery, then became the vice president of global digital at Benefit Cosmetics, where I helped manage the digital global program. In 2016, I started independent consulting for Bay Area-based brands and was soon recruited by Amyris to manage Biossance’s digital and technology program. The team here is much smaller than others I’ve managed, and I like that I can take a hands-on role that allows me to test waters, make mistakes, and use those learning experiences to succeed going forward. Having that kind of freedom is essential to building an established brand. AC: What was your strategy when you came into the company? ET: My intent coming in was to watch, listen and learn, but within a couple weeks I saw a lot of low-hanging fruit that I just couldn’t be passive about. Because Biossance is still essentially a startup, there were opportunities to have quick wins and make an impact on the business, so I accelerated observations and analytics and started moving the needle. AC: How do you create a strong workplace culture to foster overall company success? ET: The past 20 years have seen a big change in workplace culture. The old, hierarchical, fear-based management structure is completely out the window. Successful management is about humility and transparency. The workplace is competitive, and if employees don’t feel respected, they’ll move on. There’s also been a big shift in responsibility. Employees used to have the mission of nurturing the company, period. These days, successful work cultures have a balance between employees nurturing companies and companies nurturing employees. At Biossance, we focus on sustainability and No Compromise Beauty . We also try to make our employees’ lives better through little things like catered lunches, bagels, happy hours, unlimited PTO policies, etc. This shows a progressive, employee-oriented culture, and it encourages project ownership and loyalty. Instead of asking “what can I get away with?,” employees ask, “what makes sense for my responsibilities?” My own personal management philosophy is that the team comes first. Titles really don’t matter. I don’t want ego or attitude; I want transparency and ideas. When we do something successfully, I’ll put my team in the spotlight. If we make a mistake, I’ll stand in front. That’s the management philosophy that brings out the best in everyone. AC: What does it take to be a successful head of digital/technology at a startup vs. a larger corporation? ET: As you move up within an organization, it’s important to have a greater breadth of experience so that you can handle diverse expectations. In a larger corporation, it can be hard to effect change while managing up and down and dealing with legacy infrastructure. At a startup, on the other hand, everything moves fast, and you want to make as few mistakes as possible so that you can develop efficient processes for people. To do this, you have to know what success looks like in different business environments. AC: With over 20 years of experience in the digital transformation and e-commerce world, what are your key takeaways from a branding perspective? ET: Digital has evolved to become the premier medium for branding. Today, brands that don’t fill all digital channels — influencer programs, social media, paid media, search and email — cannot survive. People innately seek validation and inspiration, especially in the beauty industry, and this comes from influencers and social media groups. For instance, some beauty product customers enter SKUs into Instagram to discover how others are using products. Brands, product information, social inspiration, ratings and reviews, etc., all feed into our buying decisions, so it’s essential to give customers this information. AC: What are your plans and objectives for tech strategy and implementation across digital channels? ET: We have to get the basics right first. We’re doing that now by cleaning up our user experience so customers can find products, using basic best practice e-commerce features, showcasing best-sellers and products of the month, etc. We’re also beginning to plan for strategic innovation. I want to look at how we can push personalization and artificial intelligence. Do we need a chatbot, live chat, or a hybrid? What about a product decision tree tool to make stronger personalized recommendations? For instance, we’ve seen a lot of technology evolve over the last few years related to before-and-after imaging. Showing product payoff on live models works great, but it’s hard to accurately render a virtual before-and-after at the customer level. AC: Based on the evolution of the industry that you’ve seen so far, what can we expect in the next five years to 10 years? ET: It’s a little cliche, but I genuinely believe that using technology to connect with consumers will become more natural across all channels. There are still some areas of life that technology hasn’t entered, and there’s some healthy resistance to this related to privacy, but eventually the digital experience will permeate much more than it does today. When this happens, I think we’ll have a level of personalization we’ve never seen before. Shopping today is about making it easy for customers to find and buy products. However, we’re hitting the limits of consumers finding products. The next step will be products finding consumers based on buying habits, activities, etc. If you can stitch omnichannel activity together, a profound story about each customer will emerge that will enable retailers to spoon-feed customers exactly what they want. It will save consumers time, and conversion rates will go up as well. AC: Which innovators and leaders do you follow for inspiration? ET: My inspiration comes from individuals in all walks of life. I don’t put anyone on a pedestal no matter how well known they are. I’m inspired by Obama. I’m inspired by my wife and my son. I’m inspired by the people I get to work with every day. This shows care for people, not just their titles, which is the key to success in the workplace and in branding. AC: Love it, Eric. Thank you for sharing your vision and philosophy. We’ll stay tuned for more great things coming out of Biossance!", "date": "2019-6-12"},
{"website": "Avenuecode", "title": "Cloud Patterns: Best Practices for Cloud Projects on Every Provider", "author": ["Douglas Augusto"], "link": "https://blog.avenuecode.com/cloud-patterns-best-practices-for-cloud-projects-on-every-provider", "abstract": "Cloud computing is becoming an increasingly popular solution for companies. The main benefits of cloud environments are: paying only for what you use, quick scaling of resources, and focusing your team time on evolving your product by using managed services. It's very easy to start a cloud project, but starting without a thoughtful structure can complicate management later. This blog lists best practices for starting your cloud project. There are several cloud providers, and each one has its own products and services stack, benefits, and differentials. No matter which provider you're using, however, there are best practices and processes you can follow to help ensure better cloud governance, secure access to resources, and give better visibility to facilitate management and monitoring. To create a solid foundation for your cloud project, you'll need to consider: Project Structuring, Billing Management, Identity Access Management, Network Structuring, Logs, and Monitoring . Below, we will discuss some best practices for each of these areas. It's important to structure your cloud project so that your resources are arranged in a manner that reflects your company's organizational structure. This will help you give access to appropriate team members. It's also important to create markups on your resources to be able to view billing from different perspectives, generate reports, and make it easier to grant access to resources. Some good practices include: Several companies are segmented by area, and each area has its own budget to invest, so it's important to have several segmented billing accounts and to link specific projects to each of them. Another good practice is to record your budgets in the cloud and set up alerts. Some good practices include: It's always important to consider security, and this is no different when it comes to cloud projects. Be sure that users of your projects have granular access to what they need to use--nothing more and nothing less. Every cloud provider gives users ways to create custom roles, which helps you create permissions that reflect your team structure. Another helpful feature is to create user groups within a corporate email, making it easy to manage onboarding and offboarding processes for a member of your project. Some good practices include: Cloud projects can also have custom network rules, which are extremely important in ensuring application isolation. It is also smart to create custom firewall rules according to the needs of your applications to avoid unnecessary exposure of resources to the Internet. For companies that need to connect their local infrastructure directly to the cloud, VPN or a direct link can be helpful to ensure security and low latency. Some good practices include: Every cloud provider has its own centralized logging system, but there are good practices that make it easier to view these logs and extract insights from them. It is important to create a pattern for both the log format structure and the fields that each log has. Regardless of whether you have a diverse stack with several programming languages, it is important that your logs speak the same language. It is also important to filter sensitive content in the logs. Some good practices include: Logs are extremely important, but with logs alone, it's hard to gain insight into your applications, so it's always smart to create ways to visualize the health of your product or workload in the cloud. Try creating dashboards by product and/or workload as well as creating alerts so that you're the first to know if something stops working. Some good practices include: Good cloud utilization is based on using only what you need, so a solid management of your environment can make this a lot easier to achieve. I hope this blog helps you as you start a new cloud project and/or review decisions for current projects.", "date": "2019-9-25"},
{"website": "Avenuecode", "title": "Building a Career Path through Programming Books", "author": ["Rafael Romão"], "link": "https://blog.avenuecode.com/building-a-career-path-through-programming-books", "abstract": "In my first years as a self-taught programmer, my main source of information was not books. I read a few, but most of the time, when I needed help, I just looked for a blog post or tried to come up with something myself, often reinventing the wheel in the process. Obviously, the quality of my programming work suffered, and one day I decided to look for books that could give me a more complete understanding of my subject of interest. After a time, I concluded that I learned much more from books than from any other resource. In this article, I would like to share with you some books I've read in the last few years that have proven extremely relevant to my career as a software developer. To make my recommendations easier to follow, I have selected just 12 books, which may be organized into the following 4 categories: Agility, Pragmatism, Software Design, and Software Architecture . C lean C ode: A Handbook of Agile Software Craftsmanship , by Robert Martin: Similar to The Pragmatic Programmer, Clean Code describes a set of technical skills that will help you to be a better software developer. Because everyone is at a different place in their career, I can't definitively say these are the best books for you to read right now--though I'm sure you will lose nothing by reading them--but they have laid a great foundation for me. Because I learned so much through them, reading technical books is something I will continue doing for the rest of my career. If you want to check out what I'm reading right now, follow me on goodreads . com /rafaelromao , and if you have a book you consider a must-read for every software developer, leave a comment below! Feature Image: Freepik", "date": "2018-9-5"},
{"website": "Avenuecode", "title": "What's the Hype Around Hybris and Its Accelerators?", "author": ["Flavio Silva"], "link": "https://blog.avenuecode.com/whats-the-hype-about-hybris-and-its-accelerators", "abstract": "If you're in the world of E-commerce, you've probably heard of Hybris - the SAP omni-channel platform used to build robust virtual stores. But what exactly has caused there to be so much hype around Hybris? Let's start with an overview of what it is. SAP Hybris Commerce Nowadays, E-commerce is a hot topic. Consumers expect to seamlessly purchase products and services online. However, developing a feature-rich online customer experience has proven to be a challenge for many organizations, and implementations have often been complex, tedious, and costly. Moving from single-channel to omni-channel adds even more complexity to the process. Furthermore, consumer behavior is rapidly changing. Individuals now purchase products through print catalogues and stores, as well as online and through their mobile devices. In order to remain competitive, organizations must be able to consistently adapt to these changing trends. This requires an omni-channel commerce framework that is easily customizable and extendable without restrictions. Accelerators enable organizations to easily build a future-proof, omni-channel commerce solution. They have all the functionality and business tools necessary to create an engaging customer experience, improve conversion, as well as simplify management. Imagine delivering a brand new store from scratch in 13 weeks! Impressive, right? That can all be made possible by using Accelerators. Let's take a look at them. In order to use them, we need to install a recipe from a number of different accelerators recipes. When that's finished, we will have a complete example store running on Hybris, ready to be fully customized. Here are a few accelerators that are ready to use: -B2B (Business-to-business) -B2C (Business-to-customer) -Telco (Telecommunications) -Financial Services -China (based on E-Commerce in China) There are also some Accelerators that can be purchased outside of the Hybris suite. For example, the Travel Accelerator was used to build travel agency stores, and ended up being one of the most accessed sites worldwide. All Accelerators already contain all the business logic needed to run their main functionalities based on the Accelerator chosen, so the only remaining work would be to apply the company specifications to the website and business logic. E-Commerce projects can be challenging, but compared to many other commerce solutions, Hybris Accelerators can be implemented quickly and cost-effectively. Let's take a look now at the pros of implementing Hybris. Speed: Hybris is packed with ready-to-use code and functionalities. With the use of Accelerators, you can deliver a brand new system in weeks. Easy customization: Every functionality can be easily extended and modified by the needs of the client. Business control: With the cockpits, business users have control over several functionalities and customizations, lowering the need for big support teams and deployments for creating or updating the system. Integration: Hybris can integrate with a number of different platforms and systems, whether they're from SAP or not. It also has a ready-made web service module which allows it to expose its types (CRUD), and some functionalities, to rest endpoints. Complete: Hybris is a complete, robust, and powerful platform that contains almost all functionalities an E-commerce organization would need. See? Hybris is pretty cool. It's pretty clear why there's so much hype around Hybris, and the enthusiasm is only growing. With an increasing number of partners and clients, companies are having to create specialized services in many areas, which is where Hybris comes in.  Hybris has the unique ability to adapt and fulfill every market's need and is necessary in order to keep up with rapidly changing consumer needs. Spring Framework Lucene Search Drools ZK Framework CMS", "date": "2017-10-24"},
{"website": "Avenuecode", "title": "Why is Mentoring Important?", "author": ["Henrique Ramos"], "link": "https://blog.avenuecode.com/why-is-mentoring-important", "abstract": "When I joined the Avenue Code team as an intern, I was introduced to mentoring: a guidance system where seasoned developers share their knowledge, skills, and experience to assist their colleagues toward successful development. In this post, I will talk about my experience as a mentee, as well as what I learned along the way. As we interns were introduced to the company, we were also introduced to the technologies we would use. The project I was assigned to had a front-end that was built using React, Redux, and Sass, and its back-end was built using Java and Spring MVC. Most of the newcomers had never had contact with any of these languages, libraries, and frameworks, hence the mentors had two missions: teach their mentees and ensure everyone was at the same skill level. To do so, they sent challenges, reading material, and research directions to us, solidifying what we learned in subsequent meetings. This was a great way to start because, though the instructors gave a helping hand, the rookies were also able to learn by themselves. My team made its first sprint after two weeks of learning. On the iteration end, we had to present the results to our client and then receive feedback. There were appraisals and, most importantly, constructive criticism. I don't think anyone is a big fan of reviews that highlight what could have been done better, but they help us improve and foster a constructive debate about things that went wrong. However, criticizing is like walking on eggshells, where the eggshells are someone else’s feelings. Because constructive reviews can easily be misinterpreted as offensive, it's important to remember that feedback is an art. In his book Clean Code , Robert Martin talks about the traits needed to be a professional developer. Professionalism is a topic rarely discussed in software development companies, but it's very important. Being professional is not necessarily about wearing a suit to go to work, nor is it about using a fancy vocabulary; it’s about commitment and knowing how to deal with expectations . This was taught by the mentors both consciously, when we read and talked about Uncle Bob’s book, and unconsciously, when they acted accordingly, thus becoming role models for us. One thing that mentors should always keep in mind is that they are not tutoring mentees to be obedient. Discussing techniques and technologies is the key not only to learning about their advantages, but also about their downsides. A good practice for tutors is to question the approaches used by the apprentices, encouraging a debate so that everyone can put their selling points on the table to choose an optimal solution. There are a thousand ways to solve the same issue when developing, so engineers should be able to come up with the best way possible. Remember, there are best practices, but t here’s no one size fits all in programming. The true goal of mentoring is to prepare mentees to stand on their own . As the internship program continued, the mentors were also becoming less necessary, because the team was able to take ownership of decisions, meetings, reviews, and everything else that they learned throughout the journey. To be honest, it was really hard to let that helping hand go, because no one would be there all the time to help us make better choices, but this is how it should be - sometimes we need to learn from mistakes, and that's OK. Mentoring is great. It can be a wonderful experience for both sides when done right. I learned way faster by having an expert with me who gave me guidance when I got lost and helped me avoid common pitfalls as I took my first steps. Mentors, on the other hand, can refine their soft skills and benefit from having someone new with whom to debate ideas and solutions to problems. Finally, tutors shouldn't be bosses. Avoid giving orders and ready-to-use answers, always question the mentee's decisions, and encourage discussions. Remember, you are not meant to instruct them to be like you, but to think by themselves. Of course, mentoring isn't the only way to learn, but it is definitely one of the most efficient ways.", "date": "2020-10-7"},
{"website": "Avenuecode", "title": "Android Data Binding - Part 1", "author": ["Marcus Vinicius Corrêa Barcelos"], "link": "https://blog.avenuecode.com/android-data-binding-part-1-2", "abstract": "Android applications are, almost universally, data driven. All screens have a data object that can be fetched from the internet, database, or even created by the application user. Typically on Android apps, developers need to inflate the view, locate each view element using findById, and then set the appropriate value. To make this process faster and more effective, Google released the Data Binding Support Library, which provides a cleaner way to tie upthe code logic to the view layer, thereby minimizing the necessary work to reference each view element. The Data Binding API uses the Java Annotation Processor to generate the java classes on compile time. It's a kind of java compile plugin that inspects the code, looking for particular annotations to generate or modify the code. The only two requirements to use data binding are: android { .... dataBinding { enabled = true } } All layouts should have the tag <layout> : <?xml version=\"1.0\" encoding=\"utf-8\"?> <layout xmlns:android=\"http://schemas.android.com/apk/res/android\" xmlns:tools=\"http://schemas.android.com/tools\"> .... </layout> When the project is compiling, the plugin will look for layout files with this tag and generate an equivalent java version. This class will have the same name as the xml file, converting it to Pascal case and \"Binding\" suffix, so if the layout's name is activity_main.xml , the java class will be ActivityMainBinding.java . The layout inflate will be a little bit different, using the DataBindingUtils.java helper: Inside activities: @Override protected void onCreate(Bundle savedInstanceState) { super.onCreate(savedInstanceState); ActivityMainBinding binding = DataBindingUtil.setContentView(this, R.layout.activity_main); } Inside fragments: @Override public View onCreateView(LayoutInflater inflater, ViewGroup container,Bundle savedInstanceState) { ListItemBinding binding = DataBindingUtil.inflate(layoutInflater, R.layout.list_item, viewGroup, false); return binding.getRoot(); } One of the best features provided by data binding is the layout binding. It is similar to ButterKnife , but you don't need to create an instance of each view with the respective annotations. The entire process is automatic, and the plugin will generate a class with all elements. The only requirement is to define a view id. activity_main.xml <?xml version=\"1.0\" encoding=\"utf-8\"?> <layout xmlns:android=\"http://schemas.android.com/apk/res/android\" xmlns:tools=\"http://schemas.android.com/tools\"> <RelativeLayout android:id=\"@+id/activity_main\" android:layout_width=\"match_parent\" android:layout_height=\"match_parent\" android:paddingBottom=\"@dimen/activity_vertical_margin\" android:paddingLeft=\"@dimen/activity_horizontal_margin\" android:paddingRight=\"@dimen/activity_horizontal_margin\" android:paddingTop=\"@dimen/activity_vertical_margin\" tools:context=\"com.binding.data.avenuecode.databindingexample.MainActivity\"> <TextView android:id=\"@+id/hello_world_text_view\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:text=\"Hello World!\" /> </RelativeLayout> </layout> The API generates the view instance inside the ActivityMainBinding.java: public final android.widget.RelativeLayout activityMain; public final android.widget.TextView helloWorldTextView; A parameter will be created for each element with defined id, generating the name based on id and converting it to Pascal case. That way, it is possible to access all views and make all necessary changes, much faster and easier than findViewById or ButterKnife. Another important feature implemented by the API is binding objects. Now it is possible to use instances of objects directly on the layout. First, insert the tag <data></data> above the root ViewGroup, then add the tag <variable> : <?xml version=\"1.0\" encoding=\"utf-8\"?> <layout xmlns:android=\"http://schemas.android.com/apk/res/android\" xmlns:tools=\"http://schemas.android.com/tools\"> <data> <variable name=\"mainActivity\" type=\"com.binding.data.avenuecode.databindingexample.MainActivity\"/> </data> <RelativeLayout android:id=\"@+id/activity_main\" android:layout_width=\"match_parent\" android:layout_height=\"match_parent\" android:paddingBottom=\"@dimen/activity_vertical_margin\" android:paddingLeft=\"@dimen/activity_horizontal_margin\" android:paddingRight=\"@dimen/activity_horizontal_margin\" android:paddingTop=\"@dimen/activity_vertical_margin\" tools:context=\"com.binding.data.avenuecode.databindingexample.MainActivity\"> <TextView android:id=\"@+id/hello_world_text_view\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:text=\"Hello World!\" /> </RelativeLayout> </layout> The expressions to access these object properties need to be written inside the attribute @{} : ... <TextView android:id=\"@+id/hello_world_text_view\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:text=\"@{mainActivity.helloText}\" /> ... The property can be accessed publicly or through getters: ... public String getHelloText() { return \"Hello World!\"; } ... DataBinding is a powerful API that enables development with less code, in a faster and more organized way. One disadvantage is that because the documentation is subpar, and due to be uploaded to a new library, it is sometimes difficult to find help for advanced uses. I will introduce some advanced features in future articles, and explain the advantages of using MVVM with DataBinding.", "date": "2017-2-1"},
{"website": "Avenuecode", "title": "AC Spotlight - Rogério Pires", "author": ["Sthefanie Mingall"], "link": "https://blog.avenuecode.com/ac-spotlight-rog%C3%A9rio-pires", "abstract": "We had the opportunity to catch up with Rogério Pires, CIO at JHSF. JHSF is a leader in high-income real estate in Brazil, with significant operations in the residential and commercial development markets, as well as the development and management of shopping malls, high-end hotels, and an international executive airport. Avenue Code: Rogério, we did some research and saw that you have a lot of experience working on significant projects in large companies. Can you tell us a little about your journey to JHSF and the world of IT? Rogério Pires: I started my career in finance at Volkswagen. I joined as a trainee and then went through a job rotation where I had the opportunity to work with technology. Since then, technology has been one of my passions, along with business. After working for some time, I received an invitation to coordinate a structuring project for the technology area of a multinational company called Schaeffler. From then on, I was invited to work at Walt Disney Brazil. That was actually one of the most challenging experiences because I entered a completely different and innovative universe where IT and business were highly intertwined. After a few years, my career changed course as I shifted from the business and IT sector at Walt Disney to a construction company. There, I was able to help implement new systems such as BI, CRM, and ERP. When I arrived at JHSF in 2013, my main mission was to restructure the IT department while taking all business units into account. A few of JHSF properties throughout South America Avenue Code: JHSF has several business units: incorporation, shopping malls, retail stores, hotels, restaurants, and now an executive airport. What is it like managing multiple business units internally? Rogério Pires: Each business unit has its own technology solution, and they have specific software that is consolidated into a hold through a single ERP. With that said, managing all these business units requires the integration of business verticals (shopping malls, incorporations, hotels, retail, and etc.) in the central ERP - thus creating the possibility to consolidate results and disseminate them. Avenue Code: Since different business units use different information systems, in your opinion, what was the biggest challenge during the implementation of processes during the JHSF digital transformation? Rogério Pires: The biggest challenge was standardizing and unifying all the processes of each business unit within the central ERP. It was challenging because we had to integrate all the different softwares within a short period of time. The entire project took 7 months to run and deliver. Avenue Code: How did you manage internal communication during the digital transformation? What did you use to help integrate teams? Rogério Pires: The way we carried out internal communication was very interesting. When the project came to an end, we \"turned the key\" and the system became live for all users. Before that, we ran many tests during the creation of the project - I would even say it was an exhaustion of tests, because we ran two sets of unit tests as well as three sets of integrated tests. We had approximately 40 people involved in this project, among them, consultants who mainly participated in the deployment and key users of each area using the system in real time. I felt that the impact was very small because we never stopped billing, and there was no interruption in any business activity. We also found other ways to minimize the impacts of the transition by training around 500  users on how to use the platform prior to its launch, and the adoption of informational dashboards across the company floors. We also circulated a weekly newsletter announcing the next steps of the project and the progress of each stage. It was clear that system changes would greatly impact people's daily lives, so transparency was the key to success. Avenue Code: We know that digital security is an important issue - how has JHSF put this issue to work with cloud projects? Rogério Pires: We have some cloud projects we’re working on today. For example, our CRM. We are projected to have more activities that may go up in the cloud for next year, and we understand it’s an irreversible process. Security is always linked to good practices within the company, and we believe that it’s imperative to train people and warn them regarding the danger of opening emails or downloading files from unknown and suspicious sources. There’s no point in having a good antivirus or firewall if people are not aware of good practices on the internet. Avenue Code: At Avenue Code, all of our IT professionals have a consultant profile, which means that we ensure there is a good understanding of the client's business before working on proposed solutions. What is the profile of your IT team? Rogério Pires: Since 2016, we’ve been doing a lot of work trying to bring those working in IT closer to those working in business. For example, incorporation experts and shopping malls are essential in order to be less technical and more business. We also encourage internal job rotations so that our employees can experience all the areas of business in practice, and learn more about them. Avenue Code: Because JHSF has a good deal of retail business units, how do you think retail is being impacted by the avalanche of e-commerce, and how do you see the future? Rogério Pires: According to surveys, it’s predicted that the role of physical stores will continue to diminish. We view e-commerce, and the future of shopping spaces, as a collaborative space for people to interact with others. It’s important to captivate the customer by providing differentiated services. The Cidade Jardim shopping mall for example, is a place where we have exclusive and imported brand stores, to attract people without abandoning the experience of strolling through a pleasant environment. I believe that more and more places like that shopping center will be relevant because of the value of customer experience. Avenue Code: Thanks for your time Rogério, it was such a pleasure learning more about JHSF and your journey towards becoming a CIO.", "date": "2017-12-20"},
{"website": "Avenuecode", "title": "How to Build a Generalist CRUD API in NodeJS", "author": ["Lucas Teixeira"], "link": "https://blog.avenuecode.com/how-to-build-a-generalist-crud-api-in-nodejs", "abstract": "This Snippet presents a backend model project in Node.js that supports the generic CRUD API based on typescript-rest (express.js extension based on annotation for typescript library). This model project is capable of accelerating application development and facilitating its maintenance. Because of its standardization proposal, the results are code reuse and good development practices. The acronym CRUD (CREATE, READ, UPDATE, and DELETE) is an abbreviation for the most common operations to be performed on databases, so CRUD API is the way to carry out these operations through the HTTP protocol (POST, GET, PUT, and DELETE). The project ( ac-node-generic-crud ) intends to expose CRUD routes to the specified data model by using the Design Pattern Template Method. Another benefit of this model project is its ability to generate automatic Swagger documentation using the typescript-rest library. With a simple implementation, the developer will be able to extend concrete classes from abstract classes. These abstract classes have controller, service, and repository layers that handle generic objects, requiring only the data model to be operated by the subclass. After using these features, all exposed CRUD endpoint routes will be able to manipulate the entered data model. Before we talk about how to build a generalist CRUD API in Node.js, let's review the design patterns we'll be using and how they work. But before that, what exactly are design patterns? In software engineering, a design pattern is an optimized solution to a common problem that exists in various system implementation demands and can be reused in different situations. It is also a template that can be customized to solve a specific problem in code. Of course, each developer thinks and codes differently, and because of that, some challenges start to emerge. Continuing another developer's work can be a daunting and unproductive task, especially if there isn't a design pattern to follow. Of course, these development hurdles go beyond simplistic analysis. The demand for projects to be delivered quickly, for example, often prevents teams from worrying about documentation, so it becomes a secondary activity. In practical terms, imagine the following scenario: Developers may be pulled in and out of projects as some developers are assigned to new projects and others on bench are called in. Some internal projects may have problems in their continuity given the lack of standardization of codes and the lack of documentation, making it difficult for another developer to understand the project. A standard design scope is then required to make it easier to understand the code already developed. The biggest challenge now is understanding the business rule. From a corporate standpoint, another design pattern benefit is customer acceptance. Maintaining a standard means maintaining quality and level of development. So internal organization, if well presented, can be used as a selling point for potential customers. Let's briefly review some design pattern definitions: Creational Patterns \"In software engineering, creational design patterns are design patterns that deal with object creation mechanisms, trying to create objects in a manner suitable to the situation. The basic form of object creation could result in design problems or added complexity to the design. Creational design patterns solve this problem by somehow controlling this object creation\" ( Source Making ). Structural Patterns \"In Software Engineering, Structural Design Patterns are Design Patterns that ease the design by identifying a simple way to realize relationships between entities\" ( Source Making ). Behavioral Patterns \"In software engineering, behavioral design patterns are design patterns that identify common communication patterns between objects and realize these patterns. By doing so, these patterns increase flexibility in carrying out this communication\" ( Source Making ). Decorator - Structural Patterns The Decorator pattern lets you dynamically attach new behaviors to objects, extending their capabilities and functionality. Singleton - Creational Pattern The Singleton pattern ensures that a class has only one object as its model instance, but it gives a global point of access to that instance. Image courtesy of Refactoring Guru Template Method - Behavioral Pattern: Template Method is a behavioral design pattern that specifies an algorithm skeleton, allowing subclasses to implement their own methods without changing the base structure. Its applicability is diverse, being recommended mainly for: Image courtesy of Refactoring Guru The ac-node-generic-crud project generally implements a three-tier architecture: controller, service, and repository. The controller layer is responsible for routing and validating objects, also implemented in a generic way. The subclass simply implements the getValidationSchema() method and specifies attribute information to the Joi object validator. The second layer (service) implements business rules. Considering that CRUD operations will be performed using as reference the primary key of each object, the Service layer is responsible only for the redirection of data to the repository layer. Finally, we have the repository layer, which uses TypeORM as the interface to various relational and nonrelational databases. The first version of the template project supports MongoDB, MySQL, and Postgres. To use other databases, you only need to change the class responsible for the connection. See some examples of connections in the ac-node-generic-crud project database directory (path: src/database). The implemented model project already has all the necessary settings for the Typecript-Rest library to expose the Swagger documentation of its API. Simply use the data model for three-tier architecture (inheritance of abstract layers), and then, after application build, routes will be available in the url localhost:7000/api-docs. Mocha is a test framework that configures test suites, and Chai is an expectation framework that makes it easier to build tests against code. The model project is set up with Mocha and Chai and contains some example tests. After testing, coverage reports can be viewed in the /reports/coverage directory. The report has 4 metrics: Negative Points Typescript-rest is an annotation-based express.js extension for typescript. This library allows you to use ES7 decorators to configure their services. Using this feature provides automatic Swagger documentation generation. Typescript-rest-boilerplate is a minimalist project based on the typescript-rest library. It has an initial configuration for building APIs, with some examples, and support for testing using Mocha and Chai. It already uses some development aids such as TSLint, clustered server instances, some middleware, and MongoDB connection. I hope you enjoyed this guide on how to build a generalist CRUD API in NodeJS! For references and further reading, check out: Refactoring Guru Source Making NPM, IoC Container for Typescript NPM, REST Services for Typescript GitHub, Typescript-REST-Boilerplate TypeORM Gamma, E., Helm, R., Johnson & R., Vlissides, J. (1995): Design patterns: elements of reusable object-oriented software.", "date": "2020-2-19"},
{"website": "Avenuecode", "title": "Using Deep Convolutional Neural Networks (DCNNs) for Time Series Forecasting Using Tensorflow - Part 1", "author": ["Hossein Javedani Sadaei"], "link": "https://blog.avenuecode.com/using-deep-convolutional-neural-networks-dcnns-for-time-series-forecasting-using-tensorflow-part-1", "abstract": "Have you ever wondered how to convert a prediction problem into a new format so that you can solve it using available strong forecasting engines? In Part I of this tutorial, I will discuss how to solve one of the most challenging forecasting problems--the next state forecasting trend of electricity consumption--by using a Deep Convolutional Neural Network (DCNN) to process a series of load data that’s been converted into images. This solution enables a possible testing set accuracy of 88%, which is impressive for this type of application. For the programming component, we’ll use Python and Tensorflow. Introduction to CNNs Before beginning this tutorial, let’s review some fundamentals about Deep Neural Networks. Deep Neural Networks (DNNs) learning is part of a broader family of machine learning methods based on learning data representations as opposed to task-specific algorithms. Convolutional Neural Networks (CNNs), which we’re using to solve today’s problem, are a subset of DNNs. Convolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex, where individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field . DCNNs, however, are typically organized into alternating convolutional and max-pooling neural network layers followed by a number of dense, fully-connected layers, a structure that was introduced by Krizhevsky et al . An example of DCNN topology can be seen in the following image . (For a simple and detailed explanation of DCCNs, click here .) The structure of DCNNs makes them very suitable for image classification and automatic speech recognition. In this tutorial, we will examine how to use DCNNs to solve an electricity consumption prediction. So far, DCNNs have been primarily used in computer vision applications, such as facial recognition, image classification, action recognition, and natural language processing (speech recognition, text classification, etc.). In this blog, however, I am going to show that DCNNs can also be used in the completely unrelated field of electricity consumption forecasting. We’ll also discuss how to prepare data to feed into a DCNN, which is a process that goes almost entirely unaddressed in existing Tensorflow tutorials. Lastly, I will also discuss details about the models, along with their related technical programming parts. Short-term load forecasting is vital for all energy supply companies. If demand is underestimated, consumers’ needs are unmet, yet if it is overestimated, electrical energy will be wasted since it cannot be stored. For enterprise companies, every percentage point gained in accurate forecasting can translate to as much as $15,000 saved daily , making accurate forecasting critical. Consumer demand for electricity, however, is always in flux and is influenced by a number of complex factors for which statistical forecasting models must account. We will use a DCNN today because such models are designed to automatically extract features from data. Data For the purposes of this blog, we are using a series containing 17,520 items of half-hourly electricity consumption records for one full year of data from a big power supply company. As seen below, the time series is very complicated and contains several hidden seasonality patterns that our DCNN can easily capture and use to produce a relatively accurate prediction. As with any DNN model, our DCNN requires plenty of training data to produce an accurate result, so our time series sample below is sufficient to build a good forecasting model. You can download data here . Because DCNNs are designed to interpret data in the form of images, we will convert our half-hourly load data into gray-scale images with their associated classes. The classes/labels that we are going to predict are 1 if the consumption of the next half hour of electricity compared with the current state is going up or 0 if it is going down. Our methodology, then, involves converting half-hourly load data into several images that can be used to predict the trend in sequential electricity consumption. Data Preprocessing Before using our DCNN for time series forecasting, we have to convert equal chunks of time series into images. To show how this works, we’ll use this small and extremely simplified time series as an example: [23, 45, 31, 95, 81, 52, 83, 56] Suppose that the width and height of the images we are going to make are both 4. (Later, in our real case, the width and height will both be 32.) Step 1: From left to right, select 4 numbers and create successive vectors with four rows of windows until we reach the end of the 4 x 4 series: [23, 45, 31, 95] [45, 31, 95, 81] [31, 95, 81, 52] [95, 81, 52, 83] Step 2: Next, we need to produce a label for each window. If the last value in a vector is less than the last value in the next vector, then we will assign 1 to that vector label. If it is greater, we assign 0. This gives us: [23, 45, 31, 95]   0 (because 81 is less than 95) [45, 31, 95, 81]   0 (because 52 is less than 81) [31, 95, 81, 52]   1 (because 83 is greater than 52) [95, 81, 52, 83]   0 (because 56 is less than 83 We assign values of 0 and 1 to help us predict whether the energy consumption of the next state of time will increase or decrease from the current state. This helps us build a training set. Step 3: For each vector, keep and put the position of the members of the equivalent sorted vector correspond to other members of the main vector, in a new array, so the following is the output, [0, 2, 1, 3]   0 (position of 23 is in 0th position, 45 is in 2th, for 31 it is 1th, etc.) [1, 0, 3, 2]   0 [0, 3, 2, 1]   1 [3, 1, 0, 2]   0 Step 4: Finally, we now have enough information to build our gray-scale images. For each member of each vector obtained from a former step, add a corresponding vector. By using one hot encoding techniques , we can create an equivalent matrix that can be interpreted as gray-scale images as follows 1, 0, 0, 0 0, 0, 1, 0     0 0, 1, 0, 0 0, 0, 0, 1 0, 1, 0, 0 1, 0, 0, 0      0 0, 0, 0, 1 0, 0, 1, 0 1, 0, 0, 0 0, 0, 0, 1      1 0, 0, 1, 0 0, 1, 0, 0 0, 0, 0, 1 0, 1, 0, 0      0 1, 0, 0, 0 0, 0, 1, 0 By following these last steps, we can establish our gray-scale images and their corresponding labels. In our application, however, the width and height of images are both 32, so each image is going to be a numpy array of 32 x 32 dimensions. We will feed these images into a DCNN to train our classification model in order to build a prediction model which can predict the trend in the next half-hour consumption of electricity. In our next blog, we will develop a Python class to employ the data produced above in conjunction with developing a Tensorflow model for deploying DCNNs. In this tutorial, we explained how to build a forecasting model for time series analysis by using  DCNNs. To employ a DCNN, we first need to convert our time series into images. To do so, we showed a step-by-step process of preparing data in text. In our next blogs, we will use Python and Tensorflow to finish solving the problem. The codes related to this problem will be discussed as well.", "date": "2018-5-9"},
{"website": "Avenuecode", "title": "ETLs: Connecting End-User Transactions to Business Insights", "author": ["Arlindo Neto"], "link": "https://blog.avenuecode.com/etls-connecting-end-user-transactions-to-business-insights", "abstract": "Today, we'll discuss the ways in which ETL tools can connect, consolidate, and sync data between OLTP and OLAP systems. We'll begin by defining the uses of OLTP and OLAP systems and then explore how ETL tools can connect them. First, let's define the uses of OLTP and OLAP systems. There are essentially two kinds of database design: One is used to accomplish the requirements of an end-user application and the other to provide modeled records for data engineers. The former is designed for OLTP (Online Transactional Processing) systems and is used to persist operational data; it is characterized by a high-normalized schema, lots of tables, and a large volume of short, online transactions (INSERT, UPDATE, DELETE). The latter is designed for OLAP (Online Analytical Processing) systems and is used to persist consolidated data from foreign operational sources; it is characterized by a de-normalized schema with fewer tables (mostly using the star model for multi-dimensional expression queries), a small volume of transactions, and higher performance so that it can run complex queries over a huge volume of data — including historical records from past loads. Although OLTP databases are the foundation for any end-user application, they are not the perfect source for most of the queries required in the data mining processes. Their common-normalized schema, required to avoid redundancies  ( which could lead to data inconsistencies or the need for time-consuming transactions ) makes queries too complex and expensive, due to the many required table joins and subqueries. On the other hand, databases designed for OLAP systems do not need to account for data changes and related consistency issues. This allows for a de-normalized schema structure with fewer relationships. Also, since they commonly serve solely to analyze data and do not deal with application state repositories, time-consuming queries may be performed as part of the process when needed. OLAP systems require a special kind of schema that allows client tools to run multi-dimensional queries. This schema, commonly called star schema, is characterized by a main table, called a fact table, which stores dimensions (e.g. date, location, etc.) as foreign keys from various reference tables. It allows the system to handle the data as hypercubes, which could be sliced and diced, drilled down, rolled up, and pivoted. All these operations are the core from which data engineers can make sense of meaningless/raw data. With the advent and evolution of data mining processes and algorithms, OLAP database systems have became more necessary. Data, however, must still come from operational sources, which have been and always will be constrained by performance and consistency optimality. To allow both these worlds (OLTP and OLAP systems) to coexist, we need a process to connect them and keep them synced. This is where ETLs come in. The ETL (Extract ,  Transform ,  Load) is the main process that allows engineers to manage data for business intelligence and data mining analysis. It uses available operational databases as sources for a consolidated and well-modeled container that can handle required queries and experiments while avoiding side effects for application users. The Data Warehouse is the consolidated database, created by the ETL process, from one or more sources of data. Data Marts, which are small databases with a subject-focused dataset from the main DW, are important components of this architecture. In the majority of cases, the need for an ETL appears not during design but rather during the lifecycle of an application. Even when using new database architectures,  like document and graph databases  , which can have a hybrid model for both transactional and analytical processes, the ETL can be a valuable tool to consolidate other sources for a broader analysis. In these cases, there's a common need to sync the operational database with another to free more time and computer resource consumption, which ETLs can also do. Although ETL tools are more common in the enterprise environment, there are plenty of solutions available for low-profile companies. One of the best is the scriptella project . As a JVM-based project, it uses JDBC as its connection framework and an embedded script engine for the transform/load phases. For a more complete solution, there are the Pentaho tools . Besides offering a GUI ETL tool, Pentaho allows the architect to count with a BI platform. It also has a report designer tool as well as an OLAP database (Mondrian) that provides an analytical interface to the operational database through an in-memory, multi-dimensional schema. Many more tools are detailed in the awesome-etl project, which offers a list sorted by programming languages, including some other GUI-based tools. Conclusion ETL tools, then, are useful for connecting OLTP and OLAP systems while consolidating and syncing data. In some scenarios, you can only use your database's replication feature to make processes work. That is, an engineer has to make long and complex queries to create a continuous, synced instace of your data without affecting the application's usability. In other scenarios, you can use the same database as the application when its design is appropriate for the job. The best rule of thumb is: evaluate the effort to manage an ETL — and a data warehouse — and when it doesn't make sense, keep things simple. It's important, however, to recognize the solutions ETLs provide so that you can make smart decisions when application requirements start creating new problems. Happy coding!", "date": "2018-3-7"},
{"website": "Avenuecode", "title": "Getting Started with Ansible", "author": ["Rodrigo Rodrigues"], "link": "https://blog.avenuecode.com/getting-started-with-ansible", "abstract": "In this blog post, I will give an overview of Ansible by delving into how it works, the installation process, creating a playbook, and running commands. Ansible is a tool that allows you to automate all the tedious and repetitive tasks that we find in system administrators, such as: - Setting up the computer for new use - Installing, configuring, and entering new servers into the domain - Managing VPN users and deleting those that no longer exist - Or, for instance, preparing the new 300 VMs for the E-Commerce project, which should be created for tomorrow in Amazon AWS and RackSpace - in addition to the development on the Proxmox VE platform Impressive, right? Ansible allows you to automate all this. How Ansible Works One thing about Ansible that sets it apart from competitors is that it does not require you to install Agents or Servers. Simply install Ansible on your device, and you're ready to start. Ansible connects to the computers you want to configure using SSH, and sends you the instructions you want to run as well as the settings that should be applied. All this is parallel, so it does not matter how many computers you have to configure. Ansible Installation The easiest method is using the package manager from your operating system. # RedHat and Clones $ yum install ansible # Debian and Clones $ apt-get install ansible # Mac Os X with Homebrew $ brew install ansible The Inventory Ansible works with an Inventory of your platform or servers. This Inventory is a file in which you group and list the servers according to what you like: # Archive hosts.cfg inventory [dbs] db1 . example . com db2 . example . com db3 . example . com The hosts.cfg file contains a server group: dbs which has 3 servers. This is a simple Inventory whereas your company's Inventory can be much more complex. Creating Recipes (Playbooks) A playbook is a text file written in YAML where we indicate the commands that we want to execute on the clients. For example: --- - hosts: servers tasks: - name: Installs nginx web server apt: pkg=nginx state=installed update_cache=true notify: - start nginx handlers: - name: start nginx service: name=nginx state=started The tasks will be carried out in all the clients that are a part of the group \"servants\". In this case, the Nginx package will be installed, making sure that the package list is updated (update_cache = true). The service will only restart if this task is called \"handlers\". To run the playbook, assume we have saved it in a playbook.yml file: Ansible-playbook playbook.yml You will use the ansible.cfg file parameters. Running Simple Commands The first command that we are going to use will allow us to verify that we have access to the clients: 1 $ ansible -i hosts.cfg -m ping all Another example is where we check the memory available on all servers: 1 $ ansible -i hosts.cfg all -m shell -a 'free -m' -U root As in the two examples, we indicate the Inventory file that we are going to use: -i hosts.cfg. And in the last one, we indicate the user with which we will connect: -U root. Remember that the connection is made with SSH. Another example that can be useful is applying the update of Time to all the servers. We can assume that they are of type Red Hat: 1 $ ansible -i hosts.cfg all -m yum -a \"name=tzdata state=latest\" From the command it is important to note the following: Conclusion Looking even at the little presented so far, we can see how powerful and useful Ansible is when it comes to managing the configuration and scheduling of servers. In the examples presented in this document, we perform tasks on only one host, but what if we had tens, hundreds, or even thousands of hosts to manage? Ansible helps a lot with those types of tasks. This has just been an introduction to Ansible and doesn't display its full capabilities. There's still much more to explore, such as Vault, which lets you encrypt sensitive information like passwords to be stored in version control. If you're interested in learning more,  just stay tuned here!", "date": "2017-8-31"},
{"website": "Avenuecode", "title": "REST vs SOAP", "author": ["Guilherme Teixeira"], "link": "https://blog.avenuecode.com/rest-vs-soap", "abstract": "These questions can pop up in any Web project and must be clarified before the initial implementation. Let's start by understanding the concepts of REST and SOAP and draw comparisons about their differences. Basic Concepts Representational State Transfer is an architectural style for distributed hypermedia systems . REST is a style of architecture with a set of constraints which allows data transfer over a standardized interface, such as HTTP. REST relies on a simple URI to make a request. REST is resource based, which means that in a REST service we will work with \"things\" to define the service. The clients can utilize the REST resource to access a unique URI and the resource is then returned back to the caller. REST uses nouns ( i.e. user, country, state, etc.) to define what kind of resource the service should provide. The representation of the resources is not tied to the service itself, which means that the representation of the resource has a free format and can be represented by multiple kinds of data formats (i.e. JSON, XML, CVS, etc.). Below we have an example of REST representation: The REST architecture style has six constraints, which are design rules applied to the architecture and which establish the distinct characteristic of REST services. So, in order to define a REST architecture service, it must have the following architectural characteristics that make any web service a true RESTful API: Simple Object Access Protocol , as the name says, is a standard protocol intended to exchange structured messages in a decentralized, distributed environment . SOAP is a protocol based on XML to exchange messages across the internet. It has a set of rules which define and describe the messaging format and processing rules for information exchanged between the sender and receiver.  SOAP can use two  different transfer protocols to exchange the messages: HTTP (Hypertext Transfer Protocol) and SMTP ( Simple Mail Transport Protocol ), but the most popular protocol used by SOAP services is HTTP. Below we have a SOAP Request message as well as an example of the SOAP message structure: Since  SOAP standards for message exchange are based on XML and usually use HTTP as a transport protocol, it is possible to communicate between different applications with different operational systems. One specific characteristic of SOAP is the coupling between the service provider (server) and the service consumer (client). There is a rigid contract between the server and client in SOAP and the consumer must know the exact message structure to send to the server. If anything changes on either side, client or server, it will break the communication. Another important characteristic of SOAP is its association with Web Service Description Language (WSDL). The WSDL has the description of SOAP service and how it works. This makes the client building process easier once IDEs and frameworks can be utilized to automate the implementation based on WSDL. SOAP also has built-in error handling. When any message has an error or is missing required information, the error response message contains information about what is wrong and can be used by the client to fix the problem. This is very important to consumers when they are not the owner of the service, otherwise, the client won't be able to understand what is wrong with the request. Comparison SOAP and REST cannot truly be compared because one is a protocol and the other is an architecture. However, both have the objectives of communication between client and server and data exchange across the internet. SOAP is the more popular choice for web services when compared to REST due to the restrictive XML messaging format. REST can be used with more lightweight data formats to exchange data. However, SOAP has some key advantages when compared to REST: On the other hand, there are some advantages of REST when compared to SOAP: Below we are trying to highlight some characteristics of SOAP and REST, given the similarities and differences of the technologies used by each: There are many factors to be considered when we are talking about REST and SOAP with regards to building web services. For any situation, these variables and requirements must be considered before choosing the best approach to adopt. There is no one best solution , but there is a better approach depending on each scenario. It is up to us, IT professionals, to know how to analyze the scenario and collaborate with stakeholders to develop the best solution.", "date": "2017-7-19"},
{"website": "Avenuecode", "title": "Your Guide to Organization-Wide Digital Maturity, Pt. 3", "author": ["Holly Camponez"], "link": "https://blog.avenuecode.com/your-guide-to-organization-wide-digital-maturity-pt.-3", "abstract": "In our last Snippet , we defined five ways to create a successful business strategy that improves core business success while seeking innovation opportunities. Today, we'll explore how design thinking-oriented design services help your organization identify high-ROI initiatives. Design thinking is a philosophy of deeply understanding and empathizing with the end user. It  helps decision makers ask questions, challenge assumptions, and reframe problems to identify user-centric solutions through a process that is both iterative and expansive. In this case, the purpose of design thinking would be to ensure that any digitization of the core business and/or any innovation initiatives are meeting real consumer needs. At Avenue Code, design thinking is the philosophy that undergirds all of our design offerings and processes. While we offer specific, hands-on services like UX design, UI design, branding, etc., we also offer bigger-picture consulting services, including service design, design sprints, and design ops. DESIGN SERVICES Why Service Design Service design is a way of measuring user satisfaction and engagement with an entire service or product rather than analyzing user behavior within siloed touchpoints. For example, instead of merely collecting information on user satisfaction with a call center, a mobile app, or a web interface, service design looks at the entire consumer journey holistically to gain a stronger understanding of the consumer’s overall impression of the company. According to Lynn Shostack, “...managing the pieces rather than the whole... makes a company more vulnerable and creates a service that reacts slowly to market needs and opportunities.” The Benefits of Service Design One study from McKinsey and Company found that companies without a service design practice experience more customer defection, lost sales, high call volumes, and lower team morale. Companies with a service design practice, on the other hand, experience higher customer satisfaction, more sales, better retention, lower service costs, and higher team morale. In fact, one company reported the following after implementing a service design practice: “A 50 percent increase in customer satisfaction from the starting position, and a 15 percent reduction in the company’s customer-service cost. Employee satisfaction increased by 20 percent and churn related to this journey was cut by more than half.” The positive results of service design implementations were not limited to one industry, either. In diverse industries - from pay TV to retail banking to auto insurance and beyond - companies saw that perfecting user journeys resulted in faster revenue growth. The takeaway is that companies that improve customer satisfaction experience a corollary growth in revenue when there is a high performance across the entire user journey rather than in isolated customer touchpoints. Example Service Design Engagement A typical service design engagement with Avenue Code might include deliverables such as analysis of and complex mapping between front-end operations like the website, customer actions like visits to the website, onstage contact actions like customer chats with support, backstage contact actions like responses to chat questions, and support processes like visitor analytics. These findings would then be mapped and correlated to findings from other areas like mobile apps, individual services/products, brick and mortar customer experiences, call center interactions, and so on to create a comprehensive understanding of the entire consumer journey. A typical timeline might be eight weeks with two service designers, as depicted below: Because the consumer is at the center of all digitization initiatives, it’s smart to thoroughly map the customer journey prior to beginning any project. Customer mapping can be accomplished through service design. Looking at the entire consumer journey often exposes gaps in service and opportunities to offer a more cohesive and intuitive user experience, so it is often paired closely with the validation of specific project ideas through a design sprint. WHY DESIGN SPRINTS Design sprints help companies validate ideas before investing in them. By identifying early on which projects add value and which do not, design sprints can save companies a significant amount of time and money. It’s important to note that design sprints are used to validate specific problems such as increasing mobile app engagement through the introduction of a new feature rather than debating whether or not to build a mobile app at all. On the first day of the sprint, the team spends time understanding the problem, the context, the personas, and the current state of the issue (what is and is not working today). On the second day, the team ideates potential solutions, investing a significant amount of time exploring possibilities without critically evaluating the ideas. On the third day, the team evaluates and analyzes all proposed solutions, deciding on the idea that will be tested. On the fourth day, the team builds a prototype of the solution. On the fifth day, the team tests the solution with actual consumers and records reactions, gathering feedback without posing leading questions. A design sprint is deemed successful if the idea is proven to work, and it is equally successful if the idea does not work because the company avoids pouring resources into unprofitable projects. WHY DESIGN OPS Avenue Code also offers consulting services for companies that are just beginning to build their internal design departments and want an experienced partner to ensure that they are on the right track. For three months, Avenue Code works closely with the company’s leadership to help kickstart its design culture, processes, and strategies. This encompasses everything from hiring the right people to structuring the team, cultivating internal talent by hosting workshops, formalizing performance metrics, creating a design system, and defining the timeline and steps for when and how design is involved in each project. Each of these design services plays a critical role in a company’s overall digital evolution efforts by creating a culture of design thinking that makes user-centric strategic decisions, providing consumers with a more cohesive brand experience, accelerating time to market for new products/services, strengthening customer engagement and retention, and more. In our next Snippet, we'll discuss how to enable digital evolution through best practices related to people and organization, data and analytics, and technology. Download our free whitepaper for your complete guide to organization-wide digital maturity. What is Design Thinking and Why Is It So Popular? Rikke Friis Dam and Teo Yu Siang. Interaction Design Foundation. Designing Services that Deliver, G. Lynn Shostack. Harvard Business Review. From touchpoints to journeys: Seeing the world as customers do , Nicolas Maechler, Kevin Neher, and Robert Park. McKinsey & Company.", "date": "2020-12-16"},
{"website": "Avenuecode", "title": "AC Spotlight - Aaron Zagha", "author": ["Holly Camponez"], "link": "https://blog.avenuecode.com/ac-spotlight-aaron-zagha", "abstract": "(The following article is property of Avenue Code, LLC, and was originally published with permission at TotalRetail on August 21, 2018 .) Aaron Zagha is the head of e-commerce, international at Teleflora , an online floral retailer. In this interview, Zagha shares his unique perspective on the challenges and opportunities of globally expanding an e-commerce company that has been in business since 1934. Avenue Code: With operations in Europe, Asia, and the Americas, Teleflora is a major player in the global floral space. How do you determine which global markets are ready for expansion? Aaron Zagha: Historically, our expansion decisions were driven more by acquisition opportunities than de novo market entry. Teleflora is international and has partners all over the world, so we’re often approached when business owners in our industry want to sell. If the market presents a good cultural fit and the price is right, we expand. However, we also did an organic entry into the U.K., and right now we’re entering another new country as well. We entered the U.K. because it already had a strong connection with Australia, with families sending flowers back and forth between the two countries. AC: What are the biggest challenges and opportunities you encounter while establishing a presence in a new global marketplace? AZ: One of our biggest challenges is also our biggest opportunity: our competition. Often there’s a single market leader, and while it’s tough to compete against an established giant, sometimes customers are dissatisfied, which creates a massive opportunity. Our other challenge with global expansion is adapting to a new culture: you can miss out on 20 percent to 30 percent of the market if you don’t learn their holidays, language nuances, marketing opportunities, preferred payment types, etc. AC: How do you build a core team to collaborate with you locally? Beyond the research, how do you find that team? AZ: Sometimes we have to build a team from scratch, but more often we’re able to utilize personal connections. If an employee has a friend or family member in a country where we’re expanding, we solicit local feedback on top products, traditions, and even what each flower symbolizes in that culture. AC: In 2013, Teleflora acquired Floristworks, an e-commerce business in Australia. What challenges did you encounter when merging technologies for the two companies? AZ: I’d have to say our biggest challenge is merging companies onto the same tech platform. When we acquired Floristworks, we were able to merge our back-end systems relatively quickly, but we left the front-end system untouched. When the strategy changed post-acquisition, we decided to update Floristworks’ systems and migrate the front end to ours, which was quite a process. AC: How do you see the perception of IT in different countries, and how the U.S. in particular is either catching up or ahead of the game when it comes to platforms? AZ: Unlike U.S. e-commerce platforms, almost all international platforms are homegrown. This is partly because the cost of bigger platforms is prohibitive unless you’re a well-established player in a large market. When you acquire internationally, you have to decide if you’ll keep the homegrown platform, which is often very customized, or adopt a higher-performance platform like Oracle or Magento and lose the customization. We plan to migrate Petals onto Oracle in the coming year. AC: What does Teleflora look for in its strategic partnerships? AZ: In all our partnerships, what’s most important to us is brand integrity. We are a luxury, upscale provider, and all of our flowers are hand delivered rather than shipped in a box. We always look for local florists who can meet our quality standards. On the e-commerce vendor side, we also have to make sure service providers are well-funded so that they can see our projects through on a cost-effective basis. To be successful at marketing, you have to be profit driven. Teleflora's signature hand delivery. Photo courtesy of Teleflora. AC: Teleflora has been in business since 1934, which must have necessitated several strategic iterations to keep pace with the ever-evolving retail landscape. Tell us about this evolution from a technological standpoint. AZ: At its heart, Teleflora has always been a tech company. It was among the first big floral companies to adapt to e-commerce, and we always stay on the cutting edge of tech trends. One of our advantages is that we’re privately owned, so all decisions are made very quickly, even when it comes to larger sums of money. AC: How do you handle e-commerce as a company with a perishable product? AZ: Florists occasionally under-order so that they can keep their products fresh, but this typically means that some run out of inventory and have to stop selling the day before a holiday. We’re equipped with a huge amount of data and we try to help them forecast demand to avoid this issue, but it's challenging. AC: How do you stay abreast of technological advances in retail, and how do you decide which to adopt? AZ: I constantly read news, keep pace with the competition, chat with friends who run e-commerce companies, and attend e-commerce conferences. AC: In addition to serving as Teleflora’s international head of e-commerce, you’re also a founder and current member of Westside Analytics’ investment committee . How has your experience with WA’s technologically forward data analysis methodologies informed your marketing and investment strategies for Teleflora? AZ: The two roles are very symbiotic. WA collects and analyzes online data, and often picks up on trends long before the media does. Teleflora benefits from insights I’m exposed to through my work with WA, and I’m able to help guide WA on what data to collect on e-commerce companies based on my experience at Teleflora. AC: What's your biggest achievement at Teleflora? AZ: The biggest achievement for me has been building a wonderful team in four cities across two countries. Since I joined Teleflora, our flower sales have been increasing three times to four times faster than the overall e-commerce growth rate. We also manage over a thousand local florist sites. Allowing these small, locally owned businesses to grow faster than the market is a lot harder when they’re up against huge competitors with money to invest in advertising, so giving these companies an opportunity to grow is extremely rewarding. AC: Wonderful, thank you Aaron! It’s been great hearing your thoughts. We’ll stay tuned for the official announcement of your Latin America debut.", "date": "2019-2-20"},
{"website": "Avenuecode", "title": "A JAVA Developer's Perspective on Node.js", "author": ["Milena Santos"], "link": "https://blog.avenuecode.com/java-developer-exploring-nodejs", "abstract": "According to the latest StackOverflow survey , Javascript is the top-ranked technology in terms of overall usage. Unsurprisingly, it's also top of the list for occupations, and with the advent of Node.js, has become increasingly in-demand. In this post, we'll be exploring the similarities of applications built in Node and JAVA. Rather than going into the specifics of each one, we'll focus on wrapping up the main components to get an app up and running. Ready to get our hands dirty? Let's go! Getting Started Before starting with Node.js, it's recommended that you have some basic familiarity with Javascript. However, if you're beginning to get accustomed to programming, don't give up! There are tons of tutorials like this one to help familiarize you with the language, as well as infinite ways to practice. All you need is a text editor and a browser to get started! If you'd like to try the examples presented here by yourself, you can: Hands On Dependency Management NPM More than a build tool, the Node Package Manager (NPM) - which comes as a bonus when we install Node.js - centralizes open-source libraries and is also useful for multiple purposes such as initializing an app from scratch with \"npm init\", installing dependencies, packaging the app, running it, testing, and more. You can learn more about NPM in more detail here . The default file for assembling the project specs is the package.json , which can be as simple as the one below: The \"main\" defines the JS file that initializes the application, similarly to a Main class in a JAVA project (any class containing the void main(String[]) method), and there's no standard name for it. Once we run \"npm install\" after defining our application dependencies, a folder named \"node_modules\" is created on the project root folder, which is equivalent to the list of jars that are necessary for a JAVA app to compile. If we add more dependencies after the first install, it won't create any  problems. We can simply specify what we want to install in order to be more precise (e.g. \"npm install node-restful\"), or simply trigger \"npm install\" again. Maven As you might already know, Maven is the most popular build tool for managing JAVA project dependencies. There are a few others available as well, that are less common, such as the excellent Gradle . Maven's central point is the pom.xml , which is commonly, if not always, saved on the project root folder as the package.json is for Node. Below is the one used on our sample project: Database Configuration No hurdles here, with these lines in our main server.js file, we have MongoDB all set: On the JAVA app the configuration is also pretty simple since we are using Spring Data MongoDB, so all we have to do is to define the following on the application.properties : Backend Development See how the CRUD managed via RESTful services is structured on the Node application to the left? Not too differently from what we have on the JAVA correspondent to the right. See the similarities? Model Controller REST endpoints Putting it All Together On our Node app we are linking the pieces starting from the server.js , defining the models, database, routes (which plug the controllers), and finally the application port, whereas on our JAVA app this is in place with annotations (remember that we're getting the advantage of all that Spring Boot can offer along with the most recent JDK). Before long, though, it's likely that we'll have similar support in Node.js. Check it out - attempts are already underway: take a look here . Final server.js file: Running the Application Now it's time to see if our efforts had any results. To test the exposed services, I've used Postman . See below. Creating a product Updating a category Retrieving all products As we can see above, some of the services exposed by the Node application were successfully executed. To call the API's exposed by the JAVA app and test, all we need to do is change the port to 3001 since the same UR I names were chosen and it's already up. Frontend Development Angular.js would be my natural choice for developing the UI for consuming both backend applications' REST APIs. Why? Simply because it is Javascript based and is a big success! Working with JAVA, many of us have always had contact at one time or another with pure JSP/Servlet, Struts, JSF, or Spring MVC applications. These are still used for some newly created projects, and we'll always face them when working with legacy systems. But the world is changing. With the emphasis on Mobile First development, and the growing concern of building responsive apps that run well, multi-platform, JS-based client side apps are the way to go. Some of the many frameworks for building Javascript fullstack applications with Node and Angular include: Conclusion Node.js really worth trying, even taking into consideration that's not recommended yet for any kind of app. Note: I say yet because it's evolving at such a fast pace that this argument will be defeated any day now. It's efficient for both fast development, and enabling lightweight apps to run with minimal effort. Recommended references besides the official website: The code mentioned in this article is available for download here , have fun! What are your thoughts regarding Node? Are you already using it on your projects? Please share with us! =)", "date": "2017-5-17"},
{"website": "Avenuecode", "title": "Why Ruby Still Rocks", "author": ["Magnum Fonseca"], "link": "https://blog.avenuecode.com/why-ruby-still-rocks", "abstract": "Many programming languages become trendy, enjoy a peak, and sooner or later fall out of favor. Today, people are talking about Ruby's decrease in popularity and even its alleged death. Here's why I think Ruby remains a solid programming language and Ruby on Rails a reliable web application framework. Ruby on Rails is a web app framework written in Ruby, a dynamic, general-purpose programming language. Many people get confused about these terms, thinking \"Rails\" when hearing \"Ruby,\" but rarely the other way around. This is perhaps because Ruby made a name for itself after releasing Ruby on Rails. Also, most Ruby programmers use Ruby on Rails. There are a lot of other Ruby frameworks, and they are not necessarily lower in quality than Rails. In fact, most of them were built to outperform Rails in some way. Back in 2005, Ruby on Rails changed the web developer world with its new approach to building web applications through its convention-over-configuration software design paradigm that facilitates developer work on many levels. For example, if there is a sales class in the model, the corresponding table in the database is called “sales” by default. It is only when deviating from this convention, such as by naming the table “product sales,” that you need to write code for these names. Ruby on Rails is based on the MVC (Model View Controller) design pattern. This enables efficient development with parallel developers working on models and instances. There is no easy answer to that question. It’s as if Ruby on Rails has been around long enough that it's lost its fascination, so new programming languages are coming to take their place on the stage instead. Also, the many myths of RoR’s demise are the result of many misconceptions that grew around the framework. In fact, there are several examples of how RoR still works well when paired with well-thought-out architecture like Basecamp, Airbnb, and GitHub. Ruby on Rails is thought of as a good programming language to do an MVP or a beginning-to-programming learning path, and it does a lot for the developer. Inexperienced engineers, however, tend to make poor architectural decisions and create a lot of bad code, which causes a significant drop in performance. Often, people attribute this poor performance to RoR itself instead of to the poor architectural decisions. Another critique of RoR is its scalability issues. This Ruby on Rails issue became widely known in 2009 when Twitter decided to move  its message queue back-end from Ruby to Scala. Still, let’s keep in mind that we’re talking Twitter-size traffic here. It’s not that Rails doesn’t scale, but rather, requests for “live” data in Ruby (or any interpreted language) do not scale, as they are comparatively far more expensive, both in terms of CPU and memory utilization, than their compiled language counterparts. Ruby performance issues are actively being worked on, and Ruby 2.6.1, released in December 2018, made a lot of improvements. Also, the developers behind Ruby 3 aim to speed up the language by three times compared to Ruby 2. So, before condemning Rails, try to identify which element is responsible for its challenges. In 2018, cloud computing giant AWS added Ruby to its serverless computing solution , AWS Lambda, right next to Java, C#, Python, and Node.js, proving Ruby's stability as a programming language. The Ruby on Rails framework has over 3,500 contributors on GitHub. Frequent updates and new gems created by developers ensure that the applications created using this framework can be developed more easily and delivered faster. Ruby on Rails is among GitHub's top 10 active repositories and has ranked up in the TIOBE software quality index. Ruby's popularity may have slowed in momentum, but it is far from dying, especially since a vast amount of existing projects and major players like GitHub, SoundCloud, Airbnb and Ask.fm are using the language. With updates at least twice a year, the Ruby language is constantly evolving. And Ruby is not only Rails; Ruby's other lightweight solutions like Sinatra, frameworks offering different approaches like Hanami, and even frameworks for event sourcing are all gaining in popularity and are improving on what Ruby offers. This year Ruby on Rails 6.0 was released with parallel testing and bulk insert and upsert, and 6.0 has finally added support for multiple databases. Perhaps another notable feature for many applications that play with WYSIWYG editors is the addition of support for Trix editor natively into Rails 6 applications. This will certainly be a good upgrade/addition for many projects. In December 2020, we'll have Ruby 3x3, promising to be 3 times faster and to include type checking. Large-scale projects can benefit from using Ruby on Rails as a prototyping tool or an efficient way to deliver proof-of-concept solutions. With coding by convention, building applications is fast and easy. Ruby on Rails is great for startups and internal tools thanks to its intuitive, simple, and readable syntax, resulting in much higher productivity. Faster development means a quicker time to market, which is important for startups with a limited budget. This way, more money can be invested in the development of additional features. It is not uncommon to use Ruby even for large and high-performance applications. But it's important to note that using Ruby in this way requires engineers who know how to craft high-performing applications in Ruby. Also, this use of Ruby might require some horizontal scaling. Ruby focuses on simplicity. It's a programming language that is expressive, not verbose, and is easy to understand. Unlike other programming languages and frameworks, Ruby on Rails does not need complex configurations that add little value to the main application. This saves developers from writing large pieces of complex code. It offers many easy-to-use built-in tools and gems that ensure operational security and safety. For example, newly created Rails applications have by default a one-liner configuration protect_from_forgery with::exception to protect against all other forged requests. Tools like Brakeman let developers check Rails applications for security vulnerabilities, and rack-attack mitigates abusive requests, allowing developers to rely less on short-term, one-off hacks to block a particular attack. Ruby on Rails encourages both test-driven development (TDD) and behavior-driven development (BDD). Both of these development principles advocate writing tests first and then developing code to ensure these tests pass. RSpec is a widely accepted domain-specific language testing tool for Rails applications. With Rspec, tests are written beforehand, which means developers can avoid writing unnecessary code, making it easy to do refactoring. Its simple syntax makes it one of the most widely used testing tools for Ruby applications. Last but not least, we should keep in mind that there are no bad programming languages, there are only poorly chosen languages for particular use cases. So if you feel Ruby seems to be a great fit for you, do not hesitate to use it. It is still among the top most popular technologies!", "date": "2019-12-18"},
{"website": "Avenuecode", "title": "Unraveling Blockchain", "author": ["Rodrigo Sekimoto"], "link": "https://blog.avenuecode.com/unraveling-blockchain", "abstract": "So, you haven't bought any Bitcoin or Ethereum yet? I understand, I wouldn't buy something that I don't know anything about either - but don’t worry. Today, we are going to introduce the famous and trending topic, the tech of the hour…Ladies and gentlemen, drum roll please! Blockchain Some view this technology as the new internet revolution because of how useful and robust it is. This new approach to data transaction has the ability to change the game completely as we know it. But how does it work? Well, at first glance, the big buzz around Blockchain is because of the decentralization of data. Conventional systems usually have a database, one single point where all information is created, read, updated, and deleted. Image courtesy of www . de al . com .br . What if there was no centralized version of certain information, or maybe a way that the whole network could easily verify the integrity of it? What if, instead of having a third party make the  transaction, you could easily do it yourself? That’s what blockchain is all about. Let's use this widely known analogy to try and understand what a blockchain is. Think of a blockchain as a book. A book can have many pages that we can call blocks, and those pages might have content right? Let's call that content records instead, and imagine that a lot of people have a copy of that book. So far, so good? So think of it this way, in order to add a new page to the book, the content of that page must be coherent with the previous page, and validated by people who own a copy of that book. The owners will verify and validate the content of that new page and if everything is good to go, a new page is added to the book, and everybody who owns a copy of it will have that new page as well. Image courtesy of B lockgeeks . So, by this point, you might have heard of the terms mining and blocks, but what do they mean? For starters, we have to understand what a block is, and we’ll try to keep it simple. Basically a blockchain is a distributed ledger - a decentralized database that is updated continuously by people who mine the node… but wait, what is mining? Let's connect the dots here. We have mentioned the blockchain, ledger, the decentralization of data, mining, and nodes.  Those are a lot of terms, but bear with me - it will all start to make sense soon. First, let's define a few terms: -Private Key : A private user hashcode to access its own data -Public Key : A user hashcode publicly visible to the blockchain to make transactions -Wallet : A digital wallet used to stored digital assets such as Bitcoin and Ether -Ledger : The records of transactions made in the blockchain -Node : Users that have the client responsible to verify, validate and relay transactions -Block : A group of transactions sent out to the entire network -Mining : Computers using their raw power to decrypt a block As a distributed ledger, the concept of the blockchain is that each user that has the client perform validation on transactions has a copy of the data in the network. A transaction is made when a giving user sends out a contract to the blockchain. Let's say User A wants to send 1 Bitcoin to User B. Using a digital wallet, he can make a contract via User B's public key. He can then send that contract out to the blockchain and that contract will get grouped together into a block with other contracts sent in the past 10 minutes. Miners in the node will then validate that transaction, and voilà, everything's set . Image courtesy of T ruthcoin . Here are some of the advantages of using blockchain : -There is no third party that controls the blockchain -Each and every transaction in the network is extremely secure and reliable -Transactions are transparent -Decentralization -All records are immutable -Low cost transactions -Constant and real time updates within the blockchain Of course, blockchain is not a bed of roses. There are still a lot of challenges ahead to solidify and spread its usage globally, such as: -A lot of computer power is needed to mine blocks -Large amount of energy consumption -Transaction speed and volume due to algorithm complexity -Cultural acceptance -Scalability still needs to be improved That's all folks! Hopefully this article was able to give you some insight into the enigmas that are Bitcoin and blockchain. Let me know your thoughts, I'd love to hear from you.", "date": "2017-11-28"},
{"website": "Avenuecode", "title": "Working with Multiple Environments in Google Cloud Build", "author": ["Douglas Augusto"], "link": "https://blog.avenuecode.com/working-with-multiple-environments-in-google-cloud-build", "abstract": "Large projects require the use of several environments, such as DEV, QA, UAT, STAGE, and PROD. This Snippets article details an approach to using Google Cloud Build features to create generic pipelines that can adapt to multiple environments. Google Cloud Build is a cloud service that lets you create Continuous Integration, Delivery, and Deployment. The trigger containing the execution rules and pipeline variables can be created via interface or command line. The pipeline containing the steps is described and versioned in your repository through a configuration file in .yaml or .json. To illustrate how to work with multiple environments, we’ll use an App Engine application and create separate runs for the PROD and STAGE environments. Here's how we need to configure each environment: PROD: in the production environment, we must publish in the prod version to guarantee the promotion of the version and to ensure that all traffic is redirected to the environment. STAGE: in the stage environment, we must publish in the stage version. Configure it so that the version does not promote and only 20% of traffic is migrated to this environment. We will create two triggers, one for each environment. For SCM issues, I've associated each environment with a branch. These are, respectively, PROD: master and STAGE: rc. We will use the substitutions feature to configure variable values for each environment. These are our triggers: We will create a generic configuration file called .cloudbuild.yaml that will contain the DEPLOY and SERVICES steps of our pipeline, receiving and overriding the values of the variables described in the triggers that were previously created. Here is our pipeline . When committing a new functionality in our environment in branch rc, our Release Candidate trigger will be triggered and will perform all processes for our STAGE environment, as shown below: Checking the versions and traffic of our environments on Google App Engine, we have: When we accept a merge request from the branch rc in the master, we will activate our production trigger that will carry out the entire process for our PROD environment: Checking the versions and traffic of our environments on Google App Engine, we have: Google Cloud Build is a very versatile tool that allows you to create pipelines that fit your environments. If your workflow is quite distinct by environment, the best approach is to create a configuration file for each. Want to know more how Google Cloud Build can be a very versatile tool that allows you to create pipelines works?", "date": "2019-5-15"},
{"website": "Avenuecode", "title": "Specification by Example", "author": ["Rafael Romão"], "link": "https://blog.avenuecode.com/specification-by-example", "abstract": "The software development industry is still in its early days. Compared to other industries with centures of history and evolution behind them, software development is only beginning to mature. One of the consequences of that relative immaturity is that we still have many recurring problems to solve. In this article, I'm going to talk about one of the most important ones: communication. Communication is key to developing the right software the right way. No matter how good a development team is, if they cannot understand the problem the software needs to solve, they will not be able to deliver what their clients really need. One of the best approaches to mitigate communication problems is a set of techniques known as Specification by Example. The term, introduced by Gojko Adzic , refers to techniques that have in common the specification of software requirements using examples, as the name suggests. The most well known of these techniques is BDD , or Behavior Driven Development. Essentially, these techniques tell us to describe the software using stories, with real case examples whenever possible, and to share those stories with the whole team. The recommendation is to involve the team in the writing, in fact, and use the stories to ascertain that the software really does what is specified. There is even a language created to make it easier ( Gherkin ) and tools like Cucumber and Specflow that make executable tests out of these specifications. Most developers nowadays are familiar with the term BDD , or at least TDD (Test Driven Development) . Many quality assurance analysts use it to write their tests. Business analysts also use the Gherkin language to describe the requirements. But the true power of Specification by Example comes when everyone works together, a technique known as The Three Amigos . I will not go into details about how it works here - my intention is to show the benefits, leave some references, and let you try it out. I can assure you it will be worth it! Specification by Example brings a lot of advantages with it. The first one is bridging the communication gap. The Gherkin language is made to be easy to read and can easily be understood by anyone in the team, even business people, without any knowledge of computer programming. Furthermore, the same specification can be used throughout the whole process by business analysts, software developers, and quality assurance analysts. Even the client can have access to the specification by using a tool like Pickles . Using this common specification, developers can implement what the business analysts agreed is necessary, and quality assurance analysts can verify that the implementation is correct. A unique source of specification helps to avoid situations where a developer uses a different definition than the other members of the team. Software must do what its specification says, and each part of its implementation must be validated to ensure that it does. These validations must be repeated every time anything changes in the software, to ensure the software still does what it is expected to do. Additionally, the software must also be documented so that new team members can understand what it does. Test-driven development is a good way to test and self-document the software, but defining what to test is not always easy. In most cases, the developer will not know what is necessary to test, which in turn means the developer is left unsure as to what ought to be implemented. The existence of a human readable specification, created by developers, business analysts and quality assurance analysts together, solves this problem by making it clear to the developer what must be implemented and what must be tested. Another advantage of these techniques is that the specification is split into scenarios. Each scenario has a unique set of preconditions, a small set of actions, and expected postconditions. This way, it is easy to pinpoint what conditions are causing a test fail. Tests that verify the behavior of the software, feature by feature, scenario by scenario, allow the developer to be confident in her work. She will not only know that she met the requirements for a new scenario but also know if her changes broke anything else. With this confidence, it is easier to do design refactorings , pay technical debts , and apply the Boy Scout rule . This confidence makes the code better every day. Altogether, it will motivate developers to keep tests updated and make it a habit to always have a specification before the begin to code. Using Specification by Example, everyone can learn about the business during the specification and development processes. Disputes about who is right are reduced, since the specification is the same for everyone and was created collaboratively. Some teams even dedicate a pre-iteration time just to get everyone together to write the stories and clarify them for everyone. During these moments, business analysts can bring a business point of view, developers can recognize technical issues, and quality assurance analysts can identify edge cases. Each team member brings their own strengths to the process. A specification created collaboratively makes everyone feel like part of the team, with shared responsibility for the end product. Now it’s not that guy s responsibility anymore, it’s ours. And we are going to make sure it’s correct and addresses the client needs. Specification by Example is more than writing specification using examples. It demands the integration of the team, it results in executable tests, and ensures quality work. Without Specification by Example, these aspects are often overlooked, or never even considered. What do you think about Specification by Example? How much of it have you tried? Leave your opinion in the comments. For those wanting to learn more about the subject, try reading the book Specification By Example , which chronicles the extensive research Gojko Adzic  about this subject.", "date": "2017-4-12"},
{"website": "Avenuecode", "title": "Mule Encryption", "author": ["Anupam Gogoi"], "link": "https://blog.avenuecode.com/mule-jce-encryption", "abstract": "My main concern regarding the security was Mule Message Encryption . Mule provides three extraordinary Encryption Strategies : Time to get our hands dirty. We'll cover everything we can, beginning with the installation of Anypoint Enterprise Security . We'll also go over troubleshooting practices during the installation process. I am using Mule 3.8.2 EE for this experiment. Take a look at this link if you'd like to dive into the installation guide in detail. Here is a screenshot: Please select the Premium bundle and proceed. In my case I could not install the software initially. If the same problem continues in your case please uncheck the “Contact all update sites during install to find required software” checkbox . After successful installation, restart your Anypoint studio. You should see the following components under the security group palettes. Now you are ready to go! Let’s create a simple Mule Maven project using Anypoint Studio. The most important point is to add the following repository in the pom.xml file. <repository> <id>mulesoft-public</id> <name>MuleSoft Public Repository</name> <url>https://repository.mulesoft.org/nexus/content/repositories/public/</url> <layout>default</layout> </repository> Let’s create a simple flow. The scenario is as follows: we send some JSON data to an HTTP endpoint. Then, the JSON data is encrypted using JCE Encryption Strategy , and we log the encrypted data. After that, we simply decrypt the encrypted data and log it. Here is diagram of the flow. “The Java Cryptography Extension (JCE) is an officially released Standard Extension to the Java Platform and part of Java Cryptography Architecture. JCE provides a framework and implementation for encryption , key generation and key agreement, and Message Authentication Code (MAC) algorithms.” First let’s configure a global Encryption element. Be sure to select the Default Encryptor dropdown as JCE_ENCRYPTER . I have named the global element as JCE . Now, having configured the global Encryption element in step 1 , we'll configure the Encryption component (labeled as JCE Encryption in the flow. This is a Encryption palette found under the Security category of Mule palettes). Here is the screen shot: In the Connector Configuration , please select the global encryption element(JCE) that we previously defined in step 1 . In the Operation dropdown, select Encrypt . In the Input Reference field, I am adding the whole payload (JSON data) for encryption. Don't forget that you can use MEL expression here to encrypt some part of the payload too . Since we are using JCE_ENCRYPTER in this example, our next step is to configure it. Select the Define attributes radio button in the JCE encrypter Strategy Configuration for operation category . Key : Must be a 16 digit phrase. Key Password : (whatever you wish) Algorithm : Choose an algorithm from the dropdown list. Encryption Mode : Choose from the dropdown list. That’s it. You are ready to go! There is another way to make it more secure using Keystore, but that's another topic for another day. In step 2 we have successfully encrypted the message. Next, let’s decrypt the encrypted message. Here we are going to configure the component labelled as Decrypt payload . (This is nothing but an Encryption palette found under the Security category of Mule palettes. Here is the configuration: As you can see, this configuration is almost same as for encryption. The only difference is the Operation: Decrypt . Finally, let's run the project and then send a POST request with JSON payload. I am using Postman for this operation. Observing the Anypoint console, you can see the encrypted message as well as decrypted message. Source code can be found here . Final note : You can use mvn eclipse:eclipse to import the maven dependencies. Thanks for reading! Hopefully this was helpful. Any questions? Let me know in the comments!", "date": "2017-1-18"},
{"website": "Avenuecode", "title": "It's Time to Migrate: JUnit 5 is Better than JUnit 4", "author": ["Lucas Moraes"], "link": "https://blog.avenuecode.com/its-time-to-migrate-junit5-is-better-than-junit4", "abstract": "Are you still using JUnit 4? If so, your code will thank you for migrating to JUnit 5. JUnit 5 is the latest stable release of JUnit, and it comes with a lot of great features. Some of them are there to make your tests more readable. Some were added so you can enhance your written tests and add new scenarios. JUnit 5 also allows you to get the most out of Java 8 language features, such as lambda expressions. In general, JUnit 5 is trying to make the framework more robust and flexible than it was in its previous version. The first changes that we will be discussing are annotation changes. JUnit 5 tried to increase readability for all annotations so that they are easier to understand at first glance. The new framework also has some new annotations that we can use to increase our scenarios. Let's take a look at the major annotation changes first: @Before → @BeforeEach @BeforeClass → @BeforeAll @After → @AfterEach @AfterClass → @AfterAll First of all, let's talk about the simplest change. Before was renamed to BeforeEach, BeforeClass was renamed to BeforeAll, After was renamed to AfterEach, and AfterClass was renamed to AfterAll. All of these changes were made to enable cleaner code and better readability. @RunWith → @ExtendWith RunWith was renamed to ExtendWith. But this one wasn't just a simple rename. ExtendWith is an annotation with a lot of features. The best one is that you can now use multiple Extensions at the same time. For example, you can do something like this: @ExtendWith(MockitoExtension.class) @ExtendWith(MyCustomExtension.class) public class FeatureTest { } Alternatively, you can just use a list inside the @ExtendWith value, as shown below: @ExtendWith({MockitoExtension.class, MyCustomExtension.class}) public class FeatureTest { } Note that the execution order will follow the execution declared. @Suite → @SelectPackages or @SelectClasses Another change was made to improve the readability and to give each annotation single responsibility. So if you want to do a test suite, it's easier with the above annotations. Let's see how this can be done: @SelectClasses({PostgreConnector.class, PostgreRepository.class, PostgreDriver.class}) public class MyPostgreTests { } In the test above, we pointed to each class that we wanted to cover in our test suite. We can do this by using packages, as shown below: @SelectPackages(\"com.avenuecode.examples.junit5.database\") @IncludePackages(\"com.avenuecode.examples.junit5.database.pgsql\") public class MyPostgreTests { } In the test above, I'm using a new annotation too: @IncludePackages. @SelectPackages will make sure that all classes and subpackages under that package will be tested. If you also annotate with @IncludePackages , you will make sure that only that subpackage and its classes will be in that suite. You can also use @ExcludePackages to point a package and its subpackages to be excluded from a test suite. In addition, you can use @IncludeClassNamePatterns or @ExcludeClassNamePatterns to make sure that some classes following a regex indicated will be excluded or included in your suite. @Ignore → @Disabled This was a rename that also aims for better readability and an easier understanding of what the method does. Beyond this, if you annotate @Disabled at class level, you will get the number of tests skipped in your execution log. If you used @Ignore with JUnit 4, you would get: 1/1 class skipped , but now you will get: number of tests/number of tests skipped, which makes a slight improvement in understanding what was skipped. As stated before, JUnit 5 added some new annotations that can help us increase our test readability and make them more powerful. We will be focusing on two annotations in this article to showcase an example of what the new version is trying to achieve: @DisplayName and @ParameterizedTest . @DisplayName This annotation is one of those changes that you look at and think: \"Why wasn't this implemented before?\". It's so simple and yet so functional. All this change does is display a text before your test runs. It's that simple. Here's the code: @Test @DisplayName(\"The method withdraw should reduce the amount of money in the bank account by the amount passed\") public void withdrawShouldReduceAmount() { var bankAccount = new BankAccount(2000); bankAccount.withdraw(100); assertEquals(1900, bankAccount.getBalance()); } @ParameterizedTest This annotation is one of the most powerful ones. Why? It allows you to reutilize the same test method with multiple parameter variations. Yes, this was a feature only achieved by adding external libraries; now it's on JUnit source too. Below, I'll show how to write a simple test that can use this feature very well. Let's say we want to check whether or not our customer who has a bank account is eligible for a credit card. Our method would look something like this: public enum AccountType { BLACK(1000), PLATINUM(1500), GOLD(2000); private double minEligibility; AccountType(int minEligibility) { this.minEligibility = minEligibility; } public double getMinEligibility() { return this.minEligibility; } } public boolean isEligibleForCreditCard() { return this.balance >= this.accountType.getMinEligibility(); } So now, we can write our tests like this: @ParameterizedTest @MethodSource(\"accountTypes\") @DisplayName(\"Verify if the account is eligible for a credit card based on its type and balance\") public void verifyCreditCardEligibility(BankAccount bankAccount, boolean expected) { assertEquals(expected, bankAccount.isEligibleForCreditCard()); } public static Stream<Arguments> accountTypes() { return Stream.of( Arguments.of(new BankAccount(AccountType.GOLD, 3000.5), true), Arguments.of(new BankAccount(AccountType.GOLD, 1000.25), false), Arguments.of(new BankAccount(AccountType.BLACK, 1100), true), Arguments.of(new BankAccount(AccountType.BLACK, 980), false), Arguments.of(new BankAccount(AccountType.PLATINUM, 1600), true), Arguments.of(new BankAccount(AccountType.PLATINUM, 1200), false) ); } What we're doing above is providing a data source method. ParameterizedTest will use our MethodSource AccountTypes to supply the test parameters. So, for each of our Arguments of the Stream we're creating in our MethodSource , the test method will be run passing the Arguments as parameters to verifyCreditCardEligibility() . Pretty simple, yet powerful, right? You can provide other sources of data, such as @ValueSorce, @EmptySource, @EnumSource , and a lot of others. You can find a full guide here . The new assertions are located in a new package. Most of them are located in org.junit.jupiter.Assertions . Like the changes shown above, some assertions were changed to achieve better readability. New assertions were added so we can achieve new scenarios or make some old cases more powerful. Let's take a loot at each: Message is now the last parameter of assert[...]() Did you also hate it when you wrote some assertion like assertEquals(\"Some string\", \"Another string\") , and then you wanted it to have a custom message like \" 2 is not the same as 2\" when it failed, but to achieve that you needed to change your assertEquals parameter order? That was all because the overloaded methods were written like this: assertEquals(String expected, String actual) assertEquals(String message, String expected, String actual) It is kind of counterintuitive, don't you think? So, in JUnit 5, this was changed so the method signatures look like this: assertEquals(String expected, String actual) assertEquals(String expected, String actual, String message) This was not only changed for assertEquals(String, String) , but also for all assertions that accept a custom message. New assertions There are a lot of new assertions in JUnit 5, such as assertThrows(), assertDoesNotThrow(), assertAll(), assertTimeout(), assertIterableEquals(), assertLinesMatch(), assertNotEquals(), and  assertTimeoutPreemptively(). In this post, we will be covering the ones that I think you'll use the most frequently: assertThrows() and assertAll(). assertThrows() I'm pretty confident in saying that this is the best change made to Assertions in JUnit 5. This replaces the @Test(expected = SomeException.class) and the @Rule ExpectedException from JUnit 4. Let's see some code so we can understand it better: Imagine that we have the following method: public void withdraw(double amount) { if (amount < 0) { throw new IllegalArgumentException(\"amount to be withdrawn must be greater than 0\"); } this.balance - amount; } So let's write a test that validates that our exception was thrown. In JUnit 4, we would write something like this: @Test(expected = IllegalArgumentException.class) public void withdrawShouldThrowExceptionWhenAmountIsLessThanZero() { var bankAccount = new BankAccount(2000); bankAccount.withdraw(-1); } Or, if we wanted to validate our exception message, we could write something like this: @Rule public ExpectedException expectedException = ExpectedException.none(); @Test public void withdrawShouldThrowExceptionWhenAmountIsLessThanZero() { expectedException.expect(IllegalArgumentException.class); expectedException.expectMessage(\"amount to be withdrawn must be greater than 0\"); var bankAccount = new BankAccount(2000); bankAccount.withdraw(-1); } Now that our assertions are more powerful, we could write the same code below but enjoy greater readability and some new features. Let's take a look: @Test public void withdrawShouldThrowExceptionWhenAmountIsLessThanZero() { IllegalArgumentException exception = assertThrows( IllegalArgumentException.class, () -> { var bankAccount = new BankAccount(2000); bankAccount.withdraw(-1); }, \"The IllegalArgumentException was not thrown, check your test.\" ); assertEquals(\"amount to be withdrawn must be greater than 0\", exception.getMessage(), \"The exception message did not match the expected message\") } Can you see how much easier it is now to understand what the test is doing? It also makes the most out of lambda and Java 8. Beyond this, the assertThrows make your code more flexible. You can read more about it in its documentation . assertAll() This is a new assertion that was added so that you can create a block of assertions. Now you could be asking: why would I want that if I can simply assert multiple times in my test method? Well, if you do that, the test will stop at the first failed assertion and you will not know the result of the next ones unless you make sure that the first assertion passes. With assertAll(), you make sure that every assertion runs, whether or not they pass, and you get a detailed explanation for each of the failures. Let's see an example: @Test public void validatePerson() { var person  = myClass.getTheLastJedi(); assertAll(\"Should return Luke Skywalker\", () -> assertEquals(\"Lucas\", person.getFirstName()), () -> assertEquals(\"Skywalkar\", person.getLastName()), () -> assertEquals(35, person.getAge()), () -> assertEquals(170, person.getHeightInCentimeters()); } Now, even though two tests fail, all of the other ones will be executed and you will get something like the code below: org.opentest4j.MultipleFailuresError: Should return Luke Skywalker (2 failures) expected: <Luke> but was: <Lucas> expected: <Skywalker> but was: <Skywalkar> At first glance, JUnit 5 might look like it didn't change enough to justify migrating from the most used version, JUnit 4. But its changes do improve your test quality significantly, so you should consider migrating if you haven't already done so.", "date": "2021-5-5"},
{"website": "Avenuecode", "title": "AC Spotlight - Frantz Saintellemy", "author": ["Anna Vander Wall"], "link": "https://blog.avenuecode.com/ac-spotlight-frantz-saintellemy", "abstract": "Frantz Saintellemy, President and Chief Operating Officer at LeddarTech, shares how he’s using his business acumen to give back to his community and foster technological innovation. Avenue Code: Tell us about your personal career path. What are some of the biggest moments that have shaped your journey? Frantz Saintellemy: My background is in electrical engineering, so I started my career as a microelectronics specialist. I quickly realized I was better at building businesses when I co-founded a business unit for Analog Devices to break into the infotainment sector emerging in the automotive industry. By the time I left the company, this unit was generating 400M. After meeting my wife and moving to Montréal, I intended to start my own business but was recruited by a well-known, successful tech entrepreneur who offered me a position first as a consultant and then as CTO of Future Electronics. I cofounded 2 businesses for Future that help generate significant revenue for the company.  These businesses use new technologies to enable clean energy generation as well as increased energy efficiency. In 2010, something profound happened: a massive earthquake hit Haiti and killed 300,000 people. I was born in Haiti, and even though I moved to Canada at the age of 7, I still have an affinity for the country. I knew so many wealthy tech entrepreneurs and felt we could do something to help, so I started calling everyone I knew. I also asked my boss if I could raise money internally; he fully supported my efforts and even promised to match whatever I raised. I reached out to my colleagues and electronics industry friends and raised 250k, which was matched with a generous contribution by Future, helping us raise half a million.  And although everyone was extremely generous, I wanted to do more. I realized that I would have that ability by making an entrepreneurial leap. I left Future and partnered with a friend to acquire shares from an old company out of East Germany called ZMDI, which was the center for microelectronic research for the former GDR. We acquired 90% of shares for the division for microelectronics fabrication and development for the automotive space. Acquiring a company is like buying a house. Until you live in it, you don't know the challenges. ZMDI had been earning about 30M euros in revenue, but within a year this decreased to 12M because of various challenges like foreign ownership. We had to rebuild, refinance, and refocus the company. We boosted the revenue from 12M to 80M euros in 5 years, selling it in 2015 to a semiconductor company in the Silicon Valley. AC: You are also Co-Founder and Chairman at Groupe 3737, a philanthropic initiative. Tell us about this group and what inspired you to create it. FS: Groupe 3737 is an incubator/accelerator that helps people from immigrant/poverty backgrounds use technology to create businesses that generate income and jobs in poorer neighborhoods. My wife and I have invested over 15M of our private money in this venture. Today it’s creating a tech hub in Montréal and is one of the most well-known incubators in the province of Quebec. We’re in the process of signing a deal with the Canadian government to create a fund to help young entrepreneurs scale their businesses. I believe that success comes with a responsibility to help others. Without others, we’re soloists, nobodies. We come into this world naked, and we’ll leave naked. It’s important to reinvest in our environment and in people in greater need. If you make a lot of money, you can give back and still have more than enough. AC: In our previous conversations, you alluded to the book L'Homme Qui Plantait des Arbres, The Man Who Planted Trees . How has this book resonated with you and inspired your work, whether personal or philanthropic? FS: I came across this book in elementary school and was fascinated by the story, perhaps because I’m from Haiti, and we’re mostly  farmers in Haiti! This book is set in an environment that’s desolate and treeless, soulless and destroyed. It’s about one man’s desire to be more than just an observer, a man who says, “I can’t change the world, but I can change what I do to improve the world around me.” Planting trees is like giving back. I strive for this in everything I do in life, even if it’s something small like giving a smile or asking how someone is doing. It’s the little things, not the big things, that accumulate and create something worthwhile. I invest the most valuable currency I have, my time and my money. Investing in my community and in my environment is the most rewarding thing I can do. AC: Who are the entrepreneurs you mentor at Groupe 3737, and what are key practices you encourage them to follow to be successful? FS: Our mentees’ industries are diverse, encompassing AI, IoT, SaaS, marketing, vlogging, food tech, and more, and they range from businesses generating millions in revenue to startups in the acceleration phase. All the services we offer are free, and our only requirement is that our entrepreneurs contribute back into the ecosystem so it can continue to sustain its momentum. One of the biggest barriers, especially for those from immigrant backgrounds, is that they’re good at trading but typically trade only within their local communities. This makes their business difficult to scale, so we help them use tech as a multiplier. For example, we worked with a young lady who makes an artisanal Caribbean liquor. We advised her to create a standard recipe and publish it online. She started creating this liquor in her basement as a side project, and now she’s generated a couple millions in revenue from selling recipes and her liquor online. We’re different from other incubators and accelerators in that we don’t cater to the best and brightest who are well-connected and would succeed no matter what. We cater to a culturally diverse group of people who would struggle to succeed without our help and support. Money should never be a deterrent for someone who has a good idea. AC: Your success in mentoring businesses at Groupe 3737 is obviously a product of your own business success. LeddarTech has won CES Innovation Awards for multiple years running. To what do you attribute this success? FS: When we look at a problem, we try to identify and address its root cause. Our business is built around a segment of the autonomous driving industry that people have been trying to address for decades. The root cause of the problem is that LiDAR solutions are large, mechanical, very expensive, not scalable, and different from one LiDAR maker to the next. You can’t have an industry that has hundreds of approaches to the same problem. We tried to create a standard technology that could be used by most LiDAR makers, allowing them to differentiate their products while utilizing the same core tech. This addresses the cost problem, the vendor problem, and the scalability problem. We take this approach in each of our products, trying to solve a problem in a new way and create something that can be scalable. We also hire very bright, skilled, and entrepreneurial individuals from diverse backgrounds who have different ways of looking at problems. We try not to create a framework that will force them to think like everyone else. Instead, we allow them to make mistakes and explore. We expose what the problem is and why it’s important to solve. Most people like to see the big picture, and if you create an environment where employees only see a small part of the whole picture, most smart people will give you what you want, not what you really need. AC: What has been a highlight for you in the last few years? Was there a moment, either for you personally or for LeddarTech, that you knew you were on the right track? FS: We are focused on our key customers and strategic partners. We’re a small company, but we’ve turned partnerships into a strength by convincing large industry players who could out-invest us to join us in our quest to solve customer problems. These players have the resources to focus on areas that aren’t our strengths, which allows us to focus on what we’re good at. If we’re successful together, we can forever change the industry. AC: What do you look for in strategic partnerships? FS: We look for velocity, which is to say partners who are moving in the same direction we are so that we can solve customer problems together. For instance, at CES this year, there were five large companies presenting products around our core LeddarEngine platform. We started discussions on joint collaborations, and now we have specific goals we’ve accomplished together; these partner companies are investing time and money in developing solutions that are aligned with our roadmap and our customers’ needs. AC: What are your thoughts on partnering with staff augmentation providers that can help speed deliveries to market? FS: Partnerships are almost never written into business plans from the outset, but they should be. What we try to focus on from the beginning is what we’re really good at. We put all of our efforts here and partner with others to supply the rest, even if it means sharing revenue, because the end product will be better. It takes time for people to realize that this is a better model, even though we do it intuitively all the time; for example, Facebook uses us as content creators and distributors, which is a money-making form of collaboration. Our method of thinking individualistically in business can be traced back to our education, where our success is individual rather than group-oriented. We need to shift from our ingrained “survival of the fittest” thought process and understand that individual success and instant gratification mean nothing. It’s collaboration that ultimately makes us stronger. I was lucky to learn this at a young age. AC: It’s refreshing and inspiring to speak with someone who has built their life around this philosophy. We can’t wait to see the great things that will continue to be in store for you, for Groupe 3737, and for LeddarTech!", "date": "2020-3-18"},
{"website": "Avenuecode", "title": "AC Spotlight - Chandra Khatri", "author": ["Anna Vander Wall"], "link": "https://blog.avenuecode.com/ac-spotlight-chandra-khatri", "abstract": "Avenue Code: Tell us about your personal career path. When did you first know you wanted to pursue a career in AI, and how did you get to where you are today? Chandra Khatri: I was introduced to Machine Learning during my undergraduate research work. I observed the potential impact ML can have in making the world a better place. Since then I have never looked back. While pursuing research in AI during my graduate studies at Georgia Tech, I observed that Conversational AI would shape the future of AI for the next 5-10 years. Unlike other emerging technologies at that time like self-driving cars and AGI, which have been advancing but were not yet marketable products, conversational AI has seen significant improvements in a short period of time and has made an immediate impact. So, I chose to pursue this path and am grateful for the opportunities I’ve had to work on products like the Alexa Prize , which is a competition led by Amazon to build an open-domain conversational AI system, one of the hardest challenges in AI. I am now transitioning to multimodal AI since most advancements have been centered around domain level such as Computer Vision, Natural language, and Speech Recognition. The next wave of research and usable product advancements will be happening around multi-domains, i.e. building technologies that can address any kind of input, be it voice, text, or vision, something which is closer to how humans perceive the environment. AC: How is Uber utilizing conversational AI and multimodal research to enhance its offerings? CK: While other large companies have built their own conversational AI systems, Uber has been leveraging some of the existing state of the art tools, while also building and advancing research for the Uber ecosystem. For example, one of our products allows drivers to accept/reject ride requests through voice, allowing drivers to respond to riders’ questions through voice without touching the phone. These address both the safety concerns and legal constraints around smart phone use while driving. AC: What are the biggest challenges and opportunities for conversational AI at Uber? CK: One of our biggest challenges has been that we used an external speech recognition system. Because this system is not designed specifically for Uber, the domain and context are not fitted, and the error rate can be high. This is exacerbated by background noise from traffic, passengers, etc. that make speech recognition more challenging. To address this, we built a model on top of an external solution; the model uses context to improve performance and significantly reduce errors. We published this work at ICASSP 2020. AC: Tell us about multimodal research and its capabilities within the wider marketplace. CK: Humans communicate and react to the environment through multiple modalities (vision, touch, hearing, etc.). Our human neurons are logical units but quickly change the way they react and communicate based on sensory input. The idea for multimodal research is to build AI systems that can consume sensory input in multiple ways, mimicking human capabilities. One of the biggest challenges for AI is building in common sense understanding. As a part of Uber AI, we proposed an AI system that learns common sense on its own, over time, utilizing multimodal capabilities to create rules for basic assumptions humans make to navigate their environment. From a product point of view, we have some problems at Uber that can be solved through multimodal research.. AC: Is it still difficult to collect enough data to create such systems? CK: There’s a branch of ML that focuses on active learning, which helps us to minimize and optimize the data collection so that each piece represents unique information. This is a better long-term solution than using massive data collections. Recently proposed self-supervised learning and generative techniques seem promising towards creating training samples and thereby minimizing data annotation. AC: Can AI help companies adapt and thrive in a post-COVID economy? CK: COVID will help enhance the adoption of conversational AI and robotics because we have an accelerated need for digitization. COVID hit everyone, and even enterprise companies with large customer service departments are finding that they don’t have enough workflow to address requests, especially for medical lines, so they’re turning to conversational AI to automate systems and expedite services. Most users are already accustomed to using conversational AI; now it’s up to industries to adopt it. Similar trends can be observed with robotics applications. AC: Can small companies and startups compete with big players in pushing conversational AI? CK: Uber, Amazon, Google, and Facebook are building tools with unique infrastructures that support their own services. They are advancing the technology, but generally speaking, most of their solutions are centered around their own problems. Because of this, about 80-90% of the market is still untouched and is available for other sectors like startups, which are beginning to use conversational AI to support healthcare, e-commerce, etc. AC: What are you personally most passionate about exploring as a Lead Research Scientist? CK: In the long-term, I want to impact the lives of people in a way that is not limited to only one domain. Companies that generate a lot of revenue have an opportunity to help others by building intelligence that has a positive impact on humans, flora and fauna, and the planet as a whole. For example, AI can help us solve a multitude of problems, from predicting and controlling forest fires to predicting and controlling pandemics. I want to be part of this. AC: What are the biggest ethical considerations and concerns for developing conversational AI and multimodal research? CK: In terms of ethical use of data, I believe we’ll see a change for the better within the next few years. What happened is that we build the technology very quickly without considering the ethical ramifications of its applications, similar to what happened in the industrial revolution. Now that the technology is mature enough, we’re addressing ethical issues. I have personally been working on this within Uber. AC: What do you do to stay abreast of innovations in tools and technologies? CK: I have less time to read now, but I still follow prominent researchers like Yoshua Bengio, as well as professors from universities like Berkeley Lab, Stanford, Oxford, Georgia Tech, etc. who know where AI is headed. I review papers and organize top conferences, thereby trying to stay updated with the advancements happening in the field. I also read whitepapers and abstracts, communicate with and hire several top researchers, and check YouTube, Reddit, Medium, and other channels that distill information quickly. AC: Thank you for your time and insights, Chandra. We look forward to watching your work as you continue to drive multimodal research.", "date": "2020-9-4"},
{"website": "Avenuecode", "title": "AC Spotlight - Lila Snyder", "author": ["Anna Vander Wall"], "link": "https://blog.avenuecode.com/ac-spotlight-lila-snyder", "abstract": "Lila Snyder, Executive Vice President and President of Commerce Services at Pitney Bowes, shares global e-commerce trends and challenges related to fulfillment, delivery, and returns, as well as opportunities for adapting quickly and sustainably. Avenue Code: Tell us about your role and day-to-day responsibilities as Executive Vice President and President of Commerce Services at Pitney Bowes. Lila Snyder: In my role as President of Commerce Services at Pitney Bowes, I lead two distinct but interconnected businesses that power commerce in different ways. Our Presort Services business is the largest workshare partner of the United States Postal Service, processing more than 17 billion mail pieces annually on behalf of major US mailers, including leading banks, insurers, and health care companies. I also lead our Global E-commerce business, which provides retailers and marketplaces with a host of brand and consumer-focused fulfillment, delivery, and returns services. Global E-commerce is the fastest-growing business segment within Pitney Bowes. Both Presort and Global E-commerce depend on a shared technology and transportation infrastructure. Together, they account for more than half of Pitney Bowes’ total revenue. AC: Pitney Bowes made its name in letter mail. Why and how did it decide to build a global e-commerce business? Lila Snyder: In so many ways, shipping and e-commerce are natural adjacencies to mailing. In 2012, Pitney Bowes’ revenue was in decline and the company, under the direction of a new CEO, Marc Lautenbach, decided that the best way to leverage our strengths and change our trajectory was to stabilize our core mailing business and begin to establish ourselves in shipping and e-commerce. We entered e-commerce with a single client, eBay, and helped them create their global shipping program so that eBay sellers in the US could reach millions of buyers worldwide. Over the years, we’ve added over 600 retailers, marketplaces, and sellers, all while keeping a 1:1 relationship with each client and creating a strong client success team. What makes the Pitney Bowes client experience unique is that we approach e-commerce thinking about consumers first and how we can help retailers strengthen the relationship between consumers and their brands, which means we’re doing everything possible to grow our clients’ businesses. To that end, we’ve had a few important acquisitions, including Newgistics, that allow us to more effectively assist retailers and marketplaces with fulfillment, delivery, and returns. AC: Your educational background includes a Ph.D. in Mechanical Engineering from MIT. How has that training filtered into your role at Pitney Bowes, particularly in terms of understanding logistics and the impact of technology? LS: I think of engineering primarily as learning to be a good problem solver. My professors told me that, no matter what they taught me, technology would evolve into something new by the time I graduated. So the most important skill is not learning a particular technology, it’s curiosity. If you can ask the right questions, applying a logical construct grounded in analytics, you’ll be a quick learner when it comes to new tech in new contexts. AC: Pitney Bowes’ 2019 Global Online Shopping Study reported increasing consumer frustration related to difficulties with delivery and returns. What’s your perspective on why consumers are frustrated and what shipping companies can do to counteract this? Are there new logistical issues, or are consumer expectations changing? LS: We’ve been tracking this for the last several years and are surprised by how fast the frustration has increased. It boils down to two primary factors: First, more consumers are shopping online more frequently, which means there are more opportunities for frustration. Second, consumer expectations are rising. Every year, our expectations are changing based on how the best companies are raising the bar, and retailers that don’t adapt to these new standards within 12 months will disappoint consumers worldwide. Consumer expectations are simple: we want fast and free delivery. The back-end process, however, is enormously complex and must account for a multitude of factors, from employment rates and the number of staff in a given warehouse to shipping logistics and weather challenges. Retailers need help structuring this process while strengthening their brand, and this is where Pitney Bowes comes in. We’re different from our competitors in that we’ve built our business entirely on retail e-commerce, which gives us greater sensitivity and expertise in building tech processes that simplify the entire process. AC: With so many complex factors in fulfillment, delivery, and returns, is there any one place that’s more susceptible to breakdown? How important is data in solving these issues? LS: There are a hundred potential breaking points, and they are all important. For instance, someone might package the wrong item or put the wrong label on a package. Any number of things can happen, so the emphasis becomes managing exceptions and reducing errors. Data is incredibly important in this process because it exposes what’s happening where. AC: What are some of the biggest challenges your team faces in creating and implementing global strategy decisions? LS: The biggest challenge is the rapid pace of change, and this isn’t unique to Pitney Bowes. Global markets and technology are changing so quickly that you almost can’t get through the cycle of strategy and implementation before the marketplace changes. For example, cross-border trade laws and tariffs have had a huge impact on our business, because different markets around the world think differently about importing, and the pace of change is rapid. The other aspect of adapting quickly is being able to communicate strategy decisions quickly and clearly so that every employee understands the vision. Every small action needs to be moving toward the overall vision. AC: What trends in tools and technology do you see impacting retail in 2020? LS: It’s a big question, so I’ll focus on what’s within our wheelhouse. There will be more online shopping, and more of it will be happening via mobile devices. This creates many branding opportunities for retailers, such as branded tracking tools. No matter how much investment retailers put in the up-front consumer experience, they’ll lose consumers if the post-purchase experience isn’t equally excellent. Retailers must keep the brand experience strong throughout. We also believe that automation technology in warehouses is incredibly important, particularly given where we are in the unemployment cycle. Peak seasons are extremely hard to deal with if retailers aren’t prepared with automation, and this factor will only continue to increase in importance. AC: This is an area I haven’t heard discussed very much, but there’s absolutely a strong post-purchase branding opportunity, particularly when we consider that consumers purchase multiple items from multiple retailers at the same time. It’s easy to become confused about which packages are from which retailer. How can brands stand out after purchase? LS: The biggest opportunities are creating unique labeling and packaging and providing all tracking data when the consumer asks for it. Just watch any teenager after they place an online order - they refresh the tracking information about a hundred times! On average, we find that consumers track a package eight times after hitting the buy button. The complexity of the physical aspect of e-commerce is hard to comprehend from the consumer perspective, so providing tracking information helps consumers see where an item is and how it’s moving. It helps build trust and provide peace of mind. AC: One trend we’ve seen gain incredible momentum in the past year is subscription box services. What are your thoughts on where this trend is taking e-commerce and how it will change the retail landscape? LS: Many of our clients offer subscription box services. These services fall into two primary groups. The first is convenience - offering items everyone needs on a replenishment basis, such as personal care items, health and beauty items, pet food, etc. Convenience was the major driving force behind the first subscription box services. The other side of subscription box services - browsing - is what I find more interesting. In a world of online search, we rarely take the time to visit malls to explore new brands. Instead, we take a 40-question personality quiz, and sample items show up at our front door. If we don’t like them, we send them back. For retailers, this changes the landscape - either they’re becoming subscription box services or they’re including their products in others’ subscription boxes. AC: Do you see browsing-oriented subscription boxes as taking the place of personal recommendations from family and friends? LS: It’s more supplementary. Data and data science are great, but friends and siblings always know us better than a questionnaire. Word-of-mouth recommendations work in tandem with subscription box services. People enjoy and recommend subscription box services because they offer a curated experience when we have less and less time to shop physical stores. AC: Pitney Bowes was founded nearly 100 years ago and is now a global technology company serving approximately 1M customers in 100 countries. What is the key to Pitney Bowes’ success in adapting to and thriving in such a rapidly evolving industry? LS: Our CEO is a big believer in culture, which is what drew me to Pitney Bowes. You won’t find a company that’s been in business for 100 years that doesn’t emphasize doing the right thing the right way. We have examples from every decade of decisions Pitney Bowes made that prioritized doing the right things for employees, shareholders, and the broader community. Beyond this, we’ve kept our services relevant by focusing on innovation and the client. We are a global technology company, and we’ve consciously built a team of engineers and innovators who think broadly about how to evolve every process. Innovation has been especially critical for us over the last ten years as we’ve had to find Pitney Bowes’ future. For example, we recently delved into e-commerce, and now these services comprise a third of our revenue. Being rooted in e-commerce also allows us to focus on the client. If we’re helping our clients succeed, then we’re succeeding. AC: Tell us more about the Pitney Bowes culture and values. LS: Every company has values, but the best way to figure out what they are is to talk to the people who work there, not to read what’s written on a website. Culture comes down to how a company addresses hard decisions that affect people. When I joined Pitney Bowes, I knew I could trust their culture because it was a theme in every conversation with every employee. Since joining, we’ve conducted a sort of historic excavation of how all of our former CEOs have spoken to clients and employees, and it’s all been consistent with a common set of values. We look to our past to guide us, and it’s rare now for companies to be able to do this, because building a strong culture and solid relationships with employees and clients only happens over time. We sum it up in a simple phrase, “we do the right thing, the right way.” AC: What do you or your company look for in your strategic partnerships? LS: We’re big believers in win-win partnerships. We don’t go into strategic partnerships with the goal of getting the lionshare of the benefit. For instance, we’ve been partners with the US Postal Service for 100 years, and that’s because the value benefit is equal for both parties. Beyond this, we believe that culture fit is extremely important in strategic partnerships, because you have to have common ground to accomplish goals. One of the benefits of being a 100-year-old company is thinking of partnerships as relationships that need to be around for the next decade, not just the next year. We take a long-term rather than a quick-fix approach. AC: Was there a defining moment for you personally that made you into the industry leader you are today? LS: I can’t point to any one moment, but I can say that risk taking is at the heart of my leadership style. I believe that most people don’t take enough career risks. If we don’t step outside our comfort zone, we won’t develop professionally or personally. Growth only happens when we’re uncomfortable, so the more risks we take, the faster we develop. For example, I got a Ph.D. from MIT, and when I graduated, I decided to take an atypical path to gain business experience to supplement my tech background by joining McKinsey. I took every opportunity to grow as quickly as possible, and that mindset eventually led me to Pitney Bowes. I went from being a successful consulting Partner to being a division President with P&L responsibilities. Every risk comes with a bit of failure, but the benefit on the other side far outweighs it. AC: What can newer companies learn from a 100-year-old company? LS: Don’t lose focus on the client, and never stop innovating. AC: Great advice, Lila. Thank you for sharing your insights around consumer expectations and opportunities for e-commerce brands to add more value!", "date": "2020-4-1"},
{"website": "Avenuecode", "title": "Google Cloud AI Product Review, Part 2", "author": ["Hossein Javedani Sadaei"], "link": "https://blog.avenuecode.com/review-of-google-cloud-ai-products-part-2", "abstract": "Most tech savvy people these days know about the potential for cloud computing technology and how the cloud has already affected businesses by effectively storing data and balancing the existing workloads. Google Cloud Platform has been adopted by many organizations for building AI solutions, mostly because it originates with one of the most prominent IT companies - Google. In part 1 of this series, I provided a review of some of Google's AI products. Today, we'll review the remainder. As expressed by Talent Daily , Google's public release of Cloud Talent Solution is a further sign of its vast investment in AI and deep learning, and the swiftly increasing application of these technologies to talent acquisition and management . Image courtesy of Google Cloud Cloud Talent Solution combines pre-trained APIs for talent technology providers and enterprise hiring organizations. Applying AI, it more accurately matches job seekers to open positions, and it also controls job searches, profile searches, and more. Companies may try Cloud Talent Solution out for free with 1-10,000 queries per month (minimal costs apply for 10,000+ queries) . Google offers this new recruiting and hiring platform both to help employers with recruiting by streamlining organizational duties and to help applicants by matching their talents and preferences to open positions. To make life easy for both the hiring and the hired, Google offers Cloud Talent Solution to facilitate the candidate's search and the recruiter's process. The primary points of this service are as follows: Supports enterprise-wide talent acquisition technology through features like commute searches and profile searches; Employs ML technology to assess job content and job seeker intentions; Understands and interprets vague job descriptions, job search queries, and profile exploration queries by applying superior ML techniques; Allows recruiters to give users personalized job search experiences by matching them with jobs that fall within their desired commute time and mode of transit; Matches veterans transitioning from military careers with relevant civilian jobs; and Delivers accurate results in profile search with the aid of an advanced ML model. Companies can try Cloud Talent Solution out for free (limited to 10,000 searches per month). Cloud Talent Solution ties in neatly with another Google product --Google Hire--which is the recruiting software for G Suite. Dialogflow Enterprise Edition is a service acquired by Google for creating conversational interfaces for various devices, apps, websites, etc., giving companies an easy and natural way to communicate with clients.  It is powered by advanced AI methods to identify the intention and context of client queries so that the company's conversational interface can produce effective and accurate replies . Image courtesy of Google Cloud. Chatbots can be very easily produced via Dialogflow. These can be extended across multiple platforms and support multiple languages. These chatbots also auto correct spelling errors. Additionally, Dialogflow delivers automated phone service capabilities. Users who call your company's new phone number will speak directly with your Dialogflow agent (currently available only in beta). This service was created based on natural language understanding technology. Although this service is powerful, other giant cloud service providers like IBM Watson conversational AI from IBM, Lex from Amazon, and LUIS from Microsoft currently have more users and greater popularity. This service makes videos easily searchable and discoverable by pulling metadata, recognizing key nouns, and annotating the content of the video. Using a REST API, it's possible to search every moment of every video file to locate each event of key nouns and to evaluate their importance. Applying this service is easy, as you can see through demos on the internet with codes included . Reza Mirkhani offers another good example of using this API. Image courtesy of Hacker Noon . The following image demonstrates how this service can capture important information from each frame of a video. See following example of our implementation of this service on a video from Gbike and Dinosaur sample video. This API connects your company to Google’s image recognition abilities. Its API/REST interface is similar to images . google . com , but it does much more than showing similar images to users. If you enter an image of a face, for example, this API is able to detect whether the image shows a dog, a cat, a human, etc., and it's also able to identify parts of the image's face (see following example of using this API). It also attempts to identify whether the figure pictured is happy or sad, working in security or retail,  etc. With this API, it is possible to do the following : Facial detection (identifies each face within a picture, its emotions, whether or not the figures are wearing hats, etc.; it does not, however support facial recognition.) Landmark detection Logo detection Label detection Optical Character Recognition (OCR, which detects text, etc.) Image features (i.e. primary colors used, cropping suggestions, etc.) Web detection (pulls similar images from the internet) I implemented this API for landmark testing, and it successfully identified the Petronas Twin Towers in Kuala Lumpur (see following output). The following test is another example of how this API is a powerful OCR tool. Notice how the image is noisy, which means it would be hard for an AI system to determine the correct characters. This API is able to run in the following languages: C#, Go, Java, Node.js, PHP, Python and Ruby. Google Cloud Speech-to-Text converts audio files to text by using strong neural network models in an API. At the time of writing this blog, this service works for 120 languages (and variants) to support companies worldwide. This API enables voice command-and-control, transcribes audio from call centers, and more. With Google's ML technology, it can also handle both real-time streaming and prerecorded audio. Cloud Speech-to-Text: Works for 120 languages and variants; Automatically recognizes which language is being spoken; Renders text transcriptions in real time or stores transcriptions in a file for both short-form and long-form audio; Correctly transcribes proper nouns and appropriately formats dates, phone numbers, etc.; and Offers pre-built models suited to your needs The following table compares Google Cloud Speech-to-Text's prices with prices for similar APIs produced by Azure and AWS prices based on information from TechTarget . This API \"enables developers to synthesize natural-sounding speech with 30 voices, available in 14 languages and variants.\" Cloud Text-to-Speech produces high-fidelity audio using Google's neural networks combined with DeepMind's research in WaveNet. Business owners can use this API in across several devices and application to foster interactions that feel real. Text-to-Speech: Is powered by machine learning from Google; Offers exclusive access to DeepMind's WaveNet Voices; Allows users to choose from 30+ voices; Is easy to integrate with applications and devices that are already in use; and Supports several common use cases This API \" reveals the structure and meaning of text both through powerful pre-trained machine learning models in an easy to use REST API and through custom models that are easy to build with AutoML Natural Language BETA .\" To show some use cases of this API, we implemented it in the following sample text: \"Arthur Samuel, an American pioneer in the field of computer gaming and artificial intelligence, coined the term Machine Learning in 1959 while at IBM.\" Here are the results: As stated on Google Cloud's website , Cloud Natural Language makes it easy to analyze the sentiment behind social media posts, assess the intent of customer conversations in call centers and messaging apps, and obtain other information surrounding people, places, events, and more in text documents, blog posts, news articles, etc. The Translation API dynamically translates between supported languages using Neural Machine Translation. It is a single, programmatic interface that is extremely responsive. Websites and applications can integrate with the Cloud Translate API so that source text can be translated into a new language. This API can also identify unknown languages. Developers who don't have extensive machine learning expertise can train custom models. Best of all, Google's research teams are constantly updating the API to add languages, create new language pairings, and increase accuracy.  As enumerated on the Google Cloud website , the main features of this service are: Language detection Programmatic access Continuous updates Text translation Language detection Adjustable quota Affordable, easy pricing Cloud Inference API is for companies of a particular size that measure clicks, sensor readings, and other event-driven data provide datasets that can beat millions or even billions of rows of data. It’s no simple feat performing a system that can obtain insights from this amount of data, however, by using new plug-and-play solution from Google i.e.,  Cloud Inference API, it is easy. Time-series interpretation is essential for the day-to-day operation of various organizations. Several common examples involve analyzing foot traffic and conversion for retailers, identifying data fraud, identifying correlations in real time over sensor data, or creating high-quality recommendations. With Cloud Inference API Alpha, it is possible to infer insights in real time from time-series datasets. In the previous blog, some of the main Google Cloud AI products including Cloud AutoML, Cloud TPU, Cloud ML engine, and BigQuery ML ML were reviewed, and some details were provided about each. In this blog, we've reviewed some other APIs which can be used to build useful AI applications using voice, video, text, etc. We've also shared some examples and outputs of the models presented.", "date": "2018-12-19"},
{"website": "Avenuecode", "title": "Introducing JetBrains Academy", "author": ["Rafael Romão"], "link": "https://blog.avenuecode.com/jetbrains-academy", "abstract": "As a .NET developer, JetBrains Resharper has been part of my life since 2014, and when I decided to move from .NET to Java as my primary technology stack, an IntelliJ IDEA Ultimate subscription was one of the first things I decided to buy. Their development tools are among the best available today, so when I received an e-mail from them telling me about JetBrains Academy , I clicked the link right away. JetBrains Academy is a learning platform that, unlike most learning platforms today, primarily uses text articles instead of video lessons. Along with these articles, there are lots of exercises, from questions to code challenges, as well as very interesting software projects that allow the student to learn by doing. Just from this short description, JetBrains Academy might not seem that extensive, but take a look at the list of topics you will learn if you choose to complete the Java Developer track , for example: But don't let this image scare you. When you first enter JetBrains Academy and choose a track, you will be asked for your current expertise level and start by choosing a project. Currently, there are tracks for Java, Kotlin, and Python, but Android, Data Science, and Front End are also on the way. As you can see in the knowledge map, you can also use JetBrains Academy to review those algorithm lessons you often need to revisit when preparing for a technical interview. But that's not all. When you select a project, you have a list of skills you need to master before moving on to the hands-on component, and at this point you can choose to take the lessons, which I highly recommend, or skip the topic. The next step will be available as soon as you complete the prerequisites. In this last image, you can see what one of these lessons looks like. I will leave the exercises and all the rest for you. Navigate to https://www.jetbrains.com/academy and start learning today.", "date": "2020-7-8"},
{"website": "Avenuecode", "title": "Your Guide to Organization-Wide Digital Maturity, Pt. 2", "author": ["Alexander Carvalho"], "link": "https://blog.avenuecode.com/your-guide-to-organization-wide-digital-maturity-pt.-2", "abstract": "Last week, we discussed why tech-driven companies are poised to survive, thrive, and outperform competitors in today's globally uncertain economic environment. Today, we'll define five business environments and corresponding business strategies to support your organization's digital evolution. A mature business strategy is a prerequisite for a successful digital evolution. Whether you’re relying on in-house capabilities to lead digital initiatives or you’re partnering with digital consultants, you need to ensure beforehand that your business strategy is tailored to your market environment. A course offered by the Boston Consulting Group, University of Virginia defines five business environments and corresponding strategies. Each organization is positioned uniquely within the five environments listed above. The first step to defining a successful business strategy is for a company to determine its position and closely monitor it to see if that position changes. In unpredictable, malleable, and harsh environments, digital evolution enables companies to adapt and survive because it allows innovation to emerge in improving processes and customer experience. In more stable environments, digital evolution enables companies to be aggressively disruptive and gain competitive advantage. In every scenario, digital evolution allows companies to provide new differentiators by quickly ideating, testing, and going to market with MVP concepts. Because technology has made every market exponentially more competitive, the companies that continue to achieve success are those that are self-disruptive rather than those that wait for competitors to disrupt them. This means creating a balance between maintaining core business success and seeking new innovation opportunities. Image courtesy of McKinsey Digital There are three ways to achieve this balance: proactively refocusing an existing team on innovation opportunities to capitalize on opportunity windows, creating a separate innovation lab attached to the main business, or collaborating with a third party, like a partnership, to introduce innovation. It’s important to remember that the customer is at the center of every initiative, whether that initiative is related to improving the core business or pursuing innovation opportunities. Companies can design products and services that are truly customer-centric by relying on design thinking-informed frameworks and methodologies, including service design, design sprints, and design ops. Next week, we'll cover how to improve core business offerings and identify high-ROI innovation opportunities through design services. Want to know now? Download our free whitepaper for your complete guide to organization-wide digital maturity. Ideas based on materials from the “ Digital Transformation ” course offered by Boston Consulting Group, University of Virginia. Coursera. Accessed 9.16.2020. Why digital strategies fail , Jacques Bughin, Tanguy Catlin, Martin Hirt, and Paul Willmott. McKinsey Digital.", "date": "2020-12-9"},
{"website": "Avenuecode", "title": "How To Develop An Automation Framework - Part 1", "author": ["Meera Honde"], "link": "https://blog.avenuecode.com/a-guide-to-developing-an-automation-framework-part-1", "abstract": "In the simplest terms, automation is the test code developed to reduce human efforts whenever there are any changes in the system and the system then needs to be revalidated against new changes. Generally, in application testing, a system validation is performed by running regression test suites every time. Automation is a great way to perform regression testing as well as validate the system against new changes. It also helps to reduce human efforts and manual execution errors by running the regression suite through automation every time. If automation is done correctly, it increases testing scalability, decreases huge manual efforts, reduces execution time, and reduces costs for testing efforts, which then decreases overall project cost. T he following are some good points for any automation engineer to consider w hile developing automation framework or working on any specific automation framework: Cons: -Since they're open source frameworks, there might be a lack of available technical support. -Some open source frameworks aren't cross platform. -Some have issues with mocking, so you may need to add Mockito. -Some aren't easy to maintain. -Some may not support additional libraries or extensions. Therefore, while selecting your testing framework, consider all important factors. Check for compatibility, extensibility, and reliability for open source frameworks. Some of the examples of open source framework are JUnit, TestNg, Mocha, Karate, NUnit, MUnit, and Robot. If you choose to go for licensed frameworks, check out the following pros and cons: Pros : -Technical support is easily available. -Most licensed frameworks are highly secure. -Most licensed frameworks are well documented and provide trainings, making learning easy. -They're reliable to install in any environment. -Some frameworks provide high extensibility so you can start working while development is in process, such as QTP, which provides features like descriptive programming to solve object recognition issues. Cons: -Can cause an increase in the overall cost of your project. -Some frameworks support very specific programming languages, for example, QTP only supports VB or C++. -For some frameworks, extensibility and compatibility with other frameworks can be hard. Therefore, when you want to migrate code from other frameworks or configure other frameworks within your framework, a lot of change is required. Some examples of licensed automation frameworks are HP’s QTP ( currently called UFT - Unified functional Testing), IBM’s Rational Functional Tester, and Micro Focus’s Silktest. It's always good practice to identify the correct tests for automation. This helps reduce automation efforts with high maintainability, reduce duplicate efforts, and use automation effectively. Always choose tests from regression test suites that meet the following criteria: -Tests that test after any change is made in the application. -Tests with high risks. -Tests that are difficult to execute manually. -Tests that take more time to execute because they need to execute for all changes. -End to end test scenarios that verify that the application isn't breaking after new changes are made. -Tests that may fail with different hardware or software configurations. -Tests that need multiple data sets. -Tests that cover user acceptance scenarios. -Tests that may fail because of human error. It's extremely important to check your code for reusability because it helps to avoid duplicate code as well as maintain automation code when the application grows. Code reusability can be implemented by following some good practices such as developing utility functions, separating libraries, data files, and putting common resources together. In Selenium Webdriver, use the page object model design pattern to support reusability, inheritance, encapsulation, data abstraction, developing libraries, or common utility functions. The page object model design pattern allows you to define each web page as one Java object/class. It also implements object oriented concepts like polymorphism and overloading/overriding to make the code more maintainable and readable. No code is bug free, but to make it as clean as possible, and to find the maximum amount of defects, you can follow these simple strategies: Using different testing frameworks in the same automation code: Configure your automation framework in such a way that it can support multiple testing frameworks in your automation. Integrating your automation code with different frameworks: B efore writing your tests and preparing test data, think of possible ways that they can be bundled with other tests. Implement different testing frameworks within your testing framework if needed. Things to keep in mind when checking for high code maintainability: -Correct code configuration. -All page object related code should be under the SRC/Main/Java package. -Breaking down multiple objects that we can filter with the same functionality into sub packages. -All packages under main/sub packages should be created or named as per functionality. -Each class name should have a meaningful name. -All test classes should be under a SRC/Test/Java package. For individual classes and sub packages, the same rules apply. -Each test class should end with Test in its name. -For all the resources, there should be a SRC/Test/Resources package. -All data files should go into the SRC/Test/Resources/data package. -All property files should be in the resources package. -All common utility functions should be added to the libraries or utilities package. I hope you found this article to be helpful! I'm interested to hear how these tips work out for you. Try implementing what you've learned from this article into your practices and let me know what you observe.", "date": "2017-11-9"},
{"website": "Avenuecode", "title": "How to Determine a Performance Baseline for a Web API", "author": ["Rafael Romão"], "link": "https://blog.avenuecode.com/how-to-determine-a-performance-baseline-for-a-web-api", "abstract": "Some time ago I wrote about how to set up a .NET Core Web API to report metrics to New Relic . Now, I'm going to show you how to use New Relic--or any other monitoring platform, such as Azure Application Insights --to get a performance baseline for your API. A performance baseline is the expected performance of an application under certain conditions. For a Web API, a performance baseline is the average response time at the maximum throughput it can consistently sustain. Let's say, for example, that a specific endpoint of our Web API in a single node, production-like environment can successfully respond in ~250ms with a throughput of up to 305 requests per minute (rpm). This would be our performance baseline for that specific endpoint. If we say our baseline is ~250ms at up to 305rpm, we mean that if the workload for our API is less than 305rpm, it will respond in up to 250ms, considering only a single node. But if the workload has 400rpm, it probably won't be able to sustain the 250ms response time. With this information, we can use the performance baseline to determine: In order to determine an application's performance baseline, we need to have: In this article, I will use jMeter and New Relic to obtain the baseline for the API I created in my last post . Before starting the baseline determination procedure, it's important to select a good data set and data source. I say both because not only do we need to control the data used to obtain the baseline (since we need to use the same data every time we need to compare different versions), but we also need to know the time our API takes to obtain this data. Consider, for example, that our API consumes another API to obtain some input data, or maybe it needs to get data from a database. In these cases, the performance of the real data sources can vary, so we need to replace these data sources with something that allows us control over our own API response time. Tools like Wiremock and Mountebank are good candidates for this job. There are different ways to determine an API's performance baseline. Here I will show the one I consider most informative. For this procedure, we have to: In this first request, all we have to configure in jMeter is an HTTP Request node querying our service endpoint. This request is important to give us our reference response time, which we can get from New Relic: By selecting an interval where we see a constant response time, we get our reference average response time: Next, we use this reference response time to configure the Gaussian Random Timer for a second thread group, where we will have 10 threads querying our service endpoint. To determine the Offset and Deviation values for the Gaussian Random Timer, we follow these steps: - For the Constant Delay Offset, multiply the average response time of the single thread requests (in this case 246ms) by the number of threads (10) minus 10%: (246*10)-(246*10*0.10) = 2,214. - The Deviation must be twice the 10% we remove from the Offset (in this case (246*10*0.10) *2=492). Before we proceed, let's review exactly how we use these values: The Gaussian Random Timer will be used to delay the requests so that we can control the throughput of our service. This delay is the time jMeter will wait between two consecutive requests performed by the same thread, and its value is the fixed Offset plus a random value ranging from zero to the informed Deviation. In other words, if we have 2,214 as our Offset and 492 as our Deviation, it might have a delay ranging from 2,214 up to 2,706, which will give us an average of 2,460, which is 10 times our average response time for a single thread. This way we can ensure that, though we use 10 times more threads, we are still putting the same average load over our service and also varying the number of requests. The metrics in New Relic will show us we still have the same average response time: Now, we repeat this process for 20, 40, and 80 threads, keeping the same values for our Offset and Deviation, which should give us a variation in the throughput proportional to the variation in the number of threads while keeping the same average response time. Notice that if we modify the Offset and variation values again, according to the number of threads, we would keep the throughput the same, along with the average response time. This way we would not be able to stress our API. So, for 20 threads we have: And for 40 threads: Notice that with 40 threads we already can see some variation in the response time of our API endpoint. If we repeat the process with 80 threads, we can see that it is no longer able to respond as expected: Our response time varies a lot and is far higher than the average we could keep using up to 40 threads. If we repeat it with 30 threads, then we go back to a better state, like we had using 20 threads: From this data, we can conclude that an average response time of ~250ms at up to 305rpm is the baseline performance for our API endpoint. After determining your performance baselines, a good practice would be to write some performance tests, using Jenkins , for example, to check if the baseline is still the same. It's advisable to run these every other night so that you'll know if something starts harming the performance of your Web API. You can also integrate this kind of test in your build pipeline so that you can make sure a new version is not slower than the previous one. If you have any comments or questions about this topic, let's talk below.", "date": "2018-8-1"},
{"website": "Avenuecode", "title": "AC Spotlight - Jose Murillo", "author": ["Andressa Lopes"], "link": "https://blog.avenuecode.com/ac-spotlight-jose-murillo", "abstract": "Dr. Jose Murillo, Chief Analytics Officer at Grupo Financiero Banorte, speaks with us about using AI for revenue generating projects as well as cost cutting strategies to further digital evolution. Avenue Code: Tell us about your personal career path. How did you become the Chief Analytics Officer at Grupo Financiero Banorte? Dr. Jose Murillo: After graduating from Rice University with a Ph.D. in Economics, I joined the Bank of Mexico’s research department, working on several strategies to mitigate the impact of the financial crisis. I eventually became the Director of Information, Price Analysis, and Regional Economies and held the staff’s view on the main decision variables for the Monetary Policy Committee. When the governor of Mexico’s Central Bank started working for Banorte, he invited me to come as his advisor. A few months later, Mr. Rafael Arana, a newly appointed Chief Operations Officer, shared with me his vision for an analytics business unit and asked me to build it. He wanted to establish the unit as a profit center, which is a very unusual focus for data science and analytics teams in North America. In my first year, I produced 46 times the cost of the department, and last year, with more data scientists on my team, we made 245 times our cost, almost 1 billion dollars in net revenue. During my tenure at Banorte, we’ve seen more than 3 billion dollars in net revenue for the company, which has allowed us to become the most profitable bank in Mexico, the second largest financial group in Mexico, and one of the most profitable banks in North America. AC: How has data enabled you to cut costs so dramatically? JM: On our transformation journey to become a data-enhanced financial group, the emphasis was initially on cost cutting strategies for operational and financial risks. This was the focus of almost 50% of our projects. Progressively, revenue generating projects became much more important. We’re now primarily focused on increasing the value of our business relationships with customers. Data is the basis for our strategy to enhance customer experience because it helps us deliver products at the right price, at the right time, through the right channels. Beyond this, AI helps us accelerate the learning curve. When we test new interventions, we start small, then scale up. AI techniques help us generate results faster. AC: What are the biggest challenges and opportunities you have faced as CAO of Banorte? JM: The biggest challenge is overcoming skepticism. Many companies that want to implement data science, analytics, and AI begin by partnering with external companies. What sometimes happens is that there is not a close enough co-creation process between the business and the data scientists, which means that the technology is not used to solve the right questions, resulting in a low ROI. But as soon as you’re closely aligned, you can prove the value for the technology and foster a willingness to transform. When it comes to opportunities, it’s important to look at revenue generating projects as well as cost cutting initiatives. You have to have some degree of credibility for revenue generating projects. For example, perhaps you know that you’ve sold X credit cards, but you don’t know the long-term impact - will those credit cards have a good or bad record? Are they going to perform, or not? You have to agree on metrics for success, and those metrics must correlate to real business value. At the end of the day, revenue generating projects must help you extend the duration, depth, and width of the business relationship with the customer, which is what allows you to increase customer lifetime value. The value of a company is based on its business relationship with its customers. AC: How do you overcome skepticism and show the potential for data-enhanced transformation? JM: Implementing data science, analytics, and AI mandates transformation at the basic level, which means breaking silos and restructuring. It’s not enough to have strong analytics or strong data science expertise. You need to be able to effectively communicate your goals for these technologies, as well as the expected results - that’s how you make a case for their value. You also have to create good will between teams, giving credit where credit’s due. Finally, you have to establish the right incentives and ensure alignment between individual teams and the corporation as a whole. At Banorte, we present projects and vote on which ones have the biggest ROI potential. This way, everyone sees that if the projects are successful, they will add value to the company, which benefits everyone. AC: What is the future of AI in the financial sector? JM: AI is one of the most fascinating developments in finance. While AI has been present in academia for several years, firms are only beginning to understand its potential. In the financial sector, the goal for AI is to help build a long-term relationship with customers. For example, the goal for both firms and customers is for the latter to gain financial stability over time. That means they need to start saving now, but there are several human biases, like hyperbolic discounting, that act as barriers. So firms can help customers, and there are a lot of results that are well-established. But the problem is determining which of these are relevant to the business and how to communicate them efficiently to the customer. The way that you understand that is through small, well-designed interventions. In this particular situation, AI helps us understand how each small intervention works and predict which interventions are effective to scale for a large population, accelerating the learning curve and enabling companies to meet financial targets. In other words, AI assists corporations in better understanding the customer. AC: What are the benefits, challenges, and limits of AI in the financial sector? JM: AI helps us better understand the customer, understand the risk, and improve risk models. This, in turn, helps us to significantly expand our customer base. One of the challenges for AI is the black box problem. We can change this by explaining AI techniques and showing their value. On the marketing side, AI can also help build a more efficient process for customer relationship management. Once you can understand the customers better, you can define better products and deliver these products at the right time, at the right price, through the right channel. These are the main advantages. One of the reasons it’s hard to prove the value of AI is that it’s difficult to demonstrate an impact on ROI. Part of the problem is that people are too focused on using AI for cost-cutting strategies like chatbots. On the other hand, I think it can be used to enhance and humanize the relationship with the customer. With AI, you can be much more efficient in having meaningful conversations with customers and much more attuned to what the customer is saying. You also avoid wasting the customer’s time. Using AI only to save money is problematic. The limits of AI are primarily ethical. This technology should be used with a sense of responsibility and with a receptivity to feedback and criticism. AC: What are the current data trends, and how do you plan to adopt them at Banorte? JM: Value has to be demonstrated. If data science and analytics groups cannot prove value, AI won’t be used. The biggest trend right now is to be able to measure the impact of your work. This is one of the most important things for the industry, because data science investments are expensive, so if value is not proven, it will be a passing fad. On the other hand, the industries that understand its potential will move at a quicker pace. In that sense, there’s going to be a bigger focus on how data science helps boost revenue instead of just cutting costs. AC: Thank you for the insight into how AI, data, and analytics are being used in the financial sector, Jose!", "date": "2020-7-3"},
{"website": "Avenuecode", "title": "Finding Bugs with Git (Blame and Bisect)", "author": ["Jonathan Ohara"], "link": "https://blog.avenuecode.com/finding-bugs-with-git-blame-and-bisect", "abstract": "Everyone knows Git as a great file versioning tool. But did you know you can also use Git to debug your code? In today's Snippet, I'll introduce two tools I use all the time--Git blame and Git bisect--and explain how they can help you find bugs hiding in your code. Git blame is a well-known tool that displays which users most recently modified specific lines of code up to the last commit in a document. For example, the simple README.md file code below displays the partial commit hash followed by user, date, time, time zone, line number, and line content information. In the code above, we can view 3 commits (406b2818, 06c430e2, d44487fe), followed by committer data and line content information. Git blame has many uses, which are detailed in full in the links at the end of this article, but one of the most important uses is [-L <range>]. This option allows you to isolate and check a specific set of lines within your file. For example: Above, we're filtering lines 2 through 8. (Note that 'blame' also shows uncommitted changes.) Another interesting option is [-C], which is especially useful when you are refactoring a code. [-C] is helpful for finding code blocks that have been transferred from other files. Below, for example, there is a simple endpoint using Java and SpringBoot. If I decide to move the method sayHello to a new file called MainController, I get the following: Now, the [-C] option displays a new column, not yet committed, that specifies the code's original location: IDEs like Eclipse (figure 01 below) and IntelliJ (figure 02 below) have a built-in 'blame' integrated with the code editor. In Eclipse, right click the line number and select 'Show revision information.' In IntelliJ, right click the line number and select 'Annotate.' Figure 01: Blame In Eclipse Figure 02: Blame in IntelliJ Now that we've covered Git blame, let's take a look at Git bisect. We used Git blame for discovering who changed your code, when they changed it, and which commit they used to do so, but if you're still usnure what's breaking your code, you can use Git bisect. Before discussing usage tips, let's review some basics. \"Git bisect\" stands for \"Git binary search.\" In a nutshell, Git bisect uses a binary search to find the commit that introduced a bug. Before looking at an example, let's quickly review binary searches. First of all, why do we use binary searches instead of searching commits from newest to oldest? Very often, bugs are introduced early in projects, so if you have a code sequence with ten thousand commits and the bug is in the second, you'll waste time reviewing commits chronologically. Binary searches simplify this process: within a collection of elements (e.g. ordered integer arrays), we start at the middle element and check if it has a bug. If it does, we check the element halfway through the preceding code; if it doesn't, we check the element halfway through the following code. We continue iterating this process within smaller groups until we identify the element, saving ourselves a lot of labor and time. (If you want to learn more about binary and other search methods, listen to this lecture from MIT Open Courseware: Binary Search, MIT Open Courseware .) Now let's apply binary searches using Git bisect. When using the binary method to search your commit stack, you'll need to inform Git if the bug is present at each search phase until it identifies the problem commit. Let's take an example that starts with the first check in an entire commit stack: As you can see, the commit that added a bug in the project is \"7f3ce1e Adding a BUG not covered by tests.\" Now let's run the Git bisect: Next we need to define the range of the search: In the first line, we define that the HEAD commit is bad. In other words, we have a bug at this commit. Next, we need to define a commit where the bug isn't present. For this, I often use the last commit of the last release. (Note that tags can help you.) Now Git will scan an old commit: If you do a \"Git log,\" you can check exactly where the Git is, and you can also check some markers: Now we need to go to the project and verify if the bug is still there. You can use \"Git bisect bad\" or \"Git bisect good\" to inform Git whether the bug is there or not. If the bug is still present, we inform Git accordingly: Then Git moves our HEAD to a commit before the bug. Check the Git log: Next, we execute Git bisect good, and Git moves the HEAD to where the bug is: Checking the log, we get: This time, we use \"Git bisect bad.\" Then Git moves the HEAD one last time to verify exactly where the bug was introduced. Once more, the log shows the following: If you look at the first Git log before the bisect, you'll see that the commit \"f2945f7\" is the first commit before the bug. Now when we inform Git that the HEAD is good, we'll be able to see the commit that introduced the bug: To leave bisect mode, type: Now that we've identified our bug location, we can use a Git bisect run command to determine whether the script is good or bad: We can use the run command with Maven to generate results more quickly and easily. There is only one thing that we need to set up before running bisect with Maven: if you try to create a commit at the top of a stack, Git bisect will remove commits to find the bug, so you first need a broken test to check if the bug is present. Here are two ways I do this: 1. Create the test - commit the test - create an auxiliary branch - reset the auxiliary branch to a past point (usually to the commit where you will use bisect good). Then, we rebase our source branch so that our new branch becomes the next test's first commit. Let's review this process step by step. First, we create a new branch titled \"tempBranch:\" Next, we reset the branch to a past point: Then, we use the interactive rebase to change the order of our commits: Make sure that you move the new commit to the first commit in the interactive rebase: Check that the commit \"fc93c46 Test to Check the BUG\" is right after \"e70b341 Tests for update and get.\" 2. Alternatively, we can create a new file, code the test, and keep this new file unversioned. This method is faster, but it's also easier to create a conflicting file name or lose the new test file, depending on what you do after the bisect. If we start by creating a test that shows Git if the project is good or bad, we can use Maven to execute the next steps for us: Now we can wait for Maven to complete the process on its own: You can view all these steps in the following video: Now you know how to use Git blame and Git bisect to easily identify bugs in your code! Git Blame Docs: Git Bisect Docs: All examples are based in following repository: GitHub", "date": "2018-7-11"},
{"website": "Avenuecode", "title": "An Introduction to Mountebank, an Open Source Mock Server", "author": ["Ricardo Ribeiro"], "link": "https://blog.avenuecode.com/an-introduction-to-mountebank-an-open-source-mock-server", "abstract": "Most developers have been in a situation where they need to make mock API calls. Whether you need to mock a call to help your team implement new features or you need a mock server for integration testing, mountebank is the answer. Mountebank is an open-source tool that allows developers to create multiple endpoints with several special conditions so that they can mock real API calls to test applications. Today, we'll describe how mountebank works and guide you through a simple tutorial. Simply stated, mountebank allows you to create imposters that contain all server information. You can set the connection type and the port you want to use. Inside imposters, you can also set stubs. A stub is like one endpoint where you can set your predicate to inform mountebank of the kind of call you expect, as well as the response mountebank must return when an added element \"matches\" a request. You can set imposters and stubs by writing code using EJS files (Embedded Javascript Templates), or--what's truly remarkable--by using a mountebank API. In other words, you can actually use the mock server you created to post your new imposters and stubs, allowing you to create dynamic tests! Now that we've examined mountebank in theory, we'll provide a quick tutorial so that you can begin to use it on your own. First of all, you will need to install Node.js to use mountebank. After installation is complete, run the follow command: Then run: Now you have an instance of mountebank running on your computer. You can test it by accessing mountebank documentation . First, let's create an imposter.ejs file by writing this simple code: By creating this code, we're creating one imposter that will listen on port 1,000 only through a https protocol. In this example, we're creating only one stub. Since this is an array, however, you can create as many stubs as you like for this port, which creates multiple scenarios for potentially successful and unsuccessful cases. Next, we run the following command: We can now make a GET to https://localhost:1000/tutorial , and we'll get the response we set. As mentioned above, it's possible to create  imposters by posting to the server API. In this case, all you have to do is make a POST to http://localhost:2525/imposters and use as your data the same imposter we created  above. ( Note that you can't update your imposter on mountebank; you only have the option to create or delete.) To delete an imposter, simply make a DELETE to http://localhost:2525/imposters/1000 , remembering to replace the last number with the port you are using. The operations enable you to dynamically create imposters before an integration test. All you need to do is configure the stub, make the calls, and delete after use. The predicate we chose as our example is a very simple one, but you can use as many conditionals as you need to create your predicates. Mountebank accepts several operators, such as equals, contains, matches, not, or, and, etc. You can combine these operators with regex and create powerful predicates that will provide a response to every call. Let's walk through an example of a more elaborate predicate: In this example, we'll telling mountebank that it should expect requests on any of those 3 paths with a POST method and a Content-Type header. We'll also tell it to expect a body. If one call satisfies all these items, mountebank will return the desired response. (You may view more examples of predicates here .)", "date": "2018-5-30"},
{"website": "Avenuecode", "title": "Being Agile - How to Stay Focused on Your Outcomes", "author": ["Eduardo Bruno Silva"], "link": "https://blog.avenuecode.com/how-to-stay-focused-on-outcomes", "abstract": "“Money is a perfectly legitimate measurement of goods sold or services rendered. But it is no calculation of value... Value is a feeling, not a calculation. It is perception.” - Simon Sinek, Start with Why", "date": "2018-11-21"},
{"website": "Avenuecode", "title": "Microservices 101", "author": ["Rafael Romão"], "link": "https://blog.avenuecode.com/microservices101", "abstract": "Microservices is all the hype these days, so in this post, I'll try to highlight its most important aspects. I've split these aspects into four groups based on the different benefits they provide. Single Responsibility Principle Before this term, microservices was simply called web services, REST services, or workers when its bootstrap was a message coming from a broker instead of a web request. Today, microservices comes can be split into these two groups: web services or messaging workers. A microservice usually relies on other services for its inputs. Sometimes they will come as part of the request, or the microservice will call other services to get the inputs it needs. In either case, there will always be a dependency on the contract of the consumed service, but not on how this service produces this contract. Changes in the behavior of a microservice should not require changes in the services that consume it, as long as the contract stays the same. \"organizations which design systems ... are constrained to produce designs which are copies of the communication structures of these organizations.\" Organizations that develop software in silos, depending on handoffs of development stages, will develop a software that will also be highly interdependent on its parts. Microservices will usually have its own database and resources. There will be data duplication, which is alright because eventual consistency will be achieved. Microservices will also be developed and maintained by a single team who have complete ownership of the service as a whole - there will be no separate QA or DBA team to work on a set of microservices. Those professionals will be a part of the team even if they eventually work on more than one team. It should also be noted that a single team can own more than one microservice. Microservices can be implemented using independent technology stacks. The best technology stack to use when building a microservice is the one that delivers the best value in that case. Today it might be Java, but tomorrow it might be Go, so as long as the team that masters that stack and the integration is performed using standard protocols, there's no need to try to stick to a single stack. One of the most common misconceptions regarding microservices is about how it differs from SOA . There are even those who claim that microservices is a special kind of SOA, which I disagree with. Here's why: SOA stands for Service Oriented Architecture, and a Microservices Architecture is indeed an architecture oriented by services, but the term SOA references a different kind of integration. When the concept of SOA was born, this integration meant exposing the whole system to others via Web Services, usually refactoring those systems and using some kind of middleware such as an Enterprise Service Bus (ESB) to mediate this communication. On the other hand, a Microservices Architecture refers to systems, or parts of systems, built from scratch as microservices. The systems that an SOA integrates live by their own,  so the integration is optional. Microservices cannot live by their own, and will always be dependent upon other services in order to continue the job they started, or to provide completed bits and pieces of the job so that they may continue. You will always need more than one microservice, or at least other software components like a user interface, to get some business value from them. Another important differentiating factor is the fact that a Microservice Architecture is fundamentally based on the \"smart endpoints and dumb pipes\" paradigm. It values choreography over orchestration, and therefore, should have simplified communication channels, which is conceptually the opposite of an ESB integration. Even if MuleSoft were used to translate messages between microservices, that process would be seen as a microservice of its own. And finally, microservices are built for the Cloud so they must be adherent to statements like the Reactive Manifesto , or the 12 factor app , and none of these existed when there was hype around SOA. So, what are your thoughts? I'd love to hear your perspective in the comments below. To learn more about microservices, read my whitepaper, Microservices Architecture as a Large Scale Refactoring Tool , and check out the amazing book Building Microservices , by Sam Newman.", "date": "2018-1-17"},
{"website": "Avenuecode", "title": "Android Basics - Activities & Fragments", "author": ["André Servidoni"], "link": "https://blog.avenuecode.com/android-basics-activities-fragments", "abstract": "In this collection of articles about Android, I will guide you through the basic topics that you'll need to get started in this mobile world - even if you're completely new to the Android operating system. Let's start with a little history: What is Android? According to Wikipedia , \"Android is a mobile operating system developed by Google, based on the Linux kernel and designed primarily for touchscreen mobile devices such as smartphones and tablets. Android's user interface is mainly based on direct manipulation, using touch gestures that loosely correspond to real-world actions, such as swiping, tapping and pinching, to manipulate on-screen objects, along with a virtual keyboard for text input.\" As the most of you know, the Android system runs its features using applications. So, let's take a deeper look. In this first article we will talk about activities, fragments, show example codes, and look at the lifecycle of the Android application. When an application is opened and shows its first activity, it follow a specific lifecycle. Within this, you can handle when the activity is destroyed, paused, resumed, or created. The fragment lifecycle is embedded within the activity lifecycle but has some details and extra steps in it. The following image compares both lifecycles: on the left, the activity, and on the right, the fragment: Source In the image we can see a lot of callbacks. Some we must implement, and others no. But what is the correct execution order, and which ones do we need to implement? The only mandatory callback that we should override is onCreate for the activity and onCreateView for fragment, because we define the layout and do the mapping of the UI components inside of them. When an activity is coming to the foreground and becoming visible to the user, it goes through the onCreate method that creates the activity layout, from the XML. After that it will go through the onStart and soon after the \"onResume\", in which the activity is already visible on the screen. Now when we enter the background (we pressed home or another activity entered in front of it) it goes to the onStop method and when it comes back to the foreground it executes the onRestart methods and then the onStart and onResume normally, since it was only in the background and does not need to be re-created. And finally when the activity is destroyed, for any reason (for example, pressing the back button) it follows the following flow: onStop and onDestroy in which all the resources of this activity are released and it exits the system activity stack. In this case, if we call this activity again, as it was destroyed, then it will start its flow from the onCreate because it needs to be re-created and added again in the stack. So let's put together all this information: Now that we know about the theory, have a look at the code! Examples can be found by following the github link at the end of this article. First let's take a look at the layout file. An Android layout is where the visual structure of your app or widget is set. In it are placed the components that will appear on the screen and perform the tools to interact with the user. There are 2 ways to instantiate Android graphics components: Declaring the whole layout part of your app in an XML file, we were able to separate the graphical interface from the source code, leaving the structure of the project more organized and easier to understand. Following we have an simple layout of a login screen in the XML: <?xml version=\"1.0\" encoding=\"utf-8\"?> <LinearLayout xmlns:android=\"http://schemas.android.com/apk/res/android\" android:layout_width=\"match_parent\" android:layout_height=\"match_parent\" android:orientation=\"vertical\"> <TextView android:layout_width=\"match_parent\" android:layout_height=\"wrap_content\" android:layout_marginBottom=\"50dip\" android:layout_marginTop=\"40dip\" android:gravity=\"center\" android:textSize=\"20sp\" android:textStyle=\"bold\" android:text=\"@string/app_name\"/> <TextView android:layout_width=\"match_parent\" android:layout_height=\"wrap_content\" android:layout_marginTop=\"10dip\" android:gravity=\"center\" android:textSize=\"16sp\" android:text=\"@string/username\"/> <EditText android:id=\"@+id/main_user_input\" android:layout_width=\"match_parent\" android:layout_height=\"wrap_content\" android:layout_marginStart=\"10dip\" android:layout_marginEnd=\"10dip\" android:inputType=\"text\" android:gravity=\"center\"/> <TextView android:layout_width=\"match_parent\" android:layout_height=\"wrap_content\" android:layout_marginTop=\"30dip\" android:gravity=\"center\" android:textSize=\"16sp\" android:text=\"@string/password\"/> <EditText android:id=\"@+id/main_password_input\" android:layout_width=\"match_parent\" android:layout_height=\"wrap_content\" android:layout_marginStart=\"10dip\" android:layout_marginEnd=\"10dip\" android:inputType=\"textPassword\" android:gravity=\"center\"/> <Button android:id=\"@+id/main_button_login\" android:layout_width=\"match_parent\" android:layout_height=\"40dip\" android:layout_margin=\"30dip\" android:text=\"@string/login\"/> </LinearLayout> Now that we already did the XML for our screen, we need to tell the activity which layout to use and \"bind\" the components of the XML with the code, so we can get info and/or handle user interactions. For this we will use the method setContentView inside the onCreate (in activity) or onCreateView (in fragment) to tell the code the layout of that screen. In the following example we set the layout and after that we \"bind\" the element from the layout using the findViewById method that will search for a view in the XML with the respective id . We do a cast, (EditText), because the find view returns a parent view object. public class MainActivity extends Activity { private EditText mUserInput = null; private EditText mPassInput = null; @Override protected void onCreate(Bundle savedInstanceState) { super.onCreate(savedInstanceState); setContentView(R.layout.activity_main); mUserInput = (EditText) findViewById(R.id.main_user_input); mPassInput = (EditText) findViewById(R.id.main_password_input); Button loginButton = (Button) findViewById(R.id.main_button_login); loginButton.setOnClickListener(new View.OnClickListener() { @Override public void onClick(View view) { // do something when click } }); } } To do this in the fragment it's a little different but the concept is the same.  Let's take a look at the example. The onCreateView need to return the inflated view to its parent, so we use the layout inflater parameter to reference the view for the specific layout (in this case R.layout.activity_main). After that we \"bind\" the components from the inflated view object and at the end we return it. public class ExampleFragment extends Fragment { @Override public View onCreateView(LayoutInflater inflater, ViewGroup container, Bundle savedInstanceState) { // Inflate the layout for this fragment View inflatedView = inflater.inflate(R.layout.activity_main, container, false); mUserInput = (EditText) inflatedView.findViewById(R.id.main_user_input); mPassInput = (EditText) inflatedView.findViewById(R.id.main_password_input); return inflatedView; } } Today we covered the basics for creating a simple screen activity and binding the layout components with code in order to manipulate it. In future articles we'll discuss other topics such as persistence, network and application flow. Thanks for reading and feel free to reach out to me with any questions about it! All the code is available at my github, feel free to use it: https://github.com/DeKoServidoni/android_study", "date": "2017-2-22"},
{"website": "Avenuecode", "title": "More Teamwork is Not the Answer", "author": ["Marcio Viegas"], "link": "https://blog.avenuecode.com/more-teamwork-is-not-the-answer", "abstract": "As businesses develop globally and cross-functionally, collaboration becomes key. But when it comes to delivering results, collaboration acts as a powerful booster, not the only essential fuel. As a thought experiment, suppose that Abraham Maslow was alive today and society gave him the arduous task of building a hierarchy of needs for companies to accomplish large-scale goals: I'm willing to bet that teamwork wouldn't be at the foundation of his pyramid. Why? Without individual work and deep personal mastery, group efforts don't contribute much toward collective goals. But the current trend is for some companies to invert these values, placing greater emphasis on collaboration and less on individual achievements. Let's dive into why this isn't effective. There is no teamwork if there is no work - and that fits semantically as well as grammatically. For instance, you can have a group of masons co-operating and communicating with one another, but if they don't individually lay the bricks one after another, there will never be walls to build a house. This sounds obvious, but it's less apparent in the IT domain where results are sometimes less immediately visible. In IT, a squad - or a group of developers - can hold crucial meetings, create a smooth communication process, and build workflow boards. Yet, if each individual doesn't deliver the necessary code, there will never be an application to run. Admittedly, there is not much value in a house or a system that does not work as expected because the builders/developers didn't clearly communicate how the parts would fit together. However, how much value does a team actually deliver if its members never have the time to build anything because they're so busy attending meetings? Obviously, we need a balance here. The point is, what kind of reaction would you receive from your teammates if you rejected a meeting in order to work on something alone? In some cases, they might start wondering if you're a trustworthy employee. When Albert Einstein stated, \"we can't solve problems by using the same kind of thinking we used when we created them,\" he was referring to the thought processes both of individuals and of groups of people. There isn't a magical collaboration process that will exponentially enhance a team's knowledge; knowledge is invariably the sum total of each individual's expertise plus the potential connections within the team. That said, weirdly enough, people (perhaps unconsciously) believe that knowledge will create itself purely through collaboration. It's analogous to gathering a bunch of musicians into a room to solve one of those problematic physics enigmas and expecting them to solve it only through collaboration without previous education in physics. Indeed, if we look at historical examples of group problem solving, puzzles are generally solved with pieces of knowledge that individuals bring to a group. Even though collaboration is an enabler, personal mastery is fundamental and should come first. In the end, if your team is consistently struggling with specific problems, it is more likely that you will solve them by acquiring deeper knowledge than by scheduling a team meeting. More isn't always better, and when it comes to teamwork, it looks like we neglect this statement. Consequently, this business value is not only overrated but can also generate a collaborative overload that causes extra stress, anxiety, and sometimes burnout. Don't convert yourself into a total lone wolf locked away looking for knowledge, but don't become a team worker limited by the bubble and the kind of thinking that surrounds yourself and your team. P.S. Don't use this article as an excuse for rejecting your next team meeting - especially if it's a retrospective meeting! *Disclaimer: I'm sharing my thoughts as a self-criticism of my life and work. I'm writing to empower discussions and not to make an indisputable point. Please don't take any of the \"you\" references personally. I exaggerate events using humor because that is how I believe I can generate good conversation, so leave a comment with your thoughts on teamwork!", "date": "2020-4-8"},
{"website": "Avenuecode", "title": "What's New in Mule 4 ?", "author": ["Anupam Gogoi"], "link": "https://blog.avenuecode.com/whats-new-in-mule-4", "abstract": "We've all been excitedly waiting for the new version of Mule 4 that Mulesoft will be releasing soon. Here are some important features we want to highlight that will be available in Mule 4: In Mule 3 when we utilize DW and  transform messages we need to explicitly convert the message to a Java object so that the output can be used in Router component, For Each, etc. In Mule 4 you don't need to explictly convert the Message to Java objects, as Mule 4 will do it automatically for you. The following connectors have been updated: File, FTP, JMS, VM, etc. The connectors have been made operation-based: for example, to create a directory or file, Mule 4 has a connector named File-Create Directory, to write the File: Write directory, etc. Error Handling has been modified significantly in Mule 4. In Mule 3, Error handling is Java-based, but in Mule 4 Java exceptions are optional. And the biggest improvement is that one can catch the exception of some specific block of components (TIBCO has this feature) instead of a centralized exception block.  The exceptions can be re-propagated as well. As shown in the diagram below, we can handle the exceptions of some component(s) in  the same main flow: Exceptions can also be chosen from the dropdown list: The Studio has also been substantially improved. The following new features have been added to the Studio: The palettes have new icons. You can now save your favorite palettes. Deeper Maven integration. You can right click on a palette and can go directly to its XML configuration. The flows are collapsible. New dev tools have been added in Mule 4, one of them being Design Center which is targeted to less technical Mule users. Design Center is a web based GUI tool which can facilitate the Mule experience for many users who are less technical or who want to have a quick product review of Mule without the hassle of installing the whole Anypoint Studio in their system. Connector updates are no longer bound to Runtime updates. They are no longer stored inside Runtime. Instead, they are distributed outside which makes it easier to get the connector updates. So, the user can update the whole Runtime to get all the updates of the connectors/components, or the user can update only the specific connector/component. Thus, it provides a lot of flexibility and can decrease the problems we might otherwise experience when updating the whole Runtime. In Mule 3 the APIs are not well-defined, and are somewhat scattered. In Mule 4, however, the APIs will be defined in a structured way. In Mule 3, we have to tune the Runtime by manually defining the Threadpool, etc. However, in Mule 4, it will be dynamic and self-tuning so that the user can get the optimum performance. There will be a centralized, global location to tweak the threads, etc., instead of tweaking each flow separately. DevKit has been replaced by Mule SDK, which now allows the user to easily extend Mule, as it is completely annotation based. It's similar to DevKit, with some advanced functionalities and one key defining feature: it will not automatically generate any code contrary to DevKit. Instead, it will make the custom code (component/connectors) easily manageable and smaller in size. Apart from using SDK to extend Mule now, the user can package a flow that performs certain actions to be re-used in the form of a connector using Anypoint Studio 7. Another new feature called REST Connect has been introduced in Mule 4 as well. It can take any API specification such as RAML, Swagger, etc. and generate a Connector based on the specification. So it means that once you publish an API in Anypoint Exchange 2, it will automatically convert the API to a connector. In Anypoint Exchange 2, the API's can have dependency. To simplify the migration from 3.x to Mule 4 , the following tools are included: Application Migration Tool : This tool can migrate Mule 3 application syntax to Mule 4 application syntax. DevKit Migration Tool : This tool can migrate Mule 3 connectors to Mule 4 connectors. Here are some of the important changes that will be available in Mule 4: Message Structure: Message Structure will be changed in Mule 4. Instead of properties, there will be Attributes as shown below, Connector Configuration: The output of connectors can be stored directly in variables. Look at the Http request connector: Expression Language: DW language can be used in MEL also. Transformation Inside Connector: Basic DW transformations can be done inside Connector. Check it out in this diagram: These are a few of the many changes that will come with Mule 4. I will try to cover more in my next posts.", "date": "2017-7-25"},
{"website": "Avenuecode", "title": "Android Data Binding - Part 2", "author": ["Marcus Vinicius Corrêa Barcelos"], "link": "https://blog.avenuecode.com/android-data-binding-part-2", "abstract": "Although the DataBiding library seems very complex, it's actually very simple. To understand how the binding works, we need to dive into the BindingAdapter annotation. This annotation is responsible for connecting the view and the value to set. To create this connection you have to define a name for this binding, which will be the XML attribute. So every time that this attribute is used, the method that has this annotation declared will be called. The library looks for those attributes whose values are defined using the DataBinding tag (@{value}). This is how the attribute is interpreted as a binding one and not as a normal one. The custom attributes--the ones that don't exist--should be added on XML using the \"app\" namespace; you don't need to declare the name with that namespace, but if you do, you will get some warning saying that this tag will be ignored. Imagine the following scenario using the code below: The \" customAttribute\" is declared with \" @{myObject.text} \" as the value. In this example, the library interprets the \" customAttribute\" as a DataBiding one, because it's receiving the t ag (@{}) as the value. So the library will look and call for any annotation that has \" customAttribute\" as its name. But, in the example given in part 1 , I didn't create any BindingAdapter, so how does it work? It's really simple. The DataBinding library already has a lot of implemented adapters, such as \"android:text,\" \"android:digits,\" \"android:inputType,\" etc. We can find all the adapters by looking at the DataBinding source code . Creating a binding is pretty simple: first, create a static and public method. The first parameter of this method will be the view object that will use the binding tag. In this example, our binding tag will be \"TextView.\" The second parameter will be the object, which can be an integer, string, resource id, or any kind of object that you want. Next, simply add the annotation to the method, passing a string as the parameter; this string is the XML attribute name, which we'll call \"text\" in our example: The third and last step is the binding implementation: inside the method, you need to bind the view with the object. Let's use the examples mentioned before--we created a method with TextView as the first parameter. The second parameter will be a string, and the attribute will be \"text.\" Now, all we have to do is to set the text inside the TextView, calling the specific setter. The BindingAdapter can also declare more than one attribute. For each new attribute added, you have to add a new parameter to the method. For example, let's say you have a custom view and you want to create adapters for three properties: showButton, text, and disabled. In this case, it's possible to create only one method with three parameters for each attribute: The requireAll param defines whether or not all attributes have to be set. When requireAll is false, the non-declared parameter is set as null. To use in the view, just declare the attributes individually using the same method detailed above. For simple cases, where the binding object is the same as the setter, you can use the annotation BindingMethod. This annotation has three params: the view class, the attribute name, and the method name. This enables you to avoid a lot of boilerplate code. Look at the example below: In this example, I created a class that is responsible for declaring all the BindingMethods and BindingAdapters. As you can see, using BindingMethods is cleaner than using BindingAdapters, but only for simple cases. If you want to do any validation, you have to use the BindingAdapter.", "date": "2018-9-12"},
{"website": "Avenuecode", "title": "Message Queues: Even Microservices Want to Chit Chat", "author": ["João Moráveis"], "link": "https://blog.avenuecode.com/message-queue-even-microservices-want-to-chit-chat", "abstract": "Nowadays, building systems using microservices is almost a must, but applying and keeping microservices concepts is quite challenging. We want service providers to process all requests, even in cases of high demand, but how can we guarantee this, and how can we improve performance? The answer is message queues . So what's a message queue, and how does it work? A message queue is an architecture that provides asynchronous communication, allowing microservices to interact with each other without coupling. The communication is made by sending messages that contain information or commands that need to be processed. The sender is called the Producer. These messages are then stored (in memory or persisted) in a queue and processed by another microservice (called the Consumer). Then, once a message is processed, it is removed or dequeued, which assures that it is processed only once. Okay, this seems great, but what if we need the same message to be processed by two or more microservices? Well, there are different types of message queues: 1. A message queue can be point-to-point, where there is only one queue  and one consumer: Image courtesy of Oracle 2. Alternatively, a message queue can use a Publisher-Subscriber format, where a Publisher (Producer) sends a message to a queue (in this case called a Topic), and all the Subscribers receive a copy of the message that can be retained or not: Image courtesy of Oracle To conclude, if you are thinking of creating microservices or if you need to improve their performance and the business logic doesn't require that they be processed immediately, then message queues are what you need. They will guarantee that all received requests are processed while decoupled from other microservices, and, if needed, they allow you to easily scale for periods of high demand.", "date": "2019-10-16"},
{"website": "Avenuecode", "title": "From Digital Transformation to Digital Revolution", "author": ["Zeo Solomon"], "link": "https://blog.avenuecode.com/from-digital-transformation-to-digital-revolution", "abstract": "For the first time in history, enterprise organizations are proving it’s possible to go digital in a matter of weeks instead of years. What changed? The answer is us. When the US economy collapsed in 2008, most businesses responded by going digital. Simply, automation meant cost savings. From 2008-2013, Fortune 100 retail companies partnered with Avenue Code to pioneer e-commerce solutions that served as the model for global digital transformation. From 2014-2017, Europe and Latin America caught on by going digital, starting with e-commerce, while in the US, FinTech and others were beginning to follow suit. From 2018-2019, late adopters like automotive, manufacturing, and even waste management industries began their own versions of digital transformations, automating processes to stay competitive. Thanks to technological improvements and better leadership buy-in, digital transformations that used to take enterprise organizations 2-5 years in 2008 took only 1-2 years in 2019. Then came 2020, and overnight, the world found itself sheltered at home. Digital trends that added business value in the past became indispensable to market viability. Remote work became mandatory. Online customer service became imperative. Cloud became crucial. In retail, online sales went up 76% year over year ( Digital Commerce 360 ). In education, over 1.5 billion students were affected ( UNESCO ), resulting in a global shift toward e-learning, with some learning providers deploying over 100,000 cloud servers in just 2 hours ( World Economic Forum ). In the health sector, the Centers for Medicare and Medicaid services saw 10.1 million beneficiaries receiving telehealth services from mid-March through early July ( HHS.gov ). In short, companies were faced with a challenge - go digital or die. And the world passed the test. Now teams are going digital in 4-8 weeks and working from home, not only with comparable efficiency, but with improved efficiency. Digital transformation used to start with technology departments and slowly permeate to the entire business. Now business units are the ones demanding the digital reforms. We used to begin with low-hanging fruit like iterative processes and MVPs, and slowly build our way up stream. Now we are digitizing the entire value stream all at once. We used to have one team adapt at a time. Now entire organizations are competing for fast adaptation. We went from convincing finance that digital was a worthwhile investment to finance driving digital transformation. What changed that made accelerated digital transformation possible? The answer is us. COVID-19 has forced us to be open to change because change gives us a way forward. We know now that digital transformation is the answer, and we demand to go digital fast because we know it’s possible.", "date": "2020-9-23"},
{"website": "Avenuecode", "title": "AC Spotlight - Rafael Crema Tobara", "author": ["Andressa Lopes"], "link": "https://blog.avenuecode.com/ac-spotlight-rafael-crema-tobara", "abstract": "Rafael Crema Tobara, CIO, CTO, and Technology Director at Sapore, shares his personal career journey and discusses Sapore’s major projects and plans. Avenue Code : Tell us about your personal career path. How did you become the CTO, CIO, and Technology Director at Sapore? Rafael Tobara: I started working with IT when I was 15. I was taking a technical course in data processing at night while working at Microcamp – a Brazilian company that offers computer and language courses – and I often exchanged work for computer classes. I majored in Computer Science and then earned two MBAs, one in Business Management and another in Project Management. One of my most formative experiences was working at a US startup focused on cashback and loyalty. The experience of working with Agile models and scrum outside Brazil allowed me to take a leap in my career when I returned, because in Brazil, this business model was not common at the time. My next role was Head of Digital at Duratex, where I structured the company’s entire digital transformation. I started this project with only two other developers, and after 6 months, we had 90 people in the 17 teams that we created. After 3 years, I came to Sapore, the first genuinely Brazilian multinational corporate restaurant. I was hired to prepare the company for IPO (going public) and to structure information security teams, development teams, governance, and innovation products. At Sapore, we use artificial intelligence to help with logistics, expiration dates, and safety, as well as to prepare the company to integrate the startups that help us serve more than 1.3 million meals a day. AC: In 2017, Sapore launched Sapore Pay, an application that allows customers to order in advance, pay online, and pick up products on time or at scheduled times. Can you tell us a little more about Sapore Pay and any obstacles you faced in creating it? RT: Sapore Pay was our first online ordering and payment application that was developed to solve one of our main challenges - food waste. This tool allows us to know almost exactly which dishes are the most in-demand, and we can schedule meals without having to produce multiple menus at once. For example, we own several restaurants within the Honda Factory. We used to produce meals for more than 3,000 people a day without knowing their preferences, but now we can determine preferences and reduce food waste. After seeing the success of the app, we decided to expand and serve customers outside the factory. Sapore reinvented itself by introducing a new startup called Shipp into its ecosystem. Now we can serve not only factory employees but also schools, and we even deliver to individuals. Another interesting fact about Shipp is that in our restaurants, people can simply pay for their meals directly with the app using the QR Code. During the COVID-19 pandemic, Shipp orders tripled. AC: What challenges and opportunities is Sapore facing in terms of digital transformation during COVID-19? RT: Within Sapore, the first change was cultural. Previously, no one worked from home. Almost nobody had remote access; everyone worked onsite with desktops. In basically one week, we bought laptops so that our people could work from home, gain remote access, and migrate to virtual platforms. This extensive change was carried out by our IT team in record time. Within the restaurants, the changes were also drastic. Our self-service restaurants, for example, now have acrylic dividers, and specific employees are assigned to serve people. Our biggest challenge is changing, from one hour to the next, the way that 17 thousand people serve and receive our meals. We increased the number of virtual meetings by 200% to ensure we’re creating and following a new mindset. AC: Where technology is concerned, processes change all the time to keep up with business changes. Do you work with the Agile methodology? If so, how does this model help your team track with real-time changes? RT: Both the Labs (experimentation team) and the startups use Agile, but I also have a team working on legacy projects, and it is not yet Agile. For example, we have a new digital product under development that will save Sapore around 800 thousand reais per year. This product - a satisfaction survey that was previously administered by a third-party partner - was easily created in no more than four sprints. AC: What's next for Sapore? RT: Our first goal is to open Sapore's capital on the stock exchange, which involves changing ERP. For instance, we’re implementing SAP, among other changes. The other objective is to generate disruption in the food market - we want to be in the market for meal delivery and not just production of corporate meals. AC: What do you look for in strategic partnerships? RT: We have many partnerships, from start-up companies that we want to invest in and/or acquire to third-party companies that we keep as partners. This varies according to demand and the development of new projects. I like this mixed model of CLTs and consultants because it gives me the flexibility to assemble teams according to a project’s lifecycle stage. For example, I can focus on UX and then on development and finally QA, making my decisions faster and more scalable. AC: Thanks for your time today, Rafael! It’s been a pleasure hearing about your work, and we look forward to watching how Sapore continues to evolve and expand!", "date": "2020-6-30"},
{"website": "Avenuecode", "title": "Using Deep Convolutional Neural Networks (DCNNs) for Time Series Forecasting Using Tensorflow - Part 2", "author": ["Hossein Javedani Sadaei"], "link": "https://blog.avenuecode.com/using-deep-convolutional-neural-networks-dcnns-for-time-series-forecasting-using-tensorflow-part-2", "abstract": "In our last post , we discussed the importance of developing a strong forecasting engine to predict future energy consumption based on data from past energy consumption. We then discussed the suitability of using a Deep Convolutional Neural Network (DCNN) to form accurate predictions and examined how to use this model by converting a series of load data into images. I n today’s blog, we explain the codes we’re using for data processing. Before utilizing our DCNN, we need to convert a time series into gray-scale images by developing a class with some methods. We’ll begin this process by importing the required libraries as follows: ## import required libraries from sklearn.model_selection import train_test_split import math import pandas as pd import numpy as np import tensorflow as tf import random In this application, we will use the libraries listed above to facilitate coding. We’ll use Pandas to help process the data and NumPy to convert the data into a format that we can later input into Tensorflow. To facilitate programming, we will create a class (“MakeImage”) that has some methods. ### building a class to convert time series to images # defining variables of the class class MakeImage: def __init__(self): self._df = None self._size_width = None self._index_of_columns_2_be_batched = None Our “size_width” variable is used to specify image width. “index_of_columns_2_be_batched”   indicates which columns of our Pandas dataframe (“df”) we will use for further operations like creating batches of data. # defining setters and getters @property def df(self): return self._df @property def size_width(self): return self._size_width @property def index_of_columns_2_be_batched(self): return self._index_of_columns_2_be_batched @df.setter Our data must be saved in the first column of an excel file in a zero index sheet. In this class we have named this specific data column “load_data.” You may rename this as follows: def df(self, value): self._df = pd.read_excel(value, 0, names = [\"load_data\"]) @size_width.setter def size_width(self, value): self._size_width = value @index_of_columns_2_be_batched.setter def index_of_columns_2_be_batched(self, value): self._index_of_columns_2_be_batched = value The following function can be used to append another column (“label”) to our defined “df” (Pandas dataframe). As discussed in Part 1 of this blog series , these labels indicate whether a given instance of energy consumption has increased (1) or decreased (0) compared to the last instance: def add_label(self): df = self.df df.ix[0,'label'] = 0.0 df.ix[1:,'label'] = [1.0 if df.ix[i,'load_data'] > df.ix[i-1,'load_data'] else 0.0  for i in range(1,len(df.index))] Our next method creates chunked data from features and labels with fixed sizes that will later be used to make images. The output of this method is two NumPy arrays, namely “nparray_features” and “nparray_label” for chunked features and labels, respectively. def chunk_features_and_label(self, col_name, col_label): df = self.df image_width = self.size_width nparray_features = np.zeros(shape = (len(df.index) - image_width, image_width)) nparray_label = np.zeros(shape = (len(df.index) - image_width, 1)) for i in range(len(df.index) - image_width): nparray_features[i, :] = df.ix[i: i + image_width - 1, col_name].as_matrix().reshape(image_width) nparray_label[i, :] = df.ix[i + image_width, col_label].reshape(-1,1) return nparray_features, nparray_label The following function receives a feature set NumPy and readies it for image conversion: def features_2_images(self, x): size_width = self.size_width x_image = np.zeros(shape = (x.shape[0], x.shape[1]*x.shape[1])) print(x.shape[0]) for i in range(x.shape[0]): x_temp = x[i, :].copy() x_copy = np.copy(x[i,:]) x_int = np.zeros(shape = (x_copy.shape), dtype = int) x_copy2 = np.copy(x[i,:]) x_copy.sort() for j in range(np.size(x_copy)): for k in range(np.size(x_copy)): if x_copy2[j] == x_copy[k]: x[i, j] = int(k) n_values = np.max(x[i,:].astype(int)) + 1 squared = np.eye(n_values)[x[i,:].astype(int)] flatten = squared.flatten() x_image[i, :] = flatten return x_image To convert labels into an appropriate format for our proposed methodology, we need to turn them into NumPy arrays with the same dimension as the number of classes (in our case, 2). The following function uses a one hot encoding technique to convert them: def convert_2_onehot_encoding(self, vector_of_one_dim): nparray_lab_onehot = np.zeros(shape = (vector_of_one_dim.shape[0], 2)) for i in range(vector_of_one_dim.shape[0]): n_values = 2 nparray_lab_onehot[i, :] = np.eye(n_values)[vector_of_one_dim[i,:].astype(int)] return nparray_lab_onehot In our next method, we simply receive “batch_size” as an input argument and calculate the number of batches based on input. def number_of_batches(self, x, batch_size): return   int(x.shape[0]/batch_size) Using the input images and their corresponding labels, the following function will randomly select a batch of combinations in each call, e.g. “x_batch” and “y_batch” with “batch_size.” def next_batch(self, x, y, batch_size): index_for_batch = self.index_of_columns_2_be_batched if len(index_for_batch) >= batch_size: selected_index = random.sample(index_for_batch, batch_size) else: selected_index = random.sample(index_for_batch, len(index_for_batch)) x_batch = [x[i] for i in selected_index] y_batch = [y[i] for i in selected_index] self.index_of_columns_2_be_batched = [i for i in index_for_batch if i not in selected_index] return x_batch, y_batch The function above concludes our set-up process. Now, we must create a class and an instance of it. Then, by calling the methods of this object, the required input data for our proposed DCNN will be prepared as follows: ### create a class and initialize an instance of it makeImage = MakeImage() ## Setter instance variables #  put the path of your time series data in your system as shown below path_2_excel_file = 'path to data/load_data.xlsx' makeImage.df = path_2_excel_file  # a string that shows path of data # set the image size (we determined for this application that 32 is a good choice) makeImage.size_width = 32 makeImage.columns_2_pad = ['load_data', 'label'] makeImage.add_label() # process load data to be proper for the model X_not_ready_yet, Y_not_ready_yet = makeImage.chunk_features_and_label('load_data', 'label') X_ready = makeImage.features_2_images(X_not_ready_yet) Y_ready = makeImage.convert_2_onehot_encoding(Y_not_ready_yet) # select 70% of data for training the model and 30% for testing X_train, X_test, y_train, y_test = train_test_split(X_ready, Y_ready, test_size = 0.3) # the batch size is selected to be 100; it is possible to adjust it based on the problem’s requirements batch_size = 100 # we need to calculate number of batches number_of_batches = makeImage.number_of_batches(X_train, batch_size) In this blog, we described how to develop some codes to facilitate the data processing component of our proposed methodology. These codes are purposefully developed so that minor changes can make them reusable for solving most time series problems. In our final blog, we’ll discuss our proposed DCNN topology and establish a computation graph, using Tensorflow to process the output data from today’s codes.", "date": "2018-5-16"},
{"website": "Avenuecode", "title": "Containers, Iterables, Iterators, and Generators", "author": ["Felipe Sena"], "link": "https://blog.avenuecode.com/containers-iterables-iterators-and-generators", "abstract": "Containers, iterables, iterators, and generators are important concepts in Python that can help you create better solutions and use less of your computer resources, such as memory. All of these are abstract base classes in Python, and in order to utilize them, we need to implement some methods in our class. Containers normally have a way to iterate over the objects that they contain, and a generator is always an iterator. So all the relationships between these concepts can be confusing. Since it might not be clear what each one does, I've included the image below to explain the relationships between them more precisely. Image courtesy of nvie . Before we start talking about all these concepts, let me explain a little bit about dunder methods. Dunder methods, or magic methods, are special methods in Python that you can use to enrich your classes. The term \"dunder\" stands for “double under” and is used because these special methods have a double underscore in their prefix and suffix. This is a language feature that allows you to define your behaviors for the classes . A very commonly used dunder method is __init__, which works like a constructor in other languages. It’s called after the instance has been created but before it is returned to the caller. Containers are data structures that hold other objects and support membership tests using the in operator via the __contains__ magic method. Tuple, list, set, and dict are examples of containers. Usually, containers provide a way to access the contained objects and to iterate over them. An interesting container example is str in Python. We use this all the time and maybe don’t even notice that it's also a container. >>> foobar = 'foobar' >>> 'foo' in foobar True >>> 'bar' in foobar True >>> 'potato' not in foobar True As you can see, string is a classic container example since it contains all its substrings, and this feature is commonly used on a daily basis. An iterable is any object that can return an iterator, and an iterator is the object used to iterate over an iterable object. It works like a lazy factory that is idle until you ask it for a value. Note that every iterator is also an iterable, but not every iterable is an iterator. Confused? Let’s look at some examples. >>> numbers = [1, 2, 3, 4] >>> type(numbers) <class 'list'> >>> it = iter(numbers) >>> type(it) <class 'list_iterator'> >>> it.__next__() # same as next(it) 1 >>> next(it) 2 List is a great example for explaining the difference between an iterable and an iterator, because list is an iterable but not an iterator. You can see this because when we call it = iter(numbers), it returns a different class called list_iterator , which is the class responsible for iterating over all elements in this list. To make this possible, the class of an object needs to implement __iter__ method, which returns an iterator that defines the method __next__ . Most of the time when we call iter(), o bjects return themselves to be iterated, but as we've seen in the example above, iteration methods can be implemented in different classes. Behind the scenes, the for statement calls iter(), which returns an iterator object that defines the method  __next__ , which accesses one element at a time. When there are no more elements, __next__ raises a StopIteration exception that tells the for loop to terminate. Generators are simpler and more elegant than iterators; they work the same way but are written differently. When you call a generator, it doesn't return a single value; instead, it returns a generator that supports the iterator protocol. So, it can be thought of as a resumable function. Anything that can be done with generators can also be done with class-based iterators. The magic word yield is responsible for this behavior; it’ll return a value when the generator’s __next__() method is called, just like a return statement. The difference is that it’ll suspend the execution. By suspend, we mean that the local state is retained, including local variables, instruction pointer, internal evaluation stack, and the state of any exception handling. >>> def fib(): ...     prev, curr = 0, 1 ...     while True: ...         yield curr ...         prev, curr = curr, prev + curr >>> f = fib() >>> f <generator object fib at 0x108433c50> >>> list(islice(f, 0, 3)) [1, 1, 2] >>> f.__next__() 3 >>> next(f) 5 This makes the function easier to write and much clearer than an approach using instance variables. So, on the next call, it’ll resume at the same point that yield returned the value. The above Fibonacci generator example will generate an infinite sequence of numbers, but what if we would like to keep it finite? How can we do that? You may think: “Let’s just throw a StopIteration exception.\" Unfortunately, this won’t work. If we throw this exception by ourselves, the for loop won’t be able to handle the error, and the exception will be thrown by the for loop as well. The correct way to do it is to use the return statement. In a generator function, the return statement indicates that the generator is done and will cause StopIteration to be raised. Here's what the solution will look like: >>> def fib(n = 7): ...     prev, curr = 0, 1 ...     curr_n = 0 ...     while True: ...         if curr_n == n or n < 1: ...             return ...         yield curr ...         curr_n += 1 ...         prev, curr = curr, prev + curr >>> for f in fib(): ...     print(f) 1 1 2 3 5 8 13 As you can see, generators and iterators are very similar. They work in pretty much the same way, but generators have three functions: send(), throw(), and close(). Let’s imagine a scenario where we need to add many people to a database, but this is inside a loop since we need to process their information before inserting it to the database. Generators could solve your problem in this situation. >>> def add_person_to_database(host='localhost', port=27017): ...     try: ...         client = MongoClient(host, port) ...         db = client.snippet ...         persons = db.persons ...         while True: ...             person_data = yield ...             persons.insert_one(person_data) ...             print('Person added to database') ...     finally: ...         client.close() ...         print('Connection closed') >>> person_generator = add_person_to_database() >>> next(person_generator) >>> fulano = { 'name': 'Fulano', 'age': 30 } >>> siclano = { 'name': 'Siclano', 'age': 30 } >>> person_generator.send(fulano) Person added to database >>> person_generator.send(siclano) Person added to database >>> person_generator.close() Connection closed This is a simple example of how to use the power of generators. The send() method will pass the value into the generator, so the yield is also responsible for receiving variables that are passed by the send method. The throw() method is used to raise an exception inside the generator, and it’s raised by the yield expression. Close() method raises a GeneratorExit exception (behind the scenes it uses the throw method to raise this exception) inside the generator to terminate the iteration. As we saw, generator is a function created using a yield statement to deliver data on demand. But there is a simpler way to create generator without the keyword yield, and it’s called generator expression. >>> square_list = [x * x for x in range(6)] >>> square_gen = (x * x for x in range(6)) >>> print(square_list) [0, 1, 4, 9, 16, 25] >>> print(square_gen) <generator object <genexpr> at 0x10358abd0> >>> next(square_gen) 0 The syntax between these is very similar, but unlike list comprehension, generator expressions don’t construct list objects. Instead, they return a generator object that we can iterate over just as we do for generator functions. The main advantage is that generator takes much less memory. These concepts can help you to create cleaner and lighter code and to understand how coroutines work in Python. As an exercise, you should start searching for for loops that can be replaced by generators. This can help you gain a better understanding of the topic and also improve your code.", "date": "2020-2-12"},
{"website": "Avenuecode", "title": "Event-Driven Architecture and Its Application in Java - Part 2", "author": ["Otavio Tarelho"], "link": "https://blog.avenuecode.com/event-driven-architecture-and-its-application-in-java-part-2", "abstract": "Last week, we discussed the core concepts of event-driven architecture (EDA). Today, I will present a concrete way to help you understand those concepts. As programmers or architects, usually the best way to learn is to code. So let's get started! First of all, we need to have Kafka installed. As I mentioned in part one of this snippet, Apache Kafka is an excellent framework to work with event streams. Since MacOS is the environment in which our example project was created, we just need to follow a few steps, using Homebrew, in order to install Kafka: brew install kafka Zookeeper-server-start /usr/local/etc/kafka/zookeeper.properties kafka-server-start /usr/local/etc/kafka/server.properties 3. If everything goes successfully, your Kafka will not have any log errors and will be attached to your terminal window. Now that we have Kafka, we need to create our topic before showing the code. In other words, we will create the place where our events will be written by producers and consumed by consumers. In another terminal window, run the following code. kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic EMAIL_TOPIC After we have Kafka up and running on our development environment, it’s time to code with our preferred IDE.  In the example below, we have the representation of the state change on the service that persists the user information from a fictional company. When a user changes any information, the service sends an event to a Kafka topic called EMAIL_TOPIC.  This event is going to be read by another service that handles the emails and its templates and then sends them to the corresponding user. This project has some dependencies to be added in your pom.xml or build.gradle file in order to work. This example uses maven - pom.xml - with the dependencies below. Since this project is just an example without many business rules that you might encounter in a real-life project, we will simulate how a producer or a microservice producer writes an event in the EVENT_TOPIC when it updates a single user. This means it has the user class with an address attribute. When changing the address, the UserService is invoked to process this request through the method updateUser. UpdateUser updates only the user address and sends an event to EMAIL_TOPIC explaining that one event in the system happened, which in this case is CHANGE_ADDRESS_CONFIRMATION. The email, contained in the event, is going to be used by the system to send the confirmation email. The messageQueue class is where the magic happens. Using Kafka dependencies, and setting up some basic configurations, we call KafkaProducer to send our event to the topic to be read by the consumer. For testing purposes, we just create a simple class with a main method that will execute our service code. In the real world, it could be a microservice endpoint that updates the user address, for example. On the consumer side, which may also be a microservice, we have an events enum with the type of event that we are expecting, as well as an event information class that will hold our upcoming event information. In order to keep our project simple, I didn't add any fictional business rules. After we set up Kafka parameters for our consumer, MessageQueueConsumer keeps waiting and processing information that is being received and parsed to one EventInformation class instance that can be used to do some action, which in this case would be sending an address change confirmation email. Also for testing purposes, we will just create a simple class with a main method that will execute our code. With Kafka and Zookeeper up and running, it's time to execute our producer and consumer. Since we never executed and published anything in our topic, our topic is going to be empty until we start our producer. Due to this fact, our consumer is set up to read from the beginning of the offset. When executing the Producer.main() and Consumer.main(), you will get the messages below. Producer.main() Consumer.main() Hopefully, after reading parts one and two of this snippets series, you can visualize the concept and execution of EDA. Even though this approach has its downsides, like difficulties in controlling flow, a high demand for DevOps, and complex debugging problems, \"EDA has been widely used in decoupled systems to help companies avoid the structural problems of monolith systems. Also, there are many event stream frameworks like Apache Kafka that help you and reduce your workload during the implementation of EDA. When choosing an architectural pattern for your project, don't forget to choose the pattern that meets your business requirements. You can find the source code for this project here .", "date": "2020-1-22"},
{"website": "Avenuecode", "title": "An Introduction to Mutation Testing", "author": ["Rodrigo Rodrigues"], "link": "https://blog.avenuecode.com/mutation-testing-an-introduction", "abstract": "\"My code works perfectly because I do unit tests.\" This is something all developers like to hear when they ask for the quality control of a software development. Although we have unit tests that allow us to prevent errors in the early stages, should we also be worried about the quality of the unit testing code? To all the developers of the world, I'm going to reveal one of the great mysteries of programming: unit tests are also source code. One of the characteristics that defines a good unit test is that it must be treated with the same professionalism as with a source code. So how do I know if I'm defining my unit tests well? Don't worry, that's what techniques like mutation testing are for. So what is mutation testing? The concept is very simple: it is a type of test that makes small modifications to our source code known as mutants. The ultimate goal is to kill mutants. Each mutant that survives is equivalent to a unit test that has not contemplated a case that can lead to an error. Mutants are created using mutation operators, which mimic common programming errors such as sign changes, zero divisions, inverted conditions, etc. The end goal is to help develop effective tests in order to help identify gaps in the set of tests created. Let's look at an example using the following code: if (a && b) { c = 1; } else { c = 0; } The conditional mutation operator will create the following mutant: if (a || b) { c = 1; } else { c = 0; } The code has been altered in such a way that it now does the opposite of what is expected. So, basically, any unit test that runs that code will fail. If it does not fail, that means we've done something wrong and we have a surviving mutant. So, how can you start? If we're developing unit tests for Java code, we can use the PIT Mutation Testing tool. This tool offers a large list of mutation operators to employ in our tests. In addition, we can run it using Maven, Ant, or command line. We just need to have a set of unit tests previously defined in our project. We can select a set of mutation operators, tell them where the sources and test project are, and the tool will generate a report with the results of the execution of the tests on the mutants. It also calculates mutation coverage, which is nothing more than a relationship between the number of knocked down mutants and survivors. There's also a plugin for SonarQube which we can use to detect the number of surviving mutants in the code. Some Advantages: 1) It brings a new kind of bug to the attention of developers. 2) It is the most powerful method for detecting hidden defects, which could be impossible to identify using conventional testing techniques. 4) It brings a good level of error detection towards unit tests. Some Disadvantages: Conclusion In short, we know the importance of performing unit tests, but we have to take care of them just as we do with the source code. In order to analyze possible mistakes made in our own tests, one of the options we have are the mutation tests. There are tools that implement these types of tests that are free and easy to use.", "date": "2017-8-29"},
{"website": "Avenuecode", "title": "The Retail Ops Weekly #2", "author": ["Ivan Dwyer"], "link": "https://blog.avenuecode.com/retail-ops-weekly-2", "abstract": "Welcome to the second edition of the Retail Ops Weekly. August is here, which means it’s time to start thinking about back to school sales and summer's end... or summer's beginning as we call it in San Francisco. I've pulled together another set of relevant thought pieces, news stories, and tips & tricks from the past week. Enjoy! Every retailer has an omnichannel strategy in place, but there’s still a lot to learn about how to manage its effectiveness. Jason Ankemy does a fantastic job breaking it down with some practical advice for how to make it work. Why retailers still struggle with omnichannel—and how they can conquer the challenge [RetailDive] The rapid growth of connected devices has affected every industry. Shaun Kirby shares how monitoring and edge processing can maximize efficiency from the supply chain to the checkout line. (Gated article) Can the Internet of Things Finally Deliver on the Promise of Frictionless Retail? [RetailingToday] Hiring managers have a number of ways to filter candidates and narrow searches, but Michael Wolfe explains why it's more about culture fit than specific past experience. Don’t “Hire By Keyword” [Startup Answers] Jason Rushforth explains how the technology behind an omnichannel strategy is about Automation, Execution, and Analysis. Beyond Traditional Tools: How Technology Empowers Retailers to Create an Omnichannel Customer Experience [Apparel] Technology has enabled more effective distribution for many retailers, but it doesn’t stop there. Brian Gracely takes things further. When Physical Goods Become APIs and Services [PointB + Beyond] Pop-ups have had a bad name since the early days of the Internet, yet they are still prevalent across many eCommerce sites. Justinmind performed a study digging into what works and what doesn't. Pop-Ups Vs. Usability, Conversions And Bounce Rates [UsabilityGeek] With mobile as such an important channel for any retailer, it's crucial that the experience is clear and friendly. Jason Miller shows it's more about performance than anything. M-commerce is better, but not perfect, with responsive design [Internet Retailer]", "date": "2016-8-24"},
{"website": "Avenuecode", "title": "AC Spotlight - Alexis Smirnov", "author": ["Holly Camponez"], "link": "https://blog.avenuecode.com/ac-spotlight-alexis-smirnov", "abstract": "Alexis Smirnov , Co-Founder and CTO at Dialogue, discusses healthcare evolution at one of the world’s largest virtual care providers. Avenue Code: Tell us about your personal career path. How did you get to where you are today? Alexis Smirnov: Throughout my career, I have had several opportunities to build the first iterations of transformative technologies. Creating something out of nothing has shaped my perspective on what’s possible. I started coding at the age of 14, and, while still a teenager, I had the opportunity to help write software for the Sojourner, one of the first Mars rovers. After moving from Moscow to Montreal to continue my studies, I joined Softimage, which was later acquired by Microsoft. Here I helped pioneer software for 3D graphics. After gaining internet software experience at Zero-Knowledge Systems, an internet privacy company, I then co-founded Pi Corporation. The term “cloud” didn’t exist yet, nor did the idea of storing personal information there; we created the cloud computing technology we currently use in the form of Dropbox and iCloud. I subsequently became interested in machine learning and artificial intelligence related to natural language processing, and I helped to build and run an incubator at Radialpoint. Even at already stable businesses, there are always opportunities for innovation. When I met Cherif Habib and Anna Chif, we started asking what it would take to solve access to great healthcare for Canadians. The result is Dialogue, one of the leading telehealth companies and Canada’s biggest and fastest-growing provider. AC: You describe yourself as a technologist, a designer, and an entrepreneur. How do these unique areas of expertise inform and enhance one another? AS: I'm a technologist at heart, and when you’re building highly complex software, design becomes increasingly vital. By design, I mean more than user experience - I mean how things work. I’m a strong believer in using design thinking to create systems that solve user problems. Entrepreneurship was born out of this ability to create innovative solutions. Incidentally, I’ve also come to appreciate design as a hobby in diverse areas like furniture and lighting. AC: You co-founded Dialogue telehealth services in 2016, but I’m sure the advent of COVID-19 introduced several new growth opportunities. Can you tell us about how Dialogue has adapted during this time? Did you experience any challenges in scaling up services? AS: Scaling 10x anything is hard, and scaling 10x during the pandemic is harder. We’ve been thrust into the frontline of creating a solution for a vastly increasing number of people, and when you’re faced with a calling like that, doubts fade away; you simply step up as a team to provide care. From a business perspective, when you’re providing virtual care services and your market segment suddenly goes from niche to mainstream to a preferred option within weeks, opportunities abound. For example, we started Dialogue to provide virtual care services as a corporate benefit for businesses. During COVID-19, we formed partnerships with Canada’s two largest insurance carriers to provide group insurance plans to employees with Dialogue embedded as part of their product. When you become an essential service to so many people and the quality of your work results in kudos on a daily basis, you earn the right to offer more services. We recently announced that we’re expanding Dialogue services to include an Employee Assistance Program that includes mental health, legal, and financial services. AC: Have these opportunities for new services changed or expanded your mission? AS: Yes. Originally, we were laser-focused on providing primary care, but healthcare and wellness is much broader than what is treated in a walk-in clinic. We expanded our strategy from providing a single service to becoming a platform for multiple programs that drive health and wellness outcomes. It’s a major transition, but it was the right thing for us to do because now we can improve humanity’s wellbeing by using technology to deliver excellent care. Both technology and care are integral to our mission. AC: How are you integrating technology and human healthcare services for your customers? AS: It’s easy to create and sell an app that provides some form of health service automatically without human expertise; it’s hard to provide empathetic, expert service at scale. We’ve merged technology with human-powered service, putting humanity at the forefront of our work. While we have a strong tech stack and ML capabilities to optimize the workflow and remove bias, our app fundamentally supports a conversation between two people. AC: What trends do you see in telehealth as a whole? AS: We’re at the very beginning of a global transition from physical-first healthcare to digital-first healthcare. This is a transition that other industries have seen, but healthcare has been slower to go digital because there is a vast variety of health conditions and care plans. Because it’s a multidimensional space, we’ve seen a shift to more and more specialties within healthcare. Technology is being developed to support this care, and as more consumers go digital, we have more proof that it is a safe and effective form of care. So the biggest trend is the expansion of use cases and opportunities for virtual care. AC: In what ways is healthcare digitization unique from other industries? AS: Healthcare is highly regulated, which means that data interchange scenarios are difficult. While there’s a good reason for this regulation, it does hinder the realization of some use cases and therefore the creation of some AI models. Most digital services are global by default - if they are successful, they are scaled up. But with virtual care, you have to be mindful of how you expand internationally. Adoption is another challenge. Healthcare is innately so personal and physical in nature that many people don’t initially relate to a virtual modality. Once they experience it, however, they are converts. AC: What is the key to successful strategic partnerships? AS: Alignment of vision and values is fundamental. No matter how commercially attractive a partnership is, if there’s value misalignment, it will impact the result of the partnership at the end of the day. Beyond this, I look for complementary strengths. For example, we partner with an AI research center founded by Yoshua Bengio since it has the capacity to conduct deep learning research and we have the capacity to apply it in the healthcare domain. AC: What are you personally most passionate about in your career? AS: I get excited about opportunities that make an impact at scale. I’m drawn to deep problems experienced by many people, and I’m wired to think about technology as part of the solution. AC: What are some of your biggest takeaways and learning experiences throughout your career as a technologist and entrepreneur? AS: A lot of people who are goal-oriented celebrate only when they complete their goals successfully. I’ve learned to draw motivation and excitement from the journey as opposed to the achievement of a goal. The secret is to become inspired by the process as well as the accomplishment. This way, I get to raise a glass of wine more often than once a quarter! AC: Cheers to that, Alexis! Thank you for your time today. We look forward to watching where your journey takes you next!", "date": "2020-11-4"},
{"website": "Avenuecode", "title": "Why Should I Use ASP.NET Core to Create Microservices?", "author": ["Marcus Vinicius Guedes"], "link": "https://blog.avenuecode.com/why-should-i-use-asp.net-core-to-create-microservices", "abstract": "There's a lot of focus on microservices these days--everyone is refactoring monolitic applications into platforms built on microservices architecture or creating microservices to attend to new business needs. But though virtually every business has decided to migrate toward microservices architecture, not everyone has decided how to do so. In this post, I will try to show why ASP.NET Core is a wonderful web framework for creating applications in microservices architecture. We will not discuss why microservices architecture is superior or which specific language or platform should be used since each development team should be free to choose the language or platform that best suits their needs. We'll assess the merits of ASP.NET Core using the following criteria: One of the most critical factors of microservices is that it should not be attached to a specific language or platform. Although the classic version of .NET was limited, now any code written in ASP.NET Core can run in Mac, Linux, and Windows. This is a significant development. You can put your ASP.NET Core application inside a container like Docker. This isn't a new feature, since the classic version allowed for containerization, but the new version renders a much better image much more quickly. Taking a quick look at recent benchmarks related to ASP.NET Core , you can see that its performance is only improving since it launched. As this post is being written, the latest version of ASP.NET Core is 2.1, which is much more stable than version 1.0. In the beginning of 2017, when version 1.0 was launched, many features weren't yet completed, the support from the database connection was poor, and .NET Core looked like a beta product with great potential. Since then, the framework has improved a lot: libraries like NHibernate now support ASP.NET Core, its module performance has improved greatly, and the platform is much better and more stable overall. You can read all release notes for each version here . With ASP.NET Core, developers are free to choose which IDE is the best fit for them. Using Microsoft Visual Studio isn't mandatory. If they prefer, developers can use a free IDE like VS Code to create their application in microservices architecture. ASP.NET Core makes it possible to create, compile, run, and deploy from a command line. Using a simple command like \"dotnet new webapi,\" you have a scaffold for your microservice. ASP.NET Core is completely compatible with K8s so that you can take advantage of all its features and scale your microservices easily. Best of all, ASP.NET Core is a free, open source, cross-platform framework that you can download and start within minutes. You can check it out here . If you try it, you won't regret it!", "date": "2018-8-22"},
{"website": "Avenuecode", "title": "AC Spotlight - Yannick Meneceur", "author": ["Anna Vander Wall"], "link": "https://blog.avenuecode.com/ac-spotlight-yannick-meneceur", "abstract": "Yannick Meneceur, Policy Advisor on Digital Transformation and Artificial Intelligence at the Council of Europe, discusses the challenges and opportunities for ethical digital transformation in justice systems. Avenue Code : Tell us about your educational and career journeys. How did you get from Ministere de la Justice to Policy Advisor on Digital Transformation and Artificial Intelligence? Yannick Meneceur: Thanks to my background in law and computer science, I have had a very rich career in public administration. I have been a prosecutor in French courts leading the fight against cybercrime and counterfeiting, as well as an IT project manager in the administration of justice within the ministry. My status as a magistrate also gives me the opportunity to serve on secondment for international organizations. Thus, I joined the Council of Europe as a special advisor, first on issues related to the use of digital technologies in justice, and now on more general issues related to their regulation. I should note that the Council of Europe is an international organization separate and independent from the European Union. The Council of Europe has 47 member states, including Russia and Turkey, for example, and our mandate is not economic. The Council of Europe's mission is to ensure the protection of human rights, democracy, and the rule of law in all its member states, in a spirit of political and legal integration, according to its founders. AC: How do you define e-justice in relation to ethics, security, etc.? Why do we need it? YM: The concept of e-justice refers to the use of information technology in the field of justice. It may thus refer to the dematerialisation of procedures and documents, but today, it also refers to the resolution of disputes. I prefer the term cyberjustice. It is indeed a question of understanding how justice is transformed by contact with these new tools, and that is exactly what the Cyberjustice Laboratory in Montreal is doing. A tool is never neutral; it shapes the environment in which it operates. The use of technology, such as videoconferencing, for example, changes a lot of things in the course of a trial. We therefore need to understand what these technologies can do to improve access to justice and to help individuals settle their disputes while respecting fundamental rules. In Europe, the European Convention on Human Rights provides a very precise and imperative framework for all Council of Europe member states. Access to the judge must be guaranteed at all times, and the trial must be fair. This is not, therefore, a simple ethic that should be respected by operators, even private ones, but a binding and imperative legal framework ensuring impartial justice for all. AC: How do you think the digital world is impacting human rights? YM: I must confess I'm not very optimistic. While technological innovation has been of great benefit to humankind - we live in a world that has globally reduced poverty and hunger - I fear that economic gain often takes precedence over other considerations, including human rights. This is not a specificity of the digital world, but the latest developments, including AI, have sometimes been overvalued for commercial purposes to the detriment of individuals. Human rights may seem to some an “old-fashioned” legacy of an old world. Yet these rights are the ones that give us the direction for the sustainable development of technologies for the benefit of humankind. This is the only meaning that can be given to the vows of human-centered AI development. AC: What are the key factors for leadership in digital transformation? YM: Leading the digital transformation of any organization, public or private, requires, in my view, strong common sense. The success of a digital transformation depends first of all on an understanding of the uses and purpose of the mission of the structure for which one is working. Means should not be confused with ends. I am reminded of an anecdote related to my experience as an IT project manager at the Ministry of Justice in France. The new software deployed in courts dealing with criminal cases was to replace older tools with a national database. The ergonomics of the old software, even if it corresponded to the standards of the late 1990s, corresponded very well to the needs of the users: the new one was much heavier, with more information to fill in. User resistance was therefore perceived by some as mere resistance to change, but this was not the case! From their point of view, something was being taken away because of a higher-level administrative need. I have learned from this experience that the first guarantee of the success of a digital transformation is increased attention to the end user, to whom the tool ought to bring demonstrable added value. And I may surprise you by adding that we also have to learn to give up digital if it's not the right solution to meet a need! AC: What are the unique challenges and opportunities for you as a policy advisor? YM: Performing my duties requires a great deal of humility. I have to be able to synthesize a lot of information in a very short period of time for my colleagues, highlighting major trends. But production has been so intense in recent years that I have learned to give up: it’s an illusion to imagine anyone can know everything. The other element that seems essential to me is the search for objectivity. Even if I feel that my mission includes an important dimension of raising awareness of the issues of respect for human rights, democracy, and the rule of law, my training as a jurist and magistrate leads me to try to minimize my own bias. The complexity of computer technologies may have led to the construction of commercial discourses to which public decision-makers easily adhere. However, human rights give us another viewpoint: deploying a technological solution may very well not be commercially profitable, as it may produce a significant collective and individual benefit. Why, for example, restrict access to AI-enhanced jurisprudence search engines to those who can afford them? AC: We are in the era of machine learning and AI. How mature do you think AI tools are to be used in justice? Can they make decisions? Please provide examples. YM: This is such a vast subject that it inspired me to write a book. This book will be published in French in May 2020 under the title \"l'intelligence artificielle en procès\" (Artificial Intelligence on Trial). To sum up my thoughts on the subject, I would say that a lot of energy and money is spent on problems for which we are not close to having a solution, whereas AI and machine learning could help in very concrete but less \"sexy\" areas. Yannick Meneceur’s upcoming book, image courtesy of Yannick Meneceur The idea of using AI to make court decisions is nonsense and shows a profound misunderstanding of what a court decision is. Applying the law is not just a single mathematical calculation of interest between the parties. The famous legal theorist Herbert L.A. Hart spoke of the \"open texture\" of legal language and its interpretation, and rightly so. The application of a rule of law to new facts, whether in a common law or continental system, is not simply a matter of statistics based on previous cases. The meaning of previous decisions or of the law is never perfect - it leaves gaps that give the parties the opportunity to demonstrate the originality of their case ... and the judge the ability to interpret. If the rule to be applied to settle a dispute is sufficiently mathematicizable, without any margin of appreciation, then, in my opinion, it is no longer justice but a simple administrative treatment. Leibniz, in the 17th century, was already imagining a mathematicized law. If we have not succeeded since then, there is a good reason: without a solid mathematical model describing the world, certain phenomena will continue to resist us for a long time to come. On the other hand, AI (and data science in general) could be a remarkable tool in the service of the administration of justice, and I think it is a pity that we do not invest more in this area. Equipping the courts with the correct means to function is a very sensitive issue in Europe. The Council of Europe even has a Commission dedicated to the evaluation of judicial systems - the CEPEJ - in order to compare the means granted. Very strong disparities exist between states, and a thorough examination of activity data (number of cases dealt with, number of judges, registrars) could help in understanding why these disparities exist. AC: What was your biggest “Aha!” moment in the last 6 months as it relates to strategy/management? YM: Writing my book has led me to discover many new concepts and to reaffirm many certainties. Less than a particular Eureka discovery, I would like to highlight the method of inspiring discoveries. I advise any entrepreneur, manager, or public official to devote time to questioning his knowledge: not everyone will have time to theorize and write a book, but it seems essential to me to remain open to contradiction. There is nothing worse than the illusion of knowledge when making decisions. AC: What do you do to stay updated regarding innovations in tools and technologies? YM: I admit that the mechanisms of the \"attention economy\" developed by social networks work well on me! I follow a lot of accounts of researchers, academics, and public officials, particularly Yann LeCun, Joanna Bryson, Paul Nemitz, and Jan Kleijssen, as well as French lawyers like Adrien Basdevant. As part of my research activities with the Institut des Hautes Etudes sur la Justice (IHEJ) in France, the university network also gives me relevant reading that inspires the construction of public policies. AC: What channels do you like best for receiving information, and what do you read? YM: While I do use media, I also continue to subscribe to paper journals and buy my books in \"physical\" format. Naturally, I read a lot of essays and books on AI. Among the most striking are some rather old books, such as the writings of Jacques Ellul and Günter Anders, on the criticism of the technique, which are still very relevant. To relax, I'm currently reading Ian McEwan’s \"Machines Like Me,\" which offers some extremely compelling insights into the relationship between human and AI. As you can imagine, I'm also an avid reader of science fiction, and I read Philip K. Dick's books over and over again. My favourite novel by him is certainly “Upon The Dull Earth,” published in 1954, which deals with the search for the loved one. AC: Thank you for your insights on the applicability of AI in the realm of justice, Yannick! It’s been a pleasure hearing your valuable perspective.", "date": "2020-4-29"},
{"website": "Avenuecode", "title": "Up Your API Game with GraphQL", "author": ["Leandro Rezende Pinheiro"], "link": "https://blog.avenuecode.com/up-your-api-game-with-graphql", "abstract": "It's good practice to start new IT projects by considering how to easily and quickly consume and display data, and one of the first steps is creating or choosing the right API for your business. So why not choose an API that's a universal query language and provides a modern approach to accessing your data sources? With its growing popularity, you have probably already heard about GraphQL, another great tool that Facebook developers make available to the world in addition to React. GraphQL was developed internally by Facebook in 2012 before being released to the public in 2015. Let's take a look at the Rest call image below: Image courtesy of How to GraphQL As How to GraphQL explains, \"With REST, you have to make three requests to different endpoints to fetch the required data. You’re also overfetching since the endpoints return additional information that’s not needed.\" One solution used in Rest API is query parameters. It works, but it requires complex logic and has no flexibility. Furthermore, working in this way slows the whole process of developing other data endpoints. How GraphQL Works in the Same Situation Image courtesy of How to GraphQL Much simpler, don't you think? The syntax is simple and easy to understand. Developers who are familiar with JSON will quickly adapt to GraphQL. Look at the image below to see what this query returns. GraphQL's Schema Definition Language (SDL) serves as the contract between the client and the server to define how a client can access the data. BASIC TYPES String Int Float ID Boolean OPERATION TYPES Queries: are data requests to the server based on your needs. Mutations: are used to insert, update, or delete data. Subscriptions: push data from the server to the clients who chose to listen to real-time messages from the server. RESOLVERS Resolvers provide the instructions for turning a GraphQL operation into data. Resolvers are organized into a one to one mapping to the fields in a GraphQL schema. - Apollo Docs The following recommendations are brief paraphrases of the best practices listed on the GraphQL website . Avoid creating a suite of URLs. GraphQL supports this, but having a suite of URLs makes it harder to access GraphQL tools like the IDE, and you will also have other endpoints, which goes against the main idea behind GraphQL. In a production GraphQL service, it's recommended that GZIP compression is enabled. So in client requests, we can set in the header: As with any other API, you can start versioning your GraphQL Project, but usually you have to do this when you have limited control over the data that is returned or when you have new features. GraphQL only returns what is explicitly requested, so new capabilities can be added via new types and new fields on those types without introducing a big change in the code. By default, all types in GraphQL are nullable and can go awry in a networked service backed by databases and other services. The GraphQL type system allows for some fields to return lists of values but leaves the pagination of longer lists of values up to the API designer. Ultimately, designing APIs with feature-rich pagination led to a best practice pattern called \"Connections.\" A naive GraphQL service can be very \"chatty\" or repeatedly load data from your databases. A good way to handle that is to collect multiple requests and later dispatch them in a single request. A good tool is Facebook DataLoader. Data fetching Caching logic Easier pagination Below, I will give two very brief examples of GraphQL implementations, one for the flow and the other for the source code. CONSUMING AND PROVIDING DATA WITH APOLLO/GRAPHQL IN AN ORACLE DATABASE Back-End Provider First, you need to define your endpoint: Front-End Consumer In your React application, you must connect your app component, wrapping it with the ApolloProvider tag: As with the fist scenario, we start by defining an endpoint. Front-End Without a Client The following example shows how to consume a GraphQL service without a client: The main goal of the two different scenarios above is to demonstrate how flexible, powerful, and easy to use your APIs can be with GraphQL. So don't be afraid to use GraphQL in your next project--at least make one MVP to try it out. Be brave! For references and further reading, I recommend that you check out: 1. GitHub GraphQL dataloader 2. GitHub relayjs 3. The New York Times - Now on Apollo 4. GraphQL Best Practices 5. React, Relay and GraphQL: Under the Hood of The Times Website Redesign 6. React and GraphQL at the NYTimes 7. Introduction to GraphQL", "date": "2019-8-28"},
{"website": "Avenuecode", "title": "How to Use k6 Load/Performance Testing for Web Pages", "author": ["Joseph Lawrence"], "link": "https://blog.avenuecode.com/how-to-use-k6-load/performance-testing-for-web-pages", "abstract": "Do you want your application to perform well during peak traffic times? Here's what you need to know. The two types of testing generally discussed are functional and non-functional. Functional testing is more concerned with the functions and business requirements of a product, whereas non-functional testing is more about testing connections and environments, rollbacks, etc. Load testing is a type of software testing where we study the behavior of our web pages under the expected load. We specifically determine the behavior of the system during normal and peak loads. We basically test all real-time load conditions so that an application can perform well and can serve as many users as we want without failure. The diagram below will help you understand the basic paradigm for load testing. There is no hard rocket science behind it. It is basically testing and analyzing your results on your own. Sometimes during Black Friday or other seasonal days, websites may be down because they are not able to handle the heavy traffic load. But if your team knows what the load was last year and can predict the number of users expected this year, you can plan accordingly and write your load test cases, increasing gradually, to test the behavior of your web pages. There are various open source tools available, but k6 is quite easy to configure and use in you UI codebase. k6 is a development-oriented, open source load testing tool for making load and performance testing scripts and testing web pages. While you can use k6 without a lot of studying and training, basic knowledge of Node Packaging Manager (NPM) and JavaScript are recommended. If you need help, you can always refer to the k6 official website and write your script accordingly. Below is the dependency required to install before proceeding. Please run these commands: Mac: Windows: You can download k6 installer from this link . How to Use it – Writing Basic Script After installing k6 dependency, we can go ahead and write our scripts. Create any file script.js in your code base, and let's start writing basic load script. Below is a sample we can use for testing: Line 1 – Import the k6/http library into your script.js. Line 2 – Import sleep, a function that gives a time difference after every hit. Line 4 – Start your basic function where you write your load test case. Line 5 – Include your web page link in the get function, which will be under load test. Run your k6 script using the command below: After running, you will notice something like this on your terminal. You can go to the official link to learn how to study the output of your k6 script. Now we can increase the load testing by increasing its virtual users and its running duration. The command below will increase VUs to 10 and the duration to 30 sec. Above is an advanced load testing script. Let's go through it line by line. Lines 4 – 10: When you want to increase and decrease the load according to your own requirements, you cannot do it with the k6 run command. In this case, the options function can be used to create basic or advanced stages for your test. Your target shows your number of VUs, which can be varied according to your requirements for any duration of time. Lines 13- 26: When you want to test a group of web pages together, you can use this function, and batch is used to fire all requests in parallel so that we can test a high load. Checks are used for studying your result outputs. Today I outlined a basic introduction to performance/load testing using k6.  k6 is completely developer-centric. It can be used with Influx-Db and Grafana to see graphical output. This tool is very easy to use and configure and provides a great way to check performance and load testing!", "date": "2020-5-27"},
{"website": "Avenuecode", "title": "AC Spotlight - Angela Hsu", "author": ["Anna Vander Wall"], "link": "https://blog.avenuecode.com/ac-spotlight-angela-hsu", "abstract": "(The following article is property of Avenue Code, LLC, and was originally published with permission at TotalRetail on November 7, 2018 .) Angela Hsu, Senior Vice President of Marketing and e-commerce at Lamps Plus, discusses her strategies for using marketing campaigns to generate sales through innovative and personalized digital consumer experiences. Avenue Code: Tell us about your career at Lamps Plus and how your role has evolved. Angela Hsu: I began working at Lamps Plus in 2003 when our e-commerce business was relatively new. At that time, Lamps Plus was a retail leader in the brick and mortar space but not in e-commerce, so I saw a great opportunity to grow its presence in that sector. Over the years, I’ve scouted talent based on emerging e-commerce trends and marketing opportunities to position Lamps Plus for success. My team has grown to 40 in marketing (paid search, natural search, media, paid/earned social, influencer marketing and partnerships), site strategy (site technology, A/B testing, usability, analytics and customer insight), PR and brand, web creative services (design, editorial content and video) and marketplace (Amazon, eBay and Walmart). I report to our founder and CEO. AC: Lamps Plus has consistently ranked among the top 200 retail websites by Internet Retailer magazine.  How do you maintain such an impressive online presence in the continuously and rapidly evolving world of e-commerce? AH: I run our day-to-day internet business like a startup in terms of innovation, flexibility and agility. We embrace new ideas supported by data. We are always testing and iterating new initiatives and taking calculated risks. If tactics are working, we scale. At any given point in time, we run hundreds of tests, both small and large, for both our website and our marketing strategy. This agile approach allows us to adapt to the evolving world of e-commerce. AC: How have you tried to implement innovation at Lamps Plus? AH: I think about what our customers are going to expect from online and offline shopping experiences in the future and how we can position ourselves to exceed those expectations by working with the right technology and media partners that share our vision. Because of my electrical engineering experience and my MBA studies in finance, I take a data-driven approach to website strategy, marketing and creative storytelling. Innovative initiatives frequently come from team meetings, including those covering business, website and marketing. Usually, an abnormal metric sparks conversations on why that occurred, and then we propose hypotheses to solve these problems. We always test our hypotheses before we scale, and we embrace ideas from everyone on our team. I also look at the competitive landscape to discover what other retailers are doing and which technologies they’re using to see what makes sense for our company. AC: What are your thoughts on personalization? AH: As consumers’ attention spans shorten because of mobile, it’s more important than ever to personalize. Consumers expect the same online shopping experience that they get in stores, and vice-versa. For instance, when a customer walks into a Lamps Plus store, a sales associate recommends products based on what that customer browses in the store. The browsing behavior could be by product category, style, finish or collection. We strive to replicate this experience on our website. Personalization online allows us to not only emulate the customer’s in-store experience but also to raise the customer experience to the next level. On the web, we can take advantage of artificial intelligence to make product and content recommendations based on browsing behaviors, historical purchases, device type, geographic location and referring sites. The possibilities are endless. For example, if a customer likes watching our buying guide videos or views our curated room inspiration, we want to personalize content accordingly and highlight videos or room inspiration when they return to our website. I’ve seen a significant shift in e-commerce from focusing on products to focusing on each individual shopper through a personalized experience. Personalization seems like it will become almost required in e-commerce. Lamps Plus website on mobile device. Image courtesy of Lamps Plus. AC: How can companies differentiate themselves with a mobile-first strategy? AH: “Mobile first” has been among my top 5 priorities the past few years. We started with quick mobile wins such as graphics, copy, video and marketing. This past year we made strong progress on website speed and site design. I encourage my leadership team to review all graphic, copy and video content on their phones to see how a user would experience that content on a small device. We allocate more budget to mobile marketing and use multi-touch attribution to analyze how mobile campaigns drive traffic to our stores or our desktop website and then to conversions. We consider load time and speed as part of the design process. We run A/B testing for site features and functionalities more on mobile than desktop. As a result, our mobile site is faster than the industry average. We’re also adding additional personalization features to give our shoppers what they are looking for quickly on their mobile devices. AC: Where do you find the best ROI when it comes to investing in new technologies? AH: I prefer to partner with best-in-class vendors rather than to build technology features in-house. Vendors tend to work with a variety of companies, which allows them to enhance their solutions based on knowledge and feedback from multiple clients. I also like partnering with vendors for their product development benefits so we can grow together. We are in several beta programs with our technology and media partners. AC: How do you manage to oversee so many diverse functions within your team? AH: I thrive when I’m multi-tasking. Marketing initiatives aren’t successful unless they lead to conversions on our website – marketing and e-commerce go hand-in-hand. We invest significantly to optimize marketing campaigns, but when consumers get to our site, they need to continue the shopping funnel and convert. Marketing also includes telling our brand story via visual display, editorial content, videos, PR and social media in a cohesive and authentic way. Thus, it is critical that marketing, site, creative services and PR all fall under one umbrella. AC: Where specifically do you see the opportunity to innovate? AH: I see the opportunity to target our customers based on the marketing channels they are more likely to convert from, showing these customers creative that resonates. This requires the utilization of online and offline data and predictive analytics to understand our customers’ path to purchase with refined micro-segmentation and dynamic creative optimization. I am excited about the emerging technology that allows retailers to move in this direction. There’s more to figure out for on-site personalization. Multi-channel retailers without a significant number of login users have unique challenges in scaling personalization in a cross-device and cross-channel shopping environment. Lastly, I am also excited about visual search as well as chatbots that complement live chats!", "date": "2019-7-3"},
{"website": "Avenuecode", "title": "How to Improve the Relationship Between UX and Development Teams in an Agile Context - Part 2", "author": ["Alexandre Lemos"], "link": "https://blog.avenuecode.com/how-to-improve-the-relationship-between-ux-and-development-teams-in-an-agile-context-part-2", "abstract": "In the first part of this series, we discussed the challenges of integrating UX activities into an Agile context mainly populated by engineers (or, to use Scrum's terminology, developers). In this second part, we'll discuss various approaches to accommodating UX activity inside a Scrum framework. While many have noted the importance of creating synergy among business analysts, UX designers, and developers in ensuring successful product development in an Agile context, the Scrum framework fits better with development processes than UX design processes and does not specifically prescribe a methodology for uniting the two. Thus, the way that Scrum teams deal with their business requirements as related to UX activity reflects their level of expertise, or maturity, in adopting an Agile environment. When companies with a background in Waterfall begin to transition to Agile, or even when software consulting firms work in short-term partnerships with design agencies, there can be significant gaps that lead to undesirable Waterfall processes. The lowest organizational maturity level in adopting Agile may be illustrated in a scenario where a design department or design agency plans their entire solution up front before handing their project to a software department or consulting firm. The development work may have been completed according to Scrum recommendations, but the practice is definitely not Agile if the development team is not involved from the beginning. In this scenario, it's common that product owners won't even analyze the value of each feature in order to compose a proper backlog based on business requirements. The late involvement of software engineers causes solution mismatches, which in turn cause delays in product releases as well as investment losses--the very problems Agile was created to address. To address this issue, we move to a more mature adoption of Agile practices as proposed by Agile evangelist Jeff Patton . His approach, depicted below, tries to reduce the gap between design and development by breaking the process of product conception (or, as he calls it, product \"discovery\") into sprints. In this approach, each discovery sprint is immediately followed by a development sprint. If well enacted, this \"Dual-Track Scrum\" system can greatly reduce the time gap between ideation and software increment development. On the other hand, the system relies heavily on Agile maturity and communication best practices between discovery and development teams since the two teams are working on different parts of the same software increment and can easily revert to a Waterfall mindset if they don't regard their work as wholly united. In an even more mature adoption of Agile practices, an organization might use Google's 3 or 5-day design sprint strategy , depicted below, to gather information for starting a development sprint. In this model, the output of a design sprint can be refined during the development sprint with sufficient participation from both UX and software engineers. A 3-day sprint design can be conducted by anyone in the team with communication and information-gathering skills, and it may be performed in parallel with the current development sprint. Successful design sprints inform subsequent sprint planning sessions, or any other sprint that suits the product best, and ideas that fail the sprint design test may be discarded without regret since only a couple days have been allocated for exploration rather than months or even an entire year. With these adjustments and tools, UX and development professionals can work so well as a single team that the limits of each are barely distinguishable. As this paradigm became a common practice in development, it was applied in other diverse contexts as well. For instance, Brad Frost popularized \"atomic design,\" which uses chemistry vocabulary to demonstrate how to slice UI components into their most granular levels in order to make each level reusable. The atomic design approach attempts to frame design systems with a logical creation process.  This model envisions design systems as enormous, living policies containing design guidelines that shape the brand's web appearance, prevent the need to rework UI elements, and standardize common interaction patterns through the interface, making both design and UI development jobs easier. Design systems have recently been taken to a further level of development with the Airbnb React Sketch App . The Airbnb library allows developers to access a collection of components, customize them with React, and render every input instantly in the prototype tool Sketch. This empowers UI developers to easily find, try, and suggest commonly used components, allowing UX professionals to research, create, and reshape improved interactions instead of starting component design and development from scratch every time a basic feature has to be reused in a different context. The Airbnb approach integrates the design system and code repository. This combination prevents time consuming issues, such as tune responsiveness in different devices, transition effects, alignment, and font issues in each component reuse. Impressive developments like the React Sketch app can introduce a new mindset to the relationship between UX and development teams, revolutionizing their interactions for increased speed and capability in product development. While there are still solutions to be explored to help UX and development team interactions gain efficiency, this article provides explanations and tools that can help your Agile practices mature. At Avenue Code, we're well-versed in implementing Agile to increase internal corroboration, productivity, and overall business operation success.", "date": "2018-6-20"},
{"website": "Avenuecode", "title": "Tricks for Configuring New Relic for .NET Core", "author": ["Rafael Romão"], "link": "https://blog.avenuecode.com/tricks-for-configuring-new-relic-for-.net-core", "abstract": "New Relic is a performance monitoring tool well known by the microservices community. It supports a lot of platforms, but it's support for .NET Core is still recent. In this post, I'll walk you through how to configure a .NET Core 2 Web API to send metrics to New Relic from a Docker container inside an Azure Web App for Containers. To follow this tutorial, you'll need a New Relic account and a valid New Relic license key. Y ou will also need Docker and .NET Core 2 configured and running in you development machine, a Docker Hub account, and an Azure subscription. Let's start by creating a new Web API. In this demo, I will name  our API baseline-demo . In the baseline-demo project folder, type: This command will scaffold our API. Now, let's add Swagger to this API to give us a UI to test it: Next, change your Startup.cs file to set up the Swashbuckle middleware: Run the API and test it using the Swagger UI: Now let's look at how to add Docker and New Relic. First, download the New Relic .NET Core agent. In this demo, we'll use the Debian version. Copy the file newrelic-netcore20-agent*.deb to the folder baseline-demo/newrelic . In the folder baseline -demo, create a DOCKERFILE with the following content: Now, publish your API targeting debian-x64 : And build a docker image with the published content: Now, simply run your container: Go to Swagger and make some GET requests. In a few minutes, you should see some metrics under your account in New Relic . They will appear as follows: On your Docker Hub portal, create a new repository and give it the same name as your Docker image. Then, tag the local image to match this Docker Hub repository, like this: Log in to your Docker Hub account and push the local image to the repository: Now your image will be available for use in an Azure Web App for Containers. Finish the creation and access the /swagger endpoint in the URL provided for your app and make some test requests. Now go to New Relic again to see the metrics collected from your API, which is now running on Azure. This short tutorial shows one of the many options available for configuring New Relic for a .NET Core 2 Web API. For more information, check out New Relic's documentation . Don't forget to comment below to tell us what you think about the amazing tools we used in this demo!", "date": "2018-2-21"},
{"website": "Avenuecode", "title": "Building A Simple Microservice With JHipster In 20 Minutes", "author": ["Vinicius Kairala"], "link": "https://blog.avenuecode.com/building-a-microservice-in-20-minutes-with-jhipster", "abstract": "How long did it take for you to roll out an MVP project that validated that awesome idea you just had? Or that freelance project you've committed to delivering with a tight deadline? Or even that side project you wanted to do but weren’t really sure how to kick off? With JHipster you will take no more than 20 minutes to create a simple microservice to validade your idea and solve your problems. Before, you probably had to worry about how your application would connect to the database, how to setup a database connection pool to handle many users, how to secure your application and manage the users, and how to deploy it either on an on-premise infrastructure or on a cloud environment. I can guarantee that these are just few elementary questions you will need to solve due to the inherent distributed nature of Microservice Architecture: scaling your Microservices, discovering and communicating to others, centralizing distributed logs, and monitoring all Microservices in a single dashboard. JHipster is an open-source project that can handle all those previous concerns and many others. JHipster was originally concepted as bootstrap tool to generate monolithic applications using AngularJS as frontend ( learn about the evolution of AgularJS ) and Spring Boot as backend (if you are not a Sping Boot person learn about another option to build server-side applications with Java EE ) , with its own custom interfaces in the middle of them to seamlessly integrate both technologies. The tool has a sweet pick-and-choose option to generate applications. For example, JHipster will ask you how you want to secure your new application, either using the classical HTTP Session Authentication or a token-based authentication with OAuth2 or JWT. You even have the option of using a social login with Twitter, Facebook, or Google authentication. JHipster is largely embraced by the community and its number of options offered has dramatically increased. Nowadays you can even develop your own module, which is a plugin you can integrate within any kind of framework and share on JHipster's marketplace Besides monolithic applications, JHipster can generate modern applications based on Microservice Architecture. This architecture is mainly composed of three applications: Take a look at the depicted architecture: Learn more about refactoring monoliths to a Microservice architeture in our whitepaper \"Microservices Architeture as a Large-Scale Refactoring Tool\" As this blog post title suggests, I propose we build a Microservice in the short span of 20 minutes as well as browse through some of its functionalities. Assuming that your environment already has Java8, NodeJS, and Docker installed, you should be able to create your Microservice in about 20 minutes. To give a bit of a background story, let's assume we are professional traders and we need to build a Microservice that will manage our book of transactions. There should be a simple CRUD operation with some required fields. We'll build a Microservice to handle all backend activities and a gateway to handle our Microservice entrances. But here’s the thing: I won't show you detailed step-by-step instructions on how to use the tool. Instead, I will point you to JHipster's awesome documentation that is so extensive that you won't have trouble walking through it by yourself. So, are you ready? I am. Let's go! It's time to roll up our sleeves and to check out all the cool things JHipster can do for us. For this tutorial, make sure you have both Docker and Docker Compose installed. If you’re not familiar with Docker, check out this Docker post . Firstly, create a root directory for the project: Start the JHipster Generator container to generate the applications: Our first generated application will be the Microservice. In order to do that, create a folder and run the JHipster generator command inside it: Choose the following options for a Microservice: JHipster will generate a bunch of files, including .yo-rc.json, which contains all of JHipster's metadata for the generated application. Now, it's time to generate our first entity. Let's create the Position entity by running JHipster's entity generator command with the following options: This is what you should see for the first field: Repeat the steps for the other fields. In the end, you will get the following: Note: You can safely override all conflicts caused by generated files. The next step is to generate the gateway application, so that we can perform CRUD operations over the new entity. Go back to the project root directory and create a new folder for the gateway, enter it into the subfolder, and run the JHipster generator command again: We have now generated the gateway application, but we still need to explicitly ask JHipster to create the UI of the CRUD functionalities for the Position entity we had previously created for the Microservice. Let's ask for JHipster to create it now: Run JHipster's command. In order to generate the entity, select the option to generate an entity for an existent Microservice and point it towards the Microservice path. As you can see, JHipster will do all the heavy-lifting for us: The last step we need to complete is to generate a docker compose file to handle all Microservices, gateways, and their dependent services. So, let’s take a step back to the root directory and create a docker folder before running the JHipster command: JHipster will ask some questions and here's what we should answer: Type of application: Microservice application Root directory for gateway and Microservices: ../ Applications to include: gateway, orderbook Monitoring: Prometheus Admin password to secure JHipster Registry: admin We are now done with the generation code. Let's exit the JHipster generator container and spin up all the services we've created. Go to each application, generate a docker image, and then go to docker folder to start the containers: Let's see if JHipster generated everything correctly for us. You should get all applications up and running in a few minutes. Check the logs to see if you see the following messages: Accessing http:/localhost:8080 , you can now see your gateway's welcome page: Use admin/admin to sign in and go to the Entities > Position Menu. Now, we are at the Position list page and we can create our very first entry by clicking on Create new Position. Fill in the fields and click on Save. You should see a page similar to this one: Congratulations! Now not only do you have both the gateway and Microservice up and running, but also a full end-to-end CRUD for an entity. There are so many other cool things JHipster can do for us but I’ll save that for another post. Until then, I'd love to hear from you: Have you ever used JHipster before? Do you know of any other similar solutions? Has this tutorial worked for you? You can head over to my GitHub page and check this tutorial code. See you next time!", "date": "2017-9-21"},
{"website": "Avenuecode", "title": "How Agile Can Help Organize Your Life", "author": ["Ludmila Roizenbruch Paiva"], "link": "https://blog.avenuecode.com/feeling-stressed-agile-may-help-you", "abstract": "Have you ever felt like you are so involved with your work that you are losing control of your personal life? Have you ever seen yourself delivering all your work right on time but then forget to pay a bill or buy your wife's birthday gift? Well, let me tell you: you are not alone! What you might not have considered is that, as an enthusiastic Agilist professional, the same good practices that you utilize with all your work can also be very helpful with your personal daily life. But how? Let's start with the concept that everyone has their own personal tasks backlog. That includes bills to pay, things to buy at the supermarket or drugstore, medical appointments to schedule or attend, maintenance for a house or car, meetings with friends or family, and so on. The list seems to be endless and continuously changing on a daily basis. First, try writing these tasks down on post-its. There, you've just created your backlog just like any project on JIRA or Trello (Trello is a useful tool by the way, and it's free to use)! The list can, and should, be shared with your stakeholders - usually a husband, wife, parents, or whoever shares household responsibilities with you. The next step is to plan your week. Like we say to our clients, we have a limited capacity and can not solve everything in such a short period. This weekly planning should be done according to the urgency of each task ranging from most urgent to least. Once your week is planned, try putting it on a board - that way, you have your brand new personal scrum board that you can use to help manage your personal agenda in a more systematic way. You should check it on a daily basis to ensure that everything is on track, just like you would do with your team in a standup meeting. By doing that, you can also take action, such as changing priorities or shifting tasks to the subsequent week if you notice that something is going wrong. At the end of the week, do a personal review to check what you were able to and not able to accomplish, and re-prioritize your backlog to re-start the process for next week. It's also important to note that a kanban board is also useful for this kind of task organization. If you are using kanban, that means  that instead of a fixed set of tasks to be solved in a week or two, you have a queue of tasks ordered by priority that you can deal with without the tight restriction of time. Although extremely applicable to the everyday tasks in our lives, these practices can be even more useful when leveraged to accomplish personal projects, such as planning a trip or organizing a wedding. In these cases, you have a high-level, well-defined scope with tasks that need to be distributed on time and to multiple stakeholders. Scrum is so applicable to this kind of endeavor, that there is even a website called Scrum Your Wedding ! They offer a guide and toolkit to help people use these practices while planning their wedding. It's a creative and useful way to deal with something that could potentially become relatively complex and stressful for many couples. Another very good example of utilizing Agile methodologies to improve personal and family life organization can be found in a very popular TED talk from Bruce Feiler . In this talk, he explains how he used Agile practices to improve his family life and create better quality time with his wife and children. This included sprint planning, checklists for the children's tasks, and frequent meetings to evaluate results. So, hopefully now you've been properly incentivized to incorporate Agile methodologies into your personal life. They can help with reducing daily stress through the utilization of more planning, organization, and self-reflection. Let me know how it works for you,  and best of luck!", "date": "2017-8-24"},
{"website": "Avenuecode", "title": "A Web History: The Origin of Bundlers, Part 3", "author": ["Igor Rezende"], "link": "https://blog.avenuecode.com/a-web-history-the-origin-of-bundlers-part-3", "abstract": "In our last installment, Jay Ess was able to solve his problems when he discovered bundlers! In our final installment, he examines Browserify and webpack to see how bundlers actually work. It’s a beautiful morning. The sun is shining, and Jay Ess's work is progressing smoothly. But still, he feels restless. Why? He's discovered so many bundlers that solve the problems that have haunted him for so long, but he doesn’t know how they work. After a long sip of coffee, he cries, “this is witchcraft, man! How does it work?!” He has to start somewhere, and like a chemist picking a sample for research, he decides to choose Browserify since it's a simpler tool. (Keep in mind that every bundler works similarly and produces similar results. The idea here is not to examine code in detail; the idea is to have an idea.) Suppose one of Jay Ess’s files (something called LoaderLink.js) has the following code: const XHR = require ('../ lib / xhr.js'); const DOM = require ('../ lib / domUtils.js'); function loader {container} { const output = DOM.printTo (container); XHR.get (href, content => { let {content, js} = DOM.parse (content); // ... }); } module.exports = loader; (Most of the code was removed from the example, but the interesting part on importing and exporting modules is still here.) So Jay Ess sits in his chair, installs Browserify, and runs it on his example code. He waits and stares at his screen like a dog watching her food being prepared and baptized with a little meat sauce. Actually, he doesn’t, because the process is extremely fast. The result? A shiny, bundled file that he immediately opens to investigate. Here’s the (very simplified excerpt of) code file: { 1: ..., 2:[ function (require, module, exports) { const XHR = require ('../ lib / xhr.js'); const DOM = require ('../ lib / domUtils.js'); function loader {container} { const output = DOM.printTo (container); XHR.get (href, content => { var {content, js} = DOM.parse (content); // ... }); } module.exports = loader; }, {\"../lib/domUtils.js\":4,\"../lib/fnbasics.js\":5,\"../lib/xhr.js\":6} ], 3: ... } “Holy tools, Batman, this is weird!” he cries. “So this is thrown at an object, which is passed to some embracing function, which will execute each of the modules/function expressions. But wait a minute… this transformation basically just embraces the original script code and extracts the dependencies that each module requires in that code, reading each line where there is a require and putting all the names in a collection of dependencies to later manage these modules correctly…” At this point, Jay Ess is feeling like Sherlock Holmes. Note that this code runs in an environment where Jay Ess has access to three things: a require function, a module reference, and an exports reference. That’s basically all that’s needed for his code to work properly, and it doesn’t really matter what these things are and how they work at a low level. The loading can happen in a single bundled file, but it can also happen differently by loading on demand through an http request or on a file system (asynchronously, as we don't know how long something that runs outside our environment takes to finish). Base image from Pixabay Now that Jay Ess is familiar with Browserify, his curiosity has grown. He continues his investigation by researching one of the most popular tools in this realm: webpack. Webpack is a bundler for modern JavaScript applications. When webpack processes your application, it recursively creates a dependency graph that includes all the modules your application needs and then packages them into a small number of packages -- often just one -- to be loaded by the browser. Learning of a new version of JavaScript, Jay Ess reads an article about features built into the language that can give him greater power over his code, and at the same time he starts to appreciate this lovely language, whose name, when abbreviated -- accidentally -- and definitely not on purpose -- is pronounced like his own. Jay Ess discovers that, for a dynamic language, JavaScript has a very static module system, which has its limitations: All imports/exports are only allowed at the top level in a module. There are no conditional imports or exports, and he can’t use import in the scope of any function. That's sad. Everything must be explicitly exported by name. It’s impossible to loop and export multiple names according to some list. Where is the practicality in this? Maybe he needs a house-elf to do this for him, and his name could be Adobe instead of Dobby… Module/export objects are petrified, and this scares Jay Ess because deep inside he remembers what Dumbledore once said regarding petrification: \"Dark Magic of the most advanced kind.\" There’s no way out by adding features in a module object, like a polyfill would (worth searching if you don't know what this is). All module dependencies must be loaded, parsed, and linked before any code from any module is executed. There is no syntax for an import that can be lazy loaded or loaded on demand. There’s no way to handle import errors (e.g. no importing within a try/catch block). His application can have multiple modules in it, and if something can’t be loaded or bound in the path, basically nothing will be executed and Jay Ess will drink a fountain of the purest caffeine beverage on the market all night as he tries, without success, to troubleshoot using solely the language. There’s no hook that allows a module to execute code before its dependencies are loaded. That is, modules have no control over how their dependencies are loaded The JavaScript module system is really cool as long as Jay Ess’s needs are just static, but we know of all his sufferings, and he deserves more than this. The good news? Tools like webpack allow for dynamism, such as code execution before dependencies are loaded and error detection at compile time. The module loading system Jay Ess uses will have a programmable API to walk along with this ES6 static import/export syntax. For example, webpack includes an API that he can use to \"break the code\" and load some packages of modules on demand (lazy). And this same API can help him kill most of the other limitations above!! In fact, as Jason Orendorff says, “the static module syntax of JavaScript is designed to work in conjunction with some dynamic and programmatic API of a Module Loader,” so the static nature of ES6 can actually be a good thing when combined with webpack! Beyond this, Jay Ess can use other cool webpack features like file mining, asset management, and code transformations like minification. Webpack can run transcompilers to transform TypeScript into JavaScript, for example. There are two syntaxes for writing code for importing and exporting modules: the CommonJS require(\"something\") that Node.js uses and the ESM standard that is standardized in ECMAScript (import from \"something\"). Webpack supports both, transforming imports into require. You can’t imagine how ecstatic and relaxed Jay Ess is with so much power in hand. To be even more explicit, webpack is both a bundler and a module loader. Some even call it a compiler. Webpack, not the browser, is responsible for all project assets, treating every file (.css, .html, .scss, .jpg, .etc.) as a module, even if it understands only JavaScript. So webpack’s loaders transform the files, treating them as modules and adding them to the dependency graph! Jay Ess just wanted to manage his files efficiently, and he got so much more. These additional features came from the suffering of other characters who, like Jay Ess, needed new and better solutions. What I’ve learned as a developer is that, by accepting diverse and sometimes scary environments, I can view challenges with optimism. I remember that other developers may be experiencing the same difficulties, and these difficulties create opportunities for new tools. Webpack could have been developed by Jay Ess himself, as well as by each of you, dear readers. So why not start today and pioneer a solution for whatever challenge you face?", "date": "2019-1-16"},
{"website": "Avenuecode", "title": "Docker on Pipeline - Part 1", "author": ["Alan Voiski"], "link": "https://blog.avenuecode.com/docker-on-pipeline", "abstract": "If you have ever had to start on a new team, you know the feeling of finding yourself a bit lost amidst dozens of tutorials and all the documentation required to set up your development environment. Odds are, you may have even skipped a step or two and found yourself in a mess. Even if not, there's a good chance the guide is at least somewhat outdated, or doesn't cover your exact scenario. Either way, there's bound to be some misunderstanding. Thankfully, there's an excellent solution for your problem. By leveraging containers to create an isolated and automated environment, you can simplify matters for newcomers to the project, and help to build a reliable way to deliver your applications. This post will explain: First things first. You will need complete the following: Let's keep our machines clean: Dockers create a container that will isolate the environment. If we need to delete it, we will not have any remaining orphan files. Let's run some code in Java with Gradle as an example: The command will create a docker container for you. You can do the same to run other languages and other tools. Below, I've included an example to create a Rails application from scratch using a docker container, showing that you don't need to have Ruby on your machine: To bring it down, you simply need to exit (ctrl+c). If it is in the background (by adding the command -d ), or on another terminal, you can stop a container using the docker command: To improve even further, we can move the command line to a script and simplify some basic configurations to be part of the default environment, including some customizations to prepare the environment: /path/to/your/project/Dockerfile Then, to run the file: To recap what we achieved: Practically speaking, this also eliminates the previous need for a booked tutorial that was never generic enough to cover all machines/users/problems/needs anyway, not to mention becoming quickly outdated. Having an isolated environment is a good practice because it circumvents impacts between developers' transactions that could generate false-positive behavior, or even mess with a pre-condition from a test scenario. We can use a docker to run our database, for example, and then work, even offline: Of course, it's best if we can document this dependency: We will need docker volumes to preserve the data in case you need to recreate the container. Now we just need to run this command: Once again, here's what we achieved : So, we want to automate the delivery process and have better tools in hand? That's it. Forget complex scripts and crazy cookbooks. You can simplty run the command to add your docker on any server, and you're done. But we can do even more than that. Let's say I want: (Note: this will deploy to another machine. You can skip the security if you want.) Moreover, we can trigger the pipeline when we have the code on the master. Below is an example using Jenkins with GitHub . Last time - here's what we achieved: CONCLUSION: So, the main point here is that you will be able to document your environment by code. Your team doesn't need to know how it is running, but they can check the code if needed. Moreover, it will always be updated =) The next step is to create a robust pipeline to keep your code on track. With the same provided environment, any issue will can be easily reproduced. In the n ext post on this topic, we'll talk in greater detail about how to create a reliable pipeline for continuous delivery. Until then, if you're ready to go a little deeper you can read the docker composition documentation , or watch the last docker conference !", "date": "2017-6-7"},
{"website": "Avenuecode", "title": "Is Your Agile Process Focused on Outputs or Outcomes?", "author": ["Eduardo Bruno Silva"], "link": "https://blog.avenuecode.com/agile-process-focused-on-outputs-or-outcomes", "abstract": "", "date": "2018-11-14"},
{"website": "Avenuecode", "title": "AC Spotlight - Andy Peart", "author": ["Soheil Namvar"], "link": "https://blog.avenuecode.com/ac-spotlight-andy-peart", "abstract": "Andy Peart, CMSO at Artificial Solutions, discusses the capabilities of conversational AI for enterprise companies pursuing digital evolution. Avenue Code : Tell us about your educational and career journeys. How did you get to where you are today? Andy Peart: I studied engineering and management, specializing in IT and loved it so much that I immediately started working for an IT company that was very innovative in the market at the time. I’ve always actively picked companies specializing in disruptive technologies that aren’t massive because I want to make a difference. After working for a number of private and listed companies, all focused on software and services, I ended up at Artificial Solutions, where I’ve been for the last 10 years. It’s been an amazing journey. I was fortunate enough to join just when AI was coming into its resurgence; the ability to marry innovation with business benefit is key here. AC: What are the unique challenges and opportunities for you as CMSO at Artificial Solutions? How have you leveraged those opportunities to become CMO of the Year? AP: It’s quite a balancing act, because you need to look at every area of the business and marketing’s role within it, which means close collaboration with the CEO, CTO, CFO, VP Sales, and more. It’s a team approach in which I need to be able to look at both high-level strategy and tactical details. From a marketing perspective, we’re in a fortunate position for COVID-19 because it was already part of our strategy to migrate activities to digital channels. In fact, despite all the uncertainty that’s facing the world right now, we’re already up 33% YOY on leads. AC: Who are your top competitors for conversational AI, and what is it that you are doing differently? AP: Our conversational AI platform, Teneo, allows non-specialists to quickly build sophisticated conversational systems, delivering a range of point solutions for voice, text, touch, and gesture. One of our differentiators is that we’ve built Teneo specifically to meet the needs of large enterprises. For example, Teneo supports 36 languages. An implication of this is work groups are likely to be distributed geographically, so you also need enterprise features such as workflow that allow multiple work groups to check in and out on their part of a project, with built-in automated testing, version control, and rollback. Teneo also allows for distributed hosting both on cloud and on premise. There are so many enterprise-specific issues to consider for this particular market, and nobody else in the vendor landscape has the level of enterprise capability that we do. We also offer a more sophisticated level of conversational capability. Chatbots often fail because they’re developed to follow a linear flow of conversation. Teneo is built to handle more human-like conversations, because we use a hybrid approach to building conversational systems that combines the very best of machine learning with a linguistic approach. This correlates to data benefits as well. When people speak naturally and conversationally, as they do through Teneo, they reveal a lot about what they’re thinking, including their likes, dislikes, and desires. It's massively valuable for our clients to be able to hear what their customers want so they can improve products and services. By having a small number of very powerful differentiators, we are excelling among strong competitors like Google’s Dialogflow, Amazon’s Alexa, and Microsoft’s Bot Builder and LUIS. AC: What are the key factors for leadership in the AI product industry? AP: Many of our competitors just provide developer frameworks and leave  customers to build conversational systems on their own, but actually building and understanding how to develop truly conversational AI solutions requires a lot of expertise. We utilize our in-house professional services team and our partner network to support enterprises through the implementation process for greater success. Beyond this, we focus exclusively on conversational AI, which allows us to innovate. Way back, we built a chatbot called Elbot that repeatedly won the Loebner Prize and was crowned the king of AI, but of course, the vast majority of our solutions are built for major brands looking to automate how they deliver customer service. Indeed, we were the first company to recognize the need to build an underlying platform that enterprises can use to build out a range of solutions rather than merely using individual point solutions. The next stage of the conversational AI journey is getting individual virtual assistants/chatbots to talk together so that they can hand off questions to other bots with the appropriate specializations. This is an example of how we believe we’re able to push the boundaries of innovation in our field. Image courtesy of Andy Peart. AC: What was your biggest “Aha!” moment recently as it relates to strategy/management? AP: In 2014, we were the first to develop an underlying platform that could be used by our in-house experts, partners, and clients to build out chatbots. The nature of being a disruptor in the market is doing things differently. It was a risk, but when we took it to market, it was a great success. The second aha moment came within the last few months. This pandemic is the catalyst for a lot of change, particularly in the area of conversational AI, because COVID-19 is changing how enterprises interact with their clients. Companies are going to need to find new ways of scaling and building resilience and continuity into their organizations. Conversational AI, which goes hand-in-hand with AI process automation, is a prime solution. AC: How can AI help companies recover after COVD-19? AP: The companies that excel through any crisis/recession are those that are able to transform quickly. For this particular crisis, and the widely-predicted recession to follow, companies need to look to digital transformation to survive and grow. One of the areas of digital transformation that’s fundamental is automated consumer interaction because it means we’re less reliant on call centers to provide meaningful interactions. As an end user, I would think so much more highly of any organization that could assist me and answer my questions through an intelligent conversational AI chatbot. AC: How mature is AI to be able to handle initial interactions with customers? AP: It’s very mature. Our conversational AI technology is capable of handling intricate customer care situations like authorizing mortgages. One of our US customers is using conversational AI linked with RPA to improve turnaround time from days to minutes when helping customers move funds between accounts, all with excellent authentication and security. Conversational AI is already handling very advanced issues. Many end customers may not realize, for example, that it often provides second line support to call agents. AC: What is your perspective on the ethics of using customer data for AI model training? AP: This is one of our differentiators - from an enterprise point of view, our clients own all of their data because they can host on premise. In terms of ethics, it’s still a discussion point. Consumers want to personalize their experience and are willing to give up a bit of privacy in order to enjoy the benefits of having a chatbot remember a previous conversation and assist them more capably. That said, companies still have responsibilities to their customers. For example, our clients need to comply with legal regulations like GDPR. Also, if a customer wants their data to be deleted, there must be processes and technology in place to do so. Our products come with this built-in safety feature for end users. AC: What channels do you like best for receiving information, and what do you like to read? AP: I receive most information through social media, my team, and my peers. For reading, I enjoy fun books like Harry Potter! One book I highly recommend is The Magic of Reality by Richard Dawkins - I love how science can be perceived as magic. AC: Thanks for the insight into the capabilities of conversational AI, Andy! It’s been fascinating, and we look forward to keeping up with Teneo’s continued evolution.", "date": "2020-7-21"},
{"website": "Avenuecode", "title": "How Business and Data Science Collide", "author": ["Jimmy Mayal"], "link": "https://blog.avenuecode.com/how-business-and-data-science-collide", "abstract": "So, you’ve probably heard about Data Science and now you can’t stop thinking about how it can help you improve sales, right? Well, let’s delve into this tech topic from a business perspective. The Difficulty Not long ago, companies used to scramble over various spreadsheets trying to make sense of immense volumes of sales data in an effort to plan for the future. They essentially made their decisions based on last year’s financial results, past marketing actions, or, in the worst case scenarios, their market competitor’s actions. The difficulty lay in having to create a plan or make a prediction based on one-dimensional data from a very small slice of the market. Where Are You, Data? Companies began to pursue methods to achieve better sales forecast accuracy, and began to invest in technology used to analyze data as well as reduce cost and human effort. This prompted companies to begin storing all the data generated, such as transactions, purchase orders, sales, etc. After enough data was stored, companies believed they could finally attempt to make predictions and plan sales. However, things were not as easy as they seemed to be! Even with the addition of technology, companies soon realized there was still a missing piece. So, what was it? Essentially, the data that they were collecting had two main problems: it was extremely polluted, and the volume was unfeasible for the technology of that time to process. Simply having data was not enough. Science Over Data The genesis of data science occurred in 1962 when mathematician John W. Tukey forecasted the consequences of modern-day electronic computing on data analysis. In the 80’s, IBM and Apple released the first personal computers, causing computing to evolve at a much faster pace and giving businesses the ability to collect data with less effort. In 2000, numerous academic journals began identifying data science as an evolving discipline, and in 2005, the National Science Board advocated for data science as a career path in order to ensure that there would be specialists who could effectively accomplish digital data analyzing. When companies began looking beyond their frontiers and started collecting external data such as social media, alternative payment systems, and client support, the volume of data exploded into an astronomic amount that was impossible for any computer to analyze without an initial cleaning process to organize the data. Therefore, almost 80% of the time spent on a forecasting project has historically been spent on data preprocessing - required in order to determine which infrastructure should be used for big data. As a matter of fact, the definition of big data is data sets that are so large or complex that traditional methods of data processing are inadequate. Data Science: The Real Deal Now that you know a bit about data science, let’s go back to the question of how data science can help you improve sales forecasting. Here are some of the many ways: Case Studies H&M ( $20.3 billion yearly sales): H&M has a clear goal for their product and utilizes good demand forecasting to stick to their bottom line Zara ($14.4 billion yearly): Zara’s achievement follows the theory that if a retailer can predict demand precisely, it can implement mass production which leads to well-managed inventories, higher profitability, and more profitability for shareholders. American Express: American Express started looking for ways to predict customer loyalty by analyzing internal and external data. The company is now able to identify 24% of the accounts that will close within 4 months. A Promising Future Although Data Science is not a brand new term, its application has become more and more popular due to technological advances that resulted in the explosion of data available on the web. We can consider data science as a “work in progress” that has been taken the business world by storm. When executed properly, data science has the ability to return infinite benefits to your company.", "date": "2017-9-12"},
{"website": "Avenuecode", "title": "Getting the Most out of Your Retrospective", "author": ["Pedro Colen"], "link": "https://blog.avenuecode.com/getting-the-most-out-of-your-retrospective", "abstract": "The retrospective meeting is a crucial part of the Scrum framework. Its goal is to ensure that the process used is reviewed after every sprint and the team never stops improving. But how can you ensure your retrospective that achieves this goal? Last week, Ramon Alves published a great article on why skipping the retro is damaging to a team's ability to work effectively. Which got me thinking: what are some ways to make the most of your retrospective and ensure that it's productive for the entire team? And further, what does a great retrospective look like? According to the Scrum Guide , the Sprint Retrospective is a 3 hour time-boxed meeting for a one month Sprint. The Guide even says that the whole Scrum Team should participate in this meeting and that the process of the sprint should be reviewed. What the Scrum Guide doesn't say is how to do a retrospective . Because if the whole team just sits for 3 hours in the same room and stares at each other, the process will not improve, right? And if we spend 3 hours pointing out the mistakes that each person made we will probably get nowhere. So what should we do? Some people go to the retrospective meeting feeling very nervous. Maybe they are expecting people to talk about them. Or maybe they've been waiting for the chance to talk about someone else's mistakes. Not a great idea! Other people are just too shy to talk, so they will let the others talk for them. These kinds of situations could break down the entire retrospective and make the meeting worthless. Therefore a good idea is to start the meeting with some game or activity to break the ice. You could use the help of a website such as Tasty Cupcakes to plan something cool. Remember! Make people feel comfortable joining in the discussion, and remind them that they are part of a team that wants to improve. After everyone is feeling comfortable enough to talk and knows what they are going to talk about, it's time to begin focusing on the process. At this point, it's a good idea to facilitate the discussion with the help of a few key questions. Feel free to ask the questions in several different ways to get a deeper understanding. All you need to do is figure out what is the best way to ask these questions of your team. During the retrospective, the team should avoid speaking directly to or about specific members. This kind of situation will only serve to damage the team's cohesiveness and cause it break down into factions. Remember that if one person fails, the whole team fails! Mistakes belong to the entire team, and the team should focus on overcoming them together. Talk about the general process and use we/us pronouns to talk about what needs to improve. This will help circumvent tendencies to attack or blame individuals. Finding and understand a problem is 50% of solving the problem. But remember that the other 50% is acting! After you define what needs to improve, remember to discuss concrete actions that the team can take to improve. Use verbs to describe those actions and be sure to pinpoint action items that are reasonable for the team to accomplish within the next sprint. For example: If the team needs to improve its communication, the action item could be sitting closer to each other or using a new communication tool with better user experience. On teams where people are comfortable speaking their minds, many ideas for improvement are typically raised during the retrospective. Since it's impossible to work on everything at the same time, many teams choose to vote on the items that are the highest priority and commit to working on them in the next sprint. Dot voting is a common method that works well due to its simplicity - let's say each team member has three votes. The items for consideration can each be written on a post-it, and team members vote to prioritize particular items by marking the post-it with a dot. Then, items are ranked by the total number of dots, and the highest ranking items are prioritized first. But don't let the other points fall to the wayside! They are still important and worth consideration. Instead, keep them in your \"backlog of improvements\" and revisit them once the team has bandwidth for more commitments. After completing a retrospective and selecting the actions items for the sprint, you can begin the next work cycle. Inevitably, this will be followed by another retrospective. On the next retro always remember to talk about the last discussion. Come back to the points you committed to improving and the action items you created and talk about how much improvement you saw since the last cycle. Is it time to create new action items? Or do the older items still need work? Retrospectives are one of the most powerful tools of Scrum. As Agile practitioners we can never settle - there is always room to improve, and the retro is the perfect time to pinpoint how. So plan your retrospective carefully and make the most of this critically important meeting.", "date": "2016-10-26"},
{"website": "Avenuecode", "title": "How a Gangster Saved My Day!", "author": ["Arlindo Neto"], "link": "https://blog.avenuecode.com/how-a-gangster-saved-my-day", "abstract": "When a code change is required, the main consideration is how to reduce the impact of the new implementation on the existing code, thus reducing the possibility of introducing new bugs and instead keeping or improving components' high cohesion/low coupling relationship. That's when design patterns come to the rescue, offering proven solutions for almost all recurrent development needs. To get everyone on the same page, let's first talk about the problem. Over the last few months, I've been working on a project that is pretty much backed by an aggregator/orchestrator for a bunch of data sources. The main idea is to provide users with assembled records from these sources and to allow them to iterate and take actions like excluding or confirming the requests they represent. The main problem we currently face with it is that, since we must rely on other services, every time a contract for their output changes (the objects and attribute types they provide as responses), we need to follow up with our integration clients. But, that's not all! Deploying those changes requires time and planning, so we need to be on the same page so we can publish all of the changes at the same time. This requires a feature toggle strategy, allowing us to have all of the code available and in place for both versions of the contract, at the same time, and to switch back and forth between them when required. For languages without type constraints, like Javascript or Python, for instance, this kind of solution is not exactly a big challenge. But for Java, which is the language we're using in our  project and the one which I believe is the best for any scalable application, things become more nebulous when two different data types must work together with the same data source. When I initially took a look at the story requirements, my first thought was to have two versions of the contract and use the required logic for the feature toggle to select between them when sending data back to the client application. This, however, would work only for unsafe languages. Otherwise, it would create lots of duplicate code across the codebase, and the post-code refactoring would require an excessive amount of work. Of course, besides this un-DRY (don't repeat yourself) line of thinking, lots of tests on both ends would fail because of the contract change, which would require more and more working time when coding and refactoring. The reason for this was that the contract about to be changed was the base for the one the aggregator was exposing to the client application. Let's try to make this more intelligible with a small diagram: As we can see in the above sequence diagram, we have three contracts for all of the data that matters. In our current case, we are talking about the C2 contract returned by Service 1 . It's easy to see now that a change to C2 would be reflected in the next layer, breaking everything. So, I decided to take two days off from coding to find a better solution. If we don't want to broadcast a change to other parts of the application, we should decouple its components as much as we're able to. One way to do this is to use a GoF (Gang of Four) Structural Pattern. In this case, I found that using a Facade pattern \"gangster\" would be the best approach. The objective of this pattern is to abstract the use of more than one class to its clients. That is, the class that is using the Facade does not need to know how to interact with others on a low level, so we use an intermediate component to handle any underlying complexity with an easier-to-use interface. The solution now is easy to understand. What I needed to do was to create a new version of the C2 contract, but abstracted by the Facade object. That way I could choose from two service calls, with the respective return value based on the contract version, and then translate the new contract to the old one. Changing the client object to call the Facade instead of the gateway object (which provides the service calling methods) meant that the impact was now minimal and easy to refactor when the contract became fixed. The experience obtained with this solution is about more than just understanding a well-established design pattern; it's also about understanding that a poor solution requires more time, or at least the same amount of time, to implement, leaving behind a footprint to be cleaned up after it. So, if you don't understand a problem deeply enough to provide the best possible solution, work first on the problem and not on the code. Time is always the scarcest resource we have, so use it wisely and look around for information that can help you produce a solution, even if it comes from a \"gangster.\" Happy coding!", "date": "2019-6-19"},
{"website": "Avenuecode", "title": "The New Hiring Mindset is Development", "author": ["Laís Ferrarezi"], "link": "https://blog.avenuecode.com/the-new-hiring-mindset-is-development", "abstract": "Hiring is becoming more and more challenging for most organizations, especially those that work in a competitive market and have aggressive hiring goals. It’s time to start being creative and thinking outside the box. The recruitment model has always been finding the perfect match for the role, a Rock Star. But we all know it's unlikely that we'll find someone in a short period of time who meets 100% of the requirements. This is especially true when we talk about hiring technical professionals, where we face a double challenge: finding a professional who matches our hiring profile while beating out marketplace competition. The good news is that the market is changing. We're seeing that companies are hiring based more on behavioral indicators, soft skills, and cultural fit than on technical knowledge, and I’m not only talking about small organizations - this is also a trend I see in some of our big clients. When I first started at Avenue Code, we didn’t have any kind of behavioral assessment built into our hiring process, and having experienced both methods, I can guarantee that the behavioral learning curve is much longer than the technical learning curve. So that made me wonder: what if we have a candidate who matches our DNA but who needs some kind of technical preparation before joining our complex projects - should we wait for the candidate to be ready to join us? My answer was no. We should hire with talent as a consideration, but we also need to do a better job training our talent . ACP2P (Avenue Code Practice to Perfection) is a recently-launched program at Avenue Code that was created to innovate the hiring process and open the door to innovation, creativity, and opportunities. This program is based on the concept of hiring potential: we want to invite professionals who share our DNA to join our team, and we'll take them to the next level of technical expertise and prepare them to join our high-level client projects in a short period of time! To accomplish this goal, we count on our highly skilled Senior Consultants to support and mentor our ACP2P participants. They prepare our new consultants to reach a high level of knowledge and meet our clients' standards. Program participants are exposed to the complex problems and real-life situations that occur on client sites, giving them the opportunity to accelerate their development and level up their technical skills. They definitely need a lot of dedication, energy, and coffee! From a hiring and client satisfaction standpoint, this is the perfect match: our incredible Senior Consultants mentoring new talent with a strong Avenue Code DNA and an enthusiasm for learning new skills!", "date": "2021-4-7"},
{"website": "Avenuecode", "title": "It's Time to Redefine Project Management", "author": ["William Hom"], "link": "https://blog.avenuecode.com/its-time-to-redefine-project-management", "abstract": "I’ve always wondered what it would be like if companies suddenly stopped hiring project managers. Would more projects fail? Would there be more chaos and confusion? Would developers or development managers make better project managers or be able/willing to fulfill that role? Are project management roles transforming into scrum master roles or a hybrid of both project manager and scrum master? Are projects completed more quickly with project management involvement? Today, let’s discuss whether companies still need project managers, and, if so, what this role should look like. There are a lot of questions and debate over the future of PMO and many opinions regarding what the PMO should look like. Some believe that having a centralized PMO is a necessity in an organization that needs enforcement of compliance and process standardization. Others believe that these roles should focus on results and delivery, the priority being to get the work done with minimal energy spent on processes and compliance. Based on my experience, I suspect the answer may lie somewhere between these opinions. More importantly, the PMO role depends on the culture and politics of the organization. Just recently, I heard of a large e-commerce retailer in San Francisco that just did away with the PMO organization. I was saddened to hear the news about the cuts because I’ve worked with so many good people in that organization. Having seen a trend in that company’s push towards getting things done and minimizing bureaucracy, however, I was not surprised by the sudden cuts, but I was intrigued by how leadership came to determine that it might be good to decentralize the PMO and the benefits it brings. I believe that a PMO organization that does not re-define itself will give a diminishing return to the company and will end up with a perception problem, trying to justify its relevance within the organization. When a company has a PMO organization staffed with program/project managers, the question is about their value proposition. What do program/project managers do to provide value to the organization? Let’s first look at the traditional definition of project management. According to Wikipedia , \"project management is the practice of initiating, planning, executing, controlling, and closing the work of a team to achieve specific goals and meet specific success criteria at the specified time.\" While this definition holds true to the project management role and responsibilities, it may no longer be a relevant way to deliver projects quickly and iteratively. Traditionally, project managers oversee project progress and deliverables, call out issues and risks, provide status reports to senior leadership, and manage the project through its lifecycle. They may also manage budgets and be the “gatekeeper” to ensure that each phase of the project meets its objectives. This is WHAT project managers do, but HOW they do the work is often where the value of a project manager is realized. As organizations are breaking down and flattening hierarchy structures, team-based decision making and ownership may be becoming the new norm. Perhaps combining the role of a traditional project manager and scrum master will provide the PMO more latitude and direct engagement with the project team, all while ensuring adherence to any compliance and process standards that are required by the organization. The role should be more about being an advocate and supporter for the team and involve only minimal effort in enforcing standards and processes that are mandated by law. (For example, SOX [Sarbanes-Oxley] compliance is imposed by the government for corporate governance and accountability, and audits are conducted annually, so it is in the best interest of companies to comply. Anything to do with funding or sharing any customer information also needs to be managed carefully since it is tied to the PCI/SOX compliance governance.) As I mentioned earlier, as organizations are heading toward a more iterative and shared responsibility by the project team, the PMO will have to reinvent itself to stay relevant. Whether the PMO is the repository for processes, standards, or compliance, the perception is that project managers may be the cause of slowing things down (intentionally or not), or worse, of being a large overhead cost to the company’s development budget. Image courtesy of KeepInspiring.Me Looking at PMOs as change agents rather than managers overseeing gating checkpoints, reporting statuses, or managing budgets means a radical shift in the type of role hired to fulfill a client's needs. The new ideal for a project manager is someone  who has many years of project management/scrum master experience and has successfully delivered initiatives in an Agile environment while still navigating through processes and compliance requirements. Perhaps the new PMO mission statement would be to reassess and streamline processes, reduce or eliminate documentation, limit the length and number of meetings, and do away with constant status updates, instead focusing on making things easier and more efficient for the development team without jeopardizing any SOX compliance or development quality. Below, I've provided examples of deliverables that project managers are responsible for and proposed ways that Agile practices help us substitute, consolidate, or eliminate documentation for them. If these deliverables are required, minimum effort should be made to create the documentation. Project Business Case Kick-off document that explains why the project is taking place, including the desired goals, objectives, and outcomes. Not Needed. The kick-off document may not be relevant in an Agile environment, as goals, objectives, and outcomes change during project development. The fluid nature and flexibility of an Agile environment achieves the objectives and goals without needing to state this explicitly. For example, testing a hypothesis and making changes along the way achieves a better outcome than committing to an objective that may fail after the project delivers. Project Charter The document lays out the high-level scope of work to be completed; the requirements, timeline, investment (resources and budget) required; the definition of done; and project success factors. Not Needed. In an Agile environment, a project charter is never mentioned or created. As high-level scope of work of MVP (Minimum Viable Product) has been defined, and stories are prioritized and groomed for development work, feeding the work to a static project team with a fixed budget will negate the need for a project charter. If a project charter is required, a short, 1-pager should suffice. RACI Matrix The RACI Matrix is a great way to define and assign responsibilities. The Matrix charts who is Responsible, who is Accountable, who is Consulted, and who should be Informed. Mapping this out helps to reduce confusion, distribute workload, and increase efficiency. Optional. A RACI matrix in a scrum environment does not need to be overly complicated or detailed. The need to identify stakeholders as accountable, responsible, consulted, and informed may depend on whether the scrum team feels it is useful and helpful. In my experience as a scrum master, a RACI Matrix is never brought up unless there is a specific need for it. WBS (Work Breakdown Structure) A work breakdown structure is the core of project planning, resource management, and avoiding project scope creep. Not Needed. WBS is a great tool for tracking waterfall projects; it is used to identify dependencies and reporting on the % of work done/remaining. Because of the iterative development in a scrum environment, percentage completion is not the measurement since the project team is committed to completing 100% of their work within the iteration. Risk/Issue Log The Risk/Issue log is a log of all risks and issues the project may face. It is good practice to follow some sort of logging format; name or ID, description, impact, probability, proposed mitigation, and owner or person accountable. Needed. Tracking and calling out risks/issues is important.  As a rule of thumb, the 3 top risks/issues should be identified, and the mitigation and owner/person accountable should be included and called out to senior leadership who can help prevent the risk turning into an issue. This is an important deliverable in any project methodology. During the retrospectives, this should be called out. Change Request Management Keep track of any formal additions or alterations to the agreed upon deliverables from any of the other project documents. Not Needed. Scrum would do away with change management since scope changes are managed in an iterative, 2-week cycle. Since the MVP has already been defined and stories have been created that make up the MVP, change requests are not needed. Project Schedule The project schedule determines which work needs to be done when. It gives the time frame for completing tasks and starting others. All work associated with a project should be scheduled. In some cases, it is a good idea to document the planned schedule and the current schedule so that late tasks can be flagged and properly addressed. Not Needed. Project schedule is dictated by the number of iteration cycles needed to complete the MVP.  So a project schedule is no longer needed in a scrum environment. Lessons Learned Recording findings at different intervals of a project will produce better quality and more factual insights. The format and detail of this document will depend on the project governance and project management culture of the organization. The entire project team should contribute and agree on the lessons learned. There should be clear takeaways, and the final step is to share it with the wider team. Needed. After every showcase, a lessons learned, better known as retrospectives, provides a timely feedback of what was done well, what can be done better, and which impediments remain. This format of calling out these points after the showcases is highly interactive between the team members and the stakeholders. Blockers are discussed in this forum and escalated accordingly and appropriately. Project Communication Plan This will ensure effective communications amongst the project team and stakeholders. Sometimes it’s a simple list of dates with updates or meetings that may be needed. Other, more complex projects require email and document policies; status meetings and automated status reporting; and work, status, risks and issues. Not Needed. Project Communication Plans should no longer be necessary, as communication to the project sponsor in an scrum/Agile environment would be frequent and interactive. No exchanges of emails, documentation of policies, or status meetings are needed if weekly showcases are held by the project team and open to any sponsors or stakeholders. As shown above, I’ve listed the various deliverables that project managers are responsible for. There is no right or wrong answer as to whether these deliverables are needed, but eliminating them allows PMOs to re-invent themselves as enablers and drivers who can streamline processes. By making it easier for other people to do their work, PMOs add value to organizations since they become the change agents for the rest of the organization.", "date": "2019-4-24"},
{"website": "Avenuecode", "title": "How to Run Integration Tests Using Docker Compose and .NET 5", "author": ["Alvaro Kramer"], "link": "https://blog.avenuecode.com/how-to-run-integration-tests-using-docker-compose-and-.net-5", "abstract": "Writing automated tests is an essential part of the development process, but it's not unusual to find codebases with little or no test coverage. Compared to unit tests, integration tests bring even more challenges since they get the same results running in different OSs (different developer machines and CI pipelines), but they are necessary in evaluating the effectiveness of different software modules when they are interconnected. In this article, we are considering integration tests as a specialized type of automated test where software functionalities are combined and tested as a group. Integration tests (also called service tests) are at the middle of the test pyramid, and their purpose is to verify that separately developed modules work together properly. Unlike unit tests, integration tests can depend on databases and external APIs. The .NET 5 framework makes some great possibilities available for integration tests. One of these is the EF Core In-Memory, which is a very good option and fits in different scenarios, but it doesn't exactly replicate a database behavior. So if you're searching for an approach to use with different ORMs ( Entity Framework , NHibernate , Dapper , etc.) or an approach that will bring you other possibilities (like running an environment as code), we think docker-compose with integration tests will be a very useful approach for you. We chose a TimeSheet application as an example for this article. In this application, the employee of a fictional company will submit hours he/she worked throughout the week. The API layers are separated by folders, and there are only two .csproj files, one for the API, and the other for the tests. The code is hosted on GitHub. The docker-compose starts three containers: timesheets-api , sql-server-database , and integration-tests . The first and third ones are the same image ( dotnet/sdk:5.0 ), timesheets-api to run the API inside the container, and integration-tests to set up and run tests. Both services have a configuration map to host volumes to be used inside the containers. This setting allows the container to use the source code implemented on the host machine. The timesheets-api container is started with a dotnet run . The integration-tests has a more complicated command. The script wait-for-it.sh keeps checking if the provided port of the service (sql-server-database:1433) is available. Once it gets the right response, the dotnet test executes the tests. The sql-server-database , as the name says, works as the containerized database to run the API pointing to the database (environment as code), or as a test database. The image is an SQL Server ( mssql/server ). Also, we need to configure the database service in the compose file to expose the 1433 port and bind it in the host's same port. The database connection string is created on the appsettings.json : As you can see, our application will point to a localhost database when running. It's important to override the connection string to work on containers network , which was done in the environment step of integration-tests and timesheets-api . We chose an approach that uses only one appsettings.json and many overrides. This approach helps developers to avoid spreading appsettings.json files along with the code (which we consider a code smell), and even if the developer chooses to use different .env files in the docker-compose environment step, these will be centralized in the same place in the folder's hierarchy. To run the project locally (if you want to debug in your favorite IDE, for example), you only need to use the SQL Server: docker-compose up -d sql-server-database . Otherwise, if you want to use an environment as code approach, run in your bash: docker-compose up -d timesheets-api ; that command will initialize the database (SqlServer) and the API on docker. Finally, if you want to run the integration tests, run this command: docker-compose up integration-tests . That command will run the .NET 5.0 and SQL Server containers and then execute the tests. We use the Github Actions to run the tests when any PR or modification at the master branch is done. To set up the workflow, create a new file in the .github/workflows named integration-tests.yml This action starts the integration-tests container with all dependencies. The argument --exit-code-from uses the exit code of the selected service container as the result of the action. When the file is committed to the GitHub repository, the action will be available to be triggered on every commit to the master branch or when a PR to the master branch is created. To see the workflow results, click Actions . It is also possible to expand the logs to see the details. If a test fails, we can take a look at the results to troubleshoot. Running integration tests with docker-compose is a very helpful option to replace in-memory databases. The article showed how to quickly and easily configure a production-ready integration test approach and demonstrated how to run integration tests using the new .NET 5 containers with docker-compose and github actions for a Continuous Integration. It's helpful to test the application from a request to a database persistence, and this approach can also be applied using technologies other than .NET and github actions . Check out more about integration tests in ASP.NET Core here , and tell us how this approach works for you in the comments below! *This post was co-written by Luiz Lelis , Rafael Miranda , and Stefano Bretas. IntegrationTest by Martin Fowler Engenharia de Software Moderna", "date": "2021-2-3"},
{"website": "Avenuecode", "title": "Mule Message Encryption with JCE Keystore - Part 2", "author": ["Anupam Gogoi"], "link": "https://blog.avenuecode.com/mule-message-encryption-with-jce-keystore", "abstract": "This is the second part of my previous post on Encryption using Mule. In the previous post in the JCE Encrypter configuration, I did not use Keystore . This week, I will create a Keystore and use it for encryption. First we will create a Keystore using Keytool that comes with JDK . After that we will create a simple project and use the generated keystore for message encryption. JDK comes with its inbuilt tool Keytool to create keystores . First, let’s create a keystore using the native Keytool . Let’s create a temporary directory. I have created a temporary directory named encryption as shown below: Now let’s execute the following command: keytool -genseckey -keystore aes-ackeystore.jck -storetype jceks -storepass acstorepass -keyalg AES -keysize 128 -alias ackey -keypass ackeypass After execution of the command a keystore aes-ackeystore.jck is generated in the directory. Basic things to point out in the command are as follows: Name of the keystore : aes-ackeystore.jck Password of the keystore : acstorepass Name of the key : ackey Password of the key : ackeypass These elements constitute the principal information that we need to configure our JCE Encrypter using keystore. Please refer to the previous post on Anypoint Enterprise Security module installation. Let’s create a simple Mule Maven project using Anypoint Studio. The most important point is to add the following repository in the pom.xml file: Please note that I have copied the generated keystore (in the previous step) to the resource folder of the project. Here is a screenshot of the project structure: Let’s create a simple flow. The scenario is as follows. We send some JSON data to an HTTP endpoint. Then the JSON data is encrypted using JCE Encryption Strategy and we log the encrypted data. After that we simply decrypt the encrypted data and log it. Here is diagram of the flow. First let’s configure a global Encryption element. Do mind it to select the Default Encryptor dropdown as JCE _ENCRYPTER . I have named the global element as JCE_Keystore . Then select the JCE Encrypter tab and configure it as shown in the diagram, Now after configuring the global Encryption element in step 1 let’s configure the Encryption component (labeled as Encrypt in the flow. This is an Encryption palette found under the Security category of Mule palettes ). Here is the screen shot, In the Connector Configuration please select the global encryption element( JCE_Keystore ) that we have defined in step 1 . In the Operation dropdown select Encrypt . In the Input Reference field I am putting the whole payload (JSON data) for encryption. Do remember that you can use MEL expression here to encrypt some part of the payload too . And that’s it! No more configuration is needed; you are ready to go. In the step 2 we have encrypted the message. Now let’s decrypt the encrypted message. Here we are going to configure the component labelled as Decrypt . It’s nothing but an Encryption palette found under the Security category of Mule palettes . Here is the configuration: The configuration is almost same as that for encryption . The only difference is the Operation: Decrypt . Now, let's run the project! After running the project, send a POST request with JSON payload. I am using Postman for this operation. Upon observing in the Anypoint console, you can see the encrypted message as well as decrypted message: Sample log in Anypoint console: Hope this is useful to some of our readers! Feel free to add your questions in the comments below. All source code can be found here .", "date": "2017-3-8"},
{"website": "Avenuecode", "title": "Principles of Package and Component Design", "author": ["Rafael Romão"], "link": "https://blog.avenuecode.com/principles-of-package-and-component-design", "abstract": "SOLID principles are quite well known nowadays, but do you know their cousin, the Principles for Package and Component Design? Okay, maybe it's not the catchiest name, but that's what Robert C. Martin named them in his book entitled Agile Software Development, Principles, Patterns, and Practices . In this book, Uncle Bob tried to cover everything he knew at the time about Agile software development, and this is where SOLID principles came from. Many of the chapters are just reviews, like the one about UML, but the chapter about Principles for Package and Component Design is something I've never seen elsewhere. These principles are like a version of SOLID principles with a granular focus, as the name implies, and are quite helpful in keeping software components well defined. They are divided into two categories: In the first group, Principles of Component Cohesion , you can find 3 principles: The Reuse/Release Equivalence Principle (REP) The granular of reuse is the granular of release. REP states that the granule of reuse, a component, can be no smaller than the granule of release. Anything that we reuse must also be released and tracked. It is not realistic for a developer to simply write a class and then claim that it is reusable. Reusability comes only after a tracking system is in place and offers the guarantees of notification, safety, and support that the potential reusers will need. REP gives us our first hint at how to partition our design into components. Since reusability must be based on components, reusable components must contain reusable classes. So, at least some components should comprise reusable sets of classes. Martin, Robert C. Agile Principles, Patterns, and Practices in C# . Pearson Education. Kindle Edition. The Common Reuse Principle (CRP) The classes in a component are reused together. If you reuse one of the classes in a component, you reuse them all. This principle helps us to decide which classes should be placed into a component. CRP states that classes that tend to be reused together belong in the same component. Classes are seldom reused in isolation. Generally, reusable classes collaborate with other classes that are part of the reusable abstraction. CRP states that these classes belong together in the same component. In such a component, we would expect to see classes that have lots of dependencies on each other. A simple example might be a container class and its associated iterators. These classes are reused together because they are tightly coupled. Thus, they ought to be in the same component. Martin, Robert C. Agile Principles, Patterns, and Practices in C# . Pearson Education. Kindle Edition. The Common Closure Principle (CCP) The classes in a component should be closed together against the same kinds of changes. A change that affects a component affects all the classes in that component and no other components. This is the Single-Responsibility Principle (SRP) restated for components. Just as SRP says that a class should not contain multiple reasons to change, CCP says that a component should not have multiple reasons to change. In most applications, maintainability is more important that reusability. If the code in an application must change, you would prefer the changes to occur all in one component rather than being distributed through many components. If changes are focused into a single component, we need redeploy only the one changed component. Other components that don’t depend on the changed component do not need to be revalidated or redeployed. Martin, Robert C. Agile Principles, Patterns, and Practices in C# . Pearson Education. Kindle Edition. And for the second group, Principles of Component Coupling , we have another 3 principles. The Acyclic Dependencies Principle (ADP) Allow no cycles in the component dependency graph. The dependency structure must always be monitored for cycles. When cycles occur, they must be broken somehow. Sometimes, this will mean creating a new component, making the dependency structure grow. Martin, Robert C. Agile Principles, Patterns, and Practices in C# . Pearson Education. Kindle Edition. The Stable-Dependency Principle (SDP) Depend in the direction of stability. Designs cannot be completely static. Some volatility is necessary if the design is to be maintained. We accomplish this by conforming to CCP. Using this principle, we create components that are sensitive to certain kinds of changes. These components are designed to be volatile; we expect them to change. Any component that we expect to be volatile should not be depended on by a component that is difficult to change! Otherwise, the volatile component will also be difficult to change. It is the perversity of software that a module that you have designed to be easy to change can be made difficult to change by someone else simply hanging a dependency upon it. Not a line of source code in your module need change, and yet your module will suddenly be difficult to change. By conforming to SDP, we ensure that modules that are intended to be easy to change are not depended on by modules that are more difficult to change than they are. Martin, Robert C. Agile Principles, Patterns, and Practices in C# . Pearson Education. Kindle Edition. The Stable-Abstractions Principle (SAP) A component should be as abstract as it is stable. This principle sets up a relationship between stability and abstractness. It says that a stable component should also be abstract so that its stability does not prevent it from being extended. On the other hand, it says that an instable component should be concrete, since its instability allows the concrete code within it to be easily changed. Thus, if a component is to be stable, it should also consist of abstract classes so that it can be extended. Stable components that are extensible Martin, Robert C. Agile Principles, Patterns, and Practices in C# . Pearson Education. Kindle Edition. No matter what we call them, modules/components/packages are used far more today than they were at the time this book was written, and most of these principles are easily achievable when using package managers, like npm , maven , nuget and so on. Design your software components as if they will be released and versioned in a package manager, even when they won't be, and you will ensure that your components can evolve well. To learn more about these patterns, read chapter 28 of the aforementioned book. The most recent revision, released in August 2013, can be found here . Title image created by pch.vector - www.freepik.com", "date": "2019-8-21"},
{"website": "Avenuecode", "title": "Software Craftsmanship Manifesto - A call for professionalism", "author": ["Breno Sarkis"], "link": "https://blog.avenuecode.com/software-craftsmanship", "abstract": "Compared with other leading professions, the software industry is still in the early stages of development.  As in any field, software developers are constantly attempting to identify practices and principles to develop better and faster software. As we've grown up as an industry, we've created resources such as Software Architecture and the Agile Manifesto , the importance of which is clear and well-established. But even with these resources and guides to best practice, think about how many software projects still fail today. Is it due to our inability to apply the techniques? Or because client demands often require developers to rush through the development cycle? These are both real and relevant factors - but bear with me, and I'll share my theory as to why most software projects fail. We all know the feeling of moving fast, right? Imagine working on a greenfield project, with code being written at light speed. Everybody is happy and we're satisfied with ourselves and our work. But not long after that, things begin to get progressively slower and slower. Whereas previously we were hitting all our goals at every sprint, suddenly we're left with carryovers at every turn. What happened? Long story short, code rots at an incredibly fast pace. We all know of techniques such as TDD and refactoring, but we ignore them in favor of speed. When we create a lousy design that is never cleaned, our code rots and kills development speed. We try to convince people to allow us to do major redesigns in order to go fast again, and we keep lying about our estimates. We have all been there right? That was the Bad Code part - now let's talk about ourselves. Why do we allow this to happen? Why don't we do what we know we should be doing?  I like to think that you're only as good as this gap between what you do now and what you think you should be doing. The answer to that question is quite simple in fact. Simply put, we're not professionals. I could spend hours describing examples of poor practices most of us are guilty of on a daily basis, not only with our code but in how we interact with people. We developers tend to have a bad habit of always avoiding conflict, so we often say yes to whatever the question is. After all, it's very unlikely that we'll be the ones to pay the price for our lack of responsibility. To illustrate this better, take a look at the example listed here : Imagine this conversation between a patient and a doctor: Patient: “My arm hurts.” Doctor: “What would you like me to do about it?” Patient: “Make my arm stop hurting.” Doctor: “Do you want me to cut it off? I can do that.” Patient: “No, I just want it to stop hurting.” Doctor: “I could cut all the nerves to your arm. That’ll stop it.” Patient: “Isn’t there something less drastic you could do?” Doctor: “Ooops, sorry, time for my break.” Clearly we don’t expect doctors to behave this way. Even though the patient is the boss, the patient expects the doctor to have the answers and help set the direction. Here’s another version of the conversation: Patient: “I want you to cut my arm off.” Doctor: “What’s wrong with your arm?” Patient: “It hurts. I’m tired of it. Just cut it off.” Doctor: “Let me see your arm. Hmmm. Looks like you’ve got a sprain or perhaps a hairline fracture. We should take some X-Rays.” Patient: “No, just cut it off.” Doctor: “Sir, I do not cut off healthy arms.” Patient: “But I’m paying you. You have to do what I say!” Doctor: “No, sir, I don’t. Cutting off your arm would violate my oath.” Which of these two doctors would you rather be? Now project these two doctors into your own profession, and which would you rather be? It was with these tendencies in mind that several well-known software developers such as Uncle Bob , Ron Jeffries and Corey Haines decided to create yet another Manifesto . This time, created for us. Here it is: The first thing you may notice is that the manifesto calls its adopters Craftsmen. This is the term used to identify people who believe in the presented ideas. According to Corey Haines, there are essentially two ways to qualify as Craftsmen or Craftswomen. You can either adopt the title for yourself, or receive it from your employer. The latter is fairly uncommon in today's world. Although the principles are fairly easy to understand in theory, the first one might be hard to define in actual practice. What does it mean for software to be well-crafted? At this point in my career, I've had experience with what is clearly not well-crafted software, so allow me to share my view of what the term means. Well-crafted software is software created by developers who care. Those who practice disciplines such TDD , BDD , Refactoring , Principles of good design and never settle for anything less than their best work. We all know the difference between code written by someone who cares and code written by someone who doesn't. One of the things I'm coming to realize as my career grows is that each principle becomes easier to follow if you do the previous one. Take Steadily adding value for example: it's almost impossible to do that if you don't have well-crafted software . If things are still too abstract, I recommend the more direct message from Uncle Bob. The Programmer's Oath . Software craftsmanship is about raising the bar of professional software development. It is a mindset where you accept the responsibilities of being a professional. You're constantly seeking improvement and teaching others about the craft, you take pride in what you do, and you are disciplined in the way you work. You're open to community as you believe that we grow stronger if we're together for the greater good. All in all, it's a call for professionalism.", "date": "2017-4-26"},
{"website": "Avenuecode", "title": "Why Oracle Commerce Cloud is a Leading E-Commerce Solution", "author": ["Orlando Neto"], "link": "https://blog.avenuecode.com/why-oracle-commerce-cloud-is-a-leading-e-commerce-solution", "abstract": "In today's market, the online retail industry is growing quickly. Merely selling products online is no longer enough to guarantee success. Instead, online retailers have to consider how to optimize discounts, flash sales, and promotional events while adapting to consumer demand. Besides supporting an immense number of features, online retail stores must also be highly available and scalable. If an online store can't support heavy traffic during high sales seasons, such as Black Friday, Cyber Monday, Christmas, Mother's Day, etc., consumer outrage could cause significant damage to that store's reputation and finances. To meet these needs, Oracle designed a cloud solution to support business innovation and to quickly launch feature-rich, responsive storefronts supportable by all devices. Oracle Commerce Cloud was built with API-first architecture, an open, standards-based development platform, and simplified integrations: The Oracle Commerce Cloud storefront supports 35 languages and 60 global currencies out-of-the-box, and it also boasts several pre-integrated features, some of which are pictured below: Oracle's development structure is based on widgets. (A widget is a unit of UI functionality that can be used in many different pages. Widgets can provide custom HTML, JavaScript, CSS, localization, and component reuse.) Widgets make it easy to customize storefronts with new functionalities and/or new page structures. These changes are considered extensions. An extension can contain one or more of the following items: All functionalities are accessible through easy-to-use REST web services and front-end coding skills, such as HTML, CSS, and JavaScript. Due to its API-first architecture, OCC customization and innovation are simple since everyone (OCC, partner-built, customer-built, and so on) uses the same API . This design also makes it easy to learn and start using OCC on a daily basis. As we've shown above, Oracle has designed an incredible cloud infrastructure so that businesses can focus on operations without worrying about the framework. Oracle has also defined an architecture that's easy to use, which expedites the learning process and makes team ramp-up fast. These benefits, combined with Oracle's vast experience in the market, make Oracle Commerce Cloud a clear leader for e-commerce solutions.", "date": "2018-4-4"},
{"website": "Avenuecode", "title": "How Java Garbage Collection Works and Why You Should Care - Part 2", "author": ["Roseane Silva"], "link": "https://blog.avenuecode.com/how-java-garbage-collection-works-and-why-you-should-care-part-2", "abstract": "Last week, we introduced Java Garbage Collection and its available collectors . Today, we'll discuss how to select the right garbage collectors for your needs and how to utilize them effectively. The following chart will help you choose the right GC for your project: Specification GC Option Applications with small datasets (<= 100MB) Serial -XX:+UseSerialGC Applications running on single processors with no pause time specifications and requirements. Serial -XX:+UseSerialGC When application performance is required and one second or longer pause time is acceptable. Parallel -XX:+UseParallelGC When response time is more important than overall throughput and GC, and pause times must be up to approximately one second. G1 or CMS -XX:+UseG1GC or -XX:+UseConcMarkSweepGC When response time is the priority and a large heap is available. Z GC -XX:UseZGC One special thing about Java and GC are Weak References. A WeakHashMap is a special implementation of the map interface, which only stores keys with weak references. This means that when a key is no longer referenced outside the the WeakHashMap, the key-value pair will be garbage collected. It works pretty much like a HashMap, but there's one exception: if Java memory manager no longer has a strong reference to the key, the entry will be removed from the map. Overall, one of the most important things to understand is that GC is a non-deterministic process where we can only give hints about when should be a good time for GC to be executed with methods like System.gc() or Runtime.gc(), but there is no guarantee that GC will be executed. The best way to adjust GC is setting up JVM flags. These flags can adjust GC, initial and maximum heap size, heap section size, and more. This is why it's good to know how the application works and consider the “Selecting a GC” parameters shown above. In summary, knowing how Java Garbage Collector works can help developers create much better applications. Whether you are actually going to choose a GC or not, knowing if you need to choose one is a good start. And if the objects are being created when they are really needed and with the scope they’re really needed, congratulations! You know how to maximize GC, and hopefully JVM will be good to you. For references and further reading about Java Garbage Collection, please see the following articles: MANDIC. Java Garbage Collection: melhores práticas, tutoriais e muito mais . Accessed: Dec 4, 2019. THE URBAN PENGUIN. JAVA Object Lifecycle, de-referencing and garbage collection . Accessed: Dec 4, 2019. ORACLEa. Java Garbage Collection Basics . Accessed: Dec 4, 2019. GEEKS FOR GEEKS. Garbage Collection in Java . Accessed: Dec 4, 2019. ORACLEb. HotSpot Virtual Machine Garbage Collection Tuning Guide . Accessed: Dec 4, 2019. ORACLEc. Getting Started with the G1 Garbage Collector . Accessed: Dec 18, 2019. DEVMEDIA. Introdução ao Java Garbage Collection . Accessed: Dec 23, 2019.", "date": "2020-8-19"},
{"website": "Avenuecode", "title": "Feature Files: Concepts and Tips", "author": ["Gabriel Abreu"], "link": "https://blog.avenuecode.com/feature-files-concepts-and-tips", "abstract": "How many times have you come across a story that is ready for testing, but does not fit all the acceptance criteria? Did you know that a majority of cases are related to bad feature file writing? In this snippet, I am going to present some standards and tips on feature file writing. These tips will help prevent late and bad outcomes, as well as endorse software quality using techniques and approaches that will help the Agile team to accomplish a better outcome for the stories within the project. By definition, Behavior Driven Development (BDD) is an Agile software development technique that supports collaboration among developers, quality assurance analysts, business analysts, and non-technical people in a software project. It does so by utilizing a simple domain-specific language that uses natural language constructions in order to express the behavior and expected outcomes. The language in which these files are written is called Gherkin. Basically, the entire development process is guided by dedicated files that contain behavioral specifications of the project features. That being said, explicit texts inside the file are of the upmost importance. Confusing words and ambiguous statements may affect the final behavior, which will lead to a higher fixing cost. It is widely known that the cost of fixing a poorly written requirement is much less than fixing an incorrect behavior on a system that has already been developed. Now that we know the importance of the feature files, let’s begin with our “guidelines”. By convention, every file should contain behavior specifications of a single feature, which usually contains many scenarios. The space between the word Feature , which starts the file, and the first scenario can be used to describe the feature itself. The scenario is a list of steps that start with one of the key words: Given, When, Then, And, But. See the example below: Feature : Maintaining the fruit storage in Avenue Code Fruits are stored in the fruit bowl which can store up to 100 fruits Every fruit taken from the bowl should be decreased from the stock count Every time the bowl hits a quarter of its capacity the vendor must be contacted When the vendor arrives with new fruits, the storage must be updated Scenario : As an administrator of the fruits storage, I want to update the stock count Given the fruit bowl has 23 fruits And the vendor arrives with 75 fruits When the fruits are placed into the fruit bowl Then the stock count should be updated And the stock count should be equal to 98 Now let’s imagine a feature that all its scenarios have the same two steps, for example, the “My Account” page of a random website: Feature : My account page The page should display the company logo A direct link to “My History” page should be on the page The page should contain a “Sign Out” link Scenario : As a user, I want to verify that the company logo is displayed in the “my account page” Given I visit the website as a guest user And I sign in using valid credentials And I access my account page Then I should see the company logo Scenario : As a user, I want to verify that a click on “My History” link on “My Account” page redirects me to the “My History” page Given I visit the website as a guest user And I sign in using valid credentials And I access my account page When I click on the “My History” link Then I should be redirected to “My History” page Scenario : As a user, I want to verify that I’m able to sign out using the “Sign Out” link on the “My Account” page Given I visit the website as a guest user And I sign in using valid credentials And I access my account page When I click on the “Sign Out” link Then I should be redirected to the homepage And I should be signed out As we can see, all scenarios contain the same first three steps, which may seem repetitive and boring to read. To solve this problem, Gherkin proposes the concept of the Background. The Background can be written in a Feature File right after its description and before the first scenario. Basically, it can contain any of the keyword steps (Given, When, Then, But, And) and it represents something that will always happen before the execution of any scenario within the feature. Check out how the same feature would look using the “Background” concept: Feature : My account page The page should display the company logo A direct link to “My History” page should be on the page The page should contain a “Sign Out” link Background: Given I visit the website as a guest user And I sign in using valid credentials And I access my account page Scenario : As a user, I want to verify that the company logo is displayed in “my account page” Then I should see the company logo Scenario : As a user, I want to verify that a click on “My History” link on “My Account” page redirects me to the “My History” page When I click on the “My History” link Then I should be redirected to “My History” page Scenario : As a user, I want to verify that I’m able to sign out using the “Sign Out” link on the “My Account” page When I click on the “Sign Out link Then I should be redirected to the homepage And I should be signed out Another tool that can be used to make the file easy to read and avoid redundancy is the Scenario Outlines. Let’s use the same fruit stock manager example we were talking about. We could have some similar scenarios like listed above: Feature : Fruit count # Detailed business description of the feature Scenario : Eat 3 out of 70 Given there are 70 fruits in the fruit basket When I eat 3 fruits Then the fruit count should  be equal to 67 Scenario : Eat 15 out of 30 Given there are 30 fruits in the fruit basket When I eat 15 fruits Then the fruit count should be equal to 15 Simply copying and pasting the same scenario only to use different values may quickly become tedious and repetitive. Scenario Outlines allow us to express these examples more concisely by using a template with placeholders. Notice the changes to the same feature file below: Feature : Fruit count # Detailed business description of the feature Scenario Outline: Eating Given there are <startNumber> fruits in the fruit basket When I eat <eatenNumber> fruits Then the fruit count should equal to <remainedNumber> Examples : |startNumber| |eatenNumber| |remainedNumber| |          70           | |           3          | |              67             | |          30           | |           15        | |              15             | If we want to add more data to the steps, we can utilize the concept of tables. Tables allow us to capture a richer data structure. Notice the example below: Scenario : As an administrator user, I want to check the users list Given the following account exists: |name   | | id   |  |gender| |Chris    | |78   |  |M         | |Anna    | |79   |  |F          | |Jorge    | |80   |  |F          | When I access the users list Then I should see all of the existing accounts Now, let’s talk about how the scenarios are organized. Gherkin uses tags to create clusters and groups of scenarios. Using tags, a myriad of test suites can be created, separating the scenarios according to their severity. A tag should be written on the top of a scenario. Notice the example below: Feature : My account page Background : Given I visit the website as a guest user And I sign in using valid credentials And I access my account page @severity_1 @domain_1 Scenario : As a user, I want to verify that the company logo is displayed in “my account page” Then I should see the company logo @severity_1 @domain_2 Scenario : As a user, I want to verify that a click on “My History” link on “My Account” page redirects me to the “My History” page When I click on the “My History” link Then I should be redirected to “My History” page @severity_3 @domain_3 Scenario : As a user, I want to verify that I’m able to sign out using the “Sign Out” link on the “My Account” page When I click on the “Sign Out link Then I should be redirected to the homepage And I should be signed out It’s important to keep some things in mind when writing a feature file: See? It’s easy to write and maintain all the complex business rules through BDD and feature files. Also, Gherkin is available in many languages, allowing you to write stories using localized keywords from your language. If you speak Portuguese like me, for example, you can write “Cenario” instead of “Scenario”. Just keep in mind that the feature files represent the acceptance criteria of the stories and are made to keep the process simple for everyone involved, so the clearer they are written, the better the outcome.", "date": "2017-7-12"},
{"website": "Avenuecode", "title": "AC Spotlight - André Schenkel", "author": ["Andressa Lopes"], "link": "https://blog.avenuecode.com/ac-spotlight-andr%C3%A9-schenkel", "abstract": "Andre Schenkel, CIO at Prudential do Brasil, discusses his career journey and gives insight into Prudential's Agile and innovation practices. Avenue Code : You have a brilliant track record and have been in the technology market for over 25 years. Could you tell us how you became the CIO at Prudential? André Schenkel: My story at Prudential do Brasil started in 2018 when I was invited to participate in a selection process to lead the company’s digital transformation initiative. After six months of working at Prudential, I can say that it is a great pleasure to be a part of this moment in the company’s history. Prudential is one of the largest financial services institutions in the world, with more than 140 years in the industry and a presence in more than 40 countries. The company’s current digital transformation initiatives are challenging and motivating, which provides a great opportunity for professional growth and personal development. I am proud to belong to a great insurance company at its best. Before coming to Prudential, I worked for twenty-one years at Citibank, one of the most well-known banks in the world. I can only be thankful, because I had the opportunity to experience very important developments there, such as the institution’s preparation for the famous millennium bug. This took me to the USA for seven years, where I worked on and eventually managed development projects. In 2002, Citibank moved its development center from the USA to Latin America, so I relocated to Chile, where I assembled a software factory. My initial team for this endeavor consisted of 14 people and, at the end of 12 years, we had almost 400 people, including developers and business analysts. At the end of 2015, I received an invitation to make the transformation to Citi's core bank in Brazil, which allowed me to establish roots in the country again after 20 years abroad. After my journey at Citibank, I worked at a cryptocurrency FinTech, a fantastic and challenging experience that was a kind of hands-on MBA in digital transformation. AC: Today, many companies are adopting Agile methodologies. Which methodologies does Prudential use, and what are the challenges you face in implementing them? AS: This question is very interesting, as there is often an assumption about Agile that is erroneous. Normally, Agile practices are associated with IT and software development. This may be where Agile has better adherence and more evident results to be measured. However, I think the big challenge is to make the adoption of Agile practices reach all areas of the company so that we don't have only partial results. I believe that implementing a hybrid model can be the best cost-benefit solution for short-term results. At Prudential do Brasil, for example, we started this transformation with a significant restructuring of the IT department, adopting the squad model for each product or service area during phase 1. We also implemented the sprint model, with deliveries every 15 days. I believe that this is the safest and most effective way to implement the Agile model throughout the organization until we reach the ideal balance of the hybrid model, which serves us best. AC: Does Prudential have an area dedicated to Innovation? How does the company disseminate this culture? AS: Originally, there was an Innovation area, a kind of laboratory, that was embryonic in terms of wider business application. Now we have an Innovation area in our headquarters abroad. This serves as a reference point so that we can take advantage of the model and adjust it to the realities of Latin America. Currently, the Architecture area is a reference in innovation, and we have extended its practices to other departments. POs and squads receive this channeled innovation from the Architecture team, thereby disseminating it to the entire company. AC: What do you look for in strategic partnerships? AS : One of my goals today is to re-profile partners here at Prudential. The key idea is to aggregate. We look for partners who bring market experience, add value to our business, and teach me to do what I don't know. We are not just looking for labor placement. AC: What are Prudential's next steps regarding technology? AS: Our business model is extremely successful, and it is clear that the crisis has given us an opportunity to accelerate and continue investing even more quickly in technology, always aiming to give our clients the best experience on all channels. Let's use this moment to experiment and dare more! AC: Thanks for the insight into Prudential’s innovation culture and methodologies for IT, Andre!", "date": "2020-6-24"},
{"website": "Avenuecode", "title": "Elasticsearch", "author": ["Gabriel Neves"], "link": "https://blog.avenuecode.com/elasticsearch", "abstract": "What is Elasticsearch, how does it work, and why is it so effective? Elasticsearch is simple to configure, has incredible flexibility, and is an excellent tool for complex searches. Let's take a closer look. Elasticsearch is an open-source search engine based on the Lucene library. It was developed in Java and is designed to operate in real time. It can search and index document files in diverse formats. It was designed to be used in distributed environments by providing flexibility and scalability. Now, Elasticsearch is a widely popular enterprise search engine. Elasticsearch supports a large amount of data without losing performance. It can be deployed on any system, regardless of platform, by providing a REST API. It solves many problems by offering very complex search possibilities, including phonetic search, full-text search, multiple search and parallelism. To help understand how Elasticsearch handles data, we can make an analogy to a database. Elasticsearch stores the data using the \"schema-less\" concept. This means that it is not necessary to define the structure of the data that will be entered in advance, as happens with relational databases known in the market: Oracle, MySQL, and SQLServer, among others. In our analogy of traditional relational databases, the structure of the data used by Elasticsearch would be: In talking about the infrastructure that Elasticsearch provides, we have some important terms to learn. Image courtesy of bonsai With this structure, Elasticsearch offers us a good API. In addition to the data manipulation API being responsible for replication, it will also take care of the success of transactions, cluster health, and other management jobs. Here are the main APIs and their functions: The \"default\" configuration of the tool is very robust and scales horizontally. But if a change is needed, it's usually quite simple: this can be done by changing the properties of the application configuration file or through API REST calls. First, because Elasticsearch was built to make it easier to manage your activities in a simple and efficient way, as we have already explained, thus enabling you to scale infrastructure as needed. Second, internally, it keeps the data cached to make the result even more performative. Queries are made when the HTTP GET verb passes the desired parameters. Third, its storage structure records information in a different way than traditional relational databases; it uses a structure we call an inverted index. In this case, the index is the same as in the relational database, i.e. a structure already known, which helps to store data for future use. For example, imagine that we want to search all articles that contain the word \"Brazil\" in the title or text. How would we do this in a relational database? We would have to use a query like this: SELECT * FROM article WHERE title LIKE '%Brasil%' OR text LIKE '%Brasil%'; In this case, we know that using indexes in the columns will not be a good option, especially in the \"text\" column, because we need to use the operator \"LIKE\" involving the search on the wildcard \"%\", which introduces a big performance issue. The query won't be effective at all since the database will have to go through all the records to find the words within the columns, so it will make a fullscan in the table, and this will not be good. Imagine that this table has millions of records... It is at this time that our friend inverted index stands out. Its structure is assembled through the words, which are called \"terms\" in Elasticsearch. #CodingExplained offers an excellent introduction on the inverted index: \"The purpose of an inverted index, is to store text in a structure that allows for very efficient and fast full-text searches. When performing full-text searches, we are actually querying an inverted index and not the JSON documents that we defined when indexing the documents... An inverted index consists of all of the unique terms that appear in any document covered by the index. For each term, the list of documents in which the term appears, is stored. So essentially an inverted index is a mapping between terms and which documents contain those terms. Since an inverted index works at the document field level and stores the terms for a given field, it doesn’t need to deal with different fields.\" - #CodingExplained Alright, so let’s look at an example. Suppose that we have two recipes with the following titles: “The Best Pasta Recipe with Pesto” and “Delicious Pasta Carbonara Recipe.” The following table shows what the inverted index would look like. So the terms from both of the titles have been added to the index. For each term, we can see which document contains which term, and this enables Elasticsearch to efficiently match documents containing specific terms. The first step of a search query is to find the documents that match the query in the first place. So if we were to search for “pasta recipe,” we would see that both documents contain both terms. If we searched for “delicious recipe,” the results would be as follows: As we can see, an inverted index structure contains the following information: Elasticsearch applies some rules to save each word separately and simply for future searches. It's what we call the analyze process. Now you can see why Elasticsearch is so fast for searches: it already has the term saved and knows which documents have it, so you don't need to fullscan the data. Of course, Elasticsearch has an effective implementation to work with data in memory and manipulate the data in the operating system, but the fact that it uses inverted indexes definitely makes it more appropriate than relational databases when it comes to searches. Elasticsearch will \"break\" a text into terms so that it can create this index. To separate words from the text, it uses a technique called analyzer. This technique is responsible for processing the text being saved. The default analyzer is called \"Standard Analyzer.\" Generally speaking, what the analyzer does is: With this, Elasticsearch can store words in a very uniform way. Here's what the following text would look like: \"Is this dèja vu?\" The terms would look like this:                                                                                                                                                [ is, this, deja, vu ] Note that the procedure separated the words and applied the quoted rules before saving the term. These are some of the main procedures performed by analyzers. There are several types of analyzers, and each has its own set of rules to apply to the data. You can also create your own analyzer using an existing set of rules or even creating your own rules. In addition to these techniques, Elasticsearch has several settings to give weight to a certain field, such as frequency of the term, the length of the term, frequency of the reverse document, etc. to retrieve documents with accuracy and speed. Elasticsearch can be a great way to optimize your product, improving the search by making it faster and enabling more complex searches. It's a simple tool to configure, has incredible flexibility, and is an excellent option for data searches and complex searches. It has the power to execute queries quickly and with excellent  results. Elasticsearch provides excellent documentation, and its ever-growing community is active and helpful.", "date": "2019-8-7"},
{"website": "Avenuecode", "title": "Your ML Career Starts Here", "author": ["Richard DesCombaz"], "link": "https://blog.avenuecode.com/your-ml-career-starts-here", "abstract": "There's a lot of interest in learning some form of machine learning/predictive analytics or AI these days, but knowing where to start can be difficult. I recommend learning TensorFlow on Colab (along with NumPy and pandas), because TensorFlow has support for developing many standard AI/ML models quickly and Colab gives us a user-friendly online environment for experimenting. TensorFlow is an open-source machine learning framework developed by Google. It has support for many machine learning techniques and can be used with other popular Python frameworks to develop machine learning/AI models. Colab will be familiar to anyone who has used a Jupyter Notebook to process code. It allows you to write documentation and runnable code within the same document. Colab uses the same technology and is especially useful since there is no setup required. Colab also offers additional features like code snippets, datasets that can help you learn and explore, and the use of servers on Google Cloud, including hardware acceleration with GPU/TPU processors and data storage. You can go to Colab's website and create a new notebook, view an example, or upload one from your Google Drive or GitHub. Here is an example of getting to the ready-made code snippets to ease your development efforts: Even if you aren't interested in taking a deep dive into AI, there are many reasons why it's useful for any programmer to become acquainted with these tools. The TensorFlow framework can make some basic implementations easy to accomplish. As coders, we already have an essential head start in that we can code and often have enough background in mathematics that will allow us to dive into more sophisticated models. Data science projects don't just need data scientists. Some industry estimates assume that there is a need for 2-5 data engineers for every data scientist . A large amount of time spent on an analytics project will be in developing systems for collecting and preparing data. Still, your ability to play a part in the exploratory phase of data analysis will be a great benefit to your client. Implementing a Convolutional Neural Network (CNN) to classify cats and dogs is exciting and something that can be done with a relatively short program. And of course, this type of model could be useful in many types of applications. However, here I will focus on a more typical business-centric analytics problem. There are many use cases for clients, including using Natural Language Processing, to develop models for chat machines or analyzing customer feedback. These can be done using models like Naive Bayes for spam detection and Random forest, an ensemble of decision trees, to be used on tabular data for optimizing marketing campaigns. Next, I will show you an example of using a BoostedTreesClassifier to inspect employee attrition data. The boosted tree is an ensemble technique that works on familiar decision trees, but it employs machine learning by repeating the process and learning from prior mistakes, adding and removing elements and data for randomness. Employee attrition is a high cost for employers, and taking time to retain talent is time-consuming and expensive. Next, we'll explore an example of the IBM Attrition data set to gain some insights on root causes. I've placed my notebook on GitHub if you are interested in working with it. You can create your notebook here . Let's bring in our primary tools: NumPy, pandas, and scikit-learn. Load the data set. I've given a link to the IBM attrition data set in the code. You can upload it to your Colab work space or retrieve it from Google Drive by connecting your account. Once you have the recordset, you can open the explorer on the left, then right-click to copy the path: Now that you've loaded your data, take a look either by using the describe() function on the DataFrame object or by clicking on your original data link and opening the data explorer on the right side of the screen: List your columns so that you can copy and paste them into the code later. Our target variable is going to be \"Attrition,\" which will be used against the other fields to build the classifier. Let's take a look at the \"Attrition\" records: The records are either \"Yes\" or \"No.\" We'll need to transform this data into a numerical representation: Split the data set, using 80% for training and 20% for testing. Notice the number of records for each is from a total of 1470 records with 35 columns: Take the 'Attrition' value out since we are using it as our target: Now take a look at the data. Below is a histogram based on the time an employee has spent with a current manager: And here's another histogram-based on age: Okay, that brief inspection was easy. Now let's build the predictive model. Here we separate the categorical columns from the numeric columns. The goal is to treat categories as numeric values for the model's evaluation. For instance, \"Gender\" will internally be set: Male=0, Female=1. Set up your evaluation input functions: Now we run our BoostedTreesClassifier. With the libraries in our TensorFlow package, we can run other classifier models as well. The estimation accuracy for our first run is almost 87%, and the Area Under the Curve (auc) is over 80%, indicating decent true positive predictability. The ROC curve helps us see how quickly our model made improvements and is useful when attempting to prevent design issues with the model. The ROC chart plots the true positive rate against a false positive rate. Now the Big Payoff. What are the factors that have the greatest impact on attrition? Finding these factors is the whole point. When we discover these, we can use the information to help guide our company: Maybe there's more work to be done, but this is pretty successful for an initial exportation of the data set. We could probably start to use what we’ve found to address attrition! I hope you enjoyed this brief tutorial of TensorFlow on Colab and that it enriches your programming skills and/or kick starts your ML career!", "date": "2020-5-13"},
{"website": "Avenuecode", "title": "Cost Optimization Tips for Cloud Computing", "author": ["Douglas Augusto"], "link": "https://blog.avenuecode.com/cost-optimization-tips-for-cloud-computing", "abstract": "Enterprise organizations around the world have been forced to transform themselves digitally to maintain business operations. Resilient infrastructure is key to scaling and keeping operations running, but how can your business grow sustainably without getting lost in IT spending? When we talk about costs beyond the basics expected for infrastructure, most companies face these common problems: Poor Application Architecture Design Technical decisions are made without conducting a study to identify the best solution. Improper Provisioning Resource oversizing is a very common practice, often due to the lack of visibility of system needs. Lack of Visibility Without a tagging system and monitoring, it's difficult to find cost reduction possibilities. FinOps is a model for operating cloud resources that uses systems, best practices, and culture to give an organization the ability to understand IT costs in the cloud and make business decisions regarding investments. The term has gained popularity and interest not only in technology, but also in business and finance. In every field, the focus is the same: Cloud Financial Management. There are several principles of FinOps. The established performance phases consist of: 1. Inform (Visibility & Allocation), 2. Optimize (Utilization) and 3. Operate (Continuous Improvement & Operations). When we talk about cost reduction, there are always two factors - regardless of the methodology - that directly play into the strategy and prioritization of activities. They are Potential for Cost Reduction and Effort of Action . For the pay-as-you-need model, it's smart to focus on using only the infrastructure you need, limit who can create resources, remove waste, and optimize wherever you can. There are several activities that you can perform to reduce costs in the short term, and these do not require drastic changes in your environment or application level. Some of these activities are listed below: Activity Potential Effort Enforce controls Low Low Identify idle resources High Low Right-size resources High High Commitment plans High Low Lifecycle policies Low Low Schedule resources High High Leverage lower price resources Mid Mid Exclude unused logs Low Low Sometimes it takes much more than governance and resource optimization to reduce costs; modernization is necessary, and changes that directly affect the application are more difficult to implement, but some of them have a much greater potential for cost reduction. Below are some of these possibilities: Activity Potential Effort Redo feature selections High High Opt for cloud-native solutions Mid Mid Embrace serverless High Mid Multi-tenancy whenever possible Mid Mid Improve your CI/CD Mid Low Avoid licensing High High Partition and cluster data Low Low Create a robust cache policy Low Low Costs with cloud resources will always be a topic to be dealt with in the fields of technology, business, and finance, so regardless of your role in the organization, turn cloud cost optimization practices into culture, always keep useful dashboards at your disposal, and make recurring analyses of your environment.", "date": "2020-11-18"},
{"website": "Avenuecode", "title": "AC Spotlight - Pouya Boland", "author": ["Anna Vander Wall"], "link": "https://blog.avenuecode.com/ac-spotlight-pouya-boland", "abstract": "(The following article is property of Avenue Code, LLC, and was originally published with permission at Total Retail on May 22, 2019 .) Pouya Boland, CEO and co-founder of Chiquelle , explains how this Swedish-based clothing brand grew its sales from zero to millions in just a few years without investors or external capital. Avenue Code: Tell us about your personal career path. What brought you to where you are today? Pouya Boland: I’ve always had a desire to learn about technology and the internet, which eventually led me to start my own e-commerce business. I actually began building websites and creating content for them when I was just 14. Later, I studied international law and spent time in Washington D.C. before returning home to the Netherlands and realizing I wanted to start a clothing brand in Sweden and the Nordics. It was the idea of creating a business on the internet based on social media, technology, and a fresh, young perspective that inspired me to launch Chiquelle. AC: As the CEO and one of the founders of Chiquelle, what would you say is the key to success in today’s retail environment? PB: If you look at the six awards Chiquelle has received, you can attribute our success to three practices we’ve implemented from the beginning: Beyond these practices, our team’s dedication and commitment from the outset is a huge part of our success. AC: How were you able to grow Chiquelle from zero to where it is today without any investors or external capital? PB: We literally re-invested every profit we received back into the company. As founders and owners, we didn’t take any salaries during our first four years of business. We also tried to keep costs low but working in an efficient manner. Since we were profitable the whole way, we managed to build Chiquelle all on our own, and we're still 100 percent privately owned. We wanted ourselves to bring the company’s value and size to a certain level before looking for investors and external capital. As of today, we don't have loans, credits or debts anywhere, which puts us in a very strong position. AC: As a brand that launched in Sweden, how has Chiquelle grown to become an international business? PB: From the very beginning, we've focused on not looking at borders. Launching our online stores in multiple languages and using social media and marketing strategies applicable to all of Europe resulted in Chiquelle products selling in tens of countries. We believe that being on social media means we’re not bound by borders or restrictions, and that allows us to engage customers internationally. AC: What goes into building customer loyalty for Chiquelle? How is Chiquelle transforming the retail industry? PB: Our No. 1 rule is think long term. We think long term with every decision we make. For instance, rather than just trying to sell a product, we look at how we want to surprise our customers and give them a unique experience through our five-time wow effect. The first wow effect is when they visit our website and experience the perfect simplicity of the presentation, content and pictures. The second wow effect comes from the unique designs and products we offer. The third is the speedy processing of orders and deliveries. The fourth is our well-packaged orders. The fifth is our excellent customer care. Prioritizing these wow effects has transformed our followers into customers and our customers into fans. Chiquelle is transforming the retail industry through its fast-fashion concept. From idea to design and production to advertising a new product online, our lead time is just two weeks to three weeks. The wider retail industry struggles to make supply chains match the time-sensitive demands of customers while keeping up with ever-evolving fashion trends. We’re producing just a few weeks ahead and avoiding seasonal and financial risks. AC: You were at Shoptalk in Las Vegas as one of the key presenters; what was your message? PB: My talk was focused on the rapid change in social media and customer behavior, which has created a new era. The businesses that will thrive will be those that will meet these new consumer requirements with speed, simplicity, solid content, and a faster supply chain. Pouya Boland speaking at Shoptalk. Image courtesy of Chiquelle. AC: What does Chiquelle do to stay ahead of innovations in tools and technologies? PB: We’re always looking at the latest tech trends, but more importantly, we’re looking at what our customers want and require. We listen to society and its needs. Lately, we’ve been working with a partner company on an augmented reality (AR) project, which we’ll announce and launch soon. Through this AR app, our customers will be able to create an avatar look-alike, project it anywhere they want, live, and see how our clothing fits them. We're always testing ideas and looking at what the market is bringing forward. AC: What do you and your company look for in strategic partnerships? PB: In everything we do, we look for innovation and for opportunities that will help us create the best long-term strategy. Above all, we consider which partnerships will enable us to best serve our customers. AC: What are you proudest of having accomplished? PB: Two things: First, I’m proud of setting up a company, without investors or external capital, and growing it from zero to millions in sales in just a couple of years. Second, I’m proud of growing from zero followers to half a million and winning six awards. We’re very thankful for this recognition. AC: What are your future plans for Chiquelle? PB: Creating an even bigger global brand. We’ve just started. This is just the beginning. AC: Thanks for the insight into Chiquelle's innovative practices and strategies, Pouya! It's fascinating to hear about such a fresh approach to the clothing industry.", "date": "2019-12-4"},
{"website": "Avenuecode", "title": "The Retail Ops Weekly #4", "author": ["Ivan Dwyer"], "link": "https://blog.avenuecode.com/retail-ops-weekly-4", "abstract": "It's been a week of broken records... in Brazil with the Olympics in full swing, and in the US with Walmart's purchase of Jet.com - the largest E-Commerce acquisition in history. Remember, of course, records are made to be broken. I've pulled together another set of relevant thought pieces, news stories, and tips & tricks from the past week. Enjoy! It may be early to start decorating the front yard, but it is time to start thinking about ramping up for Black Friday. Paul Skeldon provides a few tips for getting ahead. Act now to prepare for Black Friday and Christmas, retailers warned – especially if you sell overseas [Internet Retailing] Digital transformation is placing more emphasis on E-Commerce, but that doesn't mean we're getting rid of physical stores. In fact, there's more innovation happening than it may appear. Natalie Burg explains how malls are becoming incubators for new technologies. No, Actually Malls Aren’t Dead: 3 Reasons Why [CIT] By definition, DevOps crosses different groups within an organization in various ways. Derek Weeks sits down for a candid interview with J Paul Reed to discuss security and releases with DevOps practices. Intersections: DevOps, Release Engineering, and Security [DZone] Organizational patterns can become harder to reason with as teams grow. Ben Linders provides sound advice for keeping the focus on the outputs and desired outcomes. Scaling Teams to Grow Effective Organizations [InfoQ] DevOps practices empower developers to ship software faster, but that also means consideration for every environment in the software development lifecycle. Liran Levy shares 7 things developers should know about production infrastructure [Tech Beacon] Last week I mentioned a heated Twitter debate on container standardization. As the dust settles, Matt Asay provides strong reasoning for Docker not giving it all away right away. Save the whale: Docker rightfully shuns standardization [InfoWorld] We're always looking at ways to optimize our site. UX mapping is a relatively new technique that navigates a persona through various interaction paths. Early concept, but it has potential. Mapping A Website's Strengths And Weaknesses With UX [Usability Geek] Staying on top of UX debt is just as important as minimizing technical debt. Jack Moffett provides a few key pointers, with a more comprehensive e-book linked at the end of the article. How to Avoid UX Gaps In Your Product [UXMag]", "date": "2016-8-31"},
{"website": "Avenuecode", "title": "An Easy Approach to Migrating from Spring MVC to Spring WebFlux", "author": ["Fernando Tavares"], "link": "https://blog.avenuecode.com/an-easy-approach-to-migrating-from-spring-mvc-to-spring-webflux", "abstract": "Reactive has definitely become a trending word in software development over the past few years, in part because of the high growth of internet users and the corresponding need to create more scalable applications. S ome people might say that reactive programming is a hard subject to learn, but certain frameworks and libraries are making it easier to implement even without a deep understanding of how it works. I believe that learning through assimilation is a great way to understand how things work, so today, we'll show you how to write a REST web service using reactive programming with Spring Web Flux as an alternative to Spring Web MVC. The goal is to demonstrate to developers who are used to using Spring MVC that it's possible and highly beneficial to migrate to a reactive application by implementing very small and simple changes. Before we begin, it's important to explain what reactive programming means . Phrased succinctly, reactive programming is a programming paradigm oriented toward data streams and the propagation of changes (i.e., it's asynchronous). An easy way to understand the propagation of changes is to remember how an excel formula works: every time one of its referred cells is changed, the value is updated. This behavior resembles the design pattern Observer introduced in the famous \"Gang of Four\" book, Design Patterns: Elements of Reusable Object-Oriented Software. Image courtesy of upload.wikimedia In the picture above, we can see that the Subject possesses information that is being watched by the Observers. These observers register themselves to receive notifications when the Subject suffers a change of state. Once notified, the Observers perform some operation for which they have been programmed. That is when the propagation of changes happens. When talking about reactive programming, there are some essential features we ought to define: Reactive programming is built upon four pillars : Spring Framework 5 introduces the React Streams API to create a pattern for processing streams asynchronously with non-blocking return pressure. Reactive Streams was started in 2013 by engineers from Netflix, Pivotal, Lightbend (formerly Typesafe), Red Hat, Twitter and Oracle. The Reactive Streams API defines 4 interfaces: Publisher, Subscriber, Subscription and Processor. Spring WebFlux is the support module for reactive programming in the Spring Framework's web layer. One of the most interesting things about this module is that even though the architecture behind it is quite different from the conventional Spring MVC, it can be used as an alternative to Spring MVC by utilizing some of the most relevant Spring MVC features: the annotations @Controller and @RequestMapping. We can see this in the following image: Spring Framework 5 uses Reactor - a Reactive Stream implementation - to provide two Reactive Streams Publishers called Mono and Flux. The first of these is responsible for managing operations in data sequences of 0..1, and the second manages operations in data sequences of 0..N. These publishers are exposed in many of Spring Framework's own APIs, but it also provides full support for using RxJava at the application level. Now let's look at the differences between a REST CRUD service implemented with Spring MVC and one implemented with Spring WebFlux. In order to show the difference, we are going to walk through implementing three layers in each module: Repository, Service and Controller. The difference here is merely extending from ReactiveMongoRepository instead of MongoRepository. Here we can see some small differences. For instance, instead of using Person object, we change to Mono<Person>, and we can also change List<Person> to Flux<Person>. Beyond this, in the Service save method, we needed to call block() method in the Mono<Person> parameter to convert the stream into an object, since the ReactiveMongoRepository<Person, String> save method expects us to pass a Person object. That's how we ask the data stream to perform a blocking action and give us the object to be saved. Similarly to the changes we made in the service layer, here we need to replace Person with Mono<Person> and List<Person>  with Flux<Person>. In this post, we discussed a little bit about what reactive programming is and saw that v ery few changes are required to migrate from Spring MVC to Spring WebFlux. If you are not convinced yet about why you should consider using Spring WebFlux and reactive programming, here are some benefits: it's simpler than regular threading; leads to code are easier to understand, test and debug; it makes the code more maintainable; concurrency handling is hassle-free and composing asynchronous operations is straight forward. You can learn more about its benefits here . Of course, not everything is perfect--you might have to be careful to not create memory leaks, and you probably will need to change the way you think to actually understand how to solve problems using reactive programming, but I think starting with Spring WebFlux can be a good way to start understanding reactive programming. If you'd like to see the full project from both services, you can take a look here . DZone, Spring WebFlux: First Steps Medium, Por Que Programação Reativa?", "date": "2018-10-17"},
{"website": "Avenuecode", "title": "Centering Content with CSS", "author": ["Eduardo Silva"], "link": "https://blog.avenuecode.com/centering-content-with-css", "abstract": "A few minutes surfing on the internet is already enough for us to see how common it is to have centered elements in a web page layout. But how to do it? How to have elements perfectly centered in the web page? This article aims to explore a few CSS alternatives on how we can get the job done. First we are going to talk about horizontal alignment, and then we’ll explore vertical alignment. Having horizontally centered elements is quite useful in the world of web. This kind of positioning can be used to align text, images or any other type of element. A good example is a logo centered at the top of the page within a header. We can see one example below: The picture above shows an example of a simple blog post layout where we have a header with the company logo centered at the top of the page. As an exercise, let's see how we can achieve the logo alignment proposed in this example. First we need to understand the header HTML structure. We can see it in the code snippet below: The HTML structure says that we only have a <header> element wrapping an <image> element. Pretty simple so far. The picture below shows what we can see before the logo alignment: In this case, it's very simple to get our logo centered, because we are simply talking about moving one element on the horizontal axis. To make this happen, we can use the CSS property text-align with the value center on our header element. Of course this would cause all the elements inside the header to be positioned in its center, which doesn't matter in our case since we only have our logo standing there. Our CSS code would look something like this: What is important to us here is the property text-align: center which is responsible for our magic! The question now is: what if my logo is not an image element, but it is only a div with the logo as a background? Would our simple approach still work? Let's take a look at our new HTML structure: In the above HTML snippet, we only see a div with the \"logo\" identifier. The image is now shown through a background CSS property, like below: If you refresh your browser, you will see that we still have our logo positioned on the header's left corner, although we never removed its text-align: center property. This result leads us to think that we need a different approach in such situation. But what that would be? Fortunately, it is still a simple one. The CSS snippet below shows what we did: If we pay attention on what is new, we are going to see that the only change we had to do was in the logo element. Since a div is, by default, a block element (which means it will occupy 100% of its horizontal space), we had to tell it to display as a inline-block element instead, by adding display: inline-block. This CSS property only tells the element to display with both inline and block characteristics, which in our case, means the logo element should not fill up the entire line. The good news is that this second approach also works fine with our first HTML structure, which had an image element inside our header. Now that we’ve spoken about how to make an element to display horizontally centered, we need to talk about how we can reach the same state on the Y axis. Vertical alignment is very useful when we want to create full-width home pages with a centered content, such as text or simple form elements to gather user emails for example. In our practical example, we are going to try to add the Avenue Code logo to the center of the page to drive users’ attention right off the bat. Our result will look something like what is shown in the picture below: Our HTML structure will look something like this: Based on the result we are expecting above, let's dive into the methods: In this method, we are going to make use of the position: absolute property so we can float our logo anywhere in the page. We will then move the element 50% from the left and another 50% from the top. This will guarantee our top left corner is precisely centered and aligned in our page. However, we don't want to centralize its top left corner, but we want to have the logo's center exactly in the center of the page. In that case, we need to check the image size, divide it by 2 and finally apply the value as negative margins as shown below: In our case, our logo is 70x70px, which means, we need to adjust our negative margins by -35px in order to obtain the desired result. Now our logo displays exactly in the middle of our container. However, as we can see, we are dependent on the size of the image in this case, which couldn't be the case, because there will be situations where we won't know how big our image/content is. What can we do in such situation? In this case, we will need to appeal to the second method: In our second method, the size of the logo is not important anymore. We are now making use of the translate method, which will move the element as much as we want. In our case, we are translating (moving) the element by -50% both left and top so we can adjust it to center. The code will look like this: For simplicity, we are not considering crossbrowser compatibility. We are only adding the transform property. But you can make use of frameworks or vendor prefixes in your code, to make sure you are covering as many browser versions as you can. This is a simple method where we try to remove all margins and set it as auto. It's a simple and powerful method - perhaps my favorite one: This method requires us to handle the logo itself instead of its parent/wrapper as we saw in the other methods. However, it's clean and effective. Flex is a great add-on for CSS. It makes the developer's life much easier when speaking about element positioning and layouts. Unfortunately, only the most recent browsers will support it. Flex is a topic that deserves a dedicated article, so for now, we are only going to show how we would achieve the same results by using its methods. In this case, we are defining our .hero container as a flex component by using the display: flex property. We also need to set its height. In our case we are telling our hero to occupy the whole vertical portion of the screen. Then we are adding justify-content: center so all items will be centered through horizontal axis. Finally, we are telling the browser to also centralize the content vertically, by using the property align-items: center. Again, for the sake of simplicity, we are not considering cross-browser compatibilities when using the flex properties, as this information is readily available elsewhere. The question of how to center elements using CSS is a question that comes up very often. In this article we saw that we have a number of ways to do it and each one has its own complexity or simplicity. It's up to us to define which one best fits our needs.", "date": "2016-10-12"},
{"website": "Avenuecode", "title": "Enabling Rich Previews of Shared Links", "author": ["Mark Carlson"], "link": "https://blog.avenuecode.com/rich-previews-of-shared-links", "abstract": "A hands-on look at enabling rich previews of links in iMessage, Facebook, and beyond. First published on January 9th, 2017 at markcarlson.io The iMessage app in iOS 10 provides rich previews of links when they are properly formatted to take advantage of this new feature.  In addition, these same links will contain rich previews in other apps too, like Facebook, Reddit, WhatsApp, Skype, Twitter, and others with the same configuration. In the screen shots below, you can see how these simple links to apple.com and yahoo.com display large logos and titles when they are shared through iOS's iMessage app: The rich previews inside iOS's iMessage app are generated entirely by a couple of Facebook's open graph tags .   These tags are placed inside the <head> section of a website.  Facebook supports several of these tags, but iMessage only uses 2, \"og:title\" and \"og:image\" For example, take a look at this simple html page: <html> <head> <meta property=\"og:title\" content=\"Mark's Favorites at AEO\" /> <meta property=\"og:image\" content=\" http://markcarlson.io/wp-content/uploads/2016/12/Heart_font_awesome.svg.png \" /> </head> <body> <p>Web page content goes here</p> </body> </html> Even though this page generates a simple site with only one line of text, it will generate the following rich preview in iMessage: Title text length is limited to around 44 characters in iMessage.  Additional text is clipped and replaced with ellipses.  GIF and dynamic images don't work when specified in the image tag.  The recommended preview image size is 1200 x 1200 or larger.  Reddit uses a 70 x 70 square image for its thumbnail.  Images of all sizes are automatically resized down to this.  Facebook recommends 1200 x 630 for og:image dimensions.  This gives your shared post a full-size image above the post text.  Image resolution as low as 600 x 315 will also be handled this way. Let's see what happens when we try to share the same site with a full 1200 x 1200 image on Facebook: Clearly that is not ideal.  Facebook has cropped our 1200 x 1200 image down to 1200 x 630.  Now that we know some social media will display a square and some a rectangle, it's up to us to create an image that will work well in both.  Let's try something like this (I've added a background to show where 1200 x 630 would fit inside this 1200 x 1200 image.): This image is 1200 x 1200, but the main content is contained within a 1200 x 630 space.  Now when we share in iMessage, we get this: Not too shabby.  Best of all, this same image works beautifully with Facebook as well: What happens if your og:image is much smaller 1200 x 1200?  Let's take a look.  Here's the same image, saved at 150 x 150 which is the smallest size supported by iMessage: Here's what we get when we share on a site that uses 150 x 150 og:image size: Although it displays an image at this size, it's getting visibly pixelated on a retina display.  How does this same image look in a Facebook share? As you can see, the best we get at this resolution is a smaller image on the same line as the post text. Facebook open graph tags can greatly enhance the experience of link sharing on mobile devices and on social media platforms.  Improve your user experience with visually enhanced links to share.  Remember to use a 1200 x 1200 image that will look good both square and cropped to 1200 x 630. Then add 2 meta tags at the top of your HTML, to increase the chance of additional clicks and higher conversions!", "date": "2017-2-15"},
{"website": "Avenuecode", "title": "AC Spotlight - Erik Huberman", "author": ["Anna Vander Wall"], "link": "https://blog.avenuecode.com/ac-spotlight-erik-huberman", "abstract": "Erik Huberman, Founder and CEO at Hawke Media, shares how companies can position themselves for success during COVID-19 and beyond. Avenue Code: Tell us about your personal career path. How did you get to where you are today? Erik Huberman: I graduated in 2008, exactly one week to the day before the last recession. That year, I worked in commercial real estate in L.A. and only made $350! I started working on alternatives and ended up launching an online music company providing one-on-one coaching for musicians, raising 1 million dollars in venture capital at the age of 22. Although the business was profitable, I realized it wouldn’t grow as much as I wanted since the audience was financially limited. So I hired a CEO to take my place and started an e-commerce t-shirt subscription company. This was before Dollar Shave Club and ShoeDazzle, so it was a completely innovative business model. I got a lot of press, which drove a lot of revenue, and sold the company a year and a half later. I was then offered three jobs: 1) running e-commerce for Warner Music, 2) running BusDev for Live Nation, and 3) consulting at a small incubator called Science. I chose Science, where I helped launch several e-commerce brands and ran marketing full time for Ellie, an activewear brand that we sold a year later. Then I started consulting on how to drive revenue growth using marketing. I worked with big brands like Red Bull, Verizon, and Eddie Bauer, as well as several startups, and I saw that every company dealt with the same problem: getting access to top marketing talent. Whether marketing in-house or through a typical agency, it’s hard to attract great talent while remaining cost-effective. So I started Hawke Media six and a half years ago with a small team of experts. We offer à la carte, month-to-month services that are cheaper than hiring in-house. We can spin up a team that fits any need based on a comprehensive menu of services. Now we have over 150 people and offices in New York, L.A., and Boston. AC: Throughout your career, you have identified and seized opportunities. Are you in marketing because that’s where you saw opportunities, or did you see marketing opportunities because it’s what you love? EH: I love the study of human psychology and sociology inherent in marketing. For example, it’s interesting to watch how the COVID-19 situation has completely changed the business landscape. People aren’t paying attention to anything else. So if your company’s marketing strategy is still what it was two months ago, you’re probably not going to do well. AC: Hawke has grown more rapidly and received more attention than many of its competitors. What makes Hawke so successful, and how can other companies use marketing to set themselves up for success during COVID-19? EH: A lot of agencies preach marketing but don’t drink their own punch. We went to a Google meetup recently with dozens of marketing agencies. We were by far the biggest company, and we were also the only company spending on our own marketing! If you can build a brand, a community, and a funnel, you’ll be much better prepared to sustain a recession because you can find new customers that aren’t economically affected. We are experiencing some client turnover with COVID-19, but because we’re skilled at marketing, we can pivot and change with the times. We don’t follow a template, whereas a lot of our competitors are stuck in what they’re doing, meaning they’re victims to change. Right now we’re telling a lot of our brick and mortar clients to focus on in-home delivery, and it’s working: we recently looked at our Amazon accounts across the board, and we saw a 1% revenue increase, not a decline, for brick and mortar shops. Last year, 87% of purchases were done in brick and mortar stores, 13% online. Now, people are afraid to go into brick and mortar stores, but they’re still buying. There’s going to be a pendulum swing to e-commerce, which we saw in Wuhan and a lot of China. The economy hasn’t stopped; businesses just need to pivot. AC: A lot of companies pay lip service to marketing, but it’s often the last thing to be added when companies are growing and the first thing to be cut when budgets are scaled back. Why is that? EH: When companies operate out of fear, they lose strategy. There are a lot of studies coming out right now that portray three types of CEOs: 1) the fear-based CEO who cuts everything, 2) the unfocused CEO who is just trying to figure out what to do, and 3) the strategic CEO, who doesn’t panic and instead uses marketing to explain why their services are valuable. Cutting off marketing when sales are declining is like holding your breath because you were told you’re sick. It exacerbates the problem. You have to pivot and push, not the opposite. This is what we’re doing at Hawke. We haven’t pulled back on anything, because the other option is to give up. In a situation like the one we’re facing today, there are only two outcomes: either everything blows over in a few months, just like it did in China and is starting to in Korea; in this case, companies shouldn’t cut themselves off, because when the economy comes back - they won’t. Or the economy continues to decline, and this becomes the worst pandemic in history; in this case, spending on marketing is going to be the least of our problems. AC: There are always ups and downs in the economy. How can companies set themselves up for success during downturns beyond COVID-19? EH: Keep a rainy day fund. What I’ve seen with a lot of businesses that are struggling is that they have to operate on a weekly basis because they run so thin. Having capital available helps you sustain recessions and make major investments in up cycles. A bigger balance sheet allows you to make longer-term decisions and avoid being reactionary. AC: Once you’re already in an economic downturn, what should you be doing to set yourself up for success coming out on the other side? EH: Cut all non-essentials costs, like office massages and other perks. Comb through your P&L and keep everything that’s focused on retaining current customers and generating new leads. Everything else should be out the window. Figure out how to make yourself essential to your customers, and double down where it counts. This is a time for marketing to prevail, because companies are at a critical point where they have to find new customers. AC: Companies can be wary of hiring consultancies, especially when it comes to branding. What are the biggest advantages to outsourcing services that aren’t core competencies? EH: Whether you’re working with an agency or building a team in-house, you’re working with people, so you have to explain your brand either way. Usually hiring in-house is an emotional decision. We’ve taken companies from 3 million to 139 million, only to see them decide to use their extra revenue to bring marketing in-house. Then a year later, everything falls apart because they lose access to outside perspective. My favorite example of losing perspective is when the CMO of Pepsi decided to eliminate agencies and then immediately launched the infamous Kendall Jenner ad. At Hawke, on the other hand, all of our teams start every day with a 30-minute training, and we have Google and Facebook in our offices training our staff almost every week. We meet regularly to enforce core business practices, share discoveries, and learn from mistakes. A year ago, Facebook was pretty dramatically in decline across the board. Companies marketing in-house didn’t know if the decline was due to their tactics or if everyone was struggling, whereas we saw the big picture and had better perspective on how to pivot. AC: What was your biggest “Aha!” moment in the last 6 months as it relates to marketing? EH: Marketing is and has always been about creating emotional appeal for buyers who want to change their state from where they are to where they want to be. It’s worked the same way for 2,000 years, and no new tool or strategy will radically change it. We’ve been watching a lot of D2C companies that overspent on Facebook ads fail. This validates what we’ve always known: dumping money into Facebook ads to drive revenue growth is not the way to build a brand; you have to look at marketing holistically. For us, Facebook is 15% of what we do, whereas for most agencies, it’s 80%. If your goal is to find new customers constantly, that’s unsustainable, but a lot of companies are so focused on customer acquisition costs that they forget to build a community. You have to be able to build trust, keep your customers, and create lifetime value. A positive example is FabFitFun, which got to 250 million in revenue on 6 million in funding. They are great at advertising, but they also keep their customers and make them happy. In fact, they have a whole media business that has nothing to do with their core product because they understand the importance of building community. AC: Hawke yet again made the Inc 5000 list for fastest growing companies. With such rapid and sustained growth, where do you see Hawke in five years’ time? EH: Hiring locally is important for many of our clients, so we’re looking at expanding to new territories in the US as well as internationally. Right now, there are no large marketing agencies that work with SMBs, whereas we work with everyone, both name brand and small-to-medium. Hawke’s company vision is accessibility to great marketing for everyone, and part of that vision means expanding to new geographies. AC: Thanks, Erik, for sharing insights on core marketing principles, as well pointers on how companies can pivot and change during today’s economic realities. We look forward to following Hawke’s next steps!", "date": "2020-4-22"},
{"website": "Avenuecode", "title": "Simple Deployment Architecture in Mule", "author": ["Anupam Gogoi"], "link": "https://blog.avenuecode.com/simple-deployment-architecture-mule", "abstract": "One of the biggest challenges in system development is choosing an Architecture for the system. There is not any predefined architecture that can be applied directly to your projects. In this article I would like to discuss a simple Mule deployment architecture. Avenue Code is the preferred systems integrator for MuleSoft Anypoint Platform due to our expertise with complex, large-scale e-commerce systems. Check out our guide on simple deployment architecture, and feel free to talk with us about our solutions for your organization. First, let's discuss a common problem. Over the course of time, projects/modules often increase in size due to the importation of lots of external libraries. Let's assume the size has increased to a few gigabytes. By this point, the time it takes to generate the deployable archive, copy, and deploy it in Mule Runtime would be enormous. So in this article, I will outline a solution to this challenge. Let's discuss how classloaders work in Mule Runtime to set the stage. As you may already know, Mule has a hierarchy of classloaders. Have a look at the diagram below: At Mule startup, the Java bootstrap classloader will load the necessary classes of the JDK. Then the Mule system classloader will load the classes present in lib/mule and lib/opt directories. Subsequently, the Mule shared domain classloader will load the classes present in each of the domains being used in the applications, which will search classes in the lib/shared/<domain-name> directory. Finally, the Mule application classloader will load classes bundled within the application. Here is the screenshot of the directories as they appear in Mule Runtime: As our diagram shows, the Mule shared domain classloader loads the classes  present in domains used in the applications. Furthermore, it looks for the classes in the directory lib/shared/<domain-name>. This is the key! For example, our application depends on 100 external libraries (jar).Now each time you want to deploy the application in Mule Runtime you make a zip file along with 100 external jars. The size of the archive will, of course, be huge, and consequently so will the up time. The solution to overcome this problem is to: Create a domain project and use this domain in the application/s. Let's name the domain as app-domain . In the Mule Runtime, create a folder under lib/shared directory with the name of the domain created in step 1. E.g., lib/shared/app-domain. Copy all the jars necessary for the application/s to the directory created in step 2. In the application/s, while generating the deployable zip file using maven, DO NOT bundle the libraries with the application. In other words, use provided scope in the dependencies in the pom.xml. So, now each time you create the deployable archive (zip), the archive will be much smaller and easier to deploy. In addition, all the libraries that are copied to the lib/shared/app-domain directory will be available to all the applications using the domain app-domain . The following items are required for the tutorial: Mule Community Runtime 3.8.1 Mule Server 3.8.1 CE plugin installed in Anypoint Studio. The complete source code can be found in the repository . I have a parent pom project where I am declaring all the dependencies necessary for the projects related to a domain ( app-domain ). Let's suppose that our applications in the domain (app-domain) will depend on only one jar (common-jar.jar). Here is the complete pom.xml : Note that the scope of the dependency is provided. This means that the dependency will not be bundled with the final archive. NOTE: I have copied the common-jar to lib/shared/app-domain. Now I have created a domain project named app-domain . Here is the pom.xml The only thing we are doing here is declaring the parent pom. I have created two projects: app-one and app-two . In both projects, I've declared the parent project, such that the dependencies declared in the parent project (common-jar) will be automatically imported to app-one and app-two. In the app-one project I have declared a specific dependency (demo-jar). Here is the pom.xml : You can see that I have put the dependency scope as compile - this means that the jar willl be bundled with the app-one deployable archive. This one is a simple project without any extra dependencies. It's just using the common dependency inherited from its parent . Here is the pom.xml . Create the deployables for each of the projects (app-domain, app-one and app-two) using mvn install command. Deploy the domain in the domains directory of the Mule Runtime. Please note that it should have the exact same name as app-domain. Remove the version information if there is any. Copy the app-one and app-two zips to the apps directory - and that's it! If Mule Runtime is running it will deploy the applications properly. To check the applications, just hit the urls http://localhost:8081/app1 and http://localhost:8081/app2 . Get in touch about how Avenue Code can guide you as the preferred systems integrator for MuleSoft Anypoint Platform!", "date": "2017-6-28"},
{"website": "Avenuecode", "title": "AC Spotlight - Yasi Baiani", "author": ["Anna Vander Wall"], "link": "https://blog.avenuecode.com/ac-spotlight-yasi-baiani", "abstract": "(The following article is property of Avenue Code, LLC, and was originally published with permission at Women in Retail Leadership Circle on March 8, 2019 .) Yasi Baiani, Vice President of Product Management at Wonolo , shares how she maximizes impact as a leader by pursuing her passion. Avenue Code: Yasi, one of our favorite things is to interview driven, successful women like yourself. Tell us a little about your career path. Did you always know you wanted to be an executive? Yasi Baiani: Growing up, I definitely didn’t have any idea what being an executive meant. But looking back at my childhood, it’s clear that I was always a “boss girl.” From leading all the simple games at elementary school to starting my first business when I was 14, I always had leadership drive. That innate drive led me to take on more and more responsibilities at each stage of my career, which meant I was always quickly promoted into leadership roles. AC: Do you have any tips to share with others on how to identify and pursue their passions? YB: I’ve been able to position myself to work on things that I’m passionate about by knowing my goals and values well and by constantly educating and re-inventing myself. My advice to others is to be open and honest with yourself about what makes you tick. Prioritize pursuing passion and happiness over big financial packages. If you follow your passion, you’ll most likely do big things in your field, and financial rewards will follow. More importantly, though, you’ll be happy and satisfied day-to-day, which in turn allows you to feel accomplished both personally and professionally. Another helpful practice is to write down your short-term (6 to 12-month) and long-term (2 to 5-year) goals. Then religiously refer to these to ensure you’re on track. These goals should guide you to only take on the opportunities that are aligned with your values and/or will help you reach your goals rather than distracting you from them. AC: What have you found to be the most successful approach when it comes to creating captivating products? YB: The most captivating products solve real customer problems in a really big market, and do it in a way that resonates with people. I always emphasize that understanding the underlying psychological needs of consumers, not just their surface needs, is key to building successful products that scale. In order to tap into the underlying psychological needs of consumers, product managers use various resources, including UX research, market research, data, and consumer service reports. Another factor that’s essential in launching successful products is having a team of designers, engineers, scientists and business professionals that can and will deliver high-quality products over and over again. No matter how brilliant your idea is or how much demand there is for it in the market, your product launch will only be successful if your team feels motivated and obligated to deliver all the pieces you request in a high quality and timely manner. One thing that has empowered me to motivate my teams, even during crunch times, is to paint the big picture for them by communicating the “why” of the consumer story instead of just telling them “what” to do. Humans, by nature, are a lot more invested and interested if they understand why something they’re asked to do is so important and how their work translates into business values. AC: You seem to have cracked the code on the best ways to manage teams and influence people. Any other secrets you can share with us about leadership? YB: Empathy, which means seeing things through the lens of another, is a skill that’s key to success when working with anyone. However, it’s not getting as much attention as it should in the workplace. Empathy is an attribute that’s innate to me, but I constantly try to get better at it, too. It’s been fundamental in making me an effective product manager and leader because it allows me to have a leadership style that’s not one-size-fits-all. Instead, I try to understand what motivates each individual. This is key for product leaders, as we rely on so many teams to do their jobs well and on time in order to have a successful product launch that performs in the market. AC: As you’ve grown in your career, you’ve always gone the extra mile to give back by sharing your knowledge as a speaker and writer. Where did this drive come from? YB: A lot of it is natural. Making an impact has always been important to me, so I like teaching and mentoring. As a career woman, I know how hard it is to jump the professional hurdles. I see it as my responsibility to inspire others, especially all girls and women, to achieve their goals, to think outside the box, and to be the best version of themselves. Yasi Baiani participates in a panel of experts speaking about big data, sleep, and wearable devices. We’re in an era where social networks like LinkedIn enable this. I always advise people to find their passions and get their thoughts out there. While it wasn’t part of my strategy, doing this gave me exposure and created a brand around my voice. I recently traveled to the Middle East and was recognized for my work, which made me realize that the world is really small and that you can easily impact it by putting yourself out there. AC: Do you have any advice for others on how to build a personal brand? YB: Find a topic that you’re passionate about, and build expertise around it. Take a stand and be outspoken. Be strategic and lean toward sharing positively vs. venting your frustration. Self-educate so you can continue to have something interesting to say. Above all, be purposeful! My purpose for being an advocate and speaker is to inspire others, to share best leadership practices, and to bring attention to important trends and topics in technology and digital health. AC: Yasi, it’s inspiring to hear your thoughts on impact-driven leadership and product management.", "date": "2019-10-2"},
{"website": "Avenuecode", "title": "AC Spotlight - Luis Phelipe Castro", "author": ["Andressa Lopes"], "link": "https://blog.avenuecode.com/ac-spotlight-luis-phelipe-castro", "abstract": "Luis Phelipe Castro, CIO & IT Director at Coca-Cola Brasil, discusses Coca-Cola’s digital evolution initiatives, AI implementations, and response to COVID-19. Avenue Code: Tell us about your career journey as CIO at Coca-Cola. Luis Phelipe Castro: I have worked at The Coca-Cola Company for almost 30 years, and though it sounds cliché, I like to compare it to a relationship: both parties need to continue to invest. During this period, no year has been like any other. Working at Coca-Cola is challenging, and I am always motivated to do my best. I always need to stay up to date since technology changes at an impressive speed, all while improving my soft skills since I am also a manager striving to bring out the best in my team. AC: What does Coca-Cola’s digital transformation journey look like? LC: Staying competitive in this new digital world is a challenge for any company, especially for one founded over 130 years ago! Our business models have evolved and adapted over time, and digitization is another step in this process. Within Coca-Cola, we understood early on the need to adjust how we work and use technology to our advantage. We started by betting on three basic pillars: relationship with the consumer, e-commerce, and advanced analytics. As part of this process, we formally created a Digital Transformation area with which we collaborate closely to implement new ways of working that are agile, innovative, and less bureaucratic. Of course, this process started small, with a few select projects. Today, probably more than 60% of the company is working in multifunctional groups in an agile way, demonstrating that the company's culture is already adjusting to this new reality. AC: We saw that Coca-Cola USA created a powerful AI solution with low memory usage in cell phones to boost customer loyalty. Can you tell us a little more about the project and whether there are plans to implement something similar in Brazil? LC: We have numerous initiatives using artificial intelligence in our business units around the world, including here in Brazil. I would like to mention two projects that are emblematic: the first is Coca-Cola Freestyle, a project from our US business unit that started in 2009. This allows consumers to customize their drinks on soda machines by combining different flavors and brands from our products, creating unique combinations. Since these machines are all connected in our corporate network, we are able to capture in real time everything about consumers' choices; we then use AI to generate important insights into consumer trends to inform new product development. Finally, consumers can save their preferences through an app so that they can replicate their drinks in the future on any other machine. The other project, implemented here in Brazil, is related to customer service. In 2018, we launched KORA, a robot with which consumers can chat via WhatsApp. KORA answers questions about our brands and products. AC: We recently saw that Coca-Cola donated $120 million for actions to combat the coronavirus. Can you tell us a little more about this? LC: Yes, the company donated US $120 million globally via the Coca-Cola Foundation. Locally, the Coca-Cola Brazil System - which comprises our bottlers and the Coca-Cola Institute - allocated R $45 million to combat the pandemic. Our action plan features a fund to contribute to the generation of minimum income for low-income communities and waste pickers, who are among the populations most affected by social inequality and job informality, especially in a pandemic situation. The objective is to help combat COVID-19 in 70 communities in 14 states and the Federal District, where approximately 5.2 million people live. The resources - around R $ 30,000 per community - are being spent on raising awareness, preventing contagion with the purchase and distribution of hygiene products, protecting and cleaning, and purchasing food, beverages, and basic food baskets. For waste pickers, the focus is on helping to guarantee a minimum income to around 11 thousand cooperative and self-employed people with R $600 per month for a period of two months. Based on the experience of other countries impacted by COVID-19, we also decided to donate Crystal mineral water to healthcare professionals with our manufacturers. The action, which focuses on field and referral hospitals fighting against the pandemic, is distributing more than 1.8 million bottles of water to 79 hospitals across Brazilian states. The water is being delivered in individual packages to safely serve medical teams on duty and in emergency situations. In partnership with Coca-Cola Femsa, Bradesco Seguros and Grupo Fleury, we collaborated in carrying out 26 thousand diagnostic tests of COVID-19 for health professionals in the State of São Paulo. Also with Coca-Cola Femsa, plus Braskem, Fitesa, and Renner, we make our fleet available for the distribution of 600 thousand protective masks and 83 thousand hospital gowns in hospitals and health institutions in the states of Rio de Janeiro, São Paulo, Santa Catarina and Rio Grande do Sul River. Our bottlers have made substantial donations of beverages, food, packaging, and alcohol gel to communities and healthcare institutions in all regions. These include: There was also an investment of R $2 million for the purchase of equipment for ICUs in the Federal District made by the bottler in the region. We are distributing 100,000 R $8 vouchers to be used in any order to support 54,000 small restaurants registered with iFood. We also use Coca-Cola packaging and trucks from our fleet to inform people about how to prevent the disease. AC: What are your thoughts on the future of business after COVID-19? LC: I am absolutely sure that in various aspects of personal and corporate life, we ​​will not be where we were before the crisis. We have learned to work in a completely different way - remotely - and we will hardly return 100% to our old routine. We are learning to continue business, including the sale of our products, with very little contact, a fact that was almost unthinkable in times not so distant, when the physical presence of a seller was an important part of customer relations. Today, more than ever, I am sure that this is a path of no return, and continuing to rethink the way we work will be a constant from now on. AC: What is one of the highlights of your career at Coca-Cola? LC : In terms of concrete achievements, I am very proud of the tools and systems that I helped create. For example, Brazil was the first country within Coca-Cola to have our manufacturers' daily sales information available by product, channel, and customer. In 1997, this project was a watershed because it allowed us to have visibility of results, which was hitherto impossible. Another thing I am very proud of during all these years at Coca-Cola is to have participated closely in the development of people, many of whom I hired as interns and who are now in prominent positions inside and outside Coca-Cola. AC: What is the culture of Coca-Cola in relation to partnerships, taking into account challenging situations like the current one? LC : Partnerships are natural actions and very present in our history. We partner whenever there are benefits for everyone: for us, our partners, our customers, and our consumers. The most emblematic example that I can mention is happening now, and it is the union of the companies Ambev, Aurora, BRF, Coca-Cola, Heineken, Mondelez, Nestlé, and PepsiCo in the Movimento Nos. This coalition aims to help small businesses in Brazil at this critical time. They are small companies that do not have the resources to sustain their operations, so they are taking serious risks during the pandemic, and this is the moment to unite for a single objective: to help the country through this period with the least possible impact. AC: Thank you for your time today, Luis! It’s inspiring to hear how Coca-Cola is making a difference during this uncertain time.", "date": "2020-7-15"},
{"website": "Avenuecode", "title": "AC Spotlight - Trevor Hammond", "author": ["Alfredo Moro"], "link": "https://blog.avenuecode.com/ac-spotlight-trevor-hammond", "abstract": "Trevor Hammond , Technical Director at Behaviour Interactive, discusses how cloud computing has revolutionized organizational practices and how it will continue to impact the gaming industry. Avenue Code: Tell us about your personal career path. How did you get to where you are today? Trevor Hammond: I think I was destined to go into IT and gaming from a very young age. As a child, I read PC Gamer magazine before my family even owned a computer. I recall my parents leaving me for five or six hours at a grocery store kiosk where you could pay for internet access as my first online experience, and I didn’t want to leave. We finally purchased our own computer when I was around 12, and the first thing I did was disassemble it to figure out how it worked. I skipped university entirely. After completing a network administration course, I did SMB corporate network system administration, which gave me exposure to everything from CNC programming machines to routing cables to building computers. This led to a job at Puma doing system administration and managing our global network for all stores in North and South America. During this time, I began freelancing in software development and decided to go that route full time. My former employer became my client alongside several other SMBs. This led to an interesting opportunity at a startup company called Beyond the Rack. They didn’t have a traditional data center and needed to scale very quickly, so they became one of the early adopters of cloud on Amazon AWS. We grew from about 12 employees to over 100 employees and a million members on our website within a year. When Beyond the Rack hired a new CTO, I decided to move on. I took a role that paid well but wasn’t exciting before coming across an Ubisoft job posting on Craigslist, of all places. I applied, fully expecting the post to be a scam. Working at Ubisoft had always been a dream of mine, and I applied in past years without hearing back. This time, I got the job quickly. Ubisoft had cloud ambitions, and I helped them implement public cloud, private cloud, hybrid infrastructure, etc. This was at the time when social networking grew popular. The first game I worked on was CSI: Crime City, which was a Facebook game. We needed a quickly scalable infrastructure, because we saw a trend where there would be a critical mass at launch time and then users would drop. Since it wasn’t a AAA game, we decided to risk using cloud infrastructure. It went very well, so we continued using the cloud more and more. Today nearly all critical workloads run in cloud. AC: What drew you to joining the Behaviour Interactive team? TH: I was at Ubisoft for over ten years, and as much as I enjoyed it, I’m very challenge-driven and wanted to move forward. I was drawn to Behaviour Interactive for several reasons: I wanted to stay in gaming, I am a big fan of Dead By Daylight (their best known game), and they were both well established and ambitious about future growth. Now I’m in technical management, bridging the gap between software and architecture. AC: From an entrepreneurial perspective, what were some of your key learning experiences throughout your career? TH: One of the things that stayed with me from my freelance experience was the idea of representing myself. Even when working for someone else, I consider myself Trevor Hammond, Inc. I represent the company I work for, but first and foremost, I represent my own standards of excellence and values. This helps me to be passionately engaged while disconnected from emotional investment. It’s a balance. AC: What are you personally most passionate about in your career? TH: I’m passionate about solving new challenges. I love it when leadership comes to me with a new idea and I get to figure out how to create a solution. Unlocking a challenge is the equivalent of an adrenaline rush for me, and it’s addictive. I’m still just as fascinated by games as I was when I played 2D, 8-bit, pixel art games as a kid, so problem solving in this space and creating the future of games using cloud is a dream. It’s interesting to note that gaming is one of the most challenging industries to scale due to high demands for concurrency, performance and latency. AC: As an early cloud adopter who is currently leading infrastructure teams focused on public cloud, what cultural shifts have you observed related to cloud? Do companies still need to be convinced of its benefits? TH: Cloud started as an almost mythical concept. A lot of companies put their less risky investments in the cloud to test it. In the gaming industry, a lot of people didn’t and to a degree still don’t trust the cloud fully. We’d primarily use the cloud to scale for millions of consumer users, not for internally-facing initiatives. Over time, I saw two major cultural changes regarding the cloud. First, the DevOps evolution that occurred around 2015 started to drive how companies were perceiving cloud differently. DevOps teams wanted to go forward fast where results were happening. This caused a shift in how IT departments were perceived. The red tape of opening a ticket and then waiting for it to be resolved wasn’t acceptable anymore. IT was forced to evolve toward an agile, service-oriented model, removing the heavy lifting for teams so they didn’t have to recreate the wheel every time. The second big shift we’ve seen is that companies have become more comfortable with the prospect of putting internal applications, unannounced game data, etc. in the public cloud. Some of this has to do with technological advancements like the advent of software-defined networking, and some of it is simply that the cloud has become normalized for users over time. AC: What challenges and opportunities arose for Behaviour Interactive post COVID-19? TH: There are two major challenges for the gaming industry. The first is bandwidth. Games have a lot of data, so it’s hard to access everything you need from home. We did a lot of quick transformations to things like smart proxies in the cloud, offloading from corporate VPNs. We also started using remote streaming solutions to test games. The other challenge is infrastructure security - how do you send 10,000 people home to work on unannounced game titles without creating security holes or leaking information? I was at Ubisoft at the time, and fortunately, we were already moving forward on our hybrid transformation, so we were in a very good position to implement and advise people on best practices. Interestingly, the pandemic led to better collaboration between teams, and it felt like they understood one another’s requirements and needs on a deeper level. From an opportunity standpoint, we gained a lot of users because games are a cost-effective form of at-home entertainment that also have a social aspect. In fact, many games are doing better than ever during the ongoing pandemic. AC: What trends do you see within the gaming industry as a whole? TH: There’s a global trend toward games as a service (GaaS). Rather than annual franchises or games that have a peak and then eventually die away, the revenue model for multiplayer games is iterative updates between new or customizable characters, maps and seasonal features. This doesn’t translate well to single-player games as much, but a lot of multiplayer games are now being built from the ground up with a long-term model in mind. Another big trend is streaming. The streaming model for games will be similar to our streaming model for movies. This will drive a lot of developments. On the downside, it might lead to walled ecosystems. On the up side, the amount of people who will have access to games will increase dramatically. I don’t think streaming will take over completely for another 7-10 years, at which time next generation game consoles will likely support streaming-only editions. The industry itself will become hypercompetitive, which always yields good results for consumers. Once you’re streaming from a data center, you also have low latency to massive computing and GPU capacity. There is substantial opportunity to leverage that to create innovative features based on natural language processing, dynamic hybrid AI backed by machine learning, massively multiplayer experiences, and so on. The last major trend is cross-play and cross-progression. This allows people to play with their friends across gaming ecosystems and maintain their progress as they do so. More and more gaming companies are implementing these features. In fact, Behaviour Interactive just launched cross progression for Dead by Daylight ! AC: What is the key to successful strategic partnerships? TH: Don’t fall into the trap of playing the role of “vendor” or “client.” Because everyone has a stake in the deal and expects a certain outcome, partnerships can easily become transactional. High-level executive teams brokering a partnership generally have good synergy, but that needs to carry to the grassroots of the teams working on an effort to achieve full potential. If you don’t have this in partnerships, you may have a basic delivery, but the outcome will never be as good as it could have been. The best partnerships are highly collaborative and build something together based on a common goal and the merits of the work itself. AC: Thanks for a very insightful interview, Trevor! It’s been fascinating to get your perspective on the future of the gaming industry and of the role the cloud will play in that evolution.", "date": "2020-11-11"},
{"website": "Avenuecode", "title": "How to Use UIkit for Low-Level Image Processing in Swift", "author": ["Khalid Asad"], "link": "https://blog.avenuecode.com/how-to-use-uikit-for-low-level-image-processing-in-swift", "abstract": "Ever thought about creating your own Instagram or image filtering application but don't know exactly how to approach it? Have some old C++ code but don’t want to go through all the wrappers? This guide is meant to show some of the basics of image processing in Swift using only UIkit. Computers use pixels to display images. In general, a pixel (the smallest controllable element in an image) is composed of 3 or 4 colors of varying intensities: Red, Green, and Blue ( RGB ) or Cyan, Magenta, Yellow, and Black The number of distinct colors that can be represented by a pixel depends on the number of bits per pixel (bpp). A 1 bpp image uses 1-bit for each pixel, so each pixel can be either on or off. Each additional bit doubles the number of colors available, so a 2 bpp image can have 4 colors, and a 3 bpp image can have 8 colors. The basic contrast and brightness adjustments are transformations of the form: 𝑓(𝑥)=𝛼𝑥+𝛽f(x)=αx+β (with the result rounded to an integer and clamped to the range [0,255][0,255]). Here 𝑥x is a color component value (R, G, or B). The slope 𝛼α controls contrast (𝛼>1α>1 means more contrast and 0<𝛼<10<α<1 less contrast). For easier separation of “brightness” and “contrast” modifications, the formula can be written like this: 𝑓(𝑥)=𝛼(𝑥−128)+128+𝑏f(x)=α(x−128)+128+b where 𝑏b controls brightness. Now that we have our basics covered, we can dive into the Swift portion! Let’s grab our image, then take the cgImage of it: We want to redraw this image with a specific Image Core Graphics Context, so let’s generate what we will need for this: By generating an UnsafeMutablePointer with Pixels (we already know that they are composed of RGB values, as well as an Alpha multiplier), we can manipulate CGContext with information about the size and bitmap (32-bit, big endian format). After generating our CGContext struct, we are ready to redraw the image: Now, given that the pointer is generated and updated, we can continue to our interface of the buffer by starting at that pointer: Given this buffer pointer, we can iterate through each pixel to calculate the total individual amounts of RGB pixels: Now calculate the averages by dividing by the pixel area. Finally, we’re ready to manipulate the pixels! The above code can be done in any way you want to manipulate contrast. In our case, we simply want to increase the contrast. We can do this by adding a delta with a multiplier of 2 to each of the pixels. Remember the max value is 255, and the min is 0. Now we can create the CGContext (like we did in the beginning) and use the makeImage() function to generate an image: Let’s see how the output image turned out on the simulator! Great! Let’s try the reverse by decreasing the contrast by the delta: Woah! We created a photo negative! We decreased the contrast so much that we ended up creating a photo negative. Let’s try grayscale using this forumla: 𝑦=0.3𝑅+0.6𝐺+0.1𝐵 With the use of Unsafe pointers and CGContext from Core Graphics, as well as a little bit of tinkering with RGB values, we can achieve anything with image filters in Swift! Congratulations, you now know how to create your very own Instagram image filtering application! Access to the code is available here .", "date": "2020-2-26"},
{"website": "Avenuecode", "title": "AC Spotlight - Rodrigo Ribeiro", "author": ["Andressa Lopes"], "link": "https://blog.avenuecode.com/ac-spotlight-rodrigo-ribeiro", "abstract": "Rodrigo Ribeiro, CIO at Arezzo&Co, gives us a glimpse into the IT processes and culture that have helped drive record e-commerce growth for his company. Avenue Code: We would like to hear a little more about your career journey so far. How did you become the CIO at Arezzo & Co? Rodrigo Ribeiro: I’m very passionate about using technology to solve business problems. I started my career as a self-taught developer, searching the internet for tutorials to learn how to develop my first programs. Then I found what I really love to do: leading and developing high-performance tech teams. For my education, I studied Information Systems and graduated from FGV SP with an MBA in technology and from Stanford with an executive certification focused on innovation and people. After graduating, I worked in retail companies and banks, where I learned about dynamism and agility. I was always very curious, so even in technology, I asked a lot of business questions. My curiosity opened doors for me, and in every role I adopted the motto of always giving my best and absorbing a lot of information. Today I am where I aspired to be: a CIO at a highly respected company, working in everything tech-related, from the basic infrastructure to the company's strategy, business, value chain, and team development. AC: Sales in Arezzo's digital commerce have tripled since the beginning of the pandemic. Could you tell us about this process and the strategy you used during the global crisis? RR: I believe that crises always bring opportunities, we just have to know how to find them. We did not change course with the pandemic; instead, we accelerated. The crisis showed us that the initiatives we were already adopting were on the right track. We had already launched an app that enabled sellers to have access to their customers and to their stores’ stock, to showcase items virtually, and to process payments via WhatsApp. We transformed our salespeople into digital consultants. We had to close stores, but we still managed to sell 105% of same store sales in 2020 for some of our stores that implemented digital commerce. Now this technology is being scaled. E-commerce has also accelerated a lot. While our customers are still loyal to our physical stores, we’ve added several e-commerce offerings and campaigns like placing and receiving orders on the same day or scheduling product pick up at a trusted store. In addition, we expanded the stores’ inventory to the “infinite” stock on the web, accelerating our digital process significantly. In short, we managed to bring trusted stores into the digital space. AC: At Avenue Code, the product/user journey is extremely important. We have seen most of our customers paying special attention to this topic in recent years. At Arezzo, the Smart Buyer project restructured the product journey from end to end, using machine learning and analytics, improving product offers in e-commerce. Can you tell us about the importance of this project within the company and how the idea of ​​creating it came about? RR: We are very concerned with the integration of our entire supply chain, and we regularly discuss digital/supply chain concepts. For example, we have information on consumer behavior related to an enormous collection of products, including what sold well and what didn’t over time. We use this data to help us develop new product collections with more confidence. Using this data, we developed a solution that, despite being powered by complex AI, produces a simple result - photos of past products detailing their colors, shapes, components, how long they were available, and which promotions were used (if any). This helped us anticipate customer needs and launch new collections every 15 days during the pandemic. AC: What is your biggest daily challenge in managing remote teams? RR: I really like human contact, and I miss the informal conversations that naturally arise during in-person collaboration. On the other hand, remote work can boost productivity and improve discipline because there are fewer interruptions. Like many other companies, we transitioned to a remote work model overnight, but we decided that all meetings would be hybrid, both in person and remote. I really believe in the hybrid model - it requires a lot of discipline and monitoring from the meeting leader. I’ve realized that as leaders, we need to monitor teams closely and focus on results while conducting business in the most human-centered way possible so that employees can comfortably explore problems and solutions. AC: What are the biggest trends you have seen in e-commerce globally? RR: It’s essential to talk about e-commerce, but we must also talk about platforms and ecosystems. Leaders need to decide how to position themselves inside an ecosystem to take advantage of it. At Arezzo, we want to be the biggest fashion platform in our marketplace, and part of our strategy involves sponsoring a series of other services, such as curation. There’s a lot on the horizon for us. For example, there’s no reason why the customer shouldn’t be able to use our platform to get help repairing a shoe or learning fashion tips from stylists all over the world. AC: We know that you have great experience with Agile and innovation processes. Could you tell us how Arezzo utilizes the Agile methodology? Do you have any specific areas for innovation? RR: We have a scaled Agility model. We took the best practices in the market and created our A.T.O. (Agile Transformation Office). In this group, we have a senior manager who takes care of organizational agility together with our people management team, which is responsible for culture change related to technology, strategy, and operations using OKRs. In this way, we define the team according to the strategy, and we measure the maturity of the teams to determine where they should be placed in projects, because team maturity is related to delivering a business value proposition. When we create new teams, we’re looking not only at agility, but also security. Regarding innovation, we have our in-house ZZ LABS, which is a squad that has a method for innovating and prototyping solutions quickly. We have groups of teams, so we run innovation cycles, define what we’re prototyping and testing, and then we rotate teams to give everyone the opportunity to innovate. At ZZ LABS, we believe that innovation is everyone’s competence and shouldn’t be relegated to only one team, so we equip everyone with the tools to innovate. AC: How is Arezzo dealing with strategic partnerships focused on IT? RR: We have a well-balanced mix between interns and third parties in all segments of infrastructure, from basic development to strategic consulting. We have a structure within the IT governance that manages our partners. On the development side, we select fewer partners who are very high quality; this helps us facilitate management, scale, reduce costs, add quality, and increase maturity. We’re always open to selecting new partners according to meritocracy. AC: Could you tell us about something you did or a project you participated in that you are very proud of? RR: I have participated in a few projects that made purely utilitarian technology an enjoyable experience. These technologies created great value for the company by solving business problems, and at the same time, the teams I managed grew in their ability to innovate. For example, I worked with the municipality of Campo Bom to develop a program to apprentice young people who don’t have a career direction yet: we gave them the challenge of solving a problem, and because of this, they fell in love with big data analytics. The program has grown a lot, and I feel like an influencer for people who need career direction. I have a dream that our technology area will be the best to work at in the state or in Brazil as a whole. These projects give me great satisfaction. AC: Thank you for your time today, Rodrigo. It’s been fascinating to get an inside look into IT at Arezzo.", "date": "2021-4-20"},
{"website": "Avenuecode", "title": "Migrating an Application On-Premises to AWS", "author": ["Milena Santos"], "link": "https://blog.avenuecode.com/migrating-app-onpremises-to-aws", "abstract": "There's a lot to consider when you're designing toolchains for applications' continuous integration and continuous deployment (CI/CD). Today, we'll describe what's involved in migrating an app that's on-premises into the AWS cloud. WHAT DOES ON-PREMISES MEAN? On-premises refers to installations owned/maintained by the company, where software can live and be happy, or not so happy when there is an outage or disruption on the infra that brings servers and/or applications down, or even when hardware capacity reaches its limits with more users than expected accessing systems simultaneously. On the other hand, the control over the infra is all in the organization's hands, so there are lots of pros to having applications on-premises. What's best for a company depends on its situation and needs, but that is a topic for a separate post. WHICH TOOLS ARE USED FOR CI/CD? According to Agile practitioners, continuous integration and continuous deployment are not tied to specific tools used, but rather to a certain mindset. In other words, practices were developed based on a good definition of the process for developing stable software, even where these practices involve a need for human intervention, such as manual approval for pushing changes to production. Thus , applying CI/CD for an application does not mandate the adoption of specific tools, nor does it mandate having the app either on-premises or on the cloud. Again, this subject also deserves a dedicated article of its own. WHAT ARE THE BENEFITS OF MOVING TO THE CLOUD? As with every decision in life, there are pros and cons to moving to the cloud, but it's unquestionable that doing so can help companies save tons of money in the mid-term if the transition is well planned. This is because the company won't have to expend money or effort keeping physical servers up-to-date, working late into the night training support teams on fire drills, etc. AWS provides several tools to help migrate software to its infrastructure. For more customized installations, most can be done via its web console, IDE plugins and basically everything via a command-line using AWS-SDK . Click here for more details. The sample application used to describe the migration steps below is based on a real application that had a consistent CI/CD fully on-premises as follows: Once the developer committed changes into the Bitbucket Git repository, a Jenkins instance configured to \"listen\" to the repository would kick the build process whenever there were fresh commits. If everything went right, then a CHEF recipe was invoked on the Linux-based server where the application was to be deployed and exposed to users via NGINX . It's important to note that the database pointed to by this real application used to be on-premises. Prior to app migration to the cloud, however, the DB was migrated, which proved to be a helpful exercise to start testing AWS on the storage side. Setup adjustments did need to be made, and it would have been much worse if both the DB and the application had been moved in one shot. As mentioned above, this real-world scenario involved a database, which was migrated to RDS with minimum effort since moving it was merely a matter of configuring the remote DB into VPC , and only computers and systems from inside the company VPN had access to it. After the DB was migrated, the app was updated to start pointing to the remote DB. The first proof of concept was moving the application to AWS Elastic Beanstalk , which was quite straightforward since the effort involved very few resources in terms of the amount of configuration and AWS know-how that were required. (The idea of using Beanstalk was discontinued for this case, but later Beanstalk was used for other, more complex apps since Beanstalk supports multiple platforms .) The second and ultimate migration solution for this same application was comprised of the following flow: > Bitbucket - the Git repository had already been used for the on-premises app flow, but when used in the new flow for AWS, we integrated it by configuring an access token generated in AWS IAM ; we also included a webhook , an endpoint also generated on the AWS side, for taking the source code from Bitbucket and bringing it inside Amazon S3. > Amazon S3 (Simple Storage Service) - We used S3 as our file explorer since it makes it easy to concentrate on the application's source code to be used by any service on the premises of AWS. > AWS CodePipeline - This is the AWS CI/CD \"plug-n-play,\" or, as detailed here , it \"builds, tests, and deploys your code every time there is a code change, based on the release process models you define.\" See below how the pipeline is represented in AWS web console for this case, linking \"source + build + deploy:\" > AWS CodeBuild : As its name suggests, AWS CodeBuild is a service for taking care of building applications in a manner similar to Jenkins. (By the way, Jenkins is an option for the CodePipeline build section too, but we chose CodeBuild since it only requires a buildspec.yml committed, along with the project source code, to build and create the application package.) > Amazon ECS : According to its own webpage : Amazon Elastic Container Service (Amazon ECS) is a highly scalable, high-performance container orchestration service that supports Docker containers and allows you to easily run and scale containerized applications on AWS. Amazon ECS eliminates the need for you to install and operate your own container orchestration software, manage and scale a cluster of virtual machines, or schedule containers on those virtual machines. In other words, ECS is amazing and allows us to set up plenty of applications running as services into a limited number of virtual machines, sharing their resources and, in this use case, configured to have always at least one instance of the service running. This means that if one crashes for some reason, the service puts another instance up. N.B. Relatively recently, Fargate was introduced to ECS, and it simplifies life a lot: whereas ECS requires EC2 server and Docker repositories to be set up, Fargate doesn't. > Amazon EC2 : This is the virtual machine where an instance of our application lives. It's equivalent to the on-premises, Linux-based server where it used to live, and in our case it's configured as part of ECS, as described above. > Amazon API Gateway : As its name suggests, this is where APIs are configured. In this given use case, Amazon API Gateway was used as a workaround for quickly exposing the endpoints over https because a load balancer ( ELB ) was not set instead. Switching focus to the application,  we've highlighted what had to be included in its structure below: Dockerfile The above was necessary for making the app portable not only to Amazon ECS but also to any dockerized container system. This invokes \"mvn clean package\" for downloading all project dependencies, compiling the code, and building an executable spring boot jar, followed by instructing Docker on how to execute the app. buildspec.yml This is equivalent to the CHEF recipe in the on-premises CI/CD invoked by Jenkins. Unsurprisingly, it's the \"recipe\" used by AWS CodeBuild for understanding what to do with the Dockerfile and how to install it in ECS for preparing the application for deployment. CloudFormation Template Finally and most importantly, this is the template for generating the pipeline and all resources with AWS CloudFormation , which is not used in the application CI/CD but rather for creating and recreating it whenever needed. It's particularly interesting given that if any step goes wrong, deleting the stack created in CloudFormation rolls back all resources needed for applications. This means unnecessary costs are avoided. It also mean that if we realize other projects using the same structure need to be moved to AWS, it's ready for use with only a few changes. Together, we have demystified the process of building and deploying a sample application into the AWS cloud using a real use case. Such an exercise involves detailing multiple concepts sometimes taken for granted, but it's important to do so in order to properly determine the CI/CD toolchain for steps that have already been defined and especially for steps that haven't been defined. I neglected to mention earlier that an alternative migration option is to break the application into microservices with AWS Lambda, which is quite challenging if not planned in advance and done incrementally if you're working with a production monolith-based app. You can read more about this option here . Our next post will be written for serverless applications' CI/CD in AWS, so stay tuned! Do you have migration use cases of your own? Please don't hesitate to share them with us! You may also download our sample app here .", "date": "2019-1-30"},
{"website": "Avenuecode", "title": "Rapid-Fire Protractor Quiz", "author": ["Madhu Ramachandran"], "link": "https://blog.avenuecode.com/10-questions-about-protractor", "abstract": "For the last two weeks, I have been busy studying the Protractor automation tool. During the process, several questions came to mind and as I researched and studied to uncover the mysteries of this framework, I decided to put the results together as a Q&A. This blog is the result! Consider this your well-informed Siri or Alexa guide to Protractor for beginners. Answer: Protractor is an open source E2E testing automation framework, designed specifically for AngularJS web  applications. The Protractor automation tool is a Node.js program built on top of WebDriverJS. Protractor works as a Solution integrator combining powerful technologies like NodeJS, Jasmine, Selenium, Mocha, Cucumber and Web driver. Protractor is intended not only to test AngularJS application, but also for writing automation tests for any web applications. Answer: AngularJS applications are Web Applications, which use the extended HTML's syntax to express web application components. It is mainly used for dynamic web applications. These applications use less code that is more flexible code compared with normal Web Applications. Sometimes it is difficult to capture the web elements in AngularJS applications using jUnit or Selenium Web driver. Protractor supports Angular-specific locator strategies, which allows you to test Angular-specific elements without any setup effort on your part. Answer: Well, Angular JS applications have some extra HTML attributes like ng-repeater, ng-controller, ng-model etc., which are not included in Selenium locators. Selenium is not able to identify those web elements using Selenium API. So, Protractor on top of Selenium can handle and controls those attributes in Web Applications. Protractor also speeds up your testing as it avoids the need for a lot of “sleeps” and “waits” in your tests, as it optimizes sleep and wait times. Answer: Protractor supports two behavior driven development (BDD) test frameworks out of the box: Jasmine and Mocha. These frameworks are based on JavaScript and Node.js and provide the syntax, scaffolding, and reporting tools you will use to write and manage your tests. Cucumber is no longer included by default. You can integrate Cucumber with Protractor with the custom framework option. First things first, open the terminal and start the webdriver server: webdriver-manager start After that, you can run Protractor in another terminal by typing: protractor config.js You should see a Chrome browser window open up and navigate to the AngularJS page enter Julie , then close itself (this should be very fast!). You should see “ 1 spec, 0 failures ” in the terminal Answer: That's right. Protractor exports these global variables to your spec file: browser - A wrapper around an instance of WebDriver, used for navigation and page-wide information. The browser.get method loads a page. Protractor expects Angular to be present on a page, so it will throw an error if the page it is attempting to load does not contain the Angular library. (If you need to interact with a non-Angular page, you may access the wrapped webdriver instance directly with browser.driver). element - A helper function for finding and interacting with DOM elements on the page you are testing. The element function searches for an element on the page. It requires one parameter, a locator strategy for locating the element. Answer: Protractor supports the two latest major versions of Chrome, Firefox, Safari, and IE.  In your Protractor config file (see config.js ), all browser setup is done within the capabilities object. This object is passed directly to the WebDriver builder ( builder.js ).To use a browser other than Chrome, simply set a different browser name in the capabilities object. Answer: Of course! If you would like to test against multiple browsers, use the multiCapabilities configuration option. Again, you have to tweak the config.js file : Protractor will run tests in parallel against each set of capabilities. Please note that if multiCapabilities is defined, the runner will ignore the capabilities configuration. That concludes my brief Q&A on Protractor, based on my research. I hope it was helpful! Questions? Leave them in the comments!", "date": "2017-3-1"},
{"website": "Avenuecode", "title": "How To Make Your Android App Smarter", "author": ["Heitor Souza"], "link": "https://blog.avenuecode.com/how-to-make-your-android-app-smarter", "abstract": "Artificial Intelligence and its branches such as machine learning quickly gaining in popularity with both users and developers alike. While there is no single recipe to creating the next greatest app that everyone will use, but I can guarantee that one of the ingredients in the usage of thoughtfully incorporated AI. But creating your own AI solution can be very complex and tricky. The good news is, there are already some very powerful and easy-to-use solutions provided by companies such as Google, Amazon, and Microsoft. These comanies offer services and libraries with popular features such as Face Recognition, OCR (Optical Character Recognition), Classification Model, and so on. (References and links can be found at the end of this post). One solution that we had chance to use here at Avenue Code is the Google Mobile Vision API, which is part of the Google Play service in the Android platform. Some features of this library are not exclusive to Android. For instance, iOS developers can also use the Face Recognition API - check out the documentation! This post is dedicated to how we can use Face Recognition in our app with Google Mobile Vision API. Creating the next MSQRD or Snapchat is not rocket science with MV API. To quote the API description: “It's designed to better detect human faces in images and video for easier editing. It's smart enough to detect faces even at different orientations -- so if your subject's head is turned sideways, it can detect it. Specific landmarks can also be detected on faces, such as the eyes, the nose, and the edges of the lips.” So let’s get started! First of all, create a project in Android Studio. In order to use the Google Mobile Vision API your gradle file from the application level should import this dependency. At the time of this post, the current version of play-service-vision is 10.2.4. You can check out the documentation here: https://developers.google.com/android/guides/setup Your Android Manifest file should enable the service. Place this below code in the Application tag. Our app is based on the Google sample code for the Camera2 API. To focus on how we can use the Face Recognition API, we will abstract the camera layer and go directly to the heart of what this post is all about. If you want more details on how the camera2 API works, please take a look at the github repository: https://github.com/googlesamples/android-Camera2Basic The architecture of our application consists of two packages: camera and face_recognition. The camera package is entirely based on camera2basic repository, with the only extra method being the startFaceRecognition in FragmentCameraPreview: This method only retrieves the photo that user has taken. The result of the recognition lays on the FaceRecogntionFragment. Before we can actually call the API, we need to prepare our ImageView to display the processed result. We are writing on ImageView itself so we need Bitmap, Paint and Canvas for this to work We need to make the Bitmap mutable so we can write on it later - otherwise we will end up with an exception like this: java.lang.IllegalStateException: Immutable bitmap passed to Canvas constructor Then do the little red circles which shows parts of the face: And finally, create the canvas where these red circles will be drawn: Face Recognition Coding Up to this point, nothing magical is happening. We're only warming up the stage for the big player: FaceDetector. Beware when importing this dependency: there are two FaceDetectors, one from android.media which we are NOT interested in, and the one from android com.google.android.gms.vision.face package. This is our guy. First we need to setTrackingEnabled to false to get better performance and accuracy for single image processing. We enable this property for live video, for example. Next we need to check if the service of Face Detector is operational: Sometimes it takes a little while to download this dependency. Now let’s detect faces! Create a frame using the Bitmap, then call the detect method on the FaceDetector, using this frame, to get back a SparseArray of Face objects: To make this app funnier let’s recognize both eyes and replace with the Avenue Code Big ‘A’ icon, like this: For this, create a Bitmap the references this resource: It’s very easy to retrieve the coordinates for right and left eye. Just loop through the sparseArray and get the landmark of each face like this: And finally just set the result Bitmap: That’s it. Easy and painless. With less than a line of code, we have applied an AI in our app. You can checkout this project in github: https://github.com/heitornascimento/mobile_vision_face_recognition This is a tiny little glimpse into what you can do with the Google Mobile Vision API - there's plenty more to learn and play with! Hoping this was fun and enjoyable - questions? Feedback? Leave them in the comments! References: Google Mobile Vision API: https://developers.google.com/vision/ Google AI Cloud Solution: https://cloud.google.com/products/machine-learning/ Amazon: https://aws.amazon.com/machine-learning/ Microsoft: https://azure.microsoft.com/en-us/services/machine-learning/ Camera2 Sample Repository: https://github.com/googlesamples/android-Camera2Basic Google Code Lab: https://codelabs.developers.google.com/codelabs/face-detection/", "date": "2017-5-24"},
{"website": "Avenuecode", "title": "How Machine Learning Applications Save Retailers Time and Money - Part 2", "author": ["Hossein Javedani Sadaei"], "link": "https://blog.avenuecode.com/how-machine-learning-applications-save-retailers-time-and-money-part-2", "abstract": "From controlling smart cars on earth to forecasting solar flares in space, machine learning technologies are being more and more widely implemented. Today, we’ll continue last week’s discussion on machine learning applications in retail settings, including their uses in securing payment transactions, improving customer experiences, predicting customer demand, and automating customer service. In retail, fraud and security companies are investing heavily in machine learning technologies that detect user and payment fraud 1 . These applications continuously monitor card transactions and grant near real-time authorization 2 . Most recent frauds involve stolen credit card information or fraudulent merchandise returns. Consequently, many security companies are focused on using ML applications to analyze customer transactions and activity to develop appropriate priorities for case management and investigation 4 . Machine learning applications are also widely used in upselling and cross-selling. First, let’s define these terms: Upselling is the practice of encouraging customers to purchase a higher-end product instead of the original item selected. Cross-selling, on the other hand, invites customers to buy related or complementary items 5 . To upsell and cross-sell more effectively, retailers are now using machine learning applications to analyze customer activity and data to recommend personalized rather than generic products 7 . ML applications also enable retailers to enhance customer experience by creating more customer-centric interactions with seamless, omnichannel communications. In turn, this generates more revenue for retailers 8 . ML applications improve customer experience by: -Recommending products, services, and special deals based on individual browsing data and purchasing behavior. -Systematically identifying customer groups and marketing to group tastes . -Automatically sorting products into proper navigation categories to save browsing time. -Identifying dissatisfied customers and addressing their concerns to improve experience ratings. -Analyzing customer profiles and tracking activity to offer financial services or personalized products. (For example, Netflix uses sophisticated algorithms to analyze customers’ viewing histories and present content recommendations that match their interests.) -Using new advances in speech recognition to route customer service calls to appropriate representatives, reducing call durations and increasing first-call problem resolutions. ML applications can even assess customer satisfaction based on natural language processing, enabling Business Intelligence (BI) teams to treat dissatisfied customers first. In today’s world, retailers must meet customer demands with new and innovative products if they want to stay ahead of their competitors. Forecasting sales for these products, however, can be very challenging in e-commerce settings. Reducing investment risks depends on predicting sales accurately. Thankfully, machine learning applications with predictive analytics allow businesses to anticipate consumer demand quickly and accurately. 12 This allows companies to: Machine learning applications are always developing. Let’s take a look at some of their most recent advances and uses. ML applications are now being used to improve the performance of chatbots, which are virtual agents that help facilitate customer purchases. Chatbots have become very popular additions to customer service centers and are being used by companies like Starbucks 13 , Whole Foods, Pizza Hut 14 , and Staples. Business owners can also use machine learning technology to generate simple content, such as sports reports or stock updates. In the near future, they will be able to generate even more complex and personalized content. For example, Persado generates smart content by using precise words, phrases, and images designed to engage customers’ emotions 16 . In the near future, machine learning could even make grab-and-go shopping possible. This means customers could take what they want from shelves and automatically be charged through image and video processing instead of having to check out manually. 17 Apps like Amazon Go, for instance, are designed to automate the entire shopping experience as follows: Store sensors would track which objects customers pick up and put in their baskets. Then, when customers exit stores, their accounts would automatically be charged. It’s likely that ML technology will even make it possible for robotic retailers to use facial recognition algorithms to personally greet customers at the door, predict their orders, and guide them to appropriate product locations 19 . To further automate the shopping process, Amazon is developing “Prime Air,” 20 which would employ drones to safely deliver packages up to 5lbs  in less than 30 minutes. Finally, in most retail stores today, cameras are used for little more than security purposes. For example, Walmart uses facial recognition 22 software as an anti-theft mechanism. Inventory systems of the future, however, will be able to use images and videos captured by cameras to generate accurate, real-time estimates of all products in a given branch 23 . This system would notify store managers of unusual patterns of inventory data 24 , prompting theft investigations or recommending faster reorders for in-demand products. Cameras could even detect the walking patterns and directions of customer gazes, allowing executives to analyze interest in products and restructure store layouts accordingly. There are a number of startups already focused on marketing analytics using video data. This article offers multiple examples of how retailers can use machine learning methods to enhance customer experience, minimize costs, improve processes, and increase revenues. It’s important to be aware, however, that designing proper and specific algorithms to address retail challenges is complex and necessitates a dedicated team of data scientists, proficient software developers, and application lifecycle managers. This can be expensive for small and medium-sized retailers, which is why it’s important to seek innovative services with competitive prices. Interested in learning more? We got you. Download our free whitepaper, \"Tame Your Big, Wild Data With A Robust Forecasting Method\". 1 \"Artificial Intelligence in Retail – 10 Present and ... - Tech Emergence.\" 14 Sep. 2017, https://www.techemergence.com/artificial-intelligence-retail/ . Accessed 23 Jan. 2018. 2 \"Beyond the buzz: Harnessing machine learning in payments - McKinsey.\" https://www.mckinsey.com/industries/financial-services/our-insights/beyond-the-buzz-harnessing-machine-learning-in-payments . Accessed 23 Jan. 2018. 3 \"So what exactly are virtual payments – and why should I care? « Sabre.\" 20 Apr. 2016, https://www.sabre.com/insights/so-what-exactly-are-virtual-payments-and-why-should-i-care/ . Accessed 26 Jan. 2018. 4 \"Retail Fraud Detection | Fraud Analytics | Loss Prevention ... - Experfy.\" https://www.experfy.com/fraud-risk/retail-fraud-detection . Accessed 26 Jan. 2018. 5 \"What's the difference between upselling & cross-selling? [2018 ....\" https://www.bigcommerce.com/ecommerce-answers/what-difference-between-upselling-and-cross-selling/ . Accessed 23 Jan. 2018. 6 \"Retail & Consumer Goods - Cappius.\" http://www.cappius.com/industries/retail-consumer-goods/ . Accessed 26 Jan. 2018. 7 \"How Marketers Are Using Machine Learning To Cross-Sell And Upsell.\" 16 Mar. 2017, https://insights.principa.co.za/you-want-fries-with-that-using-machine-learning-to-cross-sell-and-up-sell . Accessed 23 Jan. 2018. 8 \"Using Machine Learning to Enhance the Customer Experience ....\" https://www.hpcwire.com/solution_content/hpe/government-academia/using-machine-learning-enhance-customer-experience-3/ . Accessed 23 Jan. 2018. 9 \"Why Companies are Missing the Mark on Digital Customer Experiences.\" http://www.adamsoftware.net/2017/07/companies-missing-mark-digital-customer-experiences/ . Accessed 26 Jan. 2018. 10 \"4 Ways Machine Learning Boosts The Customer Experience - CMO.com .\" 11 Aug. 2017, http://www.cmo.com/features/articles/2017/8/10/can-machines-learn-how-to-improve-the-customer-experience.html . Accessed 23 Jan. 2018. 11 \"Operations How to Predict Demand for Your New ... - Kellogg Insight.\" 10 Mar. 2017, https://insight.kellogg.northwestern.edu/article/how-to-predict-demand-for-your-new-product-2 . Accessed 26 Jan. 2018. 12 \"Anticipating the Demand: The Need to Get Closer to Your ... - SAP Blogs.\" 27 Sep. 2013, https://blogs.sap.com/2013/09/27/anticipating-the-demand-the-need-to-get-closer-to-your-consumer/ . Accessed 24 Jan. 2018. 13 \"Starbucks Bot Steps Up The Coffee Experience – Chatbots Magazine.\" 12 Oct. 2017, https://chatbotsmagazine.com/starbucks-bot-steps-up-the-coffee-experience-2dfa1d16976d . Accessed 26 Jan. 2018. 14 \"Pizza Hut Bot on Amazon Echo chatbot on BotList - BotList.\" https://botlist.co/bots/pizza-hut . Accessed 26 Jan. 2018. 15 \"Chatbots no Atendimento a Clientes: Tudo que Você Precisa Saber.\" 6 Jul. 2017, https://www.dds.com.br/blog/index.php/chatbots-atendimento-tudo-que-voce-precisa-saber/ . Accessed 26 Jan. 2018. 16 \"Persado Unveils Major Expansion to Marketing Language Cloud ....\" https://persado.com/press-releases/persado-unveils-major-expansion/ . Accessed 26 Jan. 2018. 17 \" Amazon.com : : Amazon Go.\" https://www.amazon.com/b?node=16008589011 . Accessed 26 Jan. 2018. 18 \"Is Amazon Go shutting down the checkout for good? - PwC Chair in ....\" 19 Dec. 2016, http://www.chairdigitaleconomy.com.au/amazon-go-shutting-checkout-good/ . Accessed 26 Jan. 2018. 19 \"Data Convergence; The Role of Machine Learning in Retail - Retail ....\" 10 Oct. 2017, https://www.retailtouchpoints.com/features/executive-viewpoints/data-convergence-the-role-of-machine-learning-in-retail . Accessed 25 Jan. 2018. 20 \"Amazon Prime Air - Amazon.com .\" https://www.amazon.com/Amazon-Prime-Air/b?node=8037720011 . Accessed 26 Jan. 2018. 21 \"Amazon tem patente de paraquedas para ser usado em entregas por ....\" 31 May. 2017, https://www.tecmundo.com.br/amazon/117245-amazon-tem-patente-paraquedas-usado-entregas-drones.htm . Accessed 26 Jan. 2018. 22 \"Walmart's Facial Recognition Tech Would Overstep Boundaries - Forbes.\" 27 Jul. 2017, https://www.forbes.com/sites/retailwire/2017/07/27/walmarts-facial-recognition-tech-would-overstep-boundaries/ . Accessed 26 Jan. 2018. 23 \"Machine Learning in Retail - Near-Term Applications and Implications -.\" 21 Aug. 2017, https://www.techemergence.com/machine-learning-retail-applications/ . Accessed 25 Jan. 2018. 24 \"Inventory tracking | Mathecsys.\" http://mathecsys.com/category/inventory-tracking/ . Accessed 26 Jan. 2018. 25 \"Cameras know you by your walk | New Scientist.\" 19 Sep. 2012, https://www.newscientist.com/article/mg21528835-600-cameras-know-you-by-your-walk/ . Accessed 26 Jan. 2018.", "date": "2018-2-14"},
{"website": "Avenuecode", "title": "3 Ways to Make Your Code More Readable", "author": ["Magnum Fonseca"], "link": "https://blog.avenuecode.com/3-ways-to-make-your-code-more-readable", "abstract": "For many years, I was a big fan of beautiful code. With Ruby, I was amazed by what I was able to do in one line of code. With Java, I loved that I could stream, map, and then collect! But as time passed, I read about many impressive optimizations, small tricks, and clever algorithms, and I got less and less excited about these discoveries. The reason is that they almost always fell into the category of some very elegant expressiveness in a new language (Ruby converts from Java can attest to this) or a technique that I'm not going to ever use. In other words, I was chasing baubles. When I joined the Ruby community, I heard a lot about code clarity and code readability, as agile methodologies recommended throwing away almost all project documentation to encourage the code itself to be simpler and more intelligible. So my aesthetic sense turned toward code clarity. Developers must realize that they’re not writing code for themselves, but for others. When there's less documentation, a new developer entering the environment needs to be able to read and understand the code. Image courtesy of First Class Thoughts But code readability doesn't only affect newcomers. When developers initially write code, everything is fresh in their minds; their knowledge of the system is detailed because they have read requirements and technical specs and have worked on them for some time. A year later, however, no one knows the system that well anymore, so developers need to read the code again and learn from it. 1. Naming Don’t write complex, beautiful code. The simpler your code is, the fewer bugs it's likely to have, and the less time you'll have to spend debugging. Imagine how hard it would be to debug a data process inside a stream map. Code should do only what it needs to do without being cluttered by tons of abstractions and comments. Naming is hard, but it’s important. For example, a method that updates some data for an object should have a name that screams \"I update x data for x object, and that's all I do!\" — but this is not SRP, which I’ll explain at another time. Names of variables and functions should be distinct and provide a general idea of what they do. The important thing about naming is that the name should mean something to your team. Because of this, it should conform to the conventions chosen for the project, even if you don’t agree with them. For instance, if every request for a record in the database starts with “find,\" e.g “findUser,” then your team might be confused if you come to the project and name your database function “getUserProfile” because this is what you are used to. Try group naming when possible. For example, if you have many classes for input validation, writing “Validator” as the suffix for the name may quickly provide information as to the purpose of the class. If a variable or constant might be seen or used in multiple places in a body of code, it is imperative to give it a search-friendly name and avoid comments showing a bad code. Good code should be understandable without a line of comments; it should be self-documenting. Commented code is confusing. Did someone remove it temporarily? Is it important? When was it commented? It’s dead, take it out of its misery. Just remove it. 2. Patterns, my friends, patterns! Back in 2013, Sandi Metz created some rules for developers : There are four rules Classes can be no longer than one hundred lines of code; Methods can be no longer than five lines of code; Pass no more than four parameters into a method. Hash options are parameters. Controllers can instantiate only one object. Therefore, views can only know about one instance variable and views should only send messages to that object ( @object.collaborator.value is not allowed). For me, the most interesting rule is the five-lines-per-method rule. We agree that if , else , and end are all lines. So in an if block with two branches, each branch can only be one line: Adhering to the five-line rule means that we would never use else with else if . Having only one line per branch urges us to use well-named private methods to get work done. Private methods are great documentation. They need very clear names, which forces us to think about the content of the code we are extracting. 3. Guard Clauses May Be All You Need! There is as a great discussion happening on how if-statements should be used for better code readability. Usually, the conversation boils down to an opinion that is completely subjective and aesthetic. But it is certain that the most documented case is the guard clause, also known as assert or precondition. The main idea is that when you have something to assert in the beginning of a method, you should do this using the fast return. Image courtesy of Medium Let’s start with a snippet of code (not using a guard clause): Now look at the following one (using a guard clause): It’s pretty common to find large if-blocks in a codebase due to the fact that our brains function differently: we tend to think “do stuff if it’s enabled,” not “do stuff if it’s enabled, but if it’s not enabled, do nothing.” The above tips are just a few examples, albeit important ones, of how we can write readable code. The underlying principle is to always write code that is simple to read and can be easily understood by other developers, because the time and resources you'll spend trying to decipher confusing code is much greater than any gain you'd get from optimizations. I believe that this is the way to make things better, not only for us, but also for those who will read our code later. You also need to learn when to not use certain patterns: since each one solves a specific problem/issue in the applications, their usefulness can be affected by many different factors like the size of the project, the number of people working on it, the time/cost constraints or the complexity required for the solution. Some patterns have been named anti-patterns, like the Singleton pattern, because even though they provide some solutions, they also introduce some issues in certain cases. There are many ways to be right. If you are convinced beyond a doubt that you should always use DRY in your applications or that it's better to always keep using only a well-known design pattern, that's okay. You may choose the path that is right for you and your team; for coding, there are no indisputable truths--the takeaway is to always think about how other developers will read your code.", "date": "2019-7-10"},
{"website": "Avenuecode", "title": "5 Disasters to Avoid in the Discovery Process", "author": ["Alexander Carvalho"], "link": "https://blog.avenuecode.com/5-disasters-to-avoid-in-the-discovery-process", "abstract": "The discovery phase sets the tone for your entire project or product, but if done incorrectly, it can be disastrous. Here's what NOT to do as your technical, business, and sales teams collaborate to design a high-ROI project . Agile allows us to deliver software incrementally. We prioritize the most valuable features to be delivered first so that we can start delivering value faster. This methodology promotes transparency, flexibility, and, most importantly, collaboration. But using Agile doesn't mean we don't need to plan before we start the solution implementation. It's very important to have all team members and stakeholders looking to the same North Star that will guide the product development. To define the North Star for the project, everything starts with the Inception . THE INCEPTION The Inception is dedicated to understanding the project's objectives and scope. The outcome of the Inception is a clear idea about the project's viability. During the Inception, the team and stakeholders will: THE DISCOVERY To prepare the Agile Team and Stakeholders to have a productive and assertive Inception, the Discovery phase comes into play. The Discovery consists of meetings and collaboration sessions between Technical, Business, and Sales teams to: The Discovery phase is the first phase of the Inception process. It sets the tone for the rest of your project/product, and because of that, the Discovery phase is incredibly important. In this snippet, you'll learn about the 5 biggest disasters to avoid during a Discovery. Converting ideas into actions might be a challenge if you don't know what you want to solve. Especially when we are talking about Digital Transformation , the problem/product/improvement you want to achieve must be clear before you start the Discovery process. The Inception is the time to align project goals with the Agile Team and Stakeholders. This alignment is key for success. During the Discovery, you will define project visions and goals, but to do so, it's very important to understand the problem solution or outcome you expect from the project. If you don't know what you are using the Inception to solve, I strongly recommend taking a step back and using Design techniques to help you frame the problem . Assuming that the problem you want to solve is clear and that you are ready to start your Inception with the Discovery, the second biggest mistake you can make is: not involving the right people because you never analyzed who is directly impacted by the project. It's fundamental to have the right people aligned to have a successful project. Once again, Inception is all about alignment. It's important to understand which areas of your organization will be affected, which areas you need help or information from, and in which areas you might change the process. Are you involving the subject matter experts that you need? Are you considering the right people to collect the right information to solve the problem you have in mind? After defining the problem you want to solve, always consider it as a next step to identify the right people to be involved in the Discovery and throughout the whole Inception. You know the Problem to be solved and you have the right people to help you solve this problem, so what can go wrong? It's simple: you don't have a decision maker. Most of the time, the sponsor of the project will also be a decision maker, but sometimes, depending on your organization's structure, the decision maker is not clear. Yet this role is fundamental. During the Inception phase, a lot of decisions regarding the solution you are creating must be made, and the Decision Maker must be identified. Without this role defined, you will not be able to have a successful Discovery and the rest of the Inception will be impossible to execute. The Inception process is a continuous flow. The Discovery starts setting project objectives and goals with all stakeholders and team members. So the fourth mistake you have to avoid is involving people late in the process. Someone who is added half-way through will not have the context, will not understand why the team is taking the defined approach regarding the project, and will not understand the rationale behind all decisions. So, a person who is involved in the middle of the process can slow down the Inception or even invalidate what has been done so far. The worst case scenario? The person who is involved late in the process is the Decision Maker. If this is the case, sometimes it is better to abort the current Inception and start a new Discovery. Last but not least, the fifth mistake to avoid during a Discovery is neglecting to align with external dependencies. Imagine that you already avoided all previous mistakes and you are ready to start the Inception with the Discovery process; you start, but then you forget to align with other vendors, providers, and companies that will be involved with your project at some point. For example, imagine that you are delivering a Solution that requires third-party software that must be customized to your needs. It's essential that their development process is aligned with the Iterative process you are setting during the Inception, and it's very important to have this alignment in the Discovery phase. Why? You might start your project and get to the point where you don't have the third-party piece of code you need, and then your project can be put on hold and sometimes even be cancelled depending on the costs this mistake can generate. Based on my experience, these are the five most common mistakes organizations make in the Discovery phase when they start an Agile project. If you avoid all of them, your Inception will generate successful outcomes, expectations alignment, value generation from your project, and most importantly, save time and costs, all while increasing the accuracy of your Agile project cost and effort from 10% to 60% minimum. By the end of the Inception, you'll have all necessary information for the Go/No Go decision of your project. Ready to get started? Let's talk .", "date": "2021-2-24"},
{"website": "Avenuecode", "title": "How to Unit Test a GO Service on Google App Engine - Part 3", "author": ["Pedro Costa"], "link": "https://blog.avenuecode.com/how-to-unit-test-a-go-service-on-google-app-engine-part-3", "abstract": "This article is part of a series of articles on GO development on Google Cloud. Read the first part to learn about project creation on Google App Engine, and the second part to learn about how to implement a basic REST service to run on App Engine environment. In the previous article, we went over how to persist data on Google DataStore using GO. The final step we need to complete in order to finish the service is write the unit tests that check the stored/retrieved data from datastore. It's actually best to write the unit tests before the service (TDD), but for the sake of readability, I wrote the service first. In order to test datastore functions locally, we need to have GAE instances/contexts mocked. To help developers, Google released a special package that leverages local unit testing called aetest . With that package we can mock new contexts and app engine instances at will, but keep in mind that this has a cost because it will really start a GAE service. To make test execution faster, we’ll create one shared context that will be used across different test functions: According to the testing package documentation : \"It is sometimes necessary for a test program to do extra setup or teardown before or after testing. It's also sometimes necessary for a test to control which code runs on the main thread. If a test file contains a TestMain function, then the generated test will call TestMain(m) instead of running the tests directly. TestMain runs in the main goroutine and can do whichever setup or teardown is necessary around a call to m.Run. It should then call os.Exit with the result of m.Run. When TestMain is called, flag.Parse has not been run. If TestMain depends on command-line flags, including those of the testing package, it should call flag.Parse explicitly.\" We’ve created a shared context and executed a func that seeds datastore with 5 dummy Users. Now we can use this shared context ( mainCtx ) on the Users tests: As you can see, I'm basically testing the \"happy-path\" and some edge cases for every exported method on User service. There's one more test to be added which is the test that will check the user.GetUsers method. This is a special test because it requires a new context in order to ensure that other tests won't affect this one: Finally, to run your test: And that's how you unit test GO services that run on Google App Engine! I hope I've helped other developers see the simplicity that Google offers on its PaaS. As I mentioned in the first article, my main goal was to create the project and start writing  good code. From this point on, you can add your test execution to a CI server, or run it manually, and if the test passes, you can use the gcloud tool to deploy your application.", "date": "2017-12-13"},
{"website": "Avenuecode", "title": "AC Spotlight - Jorge Krug", "author": ["Sthefanie Mingall"], "link": "https://blog.avenuecode.com/ac-spotlight-jorge-krug", "abstract": "Banrisul employee since 1981 and IT Director  since 2015, Krug has been invited to present lectures at several IT world events, among them several editions of CARTES (France), CardTech SecurTech (USA), CLAB (Panama and Guatemala), MasterCard OneSmart (Taiwan) and Mobile Payments International (USA). Today we are pleased to interview him for another Spotlight session on security standards in the digital era of one of the largest banks in Brazil. Avenue Code: You’re an active speaker at several conferences and events. What do you want the audiences you address to see? Where does your passion to share your story with the audience come from? Jorge Krug: Technology presents itself to us in the form of innovation which pleases me a lot. I’m increasingly interested in provoking discussions that may generate results in order to leverage new concepts and produce results, services, and products within Banrisul itself. Jorge Fernando Krug Santos. Avenue Code: The new phase of digital transformation in banks is changing the interactions between account holders and institutions. As customer habits are changing and new players are entering the market, the pressure for cost reduction has led many consumer-centric banks to turn to a 100% online relationship. How has Banrisul addressed this issue? Jorge Krug: Banrisul, not unlike other banks, has had to implement digital transformation. We have already developed a series of tools and products that we are making available to our customers. In fact, we don’t call it a digital bank, but rather, a digital channel inside a traditional bank. It’s a completely changed world that primarily uses the mobile platform. But we have to be clear that this digital world is not only a pretty app on a smartphone, but also an entire model that encompasses the internal processes within the institution. The leader in account holder market share in the Brazilian state of Rio Grande do Sul. Avenue Code: We know that security is one of the most important issues when it comes to the digital transformation of financial institutions. How does Banrisul handle this? Jorge Krug: Banrisul has always viewed safety as a very important strategic point. Among the Brazilian banks, our bank is considered to have a high level of maturity with regards to applications and treatment of the data security, channels security, and dissemination of services. In the 90’s, when we implemented the first Internet Banking movement, we took a series of actions. In the 2000’s, we realized that by going beyond the perimeter of the organization itself, meaning transitioning from a physical agency to an electronic world, we would face a series of components and threats that we weren’t necessarily prepared for. In that moment, we learned that we needed to be prepared with a series of actions focused on the issue of security when delivering services to the customer on the digital channel. Currently, the Bank is very well prepared, and we won the E-Finance Award in June of this year at CIAB 2017 for having the 1st virtual card - which is perhaps the most secure virtual card currently on the market. The Bank developed this card, as well as the physical card model, by implementing security functions for an MV application and PKI application inside the card itself. Avenue Code: During the Febraban 2017 CIAB, you mentioned that banks need to review methodologies and safety standards in order to stay current in the digital age and reach the next stage. What has Banrisul done in order to be prepared when this new phase reaches the Brazilian market? Jorge Krug: When it comes to security, you have a series of visibly enforced security gaps with regards to the speed with which things hit and the development of applications at a written code level. When delivering service at this speed, it is mandatory to have a differentiated safety treatment. We chose a solution from a physical smartcard that has a PKI application.  Another issue is the Internet of Things, a movement in which security is fundamental. When I implement a cool new feature, such as a biometric or behavioral authentication, I always have to think about how it will operate within an IOT world. Avenue Code: Tell us a little bit more about the important projects you're leading, such as the implementation of a multipurpose chip card. How do you think it will it change the way we do business? Jorge Krug: Banrisul was the first bank in Brazil to impose a model of a multi-application architecture using the MV Europay MasterCard/Visa standard on a single chip, a pattern that was developed by flags in the 90’s. We used a Multos operating system, which at the time was really the first operating system of a very efficient brand. In addition to this, another application was developed based on the PKI architecture, which is a public key structure. Currently, perhaps the great leap of technology that the bank has taken in relation to other banks is having a multi-application architecture with two basic applications: one oriented to debt and one to credit. We went to the Brazilian Government and we brought forth the Banrisul card, the first ICP Brazil certificate. At that time, Brazil was launching a national PKI, called ICP Brasil, which was the public key infrastructure for Brazil. Avenue Code: What’s next for you in your career? Jorge Krug: This is a difficult to answer this question because there are so many challenges in this digital transformation. Keeping up with society’s demands for change and highly disruptive technology has also proven to be difficult. We have to be attentive to all the movements. A public bank, such as Banrisul, is not only a great Brazilian bank, but also a great world bank. There is a need for banks to address consumers’ needs and continue technological modernization, so I think that is the big challenge - to put our bank at the forefront of this movement. Avenue Code: Thanks for your time Jorge, It was such a pleasure to learn more about how digital transformation in banks is changing the financial world as we know it. We hope to see you soon at conferences and events!", "date": "2017-10-19"},
{"website": "Avenuecode", "title": "Explore the Benefits of .NET Core 2.1", "author": ["Elton Silva"], "link": "https://blog.avenuecode.com/explore-the-benefits-of-.net-core-2.1", "abstract": ".NET Core 2.1 is finally here, and it has a lot of new and exciting features that showcase the impressive effort Microsoft has put into developing this newest version. Today, I'll present an overview of the most exciting features and explain how each can enhance your .NET Core projects. .NET Core 2.1 supports a broad range of different operating systems, and it's also made notable improvements in managing platform versions. Because of this, it's shipping as a long-term support (LTS) release, which means that Microsoft will guarantee 3 years of support for this version. It is Microsoft's attempt to get a good deployment and extensibility model for .NET tools, much like NPM global tools does. Up until now, any extensibility tool was only available per project by using DotnetCliToolReference. All tools will be managed by the \"dotnet tool\" command succeeded by its available options. For now, tools can be installed in two modes only: Global - copied to your user profile location (can be invoked directly from anywhere) Ad-hoc - copied to a location of your choosing (must be invoked via full path if not added to PATH) Some of the previously existing tools have been already incorporated into the SDK, most notably the file watcher (dotnet watch) and entity framework (dotnet ef). Project references to those tools need to be removed when upgrading from previous .NET Core versions. Although this feature is clearly a work in progress and currently offers a very limited number of tools, having a way to manage the tools and having a common repository for them means that more and more tools will be developed. A list of all currently available tools is maintained here . dotnet-sonarscanner (Sonar analysis), dotnet-serve (HTTP server) and dotnet-sshdeploy (useful for SSH deploy) are some of the highlights. The Span<T> and other related framework types were implemented in C# 7.2. They were intended to provide an allocation-free representation of memory to work with all different types/classes in the language and to help boost performance. Span<T> is a ref-like struct that represents contiguous regions of arbitrary memory, regardless of what is associated with that memory, which enables parsing and other computation without allocating. Because of its characteristics, it can only live on the stack. This limits where it can be used and referenced, which creates an opening for the use of the other new Memory<T> that can satisfy those scenarios and be used mainly for async operations. A complete description of how it works may be found here . This framework is primarily used for memory-efficient and high-performance computing with .NET, but it could easily turn into day-to-day code. To use a very simple example, you could implicitly cast a byte array into a Span<T> and modify it like so: But what is really great about Span<T> is that it allows you to safely and efficiently handle subsets of arrays, data on the stack, pointers and even cast span types. It's also important to note that a lot of methods overloads have been added all across .NET to support the Span<T> family. As an example, look at the string conversion below: This c ould be turned into an allocation-free implementation: With the help of the above-mentioned Span<T>, Microsoft built a new http message handler from the ground up to form the basis of higher-level networking APIs and to provide a faster and more consistent socket implementation through System.Net.Http.SocketsHttpHandler. It is based on .NET sockets eliminating platform dependencies for better consistency, as well as simplified deployment and servicing. It also uses Span<T> as its base, giving it a significant performance improvement. This is now the default implementation in http pipelines starting with .NET Core 2.1, and it is recommended for use with any new or upgrading project. If you want to use the older implementation, you must explicitly set it by using AppContext.SetSwitch. Windows Compatibility Pack is a NuGet package that provides access to a vast number of APIs targeted for the .NET Framework. Windows Compatibility Pack helps port existing .NET Framework applications to .NET Core. Not all the APIs are available, and some of them are Windows-only, so they need to be well analysed before migration occurs. The porting process can be better structured by following these tips : Microsoft has put a lot of effort into improving runtime performance as well. The Span<T> family is again very central to these improvements, as it's now used everywhere in core libraries. An extensive explanation of everything that has been changed in .NET Core 2.1 may be found here. S ome of the most important improvements we can list include: Threading - Reduced the overhead for calling thread static fields and hot paths. The CancellationTokens implementation was changed to focus more on reducing allocation and improving throughput than on scalability. There's also been an allocation reduction in general for async calls. Formatting and parsing - As expected, primitive types have benefited a lot from Span<T> in this area by reducing memory allocation and improving throughput. Besides the features mentioned above, there are some others that are worth exploring. For instance, .NET Core 2.1 offers the ability to publish self-contained applications, which is a great build performance improvement for large projects. It also offers a number of new and enhanced cryptography APIs, tiered compilation for adaptive optimization and the consolidation of docker images into https :// hub . docker . com / r / microsoft / dotnet / . As we can see, there are a lot of new and exciting features in .NET Core 2.1, making this version another promising step forward for Microsoft in this cross-platform, scalable world. It's important to remember that brand new versions for ASP.NET Core (SignalR, https by default, updated templates and more) and Entity Framework (lazy loading!) are also available now to work on top of Core bases. If you're starting a new project, updating an older .NET Core or even maybe in need of porting a .NET framework, I highly recommend giving .NET Core 2.1 a try.", "date": "2018-9-19"},
{"website": "Avenuecode", "title": "Debugging Remote/Containerized Apps with VS Code", "author": ["Eduardo Sousa"], "link": "https://blog.avenuecode.com/debugging-remote-containerized-apps-with-vs-code", "abstract": "Visual Studio Code's Pipe transport debug configuration makes debugging many non-standard configurations easy. In this article, I will demonstrate how to leverage this tool to debug a .NET Core application running inside a Kubernetes cluster. Note: the following instructions assume that you already have kubectl installed and properly configured. Before we start debugging, we'll need to have both the debugger and the ps command available on the app's container. The .NET Core Linux installation script is available here . Assuming that the container was based on the 'microsoft/dotnet:2.0-runtime-deps' docker image, you will need to: 1. Gain access to the container, as follows: 2. Once inside the container, execute: On Visual Studio Code, we'll need to: 1. Install C# plugin 2. Set up the launch.json configuration file. Put the following content in the ./vscode/launch.json file (relative to your workspace): Note 1: The \"justMyCode\": false will only be required if your app was built with the release configuration. PDB files will always be required to exist on the pod. Note 2: Remember to replace your-pod-name by the name of your pod and /path/to/original/source/files by the path that was originally used to build your app. Now that we have everything set up, we can debug! 1. Select \"Debug\" from the side bar, then \"Attach in k8s.\" Finally, click on the green arrow to the left of the drop-down menu. 2. After a while, VS Code will show you a list of running processes from which you can select your app. Note: If you have published your app as a Self-Contained Application , the process ID will likely be 1 . In this case, you can set the PID configuration to 1 so that you won't have to manually choose it every time. 3. Debug! After a while, you should see the VS Code debugger toolbar. Now you're all set to place your breakpoint and debug your application: This tutorial demonstrates one of Visual Studio Code's many debugger use cases. A similar approach can be employed in other scenarios like: Check out more configuration details here , and be sure to t ell us how this technique works for you in the comments below!", "date": "2018-7-25"},
{"website": "Avenuecode", "title": "Mixing Heart of Agile and VUCA Prime to Enable Business Agility", "author": ["Eduardo Ribeiro"], "link": "https://blog.avenuecode.com/mixing-heart-of-agile-and-vuca-prime-to-enable-business-agility", "abstract": "In this blog, you'll learn how to use both Heart of Agile and VUCA Prime core values and principles to enable Business Agility. I'll begin by explaining the basics of Business Agility. Next, we'll look at how simple and powerful Heart of Agile is. Then, I'll talk about the VUCA Prime framework and its importance in our current way of life. Finally, you'll see how these three concepts are deeply connected when it comes to behaviors, practices, and tools. Before we start, a disclaimer : I write based on what I've learned and lived; therefore, it's my point of view. Feel free to disagree, and we can have a great discussion. Both Scrum.org and AgileBusiness.org define Business Agility as the ability of an organization to adapt itself quickly . This adaptation may mean transforming traditional, functional, internal silos with too much power at the top of the pyramid into internal end-to-end value delivery teams with a flat hierarchy and high level of collaboration. It also may encompass responding rapidly to market changes and customer needs without compromising the quality of a product or service. Hence, whether internal or external, Agility enables a business to survive and thrive. Another exciting and critical approach comes from Dr. Klaus Leopold, a respected computer scientist and Kanban pioneer. He states that \" Business Agility is not about having many agile teams, but about having agile interactions connecting them.\" This approach warns us to pay more attention to the means instead of only transforming the parts. Why Agile Teams Have Nothing to Do with Business Agility. Image courtesy of InfoQ . Why is it necessary to enable Agility in businesses? Why have we talked so much about transformation in recent years? Why do we need to change, to innovate, to disrupt? Why are we continually seeking these things? The answer is simple: changing and adapting is a matter of organizational, professional, and personal survival . If we are not trying to find a way to \"kill\" our current way of thinking, working, and solving problems, we can be sure that there is someone else doing it for us. The challenge is all about delivering more value to our customers and being able to do this in a fast, flexible, collaborative, viable, profitable, friendly way. Acting this way should grant a company good outcomes, satisfied customers, and happy employees! So, now that we know the basics of Business Agility and why it's crucial to practice in our teams and companies, we can begin exploring how to practice it. Dr. Alistair Cockburn, one of the signatories of the Manifesto for Agile Software Development (a.k.a. the Agile Manifesto ), came up with what he called a reminder of what's essential. This is called Heart of Agile . Heart of Agile is a group of four words around a heart, and this image represents a radically simpler approach to achieving outstanding outcomes , not only in the software development process, but in every area. Image Courtesy of Heart of Agile Master the Basics If you do only these four things (Collaborate, Deliver, Reflect, Improve) over and over, you should see a positive evolution in your work and your deliveries because you're doing the essential, practicing, and mastering the basics of Agility. However, despite having only four words that are as simple and direct as they seem, Heart of Agile also gives you the freedom to expand on these concepts. From this starting point, there are many powerful ways to implement Business Agility. Have you heard of General Stanley McChrystal? He is a retired military general and author/co-author of several books, including Team of Teams: New Rules of Engagement for a Complex World . While fighting in a war, General McChrystal realized that his troops could not compete against a network of small cells that attacked quickly and unexpectedly and then hid. Although his troops were more massive, they operated under an extremely vertical structure that did not allow them to act as flexibly or as fast. The General then broke a big paradigm and began to transform the structure of the troops under his command into a team network that combined: The silos were torn down. Team best practices were analyzed and quickly disseminated to the rest of the teams. The change in the organizational model - from vertical, inflexible, and decision-making in the hands of a few - to a \"team of teams\" was the key to their success. The image below gives us a good idea of what they did : From a command structure to a team of teams structure. Image courtesy of Medium. General McChrystal had to do what he did because he was in a VUCA scenario. VUCA is an acronym first used more than 30 years ago to describe the world's new behavior at the time. In the late 1990s, it began to be more discussed , mainly in relation to military education. VUCA stands for Volatility, Uncertainty, Complexity, and Ambiguity. Let's examine each of these in depth: Volatility The amount and speed of changes nowadays make it very difficult to predict scenarios as we did before. However, these occurrences are not necessarily hard to understand (knowledge about it is often available). The main problem here is the lack of stability and the sudden turn of events . Uncertainty Low understanding of issues and events due to insufficient information; this results in a lack of predictability . Complexity The issue or event has many interconnected pieces (e.g., high coupling in software engineering). Information is partially available or predictable, but the volume or nature of it can be overwhelming to process. Ambiguity Uncertainty connotes a lack of predictability because there's not enough information. With Ambiguity, on the other hand, you can have enough information, but the causal relationships connecting information are entirely unclear. There's a lack of understanding of precisely what the situation is . Despite the fact that VUCA has been around a long time, this is still our reality. We try to keep up with scenarios and problems that change all the time; we find multiple causes for our problems, instead of one single root cause; and we make decisions that always come with some level of trade-off, which often leaves us to choose the one with the least impact on our projects or businesses. It is almost as if we are sailing in rough seas without clear directions and with minimum help from navigational devices. We always want to get to a safe port or beach (a.k.a. solving problems), but we also want to do this in the best way possible. VUCA Prime framework. Image courtesy of Slide Model . With that in mind, Bob Johansen, former president of the Institute For The Future (IFTF), released a new concept about this turbulent world. He called it the VUCA Prime Framework . These are the skills/attributes we must develop, incorporate, encourage, and spread to our teams, peers, and leaders so that we can reduce the paralyzing effects of VUCA . Vision can fight Volatility When we have a clear purpose and well-defined goals , we know why we are doing something and what are the main criteria for success. Hence, the conditions do not affect us much. We'll probably recover faster and incur less damage than we would if we were clueless about the vision. Understanding can fight Uncertainty Everyone should understand the business vision, their role in the team, and their particular contribution to the vision's success. Values, strategies, and metrics should flow dynamically across teams. It is vital to promote constant communication and active listening to give and receive understanding. Clarity can fight Complexity We usually get Clarity by making things simple without being simplistic . This involves bringing matters to the surface, understanding them, and prioritizing them. For example, this might mean identifying all the actors involved in a complex situation, their problems and goals, their causal relationships (if any), and so forth. Agility can fight Ambiguity We fight Ambiguity by identifying our assumptions and hypotheses and then validating them. This requires processes that allow us to experiment, fail, learn quickly, build, test, measure, learn again, and improve . So, it's about Agility. So, let's wrap everything up! Here is a list of behaviors, practices, and tools that I consider of great relevance: As we saw, there are many possibilities and variations for enabling Business Agility. This blog's purpose is to highlight a few of these. Key takeaways: it's crucial to remember that you must seek to master the basics of Agility (Collaborate, Deliver, Reflect, Improve) and always fight VUCA effects (Volatility, Uncertainty, Complexity, Ambiguity) with VUCA Prime (Vision, Understanding, Clarity, Agility).", "date": "2019-11-27"},
{"website": "Avenuecode", "title": "Creating Adaptive/Responsive HTML Emails with MJML", "author": ["Mark Carlson"], "link": "https://blog.avenuecode.com/creating-adaptive-responsive-html-emails-with-mjml", "abstract": "Creating HTML emails is hard — much more difficult than creating a web page in HTML. Bootstrap and Foundation spoil front-end developers.  Slap a couple classes in your code and you can have a beautiful, or at least beautifully rendered, adaptive/responsive site in nearly every browser.  Not so with email.  Many email clients strip external CSS, leaving only inline styles.  Some email clients render only a small subset of CSS.  And of course, JavaScript is out of the question. This article was originally published on MarkCarlson.io and is republished here with permission from the author. Fortunately, there is a tool which makes creating HTML email pretty easy.  Plus, the email renders nicely in both desktop and mobile.  It’s called MJML . Let's take a regular old HTML email and look at it in a desktop mail application: This is a plain old HTML email. It has a masthead image, different font sizes, font weights, centered text, etc. Let’s take a look at this same email in a mobile device: Even with a retina screen, users will have to use their fingers to zoom & scroll to read the content. With MJML, you compose the email using MJML markup.  If you are familiar with HTML, it’s not hard at all to learn.   Peruse the docs and you can be composing adaptive/responsive HTML emails in an hour or so.  As we work in the app, we compose the MJML on the left and view the resulting rendered HTML in real time on the right: A closer look at the code on the left and you can see how the top of this email is created: Unlike HTML in email, we can even use a class structure for our styling.  For example, we could refactor the above markup using HTML-like classes: When it comes to converting this to HTML, MJML does all the heavy lifting for you to insure your email looks good in all kinds of email clients.  The HTML it generates looks like this: Yuck!  Aren’t you glad you have a tool that will do this for you?  But how does it look?  You be the judge.  Here’s the desktop view: And now, that same email on a mobile device: “Look, mom!  No pinching & zooming!” Try MJML .  Spend a little time looking at the features and gallery of samples .  For HTML email, you’ll wonder how you ever lived without it!", "date": "2017-10-17"},
{"website": "Avenuecode", "title": "Inside Java Collections", "author": ["Arlindo Neto"], "link": "https://blog.avenuecode.com/inside-java-collections", "abstract": "Since all computational processes take time to execute, the most important thing to understand when we have more than one option we can pursue is the trade offs between them. Then we can select the most suitable implementation that ensures both functionality and performance quality. Perhaps the most important consideration for collections of any kind is how the number of items stored in them affects their operations. We can describe this generally by using a big-O complexity notation. So when we say that an operation has a complexity of O(n), we mean that the underlying algorithm takes a linear amount of time proportional to the number of items in the collection. On the other hand, for a complexity of O(1), we have a constant time for its execution, regardless of the collection’s size. With this concept in place, let's examine what Java collections can do for us and how they work. In almost all programming languages, a collection is a way to group together values or their memory references in a way that makes it easy for us to add to, fetch, remove, or search them. In Java, the API specification defines the interfaces for all kinds of collections, including lists, sets and queues, all of which extend from a collection global interface with their particular operation contracts. Here's a list of the interfaces and the most important methods they expose: As we can see, certain collections work better in certain scenarios. Let's take a closer look at how each collection's implementations structure items to handle different situations. The simplest collection implementation available in the Java API is the ArrayList. This is the collection you'll want to use if the amount of data is small, if it can repeat, and if the order the data is stored in is irrelevant. Given its indexed array structure, adding and getting elements have a constant complexity (O(1)) with the coast of a linear complexity (O(n)) to remove or search elements. Another simple collection we can use is the Vector. A Vector has pretty much the same implementation as the ArrayList except that it includes the advent to be synchronized, which means that it is thread safe. Collections with linked data structures have the ability to keep the order in which elements were added when iterating over them. This is important when, for instance, you're handling some already sorted data from a database. When retrieving this kind of data, we don't want to lose the order in which elements were fetched from the repository, otherwise it wouldn't make sense to delegate this extra work to the database. The simplest linked structure is the LinkedList, which can add and remove items with a constant complexity (O(1)) but has a linear complexity (O(n)) when getting and searching for them. LinkedList collections implement the Queue interface as well, giving this concretion another important role in the Java API. Also, all Queue-exposed method implementations have a constant complexity (O(1)), giving this class all we need to work with FILO solutions. Hash structured collections are the most interesting kind of implementations because they use maps as the underlying repository for their items. As you may know, a map is a data structure that maps objects to keys, and, in the case of hash structured collections, these keys are the items' hash codes. You could be asking yourself why this is required and if it's worth the cost of the implementation and the bigger footprint. The answer is a huge and sonorous YES!! This is because it makes a collection's search complexity constant (O(1)). But, to do so, we must make sure all of the items are unique, given their equality contracts. That's why this kind of structure can only be used in Set interface concretions. It's important to remember, however, that giving equality contracts is not a straightforward procedure. The way we do this will mostly depend on the business rules following the Java API specification. The Java Object class, to which all other objects extend by default, has two important methods for supporting the definition of equality between any two instances. They are the equals and hashCode methods, which have the following mutual contract: The hashCode method returns the in-memory address of any object by default. This way, two objects will only be considered equal if they are literally the same instance, which is not a very useful constraint. For most of the cases, we need to compare objects given their states, that is, their attributes' values. You may think that just implementing the equals method would be enough for this, and in most cases, it is. But not for hash structured collections. As mentioned previously, elements are mapped by their hash codes when stored in this kind of collection. The important thing here is that this key is the main criterion to determine equality between items. If we don't follow the contract between those two methods, the business rule of the application could fail, even though your equals method has been appropriately overridden. When discussing hash codes, it's important to note that if two objects have the same value, it doesn't make them equal. When we have this scenario, which is called hash collision, the values are stored using a linked structure in the same key. When this happens, we can see keys as a bucket map, in which objects with the same hash are stored together and checked for equality using the equals method. The consequence of this is that we can override the hashCode method to return a constant, and it'll work as expected. There will, however be a loss in performance, because linked structures will result in a higher complexity execution time for searches, compromising all the benefits that this structure can offer. So, to make sure we are making use of all the features of a hash structured set, here is the rule of thumb: In the Java API, we have two implementations that use this kind of structure. The simpler one is the HashSet, which doesn't keep the order elements were added. If you need to keep items ordered, a LinkedHashSet should be used instead. Both have an add and search constant complexity (O(1)), but the former has iteration complexity given by O(h/n), which means that the more elements it has, the faster it will be (where h stands for capacity). Tree structured collections are another important kind of Set interface implementation. Instead of using a hashCode contract, they use another mechanism to determine equality between elements, which also results in their most important feature: elements are sorted during the collection iteration process. Tree structured collections need to implement the SortedSet interface and require that their elements implement the Comparator interface. This way, elements can define a rule to be compared with each other, allowing them to be sorted and considered equal in certain circumstances. As a price for this, we have a logarithmic complexity (O(log n)) for add, remove, search and iterate operations. When sorted sets are required and we can’t delegate sorting to an external system, such as a database, this highest complexity can be ignored for a small number of elements. The only implementation of this kind is the TreeSet. The goal of this article is to present all these important considerations in just one place to make it easier to reference collection features at any time. Of course, there's always more to say about such a complex topic, but I believe that here I could do little more than scratch the surface when discussing the core implementation details of most of the commonly used collection types. Happy coding!", "date": "2018-8-29"},
{"website": "Avenuecode", "title": "Responsive Web Design and Mobile First: 5 basic techniques", "author": ["Lucas Estevão"], "link": "https://blog.avenuecode.com/responsive-web-design-and-mobile-first-5-basic-technics", "abstract": "About a year and a half after reaching 1 billion active users, Facebook hit another milestone: over 1 billion active mobile users. Mashable, one of the biggest sources of content about social media, internet news, and digital innovation, published a survey stating that 17.4% of all web traffic comes from mobile devices. This survey was published in 2013, since which time the percentage has steadily crept upward yearly. Social networks like Tumblr are changing features formerly found only on websites for desktop to mobile platforms, synchronizing both contexts. Another study , also published by Mashable, shows that 65% of all time spent on social networks is from mobile devices. According to a 2014 report from IBM, \"for the first time, online traffic from mobile devices outpaced traditional PCs on Thanksgiving day\" ; according to a 2015 study from ComScore, mobile devices \"account for 60 percent of digital media time spent\" . Already in 2016, according to this article , \"30% of all online shopping purchases now happen on mobile phones\" . All of these articles point to the same fact: the reasons to apply the concept of responsive design are many, and they are only growing. Developing websites (or any web application) with a flexible and adaptable layout and context, capable of responding to a large variety of screen resolutions, devices and contexts, is called Responsive Web Design. Imagine the difference between a device whose screen is 320px wide and another device with a screen side of 1024px. The space left by the difference of size is too large to simply stretch the layout, adapting the content to the available space. Leaving a rough blank space is even worse. Clearly, this is a case for responsive design. Some techniques allow us to offer the final users a much better experience, adapting ourselves in a really significant way to each type of device. When planning for responsive design, we define the basis of the application design for a given screen size, and from there we can adapt the content progressively to different screen sizes, applying different design rules. These rules are directly linked to the size of the screen, since they may restrict certain features of the web site. For instance, displaying a heavy image gallery can provide a poor user experience on mobile devices. Hiding or displaying sections of the web site, reducing the size of images or adjusting font size so that the page has no horizontal scrolling, and ensuring that no piece of the content is superimposed - or even exchanging a horizontal menu with a vertical selection box - all are examples of design rules that can be manipulated to allow the application to respond to available screen size. The impact to user experience cannot be overstated, especially on devices with smaller screens. When we decide to apply this concept, we need to begin somewhere and therefore need to decide what the focus is. One option is to begin the planning by considering the restrictions of mobile devices. Mobile first, then, means that the basis of design as explained above, is defined for the smaller screen sizes found on mobile devices. In other words, begin by developing the application with an iPhone 4S (320 x 480) in mind. By focusing first on the limitations imposed by mobile devices, we can more easily develop adapted versions for larger screens, graduating to tablets, desktops, and even smart TV's. This is mobile first. So how do we go about creating responsive design? Below we'll explore 5 basic techniques to implement responsive design, focused on mobile first. This article will stick to fairly basic methods. In an HTML page, each element is represented as a rectangular box, which is interpreted by the browser using the standard box model (CSS box model). This model determines the content of the space occupied by an element, which is to say, the ratio of the element and its content to its attributes of margin, padding and border. The CSS property box-sizing is what defines how this box model will behave in relation to its size and spacing measures. The default of the browser is the content-box , which considers the original size ( width , height , etc.), added to the spacings ( padding , margin , etc). But the way the box-sizing: content-box works is not intuitive. If we want an element to occupy 50% of the screen with a border of 5px and, therefore, define width: 50%; border-width: 5px; the result will be somewhat more complicated, because the final width of this element will be equal to 50% of the screen width added to the 10px edge. If the screen has 100px , the result would be 60px width for the element. Therefore, it's important to make sure all the elements have their box-sizing as border-box , which will prevent the size of all properties from being added to the width defined for the element. To get the result of a fluid layout, we need to use the relative measures ( % , em , rem , vw , vh ), which provide fluidity to the site when the screen size or font are changed. We use each of these units of measurement as follows: PX: absolute measure used for immutable values such as background , text-shadow and html font-size . %: classified as data type and not units of measure by the W3C specification. Used in containers ( section , article , div ), fluid grids, and images. EM: relative distance unit, calculated in relation to another unit of measure. Its value is relative to the size of the parent element font (i.e., an element with font-size: 2em; will be twice the size of the parent element source, whatever it is). This is used in font-size , margin , padding . Unlike percentage, the em is a fixed value like the pixel, but its value is calculated from the value of another element. REM: like the em , it is a relative distance unit. Its value is relative to font-size document's root (i.e., an element with font-size: 2rem; will have twice the font size of the document root element, which may be the html tag or the body tag, depending on the CSS rule that was applied). Also used in fonts, margins and paddings, rem doesn't work in some versions of Android and IE <9. VIEWPORT UNITS ( vw , vh , vmin , vmax ): used in containers, grids, images and even fonts. This is a truly flexible and adaptable unit. The vh is equal to one percent of the viewport (size of the browser screen) height. For instance, if the browser height is 640px , 1vh is equal to 6.4px and, in the same way, if the viewport width is 320px , 1vw is equal to 3.2px . It is necessary to convert the typography following the specifications above. For instance, we will apply the font-size of 16px in the body and adapt our fixed units using the following calc: SIZE ÷ CONTEXT = RESULT , as shown in the example below: Before: After: The viewport is the screen visible area of the device, where the website is displayed. The viewport customization allows us to define the visible initial resolution of the website, and avoid its miniaturization. In the below example I suggest a more complete configuration, disabling the zoom feature, and assuming that the user will never need this feature (use with caution). Media Queries are conditional expressions to apply different CSS styles, depending on certain rules such as the viewport width. Through its use, we define points of considerable changes in the layout of the page. For instance, displaying a certain element only to devices with big screens, like a desktop. Beyond the size rules we saw above, we can also define rules for device orientation, screen aspect ratio, or even resolution, among others. SVG stands for Scalable Vector Graphics . It is an XML language for describing in a vector shape drawings and two-dimensional graphics, either statically, dynamically or as an animation. We can incorporate SVG in the website responsively using the object tag in order to provide an image alternative to browsers that don't support SVG. These are just a handful of basic techniques, and there are plenty more! Some other techniques to develop responsive design focused on mobile first worth checking out are user agent , HTML 5 tags, Responsive Navigation and many others. My recommendation is to use one of these two CSS frameworks: Bootstrap or Foundation , and to take some time to read this informative article from Smashing Magazine . Now it's up to you! Keep studying, keep practicing and if you find any cool technic to implement responsive design, come and share it with us!", "date": "2016-11-2"},
{"website": "Avenuecode", "title": "Does a Design Sprint Actually Improve Anything?", "author": ["Kennet Emerson Avelino Calixto"], "link": "https://blog.avenuecode.com/does-a-design-sprint-actually-improve-anything", "abstract": "What is a Design Sprint, how does it work, and how much does it contribute to successful product development? Nowadays, there are several processes for software product development. These processes, both traditional and agile, were created to methodize what was before a complete absence of procedure that eventually became known as \"the software crisis\". These methods are focused on improving product value and delivery efficiency. Traditional methodologies have very well-defined, linear steps that proceed from mapping requirements to design, implementation, and maintenance. There are some disadvantages to traditional methodologies, though, because they don't allow the flexibility to go back and redo certain steps. Agile methodologies, on the other hand, are more flexible. Within agile, Scrum is widely-used. It is very well-defined in some points, but unlike traditional methodologies, it is flexible during the product design phase. Another agile idea is the Design Sprint. A Design Sprint keeps agile flexibility while providing detailed instructions on how to run the product design phase in an organized way. The Design Sprint is a method of testing and applying new ideas for a product through five well-defined steps; the Design Sprint is currently in version 3.0. Unlike a simple consultation for developing and detailing an idea, a Design Sprint is collaborative, involving both the client and the company creating the product. It's important to mention that it isn't mandatory for an outside entity to run the sprint; it can be conducted by the internal team. Let's take a detailed look at the Design Sprint process. Before we begin... It's important to note that it's widely recommended to have seven team members for a Design Sprint. This number isn't mandatory, but it is usually the most effective for speed and quality. Building a team with multidisciplinary characteristics (different ares of expertise) is significantly important in achieving good results. Making team members aware that they are working with others who possess diverse expertise is also important, as it leads to better insights. Finally, it's important to define which team member is responsible for making final decisions. The five days: the Design Sprint process Image courtesy of Google Ventures via Charity Digital A Design Sprint takes place over five days within a well-defined time box. The first of the five days can be thought of as \"begin with the end in mind.\" This is where the long-term goal is defined, the reason and expected result are discussed, and the obstacles and tasks are listed. The whole process is conducted democratically, by voting, and considering the point of view of each team member. Also on the first day, the entire scope of subsequent steps is discussed. On the second day , each team member independently draws a sketch of his/her idea in a storyboard format. (Later, the group votes on the best version.) Also on day two, research is performed regarding similar existing ideas. Afterward, the team analyzes the weaknesses (such as threats) and strengths (such as opportunities) of each idea. On the third day , the team will analyze and compare every sketch and decide which is the closest to the goal defined on day one. In other words, the most promising idea for designing and testing a prototype is chosen. If two ideas win the vote, they should be evaluated to see whether the solutions can be unified into a single plan. After that, a storyboard is created to define the steps to create the prototype. Even ideas that were not selected as the best are still kept since they can be used in subsequent sprints if needed. The fourth day is marked by the creation of the prototype. The focus of this day is to create the most realistic artifact possible in 8 hours. In addition, part of the team must lead the preparation for tests that will be performed the next day by detailing test scripts, choosing interviewers and interviewees, etc. On the fifth and last day , the prototype is tested according to the scripts designed the day prior. While the interviewer directs the process, the other team members take notes while watching the procedure from a separate room. At the end of the day, the prototype should be improved by discarding features that were unsuccessful in the interview. What's next? After the first five days and the validation of the prototype, the next step is deciding whether or not to construct the product. The decision should be based on the results of the interviews. It's important to note that there is no defined road map to follow after the five-day sprint. The team should evaluate whether they need another week for a second sprint to add more value to the product by adding more details. Also, after following the steps of the Design Sprint above, you can start building the product following agile methodologies like Scrum or XP. If you take a close look at the steps described above, you'll find that the Design Sprint offers detailed and specific steps to guide the team in product ideation so that they can know what's finished and what's next. Another important characteristic is the presence of engineers as part of the team during product design. Having engineers in the Design Sprint can help in that it decentralizes activities and ideas from a single technical member of the team, such as the requirements analyst in traditional methods. Engineer involvement also helps when the goal is adding new features to an existing product (the Design Sprint is not only for building brand new products). Beyond all this, the Design Sprint also reduces the chances of building an idea that fails, thus reducing the number of sprints and the amount of effort needed, since the fifth day is focused on exposing problems through interviews. For this reason, day five is crucial. This is the day in which team can closely consider how the prototype would be experienced by the end user. This is where the weaknesses and strengths of product ideas come to light. Later, during production, the information acquired on the last day can be reviewed and addressed to strengthen the end product and thereby improve user experience. At first, the Design Sprint may seem like a complex process, and you may wonder whether its benefits outweigh the time and money it requires. But the benefits of a Design Sprint go beyond the improvements we listed above to include the feedback of core functions and success even before the whole product is built. In addition, if you run a Design Sprint and your idea fails, there's no need to worry, because you just saved money and development time by not pursuing an idea people won't want. Beyond this, if you don't know where you should start a project, or if you want to come up with ideas for new functionalities for an existing project, a Design Sprint is exactly what you need. For references and further reading, check out: Sprint: How to Solve Big Problems and Test New Ideas in Just Five Days by J. Knapp et al. Scrum: Gestão ágil para projetos de sucesso by R. Sabbagh. Google Venture. The design sprint, 2019.", "date": "2019-10-9"},
{"website": "Avenuecode", "title": "A Web History: The Origin of Bundlers, Part 1", "author": ["Igor Rezende"], "link": "https://blog.avenuecode.com/a-web-history-the-origin-of-bundlers-part-1", "abstract": "Are you an aspiring JavaScript Developer? Have you been writing JavaScript for a while but lack confidence when talking about modules? Have you run across terms like \"module loaders\" and \"module bundlers\" and wondered what they mean? Have you heard about webpack, Browserify, CommonJS, and AMD and not understood why we use these things? Follow this three-part series to learn more! The adventure begins with some history on the need for modules and why so many tools were then created to work with them. So have you got your coffee mug? Then come on! It will be a long story. The Origin Breaking the Code New Problems Arise The First Script A First Approach to Modules The Second Script His Own Bundler Generated Code Webpack and JavaScript Afterword Preamble: Inline Script There's no better place to start than with this oft-heard term! Inline Script brings back good memories of how I wrote my first web applications. Inline Script is when you add scripts directly to your page, inside an HTML tag called <script>. That's how I started, and I think most other developers probably start there too. It's a good way to begin coding quickly, to be honest. There are no files or external dependencies to worry about, but on the other hand, it's a perfect recipe for hard-to-maintain code over, and there are some reasons for that! These reasons include: 1. Lack of code reusability : If you need to add a page to your website that will also include some function or feature of another page you already have, you will need to copy and paste that code. 2. Lack of dependency resolution: Depending on the way you write your script, you will be responsible for organizing the functions in a specific order so that they will already exist at the time they are used. 3. Global scope pollution: All functions and variables will live within the overall scope of your application. Later on, you will understand the problem with this. Although I'm not going to explore the nitty gritty details of Inline Script in the history that follows, I could not neglect mentioning this subject entirely. We can now begin our story, having established why the use of Inline Script introduces certain problems (and by the way, these problems also apply to Inline Style!). Scene: a young programmer who owns a single script of 500 lines of code, a script written with a lot of dedication to the web page he is working on. It is composed in an epic language of unimaginable features (whose witchcraft is incredible) called JavaScript. Jay Ess is his name, and his life is a very good life. His script was, in the beginning, very handy and of reasonable size. But time is said to be cruel to some, and it was no different with Jay Ess. With the passing days, the project on which he worked day-to-day grew, and in that flow of growth he began to add more features to his page. Not only that, but he became a team member because his company hired new developers to work with him on that same web page. If Jay Ess continues with his current configuration (a single script file to which he and possibly \"others\" might start adding new features), he will encounter two problems on his journey: 1. His script file will grow. It will grow like the chuchu vegetable in the fence (those who come from the countryside know this expression). That is, it will grow very fast with each new functionality. This is bad because large files are difficult to maintain, and we know that it's complicated to find specific sections. Beyond this, we also know that it's difficult to have multiple developers working on the same file at the same time. 2. One day, Jay Ess may want to use some external libraries. While not technically a problem, this could further complicate the situation. With this problem troubling his mind, Jay Ess uses his incredible acumen and obviously decides to break this huge script file into several smaller files. He does this not only because the file is large, but also because it's good practice to separate things into independent parts to keep the code organized and clean, right? This is even justifiable since developers may want to separate functions that handle DOM manipulation from other functions that handle http requests or data processing, etc. Let's say Jay Ess eventually had a file size of 10KLOC, and in his above-mentioned change, he quickly broke the file into smaller pieces, getting 20 smaller script files, sizing 500LOC each. The result is neither great nor wonderful, but it's definitely better. ( KLOC = 1,000 lines of code [k = 1,000; LOC = lines of code].) But what is Jay Ess's plan for managing these files? He concludes that the simplest solution is to put 20 <script> tags on his page, or maybe 20 other tags for the libraries that he added throughout this process, such as lodash, jQuery (just to keep up the drama), and other plugins. Yet Jay Ess still faces a significant challenge: maintaining order. He has to be careful when inserting tags to make sure that some things (such as utility functions) will be available before other parties use them. Jay Ess's life has improved in some ways: he has smaller files, which help in his day-to-day work routine. But things are not all moonlight and roses, you know. This poor guy added some brand new concerns, and these are concerns he needs to address right away. Not only does he have to track the order of his <script> tags, but when his team members add new functions, they also have to decide where to place them. How should they store files so that they will be available when called for? Is it better to add a new file and a new <script> tag? Jay Ess is deep in thought as he opens a package of Doritos… Also, now that he understands the importance of separation of responsibilities (the first of the SOLID principles he studied in college once upon a time), he is producing a larger number of small files that he’ll have to organize into folders. Quickly his web app reaches 50, 80, 100... JavaScript files. Just as Jay Ess can see that his code is more stable, so we too, as spectators, can see new problems emerging. Jay Ess was already worried about the number of <script> tags he has produced and the order he has to maintain ... and now a new challenge emerges. His files define multiple names, such as function names, variable names, etc. And every time he chooses to change some file, he has no idea what names the other files have already declared. Collisions could happen! Development becomes a tedious task-- he has to remember several names that have been used before or keep looking for some convention, or ... well, you get the idea. Now he has a new problem to solve. Seeking the wisdom to solve his problem, desperately searching for tutorials, books, reddit and everything else, Jay Ess finally discovers the so-called Module Patterns : let something = (() => { // so called \"encapsulation\" here // ... // and then... return { firstPublic: ..., second public: ... }; }) (); Basically, this pattern allows developers, through the use of closures, to encapsulate code. The thing returned and assigned to some of the above has methods and properties visible, and these methods have access to local things defined within the expression of the function, to which no one outside it has access. So, in a way, it's this structure that can allow Jay Ess to write encapsulated blocks of code with private visibility. But why is this relevant to him? This can help him avoid his problem with name collisions. Aha ! (If you have not perused the information in the link above , I highly recommend doing so if you still do not know what this means.) So w hat Jay Ess decides to do is create this structure in each of those 80, 100 ... files he has, swallowing the entire contents of each file. And in the end, this structure will return only the things that he really wants to be visible. That is a huge gain: Jay Ess can break files into smaller files without worrying about name collisions. On the other hand, he has 180 <script> tags or even more than that on his page. And you remember that concern about the order of the files? This is still a problem, but it's a problem that will remain unaddressed until next week’s installment of A Web History: The Origin of Bundlers!", "date": "2019-1-2"},
{"website": "Avenuecode", "title": "How to Develop An Automation Framework - Part 2", "author": ["Meera Honde"], "link": "https://blog.avenuecode.com/how-to-develop-an-automation-framework-part-2", "abstract": "In a real life scenario, automation framework is designed as a structural and conceptual module that provides high extensibility, maintainability, libraries, and guidelines for future development. The f ollowing are basic steps to help start designing and implementing automation framework using Selenium Webdriver, J-unit, and Java in Eclipse IDE. If you missed part one of this tutorial, don't worry, just click here . Once you choose a framework, set up your project in the given IDE. Writing the first automation code: When you write automation tests for web applications, all tests are mostly click events. All user interactions with the system happen by clicking, selecting, dragging, and dropping events. Code sample for Chrome browser: This sample test will open Google Chrome browser. Go to google.com and click on the Gmail link. Check if the Gmail login page is displayed by checking page title. After it has been validated, the browser will close. Code sample for the Firefox browser: This sample test will open in the Firefox browser. Go to google.com , and click on the Gmail link. Check if the Gmail login page is displayed by checking the page title. After it has been validated, the browser will close. Now, let’s try a sample example for defining a page object model. In a page object model, every web page is seen as a WebObject, and every web element on that page can be considered a data element or member of WebObject. Each user action can be simulated as a web element action like click/submit/select or send text. In web application, almost every home page has a login link. Consider when a user clicks on a login link/button, the user returns to the login page. Now, declare the login page as one WebObject and the home page as another WebObject in your automation code. It should look like this: You can use Firepath/Firebug plugins to find the exact XPath or different locators for each web element. In the test class, you can just invoke the LoginPage object and call the Login method to Login so you don’t have to repeat the code for the Login method in the Test class. Here's an example: When developing code, make sure that the page object: Refactoring the Code for the Page This can be implemented using a page object pattern by making the home page a utility class that will implement functionality and web elements on the home page. The sign in page can be used as one object for the login user, and then all other apps can be created as separate WebObjects that can be aggregated from the sign in page. All the web elements that are common to each page can be moved to some common class and declared as constants. C reate a new package such as src/test/resources to add property files where you configure the details of a user ID, password, web app URL, and data files path. Create a separate test class where you can call required WebObjects shown in the examples above, and validate the test by using different assertions in your test. You can also minimize implementation by trying to avoid duplicating the same functionality. We can refactor the code above into the code below, to obtain inheritance in our test cases as well as improve readability and usability. Let’s say the login page is declared as a parent class, which will setup the driver/browser, and provide login functionality. Once tests are completed, it will close the browser. We can extend the LoginTest class, which will inherit properties from the Base Class and add new properties of its own, which can validate all login test scenarios. Configuring the data in your automation code: This is implemented in the base test class, b ut when you need multiple values for the same properties in your test runs, then defining properties in the property file is actually not good idea. In that case, either using CSV or Excel files is a better option. In order to use input data in XLS  format, follow the steps below: -Download the POI API from Apache using: http://poi.apache.org/download.html -Unzip or extract all the files from the zip or .tar.gz file and add all the jar files to the project path (right click on project-> build Path ->configure build path->libraries->add external Jars) -Once all jar files are added to the project path, test your input data file with the sample code below: Integrating your code in continuous integration server: And that's it! I hope you've found this article helpful. I'd love to hear how this tutorial worked for you, let me know your thoughts and experiences.", "date": "2017-11-21"},
{"website": "Avenuecode", "title": "AC Spotlight - Bob Land", "author": ["Holly Camponez"], "link": "https://blog.avenuecode.com/ac-spotlight-bob-land", "abstract": "(The following article is property of Avenue Code, LLC, and was originally published with permission at TotalRetail on September 11, 2018 .) Bob Land chats with us about the incredible growth he’s achieved as vice president of consumer engagement for Dorel Juvenile , a global leader in high-quality, safe and fashionable juvenile products from brands such as Cosco, Safety 1st, and Maxi-Cosi, among others. Avenue Code: Can you tell us the story behind your title? Bob Land: The traditional CMO role encompasses brand and channel marketing, but it doesn’t include consumer care. Our CEO, Paul Powers, and I, however, felt that my role should interact with all of our consumer touchpoints, from e-commerce sales to digital and consumer care. I credit this first decision for driving us into our “victory lap” year. After four-plus years of transformation, our sales are up and our teams are winning awards like Walmart’s Omnichannel Vendor of the Year, ICMI Best Social Support, Stevie Awards, etc. AC: You joined Dorel Juvenile nearly five years ago and were quickly promoted to vice president of consumer engagement. Now, Dorel Juvenile’s Amazon . com sales are up 75 percent year-over-year. How were you able to boost online sales so dramatically? BL: Actually, our 75 percent bump came between my second and third year. Over the last four-plus years, our Amazon sales have increased over five times. I credit much of this success to our CEO’s vision and our pragmatic approach to growth. We knew we wanted to grow with Amazon, but instead of just opening an account to start selling, we took a hard look at the Amazon Scorecard KPIs and decided to focus on operations. This wasn’t the most appealing path, but it was definitely the best investment in our future. Part of our work involved organizing all our SKU info into a central PIM, for which we contracted a solutions vendor, which was one of the best decisions we’ve ever made. AC: Dorel Juvenile has about 25 brands in its portfolio. To what extent do you customize the e-commerce marketing strategy for each brand? BL: Our strategy changes for each brand based on whether the products are OPP, MPP or HPP items. For HPP/Premium, for instance, we’re dealing with MAP items, so we don’t need to be concerned with price compression (i.e., when Amazon matches Walmart’s price, sending unit volumes skyrocketing but driving down margins). However, we do need software to enforce MAP. For OPP, there isn’t enough margin for us to promote heavily online, so these items are becoming a store-only play — quite the opposite of the trend! AC: With a massive global presence and products available in more than 100 countries, how do you maintain a cohesive omnichannel experience for customers? BL: Maintaining an omnichannel experience for customers globally only extends to the core brand promise. Everything else gets localized for each country, and sometimes even for each region. For instance, Maxi-Cosi enjoys a 60 percent-plus market share across Europe, so online features tend to be of the “jump the shark” variety. Customization, 3-D visualization, etc., simply communicate Max-Cosi's status as a market leader. In the U.S., Maxi-Cosi has only been around for a few years, so the experience is less “sizzle” and more “steak,” which means we focus on areas like amazing, tell-your-friends customer service. AC: You’ve spoken to the importance of creating programs that connect with consumers on an emotional level. What are the most effective technologies you’ve found for supporting this relational marketing strategy? BL: We work with some amazing technology vendors, but I feel that the best is yet to come. Recently, drip marketing has pushed beyond email and into multichannel modes. Next-gen vendors like Acquia can now map your entire customer journey pre- and post-sale to the right type of action. That could mean anything from sending a text to make sure the out-of-box experience is as great as it should be to firing off an order to ensure that a technician is on their way to do an in-home setup. Tech like Button can also tie popular apps together for brands that may not need their own app and only want to be a small part of another app. The best example I’ve seen is making a restaurant reservation on OpenTable and having a button pop up from Uber saying, “Do you want to get a ride to dinner?” This is an amazingly seamless user experience. AC: What does Dorel Juvenile do to stay on the cutting edge of tools and technologies? BL: Our teams attend large shows such as Shoptalk for e-commerce and ICMI for consumer care, but I prefer the VIP-only, invite-only groups of fewer than 100 attendees where I can speak relatively freely with my peers. Groups like Execs in the Know Know and CommerceNext are perfect examples. AC: How do you take learnings from one of your divisions and apply them to others? BL: We’re physically located in the Baby Gear group, which is one division out of three (the other two being a Furniture group and a Sports group). The Furniture group was first to dive down the e-commerce path, and our group has iterated with it. Operationally we’re separate, but when one starts seeing success with a new vendor, we all join in. Moms display Dorel strollers. | Credit: Dorel Juvenile AC: Before coming to Dorel Juvenile, you spent one year completely rebuilding a 200,000 affiliate cost-per-click ad network for Chitika, whose ads competed head-to-head with Google AdSense. How did this high-pressure experience of transforming teams, technology and workflow influence your strategy for digital culture transformation at Dorel Juvenile? BL: Working on a product that competes with Google’s core offering was like staring down a charging bull, so it was a great rush when we started winning conversion rate A/B tests on Yahoo owned-and-operated properties like Yahoo Finance. We built our own programmatic ad platform, real-time bidding platform and mobile ad platform, so we were quite prolific. Coming to Dorel, I hired eight of my former colleagues to fill key roles. In my opinion, core transformation teams are teachers as much as they’re doers. AC: What do you see as the key ingredient to successful digital transformation? BL: Achieving adoption into the culture is the ultimate goal. Transforming a company is 100 times more difficult than forming a team from scratch. Many times, this is because existing employees may have worked at the company for 20-plus years, and they have a certain social contract with the company — I promise to do X if you pay me X. When they’re asked to learn new software, let’s say, some are excited because they’ve been waiting for such a change, but others feel like they’re being asked to jump out of a lifeboat. Identifying what people have signed up to do is critical to understanding where to push and when to pump the brakes. One of our newer projects is probably my favorite: We’ve been building on a learning management system using Lesson.ly so that employees can learn at their own pace, hit goals, and make progress rather than feel left behind. AC: What attracted you to Dorel Juvenile as the next step in your career? BL: I believe that Dorel is just one of many “sleeping giants,” or midsized manufacturers, that truly holds the keys to success in an Amazon-dominated online world. If you manufacture your own product instead of contract manufacturing with others, you have a distinct competitive advantage. You can pass to consumers the margins that you would have spent with outdated marketing spends such as retailer co-op dollars. When Amazon launches another private-label brand in your category, you can actually bid to be the manufacturer. That’s why investments in operational efficiencies are so critical. You have to be the best and continually reduce expenses and hone processes to compete in an e-commerce world where operating costs (e.g., driving traffic, buying ads, creating content, etc.) continually go up. AC: How does a company like Dorel Juvenile attain and maintain its status as the world’s leading company within a specific sector (e.g., juvenile products)? BL: It all starts with rock-solid products. Ours literally save lives (car seats), so everyone here is incredibly motivated to deliver the best. We think about it as if our own children were riding in those seats, and for many of us, they are.", "date": "2019-5-8"},
{"website": "Avenuecode", "title": "API Autodiscovery in Anypoint Platform 2", "author": ["Anupam Gogoi"], "link": "https://blog.avenuecode.com/api-lifecycle-in-anypoint-platform", "abstract": "With the release of Anypoint Platform 2, there are some exciting changes in the API Autodiscovery feature. Here in this short article, I am going to describe how to make use of it in your applications. This is the basic roadmap for API development in Anypoint Platform. For the purposes of this article, we will not go into all the steps, but will just focus on the most relevant - the API Manager. This is the most important tool for managing (governing) the APIs. An API, to be managed it must have an instance created, and this instance must be associated with an actual implementation of the API. Also, to apply necessary security policies the API instance is a must need. Note: Previously in Anypoint Platform 1, there was the concept of creating a Proxy for the API to manage it. But, now with the new version of Anypoint Platform, it can be done through the API instance. This means that you do not need to explicitly create a Proxy only to apply some policy. Now you can directly apply the policies on the API instance. So, here is a screenshot to create an API instance, The most important part is to mark the checkbox. Once you save it, the API instance is created as shown below: This is the most important part of API Autodiscovery. You can create more than one instance of the same API. Once the API instance is created you can have the instance ID as shown above. At this stage, your API instance is not managed. (Status is shown as Unregistered). It means that till now it has not yet been associated with an actual implementation. After API specification is ready you can import it directly from the Design Center in the Anypoint Studio. Now, configure an API Autodiscovery global element and provide it with the API Instance ID, Once the implementation is complete publish it to Runtime Manager. Before deploying just make sure that you configure the following properties. This is the most important part otherwise your API will not be managed. Note: The client_id & client_secret are the credentials of your master organization or Business Group for whom you are developing the APIs. Once the application is up and running you can see the status of the API as active in the API manager. At this stage, your API is completely managed by the platform. Any request that comes to the application endpoint is first intercepted by the API manager to collect statistics, etc. If you apply security policies (API Manager) the request will be intersected at the API manager to apply the security policies. In this tutorial, I have shown how to use the API Autodiscovery in Anypoint Platform 2. As you can see, the release of the new version of the platform has made things far easier to understand and use!", "date": "2018-6-6"},
{"website": "Avenuecode", "title": "Using Deep Convolutional Neural Networks (DCNNs) for Time Series Forecasting Using Tensorflow - Part 3", "author": ["Hossein Javedani Sadaei"], "link": "https://blog.avenuecode.com/using-deep-convolutional-neural-networks-dcnns-for-time-series-forecasting-using-tensorflow-part-3", "abstract": "In part 1 of this tutorial, we explained the advantages of and proposed a methodology for using DCNNs for time series analysis by converting time series into gray-scale images. In part 2 , we defined a Python class and various methods to perform data processing. In our third and final part, we will explain the topology of our model. Discuss how it is possible to establish its computation graph, and demonstrate how to run this graph in a Tensorflow session. Our Proposed DCNN Topology As explained in part 1 of this series, CNNs have only convoluted layers and pooling. DCNNs, on the other hand, include more diverse layers of the dense/fully connected layer. There are several combinations of layer connections that produce many different possible topologies. Additionally, selecting hyper-parameters can produce different models. So one challenge in solving our particular problem is determining how to establish an effective architecture since architectures have a great impact on model performance. In this tutorial, we will use a simple DCNN model with two convolute layers, two pooling layers, and three dense layers. To avoid overfitting, we’ll apply a dropout technique that refers to ignoring units (i.e. neurons) during the training phase of a certain set of neurons that is chosen at random. For developing this prediction model, we adopt Tensorflow official website guidelines, which can be accessed here and here : After running the model, the following results might be produced. (Your own results will be different than the following due to the random configuration we deployed while building the model.) Epoch 1 completed out of 15  with loss: 51.4090359211 Accuracy in epoch 1 is: 0.853059 Epoch 2 completed out of 15  with loss: 36.055752486 Accuracy in epoch 2 is: 0.862016 Epoch 3 completed out of 15  with loss: 30.448112756 Accuracy in epoch 3 is: 0.87955 Epoch 4 completed out of 15  with loss: 24.6789700091 Accuracy in epoch 4 is: 0.885649 Epoch 5 completed out of 15  with loss: 19.7951619923 Accuracy in epoch 5 is: 0.874404 Epoch 6 completed out of 15  with loss: 15.8194133416 Accuracy in epoch 6 is: 0.883362 Epoch 7 completed out of 15  with loss: 12.7372778915 Accuracy in epoch 7 is: 0.877644 Epoch 8 completed out of 15  with loss: 10.4595579039 Accuracy in epoch 8 is: 0.881456 Epoch 9 completed out of 15  with loss: 7.60100067966 Accuracy in epoch 9 is: 0.879169 Epoch 10 completed out of 15  with loss: 6.00782689825 Accuracy in epoch 10 is: 0.877835 Epoch 11 completed out of 15  with loss: 5.18157571554 Accuracy in epoch 11 is: 0.872689 Epoch 12 completed out of 15  with loss: 4.55696826754 Accuracy in epoch 12 is: 0.883362 Epoch 13 completed out of 15  with loss: 3.64296960807 Accuracy in epoch 13 is: 0.876882 Epoch 14 completed out of 15  with loss: 2.26440465293 Accuracy in epoch 14 is: 0.883743 Epoch 15 completed out of 15  with loss: 2.38258726092 Accuracy in epoch 15 is: 0.879931 In this three-part blog, we provided a tutorial on how to use DCNNs for time series analysis. We first provided information on our proposed methodology, explaining how it’s possible to convert a time series into gray-scale images. Next, we provided Python codes for data processing and preparation. Finally, we used Tensorflow to complete our predictions. As seen in our results, this method allows us to reach an accuracy of about 88% when predicting trends for the next state of electricity consumption. At Avenue Code, our data science teams enjoy providing solutions for real-world challenges by using DCNNs and many other technologies. We serve diverse industries, including retail, energy, telecommunications, etc., and we invite you to contact us to discover our solutions to your challenges.", "date": "2018-5-23"},
{"website": "Avenuecode", "title": "The Retail Ops Weekly #1", "author": ["Ivan Dwyer"], "link": "https://blog.avenuecode.com/the-retail-ops-weekly-1", "abstract": "Welcome to the inaugural Retail Ops Weekly newsletter, which will highlight a few relevant thought pieces, news stories, tips & tricks, and more from the week before. Each issue will be personally curated, with a focus on delivering useful and thought provoking content for anyone working in the retail industry. Enjoy! Experience", "date": "2016-8-22"},
{"website": "Avenuecode", "title": "Swift and the Hidden Power of Enums", "author": ["Eduardo Salinas"], "link": "https://blog.avenuecode.com/swift-and-the-hidden-power-of-enums", "abstract": "Enums--or enumerated types--in Swift have been around for a long time, but I never truly explored their potential until I stumbled across an article explaining that \"enums can define any hierarchically organized data.\" So I decided to try using enums, not as I had traditionally used them, but as the very foundation of my data. Today, I invite you to explore the potential of these powerful types with me. We usually use enums as a way to define a finite set of options in a given context, but in the following scenarios, I'll use them as linked lists (which is easy to do, thanks to indirect cases), and as a simplified sort of function builder (similar to how the new SwiftUI framework functions). Let's show the power of enums by providing two examples: in the first example, we'll use enums instead of structs and classes to build a fun role-playing game, and in the second example, we'll use enums to create an app for model train enthusiasts. I love games, especially role-playing games. Nothing is better than entering a world of fantasy and taking your character along a perilous journey, watching them grow, and hopefully, not getting them killed along the way. And if enums can define any hierarchical data, then, we should be able to build game components with them. For example, in our game, we can have an enum define the possible status conditions that our units can have: enum Effect { case poison case blind case burn case bleed case freeze case damage(Int) case heal(Int) } Our units can have an array of effects to indicate what happens to them. This enum can also be used for applying effects when an attack happens. Some attacks, such as swords or axes, would only damage units, while other attacks could poison, bleed, or burn our foes. We can construct enums in such a way that they behave like semantic expressions. By chaining enums together, we can construct easy-to-understand definitions for the components in our game. Let’s say we’d like to create the logic for defining different attacks in our game, such as melee attacks or magic attacks. We can divide the attack into three parts: - The target - The effect - The attack type So our structure would be something similar to: attack with “type” to “target” for ”effect.“ TARGET We can start by defining an enum for possible targets: enum TargetType { case ally case enemy case unit } An attack can be against an enemy, an ally (in the case of a heal spell, for example) or any unit. We can also define a given range constraint: enum Range { case range(Int) } That would limit the range of our attack. We can also define the location of our targets: enum Direction { case front case behind case around } And we can even define a constraint to the number or proximity of targets. For example: enum Target { case target(TargetType, Range) case all(TargetType, Direction, Range) } Now, we can construct a very detailed description of the target, say for a melee weapon: let meleeTarget: Target = .target(.enemy, .inRange(1)) We know that we want to target any enemy that’s within our range. Or if we wanted to perform a massive area-of-effect spell: let spellTarget: Target = .all(.enemy, .around, .inRange(6)) EFFECT But what will our attacks do? Well, we can define a trigger, maybe something that happens only if the hit is successful, or if the target dies. Again we can do all of this with enums: enum TriggerType { case damage case hit case kill } The trigger type will tell us when we should trigger the effect of the attack. enum Trigger { case end indirect case after(TriggerType, [Status], Trigger) } We can use nested enums to define multiple triggers and construct even more complex attacks.  For our melee attack, let’s say that it will deal one point of damage. let meleeTrigger: Trigger = .after(.hit, [.damage(1)], .end) Or, we can have it be a poison-coated dagger that can also poison our foes, but only if we dealt damage with it. let meleeTrigger: Trigger = .after(.hit, [.damage(1)], .after(.damage, [.poison], .end)) With this logic, we can construct any type of effect, and chaining enums lets us mix and match whatever we need. ATTACKS Ok, so now that we have our effects and targets, we can define a simple list of attacks: enum Attack { case melee(Target, Trigger) case spell(String, Target, Trigger) } Maybe our heal spell is: let healSpell: Attack = .spell(\"Heal\", .target(.ally, .inRange(4)), .after(.hit, [.heal(1)], .end)) Our fire blast attack is: let infernoSpell: Attack = .spell(\"Inferno\", .all(.enemy, .around, .inRange(6)), .after(.hit, [.damage(3)], .after(.damage, [.burn], .end))) We can now build on top of this and define enums for our weapons. Adding associated values can make it so that different weapons of the same type have different damage values (such as a wooden spear versus an iron spear): enum Weapon { case spear(dmg: Int) case poisonDagger var attack: Attack { switch self { case let .spear(dmg): let meleeTarget: Target = .target(.enemy, .inRange(2)) return .melee(meleeTarget, .after(.hit, [.damage(dmg)], .end)) case .poisonDagger: let meleeTarget: Target = .target(.enemy, .inRange(1)) return .melee(meleeTarget, .after(.hit, [.damage(1)], .after(.damage, [.poison], .end))) } } } We can do the same for spells. A magical tome can have a group of spells associated with it. Or we can define a new enum for different enemies in our game. Each enemy can perform different attack actions: enum Enemy { case goblin case dragon var actions: [Attack] { switch self { case .goblin: return [Weapon.poisonDagger.attack] case .dragon: let bite: Attack = .melee(.target(.enemy, .inRange(1)), .after(.hit, [.damage(3)], .after(.kill, [.heal(1)], .end))) let dracarys: Attack = .spell(\"Inferno\", .all(.enemy, .around, .inRange(6)), .after(.hit, [.damage(3)], .after(.damage, [.burn], .end))) return [bite, dracarys] } } } Let’s now look into a different, more practical approach for using enums in apps. Say you’re a train enthusiast, and as such, you're creating an app to manage your train emporium. Again, we can use our trusty enums to create our models. Let’s start by creating an enum for the cargo of our trains. They can either hold some sort of material or be empty: enum Cargo { case chemical(String, Int) case coal(Int) case ore(Int) case lumber(Int) case none } Then, to create our train model, we can do something like this: enum Train { case tail(id: String, cargo: Cargo) indirect case car(id: String, cargo: Cargo, next: Train) } In this case, we can have an ID for each car, the cargo it hauls, and possibly the next car attached. This is simple and elegant. This also means that we can attach one train to another. So we can start creating our trains easily: let train: Train = .car(id: \"WX900\", cargo: .lumber(20), next: .car(id: \"WX120\", cargo: .ore(40), next: .car(id: \"TA763\", cargo: .chemical(\"Radioactive Waste\", 20), next: .tail(id: \"T800\", cargo: .coal(100))))) Now, if we want to know the total weight of the cargo, we can add a computed property to our enums: enum Cargo { case chemical(String, Int) case coal(Int) case ore(Int) case lumber(Int) case none var tons: Int { switch self { case .chemical( _ , let tons): return tons case .coal( let tons), .ore( let tons), .lumber( let tons): return tons case .none: return 0 } } } And a couple more for our train model: enum Train { case tail(id: String, cargo: Cargo) indirect case car(id: String, cargo: Cargo, next: Train) var numberOfCars: Int { switch self { case .car(id: _ , cargo: _ , next: let next): return 1 + next.numberOfCars default : return 1 } } var totalTons: Int { switch self { case .car(id: _ , cargo: let cargo, next: let next): return cargo.tons + next.totalTons case .tail(id: _ , cargo: let cargo): return cargo.tons } } } So getting those properties becomes something as simple as: train.numberOfCars // 4 train.totalTons // 180 As you can see, enums are a very powerful type in Swift. They are flexible enough to be used as building blocks for data and simple enough to understand and use. I had never thought of using enums instead of structs and classes, but now I imagine that we might (one day) end up doing enum-oriented programming.", "date": "2019-10-30"},
{"website": "Avenuecode", "title": "AC Spotlight - Rob French", "author": ["Alfredo Moro"], "link": "https://blog.avenuecode.com/ac-spotlight-rob-french", "abstract": "(The following article is property of Avenue Code, LLC, and was originally published with permission at Total Retail on January 28t, 2021 .) Rob French , Chief Digital Commerce Officer at Décathlon Canada, discusses the next big digitization initiatives in retail and shares his method for identifying innovation opportunities. Avenue Code: Tell us about your personal career path. How did you get to where you are today? Rob French: My career began in England, where I was born and raised. I wasn’t naturally inclined toward academics. Instead, I wanted to work in the market as early as possible and learn to be a leader. I cultivated management skills in the hotel industry for three years before moving to Canada, completing my university certification at McGill, and gaining experience in using online marketing to boost business performance. The key to my career success is the network I created. I have made many connections that have led to great opportunities in e-commerce at agencies and major retailers. Now at Décathlon, my niche is e-commerce, sales, and marketing. AC: How have your digital initiatives related to commerce and marketing pivoted since COVID-19? RF: We shifted all of our focus to e-commerce, where our two biggest concerns were stock availability and transport speed. We set up backup transportation options and monitored our customer behaviors very carefully. We also kept ourselves current on the evolving realities of the lockdown in various locations so that we could time our offerings accordingly. For example, we offered curbside pick-up as soon as possible, and when our stores opened, we had to quickly adjust to a huge demand for bicycles. We have learned from all of these challenges and adapted to sustain our supply through the ongoing realities of the pandemic. AC: What are the most important factors in sustaining and growing retail and consumer brands during and after the pandemic? RF: Having a healthy P&L is important, but at the end of the day, money is less important than people. Every business should make sure its employees are happy. This is especially key during the pandemic because sustainability hinges on people. Leadership needs to focus on finding ways to motivate and retain employees and hire when possible. Among other things, this means being transparent and putting the keys into the hands of the team so they can help you lead. AC: What are the biggest trends we’re seeing in retail globally? RF: For Canada in particular, enabling speedy delivery is a priority since it’s the second largest country in the world. At Décathlon, we’re working on testing same-day delivery in areas of Montréal and second-day delivery in Toronto. We’re also adding fulfillment centers and implementing in the future a ship-from-store model so consumers can get products faster. Mobile first remains a priority when it comes to user experience, creating a new channel and  improved conversion rate. We’re focusing on the best in class technologies and design, but there’s still a lot of room to master mobile for e-commerce. Finally, it’s key to stay abreast of emerging e-marketing channels. As an example, Gary Vaynerchuck recently announced that voice advertising is becoming increasingly important because customers want to get products fast through voice search rather than text search. We’ll see this and other innovations continuing to gain importance in retail. AC: In what ways are you digitizing brick and mortar experiences at Décathlon? RF: We’re bringing in an application that gives autonomy to customers to scan items, pay, and leave; we’re also putting apps in our employees’ hands so that they can help customers find inventory at neighboring stores and have it shipped to their location. Our applications are integral to supporting customer satisfaction as well as to upselling cross-sellers. We’re also seeing in Décathlon retail stores in other countries  robots to help us improve inventory management. In our Singapore R&D center, we’re using our RFID technology that helps customers scan and pay for items; it’s also capable of performing inventory checks for the entire store, which means we can make items available to customers within minutes instead of hours. Dartmouth Décathlon storefront. Image courtesy of Décathlon. We’re also using AI to improve in-store and online forecasting, using the data to support future in-store digital and our e-commerce initiatives. AC: From your work history, it’s evident that you’re able to identify new business opportunities others overlook - what is the key to your success? RF: The key is to recognize what to exploit versus what to explore. It’s important to explore opportunities carefully before investing in them, and it’s equally important to keep this process fast so that you can exploit something as soon as you’ve vetted it. For us, B2C e-commerce is an area we’re exploiting, but we’re simultaneously exploring other avenues like B2B and marketplaces. AC: What are your recommendations for companies building an innovation strategy? RF: When it comes to digital innovation, my advice is to always build a strong business case. Technology is never cheap, so it’s important to do competitive analysis, complete a financial analysis, evaluate market and consumer needs,  etc. It’s also important to seek advice from fellow entrepreneurs, businesses that aren’t competitors, and executives and employees within your own company. AC: What is the key to successful strategic partnerships? RF: I always look at where partners fit in terms of culture and brand, especially if the collaboration is consumer-facing. For me, partnerships should be based on DNA alignment, customer satisfaction, and quality over price. I also seek alignment in process. I believe we shouldn’t be afraid to try a soft launch of an MVP and optimize from there rather than waiting for perfection. AC: What are you personally most passionate about in your career? RF: I’m passionate about challenges that I can transform into successes. When I first joined Décathlon, it was like buying a car with chipped paint, an old engine, and missing keys. At first, it was hard to know what to prioritize, but I had the opportunity to create a vision that my team can support, and now we’re transforming the car, one piece at a time. AC: What is your vision for the future of Décathlon? RF: This is a very exciting time for us because everyone in the company, from retail to logistics to supply, has a clear vision for our future. We’ve significantly improved our technology, and now we’re introducing customer applications and even strategizing our own marketplace. I don’t think any retailer today offers everything in one place, from new products and second-life equipment to ski passes, virtual and physical fitness activities, etc. We couldn’t do any of this without a strong tech foundation, but now that it’s in place, we’re ready to innovate. AC: What advice would you give to emerging entrepreneurs? RF: Always listen. To be an entrepreneur is to learn from others and carefully understand situations before making decisions. It’s also important to remember that money is not always the bottom line - you need to consider the people you work with and the people you want to hire so that you’re learning and making the right decisions. Everyone will make mistakes, so don’t be afraid to try new solutions. Just be attentive to learning. AC: Thanks for your insights today, Rob! It’s fascinating to hear about Décathlon’s innovation initiatives and about retail transformation as a whole.", "date": "2021-3-23"},
{"website": "Avenuecode", "title": "The Three Automated Tests Every React.JS Developer Should Know", "author": ["Ricardo Horiguchi"], "link": "https://blog.avenuecode.com/the-three-automated-tests-every-react.js-developer-should-know", "abstract": "Did you know that there's an easy way to add quality, speed, reliability, documentation, and long-term value to your software? These three automated tests give you the feedback you need. \"If a software doesn’t work as expected, it doesn’t matter how beautiful or fast it is.” That's a good rule of thumb to live by, but how can developers ensure they follow it? The answer is simple: automated tests. Automated tests are programs that automate the task of testing your software, and they matter because they give developers quick feedback on errors. They interface with applications to perform actions and compare the actual result with the expected output you've previously defined. Thus, automated tests enable fast error feedback on the code developed, m aking the development process more reliable. In other words, developers can see errors quickly and fix them before software goes to production. Automated testing is also useful when integrating new features into your software. When the integration of features is being done, the written tests can ascertain which parts of the software will break after the integration. Otherwise, if the tests are done manually, the team would need to test the integration at least three times: in a new feature created, in an old feature integrated, and in between both parts. Testing manually is not only time consuming, but it's also error-prone. After all, the tester would have to follow the same user path manually without making any mistakes to ensure that the integration is a success. So, developing an application driven by automated tests can provide many benefits, including: There are many ways to classify tests, but the following three suggestions provide a solid framework for dividing your tests into separate categories: Unit tests are the smallest portion of a test that can be written. The main goal of these tests is to create a formal and official registry of the code as a live document. Instead of having to register the code elsewhere, you can register the code documentation as tests. Thus, you don't need to explain how the code works in another document other than the test suites themselves. Unit tests can only test part of the application and should not have broad coverage. Thus, they are recurrent and frequent. Unit tests can also test individual functionalities in the software. They are very quick to run and can run many times during the development of a function. They are easy to understand, cheap to develop, and quick to update and run. Integration tests verify the observable behavior and the integration between multiple functions. Thus, they offer broader coverage and are run less frequently. They guarantee adequate usage of third-party libraries and check whether the unit being tested performs the expected functionality. Integration tests take more time to run (a few seconds). They are \"medium-sized\" tests and cost a little bit more to develop. E2E tests verify the whole feature in the same way that a user would experience the application. They should be run when a feature is completed, as they check whether or not the feature is correct. They are more expensive to develop, take a long time to run, and are run fewer times during the project lifecycle. So, let’s take a look at how to write tests on the frontend using a popular framework like react.js. In this example, we are going to use Jest and Enzyme together with React.js : In the terminal or command prompt, install the following dependencies to your existing create-react-app project. npm install enzyme enzyme-adapter-react -16 enzyme-to-json --save-dev or using yarn yarn add enzyme enzyme-adapter-react -16 react-test-renderer enzyme-to-json --dev Please note: If you created your app using create-react-app, you don’t need to install Jest. It already comes installed by default. Next, let’s configure Jest and Enzyme to work together with React. In the src/ directory, create a new file called setupTests.js import { configure } from 'enzyme' ; import Adapter from 'enzyme-adapter-react-16' ; configure({ adapter: new Adapter() }); In the package.json file, add the following Jest configuration: \"jest\" : { \"snapshotSerializers\" : [ \"enzyme-to-json/serializer\" ], \"collectCoverageFrom\" : [ \"src/**/*.js\" , \"!src/index.js\" ], \"coverageReporters\" : [ \"text\" ] } enzyme-to-json helps make Snapshot tests more readable; by adding it to the snapshotSerializers , it is automatically applied to each Snapshot test via the configuration above. collectCoverageFrom indicates which set of files coverage information should be collected for. In our case, we have restricted Jest to run on all files with the .js extension except src/index.js, as it is usually just boilerplate code, which doesn’t need to be tested. coverageReporters tells Jest to output the coverage report via terminal instead of creating an HTML page to display these results. As a developer, it is much easier to check the terminal to understand where you stand in terms of test coverage. Let’s say we have a component called DefaultText.js import React from 'react' ; const DefaultText = ({ children }) => ( <h1> {children} </h1> ); export default DefaultText; To create our first test, we will create a new file called DefaultText.test.js ; this will be collocated next to the DefaultText.js component file. import React from 'react' ; import { shallow } from 'enzyme' ; import DefaultText from './DefaultText' ; const title = 'Test DefaultText' ; let wrapped = shallow( <DefaultText> {title} </DefaultText> ); describe( 'DefaultText' , () => { it( 'should render the DefaultText Component accordingly , () => { expect(wrapped).toMatchSnapshot(); });  it( 'renders the DefaultTexts children component' , () => { expect(wrapped.find( 'h1' ).text()).toEqual(title); }); }); The first block will create a new snapshot of the DefaultText component. The second block will do an assertion to check if the value in h1 is equal to the variable named title , which stores ‘Test DefaultText’ . If true, the test will pass. Type npm test in the root directory of your project via cmd/terminal. This will run all the test files in the CRA project and output the results to the terminal/cmd. Run npm test --watchAll --coverageReport to generate a code coverage report. This report will show in-depth information about each file, including statement, branch, functions, and line coverage percentage. I hope you enjoyed this practical tour of the three automated tests every developer should know and use. If you use these tests, you can feel confident that your applications are less error prone and that your developing process is stronger, faster, and smarter.", "date": "2021-4-28"},
{"website": "Avenuecode", "title": "How Well Do You Know SwiftUI?", "author": ["Marcos Rebouças"], "link": "https://blog.avenuecode.com/how-well-do-you-know-swiftui", "abstract": "The first thing you should know about SwiftUI is that it is not a new programming language, and neither is it an updated version of the Swift language that has a declarative syntax feature. This has caused some confusion, especially among newer Swift developers. So what is SwiftUI? SwiftUI is Apple’s new UI Framework that was created to replace the well known UIKit Framework with its Storyboards, XIBs, Segues, and Autolayout, enabling developers to work in the UI layer in an easier and faster manner. What makes SwiftUI’s declarative syntax possible is a combination of new features implemented in Swift 5.1 and older Swift features. Let's take a look at each of these: As of Swift 5.1, you can omit the return statement keyword in any function or computed property that has only one expression : This is what makes it possible, for example, to declare a SwiftUI Text view inside the view body: Instead of explicitly writing a return keyword: It’s worth pointing out that, even though you don’t need to write the \"return\" keyword anymore, you still have the option of writing it, and your code will work the same way. So even though it can be omitted, the return statement is still available. (This new feature used in regular functions and computed properties already existed inside closures in previous versions of Swift.) Wondering what that strange some keyword is for in SwiftUI’s body declaration? That means the body computed property returns an Opaque Type , a new Swift 5.1 feature that allows functions and computed properties to return any value that conforms to a specific protocol . The concrete returned type is hidden, and it doesn’t matter which one will be used, as long as it conforms to the protocol. In the example below, the first function returns an Int concrete type, while the second returns an Opaque Type (any type that conforms to the Equatable Protocol would fit): In a SwiftUI view body, we can return any type that conforms to the View Protocol, which includes many different views from SwiftUI Framework, like Circle( ) , for example, as well as your own custom views: In SwiftUI, you use View Modifiers to change the properties of a view. View Modifiers are simply methods you append to a view one after another (e.g. .font(.title) , .foregroundColor(.yellow) , .frame() , and so on). Although this is a new SwiftUI feature, View Modifier uses a technique called Method Chaining that isn't new in the Swift language. Many View Modifiers can be applied to all views, like .foregroundColor(.yellow) , while others are specific to particular views, like .bold() , which belongs to Text views. One important thing to notice is that the modifier’s order matters , so the same modifier in a different order can give you a completely different result. That’s because a View Modifier returns another view with the transformations applied, so the next modifier is always applied to the view returned by the previous one. If you try to put more than one view inside the body of a SwiftUI view, the compiler will complain. Remember that implicit return only works in functions or computed properties that have a single expression ? But no screen is made of just one view. To solve this, SwiftUI has container views . The most common are the 3 types of stacks, HStack , VStack and ZStack, which manage the layout of other views horizontally, vertically, and layered on top of each other, respectively. These views use a different SwiftUI syntax where the developer can declare views inside braces, one after another, in the body of the stack. This is possible due to the combination of Trailing Closures and a new Swift 5.1 feature called Function Builders . If you take a look at the HStack init method in Apple’s documentation, you can see the last parameter is a function of type ( ) -> Content : Since we can use a Trailing Closure when it’s the last parameter in a function (which means declaring the content in braces outside the parentheses and omitting the parameter name), it’s possible to declare our views directly inside the braces of the HStack init method: But inside a Trailing Closure , it still wouldn’t be possible to put views one after another in a declarative way. If you notice in the HStack init method above, the last parameter that is a function of type ( ) -> Content has a @ViewBuilder annotation. This is a new Swift 5.1 feature called Function Builders , which is what makes this syntax possible. Function Builders are an advanced Swift topic, but in simple terms, they are specially annotated functions, used to implicitly build a value from a sequence of statements or expressions, producing a single value piece. In SwiftUI, you will find some stored properties with annotations like @State , @Binding , etc. These use a new Swift 5.1 feature called Property Wrappers . A simple definition of a Property Wrapper is a generic Swift struct that encapsulates read and write access to a property while adding some extra behavior to augment its semantics. Using a Property Wrapper, you can attach custom code to any stored property. So every time the value of the property changes, the connected code is executed. What a Property Wrapper does depends completely on the code you write for it. According to Apple’s documentation on SwiftUI’s @State Property Wrapper : “SwiftUI manages the storage of any property you declare as a state. When the state value changes, the view invalidates its appearance and recomputes the body. Use the state as the single source of truth for a given view. A State instance isn’t the value itself; it’s a means of reading and writing the value. To access a state’s underlying value, use its variable name, which returns the wrappedValue property value.” As a practical example, let’s write a Property Wrapper that will assure a String is Lowercased any time a value is assigned to the property: This way, no matter the String that’s passed into the init method of ProfileView, the email will always be displayed lowercased: That covers the most important new Swift 5.1 features that make SwiftUI syntax possible. As a brand new and innovative way to build UIs across Apple platforms, SwiftUI is continually under development, and we can expect more exciting new features on this fantastic UI Framework later this year on Apple’s WWDC . The source code examples in this article can be found here. Enjoy!", "date": "2020-6-3"},
{"website": "Avenuecode", "title": "The Retail Ops Weekly #7", "author": ["Ivan Dwyer"], "link": "https://blog.avenuecode.com/retail-ops-weekly-7", "abstract": "I hope everyone enjoyed their Labor Day weekend as much as I did. We’re back in action with another fresh set of news, articles, and tips. Enjoy! With 1000+ people working on Alexa, Amazon is taking a lead in consumer AI. Artemis Berry shares 5 things to watch out for as they continue to innovate. Alexa, what should retailers know about you? [Chatbots Magazine] A key shopping season before the season is back-to-school as retailers can analyze the data for new features and programs to optimize in advance of the holidays. Dan O’Shea looks at mobile usage patterns as a key data point. How back-to-school shopping season puts omnichannel strategies to the test [RetailDive] Individuals don’t deliver software, teams do. Today’s environment warrants a fresh look at how we organize our teams, and what we can do to ensure everyone is collaborative, performing, and represented. David J Bland provides 5 characteristics for Designing Modern Teams [Medium] When we hear that ‘software is eating the world’, we often think of the new generation of startups that are disrupting the incumbents. This isn’t always the case, and a recent study shows that big companies can be more successful with new technology than smaller ones. Are Big Companies Really Innovative Dinosaurs? [DZone] The promise of PaaS has always been about developer empowerment, but early incarnations were either too complex to operate or didn’t meet Enterprise requirements. Is the next wave of application platforms up to the task? Ben Rossi shares Why businesses have reignited the search for a PaaS field of dreams [Information Age] PaaS or no PaaS, modern applications are being built with the cloud in mind. Brian Gracely walks through a Red Hat Summit presentation to break down the terms we hear when talking about cloud native applications. From Silos to Services: Cloud Computing for the Enterprise [TechTarget] Perfecting the search & browse experience is an ongoing effort for any e-commerce site, with a fair amount of trial & error. Greg Randall walks through a few important considerations when designing the browse experience. Ecommerce product filters: Best practice tips for a great UX [Econsultancy] It can be easy to push design aside when working through Agile delivery cycles, as the primary goal is to deliver working software. This doesn’t always have to be the case as Steven Lowe explains. Moving beyond MVP: 5 agile design practices you can't afford to ignore [TechBeacon]", "date": "2016-9-12"},
{"website": "Avenuecode", "title": "Vert.x is a Great Framework, and I Can Prove It", "author": ["João Moráveis"], "link": "https://blog.avenuecode.com/vert.x-is-a-great-framework-and-i-can-prove-it", "abstract": "There are many design patterns and development strategies available nowadays, and they range from traditional to modern and flexible. These allow architects and developers to analyze and test the best approach for each project. Like design patterns and development strategies, frameworks are also being created to improve quality and productivity. Recently, one framework that caught my attention was Vert.x. Vert.x was an official Eclipse Foundation project started by Tim Fox in 2011/2012 and empowered by Red Hat. It contains many of the benefits presented by Node.js, plus the benefits of running in a JVM. Vert.x is a Java framework that allows you to write event-driven applications by developing components in the language that you think most appropriate for the task, such as Java itself, Kotlin, Scala, Ruby, and JavaScript. Like Node.js, Vert.x operates a single event loop, but it also has the ability to keep a thread pool to match the number of available cores. With greater concurrency support, Vert.x is suitable for not only IO but also CPU-heavy processes that require parallel computing. The concept of loosely coupled components in Vert.x is called Verticle . It has some of the same concepts as actor model, but not all. As mentioned before, Verticles can be implemented in different supported languages by using two methods: start() and stop(). Another important concept in Vert.x is the event bus . The communication between the Verticles occurs through messages that are managed and delivered by the event bus, which accepts two types of implementation: point-to-point and publish/subscribe, though Vert.x doesn't provide a guaranteed delivery of the messages. Image courtesy of javaworld.com One interesting thing about this architecture is that Verticles stay idle until receiving a message to execute an action. The concept seems good, so why don't we try it out with some code? To try out Vert.x, let's create a Java project. You can choose to work with Maven or Gradle. After creating the project, import the following dependencies: Since the building blocks of Vert.x are Verticles, we'll create our first Verticle: So far so good. Now we just have to deploy the Verticle and start the application: To sum it up, Vert.x is a great framework when you want a non-blocking strategy for an event-driven application. You may think that you can develop the application, but you will have problems with the concurrency when querying the database. But don't worry, because there are some non-blocking connectors. On the order hand, working with this framework will be a bit of a challenge due to lack of documentation and examples. All in all, however, f or an application that has to keep its state (context, memory content, and so on) consistent and preserved, Vert.x is a good option due to the fact that each Verticle provides a thread-safe environment by isolating its state.", "date": "2020-1-29"},
{"website": "Avenuecode", "title": "Core Concepts Behind Machine Learning", "author": ["João Moráveis"], "link": "https://blog.avenuecode.com/core-concepts-behind-machine-learning", "abstract": "Over the last few years, machine learning has garnered a great deal of attention both in academic circles and in the marketplace. We now have a huge number of applicable use cases, thanks to the fact that we're living in an era where data is the new coin. Companies and universities now have access to petabytes of data, allowing them to apply machine learning to identify information that can, for example, help increase revenue. The potential for this technology is amazing, but it's essential to remember that before we approach any project, we must master the core concepts so we don't head in the wrong direction or get misinformation from a set of data. If you're wondering about the benefits of understanding machine learning, please take a look at this article, in which I highlight the rewards of learning ML and propose a course of study. Today, I'll introduce some basic terms and concepts. Machine learning is a branch within artificial intelligence that's focused on the study of learning algorithms. Although it was originally an academic field, it is increasingly used to benefit companies and assist multiple departments, such as marketing.  Alongside the growth of ML in the marketplace, we can see the increase of the framework's abstraction, which has contributed to its further adoption. Every time we start learning a new language or concept, the first learning curve we face is terminology. The same goes for machine learning. You'll encounter new words in books, videos, and blog posts (including this one!). So let's go over a few key terms. When it comes to data, a set or collection of data will be called a dataset , which can exist in many forms of media, such as csv or json files, images (.jpg,, .png), database tables schema and text, and so on. Each attribute or characteristic represented as a column in a csv file or database table is called a feature. Examples of features are height, color, and number of sides (for a geometric form, for example). A target is what we want to learn or infer from the collection of data, which can be a label/class (for example, the name of a geometric form or a numeric value when calculating the cost of gas for a trip based on previous trip data). Finally, a model is a trained function that defines the relationships within a determined set of data. A model is capable of predicting the class or value of an input , which in this case is new information that isn't present in the data used to train the model. Now that we've defined our basic terms, we can move ahead and specify the types of learning that can be used to solve these problems. There are three learning types: The learning types can be grouped into regression and classification problems. Regression models work toward trying to define the cause and effect between the data provided, which consists of numerical values. For example, you might take notes on all of your travel expenses by car over a three month period, considering both the price of gas and the distance traveled. With this data, you can train a model to estimate the cost to travel a certain distance. Another example would be inferring stock market value based on daily inputs. In classification, as its name suggests, we can use a collection of data (a group of features/characteristics) to classify or infer a label or group. For example, we can train a model to predict dog breeds. At a glance, all these concepts may seem overwhelming, but it's important to understand the basics and learn incrementally. Machine Learning isn't just a technical subject - it also requires that its practitioners build a mindset that will enable them to apply these concepts to identify and extract the expected value or information.", "date": "2020-6-17"},
{"website": "Avenuecode", "title": "Visual Regression Testing With Applitools", "author": ["Renata Andrade"], "link": "https://blog.avenuecode.com/visual-regression-testing-with-applitools-1", "abstract": "Essentially, Visual Regression is a tool for image comparison. It compares the actual page image by loading the page in real time, with a baseline saved image. If there are any pixel differences, the tool shows the result as a failure. These tools allow you to set ignored areas to work with dynamic components, thus, avoiding false negatives. This is useful when you need to check layout changes that should only impact one spot, such as when you need to do cross browser testing and validate the application across many devices/screen resolutions. Since it's an image comparison, one important thing to keep in mind is to clearly define your screen resolutions on each test. Why? Some responsive sites will have different element arrangements and selectors. Applitools is one of the tools for Visual Regression Testing. It's a paid app, but if you have less than 25 runs a week, it’s free. Here’s how it works: You code the steps to get to the page you need, using Selenium for example, and do a quick “checkWindow” command to compare the baseline image with the new application version. The first time you run the test, it takes the screenshots and saves them as baselines. From the second time on, it loads the site, takes a new screenshot of it, and compares it to the saved baselines. Now, let’s take a look at an example: Your team makes a CSS change that should impact only the font-size of the Home Page's title. However, the font-size was accidentally changed on another page as well. You run the visual regression tests to compare the version before the change to the version after the change. Expected change (\"9 Years, 4 Offices, 1 Mission\" font-size increased on Home Page): Unexpected change (\"Wish You Were Here\" font-size increased on Careers Page): The test will fail for the Home Page, which is expected since you intentionally changed it; however, you can easily update the baseline image to fix that for future runs. The test will also fail for other pages, which is unexpected, and means you caught a bug! The next time you run the test, the Home Page will be fine since you updated the baseline, and the other pages will fail until the team fixes the issue. We can see that you don't need to test every single page after the change. You also have a one-click action that helps get your tests up to date. Let's see how that works with this tool. We are going to use Node.js + Selenium Javascript for this example. You might want to check here to make sure you have Node.js set. 1. Create a brand new project, or go to an existing one. To create a new one: Create a folder: npm init (answer the questions and hit enter) 2. Create an account on https://applitools.com/users/register 3. After logging in, you need to “Run your first test” by choosing the environment of your tests and run the given command (We are using “Selenium Javascript”). Alternatively, you can go here: Applitools Tutorial Pag e 4. Install Applitools: npm install eyes.selenium --save 5. You'll also want to install selenium webdriver: npm install selenium-webdriver --save 6. Double check your package.json. Here is an example: 7. Grab your Applitools key by going to My account - My API Key menu on Applitools Test Manager : 8. Create the file first-test.js (You can take a look on Step 3 from the Applitools Tutorial Page for a better understanding): 9. Download the latest Chromedriver from http://chromedriver.storage.googleapis.com/index.html and include the ChromeDriver location in your PATH environment variable or your project folder. You’re all set! Now, let's run the first test: You should have seen the browser loading, the page opening, and all the steps we developed being executed. Go to your Applitools Test Manager to see the results. This first time didn’t really yield any comparisons because there were no baseline images saved, therefore the tool didn't have anything to compare to. Let's run it one more time: npm run test. This time, you got passes. That means the baseline images from the first test you ran were successfully saved, and by the time we load the page during the second test, there will be no differences, resulting in the test passing. I made a change to my local environment, increasing the Home Page title font-size. I ran it again, and here’s what I got as a result: You can go back to the Applitools Tutorial Page and take a look at step 4 for a better understanding. Great! Now that you have your first test running on Chrome, play with other browsers/devices and see the results. Visual Regression is a great alternative for keeping your application stable from a UI perspective, if you often deal with non-expected layout issues, or, when you have high demand for cross browser testing. Applitools has a great image manager that allows you to quickly and easily maintain the images and see the results. Just keep in mind that it's a paid tool if you need to run it more than 25 times a week. In my next post, we'll go deeper in Applitools and compare results across different browsers. Now, I would love to hear from you! What are your results? What other tools do you guys use for Visual Testing?", "date": "2017-9-26"},
{"website": "Avenuecode", "title": "An Introduction to Profiling", "author": ["Matt Riley"], "link": "https://blog.avenuecode.com/an-introduction-to-profiling", "abstract": "Let's say you've been working on your project for awhile, but can't shake the feeling that it is performing sluggishly. More importantly, your client is also unhappy with the code's responsiveness. Remember, your code doesn't need to be fully optimized. 80% of the gains generally come from focusing on 20% of the code . Make sure you have a good reason to die into performance optimization before you do so. Per Knuth, \"We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil.\" There are two main categories for code being slow: While technically classified as IO-bound, I would argue that thread contention , where multiple threads continually fight over a shared resource, is a third category. The basic idea is that, because multiple threads are trying to access the same resource, at least one of the threads will execute more slowly than it would if otherwise run in isolation. I will not attempt to cover thread contention here, but if interested, you may want to look into Visual Studio's Resource contention data (concurrency) profiling method. Perhaps you have no idea why your code is slow. Where do you begin? This is where profiling comes in. Visual Studio supports 2 modes of profiling: Sampling and Instrumentation . Sampling is a light-weight solution; it periodically polls the running binary, checking metrics such as what function is currently executing, how much memory is consumed, etc. However, because it needs to rely on polling to gather information, it can miss short changes in program state (the default polling interval is every 10 million clock cycles, or 10 milliseconds on a 1 GHz machine). The only pieces of code that will show up in a sampling report are those that happen to be executing when the polling interval triggers. This type of profiling might be a poor choice if your program primarily consists of quick, short functions, as the results will likely be distorted by whenever the the polling interval fires. It is a terrible choice if your program doesn't even last a full polling interval. However, if you are just starting trying to identify bottlenecks in your code, sampling works well, because these bottlenecks are likely to be long-running functions that will span multiple polling intervals. NOTE : Sampling is restricted to user-mode code. Because of this, according to Microsoft, \"some functionality, such as input and output operations, is not captured by sampling\". Also worth noting, is that as of this writing, sampling is not configurable on Windows 8+ (Windows 7 still allows setting the interval rate and sampling type). Instrumentation is the more heavy-weight approach. It will effectively trace through the entire path of the binary as it executes. Because of how heavy-handed this approach is, it can visibly affect performance. However, once you've identified a time-consuming part of your program (through sampling), instrumentation allows you to drill down into the finer details. With instrumentation, you can closely examine all memory allocations/deallocations, all function calls, and how much time each portion of the code takes. So, back to your code. You have a giant blob of slow code, and you don't know where to start. Provided your code runs for a while, I'd lean towards sampling for that initial pass -- you'll quickly get a rough sense of what code is responsible for the bulk of the samples. At that point, you can fire up a more targeted instrumentation session to get more specific details. How do we do that? Visual Studio exposes its profiling via its Performance Explorer window. You can access this window directly via Debug->Profiler->Performance Explorer , however this window won't be of much use without a configured profiling session. A better way to start out is to use one of the following, equivalent commands: Any of the above will open a wizard to start the profiling session. In most cases, you'll want to set: and then click \"Start\" On the next page, select either CPU Sampling or Instrumentation . In most cases, you'll be fine leaving this page as-is, with the desired project selected for profiling. Click \"Next\". On the final page, you can decide to launch the profiling immediately or wait. When the report comes back, you'll see something like the following: I'm using the PeopleTrax sample application , which can be found under the SourceCode/Chapter11/4_Performance directory. Microsoft tutorials refer to the sample application, but their links appear to have expired. Here, I'm looking at the Call Tree view, which is a good place to start to get a general idea of what is consuming the bulk of the time in your application. If you click on the fire icon in the toolbar, called Expand Hot Path , it will drill down into the actual function which is responsible for the bulk of the work. In this case, it looks like we're spending the bulk of the time on System.IO.StringReader.ReadLine . We know this by looking at the Exclusive Samples field. Inclusive Samples indicates the total number of samples a function and all of its subcalls include. Exclusive Samples , by contrast, contains only the samples that this function (and not its subcalls) is responsible for. So, in this case, ReadLine is responsible for 876 inclusive and exclusive samples -- it's the end of this callstack. PeopleNS.People.GetNames , its caller, has 1,378 inclusive samples, but only 77 exclusive ones. If we were to look at GetNames in the Function Details view, we'd be able to see that only 4.1% of the samples are in the function itself. Most of the work is delegated out to ReadLine , TrimHelper , Add , and GetString . A more established profiler might be able to discern the problem from sampling alone. However, I started my profiling with the assumption that most of the time was going to be spent on reading and processing the data, and that largely seems to be the case here. Also, I want to shine light on Instrumentation, so...let's fire up an instrumentation session! First, you should notice two things: The Summary View changes a little bit of our view -- ReadLine still does a lot of work, but between itself, Trim , and Add , we've only accounted for around 40% of the execution time, yet the calling function, GetNames , consumes 72% -- what is account for the remaining 30%? Let's click on the GetNames method to look at the function details. Can this be right? This code primarily does its work through child functions, and the big functions seem to be accounted for. Are we spending time in the ArrayList or StringReader constructors? Probably not, but we can click the Other bar in Called Functions to find out. It will bring us to the Caller / Callee view, which confirms that the constructors aren't taking much time -- both of the constructors are 0.01%. However, looking carefully at this view, some things do stand out to me. First, we see actual time values, not just percentages. We can also right click any of the headers to add additional fields we'd like to look at. Here, let's add Elapsed Inclusive Time . With this field, we can now set a benchmark for our optimizing efforts -- right now, we are spending ~14.5 seconds in GetPeople . Sure, this value is inflated by the extra instrumentation overhead, but 15 seconds is going to feel long for most of my our users. This is definitely why our program feels slow. Second, we can see call counts for each function. GetNames is being called 3000 times. WHY? Judging by the method signature, it should return all the \"names\" in our database. Calling it the next 2,999 times won't do anything differently than it did on its first run. Maybe the result isn't being cached, so let's move up the callstack to see how GetNames is being used; right click GetPeople and click View Source . Sure enough, we are not caching these values, although ironically, we are set up to do so. This function fetches number people, but for every person fetched, it overwrites this._fullNames and this._companyNames . That's where the 3000 calls are coming from. Let's cache these values outside of the for loop and profile again, to see if we made any improvements. Now, even with the instrumentation, the program feels snappy. ReadLine isn't showing up on our hot path anymore, because the bulk of time is now spent just waiting for GUI input. I can see GetNames now is called only 2 times (once for the names, once for the companies). GetPeople now only took 30ms, for a 99.8% improvement in our responsiveness! We could drill further; for example, if a user were to click \"Get People\" multiple times, we are going to overwrite our _fullNames and _companyNames fields, just as before, for no good reason. And we could potentially look at more efficient looping. Right now, GetNames fetches all records, even if GetPeople only needs a few of them. We could look for micro-optimizations, such as moving companyName and fullName 's declarations outside of the for loop in GetPeople . But is any of that worth it? Per the 80/20 rule, we already found the small percentage of our code that happened to be the bottleneck, and focusing on that percentage, we were able to boost our performance by 99%. We've got the gains we were looking for, so our time is better spent elsewhere. If you don't want to profile certain parts of your application, you might be tempted to attach to a running instance of it (Through Performance Explorer->Actions->Attach/Detach ). However, with this option, you have limited control, and can only use sampling. A better method is to create a performance session as normal, but choose not to launch it initially. With this session set as the current session in the performance explorer (right click the session and choose Set as current session ) you can then use Performance Explorer->Actions->Start with Profiling Paused . This gives you the flexibility to continually start and stop your profiling to only profile areas of the application you are interested in. If you need even more precise profiling (or are too lazy for starting/stopping profiling) you have 2 other options: Under any performance session, you'll see the targets folder. Normally, this will only contain a single item, which is generally your main assembly. However, you can add as many targets as you like. To do so, right-click the targets folder, and select one of the Add options. Each of these targets can be toggled on or off individually -- right click on the desired target and select or deselect the instrument option. When instrument is enabled, calls which pass through this target are recorded. You can perform this at either the session or target level. In either case, right-click the session/target and select properties. Under Advanced->Additional instrumentation options type: /include:<full method signature/wildcard> An example of a full method signature looks like: to grab all methods within SomeClass, you could write: For a full list of functions within a binary, from the commandline, navigate to the Visual Studio Performance tools directory. On my machine, it is located at C:\\Program Files (x86)\\Visual Studio 14.0\\Team Tools\\Performance Tools . Within this directory, you can execute vsinstr.exe /DumpFuncs <binary> for a full list of functions NOTE : This still records all calls from matching functions to external assemblies. However, ignored functions will not show up in the final report, instead having their overhead added to the parent function. For example, if function Lib1.A calls Lib1.B and Lib2.C , and I am filtering for only Lib1.A , I will get data for Lib1.A and Lib2.C -- Lib1.B will be bundled into Lib1.A 's stats. Here are examples of some different bottlenecks: Dominated by System.IO.File.ReadAllText Dominated by RestClient.Execute Note that here, Wait / GetResult dominate the output. This is because tasks are a layer of syntactic sugar over what the CPU is doing behind the scenes. As far as the main thread is concerned, it is doing nothing while it waits for the web request to come back. If you need to track individual task times, you'll want to temporarily split them out into their own synchronous methods, or approach it through alternate means (for example, using System.Diagnostics.StopWatch ) Largely the same as the task requests -- the asynchronous call doesn't tie up the main thread, so most of the time is stuck sleeping. If you are interested in profiling code that hits Microsoft system libraries, and to a lesser extent 3rd party libraries, it is important to update your symbol database. Symbol/PDB files are responsible for linking assemblies to debug information, such as line numbers and variable names. .NET symbol files are relatively minimal, as their assemblies already ship with some debug information. However, if you are working with native assemblies and do not have the associated symbol file, you will be stuck reading machine code. For more information about PDB files, check out: http://devcenter.wintellect.com/jrobbins/pdb-files-what-every-developer-must-know . To update your symbol database locations (for more details: https://msdn.microsoft.com/en-us/library/89axdy6y.aspx ) And that's it! Good luck on your optimization journeys!", "date": "2017-2-8"},
{"website": "Avenuecode", "title": "Java Mission Control and Java Flight Recorder", "author": ["Vinicius Munhoz"], "link": "https://blog.avenuecode.com/java-mission-control-and-java-flight-recorder", "abstract": "Working as a programmer you will probably always face doubts about the performance of your applications. Proving that the application you have developed is reliable and has good performance and good memory usage is never an easy task. We do have several tools available from different vendors to help us accomplish this, but the problem is that those tools are never free and they often impact the performance of the application being tested, generating results that are unreliable or downright false. When developing applications using Java, we can monitor and collect performance indicators with a tool that comes embedded in the official Java Development Kit from Oracle. Oracle’s VM comes with a great tool called Java Flight Recorder, which is responsible for collecting data from the VM running the application and for displaying the data collected in a visually appealing format. We can divide the monitoring of the applications running inside the JVM into a set of two different tools: the Java Mission Control (JMC), and the Java Flight Recorder (JFR). Java Mission Control - JMC To understand how the flight recorder works we actually need to start with the Java Mission Control, which is responsible for displaying the data from the current execution of the JVM in real time. The JMC collects the data from the current execution of the JVM through the JMX API, using the default JMX Managed Beans from the JVM. The Java Management Extensions (JMX) is a default API created to manage and monitor resources inside the JVM. Those resources are exposed through the JMX Managed Beans, which are basically simple java objects  registered in a JMX server inside the JVM called MBean Server. Every time that an event monitored by a managed bean occurs, the JVM sends a notification to the respective managed bean which has its attributes changed. Because the information comes from the JVM and is not obtained in an intrusive way such as intercepting the operations, the impact to the performance caused by monitoring is almost none. By Oracle’s benchmarks, the performance loss caused by the JMX monitoring is less than 1% in the application. Therefore, we can safely ignore the impact when analyzing an application using the JMX beans (for instance, java mission control) to obtain the JVM execution information during the application execution. The data collected by JMX’s managed beans becomes available when we have a connection to the MBeans server that allows us to retrieve the data collected in real time. To connect to the MBean server, we use MBean agents. The agents are responsible for reading the data from the managed beans and also for setting new parameters for those managed beans. We could use the managed beans to change configurations parameters in real time in the JVM, but this won’t be covered in this post since our focus is the java flight recorder. The Java Mission Control works as an agent for the managed beans, but instead of just reading the data provided by the JMX, it also gives us a graphical interface to analyze the information about what is happening in the JVM in real time. The JMC is even able to present graphics about the usage of resources in the JVM so we can analyze and take actions in our application. If you have installed the official java development kit from Oracle and you have setted the environment variables in your operating system correctly, then, you can start the java mission control by simply typing the command “jmc” in your terminal. By running the jmc command you will start the JMC interface, and on the left side of the interface you should be able to see all the instances of JVMs running on your machine. Double click the one you want to monitor and double click again on “MBean Server”. This will open the main java mission control page, which should look something like this: As you can see in the image the JMC generates, it gives us a set of information displayed in a graphical interface and organized by several graphs and gauges, in which all information displayed is really happening in real time inside the JVM. Using the tabs in the bottom of the java mission control you will be able to switch the views to check the most important information that you need. For example, if you are interested in only the memory usage and garbage collections which are currently happening, you can go to the “Memory” tab and it will show you all the information possible regarding the memory consumption and garbage collections happening in the JVM. Java Flight Recorder - JFR When we look at all the information that we can get from the Java Mission Control, we attain a vantage point from which we can begin to evaluate what is happening \"under the hood\" of our applications, and tweak our code accordingly to improve performance and memory usage.  The problem with the Java Mission Control by itself is that the data there is being collected in real time and is not saved/archived anywhere. Therefore, we should keep the JMC and the application running to analyze the data from the JMX. We cannot compare two different versions of our code, because once we turn off the application to run our new code we just lose the data collected. To solve this problem, we can use the Java Flight Recorder to save all the data from the execution inside the JVM. The JFR will save all the fine-grained information that we have in the JMC for a determined period that we can specify. After the execution of the specified window of time, the JFR will generate a simple and self contained file with all the information captured during the execution of the application. In this way, we can analyze the execution saved anytime we want, and conduct comparisons among executions of our application. To start a java flight recording, we can just right click the “Flight Record” option under the JVM in the Java Mission Control interface and specify the parameters to save the execution the way we want. Here, we are simply specifying the name of our flight record and the time that we want to monitor the JVM. If we click the finish button, the flight recorder will monitor everything happening in the JVM for the next 5 minutes and save it to the specified file in the “filename” field. Afterwards, we can open the file and check everything that happened during the execution of the application by the time we started the flight recorder. Note that the data and gauges from the flight recorder won’t have its values updated like we used to see in the Java Mission Control. One option when we have a flight record is to analyze is to see what happened during a peak of processing or memory usage. For that we can scroll to the limits in the first panel and reduce the time inside the recording that we want to analyze. Using the flight recorder we can even check how many threads were running in the application and for how long the threads moved between the possible states. We also could check which java classes have executed to understand where our resources are primarily being consumed. I encourage you to explore by clicking through the tabs of your flight record, and have fun getting to know how your application is using the JVM to process. Conclusion The java mission control and the java flight recorder are awesome tools to check how the JVM is handling our code by enabling us to see information about the execution of the applications that we develop in a very fine-grained level of information. Using both tools together, we can improve our code to avoid much bigger problems when we move a production environment.", "date": "2017-5-10"},
{"website": "Avenuecode", "title": "AC Spotlight - Kristen Stewart", "author": ["Ashley Wang"], "link": "https://blog.avenuecode.com/ac-spotlight-kristen-stewart", "abstract": "Kristen Stewart is Head of Ecommerce & Digital Marketing at Cushnie et Ochs , a luxury womenswear retailer that has come of age in the digital era. We were captivated by her talk at Luxury Interactive in October, and thrilled when she agreed to let us interview to learn more. Avenue Code: What about e-commerce and retail attracted you in the first place? Kristen Stewart: I’ve always loved fashion, so I love that I work in an industry where I get to play with pretty clothes. As for e-commerce, I’ve been a bit of a computer geek my whole life. My family was one of the first ones in our neighborhood to have internet back in the day, so I enjoy the ability to integrate my love for fashion and technology. I love that e-commerce creates a way for people all around the world to have access to the same things. AC: How did you get your start in e-commerce? KS: About 15 years ago, a friend and I were assistant buyers at Rampage. We planned to open our own business, and we wanted a store but didn’t have the capital to open a brick and mortar location. E-commerce was new at the time, so we started our own little brand where we were buying small portions of oversold products and then selling them online. My friend was the buyer and I took over the digital part. Although this little project was never enough for us to quit our day jobs, it helped me realize that I loved the whole digital side of the business. AC: Tell us about Cushnie et Ochs. How would you describe the brand? KS: We’re a luxury womenswear brand designed by Carly Cushnie and Michelle Ochs. The energy of the two designers creates a lot of great dualities in our brand such as conceal and reveal. Our clothing is often fitted and we use a lot of cutouts, but they’re strategically placed to be flattering and elegant. We like to accentuate parts of the woman’s body that people aren’t used to seeing. Who said sophisticated can’t be sexy? In Cushnie et Ochs, you can’t help but exude a sexiness that’s subtle but undeniable. AC: As a company that came of age during the digital era, how have you at Cushnie et Ochs been able to stay abreast of consumer demands? What challenges do you face? KS: We pay attention to our analytics. When we built the site, everything was tagged and we kept track of every movement customers were making. What has been working for these last six months may suddenly stop working, which is why it’s important to remain agile and pay attention to the analytics. You have to stay ahead of the customer and think from their perspective - what attracts someone to an $1,800 dress versus something they can buy at Target? AC: Makes sense! And what about mobile? E-commerce seems to be more and more mobile-centric. What is the mobile experience like at Cushnie et Ochs? KS: The majority of our traffic comes from mobile. As we move forward, I’m taking a mobile first approach, which is why we’re building our mobile experience to become as app-like as possible. AC: How do you combine mobile, digital, and web together to create a differentiated experience for the customer? KS: Right now, you can’t go wrong by taking a mobile first approach. Our methodology is to focus completely on a mobile experience and we’re building the desktop experience to be a responsive version of that mobile experience. When it comes to mobile UX, simplicity of design is the most important but you should always be sure to keep up with technology trends. For example, we find that it’s become more natural for people to scroll up and down in a mobile experience, when traditionally all mobile experiences used to swipe left to right – now swiping is on its way out. It’s little details like that that we have to pay attention to because they make the experience more enjoyable for the customer. AC: Where do you see the future of the retail industry headed? KS: I know there has been talk about brick and mortar being dead, but I don’t think that’s true. The retail industry will always need diversity of experience. E-commerce sales will definitely continue to grow in the foreseeable future - one day, of course, the idea of e-commerce and m-commerce may evolve into something that we can’t even imagine right now – but as long as we keep our eyes on the horizon, we’ll be ready for that change. AC: You  mentioned at Luxury Interactive that in order to be exclusive today, brands need to be inclusive. Can you elaborate more on that? KS: A lot of luxury brands have old-fashioned ideas of who the luxury customer is. With the advent of the internet and the e-commerce era, luxury has become more widely accessible. Many luxury brands have been slow to realize that they need to appeal to the masses – anyone within those “masses” has the potential to be a luxury customer. Mid-price brands are starting to give more luxurious experiences to lure the luxury customer. As a result, the today’s luxury consumer may be shopping at Zara as well as luxury brands, and even wearing the pieces together. Exposure and impressions are different from inclusivity - brands need to build marketing plans that are inclusive of the modern consumer. AC: It’s easy to see how Cushnie et Ochs pegs that wider appeal. More generally, is there something you would pinpoint as a highlight or shining moment for you in the last few years? Was there a moment for you personally, or for Cushnie et Ochs, where you knew you were on the right track? KS: We won the Best Website Design Award from the Graphic Designer Association of America for 2 years in a row when I was at Scoop NYC. I’m hoping to win that again this year at Cushnie et Ochs. Our sales at Cushnie et Ochs have grown substantially from when we started this e-commerce business from scratch. Our traffic is 50% larger than I expected, and we’re getting a lot more attention for the brand. Honestly, my whole experience with Cushnie et Ochs has been one big high point! AC: Great! Can you leave us with any closing thoughts? KS: My advice would be learn to harness your analytical and creative mind because you really need both in fair balance. Every decision you make in e-commerce is 50% technical and 50% creative.  Oh, and always stay ahead of the curve! AC: Thank you so much, Kristen! It was a pleasure talking with you - we look forward to keeping an eye on what you and Cushnie et Ochs are up to in the coming months and years! This article is property of Avenue Code, LLC, and was originally published with permission at Total Retail .", "date": "2018-2-28"},
{"website": "Avenuecode", "title": "Thoughts About Reactive Programming", "author": ["Matheus Eduardo Machado Moreira"], "link": "https://blog.avenuecode.com/thoughts-about-reactive-programming", "abstract": "Reactive Programming is a \"chameleon\"! :-) It can be understood as \"a style of micro-architecture involving intelligent routing and consumption of events, all combining to change behaviour\" 1 ; or it can be defined as \"a programming paradigm oriented around data flows and the propagation of change\" 2 . It has something to do with the Reactive Manifesto . Functional programming comes to mind when thinking about reactive programming. And it is a software development field where a lot of interesting progress is being made! Lots to discuss here! My objective is to register my thoughts and reflections while researching reactive programming, in the hope that it will help others understand a bit more about it and how it can be used to address current (and future) software development challenges. Andre Staltz , in his article The introduction to Reactive Programming you've been missing , goes right to the point when he affirms that: Reactive programming is programming with asynchronous data streams 3 . (...) On top of that, you are given an amazing toolbox of functions to combine, create and filter any of those streams. I recommend reading Andre's text attentively because it is a good practical introduction to reactive programming (how to think about it and how to write reactive code), and it helps build the foundation for further study. The diagrams are particularly worth paying attention to because they are a common means to describe and model streams behaviors and transformations. Now, I admit that in the beginning I associated reactive programming more with Akka and its programming model . That is because some months ago I read the Reactive Manifesto, and Message Driven (asynchronous message passing ) was an idea that I associated directly to Akka (reinforced by the fact that some of the manifesto's authors are related with Scala and Akka). Imagine my surprise when I discovered that the Akka Stream library! Akka is indeed a reactive framework, and reactive systems can be built using it. But what I think is most important is that the main abstraction of reactive programming is async data streams . As Andre reinforces in his text: Andre's text also highlights the fact that streams can be combined to generate derived streams. This brings functional programming into mind, because: Probably because of these similarities, FRP - Functional Reactive Programming - and RP were considered synonymous. But of course it is possible to write reactive systems without a functional language, so FRP is nowadays considered a misnomer. Read this write-up from Netflix and you'll begin to have an idea of what can be accomplished with reactive programming. Netflix's post has cool points that I'd like to highlight: There is a series of posts from Spring that helped me deepen the idea of \"reactive service layer\" and how a system could be entirely reactive. The first and second parts prepare us for the Reactive all the Way Down section in the third part. My conclusion after reading that was: \"Wow, an 'all-the-way-down' reactive system could squeeze every drop of computational power available in the infrastructure.\" (Any blocking call would be put aside while waiting for a response, meanwhile something else would be using the processor.) That resonates well with the Elastic part of the Reactive Manifesto, doesn't it? ReactiveX was the first reactive library I heard about. There is a fine version for Java ( RxJava ) with excellent documentation ( Wiki and Javadoc ). Then somehow I found about Reactive Streams . At first I thought it was a kind of ReactiveX competitor, where several companies were trying to conceive a specification and implement it. This was a misunderstanding: it is actually an effort to define a standard for stream processing (with non-blocking back pressure, mind you), but it is not a library per se. Library implementors code against a set of interfaces , abiding by the rules expressed by the specification . The result is twofold: You may want to take a look at Dádiv Karnok's Advanced Reactive Java . Although the text deals with operation-fusion, an advanced topic in reactive programming, the section Generations is enlightening about how reactive libraries have been progressing and what the future may hold. (By the way, Akka is a 3rd generation library, so I was not that far off in the end!) This question kept crossing my mind the entire time I was researching reactive programming. I think that it is time to experiment. Why? Think about how we currently develop systems: So I would say that if reactive programming is not mainstream in your company, try to experiment with it. Probably you'll find it is applicable somewhere, and if nothing else you can at least build knowledge about it. Reactive programming is such a cool subject, with a wealth of information and possibility. I hope my thoughts were helpful for anyone interested in reactive programming to paint a clearer picture about it. The linked resources are invaluable sources to expand/consolidate the ideas that I presented briefly here. Hopefully this was enough to get you excited about writing reactive code for yourself! https://spring.io/blog/2016/06/07/notes-on-reactive-programming-part-i-the-reactive-landscape#what-is-it ↩ https://en.wikipedia.org/wiki/Reactive_programming ↩ Asynchronous data streams are given different names by different libraries: Observable by ReactiveX , Flux / Mono by Project Reactor , and Flow by Akka Stream . Probably other libraries use other names for the same concept... ↩", "date": "2016-12-7"},
{"website": "Avenuecode", "title": "A Web History: The Origin of Bundlers, Part 2", "author": ["Igor Rezende"], "link": "https://blog.avenuecode.com/a-web-history-the-origin-of-bundlers-part-2", "abstract": "Last week, we set the stage for understanding the origin of bundlers: our protagonist Jay Ess successfully solved the problem of his single, monstrous script file by breaking it into several smaller files, but now he faces new challenges and must find better solutions... In our last installment, Jay Ess was struggling to maintain his massive script file, a task made all the more complicated because he had a development team working on it alongside him. He solved this issue by breaking his script into several small files, but now he has the following problems: 1. He has a lot of <script> tags on his page 2. He has to keep a certain order 3. Coworkers complain that it takes too long to load 200+ files Wanting to make some progress, Jay Ess begins thinking through how to solve his first problem--having a lot of <script> tags. Now, there is actually a simple solution to this problem. It won't solve all his problems, but it will help our poor developer stay on track: he could have a shell script or another similar tool that would simply concatenate all his script files in a certain order. This would solve his first problem (since he has 274 or more files by now). Using this method, the script that was included on his page would be condensed into a single script file. It could return a single <script> tag on his page. \"What great progress!\" exclaimed Jay Ess after a loud giggle as he read about this possible solution I mentioned in a post on the internet (of course). But still he has the second problem at hand: for the shell script to properly concatenate all JavaScript files, it has to maintain the correct order. Jay Ess can think of several hairy solutions, and maybe some of them can partially solve his problem. In a fictional situation (inside a fictional story), during a period of insight into the radiant light of eternal wisdom, he then thought, \"Maybe I can name my files following a pattern, like 00001-someArchive.js, 00002-anotherFile.js ..., and then proceed with concatenating following that order. Well ... keeping this going would be a bit chaotic, but it would work!\" So that's what he did. At first, he used a numeric sequence, but after having to rename 80 files one terrible day, he left some empty spaces to fill when needed. (Jay Ess… such a smart guy.) Well, as we can see, any solution that follows this line of thinking is complicated or becomes complicated very quickly, and it also does not solve Jay Ess's problem in a practical way. Is it a valid attempt at solving it? Yes, but it is only an analgesic, relieving the pain momentarily. Deep down, Jay Ess knows there are more sophisticated methods available, so he resorts again to the web oracle to seek wisdom. After reflecting on this for a while, Jay Ess arrived at the idea that it would be really cool if he could add some form of his own script file and make his own order, or rather, make his dependencies explicitly available. Base image from Pixabay That’s when he discovered RequireJS, a module loader similar to his idea! In summary, RequireJS is what many people say it is--a Module Loader. RequireJS implements a definition called the Asynchronous Module Definition, which loads modules asynchronously : // myShirt.js now have dependencies, the shopping cart and the inventory // module in the same directory as myShirt.js define ([\"./ cart\", \"./inventory\"], (cart, inventory) => { // returns an object to define the module \"myShirt\". return { color: \"blue\", size: \"large\", addToCart: () => { inventory.remove(this); cart.add(this); } } } ); Now, while he was making use of RequireJS, living happily immersed in the comfort of this world, some guys posted something called Node.js on the web , and this thing became extremely popular! This is relevant because Node.js includes a mechanism that does exactly what Jay Ess wants to do: it defines modules that can have local and private parts, export some public parts, and require other modules/dependencies. Node.js had all the power of a popular syntax of a specification known then as CommonJS. The syntax in Node.js works great on Node.js, but module support wasn't fully available for all browsers at the time. After a while, the ECMAScript standard decided to use another similar mechanism and a new extensible synta x. The takeaway? It's smart to follow popular syntaxes since they have a good chance of being maintained by the community. When Jay Ess finally discovered that this syntax would free him from the evil of that weird code about closures and allow him to write modules in a cleaner and more uniform manner, he decided to follow the Node.js path : // request dependency: let a = require ('a.js'); // Node.js modules import a from 'a.js'; // ES modules // ... your code here ... // a way to turn the things visible outside the module: module.exports = something; // Node.js export something; Photo by Sam Mgrdichian on Unsplash Since Jay Ess still needed to package all his files into a single file and make that file work in his browser (not Node.js), he had to revise his shell script, which only concatenated all his JavaScript files, into something more sophisticated. Here’s how he did this: Before each script file, the shell script would insert a number of generic utility functions because the browser does not support the Node.js (require, module.exports) syntax. The module code will be written easily, and when the time comes to execute, it will have to manage how and when it will run. If the code calls require (\"something.js\"), its utility generic functions will make the code available for that module. Usually this is done through records or identifiers that allow the system to reference the modules correctly, as if they were referencing common files. Let's recap again! Jay Ess now has a manageable code base with source code that is functionally divided into small modules. It also has a process or a tool that: 1. Puts all his small files into a single file 2. Adds generic utility functions that: Hey wait a minute! How on earth has the sorting problem been solved? I lost that part! Well, now module execution can be delayed if any dependencies aren’t yet loaded (asynchronous loading)! So Jay Ess no longer has to worry about order. It simply loads all modules and then executes what it wants to execute. Note that this is not the only benefit. The dependency system can work with its code, either encapsulated or grouped into a single file, or even, if appropriate, a separate file that was loaded on demand/import. As long as the system or tool makes the mechanisms available and understands the same syntax, Jay Ess acquires that capability under his code without his code knowing about it (remember SOLID? No? Go read the principles, paying special attention to the dependency inversion part!). Now this tool that Jay Ess utilized is one that we can use ourselves, just as we imagine here in this story. But wouldn’t it be easier if several people made similar tools that would work together, or else used the same tool we're using? Then we could treat external libraries how we treat our own code--like a contract or a common interface. Instead of building a tool ourselves, it's smart to use an existing one. These tools are Browserify, webpack, Parcel, etc. Some of these tools take advantage of the fact that we are already doing all these code transformations, as well as packaging processes, to provide other tasks like minifying our code (compressing it and reducing load time). Other tools are sophisticated enough to prevent the inclusion of files (or file parts) that aren’t in use (a technique known as tree-shaking). They can also process assets like CSS and images. Why not make use of everything? COMING SOON! Want to know what's next? In our third and final installment, we'll examine the generated code and introduce one of the most popular tools available--webpack!", "date": "2019-1-9"},
{"website": "Avenuecode", "title": "AWS Serverless Apps' CI/CD", "author": ["Milena Santos"], "link": "https://blog.avenuecode.com/aws-serverless-apps-cicd", "abstract": "Last week, we discussed migrating an application that's on-premises to AWS . This time, we are going to talk about deploying serverless applications in AWS! But wait, what does serverless mean? As explained in another of our Snippets posts dealing with telegram chatbots , serverless is a paradigm that allows us to build and run functions without having to deal with all the infra by using a cloud provider to host the functions and take care of everything for us. In AWS, this event-driven, serverless platform is known as \"lambda\". Imagine a huge variety of solutions available through a large selection of platforms--this is what you can do with lambda functions, especially now that it supports custom runtimes besides the official ones (you can find out more about this here ). AWS Lambda charges per use and based on the amount of memory set, so it's important to build solutions that will take as little time as possible to run, and be aware of timeouts to avoid unexpected charges later, e.g. you'll incur extra charges if your function calls for an external service that's unavailable or if a large file is uploaded without its size being limited.  Sometimes there are cases where we cannot escape extra time and charges, but knowing the implications of certain actions and implementing the functions with an understanding of how AWS Lambda works is the key for success. Before going into the specs of automating the deployment, let's check how it works if we do it manually. Below  we have an example of a Node.js lambda function that's small enough that we can edit the code inline in the GUI console. Keep in mind that this may not be possible with certain package sizes or with runtimes such as Java. We can create and edit a lambda function, publish new versions, and define aliases to invoke them either via the console or by using your favorite IDE with an AWS SDK plugin. That's about all we have do to make the function available. Furthermore, there are ways to \"talk\" to this function. These include merely invoking it from another lambda, cloudwatch rules to behave as a scheduled job, exposing with API Gateway endpoints, and more. The example created for this article generates a simple API with POST method only. Similarly to the lambda function, this can be managed manually, as depicted in this console view: With the lambda function in place and the API Gateway endpoint properly configured, we have a service ready for use--imagine the possibilities! Beyond simply exposing the API, AWS Lambda allows you to define authorizers for the endpoint, limit Lambda to work only on a private network, create tests to validate the code in the Lambda console, and test the API through the above GUI prior to deploying it to stages . All that and more make this quite a flexible solution. Finally, let's talk about the automation of our lambda function integration and deployment. This is very crucial not only for cases where the code changes often, but also for that disastrous moment when we delete a lambda function we manually created only for doing some tests and accidentally delete another function! Okay, so the following represents the flow from a generic git repository, which will have the latest version of the code from a branch cloned and uploaded into S3. This will be the source used in the CodePipeline configured with build and deploy sections. You may notice that this is similar to the flow described in our previous post. The difference is that these resources (CodePipeline and CodeBuild project) are all created via a CloudFormation template and the deploy section invokes a CloudFormation template too! How is that? First, let's look at some examples of the flow for deploying lambda functions in AWS: Standard CodePipeline Unveiling the deploy mystery, the CloudFormation templates are there for creating any type of resource we need inside AWS, and that's no different for lambda functions and anything else we do with them. So first we use a template to create a pipeline and CodeBuild project that's unique to the application we're working on. Beyond this, we need another template, also known as a SAM file, that will be updated in the deploy section of the CodePipeline in every build/deploy to publish a new version of the lambda and update anything pointing to it to invoke the newest code. You can find more details on this here . Below, you can see the structure of a simple Node.js lambda function and its underlying resources. This is comprised of: - the index.js with the entry point \" handler(event, context, callback) \"; - app-aws-cf-stack.yaml , the template used for creating the pipeline etc; - buildspec.yml , which gives instructions for the CodeBuild project to run the build; - templates/serverless-example-sam.. , which is the cloudformation template used in the CodePipeline build section to publish the new lambda function version; - templates/serverless-example-apigateway.. , which is our swagger-formatted file invoked by the SAM above to instruct the API Gateway to create a new endpoint if there isn't one yet, or to update it if necessary. You can view the source code for this example here . That's all, folks! Now you know how to deploy an AWS serverless application. Thanks for reading, and please add comments, questions, and suggestions of your own below!", "date": "2019-2-5"},
{"website": "Avenuecode", "title": "Java Microbenchmarks with JMH, Part 2", "author": ["Andre Brait"], "link": "https://blog.avenuecode.com/java-microbenchmarks-with-jmh-part-2", "abstract": "In last week's blog , we discussed the basics of creating a JMH project and getting it to run. We also proposed a small benchmark and showed what the implementation would look like. Today, we'll be looking into how to configure JMH, how it works, and why certain parameters are necessary. (Be sure to join us next week for tips on how not to create benchmarks!) In the first part of this series, we showed a comparison of different ways to use multiple threads to get the total sum of a list of integers, as in the code below: We'll refer to this code throughout the rest of this article. First things first: we can see there's a number of annotations that have been added to our code there. Those are JMH-specific annotations that allow us to tune and configure the benchmarks to suit our needs and fit our constraints. Let's explore what each one means: Benchmark Modes: JMH allows users to determine what they want to measure. There are 5 modes in which JMH can run a benchmark: You can specify in which mode each benchmark is to be executed using the @BenchmarkMode annotation. Each benchmark can have its own configuration here. If the annotation is not used, the default mode is throughput. You can see we used the Average time mode for the benchmarks in our code since we're interested in knowing which is the fastest approach. Warmup: JMH performs a few runs of a given benchmark and then discards the results. That constitutes the warmup phase, and its role is to allow the JVM to perform any class loading, compilation to native code, and caching steps it would normally do in a long-running application before starting to collect actual results. The difference between running the code with and without a warmup can be quite noticeable, so it's recommended that the benchmark is allowed to warm up at least a few times. The number of iterations per warmup phase can be configured using the @Warmup annotation. The default value is 5. Fork: There are many things that can affect the performance of an application in the Java world, such as OS-related things like memory alignment when a process is created, the moments the GC pauses occur, differences in how the JIT behaves (both as a whole and in each execution), etc. For instance, if two benchmarks deal with classes that implement the same interface, the benchmark that gets executed first might tend to be faster than the other one since the JIT compiler may replace direct method calls to the first implementation with interface method calls when it discovers the second implementation. That could produce inaccurate results since the order of execution of the benchmarks can affect the performance. That is precisely what happened in the bad benchmark example in Part 1 . Each one of those methods ran from inside a class that implemented Runnable . Doing nothing was slower than doing something merely because the former was run after the latter. (A more detailed explanation can be found here .) So, how can JMH deal with these issues? Simply put, it avoids them by forking the JVM process for each set of benchmark trials. The number of times this is done can be controlled with the @Fork annotation. It's highly recommended that at least 1 fork is used. Here again, the default is 5. Sometimes you'll want to initialize some variables that your benchmark code needs, but which you do not want to be part of the code your benchmark measures. Such variables are called state variables. State variables are declared in special state classes, and an instance of that state class can then be provided as parameter to the benchmark method , like in the example. The state class needs to be annotated with the @State annotation, and it needs to adhere to some standards: State Scope: A state object can be reused across multiple calls to your benchmark method. JMH provides different scopes that the state object can be reused in. These are: Since this is a single-threaded benchmark (even though it uses multiple threads inside each benchmark), the scope is not all that important, but we'll be using the benchmark scope in this example. Setup and Teardown: State objects can provide methods that are used by JMH either during the setup of the state object, before it's passed as argument to a benchmark, or after the benchmark is run. To specify such methods, annotate them with the @Setup and the @Teardown annotations. The @Setup and @Teardown annotations can receive an argument that controls when to run the corresponding methods. Those arguments, called Levels, can be: In the example, we've annotated the setup method with @Setup(Level.Trial) in order to create a new list of random integers for each trial of benchmark executions. It might be interesting, for a number of reasons, to run the same benchmark for a given set of parameters. One common reason to do this is scaling. Even slow solutions can appear to be fast if the number of items on which they operate is small enough. Another common use-case is testing different parameters for a configurable algorithm or data structure, like the max depth of a queue or the number of reader and writer threads. There's no need to run the benchmark separately for each one of those cases. A state object can contain a field on which JMH will inject a set of parameterized values, one for each set of benchmark runs. This field needs to be annotated with the @Param annotation, which receives as argument an array of strings. In the example, the @Param was used to set a different list size for each set of executions (1 million, 10 million and 100 million). Finally, it's time! Upon running the example, we get something analogous to the following: The first paragraph is extremely important: do not take these numbers for granted. Follow that advice, always. I think you might better understand why I need to stress this after you read part three of this series. Until then, keep in mind that benchmarking is tricky, so always take those numbers with a grain of salt. So, what do the results tell us? Let's put those numbers on a graph so we can visualize them: It's quite clear that the fastest way was using a LongStream's sum method, followed by the LongAdder, then the AtomicLong and, finally, the synchronized volatile long . While the difference is always there, we see that LongAdder started just barely slower than LongStream's sum, but as soon as we started increasing the size of the list, the difference became more pronounced. What did you expect? More importantly, why does this happen, and why is volatile + synchronized slower than AtomicLong? I guess we have some homework to do now ;-) There's a section above that I did not explain. I did that on purpose ;-) The reason why it's there, and specifically why it sometimes needs to be there, will be fully explained in Part 3. Stay tuned!", "date": "2019-4-3"},
{"website": "Avenuecode", "title": "AC Spotlight - Isabelle Fesale", "author": ["Anna Vander Wall"], "link": "https://blog.avenuecode.com/ac-spotlight-isabelle-fesale", "abstract": "(The following article is property of Avenue Code, LLC, and was originally published with permission at Total Retail on November 7, 2019 .) Isabelle Fesale, Founder and CEO of The Izzy Box , tells the story behind the subscription box-based business dedicated to delivering customers products from minority-owned and women-owned businesses. Avenue Code: The Izzy Box launched Sept. 30 of 2019. For those who haven’t yet heard about The Izzy Box, can you give us a brief introduction? Isabelle Fesale: The Izzy Box is a subscription box-based business that’s unique in a couple of ways. First, our mission is to support minority-owned and women-owned businesses, so all of our products are crafted by these brands. By purchasing our subscription boxes, customers are supporting these businesses while enjoying, in each box, eight full and sample-size, health-conscious products that they won’t typically find elsewhere. Secondly, The Izzy Box’s products aren’t targeted toward a specific demographic. They're for both men and women, and provide a wide range of products, from fashion to beauty, grooming, skin, hair, health, and packaged food items. AC: Tell us about your personal career path. When did you know you wanted to be an entrepreneur? IF: I was about 19 when I decided to become an entrepreneur. During and after college, I held several marketing internships with startups, small businesses, and women-owned companies. It was a great learning experience, but in retrospect, I can see I was always meant to create and own my own business. My passion for other jobs was always fleeting, and ultimately, I wanted to leave my own legacy. While I was in college, my dad fell ill, and I took time off to care for him. Later, my mom was diagnosed with stage two breast cancer. Because my family is from the Caribbean, we started by pursuing holistic remedies for both of them. Because of these experiences, I couldn’t stop thinking about creating a business that was both health-conscious and philanthropic in nature, but it wasn’t until I stumbled on a motivational Instagram post that I decided to act on my idea for The Izzy Box. The post said, “If you can’t stop thinking about something, go pursue it.” So I moved to Florida and started The Izzy Box at 24 years old! AC: How did you come up with the idea for The Izzy Box? IF: The inspiration came in 2017 when I was riding a train home from work. I had recently read an article on the exponential growth in subscription box businesses. I realized that almost every subscription box business served a niche market, whether that was Dollar Shave Club for men or Scentbird, a fragrance company for men and women. I wanted to create a box with a variety of products for men and women of all demographics. At the same time, I knew I needed to do something philanthropic. Consumers want their purchasing power to go to businesses that intentionally yearn to have a positive impact on the world. That’s the story of The Izzy Box: our customers get everything they want and need while supporting small businesses run by minorities and women. The businesses our customers support are often home-based, run by a young mom trying her best to craft products she’s passionate about. AC: Tell us about your personal experience as a minority and a woman starting your own business. Where did this passion and heart come from to help others in the same situation? IF: I haven’t personally encountered too many roadblocks in starting The Izzy Box, but I take pride in giving a voice to other women and minorities who have faced struggles of their own in the business world. I’ve always believed in the underdog. Those whose voices we don’t hear are those we should invest in the most. Growing up, I felt I didn’t have my own voice. When I became an entrepreneur, I became tenacious, passionate, creative and brave. I'm confident and expressive because I'm pursuing my purpose, and I want to give other women and minorities that same opportunity. Let me give you an example. One of the products we’re launching with is a prebiotic coffee drink created by a woman who has her products in a few local cafes. Her personal story is profound: she has adopted children and, while caring for them, is also promoting health by creating and distributing her drinks to local consumers. Most people have never heard about her products or her story, but they need to. If I can help make her voice heard, my purpose is fulfilled. AC: Beyond working with women and minority-owned companies, how do you decide which brands to partner with? What products might come in a typical Izzy Box? IF: For our launch, our initial products include beauty, fashion, grooming, hair, skin, health, and packaged foods. When we partner with brands, we ensure their products are clean, cruelty-free, and sustainable. Our makeup and skincare products have no carcinogens, our coffee is fair trade, and our health-conscious foods can be GF (and hopefully paleo soon as well!). Even our packaging is BPA-free. Beyond this, you’ll receive unique products you won’t find elsewhere. For example, our razors and shaving products for men have a very unique design. I asked several people on the street about their shaving preferences, and that led me to a women-owned razor business with unique razors and natural shaving products. The Izzy Box is the easiest way to support small, minority and women-owned businesses, and our products hit all the categories consumers want. Our customers receive high-quality, unique items that make them feel whole, whether that’s essential oils, healthy snacks, or ties, scarves and other fashion items that are made from high-quality fabrics. AC: What are your pricing plans? IF: For our quarterly subscription boxes, you receive a box filled with eight items every three months. You can pay $49.99 quarterly for each box or pay $174.99 annually for all boxes. We also know that, as consumers, we like to sample before we buy, so we’ve created a one-time sample box with three to five items for $29.99. Some of our products can also be purchased directly through our website . All of our boxes, even our sample box, contain personalized items when our consumers complete The Izzy Taste Test. AC: Tell us about The Izzy Taste Test and how you designed it. IF: The Izzy Taste Test is a fun, short quiz our customers take prior to paying for their subscription boxes. It allows us to get a sense of who they are, what they like, and what they need. No two customers are alike. You can have people from the same city of the same demographic who like totally different products. We're not pushing products on customers. I want to hear each of my customer's voices and give them what they want. One size doesn't fit all when it comes to a subscription box! AC: We heard there’s a story behind The Izzy Box company name. Can you tell us about that? IF: My family nickname is Izzy, but because we’re Carribean, the pronunciation sounds like “easy.” The Izzy Box is a pun — it’s the easiest way to support minority- and women-owned businesses while meeting your own personal needs as a consumer. Of course, our Izzy Taste Test is also short and easy. Our customers can even earn Izzy Loyalty Points that go toward free shipping and free items. AC: The Izzy Box stands for a whole way of life — healthy products for consumers that support small businesses. This mission could create endless possibilities. Where do you see The Izzy Box one year to five years from now? IF: This question gets me so excited! The Izzy Box isn't just an idea, it’s a vision, and visions are long lasting. Your vision is your legacy, and that’s something you should work on every day. We’re starting as a quarterly subscription box company, but we’re not going to limit ourselves to products strictly in beauty, health, grooming, etc. I don’t want to reveal too much, but I can’t wait to go into other areas of lifestyle and customer convenience like baby products, home decor, and technology. I see so many possibilities for the products we can bring our customers, but no matter how we evolve, our mission will always remain at the core of what we do. AC: What would you say you’ve learned along the way? IF: I want to take a moment to speak directly to entrepreneurs: I’ve learned that fear of failure is what cripples us most in pursuing our dreams, but failure is never actually failure — it’s only a learning experience that directs us to new avenues. In a way, failure should almost be something you’re more excited for than success because it teaches you more. I’ve been told I’m far too ambitious, but that will never stop me. I grew up in a fear-based environment, but when I moved away, I no longer had any doubts or regrets. The amount of confidence I have in pursuing my business goals is astronomical. If you’re an entrepreneur, don’t let fear cripple you! AC: Great advice, Izzy. Thanks for allowing us to be a part of the early stages of your journey. We can’t wait to watch how The Izzy Box evolves!", "date": "2020-2-5"},
{"website": "Avenuecode", "title": "AC Spotlight - Ming Lu", "author": ["Anna Vander Wall"], "link": "https://blog.avenuecode.com/ac-spotlight-ming-lu", "abstract": "(The following article is property of Avenue Code, LLC, and was originally published with permission at Total Retail on November 30, 2020 . ) Ming Lu , CTO at eBay Classifieds Group Emerging Markets, shares takeaways from his global e-commerce career and explains the people-first philosophy that guides his management style. Avenue Code: Tell us about your personal career path. How has your global experience impacted your career? Ming Lu: My undergraduate and graduate degrees are in Computer Science, and I have worked in technology ever since. International collaboration has added a lot of fun and flavor to my career, and it has also given me a broader perspective. For example, when I was at Walmart Global eCommerce, we were researching local grown eCommerce services in the international markets. We discovered that other countries like the UK, Chile, and Mexico already had very sophisticated grocery delivery models; this was one of many experiences that opened my eyes to appreciate international collaboration. AC: How did working with a global team improve Walmart’s e-commerce platform? ML: In my view, global collaboration was more instrumental in the journey than in the end product. I helped set up the first overseas development team in China for Walmart.com and then created the first business process off-shore team in China, and I found that off-shore teams provide really good service. If we found an issue late in the day, it would be addressed by the time we came in the next morning. Another important point is the perspective you gain from international collaboration. In the US, if you need a new website or app, you build it without much concern about internet connectivity, but if you’re developing a product for an emerging market, you have to approach it from a different angle. In Africa, for example, you have to be very sensitive to the bandwidth your website or app takes up. Working with global teams reminds you to ask yourself if you’re looking at your product from a Silicon Valley perspective or if you’re considering the diversity of your customers. AC: What drew you to joining the eBay Classifieds Group? ML: eBay Classifieds Group encourages an innovative and Agile culture. Even though eBay is a big company, our team has a lot of autonomy while also benefiting from constant knowledge sharing across teams, even if they’re 10 time zones away. There are a lot of players in the online classifieds space, including Facebook Marketplace, and I believe my team and I can build something potentially groundbreaking to change the entire landscape of the industry. Ming Lu speaking at the Becoming eBay Summit. Image courtesy of Ming Lu. AC: What new challenges and opportunities has COVID-19 created for eBay classifieds and e-commerce as a whole? ML: E-commerce has, of course, expanded significantly. I just looked at my credit card statement from last month, and every item was an online transaction or a grocery delivery charge. The classifieds sector, however, presents a different challenge, because the transaction model entails finding an item for sale online, reaching out to the seller, and vetting the item/exchanging goods in person. Because of the in-person contact, we initially saw a dip in the vibrancy of classifieds, but now it’s making a comeback. In tough times, people want to buy items at a reasonable price, which often means shopping locally for used items. People are finding creative ways to make this transaction safe. For example, our sellers provide video demos of products, payments are made virtually, and items are picked up with social distancing protocols in place. From the product perspective, our team is also adding new features that make buying and selling easier. For example, we want to make the platform safer by eliminating potential scammers. We’re trying to add new features that give people a better idea of the products before meeting in person so that we save people time and reduce exposure. AC: What’s your strategy for managing a new team? ML: It depends on the state of the team. If the house is “on fire,” I bring about change very quickly. Typically, however, it’s better to be patient so that you can fully understand what state the team is in and how to introduce effective change. I think of it like purchasing a house in winter - the curb appeal may change significantly come spring when the flowers bloom. In the same way, managers should give people time to showcase their skills and processes. AC: How can executives successfully manage distributed teams during the lockdown? ML: Because my team was already collaborating with distributed teams in San Francisco, Shanghai, and Cape Town, it was very easy for us to transition to working from home. That said, I now check in with my team more consciously. I set aside time for informal coffee hours where we connect and communicate relationally without a feeling of rank and position. I also do a lot more skip level meetings to ensure that everyone understands I am available to them. Before the pandemic, executives would run into employees in the office or elevator, but now we don’t have that opportunity. Executives need to make a commitment to their people and make themselves available and approachable. Building relationships with your team is just as important as delighting your customer. Now more than ever, executives need to focus on empathetic leadership. Sometimes my team members will apologize for the noise from a child or pet in the background, but the truth is that kids are not invading work life - companies are invading home life. People are people, not titles or output. Companies and positions will come and go, but the relationships we foster along the way are what we hold on to. AC: How would you describe your management philosophy? ML: I have three objectives for my team. First, I want them to feel like they are learning by working with me. Second, I want them to know that they are part of a team of people they can call friends. Third, I want them to understand the “why” behind the products we’re building. For me personally, it was incredibly important to see the disadvantaged people who are using my app to flip cars for a living in remote cities where the unemployment rate is 33%. The goal of my work is to serve people and to improve their standard of living. I want my team to understand this. Before COVID, I even created an exchange program between offices so that people could learn new languages and cultures as well as new technologies. AC: What do you enjoy the most in your career? ML: I love developing solutions that really delight our customers. Technology is about serving people, so you have to ensure that the end result solves problems to take care of users. Technology itself is fascinating and fun, but it’s important not to lose sight of the people who build it and the people who use it. AC : How do you personally manage so many diverse projects at once? Do you have a methodology that helps you stay on track? ML : You have to compartmentalize yourself and separate activities so as not to lose sight of the big picture. One of my biggest challenges is keeping good notes from so many diverse meetings. I’ve experimented with note-taking on my laptop and iPad, but I always go back to pen and paper. Writing with a fountain pen allows me to remember my psychological frame of mind during any given meeting based on my handwriting. Technology solves many problems, but we should not forget about the fundamentals. AC: What has been a highlight for you in the last few years? Was there a moment, either for you personally or for eBay, that you knew you were on the right track? ML: There is always a little bit of tension between technology and product teams. Tech teams focus on the foundation, the infrastructure, and the functionality. Product teams focus on business initiatives and improving KPIs. In less healthy dynamics, there can be a conflict over whether you’re paying back technical debt or building new products. My highlight was when I wanted to use the technical debt capacity to build new features and help the product team meet the business goals, and the head of product thanked me but said we couldn’t build new products at the expense of technology. At that moment, I realized we were totally aligned. It was beautiful that we got to a point where we were advocating for each other. If we can see requests from other teams’ perspectives, organizations can fire on all cylinders to achieve more. Organizations should avoid in-fighting at all costs and strive instead for empathy. AC: Thank you for your time today, Ming! It’s inspiring to hear about the measures you’ve taken to care for your team during this time. We also look forward to hearing about how eBay will continue to evolve the classifieds marketplace!", "date": "2021-1-27"},
{"website": "Avenuecode", "title": "How to Improve the Relationship Between UX and Development Teams in an Agile Context - Part 1", "author": ["Alexandre Lemos"], "link": "https://blog.avenuecode.com/how-to-improve-the-relationship-between-ux-and-development-teams-in-an-agile-context-part-1", "abstract": "If you work with Agile development, you probably know how difficult it is to find the right project pace so that your software engineering team and UX design team can work together to deliver software increments. Before we discuss potential solutions for coordinating development and design teams, let's examine our problem on a granular level. First of all, it's important to remember that the development team is not a monolithic engine that delivers code; instead, it is usually composed of several groups, including front-end specialists, back-end specialists, quality specialists, and so on. It's already challenging enough to get each of these development teams, all of whom have common software engineering backgrounds, to coordinate. Frequent complaints abou t Waterfall gaps between development and QA exemplify this challenge well. The problem is naturally magnified when a UX design team is introduced into the equation. Just like software engineers, UX design teams have evolved into specialized groups. Today you can find research specialists, prototype specialists, information architecture specialists, and, of course, usability test experts. Depending on the complexity of your project, you may need to involve all or at least a few of these teams in your project. The problem I want to address here is the very different nature of software engineering development and UX design activity. Each team has its own distinct methods of planning and creating, its own sets of rules and best practices, and its own training backgrounds and values. Because of this, UX design activity doesn't fit well within the event framework of an Agile project. For instance, if we were to strictly follow a Scrum framework, a UX designer inside a development team would have to use a maximum of 10% of his time to refine a story so it would be ready for estimation in a planning meeting. UX activity, however, differs from development team activity because its responsibilities focus on grooming (tasks such as interviewing users, creating personas, drawing user journeys, writing user stories, etc.) These grooming tasks sometimes have more similarities with product owner responsibilities than development responsibilities. Given these differences, a problem might occur if, for instance, a UX designer was asked to estimate a given story during a sprint planning meeting. This request would be counter-intuitive given the exploratory nature of UX activity, which often requires in-depth analysis that could extend the developer's work for a period of time that's difficult to estimate. For this reason, the timing of some UX activities may have to precede the development sprint. In recent years, various approaches have been proposed to formalize this peculiar condition of UX activity within a Scrum framework in a way that's organic to UX design work. In part 2 of this post, I will present some of these approaches that could help establish a way to organically coordinate UX teams and development teams within an Agile development context.", "date": "2018-3-28"},
{"website": "Avenuecode", "title": "BDD is the Game Changer Your Project Needs", "author": ["Lucas Gomes da Silva"], "link": "https://blog.avenuecode.com/bdd-is-the-game-changer-your-project-needs", "abstract": "\"Is this project ready to be deployed?\" Every developer is familiar with this question, whether it's asked by a manager or a non-technical stakeholder. Here's how to prepare a good answer. The main goals of this Snippet are: Before we explain what BDD is, I will leave a question in the air for you (that we will try to answer later): before deploying your system into production, how do you show non-tech stakeholders that your system is tested (trusted) and ready to go (or not)? Perhaps you do one of the following: BDD stands for Behaviour Driven Development . It was created in 2003 by Dan North as an improved version of TDD - Test Driven Development. Briefly, TDD states that you should write your tests before you start implementing your code. This way, you won't miss any scenario and you'll focus your efforts on making those tests pass while implementing your code. The biggest flaw of TDD, in my opinion, is that although the concept is awesome, it doesn't give you a clear direction for what you should test and how much you should test. The concept is a little bit vague. That's where BDD comes in handy. BDD, as the name suggests, is about testing the behaviour. The main premise of BDD is that we use a ubiquitous language, which means a language that everybody understands, tech and non-tech people alike. Ok… enough chit chat. Let's see some code. I'll simply present the code in this section and then explain it below. Properties: Super class method: Test: Feature File: Step implementation example: Imagine a scenario where a global pandemic affects the global economy, and hence your project loses almost the entire tech team which was there since the beginning. Sounds crazy, right? Yeah I know, nobody is expecting or well prepared for sudden changes like that. At the time I joined the project, the scenario was: the project had two years of life and less than three months in prod (but it wasn't being fully used yet). It had three devs, myself included. One of these devs had joined six months prior, and the other had joined 2 months prior. There was also one tech lead, who had been there since the beginning, and only one BA/PDM, who had also joined the project very recently and had less context than the devs. Ok, so now that we have this context... what does BDD have to do with my project? The examples I gave you above were tweaked but were very close to what we faced when we joined the project and how we converted to BDD. Although there were a lot of acceptance tests written, they were practically unreadable by the non-tech stakeholders, and it was hard even for us to say what we had covered. As you can see in the test code without BDD section, we had a bunch of json files mapped in our properties, a generic framework to execute the tests based on those json files with some predefined steps, and a simple call to the framework passing the set files we wanted to run based on the configured properties. It was confusing and hard to read. After creating the feature files with all the scenarios we wanted to test, we implemented those steps and refactored the test code. So we not only had a more readable test, but we also made the values visible, and the steps could be reused independently to create a variety of different scenarios instead of a set of sequential steps like before. Although thoroughly adhering to BDD would mean that we should have written our tests first, this example shows that you can and should implement BDD in existing code as well, converting your tests to BDD and changing your code accordingly in case there are steps not fully implemented by your code. In the beginning of the article I raised the following question: Before deploying your system into production, how do you show non-tech stakeholders that your product/system is tested (trusted) and ready to go (or not)? My answer to this question is simply: BDD . Why? One of the most famous/used BDD frameworks is Cucumber, and it has support for a variety of languages, including: Java, Javascript, Android, Ruby, Go, C++, Kotlin, Lua, Scala, etc. BDD can make a big impact on your project, whether you use it in the beginning of the project or adopt it in the middle. It helps you build trust and confidence within your team and all stakeholders. It doesn't matter the nature of your project, whether it's just backend, front end, or both -- the bottom line is the same: use BDD. Although most teams use BDD in acceptance tests, it can be applied in all levels whenever a behavior exists. Have you found yourself or anyone you know in a similar situation to mine? Was BDD a game changer for your project too? Tell us about your experience with BDD in the comments below!", "date": "2021-3-16"},
{"website": "Avenuecode", "title": "AC Spotlight - Mohannad El-Barachi", "author": ["Anna Vander Wall"], "link": "https://blog.avenuecode.com/ac-spotlight-mohannad-el-barachi", "abstract": "Mohannad El-Barachi, Co-Founder of Wrk Technologies Inc., speaks about disrupting the workspace by changing the value chain and leveraging human talent alongside AI to achieve greater output as well as global socioeconomic benefits. Avenue Code: Tell us about your career journey. How did you get to where you are today? Mohannad El-Barachi: I’m 37 years old, and I feel like I’ve already lived two lifetimes! I was born in Egypt and moved to Canada at the age of 7. It was a typical immigrant success story, where I gradually worked my way up, graduating from Concordia University with a degree in Computer Science and Political Science. I tell everyone that the two things I love most are coding and arguing. After working as an IT Consultant at Concordia for a few years, I began consulting for several companies on my own. I was fortunate to take on an EVP role for a company that built technology solutions for the transportation industry. It soon became clear to me, however, that the owners of the company didn’t have the same level of ambition I did, so I transitioned into a few different roles and ended up moving to Dubai, where I built technology, sold commodities, and worked on financing deals. After the financial crisis of 2008, I moved back to Canada and worked for an HR startup before being recruited to New York City to work for a telecom provider called Via One that was disrupting the international roaming business. Tired of travelling constantly, I returned to Montréal and co-founded SweetIQ, a local digital marketing platform. We raised venture capital funding from VCs in Canada and the US, and in the span of 4 years of commercialization, we quickly achieved rapid international growth, taking the company to $10M in annual revenue. In 2017, we were approached by a multi-billion dollar publicly traded company that wanted to gain an edge in the digital-marketing space. They made us an offer we couldn’t say no to, so we decided to sell SweetIQ. I stayed on for a couple of years to turn it into a profit center for the new owners, then took a year off to spend time with my family. Now it’s time to start something new! AC: You recently announced the launch of your new company, Wrk.com. Tell us about the concept behind it. ME: The current buzzwords are AI and ML, but the reality is that their potential hasn't been realized, because we’re not at the point where humans fully trust machines yet. For example, Tesla’s autopilot function still requires drivers to have their hands on the wheel. The language we’re using today is that of a self-fulfilling prophecy where the machine will make the value of human talent obsolete. Quite frankly, this is a very bleak future that no one wants! But at the same time, we need machines to perform tasks for us. There are several companies that want to one day implement AI, but they don’t know how to get there. The idea behind Wrk.com is to help mid-market organizations automate their medium-complex, multi-process tasks by helping them describe tasks from beginning to end, break each step into microtasks, and then delegate the output of each microtask to a machine or a human aided by a machine. To start with, we will delegate that output to humans so that we have an Uber model of desktop jobs, creating a marketplace for individuals to complete medium-complex tasks from home while simultaneously generating data to train machines in the future. So while tasks traditionally performed by humans will eventually be performed by machines, we’re taking into account the fact that humans will also continuously evolve to perform increasingly higher-level tasks. What we’re trying to do is reinvent how people are thinking about work and the interplay between humans and machines. AC: Can you give us an example of these medium-complex tasks and explain how it’s possible for midmarket organizations to afford the customized solutions you’re describing? ME: A typical medium-complex task involves about five steps. For example, many organizations need to receive input from their clients, process the information, translate it, transcribe it, and generate a report. Each of these steps can be broken into a multitude of microsteps. What we do is help clients identify each microstep within a process and then delegate these steps to several people so that the output happens at a faster pace. Beyond the education aspect of Wrk.com - teaching people how to think about projects in terms of microtasks - we help companies convert their fixed cost to variable cost. If you have 100 full-time employees who are copywriting and you transition to delegating this output via microtask process to hundreds if not thousands of other individuals who are compensated for the successful delivery of each individual output, you’re improving your monetary investment. We are trying to change the value chain equation so that salary is predicated on deliverables, not number of hours. We’re developing an economic index to understand how to compensate someone based on the delivery of the output, taking into account the living conditions of their location. Work needs to be disrupted, and that starts by changing the value chain. AC: Do you have any competitors? ME: It’s difficult to describe because we could be considered as competitors to companies that have a point solution for scaling a process, especially in the AI space. But whereas our competitors take weeks and months to launch, our time to launch is a few hours. I don’t want to be limited by the time it takes to train machines; I want to address my clients’ needs immediately. From there, we can take our time to automate. When a client comes to me, I don’t actually give them any price reduction: they tell me a project costs $500, so I’ll use that $500 and give them the net benefit of moving from fixed to variable cost. Then, over the course of the first 12 months, we’ll achieve a cost reduction of 5-20%. We’re focused on delivering high-quality output at scale, not providing cheap labour. AC: Every company concept arises out of a real need or challenge that demands a solution. What challenges inspired the creation of Wrk.com? ME: When I worked at SweetIQ, we were trying to scale some of our internal operations and found it very challenging. I was certain we could accomplish our processes purely through machines, and I burned through several developers because I was very uncompromising about the quality of the output. After selling SweetIQ, I could have done a million things, but I want to do something that has a big socioeconomic impact. If we are successful in our mission of enabling people to earn a good living by creating a state of flow and job consistency, we’ll also allow them the opportunity to advance their careers by figuring out the next levels of tasks that can be paid out. The ambition of the platform is not to do menial tasks. It may start there, but why can’t we have doctors offering medical expertise or lawyers offering legal reviews? Why can’t we unite the collective human capability and put it somewhere that everyone can access it? AC: What is your target market? ME: The majority of companies trying to help clients transition to automation are only focused on the big players who have money and time to train machines, leaving the midmarket underserved. So midmarket is our current focus, but this will evolve. Our main differentiation from competitors is that we’re focusing not on menial tasks but on medium-complex tasks. Organizations’ needs are always unique and nuanced, so instead of providing one standard solution that customers adapt, we’ve created a flexible system that can accommodate their customized needs without having to build custom solutions every time. We want to build the most horizontal set of capabilities so that we’re not entrenched in a specific position. We’ve identified 25 key functionalities like data acquisition, data manipulation, data transitioning, etc., and we’re offering these in a customizable format where clients can connect them to accomplish any process. Our first five clients couldn’t be more diverse, but they’ve all been able to leverage our technology to accomplish their needs. Right now we’re working with a company in the AI space that’s doing computer vision for the sports industry to track movements in a game. While this is an easy concept to understand, the microsteps are highly complex. The output of one video is 350 megs of pure, raw JSON, so the amount of data and insight we can glean from multiple videos is incredible. Each individual micro-task is performed directly inside our platform which allows us to track anomalies in work performance and, in the future, use the accumulated data to train machines. AC: How do you personally stay abreast of innovations in tools and technologies? ME: I network constantly, and I have learned to be completely unashamed in asking for access to events and introductions, so I’ve ended up slowly meeting the movers and shakers of the industry. I’m out on the road traveling and meeting people everywhere. I attend and speak at multiple tech events, which means I see trends in innovation. For me, it’s all about connecting with people and being curious. I also have a subscription to Crunchbase to see what people are investing in and which new companies are in the field. AC: What is your creative process for utilizing these innovations effectively? ME: My favorite app is Turo, a car sharing network. I love going for long drives and spending a lot of time thinking. Then, I write my ideas down, which helps me crystallize my point of view. I’m also very blessed to work with teammates in UX and design who are now creative directors. We spend hours arguing about the most minute details with vigorous passion. When you have something like our platform that has a potentially massive socioeconomic impact, every minor decision is monumental, so ideas need to be challenged and refined. Any creative process starts by being curious, then deciding where you want to go down the rabbit hole, going as far as you possibly can, and surrounding yourself with people who are willing to argue with you. AC: What are the biggest lessons you’ve learned as an entrepreneur, and what advice would you give to emerging entrepreneurs? ME: My advice is to take everyone’s advice with a grain of salt, because everyone’s journey is different. Beyond that, create a defined purpose behind why you’re doing what you’re doing so that you can make clear decisions predicated on your north star. Finally, be aggressive when the occasion calls for it. AC: What would you say is your purpose for Wrk.com? ML: I’m a great admirer of Elon Musk, and a lot of his ideas resonate with me: the world can be pretty bleak and depressing, but if I have an opportunity to do something to contribute and make the world slightly better than it was, I’ll be satisfied. I’m fortunate enough to have made money in my last company, to have a family, and to have traveled a lot. Now, I need to do something that has a material impact for others. This is the purpose behind Wrk.com. AC: Thanks for your time today, Mohannad. It’s been a fascinating conversation, and we can’t wait to check in with you as Wrk.com grows and changes the global workspace!", "date": "2020-5-6"},
{"website": "Avenuecode", "title": "Java Microbenchmarks with JMH, Part 1", "author": ["Andre Brait"], "link": "https://blog.avenuecode.com/java-microbenchmarks-with-jmh-part-1", "abstract": "You have an application running. It's doing pretty well, except for the fact that it's somewhat slow. You have identified the bottleneck, and now you're trying to come up with the ideal solution. There are a myriad of mechanisms you could try, but you don't want to deploy each one of your attempts to the development environment and then check the performance there. And running the application locally might not be the ideal solution either. So, what do you do? You create a local benchmark for that small piece of code, of course! But there are problems with this approach—namely, the JVM and its optimizations might differ between your small local benchmark and the significantly larger application you have running in production, leading you to believe a solution is faster than it really will be when you deploy the application. How do we avoid this? Enter the Java Microbenchmark Harness, or JMH. While JMH itself is quite simple to use, the devil is in the details. For that reason, this will be a three-part series. In this first part, we'll start by discussing why you'll want to use the JMH. We'll also be touching on how to get started with it and how to implement a simple benchmark. This part is designed to serve as an introduction for our next two articles. Benchmarking has always been a source of debate since benchmarks might not correspond to real-world usage scenarios. Because of this, it can be easy to get results that you want rather than results that are accurate . Yet, benchmarking is a necessary practice in a number of areas. Evaluating the performance of a piece of code can be crucial to achieving the best possible performance in a critical application. Benchmarking appears to be a simple task. After all, it should consist of compiling your code, running it in a loop, and measuring how long that takes. The faster the better, right? That can be the case for statically compiled languages, such as C or C++, but for Java developers, there is an additional layer of complexity: the JVM. Stressing that specific piece of code and extracting meaningful metrics can be difficult in the JVM world, because the JVM is an adaptive virtual machine and can perform optimizations that can produce misleading — or even completely useless — results. Unlike with statically compiled languages, the actual machine code that is executed is determined by the JVM while it is running, and it can change the code depending on what happens during the execution. This allows for the above-mentioned optimizations, but it also means Java developers need to be extra careful when writing benchmarks! The need for specific precautions when writing benchmarks for Java culminated in the development of the JMH. That's a good question. According to an article published in Java Magazine, given the following code: ...manual benchmarks can be this bad: As we can see, we have three computational operations going on here: And the results say doing nothing is slower than returning a constant , which in turn is slower than operating on double precision floating-point numbers - including a square root operation that's quite expensive! Taking the first and last numbers, we see that doing nothing is more than 100x slower than doing something ! That is literally the opposite of what should be happening. JMH is a Java harness library for writing benchmarks on the JVM, and it was developed as part of the OpenJDK project. JMH provides a very solid foundation for writing and running benchmarks whose results are not erroneous since they are not affected by unwanted virtual machine optimizations. JMH itself does not prevent the pitfalls that were briefly mentioned earlier, but it greatly helps in mitigating them. (The third part of this series is dedicated to explaining practices that help you avoid such pitfalls, which is why it's arguably the most important part.) JMH is popular for writing microbenchmarks , that is, benchmarks that stress a very specific piece of code. JMH also excels at concurrent benchmarks. That being said, JMH is a general-purpose benchmarking harness, so it is useful for larger benchmarks, too. The recommended - and easiest - way to get started with JMH is to generate a JMH Maven project using the JMH Maven Archetype. You can do this by using your favorite IDE, but I'll present an IDE-agnostic method, which is, of course, the good old command line interface. Assuming you have Maven installed and in your PATH , run the following command: This will generate a new folder called first-benchmark, inside which you'll find a pom.xml file and a Maven source directory structure. This project will already have declared in it the correct dependencies to compile and run the benchmarks (namely the JMH libraries and annotation processor, as well as some Maven plugins to make things easier). Now it's time for us to write the actual benchmarks. In the generated source structure, you'll find the MyBenchmark class, which will look something like the following: The first thing we see here is the @Benchmark annotation. Every method annotated with @Benchmark is considered by JMH to be a benchmark to be executed. You can have any number of benchmark methods inside the benchmark class. Just be aware that the more benchmarks you add, the more time it'll take. In order to implement our own benchmarks, all we have to do is put our code inside one of the benchmark methods. Easy, right? The JMH Maven archetype includes all the necessary dependencies and plugins that make it easy to build and run our benchmarks. In order to build it, all we have to do is run: This will produce an executable jar for us in the targets directory. To run our benchmarks, just use the command below: In order to give you an example of what a somewhat decent implementation of a benchmark looks like, let's use a simple yet interesting performance test. There are a number of ways in which you can obtain the total sum of a list of integers while leveraging the power of multiple processing cores to speed up the process, even in scenarios where some level of synchronization is needed. But which one is the fastest? It's time to find out! First, let's enumerate a few ways one could obtain this total sum while keeping things thread-safe and taking visibility into account as well: Let's test this by writing a benchmark class like the one below: ...so, there's quite a number of new annotations here! Part 2 will be all about these new annotations, how JMH works, and the results of our benchmark.", "date": "2019-3-27"},
{"website": "Avenuecode", "title": "Example Mapping: Getting the Most from Cucumber", "author": ["Rebeca Benavides"], "link": "https://blog.avenuecode.com/example-mapping-a-way-back-to-cucumber-purpose", "abstract": "Within the Agile community, Cucumber has become a widely-used framework for test automation. It can be a great help when used for its primary purpose: namely, helping to build a ubiquitous language for your business and sharing reliable documentation easily understood by everyone from your Product Owner to your customer. That’s why you'll often hear users say that Cucumber is essentially a collaborative tool: to really get the most out of it, everyone on the team should share the responsibility for creating system documentation. Unfortunately, however, it's not unusual to find Agile teams using Cucumber solely to automate their tests after implementation is already complete, in a process similar to the following (thanks for this, Matt Wynne !): If this is how your team is using Cucumber, the unfortunate truth is that you're doing it wrong. Actually, you don't need Cucumber at all in this process. On the other hand, if you want to enjoy the plethora of benefits it can provide, you should start by using it at the very beginning of your iteration, creating your feature files prior to beginning development and - once again - involving the whole team in the process. If everyone participates in writing the feature files, everyone will subsequently be able to read and understand them. Not sure how to get started? One interesting and helpful technique is called Example Mapping . Essentially, Example Mapping suggests holding a quick meeting (no longer than 25 minutes) to discuss a story before development begins. The meeting can be held in a Three Amigos format to ensure that different opinions and perspectives are presented for discussion. For this meeting you should use colored sticky notes representing various elements at play. A yellow sticker represents the story under discussion and should have the story summary written on it. Blue sticky notes will each denote one rule or acceptance criteria for the story. Next come the green sticky notes, and with them, the purpose of this meeting. The meeting goal is to describe the rules by adding green stickies with real examples that represent them. It's not important to define the whole example in a given/when/then format - this can be left for later. Instead, the goal is to engage the team to think together about different scenarios and develop a common understanding of how the system should behave in each particular case. All that's necessary for this purpose is to write a descriptive title for the example and ensure each member of the team shares the same understanding of what this example means. When the group decides there are enough examples to represent the rule under discussion, move on to the next rule. During the meeting, questions will inevitably arise about how the system should behave in particular examples. If the team is not sure abut the answer, write the question on a red sticky note and move on. When the meeting time is up, the team can quickly vote to decide whether or not the story is ready to be developed. If it's not ready yet, schedule another meeting to finish the story mapping. This way, you'll avoid long and boring meetings in favor of short, productive meetings that stimulate team communication. As with any project undertaking, you may need to appoint one person to drive the meeting and keep the team focused if this becomes a challenge. Stay alert for red flags. If a rule has too many examples, for instance, this could suggest that it's too complex and should be split into separate rules. The same logic applies to the story - if you find that it has too many rules, consider breaking it down into multiples. And of course, if there are too many red stickers, it may mean thta the team has more to learn about the story before undertaking its development. After the meeting, the examples generated will naturally drive the story development and will evolve into your feature file scenarios. The whole team will share a common understanding of the story, and the automated tests will simply confirm that the system behaves as expected. In this model, Cucumber becomes far more useful as a collaborative tool for the entire team. Excited to give example mapping a try? If you have any questions, considerations or want to share your experience with example mapping or any other BDD techniques, please bring them up in the comments! References: https://cucumber.io/blog http://lisacrispin.com/2016/06/02/experiment-example-mapping/ http://blog.xebia.com/example-mapping-steering-the-conversation/ http://www.velocitypartners.net/blog/2014/02/11/the-3-amigos-in-agile-teams/ https://speakerdeck.com/mattwynne/rules-vs-examples-bddx-london-2014 https://speakerdeck.com/mattwynne http://blog.avenuecode.com/specification-by-example", "date": "2017-7-5"},
{"website": "Avenuecode", "title": "9 Ways To Avoid Pitfalls Using Node.js", "author": ["Wellington Soares"], "link": "https://blog.avenuecode.com/9-ways-to-avoid-pitfalls-using-nodejs", "abstract": "Node.js has been widely adopted over the last few years, even by such well-known giants as Netflix, IBM, Walmart, Intel and NASA. Benefits like a support plan and a vast collection of open-source libraries account for much of the growth in Node adoption in the enterprise, replacing typical enterprise solutions like Java and .Net. Node.js went from a common technology in the startup world to a mainstream development approach used by companies of all sizes. Node.js is easy to learn, especially if you already know Javascript. And maintaining it can be less tiresome than some other languages. However, when you are a developer with a strong background i n languages like Java, there are some pitfalls that you can face when using Node.js for projects more complex than a trivial \"hello world\", and if this happens you might run into trouble keeping a big project organized. To avoid this and save you some time (and money) I am going to share with you a list of features that will help you use the full power of NodeJS. Lack of Testing and Linting Node.js is an interpreted language, which is highly useful. The f ollowing checklist may be useful to improve your project: The require command is always \"require once\", after you import a file for the first time. Node uses a cache and will always return the same object. So, there's no need to create huge files with big functions: The \"globals\" variables must be avoided. Consider removing from your code the keyword global. For example, it is acceptable to use the proccess.env.NODE_ENV on the configuration file. However, the other modules should avoid accessing that. You should consider writing your business rules. When your application is starting, you can use methods like `fs.readDirSync`, `fs.readFileSync`, `fs.accessSync`, `fs.changeModSync`, `fs.chownSync`, `fs.closeSync` and `fs.existsSync`. However, when the application is ready, you can't call synchronous methods. When you execute a synchronous function, you will block the event loop. Try to avoid creating functions with a long list of arguments. Consider the following function: When you need to receive more than three or four parameters, you should consider using only a parameter called `options` with all values stored as a map. Many libraries write functions using optional arguments. However, when you write functions for your application with some business rules, it's a common necessity to evolve and make changes. So, you should avoid creating functions with optional arguments because it is hard to evolve with more arguments. Ignoring Error Statement If you ignore an Error Statement, it can lead to unexpected behavior: namely, always returning when calling callbacks. The following code has one problem because, if an error occurs, then the execution will not stop, but will continue. The Node.js provides a rich  API to handle I/O asynchronous operations. You should be careful when parsing huge JSON data, because the method `JSON.Parse`  is synchronous and can block the event loop too. In JavaScript, over-usage of callback functions often leads to complex chains of functions. It is hard to understand the flow of execution. Many times, code ends up looking like a lot of nested callbacks: Before you go further on advanced topics, you can resolve that by keeping your code shallow and still using the JavaScript callbacks and consider take a look at library called async . Or if you are starting a new project, consider using Node.js 7.6+ , a new version with default support to async/await . If we rewrite our code using that feature, the code will look like \"synchronous\", increasing its readability. The same code above is rewritten here using async/await: Keep in mind that you should wrap await in try/catch to capture and handle the errors in awaited promises. Besides that, you should also consider using the generator or promises . In summary, these are some of the most common \"anti-patterns\" to avoid, or at least be aware of when working in Node. My hope is that this helps someone who is starting on Node.js. If you are starting with software-as-a-service apps, consider reading The Twelve FactorApp . I also highly recommend checking out DevMedia’s DevCast: Introducing Node.js . In this video, you’ll learn about the purpose of Node.js, discover the advantages of its asynchronous, event-oriented programming model, and understand how it differs from object-oriented/multi-paradigm languages such as PHP, C #, Python, and others. Let us know what you think in the comments below!", "date": "2017-3-22"},
{"website": "Avenuecode", "title": "Why You Need a CaaS Strategy", "author": ["Prashant Palsokar"], "link": "https://blog.avenuecode.com/why-you-need-a-caas-strategy", "abstract": "Most modern e-Commerce platforms are omni-channel, and some of the challenges they encounter include: Functionally, communication as a service is the ability to decouple content - both for creation and storage - from the delivery mechanism. Technically, it is the ability to serve up that content that insulates content consumers from the internal storage mechanics, through the use of API. Let’s take a look at how CaaS can help some of the previously listed content challenges for us: Separating content from delivery channel: Separation of concerns paradigm. For example, content that comprises a “page” when delivered on a mobile app may instead be rendered as a portion of one when delivered on a desktop application. Yet, in the repository the content is the same. Typically, content is stored as “chunks” instead of pages. API-centric: Content is exposed as resources, through REST API, in structured formats (typically as JSON or XML files). Visualization for content creators: Since the content can be repackaged for different channels and formats, it's important that content creators can easily preview the use cases. Obviously, this can be very challenging to implement. Typically, these tenets are implemented by headless CMSes, that may also be de-coupled in their architecture. Some of the legacy CMS vendors also provide plugins that expose content as API's (see Joomla and others). Some CMS systems that have been built to implement these principles include DOTCMS , Contentful , and Cloud CMS . In theory this all sounds good. But as with any tool, there are several caveats that may make traditional integrated CMSes that serve up decorated content a better choice. CaaS may not be right for you if: In short, there are probably more pros than cons to following this strategy, as long as your organization can afford it.", "date": "2017-3-15"},
{"website": "Avenuecode", "title": "Building Bridges Instead of Products", "author": ["Jimmy Mayal"], "link": "https://blog.avenuecode.com/building-bridges-instead-of-products", "abstract": "What if I told you nobody actually buys products. Weird, right? Maybe, but let's delve into the why. Consciously or not, what people buy is value. Value, on a more abstract level, brings us back to happiness, status, sensations, and perceptual experiences in general. This is what people seek when they want to acquire something. Last week, during a lunch break, I asked some colleagues why they were willing to spend a significantly higher amount of money on purchasing the iPhone, which has an inflated price here in Brazil. The answers varied, but essentially, they all pivoted around the increased satisfaction of carrying that particular device over other similar devices. Try asking the owner of a BMW what attracts them to their car and you'll hear things about the engine, the comfort, or the design, but know that what really attracts them is the sensation they get from driving and the excitement they get when when the full force of the engine makes them stick to their seat. You can also ask the same question to owners of Harley motorcycles or Vanquish II suits and will probably elicit a similar response centered upon the intangible value the product brings to the owner. Users of Spotify and Deezer, for example, aren't just buying a music app, they're buying inspiration, fun car trips, their favorite throwback songs, and souvenirs, etc. This is what I call bridges: the connection that brands make to reach their customers. I always say that the product is just a bridge - a bridge that connects you to your client or a bridge that transports the value you generate. Reflecting on my colleagues' replies regarding the iPhone, I realized consumers are tired of seeing competitors create copies of well-performing products because they don't carry the desired value. In other words, no matter how many bridges, or products, competitors have created, some bridges such as the Apple bridge (iPhone) will always be worth more - at least until another brand manages to create an even more powerful bridge that can bring more perceived value to consumers. But is that all? No. When I talk about creating bridges, I do not mean purely \"create bridges\". In order to build a successful bridge you have to observe the world, people, places, and behaviors - good product creators do this all the time. With these observations in hand, next try to understand what the construction of your bridge will be like: Is the valley that will be covered too large? (Market unknown) Will this bridge really be able to carry all the proposed value? (Misleading product) Is the bridge sturdy? (Good product) They key is to always be thinking, studying, designing, building, and improving. Also, never forget that a product is no more than a means to an end. I hope I was able to provide some insight, and I'd love to hear your feedback in the comments below. In order to deliver good products, you must build bridges and deliver value.", "date": "2018-1-24"},
{"website": "Avenuecode", "title": "How to Structure Visual Regression Testing Based On Business Needs", "author": ["Renata Andrade"], "link": "https://blog.avenuecode.com/how-to-structure-visual-regression-testing-based-on-business-needs", "abstract": "In my previous article, I discussed the basic concepts of Visual Regression Testing . Now, let's delve deeper into defining a visual testing architecture based on a business model. Since technology is constantly changing, architectural decisions require a lot of careful consideration. We need to think about how sustainable, maintainable, and scalable the source structure is in order to make a tailored decision. In this article, we're skipping the technology decision and focusing on the visual testing architecture decision. We'll take a look at two case studies, go through the design thinking process, and end up with a solution for each case. The first step is to analyze the context of the project and see which visual testing architecture fits best. Start by i nterviewing your PO, or whoever best knows the visual regression testing needs of the project. Here are a few questions that should help you gather information. These questions can serve as a base, but make sure to also include any questions that are specific to your project. With those questions answered, we're ready to move forward! Now that we're familiar with the business, we can start thinking about the visual testing architecture. For this, we also need to have some questions answered: These are some of the questions you can ask yourself to help figure out the best visual testing architecture for your business needs. The next step would be to analyze the information acquired and come up with a solution. At this point, you'll be able to define: Now, let's get to some real cases and put the steps together. Here are two case studies we're going to use as examples. Take some time to go through them in order to gain a better understanding of the information gathered from steps 1 and 2, as well as the proposed solutions in step 3. Here's the URL for the first case study: https://www.avenuecode.com/ Information gathered: By analyzing the business, we can conclude that there's no big need to conduct visual regression tests because there aren't a lot of CSS changes. However, if the company allows for investment, we can run a check for the most important components such as headers, menus, footers, contact page, and careers page for a desktop and mobile browser. The visual regression test source code will be different from the functional tests. Check out the second case study for more details. Here are the URLs for the second case study: https://www.fansedge.com/ and https://www.collegebasketballstore.com/ Information gathered: By analyzing the business side, we're able to see a high need for visual regression tests because there are large amounts of CSS changes, some of them specific to only one or two brands. One of the challenges is working with dynamic areas, so we need to have a large coverage with regards to visual regression testing - mainly component oriented, since we reuse components throughout many pages. We also need to be able to work with different browsers and resolutions, such as for tablets and mobile devices. T he number of different websites is projected to increase approximately every quarter of the year as well. For both cases, we decided on decoupling the visual regression testing from the functional testing for the following reasons: To sum up, we went through the steps proposed to build a good architecture solution by identifying the business needs and brainstorming the architecture. Now that we understand the needs, it's time to build some examples and evaluate the results for your particular situation. In the next article, we'll implement the topics we just discussed in a visual regression test project for these two use cases. I'd love to hear back from you! Let me know if you found this article to be helpful, and feel free to send over any questions you might have.", "date": "2017-11-7"},
{"website": "Avenuecode", "title": "Directives to the Point: Manipulating the DOM with Angular", "author": ["Mauricio Correa"], "link": "https://blog.avenuecode.com/how-to-customize-your-angular-app-using-directives", "abstract": "So, you've read the first article (if you haven't, click here http://blog.avenuecode.com/angular-2-should-i-learn-it ),and you’re now wondering what the next step is. You've created your own classes, fiddled with Angular components, and learned how to fetch data using services. Now, you’re in  need of something fancier to help you add a little more spice to the mixture. Well, you came to the right place. Modern Web Development is now focused on the creation of Components and employing them like reusable Lego blocks. In order to easily create these blocks, we'll be using the well-known Angular-CLI which is new to the official Angular project ( https://github.com/angular/angular-cli ). This way, you'll be able to scaffold useful snippets of magical Angular code without having to deal with syntax specificities for the time being. Let's start with the so called \"blueprints\" you can use with Angular-CLI. Scaffold Usage Component ng g component my-new-component Directive ng g directive my-new-directive Pipe ng g pipe my-new-pipe Service ng g service my-new-service Class ng g class my-new-class Guard ng g guard my-new-guard Interface ng g interface my-new-interface Enum ng g enum my-new-enum Module ng g module my-module As you might remember from the previous article, you'll be using the Component, Service, Directive, and Pipe blueprints a lot. You've learned how to use components to create interactions using the Input Output flow and Event Emitters , but in order to further understand what Components are truly made of, you need to learn about Directives as well. Directives Directives are multi-purpose classes that are meant to interact with the visual part of your application, namely what the user sees in front of him. Back in Angular JS 1.x, we were taught that directives were meant to be the only way one should \"Touch the DOM the Angular Way\". That was because all versions of Angular have specific lifecycle hooks, and trying to mess with elements using plain javascript or JQuery alongside Angular used to result in unstable code. In the latest version of Angular, things have gotten even more organized with regards to Directives. There are three types of directives: Source - https://angular.io/guide/attribute-directives . This means  your building blocks are actually @Component decorated directives with a template. Now it's time to create your first directive using Angular-CLI: Let's say you want to create a directive that changes the text of a given DOM element to a famous politician's quote. ng g d Quote Note that directives have a 'selector' attribute. It’s the CSS selector for that directive, which is the way Angular locates all elements with that selector in a given template. In the example above, Quote will be the Directive's name, which will lead to a similar CSS selector. You don’t need to translate the word 'Directive' to 'Quote', because the CLI will take care of that for you. This will create the following snippet: You won't need to use the snake-case selector name for Angular, and use of camelCase is actually encouraged. Let's start with Attribute Directives If you write the selector name with brackets \"[appQuote]\", the directive should be used as an attribute, otherwise it will be treated as a new HTML element altogether, which will wrap the directive's template. In this case, we want to use the existing HTML tag \"<pre>\" and then add the CSS selector appQuote from the directive as an attribute because we want to augment an existing element by adding extra content to it: Now we also need to insert the directive Inputs into our recently created directive. You can create them either by: Don't forget to import the Input symbol as well, in order to use @Input. Also, add them to the <pre> tag in the HTML file so we can input information into the recently created QuoteDirective. If you surround the input variables with brackets (bind values instead of strings), you'll be able to input typescript expressions into the Directive as well. And inside app-component.ts create the \"getPoliticianName\" method inside the ts class: But neither approach will yield anything on the browser, so we need to actually manipulate the DOM using the element reference . Back to quote-directive.ts, add the element reference to the class's constructor using Angular's ElementRef: Now, in order for things to work, you need to: The OnInit lifecycle hook needs to be implemented because it's executed right after the directive's data-bound properties, which have been checked, and thus, are available to the Directive. Save it, serve it, and you should see something like this on your browser: But now, you want to actually modify the DOM by adding/removing/manipulating elements and customizing them as data flows through your class, it's time to create your first: Structural Directive Start by creating it the same way you created the Quote directive: ng g d Names Structural directives will impact their host element and descendants, so they should be preceded by the \" * \" (asterisk) symbol such as *ngFor and *ngIf , which are also Structural Directives. The asterisk is necessary because it wraps the host element with an <ng-template> element. The expression passed to the asterisk preceded attribute will be bound to the outer <ng-template> element as a property binding. Source - https://angular.io/guide/structural-directives What that means is, whatever that template renders will be related to the variable passed to the structural directive, which might add /remove/manipulate the host element and its descendants. In the *ngIf built-in structural directive, the expression passed to it will determine whether or not  the element should be rendered on the DOM. But we've just created our own Structural Directive which is going to behave similarly to the *ngIf directive. However, instead of inputting a boolean expression into the directive, we'll be inputting a very specific string. First, we need to create an input field to bind different user provided values to our directive. Don't forget to add the FormsModule to the App Module. Then, we need to create the secretSentence variable in our AppComponent. Next, the Structural Directive must be applied to a given HTML host element. Don't forget the asterisk preceding the directive selector. Now, two classes and an interface need to be imported and injected into our Directive in order to call it Structural: Then, we need to add the appNames @Input It's necessary to make it a setter by adding the 'set' keyword before the directive property name, since we'll be using the appNames property value to perform actions inside the structural directive's set method. What this snippet now does is: -> Check if secretSentence equates to 'show presidents please' If so, and the template reference is also present, then the template should be created. The viewContainer instance should then render both the host element and its descendants using the templateRef. -> If the sentence provided is not 'show presidents please' , then the viewContainer should be cleared, and no trace of that host element should be in the DOM. Therefore, if the user types 'show presidents please' into that field, something like this should show up: And the corresponding generated HTML should be something like this: If the secretSentence was different from 'show presidents please' , then only the commented section would be displayed and no DOM elements would be rendered. And last, but not least. Components . Now that you know about the two other types of directives, it's much easier to understand how components work, because Components  are a subset of directives with their own set of methods. They're also called Directives with a template , since you're going to build your own HTML element with its own template, functions, properties, and lifecycle. As you've done in the previous article, create your component by using Angular CLI and type the following command: ng g c States This will create the component and add its reference to the App Module. The next part you need to learn about Directives and Components, is how the Lifecycle works. The Angular team has created lifecycle hooks for different moments on a Directive's timeline. You can check them all out in the official documentation right here: https://angular.io/guide/lifecycle-hooks#component-lifecycle-hooks-overview I'll be covering three of the most important Lifecycle Hooks. After creating the States component, a stub for the OnInit interface will be created. This will hook us to the ngOnInit method. That Lifecycle Hook is triggered right after Angular displays the Directive's data-bound properties and sets its input properties to the local Directive class properties. This method is useful when passing @Input values to that Directive/Component, since we won't have those, once the class gets instantiated and the constructor is executed. Those will only be available at the OnInit hook in the lifecycle timeline. Which means if we try to console.log some @Input property inside the constructor function, the variable will be undefined. Typescript code: HTML template on App Component: HTML template on States Component: Console log: states.component.ts:13 Variable is: undefined inside constructor states.component.ts:17 Variable value: This is the President's Speech inside ngOnInit The next lifecycle hook you should know about is OnChanges . It gets triggered whenever data-bound input property changes. Surprisingly enough, it gets triggered before the ngOnInit hook and provides a SimpleChanges object with both the current and previous value of that data-bound input property. This is useful when a parent component sends different types of data that need to be parsed prior to being interpolated into the template. Let's say we want to send some data to StatesComponent using @Input, but this information can either be sent as a string or as an object. In this case the rollState function is going to be triggered by a button on the AppComponent template. This function will return either 'West Virginia' ( string ) or {stateName: 'California'} ( object ) Which means the selectedState property will be assigned with either of the above values and sent to StatesComponent as an @Input property. Template: app.component.html Template: states.component.html TypeScript class: states.component.ts In the ngOnChanges hook described in the snippet above, all changes to data-bound input properties will be verified and applied to the variable change: SimpleChanges. Now whenever the rollState function gets executed by the parent component, StatesComponent will check its current value andverify whether the selectedState property is an object or a string. If it is in fact an object and has keys, the variable will then be re-assigned in order to display the actual value of that stateName, otherwise, the interpolated value on the markup would be [object Object] After clicking that button, you should have something like this: If the information sent is an object Or like this If the information sent was a string, and only because we've explored the lifecycle hook before something like this Could have happened. Speaking of getting rid of the unnecessary, it's time to talk about the ngOnDestroy lifecycle hook. Back in Angular 1.x, there was talk about memory leaks and performance issues, which have been due to the way the garbage collector in the browser worked. Instead of purging a given resource after using it, developers usually relied on the garbage collector to get rid of unused references in the memory, but since it might not really work exactly how we expect it to work, and those references might still be in the memory, performance might drop. That's why the ngOnDestroy lifecycle hook has been created. It's called whenever a directive, pipe, or service is destroyed. This way custom cleanups can be executed in order to properly remove any trace of unwanted data. Assume we want to monitor changes on the selectedState and keep track of those states. We need to subscribe to those changes using rxJs Observables. Here’s another button that controls if the StatesDirective should be created, using the built-in structural directive *ngIf : And the app-states directive will then be rendered depending on the showStates property value: Whenever that variable is set to false, the DOM element gets removed from the node tree and the ngOnDestroy lifecycle hook is called. So in order to subscribe to the variable changes, we need to create a Subject and an Observable . This subject will push (next function) data whenever data changes through the ngOnChanges hook: And the stateObservable is subscribed to at the ngOnInit hook: Now, if we remove the app-states component from the DOM using the *ngIf triggered by the button in the parent component, the ngOnDestroy hook will be triggered. This is also the right time for us to unsubscribe from that property since the component will be hidden/removed/thrown away. Unsubscribing from previously created Observables will allow the garbage collector work more easily  by actively freeing up memory. Conclusion Creating Directives is the best way to customize your Angular application and deal with the DOM. If the JS libraries you've always used don't have an Angular port, it's time to create your own. Devising your own building blocks is the best way to understand Angular's lifecycle hook. I hope this article helped you understand a little bit more about directives. But if you feel like you need more information, be sure to follow the links below and download the Github Repo with everything that has been covered in this article. Github Repo: https://github.com/mauriciotasca/angular-demo-part2 Useful Links: The Power of Structural Directives in Angular https://netbasal.com/the-power-of-structural-directives-in-angular-bfe4d8c44fb1 When to Unsubscribe in Angular https://netbasal.com/when-to-unsubscribe-in-angular-d61c6b21bad3 Rangle.io Angular 2 Training https://angular-2-training-book.rangle.io/handout/directives/", "date": "2017-11-23"},
{"website": "Avenuecode", "title": "Java Microbenchmarks with JMH, Part 3", "author": ["Andre Brait"], "link": "https://blog.avenuecode.com/java-microbenchmarks-with-jmh-part-3", "abstract": "In Part 1 of this series, we covered the basics of creating a JMH project and getting it to run, proposed a small benchmark, and showed what the implementation would look like. In Part 2 , we looked at how to configure JMH, how it works, and why certain parameters are necessary. In this last part, we'll focus on how not to write a benchmark for JMH and why some precautions are necessary. This may be the most important part of the three, as it'll give you insights into some quirks of the JVM. We know how to write benchmarks. We know why the JMH does certain things to prevent inaccuracies in the benchmarks. But there are still a plethora of pitfalls and quirks that can get us. One common pitfall is the set of optimizations the JVM can end up applying to our benchmarks that it would not apply in our original application, making our code appear to be faster than it really is. There are a number of publications addressing these issues, but I'll try to summarize some of the most frequent pitfalls that can make our benchmarks produce inaccurate results. You might be tempted to put your code inside a loop so that it will be executed more times per benchmark iteration and hence reduce the overhead of the benchmark method call. Don't . Only ever put loops in if they're part of the code you want to benchmark, that is, the loop is in the code and not around it. The JVM is quite good at optimizing loops, so it may produce a different result if you wrap your code in a loop, and you may end up with a test result that does not correspond to the code running outside of a loop. Another optimization the JVM commonly uses is getting rid of dead code. Dead code is - as the name implies - code that does nothing or is never used. The JVM can detect such pieces of code, and it'll happily remove them for you. In the example below, the JVM will detect that the calculation a + b , which is assigned to sum, is never used, and it will remove the a + b operation from the method. Since a and b are also never used, they too can be removed. Because of this, you'll end up with a benchmark that is measuring how fast Java can do - literally - nothing . To avoid dead code elimination, a couple of things can be done: We'll look at some examples of both approaches in the next subsections. Return Value from Benchmark Method The point here is to trick the JVM into thinking the code is being used by returning the result of the calculation from the benchmark method. This way, the JVM cannot just eliminate the addition, because the return value might be used by the caller. JMH will take care of tricking the JVM into thinking the value is going to be used. The change itself is trivial, as shown below: However, it does not solve the problem of having dead code inside the method, say intermediate values or other calculations, that won't be returned. That code would still be eliminated. For such cases, JMH provides what it calls the Blackhole. Passing Value to a Blackhole In order to avoid returned combined values, JMH includes a Blackhole class that contains a consume method. This method can be used to trick the JVM into thinking code is not dead by passing the intermediate and non-returned values to it. Notice two things here: the method now takes a Blackhole as an argument, and the blackhole object is used to consume the result of the addition operation. This will trick the JVM into thinking the sum variable is being used, and it'll avoid eliminating it as dead code. If your benchmark method produces multiple results, you can pass each of these results to a black hole, meaning calling consume on the Blackhole instance for each value. The JVM is quite smart. So smart that even with the Blackhole or returning a value, our benchmark would still produce inaccurate results because both a and b are constants. A calculation that is based on constants will always produce the exact same result, regardless of how many times the calculation is performed. The JVM may detect that and replace the calculation with the result of the calculation. This process is an optimization known as constant folding. Our benchmark could become: It might go even further and replace the variable sum with return 3 since it's able to detect the method always returns 3. It can even replace every external call to the method with a literal, since it already knows the method always returns 3. Avoiding Constant Folding To avoid constant folding, you must not hardcode constants into your benchmark methods. Instead, the input to your calculations should come from a state object. This makes it harder for the JVM to see that the calculations are based on constant values. Remember, if your benchmark method calculates multiple values, you can pass them through a black hole instead of returning them, avoiding the dead code elimination optimization. Whew! That's it, guys. Thanks for coming along! I hope this series provided you with what it takes to write those benchmarks, as well as some food for thought about how the JVM works.", "date": "2019-4-10"},
{"website": "Avenuecode", "title": "6 Reasons Products Fail (and How to Make Sure Yours Succeeds)", "author": ["Douglas Lopes"], "link": "https://blog.avenuecode.com/3-reasons-products-fail", "abstract": "Product creation is a very demanding process, and oftentimes, it ends in failure. Why is that? Today we'll examine real cases to analyze why products fail, and then we'll highlight best practices to help your product succeed. Even for highly successful companies like Netflix, Facebook, Amazon, and Apple, building products is a huge challenge. Why? Building a product is not only about identifying what the market needs (understanding user pain points), but it's also about structuring the product creation environment successfully behind the scenes in order to fully comprehend the problem and get a handle on building the right solution, or at least the right MVP. In this article, I'll highlight my takeaways after reading Marty Cagan's incredible book titled Inspired: How to Create Tech Products Customers Love . Cagan is a well-known product expert who worked at HP and has written and helped giant Silicon Valley organizations build the right teams to create the right products many of us use on a daily basis. The first thing I noticed as I read is that problems arise when companies are too focused on old-fashioned ways of managing teams, problems, or the solution itself. Don't want your product creation to go wrong? Here are six \"don'ts\" of product creation: 1. Don't Focus Too Much on the Solution 2. Don't Use Modern Techniques the Wrong Way 3. Don't Be Afraid to Use New Methodologies to Discover Products 4. Don't Make the Product Manager Exclusively Responsible It's never good if the product manager is the only one responsible for finding the problem, identifying the solutions, and measuring the results, making them the only team member who is truly passionate about the product. One of the secrets to creating a great product that Cagan highlights is evangelism: have missionaries (not mercenaries) involved in a product team to co-create a goal and co-create a path to achieve that goal, never forgetting that a product is related to several areas in the company, such as: marketing, business strategy, and so on. In just a moment, we'll take a look at a scenario in which this was done very well. But first... 5. Don't Be Afraid of Prototyping Another important point to emphasize is not to be afraid of prototyping. In fact, it's ok if you prototype a lot - that's why it exists. So once everyone in the team has had an opportunity to give enough input, go ahead and build an MPV, which is nothing more than a living prototype for validating assumptions. We must not be afraid to undergo a thorough research and discovery phase, because a lot of knowledge must be generated in the creation phase. 6. Don't Neglect to Measure Results Last but not least, define how you will measure results. Don't be afraid to pivot your ideas when results indicate that you may be headed in the wrong direction. Data is everything, and it's vital to validate each decision. Many times, data will show us a clear result that we choose not to believe. Never think that your team is the only source of truth regarding the product under development, because every day is a new day, and the market changes. COVID-19 was a big-picture example of this ever-changing market landscape. One good example of product creation done well is Google Ads, which generates more than $60B in revenue. I am going to give you a brief background on how this product was created, and if you're curious, you can research it further on your own. Among several challenges regarding Google AdWords, one major challenge was to design it in such a way that it would not generate user confusion or frustration. Imagine searching for something and seeing that the first several results were advertisements, somewhere below which would be the results you actually hoped your search would generate. Jane Manning, the Product Manager at the time, took care of this potential problem by building relationships, listening to everyone's concerns,  finding a solution, and asking a creative question: What if we put aside the results of the ads?! Image courtesy of Methods & Tools. Helping Software Development with a Product Discovery Phase, accessed 12.27.2020 If I could summarize everything Cagan presented in his book, and if I could emphasize one idea, it would be this: Some of the principles we're discussing are not new or mysterious, but applying them correctly is the secret, and that application will depend on the unique scenario of each product, company, and team. But regardless of your scenario, here is one consistent truth to keep in mind: building a great product depends on thoroughly understanding the problem and creating a well-structured product vision based on business goals, market goals, and market fit. With this in mind, evangelize your team on the product vision, thoroughly define each role in building the solution, and use your leadership skills to guide your team while emphasizing the worth of the product. (Please note, however, that if you end up spending too much time trying to convey the product's worth, you may need to think more about the product vision.) Make everything as clear as possible, including each role, contribution, and idea. Finally, rely on teamwork. Do you have another real business example or a tip on building a product? While the principles above apply widely, there are a lot of particularities for each product, company, and market that go into creating a powerfully successful product, so tell us about your own experience in the comments below!", "date": "2021-2-11"},
{"website": "Avenuecode", "title": "AC Spotlight - Justin Sellman", "author": ["Holly Camponez"], "link": "https://blog.avenuecode.com/ac-spotlight-justin-sellman", "abstract": "(The following article is property of Avenue Code, LLC, and was originally published with permission at TotalRetail on March 2, 2018 .) Justin Sellman is the vice president, e-commerce and digital strategy at GHURKA , a luxury leather goods retailer. We spoke with him to get his perspective on how luxury retail has adapted to e-commerce and where he sees retail headed. Avenue Code: What changes have you seen in consumer behavior in the last 5 to 10 years, and how has that impacted your strategy at GHURKA? Justin Sellman: I think two major things have been happening in retail. First, a switch to e-commerce. Consumers are much more accustomed to and comfortable with buying online. In 2007, I started a company, and by 2008, it was a direct consumer brand. In the following years, Warby Parker and Bonobos were launched, which is when you started to see brands conducting business primarily, if not entirely, online. Of course, Amazon . com had been around for awhile, but 15 years ago it was just selling books. This is a big shift in consumer behavior because these days, if you’re not transacting online, you’re not in retail. The second major change has been the emphasis on mobile. This changes how consumers shop and browse, as well as technology. In 2007, retailers weren’t creating websites that look like what you expect them to look like now. Yahoo had a commerce site, but there was nothing like Shopify. So when Shopify came out in 2010, it really changed the perspective of e-commerce brands because it was so slick. Ten years ago it was extremely difficult to create a visually appealing website and almost impossible to translate that onto a phone. A shift to mobile has really shifted the technology plays, and people are aiming for a mobile-first approach now. AC: What shifts have you noticed specifically in luxury retail, and how have you adapted to those changes with regards to digital strategy? JS: Luxury was one of the last verticals to really embrace digital. Gucci and similar companies had a lot of hesitancy to enter the digital realm, fearing that they wouldn’t be able to do their brand justice, or that consumers wouldn’t transact as they do in-store. Today, I think Gucci is actually leading the way for luxury brands online. It recently did a site redesign, has a huge e-commerce team, and is reaping the benefits of this digital revolution. At GHURKA, we really understand this. Digital first is a great way to connect with people who don’t have access to a wholesale or retail location. A lot of our heritage customers have really enjoyed the accessibility of GHURKA’s online presence. We’ve seen people who will come back to GHURKA after not buying something for 15 years, 20 years. Let’s say they have a GHURKA bag and the strap breaks and it’s time to buy a new bag — the fact that they can now go to our website and buy a new bag without having to go to a store to check if the item is in stock is great. For us, and for luxury in general, it gives people access to brands that they didn’t previously have access to. For example, Springfield, Missouri is a rather affluent area, but it might not be in proximity to a Gucci or Louis Vuitton store. With e-commerce, clientele in Springfield now have access to these brands, which opens up a whole new demographic for luxury brands. The GHURKA collection. Image courtesy of GHURKA. AC: What would you say is the key to success in today’s retail environment? JS: I think there are a couple keys. The first would be piquing potential customers’ interest. Referring back to previous questions, the one thing that’s very apparent is that with mobile, consumer attention span is going out the window. Consumers are able to see so much in such a small amount of time that retailers now really have to have something that catches the consumer’s eye. In the case of Instagram, it’s all about scrolling. The first thing on our brand site was that we needed to put something in our scroll feed that would catch the consumer’s eye. There’s not much that hasn’t already been done, so brands are probably not inventing new types of clothing, but if a brand can do something buzzworthy from an innovation standpoint, then that’s essentially free PR. The second would be once you have somebody’s interest and you've converted them, you need to keep them engaged. That’s where technology comes into play with targeted ads and successful email marketing programs. If you don’t have innovative content, then there’s too much distraction keeping consumers from coming back to your brand. Finally, the last key point is that after a prospect becomes a customer, it’s really important to ensure they remember you. Once someone chooses to make a purchase with your brand, you need to go above and beyond and pay attention to the details, right down to the packaging. AC: What are some of the biggest challenges your team faces in implementing strategy decisions? JS: We’re relatively small and we don’t have a large in-house technology team, so implementing a lot of new technologies and keeping up with the pace of change is challenging. It’s incredibly difficult to take things that Amazon might be doing, such as a voice-activated ordering through Alexa, because we don’t currently have the bandwidth to bring that into our business. AC: You’ve had such a great and expansive career in retail with experience in the broader fashion category. What has it been like adapting to a specialized, niche market with GHURKA? JS: One of the most exciting things about transitioning to GHURKA is that with a niche product, you get a very loyal following. Companies that are conducting business on a broad spectrum of products have all sorts of customers with varying levels of loyalty to the brand. The group of people who choose to buy GHURKA love us and love everything about the company. We’ve implemented a program where we’re able to capture user-generated content so that individuals can build their own mini web pages, and we can use that content on our site as well. We’ve always had a very healthy following of fans of the brand, but now we can allow them to tell their story about what’s important to them. We’ve seen a lot of engagement from that, and even some moving stories about individuals connecting offline who wouldn’t have met if not for their shared love of GHURKA. AC: What technologies do you think will most fundamentally change your business in the future? JS: It’s not going to be too long before virtual/artificial reality will be able to mimic an in-store experience in a digital environment. Also, it won’t be long before I tell my kids that I used to shop for clothes on a 2-D screen. There are many large companies moving this way, including Lowe's and North Face, and it’s going to be the next big innovation. Basically, you’ll be able to mimic a real person and a real physical store, which will in turn be able to provide a real product. AC: Roughly 8 in 10 Americans are now shopping online, according to a recent study from Pew Research . That’s up from just 22 percent in 2000. Over half (51 percent) have also bought something from their mobile phone. With customers engaging with brands in new ways and on multiple devices, how can companies attract and convert customers through multiple channels? JS: It’s important to put yourself in the customer’s shoes and understand how to communicate with them on each device. You need to be able to market to customers in different ways depending on what device they’re using. For example, on a phone, customers are typically in a more exploratory mode, so you need to find something to pique their interest. If they’re in a shopping mode, then you need to be able to convert very easily. A one-step checkout process is really important because it’s so much easier, which is why there are so many people purchasing from Amazon. Another aspect of mobile is ensuring that when customers are in an exploratory phase, they have an easy way to save or bookmark their favorite items. To sum it up, maintaining awareness of the different stages of a customer’s journey is critical. AC: What’s next for GHURKA? JS: We’d like to get smarter with segmentation; we want to provide a personalized experience for our customers. Many brands are struggling with this challenge in the face of large amounts of data to aggregate and digital events to coordinate. There are a lot of really solid companies doing great work in personalization now, so we plan to partner with them in order to provide better segmentation in 2018.", "date": "2018-12-5"},
{"website": "Avenuecode", "title": "Understanding the JavaScript Concurrency Model", "author": ["Rafael Bicalho"], "link": "https://blog.avenuecode.com/understanding-the-javascript-concurrency-model", "abstract": "You have probably heard of terms like “V8,” “event loop,” “call stack,” and “callback queue.” In this post, we are going to examine what these terms mean, how they relate to one another, and, more generally, how JavaScript works. Javascript is a powerful language that has stood the test of time and continues to gain popularity. By cultivating a strong understanding of its core concepts, you'll be able to write better code and better apps. Let's start by defining V8. V8 is a popular open source JS Engine. It is the engine inside Chrome that runs the JavaScript code. But when running a JavaScript in the browser, there's more involved than just the JS Engine. Most JavaScript code makes use of web APIs like DOM, AJAX, and Timeout (setTimeout). It turns out, however, that none of these APIs are provided by the engine; instead, they're provided by the browser. In the picture below we can see an overview of the elements involved in the JS execution in the browser. The V8 Engine contains a call stack. The engine communicates with the web APIs, and there is also a queue and a mysterious event loop. I n the following sections, we are going to describe in more details this theoretical overview (If you are running JS on the back-end, the idea is pretty much the same, but instead of web APIs, you have NodeJS APIs). Javascript is a single threaded programming language, which means it has a single call stack and can do one thing at a time. The call stack is a mechanism so the interpreter knows its place in a script. When a script calls a function, the interpreter adds it on the top of the call stack. When the current function is finished, the interpreter takes it off the stack (Last In, First Out). Consider this simple JS function: Let’s see the call stack in action when this code is executed. 1. The interpreter starts calling foobar . 2. foobar is added to the stack. 3. The interpreter steps into the function, and console.log(‘A’) is called and added to the top of the call stack. 4. console.log(‘A’) is executed, and the letter “A” is printed in the console. 5. console.log(‘A’) is removed from the call stack since its execution has been completed, and the interpreter moves on to the next command. 6. console.log(‘B’) is processed the same way as the previous one, and the letter 'B' is printed in the console. 7. console.log(‘B’) is removed from the stack, and the interpreter moves on. 8. console.log(‘C’) is processed the same way as the previous ones, and the letter 'C' is printed in the console. 9. console.log(‘C’) is removed from the stack, and the end of the foobar function is reached. 10. foobar is also removed from the call stack, which becomes empty again. Cool! But what if we have a very slow operation, such as image processing or network requests? Since JS can do only one thing at a time, this slow operation will block all other operations until it is finished. During this time, the user won’t be able to interact with the page; the browser will simply freeze. This is not ideal, and this is not what actually happens. JavaScript concurrency model is different from other languages like C and Java, and it is based on an “event loop.” In this model, we run an operation and give it a callback function that is going to be executed later when the first operation is completed. This way, the call stack is not blocked and other operations can be added to it. Let’s modify our previous example a little bit so we can see how these async callbacks are handled by the browser: 1. Now, instead of calling console.log(‘B’) directly, we are putting it inside a callback function that will be called later. The setTimeout is an asynchronous function that we are using to simulate a slow operation. Here again, the interpreter starts by calling foobar . 2. The first steps are the same: foobar is added to the call stack, the interpreter steps in, adds console.log(‘A’) to the stack, prints the letter 'A,' and removes the call from the stack. Then the interpreter reaches the setTimeout function and adds it to the call stack. 3. As we noted in the beginning of this article, setTimeout is provided for us by the Web API. So it receives the setTimeout call alongside the callback and starts processing it. 4. Now the call to setTimeout itself is completed, so it can be removed from the stack. The Web API continues the processing, but the call stack is not blocked anymore, so the interpreter can move to the next command. 5. This next step is executed normally, and the letter 'C' is printed to the console. 6. console.log(‘C’) is removed from the call stack, and so is foobar since the interpreter has reached the end of the function. The call stack is empty again. 7. Ok… we are almost there, but what about the letter “B”? How is the callback sent to the call stack so it can be executed? When an asynchronous call is completed, the callback function is pushed to the Task Queue. 8. This is where the last piece of the puzzle comes in--the event loop. The event loop has one simple job: it looks at the call stack and the task queue, and if the stack is empty, it takes the first item in the queue and sends it back to the call stack. 9. Finally, the callback is executed, the letter \"B\" is printed in the console, and the callback is removed from the call stack. And that’s it! If you want to dive a little deeper into the concepts presented above, be sure to check the links below. Happy coding! • Concurrency model and Event Loop: https://developer.mozilla.org/en-US/docs/Web/JavaScript/EventLoop • How JavaScript works: an overview of the engine, the runtime, and the call stack: https://blog.sessionstack.com/how-does-javascript-actually-work-part-1-b0bacc073cf", "date": "2019-2-27"},
{"website": "Avenuecode", "title": "How Digital Evolution Saved the World", "author": ["Zeo Solomon"], "link": "https://blog.avenuecode.com/how-digital-evolution-saved-the-world", "abstract": "Despite the devastation caused by COVID-19, technology has safeguarded our economy and prevented the world from devolving into chaos. COVID-19 has caused devastation on a personal and global level, yet had this pandemic hit at any other time over the last 2,000 years, the effect would have been significantly more catastrophic. Imagine a world where 80% of the workforce is in lockdown, 33% of the population is unemployed, the global supply chain is massively disrupted, medical practitioners are ordered to shut down, physical distancing is mandated, and a food supply panic sweeps the nation alongside the highest-ever spike in gun sales (data specific to the US). This is a recipe for total chaos. Yet thanks to technology, businesses have continued operating with some degree of normalcy. Technology systems have safe-guarded supply chains; remote communications have leveraged in education, medical services, customer services, logistics, and entertainment; and essential businesses like commerce, insurance, and finance have continued providing services online. In short, technology has played a vital role in enabling our economy to survive. Whenever unexpected disaster strikes, there are winners and losers. Today’s winners are telecommunication, grocery, e-commerce, fintech, digital media, sporting goods, and logistics/warehouse management. Losers include: the fuel industry, tourism, fitness/gyms, hospitality, transportation, arts and entertainment, local government, and shared economy. What separates the winners from the losers? The losers are tech neutral, and the winners are tech driven. COVID-19 has in some ways made us stronger as the world has gone digital: older generations have been rapidly indoctrinated into using technology, remote work has accelerated digital transformation, online commerce has stimulated price competitiveness, and our ability to stay connected and leverage technology for supply chain and business operations has quadrupled. More tangibly, remote work has boosted hourly productivity by approximately 40%, with actual work time per day going from 5.23 hours to 9 hours. Meeting attendance has gone from 70% to 90%, and business operations are running leaner. Social distancing has produced closer collaboration and communication, with teams gravitating toward phone and video conferencing (email usage is down by 30%). All of this has led to a quick injection of globalization. For the first time ever, we have a shared collective global conscience and a global acceptance of distributed teams. A new level of perfection is often born out of devastating events, as is the case with COVID-19. As terrible as it has been, we’re learning more and more, not only about how resilient humanity can be, but also about how adaptable we all are. Within a few months, our civilization has undergone an evolution for the better. This is a new beginning. Let’s embrace it and build upon it. The situation with COVID-19 is relatively new and is still developing. The data collected for this article has been found by consolidating a variety of different publications, news articles, daily news, and Avenue Code‘s own business operational data. The author warns the reader that the data is still young and is frequently being reshaped as the pandemic unfolds.", "date": "2020-7-29"},
{"website": "Avenuecode", "title": "GraphQL 101", "author": ["Abraao Carmo"], "link": "https://blog.avenuecode.com/graphql-101", "abstract": "Today we will talk about a powerful technology and specification created by Facebook, GraphQL . According to GraphQL main site: \"GraphQL is a query language for your API, and a server-side runtime for executing queries by using a type system you define for your data. GraphQL isn't tied to any specific database or storage engine and is instead backed by your existing code and data.\" So as you can see, GraphQL proposes a change in the way a client asks for data from a server. Instead of needing to know several endpoints, the client now only needs to pass a query asking for the desired data. This is, indeed, very powerful! For example, if a client’s data requirements change, it is no longer necessary to reflect that change in the backend - the client can simply change the query according to its needs. Another powerful feature is that you can specify all the data you need and query it in an unique request. With a traditional Rest service, on the other hand, it would be necessary to send more than one request to get the same data.  Eliminating this necessity creates spave for some useful optimizations. For example, a query could be parallelized in the server, dramatically reducing the time spent. As a language, GraphQL query is very simple and intuitive to master. A basic example is below: The response of a GraphQL implementing server is in JSON format and must reflect the format of the query. A response for the query above could be: As previously mentioned, GraphQL is also a specification and can be implemented in any language and platform. In this article I will present a brief overview of its features. When you're ready to learn more, you can access the specification site: http://facebook.github.io/graphql/ . So how does querying look? Let's start with some code: A possible response could be: As you can see, the GraphQL query is basically about defining the fields you want to retrieve. The execution of the query above brings the field fruit of the root of the graph (also called \"root query fields\" ). Now let's look at a more complex query: A result could be something like: The query above contains powerful features of GraphQL, such as nested querying ( author field) and arguments (the post _id ). Nested querying allow us to retrieve both primary fields and their nested fields as needed. In other words, we can go deeper in our graph as required. Arguments allow us to filter the results. A query is able to receive multiple arguments, even in the nested fields. In the example above, we were able to filter the post by its id . Up until this point, we've looked at how to query for data from a GraphQL implementing server. In the event that we need to insert or change data, GraphQL provides what it calls \"mutations\".  According to the GraphQL official site: \"In REST, any request might end up causing some side-effects on the server, but by convention it's suggested that one doesn't use GET requests to modify data. GraphQL is similar - technically any query could be implemented to cause a data write. However, it's useful to establish a convention that any operations that cause writes should be sent explicitly via a mutation.\" Let's see some examples: The response could be: As you can see in the example above, it's not hard understand the mutation. Basically, it creates a new author on the server side and returns the written field with its id and name fields. We can also see that, as opposed to simple querying, the values of the mutated fields are settled in the mutation, and the key word mutation should be present at the root of the query. Another exciting feature of GraphQL its ability to accept multiple mutations sent together. For example: The response would be: In the example above, we can see a feature called \"result variable\". In this case, each result from the mutation is put in a previously specified variable. The aim of this article was to give a brief description of GraphQL and its features. Hopefully it helps anyone interested in using it! Clearly, the tool has some incredibly useful components, and I believe we'll see its presence in the market increase due to its ability to solve common issues related to RESTful services. Long term, it doesn't seem that GraphQL aims to replace RESTful services, since they can be used together. But that's a conversation for another day. An excellent, free online course for those interested in studying GraphQL: https://learngraphql.com Are you already using GraphQL in your projects? What do you think? Share your experience in the comments!", "date": "2017-4-5"},
{"website": "Avenuecode", "title": "Creating A Data Visualization From Scratch Using d3.js", "author": ["André Gonzaga"], "link": "https://blog.avenuecode.com/creating-a-data-visualization-from-scratch-using-d3.js", "abstract": "Deriving information from data can be a big challenge. In this post, we will learn how to create a simple data visualization using d3.js. Moreover, we will try to understand the basic concepts under this amazing library as well as go through a brief overview of javascript, css, and svg. An old and common approach to retrieve information from data is to use report files such as PDF, XLS, or static charts. However, the power of computers today is infinitely greater than that of computers 15 years ago. Therefore, there are many new ways to look at data. Although the power of computers has significantly increased, the size of screens has remained the same and hasn't kept up with the improvements of other components such as processors, memories, and secondary memories. In other words, utilizing the inifinite power of computers inside screens of such small and stagnant sizes can prove to be a real challenge. D3.js is a javascript library for manipulating documents based on data. This is a description from its creator, Mike Bostock : D3.js is a JavaScript library for manipulating documents based on data. D3 helps you bring data to life using HTML, SVG, and CSS. D3’s emphasis on web standards gives you the full capabilities of modern browsers without tying yourself to a proprietary framework, combining powerful visualization components and a data-driven approach to DOM manipulation. So, let's get started! For this post, we are using a public dataset from the 2014 World Cup which has information about players that participted in this World Cup.  You can download it here . Basically, the dataset has the following information on each player: Before we start, we need to clean up our data set since the entire set of data would be an overload for our little 1200x900 screen. In order to clean our dataset, we can remove countries that weren't top performers, so that the remaining countries are: Brazil (Vai Brasil!), Spain, Colombia, Uruguay, France, Argentina, Germany and Belgium. The cleaned dataset can be downloaded here . The first step to understanding what a data visualization is and how it works through the web is to understand how html, javascript, css, and svg can work together to collect data and show it properly. Here is a quick review of what html, javascript, css and svg are: Web browsers receive HTML documents from a webserver or from local storage and render them into multimedia web pages. HTML describes the structure of a web page semantically and includes cues for the appearance of the document. JavaScript is a high-level , dynamic , weakly typed , object-based , multi-paradigm , and interpreted client-side programming language . Cascading Style Sheets (CSS) is a style sheet language used for describing the presentation of a document written in a markup language . Scalable Vector Graphics (SVG) is an XML -based vector image format for two-dimensional graphics with support for interactivity and animation. The SVG specification is an open standard developed by the World Wide Web Consortium (W3C) since 1999. Basically, we need to get data using javascript functions and create an svg graphic inside the html. We can also utilize css to make it clear and stylish. Fortunately d3.js already has a lot of functions to help us do all of this. So, the first step is to create a html file from scratch and include the d3.js library like this: We just defined a linear scale for x and y with the size of our width and height predefined. d3.js has several types of scales, which you can check out here . Once our scales are done, let's create the axis with the proper orientation: Last but not least, we need create the svg and attach it to the body. d3.js has a method for attaching elements into the DOM. We need to select which part of the page we want to attach with our element, and then create the element. The code to do that is: We are selecting the element \"body\" and attaching a \"svg\" element inside it with width and height as attributes. We are also attaching a \"g\" element with a transform attribute into the \"svg\". If you open the page now, you will be able to see something like this: The svg was successfully attached to our body and the g element is there under the svg. So far so good! Before we continue, let's create another scale using colors. It will be useful to be able to identify each country in terms of colors in our graphic. This function basically generates 10 random colors to be consumed by any property that we assign, so just add this line above the svg creation: The next step is to get the data and add our visualization inside the svg element.  The d3.js library has a lot of functions that allow the consumption of data from a lot of sources, like csv, json, and tsv. For this post, we are using tsv and can properly load the data like so: We now have a colored static graphic which gives us a visual interpretation of the players. However, we don't know much about this graphic because we have no idea what each color represents. So, we need to create a legend. We can do so using the same approach as before. We can select and apply the data and then create the box and the legend text like this:", "date": "2017-7-27"},
{"website": "Avenuecode", "title": "How to Move to and between Clouds Providers", "author": ["Douglas Augusto"], "link": "https://blog.avenuecode.com/moving-to-and-between-clouds-providers", "abstract": "Cloud computing is one of the main investments of Amazon, Microsoft, and Google, which directly compete against each other with their respective AWS, Azure, and GCP cloud providers. With a fierce competition for the market share of the infrastructure of several enterprise organizations, each provider has adapted differently to the needs of customers to offer services that optimize businesses. The main reasons that companies migrate their workloads between clouds are related to cost and application performance. But is migrating worth the effort? The answer is often yes. Migrating is also an opportunity for modernization, which can bring benefits not only for resource optimization but also for the end user. When it comes to migrating from an on-premise environment to the cloud, the benefits are even greater. Some of the top benefits include native scalability, flexibility, less overhead, and a pay-as-you-go model. Moving workloads to the cloud or between cloud environments requires planning and management so that nothing goes wrong and business operations are not affected during the migration. Nobody wants a costly mistake during the process, so every detail is important. The migration process is complex, but when carefully planned, it can happen smoothly. Some of the main pillars in the migration process are: Assess, Plan, Migrate, and Optimize. Assess During the assessment process, the objective is to extract metrics and information from the environment for decision making, which entails understanding which workloads are ready for migration and what the business and critical need is for each of these workloads. Plan With the information about the environment, we can use the mapping of applications and their relationships, together with the prioritization information on the business side, to decide how many waves we will migrate on and what the priorities are for each of these migration waves. In this step, we also decide what the strategy will be: for example, it might be a simple Lift & Shift or else a Lift & Modernize when possible. Migrate This phase involves the design process of the migration solution, where we select the best tools to migrate the workloads according to the scenario. Tools can vary due to several factors, such as the OS, application, and database. After selecting the tool, we configure it and start the process. During migration, continuous monitoring and tests are performed to ensure the cutover. Optimize There are several ways to optimize the workload after migrating. The most common way is the rightsizing of resources, avoiding over-dimensioning and generating a cost reduction. There may also be optimization opportunities regarding the use of managed services that can reduce the team's overhead in managing that resource. Moving to and between clouds can generate several benefits for your business, whether on the financial side or on the management of your infrastructure. If a migration isn't already in your roadmap, it may be time to consider the benefits of incorporating a cloud computing update into your 2021 business strategy.", "date": "2020-11-25"},
{"website": "Avenuecode", "title": "Behind the Scenes of Blue-Green Deployment", "author": ["João Moráveis"], "link": "https://blog.avenuecode.com/behind-the-scenes-of-blue-green-deployment", "abstract": "The last few years have seen huge improvements in the speed of software development. Combined with the adoption of methodologies like Agile, this has resulted in more and more frequent deployment requests and releases. So how do DevOps teams provide the required CI/CD practices to support the speed and demands of these new technical capabilities and business strategies? Software development has improved dramatically over the last few years by establishing new paradigms, designs, and well-know practices. Additionally, companies have  incorporated methodologies like Agile that result in more frequent releases, unlike the yearly releases that used to be the norm for updating monolith systems. DevOps professionals have been working hard to keep up with the rapid evolution of software development. Nowadays, we have frameworks to provide automated deployments, maintain quality, and ensure security. Good examples of these frameworks are Jenkins and SonarQube. However, for modern solutions, the most frequent requirement is to have a no-downtime deployment when releasing a new version. In response to this challenge, a couple of strategies have been developed and successfully implemented. Some interesting examples include big bang deployment, rolling deployment, canary deployment, and blue-green deployment. In today's post, we'll take a closer look at blue-green deployment. Blue-green deployment was introduced by Dan North around 2010. The idea is to reduce deployment risks and avoid downtime that could cost companies millions. It also provides reliable and consistent environments for developing, as well as a QA team, mirroring the production environment. Another benefit of blue-green deployment is that it can be easily rolled back to the old version in case the new release exhibits unexpected behaviors. The idea for blue-green deployment is that there is a production environment (the blue environment) that is up and running. After ensuring that it is stable, we deploy a new environment (the green) in which we have the same version but idle. Both environments are managed by a load balancer, as shown below. Image courtesy of ThePracticalDev For the next release version, all changes will be applied and tested in the green environment. Once we determine the stability of the changes, the process to switch to the active environment can be executed by requesting the load balancer to migrate all sessions and incoming requests. In some cases, the first step is to block the blue environment from receiving requests, keeping it alive until all active sessions are finished, while redirecting new requests to the green one. Another option is to migrate sessions at the same time. After the blue environment finishes the migration, we can set it as idle while the green environment assumes the production role. Image courtesy of ThePracticalDev Managing a database can be tricky, so mirroring can be very helpful to keep both the blue and the green environments synchronized. Sometimes both blue and green can share important resources, such as a database. Because of this, updating and keeping it compatible between releases is extremely important. Image courtesy of cdn2 Last but not least, some applications use message queues, which can present a challenge. Message queues require special attention because queues need to be managed while switching from one environment to another. In most cases, message queues prevent the usage of blue-green deployment, so another deployment solution must be used instead. As explained above, blue-green deployment can be easily implemented and provides several benefits like reducing risks and eliminating downtime (in most cases). Blue-green deployment is a colorful solution that can be implemented in many different ways, so DevOps teams can choose the solution that fits their scenario.", "date": "2020-1-8"},
{"website": "Avenuecode", "title": "AC  Spotlight - Scott Emmons", "author": ["Holly Camponez"], "link": "https://blog.avenuecode.com/ac-spotlight-scott-emmons", "abstract": "Scott Emmons is Head of the Innovation Lab at Neiman Marcus Group, where he identifies and invests in technologies that will enhance both the overall consumer experience and the ability of NMG sales associates to connect with shoppers. Avenue Code: Thanks for joining us today, Scott! Can you share how the Innovation Lab at Neiman Marcus came to be? Scott Emmons: I’ve personally been with Neiman Marcus for about 12 years, in various roles. Prior to the Innovation Lab, I founded the enterprise architecture team here, which was really focused on innovating and modernizing by using technology to tackle problems. Of course, we were doing this in the back office, but we were also paying attention to what was happening for our customers. This was really the origin of the Innovation Lab, and a little over 6 years ago, we got it kicked off. My work operates across all areas of the business and involves collaboration with colleagues in a wide variety of roles. At this point it’s hard to imagine doing anything else - the last 6 years have been amazing. AC: Tell us about some of the things that have come from the Innovation Lab, such as Memory Mirror. SE: This technology is a great example of what we’re up to here. We have to look at investments from the business side and the customer side, as well as from the perspective of the technology teams and the underlying infrastructure required to make the ideas successful. My key role is to keep these visions aligned. I’ve spent a lot of time talking about Memory Mirror in particular because the concept so clearly solves real problems for the customer. When we tell people for the first time about a mirror that allows you to try on an outfit in a different color or pattern, record a makeover session, or try on different sunglasses and then remember the results, we always get the same response: “Why haven’t we always done this?” The answer, of course, is that it took some very innovative technology that was realistic and workable for the customer. When I first met with MemoMi, they had developed it. They had amazing patented technology they were using to deliver the mirror experience, but they didn’t necessarily have the retail background to deliver it successfully. It became a great partnership in which both parties were very receptive to feedback and collaboration, and after nearly a year spent perfecting the Mirror, we introduced it into stores. Our expectation was that Memory Mirror would primarily interest younger people, but in fact, it was appealing to shoppers across all age groups. As with any new technology, there’s some shyness about trying it, so it’s critical for the sales associates to introduce it and ensure it’s accessible. AC: How do you find these kinds of gems that are not fully developed yet? You’re helping them get to the finish line and achieve their potential. What is your vision as the creator? SE: That’s really a major question in its own right. For me, it always comes down to the same question: does it really solve a problem for the customer? Oftentimes you find technologies that feel like solutions looking for problems - but every once in a while, you hit upon those that are actually doing something for the customer. And of course, our sales associates are a very important part of what makes Neiman Marcus special. So here again, it’s about finding technologies that enhance their ability to be great at what they do. When I find a technology that solves a real customer problem and enables our associates to go to the next level, I know it’s a winner. Of course, demonstrating potential ROI is also critical - in the innovation business, you’re never going to get a \"yes\" right away, especially if what you’re proposing means changing the way things have been done. You have to be persistent when you know the idea has potential. ROI, of course, can mean a lot of different things and isn’t easy to quantify. Especially as a luxury retailer, part of what we’re investing in is the customer experience, regardless of whether there is a direct profit attached. AC: It seems that the Innovation Lab has been producing features to enhance consumers’ in-store experiences. Much of retail is currently shifting from brick and mortar stores to online transactions. How does the Innovation Lab bridge the gap between physical and digital consumer experiences? SE: In the early years of the Lab, we had a greenfield opportunity to introduce customer-facing technology. Of course, we had back-office tech in place, but we were fortunate enough to have a whole new opportunity to introduce customer-facing tech coupled with a mature digital organization. This afforded us some valuable resources we could bring to bear when it came to delivering these experiences in store. As the pendulum has swung more toward digital initiatives in the last several years, the larger and faster growth for retail in general has been on the digital side. Our thinking at the Lab has been to identify what we do well in stores and create it in a way that a digital-only customer can have the same feeling and experience. I see myself as a bridge between these two channels, and many of my colleagues outside the Innovation Lab are thinking the same way. The goal for us is to blur the line to the point where we deliver an amazing customer experience that is recognizably Neiman Marcus, regardless of where customers are shopping. AC: At what point did you realize you were on the right track with the Innovation Lab? SE: The confirmation came fairly early on. We spent the first four months or so building out concepts in the Lab without much idea of whether the larger business would be receptive. But during our first show-and-tell with the CEO, it became abundantly clear that we were on the right track and would have support to try what we wanted to try. Of course, we didn’t create the Lab to innovate just for the sake of innovation, but there was an appetite across the wider business for technology that would go beyond order-taking. We’ve done so responsibly, of course - I anticipate further growth of the team, but historically I’ve been the full-time guy in the Lab, rallying the troops around whatever projects were active at the time across all areas of the business. Because we've generated only minimal expenses that are directly related to active, high-priority projects, we’ve been successful and stayed productive. We also try to be creative - we work with interns, in-house staff, and our solution providers themselves. The outcome has really been remarkable. If you look at the way our entire IT organization runs today, you'll see we’ve evolved to be more Agile, more collaborative, and freer from process for its own sake. AC: Do you have any closing thoughts for us today? SE: My final thought is this - retail in general is experiencing a revival. For the last couple years, the story has been one of retail doom and gloom, and that’s changed recently. Retail in the end is about delivering what the customer wants and needs, and a lot of retailers get that. I’m excited to work for one of them! AC: Thank you so much, Scott! It’s inspiring to hear what you’re up to at the Innovation Lab, especially in terms of the alignment between business and tech to enhance customer experience.", "date": "2018-11-7"},
{"website": "Avenuecode", "title": "How To Build a WebApp Using Swift - Part 1", "author": ["Lucio Fonseca"], "link": "https://blog.avenuecode.com/how-to-build-a-webapp-using-swift-part-1", "abstract": "During the Wordwide Developers Conference (WWDC) 2014, Apple brought to the world something every iOS developer was expecting: A new language named Swift , that was presented as the solution for the already old-fashioned Objective-C. The promise was that Swift would be modern, and would bring more safety and power for apps. But the new language was so good, that webdevelopers decided to try it to create webapps. The problem for the webdevelopers was that, after realizing the potential of the new language they had created, Apple decided that Swift was good enough to be restricted to iOS and Mac apps only. Not until an entire year had passed, during the WWDC 2015, did Apple make the leap to Open Source with Swift. With its change to Open Source, it didn’t take long before a slew of new technologies based on Swift began popping up all over the place, especially for the web. Today, the most frequently used web framework for Swift is Vapor , and it is widely considered a good choice for web development because it is fast, secure, extensible and also has integration with XCode. In this Tutorial, we will learn the basics of how to build an webapp using Swift. To do so, we will develop a simple app that allows users to register their pets and also include the next date their pets need to go to the vet. In this first part we will learn how to setup the environment using the Vapor Framework, create the project and configure our database. So, let’s get started! This tutorial was created using Swift 3.0 and Vapor 2.0. Newer versions of the framework might not be compatible to the tools used. Creating a new Vapor project is pretty straightforward. Go back to Terminal and execute the command: TaDam! Our Project has been created! If you are so excited about Vapor that you can't wait to see it run, you can execute the following commands: Before moving on to the next Section, let's create our Xcode project. After all, what's the advantage to building a Swift project without XCode? Let's run one more command, vapor xcode , on Terminal. Now, in the folder you created the project in, you will find an XCode project. Now with XCode open, go to the file Source/App/Main.swift . There you will find a instance of Droplet. The Droplet is a service container that gives you access to many of Vapor's facilities. It is responsible for registering routes, starting the server, appending middleware, and more. But, what is a Route ? A route is a declaration of a path that is mapped to a handler. You will note that we already have a route declared. This route points to the root directory and loads a default welcome page created by Vapor: For now, we will simply remove the default route and create our own first route: Now, in your browser, you can go to localhost:8080/pets and see that our page is created. Need help to build your Web & Mobile Apps? Avenue Code can help you! One of the most essential capabilities of an App is persisting information. Vapor supports some of the best persistence technologies - like MySQL, PostgreSQL, MongoDB, SQLite and Redis - through the use of providers. The Provider protocol creates a simple and predictable way to add functionality and third party packages to your Vapor project. In this project, we will utilize MySQL, but feel free to use any other technology you may be more comfortable with. We will use Sequel Pro to create our MySQL database. First, you will need to connect to MySQL. Choose Standard connection, enter 127.0.0.1 for host. Default username for a new MySQL installation is root, with a blank password, in case you have not set up your own MySQL username and password. To create a new database you just need to select the option Add Database as in the image below, and type the database name - in our case, \"PetDB\". Now go to Terminal and type vapor build to link your project with MySQL. So far so good, but now let's configure the database on our Vapor project. First, we need to include the provider for MySQL in our dependency file. To do so, open /Package.swift and include the following line .Package(url: \"https://github.com/vapor/mysql-provider.git\", majorVersion: 1, minor: 0) on the dependencies array, and don't forget to add a comma between the objects inside the array. Now, let's update the project, ensuring all dependencies will be downloaded and included. Go to Terminal and run vapor clean and then vapor xcode . Now, back to Xcode, and open the /Config folder. There, create a new folder named /secrets, and a file named mysql.json . Your project navigator should looks like this: Inside mysql.json, you will enter a JSON with your database information. In our example, the JSON will look like this: Let's open /Source/App/main.swift and include import VaporMySQL , and also update our Droplet instance to include the new provider: let drop = Droplet(providers:[VaporMySQL.Provider.self]) . Our MySQL configuration is pretty much done. We can check if everything went well provisionally by adding the new route: and accessing localhost:8080/version through your browser. MySQL current version number will be printed. Alright! We've come a long way. But we still have a lot to learn. In this first tutorial, we learned how to install and configure our environment and our database. In Part 2, we will learn how to make the Create our Pet Model, and create our CRUD operations. And it will be way more fun, because we will finally start to do what we like most: code! Check out Part 2!", "date": "2017-1-11"},
{"website": "Avenuecode", "title": "AC Spotlight - Sameer Hassan", "author": ["Holly Camponez"], "link": "https://blog.avenuecode.com/ac-spotlight-sameer-hassan", "abstract": "(The following article is property of Avenue Code, LLC, and was originally published with permission at TotalRetail on July 26, 2018 .) Sameer Hassan is the senior vice president of enterprise product strategy and user experience at Williams-Sonoma Inc., where he heads up product management, UX and product analytics for the WSI brands. We caught up over lunch recently to learn more about WSI’s culture of innovation. Avenue Code: At WSI, you’re working with cutting-edge technology such as virtual reality (VR) and augmented reality (AR). How would you characterize the changes in how consumers shop over the last few years? Sameer Hassan: Honestly, it’s still early days when it comes to AR and VR. Consumers are still learning about the technology, and even the user experience patterns haven’t really developed yet. Everyone’s still learning about how people use these tools, and ultimately how to design them with the user experience in mind. For Williams-Sonoma, we see AR/VR as another opportunity to simplify home decorating for our customers. Our use of AR bridges the gap from a tactical space planning perspective, but also helps our customers imagine what their homes could look like. We’ve even created a feature that allows you to digitally empty the room so that you can play with different looks without superimposing them onto what’s already there. AC: How well have these tools caught on with your shoppers? SH: We’ve already begun to see increasing adoption among our shopper base as customers become more comfortable with these tools. But ultimately, I see it as akin to the way that mobile shopping developed. In the beginning, there were a lot of questions about whether people would actually shop this way — which today sounds absurd. I believe there will be the same tipping point moment where AR isn’t a shiny new object anymore, and that it becomes a part of the user experience just like mobile shopping is today. A screen capture of Williams-Sonoma’s augmented reality technology at work. Image courtesy of Williams-Sonoma. AC: How did you get started working with this technology? SH: For us, it started with our partnership with Outward, Inc. , a 3D-imaging company, which we've since acquired . We were also partnering with Google’s Daydream group to explore how we can use this technology to solve user problems. Once we started to get our heads wrapped around what it can do — and really, we’ve barely scratched the surface of what’s possible — we got excited and everyone was all in. AC: What can we expect next? SH: We’re very excited about all of the potential applications of AR/VR that will allow us to continue to simplify the home decorating experience for our customers. For instance, when we looked at the return data for our customers, one of the issues many of our urban shoppers were facing was that although the furniture may have been accurately measured and perfect for the space, the stairway or elevator to their apartment wasn't wide enough. One of the concepts we’re exploring now is the ability to wave your phone at your various entryway points and know instantly whether a product you’re considering will fit. Another example for AR extends beyond furniture to home entertaining. Imagine the possibility of using your phone to visualize your dining table for Thanksgiving, with different centerpieces and place settings. AC: Looking at the big picture, what's the vision for innovation at Williams-Sonoma? SH: At Williams-Sonoma, our goal is to create a culture and environment where the spirit of innovation becomes a shared characteristic embodied by each of us. We’re using creativity, experimentation and agile thinking to disrupt ourselves before we can be disrupted by others. Technology is a huge driver of this. AC: How do you foster this culture and encourage innovation? SH: That’s an important question that strikes at the heart of creating culture. It touches everything from who we hire, how we structure our teams, and the processes we put in place. For one thing, we try to design everything around measurability, so we can understand what’s working and what’s not. We also start small with each new initiative and then evolve the experience with iterative processes to test whether an idea has traction. Curiosity and the ability to learn and act quickly are central to our culture of innovation. And we’re particularly focused on these attributes when we’re hiring and selecting strategic partners. In fact, we created an award in honor of our former CMO Pat Connolly, who was brilliant and instrumental in shaping our company culture. This award, named the Pat Connolly Spirit of Innovation award, recognizes associates who propose innovative, practical ideas that have made a significant impact on our company. Williams-Sonoma’s unique culture of innovation is evident at every level of the organization. Image courtesy of Williams-Sonoma. AC: What’s an example of one of these moments from you or your team where you brought something entirely new to market? SH: One example that stands out to me has to do with solving a customer pain point related to abandoned carts. We identified that one significant contributing factor to abandoned carts was shoppers switching from one device to another and needing to start selecting their items all over again. Rather than re-architecting and spending months solving for this in a traditional way, we came up with a quick and sustainable way to stitch your identity across devices. Rather than becoming consumed with a huge solution, we focused on efficiency and speed, and the end result for the customer. AC: What was your biggest lightbulb moment in the last few years? SH: The steps and conversations that led to our acquisition of Outward brought about a huge realization for me: this wasn't about changing one experience or one platform, it was internalization of the fact that in the not-too-distant future, commerce is going to look fundamentally different than it does today, and that technology is going to enable that. The way we shop online today simply doesn’t mirror the way consumers want to discover products. There’s a point where shopping gets turned on its head with personalized, immersive experiences, and emerging technology is getting us there. I’m excited to be at a company that's at the forefront of that revolution, and we're always on the lookout for great tech talent to help us achieve our vision! AC: Sameer, thank you so much for sharing your culture with us. It’s clear you and the team at Williams-Sonoma are creating something very unique, and we’ll look forward to seeing what’s to come!", "date": "2019-1-23"},
{"website": "Avenuecode", "title": "What You Need to Know about Swift Generics", "author": ["Antonio Netto"], "link": "https://blog.avenuecode.com/what-you-need-to-know-about-swift-generics", "abstract": "Over the years, Swift has become a very powerful language with many features, and one of the greatest of Swift's powers is g enerics . Generic code is a familiar concept for many developers because it exists in other, older languages like Java, C# and even C++ (where generics are called templates). Generics solve many problems and enable developers to write code very concisely, resulting in less code duplication and more type-agnostic. In fact, you’ve been using generics for a long time, even if you didn’t realize it, because Swift’s arrays and dictionaries are both generic collections. It's true! Let’s take a closer look: Can you create an array that holds Int values? And how about an array that holds String values or UIViewControllers ? See, that's a code structure that generic types allow! But wait! That does not mean that Swift has a weak type system! In fact, it’s the opposite . Swift has a strong type system. In other words, once a variable is declared as a type X, you can’t just assign a type Y value to it. Cool? Let’s create a code sample to illustrate what we are talking about. We're going to create a function that returns the sum of 2 Int parameters: Nice! But this function only supports the Int number type, and if you send a Double type parameter, you will receive an alert telling you: The first option you have is to create a new function with Double type parameters. But this is code duplication and that is not good (see the DRY principle ). So let’s turn our previous function into a generic function. Instead of setting the Int type, we will set as a placeholder type T and inform the compiler that the function is generic, which means defining it with <T> . But Swift doesn’t know that the generic type T has a ‘+’ binary operator. Can you imagine sending two random type parameters like UIViewController and summing them? That does not make too much sense, right? So for this particular kind of generic function, you will need to set a protocol conformance or a type constraint to limit our function usage to only work with Numeric types. Numeric is a built-in protocol (Swift4+) for any numeric values. Now, our function works like a charm: Wow! This is not just powerful. It is really beautiful. Don’t you think? Even if you don't agree now, you will in a minute. Now, let’s add more complexity here: this generic function works with all types of numbers because of the conformance with Numeric Protocol , which is natively provided by Swift. But what if you don’t have a free protocol to use as a Type Constraint ? Well, you will need to create this protocol on your own. Yes, this requires more effort, but it isn't too difficult: You'll need to do this when you do not natively have a protocol, but for most cases, Swift already provides a collection of basic protocols as constraints, like Equatable , Comparable , Hashable , Numeric , and many others. Now, let’s try to think of a new scenario where we want to simulate how a restaurant works. Since we just started our amazing restaurant, we only have one person to cook, so what would be the best basic data structure for preparing orders? The best structure is one where the first customer receives their order first, right? First-In-First-Out. That’s the definition of a queue . Cool! So we will need a protocol to enqueue , dequeue, and show the current orders we have. In the future, we're hoping to add different menus, but for now, we just have one simple menu. Because of that, our protocol needs a placeholder type to work with any present and future menu items. This generic placeholder type is called Associated Type . The basic format of the Orders protocol and the implementation in our RestaurantOrders class will look like this: But we only prepare food from our SimpleMenu . It does not make sense for the customer to come to our restaurant and order food that we can't prepare, right? So we will determine the type of items we accept in our RestaurantOrders instance, which should look like this: Now we have our restaurant running full steam, and when we launch the new ItalianMenu , everything will be ready. Check this out: Amazing! W e've made our restaurant fully functional! This is the power of generics that I alluded to at the beginning of this post. Generics can turn your code into much cleaner code. Generics are also important for the construction of useful libraries because they allow the library to adapt to application-specific data types without losing type safety. To understand how important this concept is in Swift, you just need to look at the Swift standard library . Also, check out the Swift Language Documentation to better understand this topic. P.S. You can download a playground file with the above code here .", "date": "2019-7-31"},
{"website": "Avenuecode", "title": "AC Spotlight - Heather Craig", "author": ["Holly Camponez"], "link": "https://blog.avenuecode.com/ac-spotlight-heather-craig", "abstract": "(The following article is property of Avenue Code, LLC, and was originally published with permission at TotalRetail on August 28, 2018 .) Heather Craig, head of retail experience at thredUP , talks with us about the rapid growth the consignment shop is achieving by building on a unique business model with customer-oriented technologies and a spin on a physical retail presence. Avenue Code: ThredUP is quickly gaining widespread recognition, but the concept behind it is innovative enough that it may need explaining. How would you describe the thredUP brand, vision and business model? Heather Craig: T hredUP is the world’s largest online thrift store, where millions of women buy and sell over 35,000 different brands, from Old Navy to Gucci. You order a free clean out bag on thredUP . com , fill it with clothes you no longer wear, and receive a payout from us. When I first discovered it seven, eight years ago, it made cleaning my closet incredibly easy! AC: What goes into building consumer loyalty for a brand such as thredUP? HC: One in 10 women in the U.S. have joined thredUP, and they’re loyal because they’re invested in our full process, both clearing their closets and purchasing new items constantly. With over a thousand items added to our inventory every hour, we give women an affordable way to continually refresh their wardrobes. AC: T hredUP is investing heavily in a physical store presence, as opposed to many retailers which are moving in the opposite direction. What's the importance of a physical presence for thredUP shoppers? HC: Our mission is to change the way women think about secondhand. Our primary selling platform is online, but since 85 percent of women’s clothing is still purchased in-store, we’re converting skeptics by opening physical locations and allowing shoppers to see and touch products that are beautiful, high quality and fashionable. Our physical presence builds confidence in our online offerings. In addition to enjoying incredible customer service, customers can scan any clothing items while in the fitting room and quickly find similar products online. We also track data on top-selling items geographically to help determine our in-store inventory. It's more complex than traditional retail, but our state-of-the-art logistics makes this all possible. The interior of one of thredUP’s store locations. Image courtesy of thredUP. AC: How can digital native brands create a cohesive omnichannel experience for consumers? HC: Our stores are just another portal to our site. You can order a clean out bag online or pick one up in-store. You can mail it back or drop it off in person at the store. We leverage the data we already have on the customer’s favorite items, stock them in-store, and notify you on your thredUP app that your brands are back. Every experience is seamless. AC: What about in-store technology? How is thredUP investing in this aspect of the consumer experience? HC: If customers like an item, they can scan it and find similar item recommendations. Our in-store teams use these technologies as well, scanning popular purchases and ordering similar items, giving them great local control over their inventory. We also use a variety of tools to determine ideal styles for different body types. We have a lot of information on how different brands fit different bodies, and we use this to help our customers find the right jeans, evening gowns, shoes, etc., allowing them to enjoy the luxury of a virtual personal stylist but in a physical store. A shopper scans an item to get personalized product recommendations. Image courtesy of thredUP. AC: Heather, it’s always inspiring to see women forging an impressive career path in the world of e-commerce. How did you to get into this line of work? HC: One of my first jobs was as a stock girl at a kid's store when I was 15. I fell in love with helping customers find an item they were looking for and discovering what they didn’t know they needed. As I’ve grown in my career and my knowledge of branding and marketing, I always come back to the idea of customer satisfaction. Retail is always changing, and I love the challenge of meeting customers’ needs through every change. AC: Tell us a little about your time at The North Face, and what eventually led you to consider thredUP as the next step in your career. HC: I was with The North Face a little over six years, overseeing all buying and merchandising for retail stores as well as e-commerce. I learned so much about consumer connections and product marketing through managing and owning such a massive brand! I later joined thredUP because I had the opportunity to meet with James [Reinhart, CEO] as a fan of thredUP. I was curious about the business and how thredUP worked. I had used thredUP from the beginning, and I didn’t even know physical stores were on the horizon. During that conversation, we discussed the possibility. After a few discussions, we decided to give it a go. It was a natural transition for me because I was just so passionate about it. AC: With a year-and-a-half under your belt at thredUP, what are your key takeaways? What are you proudest of having accomplished? HC: I can’t believe how much we’ve accomplished in a year-and-a-half, but that’s the nature of a startup. ThredUP has shown me retail from a completely new angle, exposed me to secondhand shopping, and added to my understanding of logistics and business. At the same time, key values like customer service and great products at great prices never change. I’m proudest of the fact that a year and a half ago, the idea of physical stores didn’t exist for the company. Now, we have full retail teams in Walnut Creek, Burlingame, and Los Gatos [all in California] that we built from the ground up. We have plans to continue to expand along the West Coast and eventually beyond. ThredUP has huge potential to change people’s lives and the way they think about secondhand. The sky’s the limit! AC: What's your next big project? Tell us about your vision for the future. HC: We’re an innovative company that wants to stay at the forefront of change. We’re currently brainstorming new ideas for how customers interact with clothing and purchasing in physical stores. AC: Any closing thoughts for us? HC: T hredUP does an incredible job of considering long-term goals from both a business and customer standpoint. It’s extremely unique to be a part of a company that isn’t focused on short-term profit, and it allows us freedom to brainstorm and test new ideas. In the wider retail landscape, you see discounts and promotions pumped left and right to drive traffic, but we believe our prices are excellent — we’re up to 80 percent estimated retail. Of course, we could do markdowns and put huge sales signs in our windows, but we want to engage customers long term rather than push a quick sale. This vision fosters a lot of creativity! AC: Thanks, Heather! ThredUp is a breath of fresh air in the retail landscape, and it’s great to see the mindset shift around secondhand.  We’ll stay tuned to see what’s coming next for you and the team!", "date": "2019-3-20"},
{"website": "Avenuecode", "title": "How to Perform UITests on iOS Without Getting A Headache", "author": ["Jean Sandrin"], "link": "https://blog.avenuecode.com/uitests-on-ios", "abstract": "Designing and writing tests aren't the most exciting things in the world, we know, but tests are absolutely essential for any app you write. They might be the difference between a shining 5-star app on the AppStore, or a bug-ridden collection of code. We all know what Unit Tests are and how to write them, but since XCode 7, Apple has introduced us to UI tests inside their IDE. With these tests, you can record a user's interactions with your app and check to see whether it's behaving as it should. The accessibility label is your best friend when developing an iOS app. It provides users who have hearing impairments with a way to navigate through your app and also helps you better test your UI. You must be asking yourself why I introduced Viper to you in the first place. The tests we did could be done with MVC, sure, but what if you have to test something that's deeper down in your app? Since Viper is so modular, you can write your classes thinking about tests, which make them extremely injectable. You can also create a class solely for instantiating all the elements of Viper, where you can inject anything you want. In our example, I created a builder called ProfileBuilder that instantiates all elements of Viper and returns the Router element.", "date": "2017-10-10"},
{"website": "Avenuecode", "title": "AC Spotlight - Martin Gilliard", "author": ["Holly Camponez"], "link": "https://blog.avenuecode.com/ac-spotlight-martin-gilliard", "abstract": "(The following article is property of Avenue Code, LLC, and was originally published with permission at TotalRetail on April 27, 2018 .) While in New York City, I had the opportunity to interview Martin Gilliard, chief information officer, senior executive vice president at Barneys New York. I'm excited to share his fresh perspective on retail and the ever-evolving role of CIOs, which is informed by his expertise in technology and other diverse studies. Holly Vander Wall: When you were a child, what did you want to be when you grew up? I have a feeling that only a few CIOs know they will go down that path. Do you agree? Why did you become a CIO? Martin Gilliard: I wouldn’t say I knew I wanted to be a CIO, but I always knew I wanted to solve challenging problems. Becoming the CIO of Barneys wasn’t about the role; it was about the opportunity. What I do differs from the traditional CIO role because I also operate the e-commerce business, employing my technical expertise to change how we present products to customers. Everything is about the customer. In my opinion, retail isn't dying, it’s transforming. What’s killing retail isn't e-commerce, but the inability to adapt to it. HVW: What expectations did you have for the CIO role, and what opportunities opened up for you when you took it on? MG: When people talk about a digital transformation, they’re often referring to changing technology, but actually, changing technology is the easy part. What’s harder is changing the perspective of the teams. The people who win in tech are the people who can transform and execute quickly, and part of that means making fast decisions and delivering quality products. What I’ve realized over the past 20 years is that the No. 1 way to make that happen is to get everyone aligned and in agreement. E-commerce demands a different type of executive — one who needs to understand not only the data and technical components, but also the customer component. HVW: How have you adapted to this evolving role? MG: It's more about what I've been able to bring to the role. When you work in retail, where margins aren’t big and there are pressures from different types of competitors, getting something wrong can be very risky. Are we brave enough to take a chance? Can we create an environment that allows us to fail and try new things? I’ve invested my time to build trust across the organization, solicit opinions, and understand what sales look like. If the internet doesn’t work, we can’t process transactions in the stores. If all the little components don’t work together, we can’t deliver the experience we want. So for us, it’s important to have our different teams get together and talk strategy in order to present one unified experience for the customer. My team and I actually go to our stores every month, not just as employees, but as customers. It’s easy to only consider the online customer when you work on the tech side of e-commerce. By going into the stores and distribution center, my team and I are able to think critically about the entire process from start to finish, including what happens once you place an order online and what that looks like in distribution centers. It’s easy to become disconnected, so it’s important to me that my team understands each moving part. The Barneys Downtown store in Manhattan. Image courtesy of Barneys. HVW: IT is transforming and evolving. Do you feel the CIO role is changing at the same speed? What have you observed as an innovative leader who is a part of a new wave of CIOs? MG: Different organizations have different CIO roles, but in general, we’re seeing CIOs take more of a leadership role in business transformation. There’s always a balance between what we aspire to do and the quality of what we actually do. The success for anyone in a CIO role is not about the number of things they do, but the quality of the things they do. I think pricing and e-commerce are just variables. For us, our focus is on how we actually get a full view of who our customers are and how we can meet their needs. We need to be able to deliver to all types of customers, whether they shop online, in-store or both. Our main goal for the upcoming year is to mimic on a mobile phone the same feeling customers get when they walk into our store. We’re paying attention to all details of the in-person experience so that we can translate them into the online experience. HVW: How have you tried to implement innovation at Barneys? MG: First of all, I strongly believe that innovation isn't just one person’s job — everyone needs to be committed to it. Near the end of October, we hosted an in-store and online event called thedrop@barneys . We dropped 30 capsule collections geared more towards younger customers and created an in-store experience that actually went viral because people were able to meet designers in-store while interacting with the brand and product. Traffic almost doubled in-store as well as online because of the amount of engagement. We were also able to introduce a lot of new customers to our database as fans of designers discovered we carry their brands. thedrop@barneys event. Image courtesy of Barneys. HVW: How do you stay up-to-date with current technologies and trends? MG: I tend to read about a variety of topics quite regularly, while also connecting with people who are problem solvers in a variety of industries. I talk with people who work in finance and solve security issues, as well as people who work in the auto industry. I like getting a perspective on other industries to see how they solve problems. HVW: Is there any brand whose operation methods you admire? MG: When it comes to brands I admire, I mainly think about where customers spend their time, because that indicates what they want. If I know a customer goes into our store and shops online, I look at how much time this customer spends with us. If a customer spends a certain amount of time online and doesn’t end up buying the product, that means we didn’t have the product selection they wanted. We’re not a discount company, and our customers buy at full price because we carry the selection they want. I myself shop at Barneys, and the thing I appreciate about Barneys is that it has taught me how to shop. Now, I enjoy seeing how I can give that same experience to our customers. HVW: At what moment at Barneys did you feel like you were accomplished? What was your “aha” moment? MG: Accomplishment is an ongoing process. For me, signs of success revolve around how my team works together and how we communicate. Are we willing to take chances? Are we willing to think outside of the box? On a macro level, successful team interactions translate to building great experiences for customers. The businesses that will succeed in retail are those that put the customer first, which is our ultimate goal.", "date": "2018-12-26"},
{"website": "Avenuecode", "title": "How to Create a Go Project on Google App Engine - Part 1", "author": ["Pedro Costa"], "link": "https://blog.avenuecode.com/go-appengine-hello-world", "abstract": "As Cloud computing continues to expand, I'm always looking to learn new things, and looking out for the next big thing on the web. But from time to time, I like to come back to Google to see how its Cloud Platform is doing - and I must say, Google seems to always have everything figured out. App Engine is a platform provided by Google as a service (PaaS) to leverage the easy development of cloud applications with flexibility and reliability. It abstracts away infrastructure, so you can focus solely on code, and also offers automatic scaling for web applications. In App Engine, applications are sandboxed and run across multiple servers. Google offers a free plan, that enables the basic usage of almost every Google Cloud feature. If you want to learn more, check out this link: https://cloud.google.com/free/docs/always-free-usage-limits . First Things First I'd recommend these articles if you're not used to GO packaging/configuration: Last but not least, you will need Google Cloud SDK, which can be downloaded here: https://cloud.google.com/sdk . Setting Up Your Cloud Project If you already know how to create an App Engine project, you can jump right to Creating Your App . If you're still learning, then the very first step is to open Google Cloud Console , then: 1. Click on ' Select a project' : 2. Click on '+' to add a new project: 3. Set your project name, which may differ from your project ID: 4. Now, your project is being created. It can take up to 5 minutes, so keep your eyes on the notification panel. As soon as the project is ready, you should be able to select it: 5. Now, open the App Engine dashboard ( https://console.cloud.google.com/App Engine) , and select Go as the project language: Then, you will be redirected to the location page. Select any location (I usually go with us-east1 ). After that, you will need to execute backend creation, which can take up to 5 minutes to execute. As usual, to be sure that the GAE project is working, we'll create a basic Hello World! app. In order t o do that, go ahead, and create a Github r epo for your code. Within this repo, I'll add two files: Assuming you have already installed Google Cloud SDK on your machine, you should be able to run the following command from your terminal: This will start a command-line wizard that will ask you to authenticate via your Google Account, and set your project. The third step should ask you to select the GAE project, or select the project created earlier. Now, it's finally time to test your app. In order to do that, first run this command from your project's root directory. If everything worked, you should see an output similar to this one: Now, go ahead and check your http :// localhost : 8080 . You should see the Hello World!!! message. Now that the app is working, we can deploy it to see it run on GAE effectively. Run this command from your terminal: That's it! If you're error free on the last command, then you should see the app URL. Alternatively, you can run this command to open your browser on the app URL: As I mentioned at the beginning of this post, Google makes it easy for you to get your services up and running on its platform. The project setup is super straight-forward, and the outcome is pretty amazing. I hope I was able to help you create your basic project on GAE. I'll be publishing a new article in the near future showing how to create a basic REST service and put it onto GAE, so stay tuned!", "date": "2017-11-14"},
{"website": "Avenuecode", "title": "((All Those Parens!))", "author": ["Matheus Eduardo Machado Moreira"], "link": "https://blog.avenuecode.com/all-those-parens", "abstract": "Functional programming paradigm and languages are usually associated with core features like pure functions and immutable data (structures) 1 , and other properties like referential transparency, higher-order (first order) functions, lazy evaluation , and recursion 2 . They arguably provide programmers with better tools and abstractions to deal with difficult actual problems that involve concurrency, parallelism, big data, etc. Despite all those important characteristics, the most commom remark I hear about functional languages when talking about them with friends and co-workers is: \"What a strange syntax! And look at all those parentheses, they must make the code very hard to read and write!\" So let's try to demystify this \"strange\" syntax and see what benefits it can give to developers. The kind of \"strange\" syntax we are talking about is commonly associated with LISP 3 and derived languages. It is easily recognized by the usage of parenthesis to denote function call/application (together with prefix notation , in which the operator is placed before its operands) and as scope delimiters (instead of { and } , for example). LISP uses a notation called S-expression (for Symbolic expression). It is a notation for nested list data and is used for source code as well as data representation 4 . The authors of the excellent book SICP ( Structure and Interpretation of Computer Programs ) 5 introduce s-expressions in a simple way in section 1.1.1 Expressions . Expressions representing numbers may be combined with an expression representing a primitive procedure (such as + or *) to form a compound expression that represents the application of the procedure to those numbers. For example: (+ 137 349) 486 (- 1000 334) 666 (* 5 99) 495 (/ 10 5) 2 (+ 2.7 10) 12.7 The authors call those expressions combinations . Combinations can accomodate procedures that may take an arbritary number of arguments, can be nested, and do not suffer from ambiguity: (+ 21 35 12 7) 75 (+ (* 3 5) (- 10 6)) 19 Those characteristics are already an improvement over languages that use infix notation: it is not necessary to repeat the operator between operands (e.g. 21 + 35 + 12 + 7 ) and to remember operator precedence (e.g. multiplication is evaluated before addition). Wikipedia defines homoiconicity (also know as code-as-data ) as \"a property of some programming languages in which the program structure is similar to its syntax, and therefore the program's internal representation can be inferred by reading the text's layout.\" 6 Put another way, a homoiconic language uses the same ( homo ) representations ( icons ) for source code and data structures. Most languages need to read a program's textual representation and parse it into something that can be compiled and/or evaluated. This \"something\" is known as AST (Abstract Syntax Tree), a data structure that represents formally what is expressed in the source code. The following image is a sample syntax tree for the expression requiresRole && userHasRole(\"MODIFY\", assetId) 7 . Most languages do not provide a straightforward way to manipulate the syntax tree - it is an internal representation only accessible to the compiler during compilation, for example. Homoiconic languages, on the other hand, have programs written using data structures that represent the syntax tree directly. The same requiresRole ... code can be written as (and requiresRole (userHasRole \"MODIFY\" assetId)) . Note how the code structure resembles the syntax tree. Homoiconicity enables higher level metaprogramming facilities that cannot be attained by languages that do not exhibit this property. Higher level metaprogramming means higher level abstractions, which frequently translate to less code, more work done, and a lower margin for mistakes. Let's take a look at some interesting examples of what can be accomplished through this powerful feature. (The Clojure 8 language will be used in the examples, but the ideas and capabilities are shared among other LISP-like languages.) Java 5 added a new syntax, know as enhanced for loop , to the language. It enabled developers to rewritte an \"old style\" for loop as The new syntax is easier to read and write, and hides complexities (such as index manipulation). Java programmers had to wait until Java 5 to use the enhanced for loop because this addition required a change at the compiler level, and most programmers don't have the knowledge or ability to make it. Clojure's metaprogramming workhorse is the macro 9 . A macro is a special function that is executed during compilation and whose arguments are unevaluated code . The arguments are data structures that represent code! Given that m is a macro with a single parameter, the call (m (+ 1 2 3)) will result in the parameter's value being (+ 1 2 3) 10 , not the value 6 . With macros a programmer could define in a few lines a foreach 11 construct, similar to Java 5's enhanced for loop, without touching the compiler or waiting years for it 12 : S-expressions sometimes make writing and reading chained function calls a bit cumbersome because we need to write and read the expression from the inside out: In natural language we would say: \"Reverse the vector [1 2 3] , append 4 to it and print the result. It would be nice if we could write code similar to what we just wrote in natural language: thread would be a form that passes the first value ( [1 2 3] ) through a series of method calls, passing the result of a previous call (e.g. (reverse [1 2 3]) ) as the first argument of the next call (e.g. (conj [3 2 1] 4) ). Clojure indeed has a macro that performs what was described: -> (thread first). But the point here is that -> is not a privileged language construct, it is a macro that we could have written ourselves: S-expressions, homoiconicity, and prefix notation are not weird characteristics of esoteric programming languages. They are well-thought out features that give rise to powerful tools at a programmer's disposal. With proper care, we are able to write code at a higher abstraction level, leading to less verbosity, more meaning (think DLSs 13 ), more maintainability, and better code structure in general. If you are interested in learning the functional programming paradigm, my humble recommendation is the SICP book. The authors use the Scheme programming language 14 and present the most important functional paradigm features and characteristics through several examples and exercises of increasing difficulty 15 . Those who have an object-oriented background, particularly with Java, may find Scala 16 and Clojure very appealing. https://clojurefun.wordpress.com/2012/08/27/what-defines-a-functional-programming-language/ ↩ https://en.wikipedia.org/wiki/Functional_programming#Concepts ↩ https://en.wikipedia.org/wiki/Lisp_(programming_language) ↩ https://en.wikipedia.org/wiki/S-expression ↩ https://mitpress.mit.edu/sicp ↩ https://en.wikipedia.org/wiki/Homoiconicity ↩ Example extracted from the book Clojure Programming , by Chas Emerick, Brian Carper, and Christophe Grand. ↩ http://clojure.org ↩ http://clojure.org/reference/macros ↩ In the macro context, (+ 1 2 3) is a list with four elements: the symbol + and three numbers. At runtime, (+ 1 2 3) is a function call . It is easy to see, then, the expression's \"dual\" nature: data structure (a list) and (executable) source code. ↩ During compilation, macros are recursively called to generate code that, in the end, does not have macro calls (only function or special form calls). In this sense, we could say that the compiler performs a macro expansion , in which one code (the macro call) is expanded into something else (a combination of function and special form calls). For example, the macro call (foreach [x [1 2 3]] (println x)) would be expanded to something similar to: ↩ 12. Clojure programmers do not need to define the foreach macro, since the language already has iteration facilities such as doseq . ↩ 13. https://www.gnu.org/software/mit-scheme/ ↩ 14. There are a lot of references and resources about SICP online. Particularly useful are sites where it is possible to find                                         solutions for most of the exercises, sometimes with different implementations. But don't cheat! Try hard to solve the                                         exercises by yourself before searching for someone's else solution. It will pay off! ↩ 15. http://scala-lang.org/ ↩", "date": "2016-11-9"},
{"website": "Avenuecode", "title": "AC Spotlight - Charlie Cole", "author": ["Holly Camponez"], "link": "https://blog.avenuecode.com/ac-spotlight-charlie-cole", "abstract": "This week we had the pleasure of chatting with Charlie Cole, Chief Digital Officer and VP at Tumi. Charlie joined Tumi just under two years ago, ahead of their acquisition by Samsonite last August. Having spent an impressive career in e-commerce and retail, Charlie shared with us his unique insights on how retailers can survive and thrive in a rapidly changing market. Avenue Code: Charlie, we’re hearing more and more about how the way people shop is changing rapidly. What changes have you observed in consumer behavior over the last several years? Charlie Cole: The obvious change is that people are buying more online, specifically via mobile. I think mobile as a channel has only begun to catch up as a transactional tool vs. an informational. At this point it’s still only indexing about half to one third of purchases, but it’s catching up quickly. Mobile payments aren’t yet ubiquitous, but very quickly they will be. The other big shift in shopper behavior is that consumers have gotten a lot smarter about webrooming - researching products online prior to purchase - and finding the relevant information regardless of channel. So, responsible retailers have to find their niche, know what they excel at, and commit to it. Avenue Code: Tell us about that in regards to Tumi. Was there a moment where you knew that the brand was on track? Charlie Cole: When I joined Tumi in January ‘16, it was clear that we had lost our way as a brand from a digital perspective. I’m not shy about telling this story, because it’s something I truly believe in and has become my #1 talking point. If you look at the 2015 calendar year, there were 110 days on which we were offering some kind of promotion on tumi.com. As a high end brand, that’s not sustainable. So, we took a huge leap of faith and ripped off the bandaid by taking away the vast majority of the promotions. This was a bigger step than someone outside of retail might initially be able to fathom. As an industry, retail is obsessed with incrementality. So when you eliminate the tool that propped up your sales throughout the prior calendar year, and you don’t see that incremental week over week, month over month increase - that’s a scary reality to face. But our hypothesis was - and we had real buy-in from our CEO to give this a shot - that when we got to holiday, the demand would be there. Sure enough, we made it to Black Friday and Cyber Monday, and we started seeing volumes that were up 50% - 80% from the previous year. This was a huge confirmation that we had successfully rehabilitated the brand. That first year was really hard - but already this year we’re up close to 40%. Avenue Code: Interesting. Would you say that an over-reliance on promotions gets in the way of success when it comes to today’s retail environment? Charlie Cole: In a way, yes. I think if there’s a single ingredient to success, it would be protecting your brand from commoditization. For brands and retailers who have become super commoditized, I’m really not sure how they will disintermediate from marketplace sellers and resellers. The really critical thing is to understand your brand and your niche. Going back to the previous point about webrooming, think of yourself as a consumer - what are you looking for? For Tumi, we have a heritage and well-known brand. We’re known for quality, and the differentiator cannot be price. You can’t win on price in today’s market, especially as a high end brand. Avenue Code: You’ve got an impressively varied background as an executive leader in the digital and e-commerce space, having held positions at Lucky Brand, Schiff, The Line, and many more - in your own words, an eclectic career! What made you choose Tumi as the next step in your career, and what keeps you there today? Charlie Code: Really for me the most important thing was that when I spoke with the CEO, I learned that he was willing to reinforce the premium brand, and expand it internationally. Then, with the acquisition by Samsonite, the ability to operate with a portfolio strategy is really exciting. Suddenly, you have 14 brands with a continuum of price points. We’re able to serve consumers with a very targeted experience. Avenue Code: Charlie, you’re a frequent traveler yourself. What’s your favorite piece of luggage? Charlie Cole: That’s right! I spend every other week at one of our offices, which means I’m traveling between New Jersey, New York, Seoul, and Santiago all the time. I have an Alpha Bravo that I LOVE. The front functionality has two little pockets, perfect for passports and cell phones. Avenue Code: It’s easy to hear your passion for Tumi as a brand, as well as your commitment to upholding the integrity of its premium nature. It sounds like taking the plunge into a more value-centric messaging has turned out to be well worth the risk. We’ll keep our eyes on Tumi and the Samsonite portfolio - we’re excited to see what comes next!", "date": "2017-8-1"},
{"website": "Avenuecode", "title": "AC Spotlight - Andrew Berg", "author": ["Ashley Wang"], "link": "https://blog.avenuecode.com/ac-spotlight-andrew-berg", "abstract": "After hearing Andrew Berg speak at Luxury Interactive, we had to learn more. Andrew is President at Robert Graham, a self-described “American Eclectic” purveyor of luxury menswear. Andrew’s vision for a brand that encapsulates the feel of luxury while maintaining a vibrant personality was contagious, and we loved hearing his strategy for brand cohesiveness. Avenue Code: Andrew, we loved connecting with you at Luxury Interactive! A key takeaway for us is that luxury retailers are adapting to the digital age by doing what they’ve always done best: centering the customer experience. How do you put this into action at Robert Graham? Andrew Berg: We strive  to put our customer first in every conversation we have as a brand. Along those lines, every consumer touchpoint must be consistent. Whether it’s brick and mortar or digital, the brand experience is cohesive and authentic across channels. This translates to empowering the consumer to choose their preferred channel of interaction with the brand - whether they prefer to experience Robert Graham in person, via email communications, or on social media or our website. The consumer can choose - it’s our job to make sure the experience is comfortable and consistent for them. For example, we’re currently showcasing our Downhill sport coat across channels - walk into any store, and you’ll see it on the front mannequin. We’ve emailed our subscribers, and you can find it featured on Facebook and as a trio of images on Instagram. In parallel, we’re doing an outpost at Bloomingdales focused on the coat. To me, this is how brands today cut through the clutter and noise and really make an impact - universal consistency through all touchpoints. Of course, it’s much easier said than done. You’re dealing with hundreds of SKU’s, you need to coordinate marketing initiatives around the website, and at any given moment other opportunities will come up that may conflict with the expressed plans of your current campaign. But this consistency is at the core of what can make brands today win or not. A sneak peek of the Spring 2018 catalog from Robert Graham. Image courtesy of Robert Graham. AC: The Robert Graham style is self-described as “American Eclectic”, and strikes a balance between colorful and expressive and old-world luxury - notably with the exclusive look and feel of your Collector’s Club. How do you balance this juxtaposition? AB: I see this balance as speaking to the core of our value proposition. The beauty of Robert Graham is that our collectors get to experience the brand based on how it feels to them. We’re a distinctive brand, of course, with our own unique point of view. But the reason it resonates so powerfully with our collectors is it allows them the opportunity to express their individuality through the brand. For some it’s the eclectic look, for others, it’s being part of an exclusive club. For most, it’s a combination of the two. This is an intentional result of everything we do, from marketing, to design, to production. That’s how we’ve set out to create an authentically luxury company with a bold, distinctive personality. We’ve also broken the rules that luxury has to fit into a traditional mold. Of course, before you can break the rules, you have to first master them. Robert Stock, our founder, has been a menswear designer since the 1960’s, moving in the same circles as more traditional menswear brands such as Brooks Brothers and Ralph Lauren. Knowing those rules, and then intentionally breaking them, was his strategy from the beginning of Robert Graham’s launch in 2001. AC: As a younger company than some, do you feel you’re at an advantage over some of these iconic brands when it comes to keeping up with the digital transformation of retail? AB: In a way, I suppose the fact that we’re only 17 years old as a company is a benefit in the sense that we aren’t so ingrained in the old way of doing things and weighed down by decades of legacy infrastructure and processes. At the same time, every year is a renaissance around new technologies so in today’s world a 17 year old company is by no means young!  We came of age in the middle - we’re not new in the same way that a digital startup is approaching the business, but then again I don’t think only companies that are a few years old can innovate and transform retail. I think about Uniqlo as a great example of an older company that continues to innovate. It’s been in business for decades but has remained innovative in the brick and mortar space in terms of merchandising, consumer experience, and product. I previously worked at Theory, owned by Fast Retailing, whose CEO preached the philosophy of change or die. With great leadership and processes, a company of any age can be cutting edge and a great innovator. AC: Andrew, in your role as President, what is your primary challenge in staying abreast of innovations in tools and technologies? AB: I think the primary challenge is the sheer pace of the changes that are happening, in both technology and the retail landscape. Just when you’ve gotten a company up to speed, there’s something new to consider. It’s critical to have the right team in place to help you evaluate all the different opportunities at your fingertips, while keeping a close eye on the core operations of the business. AC: To your point about having the right team in place - what do you look for in your strategic partnerships? AB: It has to start with partners who have a track record of successful execution in the relevant area of where you want to go and what you’re looking to achieve. Regardless of their tenure and accomplishments, it’s essential that they have relevant experience. Of course, there are times when a technology is so new - machine learning, for instance - that there simply isn’t much to point to. In those scenarios, it’s crucial that you have trust in the relationships and the people you’ll be working with. There are instances where we run parallel paths in order to evaluate different partners who come to the table, and quickly learn who’s going in the right direction. AC: What was your biggest “Aha!” moment in the last 6 months? AB: If you’ve ever done one of those spin classes, you’ll know that each bike has a resistance knob which is really the key to the whole workout. The interesting thing is the resistance knob is a personal decision of whether you turn it up and how far. From an outsider’s viewpoint, two people could be doing the same pedaling, standing, and other movements, but if one of them has turned up the resistance knob, they’re getting a much better workout and return on their time. My recent aha moment was when it hit me that many brands have websites full of pages, product, and content. But how much work goes into optimizing that channel, while not always visible from an outside or consumer point of view, is of the utmost importance. What’s behind the scenes matters - whether it’s having the right technology, personnel, business processes,  personalization, or the right dedication to constant evaluation to get the best return on your time. Just because it looks like you’re in business, it doesn’t mean that you’re really “in business”. AC: So true! I think that will resonate with anyone who’s put extensive work into behind-the-scenes optimization. Thank you so much for sharing your insights with us, Andrew! It was great to get a glimpse into your vision regarding brand consistency, innovation, and integrity. Looking forward to seeing what’s next from you and Robert Graham! This article is property of Avenue Code, LLC, and was originally published with permission at Total Retail .", "date": "2018-1-3"},
{"website": "Avenuecode", "title": "Prototype Inheritance in JavaScript", "author": ["Rafael Bicalho"], "link": "https://blog.avenuecode.com/prototype-inheritance-in-javascript", "abstract": "JavaScript uses a prototypal inheritance model, and this can be a little confusing for developers coming from class-based languages such as Java or C++. Prototype inheritance is simple but yet a powerful model. In this post, we are going to talk a little bit about it and how it is built in JS. If you haven't used a prototypical model before, don't be intimidated - we'll break it down! Each JS object has a property called [[Prototype]] , which holds a link to another object referred to as the object's prototype. This prototype object has its own prototype, and so on and so forth. When trying to access a property of an object, JavaScript will look at the object itself first. If it doesn’t find the property, it looks in that object's prototype. If it doesn’t find it in the prototype, it looks in the prototype of the prototype, and so on until a property with a matching name is found or until it reaches an object with null as its prototype, meaning that the prototype chain is over. The [[Prototype]] property is accessed using the accessors Object.getPrototypeOf() and Object.setPrototypeOf() . It can also be accessed via the property __proto__ , which is non-standard but implemented by many browsers. For the sake of simplicity, we’re going to use __proto__ from now on in this article, as shown below: Note that the code above is merely a simplified version of the following: Object is called a constructor function, which is a function used to create objects. (If you want to learn more about the different ways we can create objects and the resulting prototype chain of each method, be sure to read next week's Snippet on Object Creation in JavaScript.) In JavaScript, functions have a special property called \"prototype\" (not to be confused with the object property __proto__ ), and this is used when the function is called with the new keyword in order to set the prototype of the object being created. In fact, if we check the value of Object.prototype, we get the same result as the x.__proto__ . To make things clear, let's look at another example. First, we’ll create a function called “Person.” This will be our constructor function, and in JavaScript, the first letter of a constructor function is capitalized by convention. As we’ve seen before, Person has a property called prototype . So we are going to create a function called greetings inside of it: Now we’ll create an object using our constructor function: If we check johnDoe.__proto__ , we will see our greetings function there. When we call johnDoe.greetings() , the interpreter will check johnDoe’s properties. Since the function is not defined there, it will check its prototype (referenced by the __proto__ function), and then we’ll get the expected result: Now, let's find out what happens if we create our object and then change our constructor function after that, like this: What do you think will be the output? … If you said “Hello! =),” you are correct! This is because, as we said before, the __proto__ property is a reference to the object’s prototype and not a copy of it. You can also check that johnDoe.__proto__ === Person.prototype is true , meaning that both properties point to the same place. Happy coding!", "date": "2019-3-6"},
{"website": "Avenuecode", "title": "How to Use Terraform to Create a Virtual Machine in Google Cloud Platform", "author": ["Thiago Costa"], "link": "https://blog.avenuecode.com/how-to-use-terraform-to-create-a-virtual-machine-in-google-cloud-platform", "abstract": "Interested in developing cloud infrastructure and automating repetitive tasks? This blog is for you. Today, we're going to show you how to get Google Cloud Platform and Terraform to work together. Then we'll create a virtual machine in just a few simple steps. Let’s suppose you need to create many servers, all of which have different memories, disk sizes, and operating systems. This will take hours to set up, and we might make some mistakes during the process. Fortunately, we have tools like Terraform that allow us to turn a little bit of code into something that can plan, deploy, modify, and destroy all of our systems. Instead of modifying an existing system using SSH, which is a mutable process, Terraform allows your systems to be rebuilt from a well-reviewed template, validated for correctness, and then deployed if they pass all the required checks. Now let’s walk through some basic examples, define some important terms, and talk about the benefits of using Terraform. Here are our basic terms and technologies: The first thing you’ll want to do is install Terraform. If you're working on Linux... $ wget -q https://releases.hashicorp.com/terraform/0.11.6/terraform_0.11.6_linux_amd64.zip $ unzip terraform_0.11.6_linux_amd64.zip $ sudo mv terraform /usr/local/bin/terraform $ terraform version If you're working on Mac... $ brew install terraform $ terraform version Next, you'll want to download a sample project to create a virtual machine on GCP. Below, we'll explain how to run it. Clone project locally: $ git clone https://github.com/thiagofernandocosta/gcp_vm-in-few-steps && cd gcp_vm-in-few-steps A service account is a way to give granular access to a vendor or someone else. It's really useful when you want to give specific resources to a group or user. After you create your account on Google Cloud, you should create a service account that will access Google Compute Engine (GCE). This is needed to create and handle a virtual machine. The JSON file you just downloaded should be protected from non-authorized users. This is a private key or password to manage your infrastructure’s resources. For development purposes, we can add a .gitignore file to our project, adding credentials.json so that it’s not versioned to our repository. These steps are shown below: Now we’re almost able to create a virtual machine instance! Perhaps you noticed that the project cloned above contains files that end in something.tf. These files belongs to Terraform. This name format allows Terraform to know which files to work with when initializing, planning, applying, and destroying. provider.tf: This file contains the configurations needed for provisioning a resource on GCP. Notice that credentials.json is not versioned in our project; it was built during previous steps. create-instance.tf This file contains the resource's configurations on GCP that we want to run. In the first section resource \"google_compute_instance\" \"default\" We're describing information about our virtual machine, such as type of image, scripting to execute when bootstrapping, and tags to identity this resource. In the second section resource \"google_compute_firewall\" \"http-server\" We're describing information about our firewall and allowing access to a specific port and its protocol. If you noticed, there is information about our target. This target aims all resources that contain this tag. In other words, our virtual machine instance will be accessed through the internet because we allowed this through the firewall configuration. In the last section output \"ip\" Our output will be an external ip that will print a message configured on metadata_startup_script You can access further details here . This command sets up the environment. This command reports which configuration will be applied. This command approves the changes automatically and applies the configuration defined on Terraform files. Counteracting the command above, this removes everything created. Doing things manually is inefficient and can also cause misconfigurations. Terraform provides infrastructure as code in an easier way, has a simple syntax, which helps management, and enables multi-cloud provisioning. I hope this guide has given you a simple example of the principles behind Terraform and how powerful it can be!", "date": "2019-11-13"},
{"website": "Avenuecode", "title": "How to Build A Recommender System In Less Than 1 Hour", "author": ["Hossein Javedani Sadaei"], "link": "https://blog.avenuecode.com/how-to-build-a-recommender-system-in-less-than-1-hour", "abstract": "Ever suffered from information overload? Finding simple pieces of information can feel like a daunting task. With all the choices and sources of information available on the internet today, it's absolutely necessary to find a way to prioritize, filter, and professionally deliver appropriate information. Recommender systems resolve this problem by searching inside large amounts of generated information to provide users with personalized services, information, and content. In very simple words, a recommender system is a subclass of an information filtering system that predicts the \"preference\" that a user would give an item. Collaborative Filtering (CF), filters information by using the recommendations of other individuals. It's based on the logic that people who agreed in their assessment of some items in the past are expected to come to the same consensus again in the future. An individual who desires to read a book for instance, may ask for recommendations from friends. The recommendations of certain friends who have similar interests are trusted more than recommendations from others. This information is used in deciding which book to read. 1 Advantages: -They can be useful for any domain and in addition to domains -CF engines work best when the user space is large -The selling point of CF engines is that they're totally independent of any machine-readable representation of recommended objects, and work well for complex objects such as music and movies, where differences in taste are accountable for much of the variation in preferences (Good, Nathaniel, et al) 2 . Drawbacks: A Content-Based  (CB) filtering system selects items based on the correlation between item content and user preferences as opposed to a CF system which chooses items based on the correlation between people with similar preferences (Meteren, et. al) 3 . 4 Advantages: - Does not require data from other users -No cold-start or sparsity problems -It can provide recommendations to users with unique tastes -It can recommend new and unpopular items -No first-rater problem -Can provide explanations of recommended items by listing content-features that caused an item to be suggested Disadvantages: Hybrid recommender systems combine two or more recommendation methods, which results in better performance with fewer of the disadvantages of any individual system. Typically, CF is combined with another method to help avoid the ramp-up problem. For example, a weighted hybrid recommender is one in which the score of a recommended item is calculated from the outcomes of all the available recommendation methods present in the system. So then, the simplest combined hybrid would be a linear combination of recommendation scores (R. Burke) 5 . Are recommender systems useful? In 2006, Netflix offered a one million dollar prize for anyone who could advance their recommender system algorithm by a minimum of 10%. There were over 44,000 entries from over 41,000 teams on behalf of approximately 51,000 contestants. On 21 September 2009, the grand prize of US$1,000,000 was given to the BellKor's Pragmatic Chaos team who used tiebreaking rules (S. Lohr) 6 . By decreasing the error of their recommender system, Netflix was able to boost business and create profit tremendously. It's clear, then, the value that a great recommender system brings for users. But how can we create them? After learning a bit about recommender systems, it's time to build a simple one using AML. For this, we need to have an AML account, which is free, and some data. The MovieLens dataset is perfect for our purposes - it is a stable benchmark dataset, composed of 1 million ratings from 6000 users on 4000 movies and released 2/2003. GroupLens Research has collected and made rating data sets available from the MovieLens web site ( http://movielens.org ). The data sets were collected over various periods of time, depending on the size of the set. These files contain about 1,000, 000 anonymous ratings of approximately 3,900 movies made by 6,040 MovieLens users who joined MovieLens in 2000. All ratings are contained in the file \"ratings.dat\" and are in the following format: UserID::MovieID::Rating::Timestamp -UserIDs range between 1 and 6040 -MovieIDs range between 1 and 3952 -Ratings are made on a 5-star scale (whole-star ratings only) -Timestamp is represented in seconds since the epoch as returned by time (2) -Each user has at least 20 ratings Uploading the Required Dataset to AML First, extract the downloaded file and select rating.dat from the listed file. The format of this file is “dat”, so we need to convert this file to a CSV format. After that, select the DATASETS module from the AML main menu, click on new icon on the bottom-left corner of the page, then upload the converted file from the local storage of your PC to AML.  While uploading, select the file type as CSV without a header, and name that file “rating”. It will take some time to upload. Create a Blank Experiment in AML In this step, we will create an experiment in our default workplace. Go to the experiment tab, and then create a blank experiment like below: Name your experiment at the top of the page. For example: “Recommender Systems”. Importing the Dataset in the Experiment From the left hand-side menu, open saved datasets and drag your uploaded dataset ,i.e., “rating.csv” from my datasets. You can see some information about this file by right-clicking on the reader module and selecting Visualize from the menu. As shown in the picture above, there are 4 columns in the dataset with 1,048,575 rows.  The first column is related to the user ID, the second column is for the movie ID, the third column corresponds to ratings, and the last column is for the timestamps. In this experiment, we will only be using the first three columns. Select Column in the Dataset We can select the three required columns using  the specific module in AML called “Select Column in Dataset”. Just search the name of this module in the search box on the top-left of your experiment, and then drag and drop it into your experiment environment in the provided space. Then, select this module and click on “Launch column selector” in the right hand side panel at the same time. Next, select corresponding columns related to user ID, movie ID,  and rating  while excluding timestamp as shown below: Edit the Metadata Now, it's time to give proper names to the already selected columns and define the data type. In order to do this, perform a search for the module named “Edit Metadata”. We'll do the whole procedure using two of these modules in two steps. First, drag Edit Metadata from the left-hand side menu, and after selecting that, try to configure it as shown in right-hand side panel of the figure below: Next, select two columns and change their type to string. They're also selected as features. Then, we have to change the name of  “Edit metadata” to “rating” in the next column as well as change its type to float and field to label. See the snapshot below: In order to ensure that everything is going smoothly, we can run what we've developed so far by clicking on the ‘Run’ button on the bottom horizontal menu. The red tick next to each module shows that the running of that part has been finished with no error. The output can be seen by right-clicking on the module added last, and then visualizing that content. The output should be something like this: Splitting the Dataset Into Training and Testing Sets Now that the data processing part is finished, everything is prepared for building the recommender system. While building any machine learning model, including the current one, we need to make sure that the model is valid for training and testing data. So, in this step, we need to split the dataset into training and testing datasets. For this reason, we can add a “Split Data” module from AML, and configure it as follows: Note: Be informed that these configurations are unique to recommender data and control how values are divided among training and test sets, or among training and scoring sets. In all cases, you specify a percentage represented by a number between 0 and 1. The explanation of configurations is as follows: Training the Model After splitting data into training and testing sets, it's time to select a proper model and then train the model. Fortunately in AML, we can easily train a recommender system by using the “Train Matchbox Recommender” module. The Train Matchbox Recommender module reads a dataset of user-item-rating triples and, optionally, some user and item features. It delivers a trained Matchbox recommender. You can then use the trained model to generate recommendations, find related users, or find related items by using the Score Matchbox Recommender module. More information on how this model works can be found on the AML official website in this link .  We also need to add a “Score Matchbox Recommender” for prediction purposes as well as an “Evaluate Recommender” to evaluate and validate the model. These modules must be connected as shown in the snapshot below: Running the Model Now that the building of the model is finished, it's time to run the model and test its accuracy. Simply save the model and then run it. The whole model should be similar to the following model: Evaluating the Model After running the model, we'll have following results in the “Evaluate Recommender” model. So, by using this simple model we were able to reach 88% accuracy with the testing data set, which isn't too bad. If we want to boost the results, we need to use a more advanced model and datasets (e.g., hybrid models and other datasets available in a downloaded bundle). Creating Web Services Now we've finished the model development process and can create a web service to use this model in real application.  In order to setup a web service option, simply select the “Predictive Web Service” button. AML will automatically complete all required tasks and create the model for you, ready to be deployed as a web service. The output after completing this should be similar to below: Deploying as a Web Service For deploying the model, it's better to run the experiment one more time and then deploy the web service. The output should be like this: In this blog, we employed AML to build a recommender system using data related to movie ratings. In the end, we were able to reach 88% accuracy in the testing set. Moreover, we created and deployed a web service as well. 1 \"1 Collaborative Filtering Rong Jin Department of ... - SlidePlayer.\" http://slideplayer.com/slide/5005595/ . Accessed 21 Nov. 2017. 2 \"Combining collaborative filtering with personal agents for better ....\" 18 Jul. 1999, http://dl.acm.org/citation.cfm?id=315352 . Accessed 21 Nov. 2017. 3 \"Using Content-Based Filtering for Recommendation - CiteSeerX.\" http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.25.5743&rep=rep1&type=pdf . Accessed 21 Nov. 2017. 4 \"How do Recommendation Engines work? And What are the Benefits?.\" https://www.marutitech.com/recommendation-engine-benefits/ . Accessed 21 Nov. 2017. 5 \"Hybrid Recommender Systems: Survey and Experiments ... - CiteSeerX.\" http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.88.8200&rep=rep1&type=pdf . Accessed 21 Nov. 2017. 6 \"A $1 million research bargain for Netflix and maybe a model for others.\" 21 Sep. 2009, http://www.nytimes.com/2009/09/22/technology/internet/22netflix.html?pagewanted=all . Accessed 21 Nov. 2017. 7 \"MovieLens 1M Dataset | GroupLens.\" https://grouplens.org/datasets/movielens/1m/ . Accessed 21 Nov. 2017.", "date": "2017-11-30"},
{"website": "Avenuecode", "title": "Your Guide to Organization-Wide Digital Maturity, Pt. 5", "author": ["Chrystiane Symoes"], "link": "https://blog.avenuecode.com/your-guide-to-organization-wide-digital-maturity-pt.-5", "abstract": "In our previous articles , we've provided step-by-step guidelines to help your organization add business value by increasing digital maturity in three key areas: business strategy , design services , and digital evolution enablement . In our last article, we'll introduce a way to accelerate your digital maturity to realize results faster. Coach by Emergence (CBE) is a trademarked Avenue Code methodology that accelerates organization-wide digital transformation. A coach by emergence is a role-specific, hands-on mentor who works diligently within the team to seed Agile best practices for a specific role until the relevant team members emerge as thought leaders. This methodology pairs Avenue Code coaches with specific roles inside a company. The CBEs are distributed throughout the organization as needed. They first understand the company culture and then model how to perform the responsibilities of each role within an Agile framework. Coaches by emergence are catalysts. Companies can leverage CBEs to enhance the technical skills of specific roles as well as to create or improve an Agile culture in order to accelerate company-wide digital transformation. Avenue Code deploys coaches throughout the organization to be paired with delivery teams, product managers, finance, HR, business development, stakeholders, etc. These coaches work within the organization, mentoring each team for a period of about two months (or 2-4, two-week sprints). In the DevOps team, they enable the cloud environment, build the pipeline, and train the team. They are also responsible for release training maintenance handover, ensuring the continuous measurement of results and feedback to improve processes. In addition, they are embedded in human resources to encourage a flattened hierarchy, within finance to look at both quantitative and qualitative budget considerations, and within every team in the organization to enhance specific functionalities. In this way, an organization-wide transformation can take as little as eight weeks, adding huge value up front and creating a method for ongoing improvement afterward. In nearly every industry, the importance of digital evolution for continued market viability and improved competitive advantage is rapidly increasing. A successful digital evolution depends first on a solid business strategy, and then on accurately defining high ROI opportunities through design thinking-oriented services. Finally, the best results are realized when every area of the company has the right mindset, methods, and technologies in place to capitalize on business opportunities. Within today’s globally uncertain environment and increasingly competitive marketplace, a high level of digital maturity is more important than ever, as it enables companies to reinvent their products and services. If you'd like to keep this blog series as a reference, you can access the full whitepaper for free here . Avenue Code has accelerated digital innovation for Fortune 100 companies and enterprise org anizations in every vertical since 2008. Over the years, our consultancy has developed a highly successful method of kickstarting the next phase of our clients’ digital evolution journeys in just 90 days. Here’s how it works: Innovation is a journey, and there’s always a next step to be taken. If you’d like to understand more about your organization’s level of digital maturity and explore personalized transformation opportunities, we invite you to schedule a free consultation with one of our in-house experts. Let's Talk!", "date": "2020-12-30"},
{"website": "Avenuecode", "title": "AC Spotlight - Lorri Rowlandson", "author": ["Nareeman Jamal"], "link": "https://blog.avenuecode.com/ac-spotlight-lorri-rowlandson", "abstract": "Lorri Rowlandson , Senior Vice President at BGIS, shares her engineering-based approach to innovation strategy in the Proptech industry. Avenue Code: Tell us about your personal career path and how you got to where you are today. Lorri Rowlandson: My career path was not a straight line. I wanted to be a teacher, and I lasted all of one day. Instead, I ended up working as a summer student at IBM; I quickly realized how much I love business, so I stayed. Since then, I’ve worked with large, complex outsourcing arrangements, leveraging respective abilities and ensuring win-win collaborations. I have a strong background in operational expertise, and I also have a very curious mind. Together, these skills translate well to leading strategy and innovation. AC: Tell us about your role and your team - what does your day-to-day look like as the SVP of Innovation and Strategy at BGIS? LR: I have the best job in the world - it’s so much fun! I’m a self-confessed tech and real-estate nerd, and I love our profession. I’m BGIS’s global sponsor for innovation, which means solving problems and adding value in multiple geographies and contexts, from the US to Canada, Australia, New Zealand, and the UK. BGIS is an unapologetically operational company built for recurring business and facilities management. My team specializes in value-add offerings integrated into our core business. We’re guided by practical innovation. I notice that when many companies talk about innovation, they’re talking about far-out ideas that sound like the futuristic inventions of SciFi. At BGIS, we look at what can be implemented in one to three years and move the needle quickly. Targeting real business cases helps us gain quick approval for our solutions. AC: How would you describe your innovation strategy? What is your method for identifying opportunities and evaluating ROI? LR: Our innovation program is very closed-loop. It isn’t innovation theater for marketing purposes; it’s a highly accountable and evidence-based system. We start by helping our accounts understand the art of the possible. The first thing we do is ensure understanding of the problem we’re trying to solve and how we’ll measure success. This can be a challenge since many organizations get caught up in the Shiny Object Syndrome and jump to the solution without fully understanding the problem. With our problem and success metrics clearly defined, we run pilots, measure results, debrief, and continuously improve. Each quarter, I report to our CEO on the progress of our implementations. Together, we ensure our solutions are fully managed at every chevron of the process and that they’re outcome-based. AC: What are common pitfalls you see around innovation, and how can they be avoided? LR: A lot of organizations struggle to approve innovation solutions. Why? This goes back to my earlier comment: if you don’t anchor the solution in a real problem, it’s hard to get approval. Foundational work is key. Another pitfall is confusion. There are too many solutions, and they’re oriented the wrong way. Stakeholders don’t want to sign into 50 different dashboards to read their choices. Be selective about the solutions you propose. Finally, a lot of technology for the built environment has an asset view, meaning it’s building centric rather than occupant-centric, but ironically, a lot of buildings themselves are now focused on enabling occupant productivity. This can generate confusion for the occupant in making buying decisions. AC: What are your current digital projects at BGIS? LR: Right now, we’re very busy with features and innovations that pertain to health, safety, wellness, and wellbeing, especially as organizations are looking at returning to the office. We’re also well known for our sustainability efforts; our CEO is personally passionate about sustainability, and he’s won several awards in this space. Finally, we’re focused on efficiency. A lot of organizations were heavily impacted by COVID-19, and now that they’re returning to office spaces, we’re here to help them determine how to use their spaces in the most cost effective way possible. AC: What trends are you seeing in the wider real estate industry as a whole? LR: As of March 2021, one big trend is back-to-office strategies. Organizations are deciding who should come back and how, which means determining how much office space they need and how they’ll use it. Another related hot topic is building standards and certifications that promote wellness, wellbeing, health, and safety. A lot of organizations are investing in this area to evince their commitment to the health of their employees and to prepare for the future. Finally, I see a trend toward boutique coworking spaces that enable people to work outside the home environment in a flexible manner, whether that’s a coworking space sponsored by a hotel, a restaurant, or something else. AC: We saw that BGIS recently acquired Cormant. What role does data play in your upcoming offerings? LR: If you’ve ever worked with engineers, you know that they like to measure everything. Because we’re a very engineering-led culture, our decisions are data-driven. Data is significant not only in gaining insights but also in automating activities that typically require human intervention. AC: In your opinion, what is the key to a successful strategic partnership? LR: Rapport and communication. Internally, we hire for culture fit, high performance, and trust, and this culture is a big part of the reason we continue to be successful as we grow. BGIS has doubled in size four times over the last eight and a half years, and we’ve continued to retain an entrepreneurial spirit because of our culture. For our partnerships, we look for the same traits. The other key to success is looking at what you want to achieve. The old school, zero-sum principle is long gone. Now, creating a win-win relationship is key. Personally, I’m a big proponent of the vested model. AC: Where do you see BGIS in five years? LR: Our sweet spot is in complex technical environments. As more and more tech gets layered into building, every building is going to become a type of data center or critical environment, so our skillset of enabling complex technical ecosystems is highly strategic. As long as we continue to drive innovation efficiencies in this sector, we are confident that we’ll fill this niche need. AC: What are you personally passionate about in your career? LR: I love learning, and I feel that learning is an important part of the future. 85% of the tech we’ll be using in 2030 hasn't been invented yet. Therefore, a lot of the jobs that will exist are based on tech that hasn’t been invented yet. Building a love for learning and problem solving is what keeps people relevant in the future. Personally, I listen to a lot of audiobooks and am always thinking about how to apply what I’m learning to solve work problems. For me, this is so fun. Actually, I have a web page dedicated to my book recommendations, including titles like “The Compound Effect” by Darren Hardy and “Lean In: Women, Work, and the Will to Lead” by Sheryl Sandberg. So my advice is: stay curious and be a continuous learner, because that’s what will keep you relevant and ahead of whatever profession you’re in. AC: Thank you for your time today, Lorri. It’s been a pleasure to get to know you and to hear your insights around innovation and strategy for the Proptech industry.", "date": "2021-4-21"},
{"website": "Avenuecode", "title": "AC Spotlight - Eric Marcoux", "author": ["Anna Vander Wall"], "link": "https://blog.avenuecode.com/ac-spotlight-eric-marcoux", "abstract": "Eric Marcoux, Vice President of IT Business Solutions at La Capitale, speaks with us about La Capitale’s innovation and growth strategies. Avenue Code: Tell us about your educational and career journeys. How did you get to where you are today? Eric Marcoux: I’ve been working in IT since 1997. I started at two consulting companies, LGS and Fujitsu Canada. In these roles, I was exposed to various clients, systems, technologies, products, and markets. I also had the opportunity to work in the US, which gave me great insight into how business works in different geographies. As an account manager, I helped Fujitsu’s Montréal office develop a new IT consulting practice area. Plus, during my career, I had the opportunity to travel and present often on different subjects. Now presentations are not only not hard for me, but they’re very fun! After this, I worked at Université Laval for almost five years. Working as a manager in education is very different from consulting! The IT environment at universities is very impressive and I was able to participate and gain experience in several research and development projects. I subsequently joined La Capitale in 2012 as Senior Director of Architecture. When La Capitale merged its Architecture and Business Solutions areas, I was named Vice President of IT Business Solutions covering IT architecture, all the development and innovation. Each business experience has taught me a lot. As a consultant, I learned about architecture and development. At the University, I learned a lot about management and R&D. At La Capitale, I’ve brought all of these skills together. I particularly enjoy presenting La Capitale’s solutions, not as a company trying to sell products, but as a showcase of innovation and talent. I love sharing knowledge and always hope that my presentations will help someone in the room who is trying to accomplish similar goals. AC: La Capitale is experiencing striking growth. It’s become one of the top ten companies in Quebec in terms of revenue and assets. What is it that you are doing differently that allows you to so effectively compete with veteran insurance and financial companies? EM: All of the presidents at La Capitale, even before I joined the team, have had one goal in common - to be an innovative company. 25 years ago, we were one of the first companies worldwide to offer IP phones to all of our agents, so we’ve always been known as forward-thinking. That mindset is shared by all of our leaders - we’re always trying to improve ourselves in terms of marketing, sales, services, customer experience, etc. Customer experience is extremely important in our industry because it’s the biggest differentiator between competitors. The processes behind life insurance, group insurance, auto insurance, home insurance, etc. is very similar company to company, so the way to set yourself apart is through the customer experience you provide. At La Capitale, we’re always thinking about how to quickly give our customers detailed information when they need it, whether they contact us by phone, in person, or online. AC: Who are your biggest competitors? EM: We compete with newer companies that are operating 100% on the web. This wasn’t our model in the past, but now we have a presence both in person and online to attract more customers. We also compete with mature companies that have investments we don’t. And of course, we need to be extremely creative to compete with fintechs and companies like Amazon, which now offers insurance products. Evolving and competing is a challenge every day, but it’s also who we are as a company. Our goal is to be successful, and we’re convinced that we can achieve this goal at the end of the day even against fierce competitors. We always find ways to attain our goals, and we’re confident that the new colleagues we’re joining with will further this mindset and mission. AC: What are the unique challenges and opportunities for you as VP of IT Business Solutions? EM: The insurance industry is highly competitive, so we at La Capitale always have the mindset that we need to constantly reinvent ourselves. Primarily, this comes through technological innovation. Without IT, a business simply cannot achieve much today, so IT is integral to business strategy. Companies should evolve as IT evolves. Personally, I’m very passionate about this! AC: What do you do to stay abreast of innovations in tools and technologies? EM: I have many strategies: I read the news, as well as books by industry leaders like Steve Jobs, Elon Musk, etc. I try to stay up-to-date on management trends as well as technology trends. I ask my employees and managers to always keep an eye on what’s going on in the marketplace and share their findings with me so that I can utilize their individual areas of deep expertise. La Capitale also works with investment fund organizations like WhiteStar, Portag3 and Luge, and I schedule regular meetings with these contacts to keep myself informed on emerging technologies. Beyond this, I present at several industry events, primarily in Montréal and Quebec city. Volunteering my time as a speaker gives me opportunities to listen to presentations and network with creative companies. It may be surprising, but I also love YouTube! My teenagers were watching videos, so I decided to watch videos focused on IT trends. There are so many real-life learning scenarios centered on big players like Facebook, Spotify, Tesla, etc. AC: In the insurance industry, I’m sure you deal with a lot of data. How do you harness the power of data? EM: The data we get is mostly from the policies we manage. When we need to generate a quote for a car or life policy, we ask some questions, and the information we receive becomes our data. Sometimes we use specific data to create a model for a better plan, but we’re very concerned about our clients’ privacy. The data we’re handling involves people, so we choose to be very careful about the information we keep and utilize. When we do fraud detection in group insurance, for example, we carry out image recognition behind the scenes. This is a new way to acquire data, but according to our policies, we do not correlate the acquired data with a specific person, and we do not keep certain information about our clients. AC: We are in the era of machine learning and AI. How mature are AI tools in the context of insurance and the financial industry, and what is the impact? EM: There are a lot of possibilities we can achieve with AI, from machine learning to deep learning algorithms and image, voice, and text recognition. In all of these areas, AI as a tool is very mature. Combining these technologies also creates several opportunities for process automation. In the insurance industry, we’re using AI for image recognition and NLP. At La Capitale, we prepare information and use AI to automate simple tasks so that our employees can focus on more complex tasks. We use it to ease quoting processes for our customers over the web and on the phone with our agents, and we also deliver fast web service for them to provide autonomy in some service areas. All the individual tools and components are quite mature, but the real challenge is finding expertise to implement solutions advantageously. We have partners who help with this. Of course, I’m not talking about some super-robot agent with human service capabilities - we’re not there. But we are at the stage where the technology itself is mature and ready to be utilized if we can implement it creatively. AC: What was your biggest “Aha!” moment in the last 6 months as it relates to strategy/management? EM: Technology is always changing, and I used to go too micro with details, trying to understand everything. For a while, I didn’t even realize I was doing this. Now, however, I focus on the big picture and trust my managers and employees to find ways to attain goals. I’m more of a servant leader. As the VP, I tell my team of managers, “Here’s the goal: how can I help you achieve it?” instead of saying “Do it this way.” I use this strategy with my employees too. This is more motivating for my managers and for all of our employees. AC: Thanks for your insight into management strategies in the insurance and financial sectors, Eric! We look forward to keeping up with La Capitale as it continues to innovate and automate.", "date": "2020-3-25"},
{"website": "Avenuecode", "title": "API-Led Connectivity", "author": ["Anupam Gogoi"], "link": "https://blog.avenuecode.com/api-led-connectivity", "abstract": "Today, in the age of APIs, an API is not just a technical interface on top of a database. On the contrary, your API is your new business model. In the past, APIs were seen as just tools for Developers. But nowadays, their scope is not limited to internal use. The companies who make APIs are now exposing them APIs to external users around the globe. For example, Google Maps' APIs use Uber's APIs to calculate fares and travel times to destinations. The API-Led business model brings forth a new way of thinking focused on how to engage with partners and customers via APIs. In other words, your product is the API. So, one must take proper care while designing based on business criteria as well as the deployment and management of APIs. In this article, I am going to discuss the paradigm of API-Led connectivity , which has gained a lot of popularity throughout this decade. Nowadays, digital transformation is occurring everywhere due to the involvement of Mobile and Cloud technologies. APIs which were once seen solely as tools for developers, are now being exposed to the market. For example, Amazon utilizes the Product Advertising API to sell its product via third parties. However, digital transformation is not an easy task. Essentially, organizations are creating revolutionary technologies to bring forth distinct and disruptive services to the market. In order to do so, organizations must be able to retrieve data/information from disparate sources and provide them to multiple consumers (customers, suppliers, and employees) in various formats. The traditional approaches used for integration applications do not work for digital transformation. These approaches were designed at a time when fewer endpoints were necessary to meet business needs, and when the speed of delivery was not considered to be important. Here are the problems faced with traditional integration approaches: In the P2P approach, one business operation is connected to another operation by a direct connection. In an organization where a lot of applications need to be integrated, the P2P approach can be extremely messy. Here are the three main drawbacks of this approach: Hard to change Maintenance High operation risk Time to market This approach focuses on centralizing information as much as possible. In this approach, an integration platform ( ESB ) is used to act as the base for collecting all the information and serving them to the final receiver. It  centralizes and re-uses components: e.g logging, error handling, transaction etc. This approach is much more efficient than the P2P approach. However, in order to meet the digital transformation of today's age, it is still not efficient enough. So, in order to overcome these problems, the new approach of API-Led Connectivity was created. This approach is based on Pace layers . The main purpose of API-led connectivity is to enable the integration flows to be re-used by many parties, and to be re-used inside the integration platform . With the re-usability of the already available logic implemented in flows, the developers can evolve their logic in faster and safer ways, resulting in high uptime to the market. APIs are created in layers , and the key factor that differentiates them from the E2E approach is that more components (flows) can be re-used, which allows an easier implementation of new systems and services. Research shows that the API-led connectivity approach makes the development process 3 times faster because there is no need to constantly re-invent the wheel and decrease the uptime to market. Because the re-usable APIs are already tested, new implementations will be bug free. According to statistics, the reduced development time reduces the integration costs by around 70% . In this approach, the APIs are based on three distinct layers: System , Process, and Experience . With API-Led architecture, the IT infrastructure of an organization should look more or less like in the diagram below: This is the foundational layer of the three-layer architecture. These APIs can be defined for various domains of an organization such as ERP, key customer and billing systems, proprietary databases, etc. System APIs provide a way to access these underlying systems of records and expose the data in canonical formats. A System API defines the contract RAML/WSDL in order to describe how to interact with the the domain. For example, a System API for a Customer domain can contain resources with methods GET,POST,PUT,DELETE, etc., and the related schemas (XSD,JSON) as well as responses (200,400,500 etc.). So basically, we can see that System APIs often expose the sensitive information of an organization. They should not be exposed for public use. Process layer APIs are responsible for shaping the data by orchestrating and choreographing various data through calling multiple System APIs. The orchestration involves the aggregating, splitting , and routing of data. The main purpose of Process APIs is to strictly encapsulate the business process, independent of the source systems (System APIs) from which the data originates. For example, a Purchase Order process requires various domains within the organization to interact. So the Process API (Purchase Order/Order Fulfillment) interacts with the already available System APIs to implement the logic. The Process APIs shoud be held privately inside the organization and should not be exposed for public use either. Now at this point, we have all the sensitive information of an organization that are exposed privately by System APIs, and the Process APIs have already exposed the business process logic. The business process data are consumed across a broad range of clients and channels in different formats. For example, our Order Purchase API (Process Layer) has exposed data in JSON format, but we have a client application that only accepts XML format. So, we utilize simple transformation logic in the Experience Layer and the implementations are exposed as Experience APIs. In other words, Experience APIs are the means by which data can be easily re-configured in order to meet the needs of multiple audiences. We can also utilize Experience APIs to simply remove unnecessary methods and expose only necessary methods. Experience APIs may be publicly exposed for consumption. In short, they are the final products of an organization in the API-Led connectivity approach. Various policies can be applied to the APIs and they can also be monetized to earn revenue for the organization. The main benefits of the three layers can be summarized below: One can modify the System API logic without affecting the other APIs (Process and Experience). An example would be when a System API is using SAP, but SAP will need to be replaced by Salesforce in the future. This replacement can be done easily by modifying only the System API and not the Process and Experience layers. Common business logic can be shared across the organization. For example, if an organization already has the Purchase Order process API implemented, it can be re-used whenever necessary. Experience APIs are simple. Basically, they only involve transformation of data. So, in order to satisfy diverse clients that accept data in various formats, the Experience APIs can do so rapidly and decrease uptime. Role of Mulesoft in API-Led Connectivity The Anypoint Platform provides a very nice Integration Framework and an API Management platform. Using the Anypoint Platform can result in achieving a smoother API-Led connectivity. In this post, I have given a brief overview of API-Led connectivity. In my next post, I will try to show how to achieve this with some examples using Anypoint Platform.", "date": "2017-8-10"},
{"website": "Avenuecode", "title": "AC Spotlight - John Gregory", "author": ["Holly Camponez"], "link": "https://blog.avenuecode.com/ac-spotlight-john-gregory", "abstract": "With an impressive career in media and marketing, Pandora's Head of Industry - Retail, John Gregory, brings a unique perspective to changes in consumer behavior, creating a differentiated value proposition within the marketplace, and fostering a more holistic approach to media. We loved hearing his thoughts! Avenue Code : John, we've been seeing a lot of talk lately about the rapid evolution of shopping habits. What changes have you seen in consumer behavior in the last 5 years, and how does that inform your strategy at Pandora? John Gregory : I guess the biggest influence is the impact mobile devices have had on the shopping experience. When smartphones initially launched 10 years ago, consumers quickly adopted the use of the devices to shop, research products, and compare pricing, but were very hesitant to actually make purchases. As we saw in 2016, especially during the Christmas holiday period, consumers have certainly lost their fear of transacting via mobile device. I'm sure we'll see more and more \"one-click\" shopping opportunities in the short-term, especially for those retailers who need to make the experience as seamless and easy as possible to keep up with the likes of Amazon. For Pandora, it's absolutely necessary to develop ad products that allow listeners to take advantage of this easy purchasing experience. Avenue Code : Given this shift, what would you say is the key to success in today's consumer environment? John Gregory : From a retail perspective, I'd say it's more important than ever to have a point of view with unique product with careful attention to how you position it in the marketplace. Over the past 20 years, many retailers fell victim to the discount game and focused more on low price and cheap sourcing than maintaining differentiated product to help set them apart from perceived competitors. Absent of technology or other external influences, retail strategy must begin with merchandise consumers want to buy. All the technology and marketing in the world won't help sell something that no one wants. Avenue Code : It's something you're well-positioned to comment on - you've got an impressive background in retail, having held VP and executive roles at Bloomingdales, Macy's, and Ann Taylor, and heading up the Retail Category at AOL. What made you choose Pandora as the next step in your career? John Gregory : First and foremost, I wanted to continue making use of my many years of experience in the retail sector. I've covered a lot of territory over the past 30 years in diverse sub-categories of the industry and have discovered that my knowledge is a valuable asset. Since digital media companies have become eager to better understand the industry verticals they support and how to more effectively serve clients' needs, it seemed like a natural fit for me to join their forces. What attracted me most to Pandora was their focused business strategy, the registered mobile user base with the data it brings to bear, and the company culture which is, bar none, the best I've ever experienced in my career. I love those Pandas! Avenue Code : Sounds like the perfect opportunity. Where you are now with Pandora, you're at the forefront of bringing retailers into a new, more experience-centric way of presenting themselves and their products. What are some of the biggest challenges your team faces in implementing strategy decisions? John Gregory : With all the tools at our fingertips, it amazes me how the lack of communication amongst the parties involved creates many difficulties in our client and agency partner relationships. Many retailers still have a \"siloed\" internal marketing/media organization. This, combined with the sometimes disjointed responsibilities of their medi agencies, is a formula for inefficiency and can be especially difficult for Pandora, as some partners see us as an audio media channel and keep us aligned only with their radio teams. In fact, we have a very robust display and video media delivery capability which crosses into the accountabilities of multiple media teams at both the client and agency. We spend a lot of time fostering these cross-channel relationships. Today's consumers take a mobile-first approach to entertainment and shopping. Avenue Code : What are you working on today that excites you? What's next for Pandora? John Gregory : At the moment, I'm most excited about the Pandora Dynamic Product Showcare (DPS) mobile initiative that was recently launched at ShopTalk in Las Vegas. This program is directed at retailers who still have a large presence with weekly newspaper print circulars. While these are still an important part of the media mix, a large audience under the age of 45 who aren't Sunday newspaper readers are missing this weekly item and price content meant to drive store traffic. Our DPS program is meant to be a digital companion with print circulars by dynamically serving key item content to local markets and driving them to view the full content on the retailer's own hosted digital circular experience. Avenue Code : Another great example of how innovative companies are finding ways to enhance and complement existing channels to meet consumers where they are. With all the changes in the industry, are there any constants you've found that never change? John Gregory : The one constant that hasn't changed in my entire career is the demand for service. While the definition of service has changed over the years due to new store formats and e-commerce innovation, it's something that retail managers need to fully understand in context of their business model. Avenue Code : Thanks, John - as we continue the conversation with leaders across the e-commerce sector, we're hearing the same thing with regards to service. Your perspective, standing at the intersection of marketing and media for a whole new way of engaging with consumers, is fascinating. You're clearly invested in breaking down barriers of communication and finding what works for today's consumers - keep up with good work with your fellow Pandas, and we'll look forward to seeing what comes next!", "date": "2017-8-15"},
{"website": "Avenuecode", "title": "How to Build A Simple JAX-RS Application with CDI using Java EE", "author": ["Felipe Moraes"], "link": "https://blog.avenuecode.com/simple-jaxrs-application-with-cdi-javaee", "abstract": "The project will consist  of a simple project with JAX-RS and CDI. To follow the example presented here you will need: To create this project I will use Maven, and to keep the creation IDE agnostic, I'll use it from command line. If you don't feel comfortable doing the same, you can use your favorite IDE for the job. Maven has several archetypes that can assist us with this task. The one that I'm going to use gives us just what we need to create a Java EE 7 project. In a terminal, use the following Maven command to create the project: This command should give us something like this: We choose number 1 and after this we choose number 3: The difference between number 3 and 4 is that the latter is already configured with JAX-RS and with CDI. But don't worry: we will get there in a few more steps. Afterward, we need to set the final properties for Maven to create the project. You can choose any values that you want. For my project, I used the following: This will create our project and give us the following structure: The content of the pom.xml file should be the following: To create a rest endpoint, we first need to configure the entry point for all rest resources. To do this, we'll create a new class in our project, as seen below: JAX-RS requires that you create a class that extends Application to identify that you are using it. The annotation @ApplicationPath(\"rest\") identifies the path that will serve as the base to all our endpoints. Next, let's create and expose our endpoint. We'll begin by creating a new class called HelloRest : The @Path annotation dictates that this class will handle calls made to the URL /rest/hello , and the @GET dictates that the hello method is responsible for Http Get calls. Time to test our rest endpoint! If you are using an IDE, you will probably have to setup the application server that you downloaded into it. To keep it simple, we are going to use the terminal. First, navigate to your project home folder (mine was \\ac-blogpost\\javaee7 ) and run: This will generate the javaee7.war file of our application in the target folder. Since I'm using the Wildfly Application Server, I'll copy my war file to my <wildfly-home>\\standalone\\deployments folder and start Wildfly by running the standalone script. Depending on which OS you are using, you will either run .sh or .bat . The script will throw a lot of logs output about the server startup. The part we are interested in should look like this: This basically tells us that it recognized our war file and is deploying it. It also shows that it's using the RestEasy implementation of JAX-RS (depending on which server you used to deploy our application, it may show a different implementation of JAX-RS). Our application should now be listening to calls on this URL, http://localhost:8080/javaee7/rest/hello , and you should see the following response: Need some help building backend systems? Let Avenue Code be your guid. CDI (Context and Dependency Injection) is much more than just a simple DI framework for Java EE, and could be its own complete article. For now, since this is not the focus of our current post, I'll simply leave this link here with more information on the subject for those who are interested: Overview of CDI For our project, we'll stick with the basics for demonstration purposes. So, to enable CDI in our project, we need to create a file under webapp/WEB-INF folder called beans.xml with this content: That's all that we need to inject our classes using CDI. To demonstrate, I created another endpoint that simply returns the double of the number that we passed as parameter. The code follows below... First I created a service class: Nothing new here, just a simple class that doubles the value passed as parameter. Next, I created a new endpoint as follows: In this class are three new things. First, in the method doubleOf , we used the @PathParam annotation to bind the {number} inside the @Path annotation with the number method param. Secondly, the @Inject annotation, which tells the CDI to - naturally - inject CalculatorService for us. Finally, the @Produces annotation, which simply entails returning a plain text as the response of our rest method. Run mvn clean package again and copy the war file to the deployment folder of your Application Server. If the server is still up, it will deploy the application for you automatically. Otherwise, just start it over again. Now you should see the following output if you access http://localhost:8080/javaee7/rest/calculate/doubleOf/2 : To finish this up, I'll show some alternatives to the Springboot executable jar that can be done with Java EE. Wildfly has a side project known as wildfly-swarm, the idea of which is to \" provide just-enough-appserver to support whatever subset of traditional JavaEE APIs your application requires\". For more information here is the project site Doing this is pretty straightforward. We just need to add the plugin to our pom.xml as follows: To execute this and generate our executable jar, use the command mvn clean package -P wildfly-swarm This will create a file named javaee7-swarm.jar under your target folder. An interesting note about the build process: the plugin  detects the dependencies that we are using in our projects, and adds them to the final jar. Here is the snippet of the Maven proccess that demonstrates this: Now you can simply execute the jar file with: java -jar target\\javaee7-swarm.jar The endpoint is now changed from http://localhost:8080/javaee7/rest/calculate/doubleOf/2 to http://localhost:8080/rest/calculate/doubleOf/2 . The Payara server also has an alternative to run war files from command line - here is a good description. To create an executable jar using Payara-micro, you can use the following command: This will create the javaee7-app.jar , and you can execute it with: java -jar javaee7-app.jar The endpoint will still be the same as before: http://localhost:8080/javaee7/rest/calculate/doubleOf/2 and http://localhost:8080/javaee7/rest/hello So as we can see, creating a Java EE application is quick and easy. In this example, we used just one dependency for the entire project in the pom.xml , which is possible because Java EE provides all the tools needed for the job. In my experience, this allows for increased focus on your business needs, improving overall productivity (talking about productivity, learn how to build a microservice in 20 minutes with Java ) . For the unconvinced, don't forget that you can even have executable jars using Java EE! There are so many other useful features we could add - maybe these will be topics for future posts. Thoughts? Do you already use Java EE in your projects? If not, would you give it a try? Let me know in the comments!", "date": "2017-6-21"},
{"website": "Avenuecode", "title": "So, You Want to get OCMEA Certified - Part 1", "author": ["Vinicius Kairala"], "link": "https://blog.avenuecode.com/oracle-certified-master-java-ee-6-enterprise-architect-ocm-part-1", "abstract": "OCM is the highest level of Java Certification offered by Oracle. It's aimed at professionals responsible for modeling and architecting Java EE applications with several nonfunctional requirements (NFR), such as security, scalability, maintainability, reliability and availability. For this certification, Oracle expects that Java architects have solid knowledge of the Java EE stack and choose the most suitable technologies when addressing real life problems. What is OCMEA? For those who are not familiar with OCMEA, it is composed of three parts, the first of which is an exam with 60 multiple-choice questions that must be finished in  150 minutes or less. In this part, you can expect to see a broad spectrum of questions ranging from best enterprise system decision choices (NFR, risk evaluation and mitigation, communication protocols, etc) to common Java EE architecture choices (Java EE stack, OO best practices and patterns, integration and messaging, deployments, tiers and layers, security, etc). The second part is a take-home assignment to be completed within 6 months. Oracle randomly assigns to you one of the many likely real-case scenarios containing the business requirements and the specifications required for the candidate to model and architect an entire solution for the given application. In this part, you have to deliver several UM- compliant diagrams and HTML documents all packaged together in a compressed jar file. Oracle expects at least the following kinds of diagrams: Class diagram, Component diagram, Deployment diagram, and Interaction diagram (Sequence or Collaboration diagram). The third and final part of the certification is an essay in which you must answer 8 open-ended questions about your delivered assignment. During this phase, you defend your architecture and the decisions made for the most diversified NFR aspects, such as how you have handled security in your application or how your application can handle scalability. If it sounds like a lot, that's because it is. Let's break it down step by step. In this post, we'll discuss the first part of the certification in-depth and leave the remaining parts for another post. Ready? Let's get going. In terms of common architecture, you need to understand how an application is organized by layer and tiers. The former represents component relationships within services, whereas the latter represents processing chains across different components. In other words, layers can be thought of as an architecture pattern in which components lean on services at a lower layer. In regular enterprise java architectures, you'll likely see applications being layered in these parts: the application layer (user and business functionalities), the component API layer (interfaces to the application infrastructure component APIs, such as Servlets), the middleware layer (containers or products for the operation and development of the application, such as application servers), the enterprise services layer (virtualization, Operating Systems and DBMS programs) and physical layer (computers, storages, etc). Similarly, tiers are the physical or logical components in the hierarchy of service. In terms of the exam perspective, a multi-tier application should have the following tiers: Client tier (browser, mobile phone, etc), web or presentation tier (services which manage the user session and route the requests to services, i.e. servlets, front-end controllers), business tier (services which handle the business logic), integration tier (services which integrates with other external services) and resource tier (services which access database, filesystem, active directory users, etc). You can expect to see many questions in the exam comparing the advantages and disadvantages of 2-tier against N-tier systems regarding security, availability, maintainability, scalability, etc. As a simple example, a 2-tier system can be easier to secure because it usually has just a single point of access, but at the same time, a single point of entry can decrease the system's availability. Moreover, you need to understand in a broader sense what the key NFRs are about. Namely, performance, scalability, reliability, availability, extensibility, maintainability, manageability and security. You'll also need to know how to address each case. For instance, to handle availability, your solution should have mechanisms to mitigate system downtime and long response time. You can absolutely achieve that by clustering the components, but you need to evaluate different scenarios.  You need to determine if an active replication, where all redundant components receive the same requests and process them, is a better fit than a passive replication, where only the primary component processes the request but, if a failure occurs, a redundant component takes place as processor. Further, you need to understand the benefits and drawbacks of different kind of clustering: two-node cluster (symmetric and asymmetric), ring cluster, star cluster, N-to-N cluster, etc; and how to load balance clusters in order to guarantee not only high-availability but also fault tolerance and failover. The exam will also test your knowledges on web tier technologies. There will be questions on what the MVC pattern is, how it works, and what its benefits are, to major Java EE web tech stack such as Web Containers, Servlets, filters, listeners, JSP, JSTL, EL and JSF. You don't need to know how to implement those technologies in-depth, but you do need to understand the overall point of each technology, its purpose, trade-offs and life-cycle (in the cases of Servlets and JSF). As this is a Java EE6 certification, there will be times when you will be asked for the best technology choice for a rich web application and, when that time comes, don't hesitate to choose JSF over other choices, even if you are a big fan of HTML5 + JQuery. Oracle expects you to make that kind of choice, so wait for part II to make your own preferred choices part of your architecture, and be ready to support them in part III. In addition, you need to understand the common approaches to store states on the client side of a web based application, such as URL Rewriting or Cookies, and on the server side also, as HTTP Session or HTTP Request. For example, given a scenario where two applications must talk to each other over a WAP protocol, the best fit for that is URL Rewriting, as this protocol doesn't support cookies. The drawback of this approach is that URL Rewriting exposes user's id in the URL and it can easily be intercepted by intruders. Finally, you need to identify the most suitable scenarios to use some emerging web approaches such as server push communication (Java WebSocket) or asynchronous requests (promises, callback methods, etc). From the business tier perspective, be prepared to choose EJBs as the best choice for your service components. Again, this is a Java EE6 certification and Oracle will expect you to support its major technologies. Just as with web technologies, you will not be required to know how to implement EJBs in-depth, but you should know the usage of stateless and stateful EJBs, their life-cycles, and in terms of scalability, which is preferable. Also, it's important to understand why MDB is the best fit for asynchronous process and what the best practices are for it. Expect to point out the advantages of using EJBs, such as scalability (EJBs can be clustered), fine-grained transaction and security (declarative or programmatic), highly reusable, etc. There will also be questions about new kinds of EJB, such as Timer Service and Singletons. Finally, you'll need to know how to both expose and consume web-services using the Java EE technologies. One important aspect of the certification is system integration. You can expected to see several questions which describe different scenarios and ask you to choose the right integration technology to address it. For instance, given a scenario where you need to integrate a java application with a non-java legacy system with no internet access, which technology should be chosen? To answer that question, you will need to understand and evaluate integration concepts as CORBA, IIOP, RMI, JCA, JDBC, etc. You will also need to comprehend when to use web-services, the main differences between SOAP and Restful, and the usage of JMS. Questions regarding how JCA manages transaction, how to integrate (a)synchronously java to java, java to non-java,  and java to EIS (Enterprise Information Service) are all likely to appear on the exam. Another key aspect covered by the exam refers to security constraints. In a real world scenario, security threats can truly jeopardize a company's resources, causing severe damages to its infrastructure. To survive that, an architect must identify, minimize, anticipate and eliminate these threats, avoiding unauthorized use of protected services and assets. You need to distinguish the main possible attacks and to diagnose strategies for mitigating these threats. Here is a non-exhaustive list of some threats that you may want to know: Additionally, you need to understand how Java Web Start secures an application. Restrictions such as no access to the local disk, network connections can only map to the same domain, and no access to native libraries are inherent to this technology. Similar restricted constraints are applied to unsigned Java Applet programs, whereas when launched via JNLP, applets can have more relaxed restrictions, including the possibility to access printing functions or the shared system-wide clipboard. Furthermore, you need to grasp many security fundamentals such as the differences between declarative and programmatic security, authentication and authorization, or data integrity and confidentiality, or symmetric and asymmetric encryption. Finally, you will need to know the most important practices for security enforcement, including how to use digital signatures and certificates, how to set up firewalls and DMZ, tunneling, etc. Last but not  least, OCMEA uses some questions to ask about design patterns. You can expect to see different scenarios with recurring problems that you need to apply known design patterns in order to address these issues. To properly do that, you need to know the GoF design patterns,  their motivations and benefits. Without a doubt, the best source of knowledge regarding these patters is Design Patterns: Elements of Reusable Object-Oriented Software by Erich Gamma, Richard Helm, Ralph Johnson and John Vlissides. In addition,  there is another gamma of other patterns that you will need to know for integration, web and business tier security, etc. For these patterns, I'd recommend the books Real World Java EE Patterns: Rethinking Best Practices by Adam Bien, and Patterns of Enterprise Application Architecture by Martin Fowler. You can bet on using these patterns heavily during parts II and III of the certification, which we'll discuss in the next post. See you there!", "date": "2017-6-14"},
{"website": "Avenuecode", "title": "ITIL and Agile: How Each Contributes to IT Best Practices", "author": ["Paulo França"], "link": "https://blog.avenuecode.com/itil-and-agile-how-each-contributes-to-it-best-practices", "abstract": "The vision for IT service management has changed over the years. For a long time, ITIL was the standard, but ever since the famous Agile Manifesto was released in February 2001, many new methodologies have been created and successfully implemented. Before the Agile Manifesto was released, most software developers followed the ITIL framework. ITIL is a library comprised of several books that describe and guide the practices of various aspects of IT. It was written in the 1980s by the British Government's Central Bureau of Computing and Telecommunications (CCTA) as a set of recommendations for best practices for all IT departments of government bodies. ITIL soon attracted interest from the British private sector, which saw in this work an opportunity to improve the efficiency and quality of IT service providers. It was also appealing because these practices were not linked to any supplier but focused instead on the user's point of view. In 2000, a year before the Agile Manifesto was released, the second version of the ITIL framework, ITIL V2, was published. It contains extensive documentation composed in 7 main books. At the time, this version was the standard for the management of IT services and processes and was adopted by several large companies. Everyone wanted to implement and sell ITIL to the market. ITIL still has a large following. Currently in version 3, which was launched in 2007, it consists of 26 processes and functions meticulously addressed in five main books : 1 - Service Strategy 2 - Service Design 3 - Service Transition 4 - Service Operation 5 - Continuous Service Improvement Today, the adherence of large companies to the ITIL framework is still very prevalent. Many IT managers have adopted ITIL within their organizational roadmap. This is because it is a very descriptive work composed of several milestones provided in its vast documentation. ITIL describes the means of measuring and reporting the level of services provided and standardizing processes to ensure efficiency and quality. This is not to say it isn't costly for organizations to adopt ITIL. Experts estimate that it takes most large enterprises up to 5 years to implement ITIL. Because of this, the endeavor should be treated as a long-term project whose main goal is to foster positive organizational change. For several years now, however, ITIL has not been the only option. In 2001, 17 representatives from the most diverse types of software development process methodologies gathered at a Ski Resort in the Wasatch Mountains in Utah with the goal of finding a viable alternative to the highly descriptive and documented processes that were very common at the time. Their meeting resulted in the now widely implemented Agile Manifesto. In the same way that the adoption of ITIL had strong market adherence, the Agile methodologies, mainly influenced by the Agile Manifesto, have influenced many companies. Unlike ITIL, the Agile Manifesto is composed of only 12 principles - not books - whose main goal is to simplify software development processes and also to identify what is most important in software development and services. Broadly speaking, the Agile Manifesto may be summarized as follows: Currently, the companies that have benefited most from Agile methodologies are the newest companies, because they perceive in Agile a way to quickly maximize their efforts and deliver new features that add value to their businesses and products, thus accelerating financial return as well as customer satisfaction. A practical approach to this may be seen in ROI. In Scrum, the ROI represents the benefits achieved from an investment versus the cost incurred for each feature developed. In this way, business operators can measure and maximize their ROIs so that features that are most relevant to customers are delivered first. Some of the most famous and successful IT companies in the world began their efforts by betting on Agile. Their success only confirms that Agile methodologies have transformed and continue to transform the vision we have of IT today. It's important to note that companies who have used ITIL to govern their IT processes in the past can still benefit from the use of Agile Methodologies. In fact, ITIL may coexist well with Agile. Adopting one approach does not mean abandoning the other. Both ITIL and Agile are customer-focused, but each one uses a different approach to address the same goal. The most practical way to deal with this difference, to start, would be to integrate some of the ITIL RFCs already implemented within Agile. Companies' eager adoption of Agile practices indicates a response to the new challenges faced in the IT world today: the ability to implement new ideas and adapt without \"getting stuck\" in long processes. Agile's pillar of quick response to change provides a solution. Nonetheless, does ITIL deserve a place at the table? The answer is yes. ITIL can help companies organize the fast changes introduced by Agile. In the end, both ITIL and Agile are useful.", "date": "2018-9-26"},
{"website": "Avenuecode", "title": "Event-Driven Architecture and Its Application in Java - Part 1", "author": ["Otavio Tarelho"], "link": "https://blog.avenuecode.com/event-driven-architecture-and-its-application-in-java-part-1", "abstract": "Have you ever developed an event-driven application or heard about this architectural approach? You might be surprised to learn that you may be using it even if you've never heard of it! This post series will explain what EDA architecture is, why it was created, what its components are, and how it is used in the Java ecosystem. In part one, you'll learn the core concepts of EDA. In part two, you'll walk through the implementation of a simple EDA project in Java. First, we need to understand what an event is in the context of a system. Every change of information, even the tiniest change, can be considered an event. For example, the change of a user address can be considered an event. Changing an address - let's say in a mobile company's system - is an event that can trigger other events in other systems, such as billing, email, etc. Instead of having the producer calling the billing system directly, which would be tightly coupled, in the event-driven architecture we basically hide both, keeping them apart from each other. This way, the producer system will send an event to an event bus, which will be listened to by the consumer. An event does not carry much data on it; it only carries the information that is necessary for the consumers to be able to do their job. This approach helps developers in terms of application responsiveness. On monolith systems, the whole application is coupled together, so when scaling needs to happen, the whole application has to be taken down. In fact, the entire application has to be deployed every time there is a single change in one part of the system. Event-driven architecture was created to help developers have a decoupled and responsive application. Because of this, it has been widely used in applications that have been broken down from monoliths to microservices. Most of the time, producers do not care about receiving responses from consumers. Producers don't want to know about the existence of consumers or what they are doing with information. They just want to convey information about an event for another system to work on. This is called event notification. It segregates the responsibilities of each service. A common pattern when we talk about event-driven architecture is event sourcing. This core concept enables systems to process the event at any time in the future since all events are logged, and the state of the application will depend on the order of its insertion. Examples of this pattern include the version-control system GIT and change logs of databases. From event sourcing, stream processing tools were created in order to permit the development of systems that process information in parallel. Apache Kafka framework is a very good example of how the event stream works. It stores the events, in the same order as they were received, in logical collections called topics. Topics can be imagined as queues and are grouped into partitions that permit parallel processing. In part one of this series, you were introduced to the EDA concepts that have been driving the world of microservices. It's important to note that even though there are advantages to using this architectural design, there are also challenges. For example, this approach requires much more expertise from developers, since there may be debugging and testing problems that are far more difficult to resolve due to their asynchronous nature. This architecture also requires a lot from DevOps teams, since they need to control the flow of what is happening in an application because it's essential to know if everything is up and running correctly. When choosing an architectural pattern, choose wisely and select a pattern that meets your business requirements. If you have a requirement that has divided transactions that are processed separately, EDA may be the pattern that you are looking for.", "date": "2020-1-15"},
{"website": "Avenuecode", "title": "Your Guide to Organization-Wide Digital Maturity, Pt. 4", "author": ["Chrystiane Symoes"], "link": "https://blog.avenuecode.com/your-guide-to-organization-wide-digital-maturity-pt.-4", "abstract": "Last week, we illustrated how to identify high-ROI business opportunities through design services guided by design thinking. Today, we'll cover the three pillars of digital enablement: people and organization, data and analytics, and technology. After using design services to identify innovation opportunities and improvements to products/services, it’s time to move from idea to execution. Companies must have refined methods and technologies in place in order to successfully build, assess, and deliver each project. In other words, design services help companies explore the “Why” behind each project, and digital evolution enablement provides the “How.” Of course, both design and digital evolution enablement also touch on the “What” of the project itself. Together, these two practices take companies from idea to execution. There are three pillars of digital enablement: people and organization, data and analytics, and technology. The first pillar - people and organization - encompasses departments like HR, finance, and talent retention. The way that a company manages its people and its finances is critical to successful digital evolution. This pillar is closely tied to Agile transformation and mindset. The second pillar - data and analytics - is also key to digital enablement because it prioritizes feedback related to ROI on digital evolution initiatives. The third pillar - technology - significantly affects how innovation projects are delivered, with top technologies and techniques like cloud, data science, and iterative delivery improving processes and results. Transformation in each of these areas is accelerated by Avenue Code’s trademarked program, Coach by Emergence. Because digitization creates new ways of working, any effective change in tools and technology mandates cultural change as well. IT departments enjoy success in working with the Agile methodology because it enables individual teams to adapt and adjust swiftly while remaining aligned with the goals of other teams. Through iterative delivery and constant feedback, a team avoids taking a product in the wrong direction, gets products to market faster, and realizes benefits earlier. The same is true of the Agile approach applied more broadly throughout the organization in departments like finance, human resources, talent acquisition and retention, and overall business strategy. Data analytics is directly linked to business intelligence. By deeply understanding customer behaviour and preferences, companies have a big advantage in supporting decision making, as well as in building or adapting innovative products/services. Accurate and relevant feedback also supports the Agile approach to generate business value faster since companies will have the ability to predict behavior, prioritize projects, check product and service trajectory, mitigate risks, and reduce costs. Technology enables the digitization process. Rapid technological evolution makes this process even faster, more reliable, and more connected. Companies need to deliver their products/services faster, as the market is constantly changing, and losing time to market means losing revenue. Digital transformation approaches that expedite time to market include cloud pipeline and release training maintenance, among others. Combining these technical approaches with data analytics and cultural transformation within every department will accelerate the delivery of the product or service and its constant updates/improvements, generating business value iteratively. Digital enablement in all of these areas is fueled by Coach by Emergence. In next week's final article, we'll introduce Coach by Emergence, the methodology that dramatically accelerates digital evolution. Want to meet CBE now? Download our free whitepaper for your complete guide to organization-wide digital maturity. Digital Transformation , Red Dot.", "date": "2020-12-23"},
{"website": "Avenuecode", "title": "Creating Extensions and Widgets in Oracle Commerce Cloud", "author": ["Henrique Elias"], "link": "https://blog.avenuecode.com/creating-extensions-and-widgets-in-oracle-commerce-cloud", "abstract": "Oracle Commerce Cloud has gained popularity as an optimal platform solution because it provides all the features needed to build a world-class e-commerce website while also allowing for a high level of customization. Today, we’ll discuss how to customize and enhance functionality in your OCC implementation through extensions. Before we begin our tutorial, let’s review the subcomponents of extensions. Extensions are composed of: Widgets A widget provides a unit of UI functionality that can be deployed on one or more pages of your web store. Widgets are able to display content to visitors or execute specific functions. They provide custom HTML and/or JavaScript code, as well as, optionally, several other types of auxiliary data for styling, localization, and component re-use. JavaScript Modules Before we discuss how to create and customize widgets in OCC, we’ll first explain how to create and integrate extensions. Creating an Extension ID In order to upload an extension into Commerce Cloud, you must generate an ID for the extension and then use that ID when you define the extension’s metadata. To generate the extension ID: Once you’ve created a unique ID for your extension, you can start to develop it by creating the folder structure. Make sure your extension uses a name that is unique within your Commerce Cloud environment. In other words, do not give your extension the same name as a default widget. Each extension provides metadata describing the author/developer of the extension, as well as other information related to its creation, and this metadata is contained in a manifest file called ext.json, as seen below: <extension-name> : The root folder of your extension, must be unique ext.json <additional directories to define widgets, elements, payment gateways, site settings>/ Without the manifest file, the extension cannot be loaded. (See more details at ext.json file rules .) An example of ext.json is below: { \"extensionID\" : \"79f39268-e814-4a2a-9e04-16fd6dc390c2\", \"developerID\" : \"987654\", \"createdBy\" : \"Henrique Elias\", \"version\" : 1, \"timeCreated\" : \"2018-03-17\", \"translations\" : [ { \"language\" : \"en_US\", \"name\" : \"WIDGET POC - English\", \"description\" : \"Extension description in English\" }, { \"language\" : \"fr_CA\", \"name\" : \"WIDGET POC - French\", \"description\" : \" Extension description in French\" } ] } When you have finished developing or changing your extension contents, zip up all the files within your <extension-name> directory. This then becomes the file you upload to Commerce Cloud to make the extension available for use: Click the “Menu” icon, then click “Settings” Click “Extensions” This tells the system to begin the upload and validation process. Any problems identified during the validation checks are displayed in a warning message. Once uploaded, the file will appear as below. (You may use this file , which already includes a widget definition, but remember to update the extension ID in the ext.json file based on the ID generated for you.) Now that we’ve shown how easy it is to create and upload extensions into OCC, let’s discuss extension subcomponents: widgets. Widgets may be composed of a set of source files and resources, such as: Templates: display templates for showing content using knockout.js data bindings. Widgets are added to an extension’s structure in a /widget directory that is at the same level as the extension’s ext.json file. Each widget should have its own child directory in the /widget directory. The following example shows the directories and files that may be included for a widget: <extension-name> : the root folder of your extension ext.json widget/ <widget-name> / widget.json templates/ display.template js/ <widget-name> .js less widget.less locales/ <locale code , for example , en or en_US > / ns. <widget-name> .json <other locale codes > / ns. <widget-name> .json images/ The basic  structure required for a widget to pass validation in an upload is: <extension-name> ext.json widget/ <widget-name> / widget.json With the exception of global widgets, which we describe in more detail below, all widgets require a template file, titled “display.template,” in the /widget/<widget-type>/templates directory. The template is rendered within the context of the widget and should be written as straight HTML with no surrounding script tag. All knockout bindings and behavior are available in the HTML template code: <extension-name> : the root folder of your extension ext.json widget/ <widget-name> / widget.json templates/ display.template Much like the ext.json file defines metadata for an extension, a widget.json file defines metadata for a widget. (More details on this are available at widget.json file rules .) An example of a widget.json file is provided below: { \"version\" : 1 , \"global\" : false , \"javascript\" : \" widget-name-js \" , \"i18nresources\" : \" widget-name \" , \"availableToAllPages\" : true , \"jsEditable\" : true , \"config\" : { }, \"translations\" : [ { \"language\" : \"en\" , \"name\" : \" Name in English \" }, { \"language\" : \"de\" , \"name\" : \" Name in German \" } ] } You may create both global and local widgets. Global widgets are loaded within each page and only need a JS file rather than a template file, whereas local properties apply only to specific instances of widgets. To define a widget as global, set “‘global’ : true” in the widget.json file. Widgets may be placed in all page types or just specific page types. To make a widget available in all pages, set “‘availableToAllPages’: true” in the widget.json file; otherwise, omit this property and set the “pageTypes” property in the widget file. The available page types are: Note that if the widget is global, you must define the page type and omit the “availableToAllPages” property. By default, global widgets apply to all sites in your Commerce Cloud instance. You may override this default and assign a global widget to be used on only specified sites. To do this, you must issue a POST request using the ‘updateSiteAssociations’ custom action of the widgetDescriptors resource and provide a list of sites in a sites property. For example, the following request updates myGlobalWidget to execute on siteA and siteB only: POST / ccadmin / v1 / widgetDescriptors /< widget-name> / updateSiteAssociations { \"sites\" : [ \"siteA\" , \"siteB\" ] // to remove site associations, define as [] } To add JavaScript  code to a widget, you must create a JavaScript file under the <extension-name>/widget/<widget-type>/js directory. The name of the JavaScript file must match the value of the javascript property in the widget.json file, minus the .js extension. The convention is to use the “widget-type” as the JavaScript file name without the .js suffix. The Javascript file will perform some logic and return an object with extensions to the widget’s view model. The JavaScript file should be in the following format using RequireJS . The module must be defined anonymously. In other words, as shown below, it must not  have a package name defined in the module: define ( // Dependencies [ 'jquery' , 'knockout' ], // Module Implementation function ( $ , ko ) { 'use strict' ; var SOME_CONSTANT = 1024 ; var privateMethod = function () { // ... }; return { // Widget JS available // Some member variables... textInput : ko . observable (), // Some public methods... doSomethingWithInput : function () { //... } } }); Oracle Commerce Cloud allows for multiple versions of a widget. The first version should have the following structure: <extension-name> ext.json widget/ <widget-name> / widget.json Subsequent versions should have the following structure: <extension-name> ext.json widget/ v2 widget.json In today’s article, we provided a brief introduction to the structures of extensions and widgets and showed how to create and customize them in Oracle Commerce Cloud. OCC makes it easy for e-commerce companies to quickly design a highly stable, functional, and customizable platform for enhanced business operation. At Avenue Code, we’ve built several specialized platforms from scratch, but we’re also adept at helping you quickly adopt the stable infrastructure and preexisting capabilities of Oracle Commerce Cloud while customizing your implementation to accurately represent your brand. Appendix A: Site Settings An extension can contain site settings , which are configurable parameters that are globally accessible to the storefront code. Site settings are added to the site view model and are available for use by all widgets. Site settings allow you to create a single setting that controls a feature across multiple widgets. Site Settings files A site settings extension creates a custom settings panel in the administration interface after it has been uploaded. This panel allows merchandisers to configure the settings. (To see a site settings panel, go to the “Settings” page and click the name of the site settings extension under “Extension Settings.”) Site settings are defined using a JSON-based schema. To add site settings to your storefront, add the following files to your directory structure: <extension-name> : extension root directory ext.json config/ <settingsID>/ : site settings root directory config.json locales/ en_US.json fr_FR.json The resource bundles for site settings are stored in locale files under the /config/locales directory and look similar to the following: { \"resources\" : { \"enabledHelpText\": \"Enable the cart message.\", \"enabledLabel\": \"Cart Message\", \"couponHelpText\": \"Define the coupon name.\", \"couponLabel\": \"Coupon\", \"minSpendHelpText\": \"Define the minimum spend amount for the coupon.\", \"minSpendLabel\": \"Minimum Spend\", \"sizeHelpText\": \"The size of the banner.\", \"sizeLabel\": \"Banner Size\", \"sizeSmallLabel\": \"Small\", \"sizeMediumLabel\": \"Medium\", \"sizeLargeLabel\": \"Large\", \"passwordHelpText\": \"Set the value for API key.\", \"passwordLabel\": \"API Key\", \"title\": \"Sample Site Settings\", \"description\":\"Examples of site settings.\" } } The structure of a config.json file looks similar to the following: { \"widgetDescriptorName\": \"multisiteconfigdemo\", \"titleResourceId\": \"title\", \"descriptionResourceId\": \"description\", \"properties\": [ { \"id\": \"enabled\", \"type\": \"booleanType\", \"name\": \"enabled\", \"helpTextResourceId\": \"enabledHelpText\", \"labelResourceId\": \"enabledLabel\", \"defaultValue\": true }, { \"id\": \"coupon\", \"type\": \"stringType\", \"name\": \"coupon\", \"helpTextResourceId\": \"couponHelpText\", \"labelResourceId\": \"couponLabel\", \"defaultValue\": \"SHIP100\", \"minLength\": 6, \"maxLength\": 10, \"required\": true }, { \"id\": \"minSpend\", \"type\": \"stringType\", \"name\": \"minSpend\", \"helpTextResourceId\": \"minSpendHelpText\", \"labelResourceId\": \"minSpendLabel\", \"defaultValue\": \"100\", \"required\": true }, { \"id\": \"password\", \"type\": \"passwordType\", \"name\": \"password\", \"helpTextResourceId\": \"passwordHelpText\", \"labelResourceId\": \"passwordLabel\", \"required\": true }, { \"id\": \"bannerSize\", \"type\": \"optionType\", \"name\": \"bannerSize\", \"required\": true, \"helpTextResourceId\": \"sizeHelpText\", \"labelResourceId\": \"sizeLabel\", \"defaultValue\": \"s\", \"options\": [ { \"id\": \"sizeSmall\", \"value\": \"s\", \"labelResourceId\": \"sizeSmallLabel\" }, { \"id\": \"sizeMedium\", \"value\": \"m\", \"labelResourceId\": \"sizeMediumLabel\" }, { \"id\": \"sizeLarge\", \"value\": \"l\", \"labelResourceId\": \"sizeLargeLabel\" } ] } ] } Once you add a site setting, you will have access to a new section in “Settings,” as shown below: The titleResourceId property specifies a key in the resource bundles that is used to retrieve the title for the panel in the administration interface, e.g. “Sample Site Settings” in the illustration above. The descriptionResourceId property specifies a key for the descriptive text that appears below the title. In the illustration above, this appears as “Examples of site settings.” The remainder of the config.json file consists of a properties array that defines individual site settings and their key/value pairs. Site settings use the same standard keys as configurable widget settings: id, name, type, helpTextResourceId, labelResourceId, defaultValue, and required . Site settings can also use the same data types that are available to configurable widget settings, e.g. stringType, multiSelectOptionType, and so on. Site settings can be configured on a site-by-site basis. If your Commerce Cloud instance is running multiple sites, the values a merchandiser specifies in a settings panel apply only to the currently selected site. The merchandiser can then select another site and supply it with different values. In some cases, a site settings panel may have settings that make sense for certain sites but not for others. In this situation, you can give merchandisers the option of disabling a site settings panel completely for individual sites. To do this, include the following in the config.json file of the extension that creates the panel: \"enableSiteSpecific\": true The first time a page loads in Commerce Cloud, the server returns all of the data associated with the specified page layout, including the widget template source, element source, and so on. On subsequent page loads, it is possible to limit the returned data to only those widgets that have not been previously rendered. This feature is not enabled out of the box because you should understand the trade-offs involved before using it. When this feature is disabled, all of the data for the page is returned and cached for every page call, giving you the performance improvements associated with caching. When the feature is enabled, the system tracks the URLs that have been visited to determine which widgets have not been previously rendered and then limits the returned data to those widgets. Caching all of these URLs is not feasible, however, so caching is turned off when this feature is enabled. Instead, the performance improvements that are gained are driven by the drastically reduced amount of data that is returned from the server. This approach is especially beneficial on mobile devices. To enable this feature, you must create an extension that uploads an application-level JavaScript module that depends on the cc-store-configuration-1.0.js library and sets the enableLayoutsRenderedForLayout flag to “true.” In addition to enabling the feature, you may also choose to override the storeLayoutIdsRendered() and getLayoutIdsRendered() methods: The storeLayoutIdsRendered() method stores IDs for the layouts visited by the shopper until the page is refreshed or the browser is closed. The stored layout IDs let the server know which pages have been cached so it can limit the returned data to new widgets. The number of IDs that the storeLayoutIdsRendered() method stores is determined by the size of the layoutsRenderedArraySize object and is set to 15 out of the box. You can also choose to override the storeLayoutIdsRendered() method for other purposes, e.g. so that layout IDs are only stored when the store is accessed on a mobile device. define( //------------------------------------------------------------------- // DEPENDENCIES  //------------------------------------------------------------------- [ 'ccStoreConfiguration' ], //------------------------------------------------------------------- // Module definition //------------------------------------------------------------------- function ( CCStoreConfiguration ) { \"use strict\" ; return { storeConfiguration : CCStoreConfiguration . getInstance (), enableLayoutsRenderedFeature : function () { storeConfiguration . prototype . enableLayoutsRenderedForLayout = true ; } }; });", "date": "2018-4-25"},
{"website": "Avenuecode", "title": "How to Configure Payment Systems in Oracle Commerce Cloud", "author": ["Rodolfo Ladeira"], "link": "https://blog.avenuecode.com/how-to-configure-payment-systems-in-oracle-commerce-cloud", "abstract": "Current retail data indicates that 51% of Americans prefer online shopping to in-store shopping and that e-commerce is growing at an average rate of 23% year-over-year. Because of this, r etailers and companies in other diverse verticals have recognized that pursuing online sales is the easiest formula for profit. Establishing a strong online presence, however, often requires the use of multiple tools and procedures to help companies pursue customers, improve processes, and keep an eye on sales conversion rates. What if we told you that there's one comprehensive tool that makes it easy to build a fully-functional e-commerce system from the ground up? This tool is called Oracle Commerce Cloud. OCC is a fast, stable, and scalable technology that makes it the top-choice tool for high-level e-commerce growth and performance . Today we'll take a look at how OCC handles payment systems, which is arguably the most important component of e-commerce success. When it comes to payment, there are two major concerns: payment options and security. This article will address how OCC handles payment options. Oracle Commerce Cloud includes several built-in integrations with payment gateways. Each gateway handles a different type of payment: CyberSource for credit cards, Chase Paymentech for credit cards and gift cards, PayPal Express Checkout for PayPal, and PayU Latam for international checkout (see figure above). Administrators also have the option of adding custom integrations for alternative payment gateways. Each gateway has a unique configuration, which we'll address individually. Each also has a specific strategy for validating payment information. It's important to note that OCC never stores sensitive shopper information under any circumstance. CyberSource comes as the default payment gateway. To configure CyberSource , go to Menu->Settings->Payment Processing->Payment Gateways. In this panel, select the \"CyberSource\" option from the Service Type menu drop down. Next, set up a CyperSource merchant account with a HMAC-SHA256 signature. Account information and Preview configuration fields must also be filled in, as indicated below: Once the merchant account signature has been validated, shoppers will be allowed to proceed with transactions. Shoppers' payment information is sent directly to CyberSource and is never stored on Oracle servers. This means that CyberSource is directly responsible for payment security. In order to secure payments, CyberSource replaces payment information with a unique code called a payment token. This token is shared between CyberSource and the merchant's Oracle server.  In every stage of order processing, including payment collection, refunds, and shipping, the token is used in place of sensitive payment information. To configure Paymentech , go to Menu->Settings->Payment Processing->Payment Gateways. Next, select \"Chase Paymentech\" from the Service Type drop-down menu. This will allow you to set up a Chase Paymentech merchant account in order to use this integration with a generated HMAC-SHA256 signature. Once you create your merchant account, shoppers are allowed to proceed with transactions. See the figure below and note that OCC sends data related to credit cards and gift cards to the Oracle Server, which then passes it on to the Chase Paymentech Server. The gateway processes the related data and creates a corresponding transaction reference number and an authorization code, which are also sent back to the Oracle server. Because the gateway does not support tokenization via the Orbital Customer Profile Management , shoppers must re-enter credit card and gift card details for all subsequent transactions . To configure PayPal , go to Menu->Settings->Payment Processing->Payment Gateways. In the opened panel, select \"PayPal\" from the Service Type drop-down menu. Then, set up PayPal with a Business or Premier PayPal account: Because PayPal is a third-party service, shoppers will be redirected to the PayPal page for checkout. Then, when PayPal confirms the payment, OCC users have the option of collecting funds immediately or later (e.g. when the order is shipped): As per Oracle's documentation, this gateway allows your store to accept payments from Argentina, Brazil, Colombia, Mexico, Panama, and Peru through WebCheckout, PayU’s hosted checkout solution. To configure PayULatam , go to Menu->Settings->Payment Processing->Payment Gateways. In the opened panel, select \"PayU Latam\" from the Service Type drop-down menu.  Next, set up a PayU business account. Note that a sandbox is also required: As PayU and PayPal work similarly, shoppers will be redirect to a customized PayU page during checkout. Unlike PayPaul, PayU accepts many different types of payments and other configurations specified in your Business PayU account. Also unlike PayPal, OCC users do not have the option of collecting funds later when processing payments through PayU. To configure integrations for processing orders through custom payment types, OCC customers must use the Generic Payment Framework. Generic payments are not supported by default, but Oracle Retail Order Management APIs can support sending payment details separately for an order. Whenever a generic payment is used in Commerce Cloud, payment details are not sent and the order is put into an error state. Once you update the payment details for the order, the Oracle Retail Order Management System changes the order to an open state. This is an important and powerful feature of OCC that we discussed in more detail last week . To learn more about Oracle Commerce Cloud, be sure to check out our other articles introducing OCC , detailing why it's a leading e-commerce solution , and showing how easy it makes product creation . This article provides a brief tutorial on how to configure payment gateways in Oracle Commerce Cloud. These gateways are the key to allowing OCC users to set up payment options on their websites. OCC users can even create custom payment gateways . Allowing shoppers multiple methods of payment is an imperative component of e-commerce business success. With OCC, this payment configuration becomes easy, safe, scalable, and customizable. Learn more about why we at Avenue Code are proud to partner with Oracle Commerce Cloud.", "date": "2018-5-2"},
{"website": "Avenuecode", "title": "Try Dev Box Testing And See How Much Time You Can Save", "author": ["Leonardo Gallardo"], "link": "https://blog.avenuecode.com/dev-box-testing-how-agile-testing-can-save-your-time", "abstract": "Every time a developer pushes a change scripts, it automatically triggers dozens of tests and validations. Some other developer reviews the code and makes improvements, changes are made, some more pipelines run, and then the code is automatically deployed to an environment where a QA Engineer can start testing. During the first five minutes of testing, the QA Engineer notices that the developer has forgotten a simple, but necessary requirement. He then creates an issue in some track system, and the whole process repeats, perhaps many times. Does this sound familiar to you? It shouldn’t. Let me explain why. We're so stubbornly set in our processes, that we forget an important Agile manifesto value: 'Individuals and interactions over processes and tools.' That's what Dev Box Testing is all about - setting the tools aside for a moment, and interacting with human beings. Here's why: So what are the main advantages of this technique? Here's a diagram to help you visualize how much time you can save compared to a normal bug fixing cycle: It's a free and easy  time saving technique that's simple to implement, so why not give it a try? Whether you're a QA or a developer, you can bring this culture into your team. Happy testing!", "date": "2017-11-16"},
{"website": "Avenuecode", "title": "AC Spotlight - Alarico Assumpção Júnior", "author": ["Andressa Lopes"], "link": "https://blog.avenuecode.com/ac-spotlight-alarico-assump%C3%A7%C3%A3o-j%C3%BAnior", "abstract": "Alarico Assumpção Júnior, President of Fenabrave, tells us about the history of the automotive industry in Brazil and how Fenabrave is evolving digitally to meet new consumer demands. Avenue Code: You’ve worked in the automotive sector since you were 14 years old, and today you’re the President of Fenabrave. Can you tell us more about your career journey? Alarico Assumpção Júnior: I was born and raised in the automotive and distribution industry. My children work with me, and soon my grandchildren will as well. My father was the first Volkswagen dealer in the State of Minas Gerais in 1959, and I worked at his dealership in every department. When he passed away, I dedicated everything I accomplished in my career to him. I have no other story: I like to say that anyone who listens to me will hear about my roots and my family. When my father was a dealer in `59, there were no brand associations dealerships, so he founded UNIMINAS to have a collective voice. When Volvo came to Brazil, we were the first dealers in the country, and I presided over ABRAVO for 12 years. I was also on Fenabrave's board of directors and was invited soon afterward to become Fenabrave’s VP in the truck and bus sector. I am very happy, and I love what I do! Today, we are proud to have 30 companies in Brazil, most of which are in the distribution sector. For the second year in a row, one of our companies was elected by Transporte Moderno magazine as the best truck and bus dealership in Brazil out of 900 companies. This is a very big responsibility for all of us. Fenabrave has 52 dealerships, representing the entire automotive segment. We also have 23 regional offices in almost all states, representing 7,300 vehicle dealerships that directly employ 315,000 people. According to 2019 data, our sector participates with 5.12% of GDP. AC: We are now facing a very unusual situation that has led many companies to rethink the way they do business. Can you explain how Fenabrave sees the pandemic? AA: In 2015 and 2016, the industry went through a very difficult time. We lost 2,100 dealerships in 2 1⁄2 years, and 172 thousand jobs were also lost. At that time, more brands came to Brazil. During the pandemic, which is more challenging than we expected, our activities were paused for a long time, and we lost a lot in sales. The two main challenges that I see here are: liquidity and lack of predictability. 90% of dealerships are small and medium-sized companies, hence the lack of liquidity. Regarding predictability, not even infectious disease specialists or other experts are able to predict when the pandemic will end. These are the primary challenges for our industry. AC: At Avenue Code, we are increasingly watching our customers ask for help to improve customer experience through technology. Do you see this as a differentiator in the automotive industry? AA: Before the pandemic, Fenabrave hired ICDP, a European-based institute that works for several international brands and federations. We researched the role of the dealership today and the demand for its services in the future, and we discovered that we will continue to provide value for a long time. As much as we increasingly rely on cutting-edge technologies for our continued market viability, these same technologies cannot serve our remote locations, which rely on in-person representatives. Dealers offer more than just commodification. Fenabrave offers market intelligence (fleet database), political representation, and educational opportunities for employees through Fenabrave University. AC: Digital transformation is essential for market survival. How do dealerships see this? AA: We firmly believe that digital transformation is extremely important for the maintenance and survival of our sector in the market. The pandemic has only accelerated the transformation process, which has been ongoing for a long time. It is essential to offer customers a way to understand a vehicle through apps and the internet so that, when they arrive at the dealership, they already want to know more about the vehicle at the showroom. Therefore, we can walk hand-in-hand with technological developments, always adopting the best of what tech has to offer. AC: How does Fenabrave see the use of Agile methodologies? AA: Fenabrave seeks a balance between remote and face-to-face service. Agility is fundamental for digital relationships; during the pandemic, things became more Agile as consumers demanded faster service. Dealerships must remain focused on the customer and use this digital strategy to attract the customer, who still prefers to close deals in person. AC: In the 2015 Dealer Magazine editorial, you use the mantra, “Make a difference. Innovate to win!” What has changed since 2015, and what are the next steps, since innovation is an ongoing process? AA: From 2015 to today, what has changed the most is the role of technology. We are working more with digital tools to help us create new ways of contacting leads. We are always improving our process and developing new approaches and techniques that bring us closer to consumers so we stay competitive in the market. We are always ready to face challenges. AC: Thanks for telling us your personal story at Fenabrave and for your insights into how the automotive industry is evolving today, Alarico!", "date": "2020-9-1"},
{"website": "Avenuecode", "title": "JSR 354: The Java Money API", "author": ["Jonathan Ohara"], "link": "https://blog.avenuecode.com/jsr-354-the-java-money-api", "abstract": "For a long time, Java  didn't have a satisfactory way to represent money. Early versions exhibited issues with currency, precision, rounding, and formatting, and subsequent versions only partially improved these issues. Now, however, we have the best Java specification for handling money to date: JSR 354. In early Java  versions, we had to use float and double to work with money, and it was hard to deal with the money's currency, precision (see IEEE-754), rounding, and formatting. Then the BigDecimal and DecimalFormat entered the scene. These helped improve precision and made formatting slightly easier, but there were still issues. At last, in Java 1.4, Currency class was released to improve money formatting. Nevertheless, users still encountered problems like having to create a couple of objects to deal with money amounts, not having a proper way to deal with currency conversions, not having support for new currencies (like crypto coins), and having difficulties formatting money amounts to different regions. This is the context in which JSR 354, the Java specification for handling money, was introduced. While the first release of this JSR was early in 2015, the current version is 1.0.3 and resolves the above-mentioned issues. In today's blog, we'll take a closer look at JSR 354's capabilities. Before we do so, it's important to note that the JSR 354 specification is written for Java SE but can also be used in Java ME and Java EE. The reference implementation was written to work with Java 8, but there's a backport version available to Java 7. Since the JSR 354 is still not released in Java ME or EE, we need to include two libraries: the JSR 354 API and an implementation of this API. (If you want, of course, you can make your own implementation.) In this snippet, I'll use the Moneta implementation. (See the references section at the end of this snippet for important links.) Maven: Gradle: This API adds a lot of classes, interfaces, utility classes, and builders (builders are very popular today). In this snippet, we'll review its main features. I've also provided an example in the references section showing a simple usage of the JSR 354. The main class of this API is the Monetary class. From this class we can reach currencies, rounding, and some factories. We have two main ways to get the CurrencyUnit: There's a CurrencyQueryBuilder to find the currency that you want: There's also some important information inside the CurrencyUnit like the currency code and the number of fractal digits shown here: The MonetaryAmount is the object that joins the money value with the CurrencyUnit. In the code below, we have two examples: The main differences between Money.of and FastMoney.of relate to precision and performance. Money.of has a a precision of 256 digits while FastMoney has a precision of 19 digits. Of course, we can also get just the currency of the MonetaryAmount or just the number (the value) : With the NumberValue object, we can access some helpful methods, such as getting the amount doubled: Getting just the fractional part: Or just the non-fractional part: We can use the MonetaryQueries to do operations with money: Finally, we also have a builder to set parameters like the precision of the MonetaryAmount object: Another important tool to have access to when we work with money is rounding. When we use Math.round, Math.ceil, or Math.floor for float and double values, the fractional value is removed; the rounding must work with the number of fractional numbers in that specific currency (for instance, currencies like LYD, KWD, BHD and CLF have more than 2 fractional digits). Let's look at an example: We can also use builders. Let's look at how to do that: One of the most amazing features of this API is its ability to convert between currencies. To start, the API uses a provider to get the exchange rate. (The providers use the internet to get the current values, but you can also write your own provider that uses offline resources.) Let's look at this simple example of money conversion: The default exchange provider is the European Central Bank. When you run this piece of code, you can check some messages about reading the rates. The Moneta implementation comes with some other providers as well; you can change the provider as follows: Formatting entails more than just setting the currency code and the value, because some countries use \",\" and others use \".\" to separate fractional digits. There's a difference between the disposal order of currency and value, as seen in the following example: Here is the Builder for that: You can parse a String directly to the MoneyAmount, but you need to set which format the String is using, as shown below: The JSR 354 is a very flexible and extensible way to handle money. These days, applications reach a huge number of countries. With this JSR, we can offer our users a better experience when they visualize money values (in terms of conversion and formatting), and JSR's ability to handle new currencies like crypto coins is another great asset. Below, you'll find a link to a project in my GitHub. In this project, I created a very simple REST application to show some of the operations that I mentioned in this snippet along with an implementation of a new currency as well its own conversion handler. The Java Community Process: JSR Java Money Jonathan Ohara Git Hub", "date": "2018-10-31"},
{"website": "Avenuecode", "title": "AC Spotlight - Jeremy Parker", "author": ["Anna Vander Wall"], "link": "https://blog.avenuecode.com/ac-spotlight-jeremy-parker", "abstract": "(The following article is property of Avenue Code, LLC, and was originally published with permission at TotalRetail on January 7, 2019. ) Jeremy Parker is the founder of Swag.com , a custom promotional products e -commerce retailer. He spoke with Avenue Code about the importance of passionate entrepreneurship and focusing on consumers’ core needs. Avenue Code: Swag . com customizes an impressive breadth of promotional products for Facebook, Google, Amazon . com , WeWork, Microsoft, Harry’s, Starbucks, Netflix, and more. How did you become so driven to create? Jeremy Parker: My dad and brother are entrepreneurs, so I guess it's in my blood. I've also been fortunate to work alongside some amazing entrepreneurs. A few years ago, I started a company with my brother and Jesse Itzler. Jesse wrote the Knicks theme song, \" Go NY Go ,\" co-founded Marquis Jet, is a partner in Zico Coconut Water, and an owner of the Atlanta Hawks. AC: How has a creative degree in film production helped you run an e-commerce business? JP: I wouldn’t have called film production entrepreneurial when I was studying it, but it is. Filmmaking is creating. I realized early on that I didn’t love filmmaking enough to make a career out of it, but storytelling has played a big role in helping me build businesses. With both film and business, there’s risk involved. You go in knowing you may not make it. I’ve run several startups, and some have succeeded and some have failed. AC: How do you move on from failure? JP: I always say to myself \"win or learn.\" When you approach life this way, with this mentality, you're always going to end up ahead. The goal of an entrepreneur, and frankly a person, is to continuously learn and improve. Oftentimes in failure you learn the most. AC: Can you tell us about the journey of Swag . com ? JP: I became interested in promotional products 10 years ago. When I was 22, I launched a high-end T-shirt line. This was in 2007, when the recession hit. So every store we sold to either closed or stopped taking new orders. Not wanting my business to die, I created a market-related pricing model: for every 100 points that the DOW dropped, we would give the customer a discount on their order. This initiative was written about in several major blogs, including Ad Age. It also led me to meet the CEO of MV Sport , one of the largest players in the promotional products industry. I ended up running a creative division under its label, and I started to fall in love with the industry. I worked there for three years and then decided to pursue some other interests. But over the course of my career, I always followed the industry, and in 2016 it hit me: the promotional products industry is a $40 billion industry. There are 30,000 players in the space. There's no clear winner. The industry hasn't changed much in the last 10 years since I was working under MV Sport. However, the buyer has changed. The buyer is now a millennial and has very different needs and wants. I thought it would be the perfect time to build a brand that appeals to today's buyer. AC: What challenges does your business face? JP: Traditionally, promotional product buyers expect a lot of hand-holding. They honestly don't really want this, they're just trained to think that things should be clunky and manual because that's how it has typically been. We're giving them a better option. We're streamlining the entire process and making it really easy to buy quality promotional products that people will actually want to keep. When you're changing someone's behavior, it's always a challenge. However, once they see how easy the platform is, they never look back. In 2016, we launched the business, and we decided we wanted to hold off on building anything until we fully understood the pain points of our customers. What did they hate about the current buying experience, and what did they love? Only after we really understood the challenges and handled tons of manual orders ourselves did we understand the platform that we needed to build. Now the platform is working, and we're able to easily process orders. It's all about awareness and teaching our potential customers that promotional swag buying no longer needs to be a pain. AC: Do you buy or build most of your tech? JP: We buy certain tech, like transactional email and marketing automation — more of the stuff that's not unique to us in any way. But we build whenever possible. We originally intended to use Shopify, but our industry requires dynamic pricing based on quantity, print locations, and how many colors are in the print, and we learned quickly that this wouldn't be possible from an out-of-the-box solution. So we developed our own proprietary platform. For example, now when a customer uploads their logo, our system can not only detect how many colors are in the logo, but also identify the nearest pantone match so that we make sure to print exactly the right shade. There's obviously a big difference between Coca-Cola red and YouTube red, and we need to make sure that our customers are extremely happy with everything that we produce. Now that we're fully custom, we're able to quickly build anything that we think will benefit the customer's experience. There are no barriers. AC: What has been a highlight for you in the last few years? Was there a moment, either personally or for Swag . com , when you knew you were on the right track? JP: For the first year of business, our goal was to make sure we understood the model and what was needed before we built a single line of code. In order to really learn, we had to handle orders manually. The moment we knew we were on the right track was when we got our second customer — WeWork. Our strategy from the beginning was to start from the top by getting a few major companies to be customers. This would give everyone else confidence that we would do a great job for them. We got Facebook first, but really it was the second customer that proved to me we were on the right path. WeWork asked us who else we worked with, and when we said “Facebook,” they felt confident in working with us and gave us a large order to handle. We delivered, and everyone loved the products! That’s when we became confident in our top-down strategy and in our ability to handle large orders and deliver amazing results. This was a defining moment for us. We now have thousands of customers, and I’ll always remember that first WeWork order. AC: E-commerce is changing every day. What are some of the constants that don't change, and what, in your opinion, is the single most important ingredient for success in today’s retail environment? JP: Obviously, successful companies need to offer products that people want to buy. That's a given. But because there are so many companies trying to sell the same thing and get market share, having a strong brand is the only way to stand out amidst all of the noise. Having a strong brand that gives customers confidence is essential. Once you've built a strong brand that differentiates you from your competitors, you have to really focus on making the buying experience effortless by removing all of the friction. Because we want Swag . com to be synonymous with quality, we've had to be extremely careful with our curation, offering only the best of what's out there. Over 95 percent of the products we tested didn't make the cut. We also offer unique, quality items not traditionally found in the promo industry from brands like Bellroy Bags, Knomo London, and Incase. Our belief is that people no longer want to buy throwaway promotional products. Doing so not only costs your company money, as no one will use it, but it also tarnishes your brand — the exact opposite of your intention. Swag . com is a quality source, only offering products that people will actually want to keep. Swag . com promotional product | Credit: Swag . com AC: How is technology influencing Swag . com ’s next move? JP: We're constantly trying to further streamline and automate the process both for our customers to have an easier time finding the right product and placing an order, and also for our operations team to give them the tools to process orders more easily. We like to think of our website as the ultimate salesperson who never sleeps. We're fully investing in our platform because we know that technology improvements will allow us to remove all of the friction. We want the entire process of buying swag to be as effortless as ordering food online. AC: What are your top three books and why? How have they impacted you? JP: AC: If you could give one piece of advice to your younger self, what would it be? JP: Before you start anything, be super passionate about it. There are so many hurdles and challenges you’ll face, so if you’re not passionate, you’re guaranteed to fail. Once you're passionate about something, just start. Don't overthink it. It's not going to work out the way you intended anyway, so there's no point in overthinking it. AC: Thanks for talking with us today, Jeremy. It’s inspiring to hear the story of Swag . com , as well as your thoughts on passionate entrepreneurship based on real consumer needs.", "date": "2019-9-4"}
]