[
{"website": "Lyft-Engineering", "title": "meet naomi the pm of lyfts emergency assistance feature", "author": ["Jennie Braunstein"], "link": "https://eng.lyft.com/meet-naomi-the-pm-of-lyfts-emergency-assistance-feature-13b6eb8c62ab", "abstract": "Data Data Science Engineering Mobile Product Security Lyft recently announced the launch of Driver Emergency Assistance! This feature gives drivers the ability to quickly access critical help directly from the Lyft app. The app will also display important information such as current location and vehicle information, so help can be on the way faster. Most importantly, it enabled Lyft to be there for drivers every step of the way. Read more about it here ! Meet Naomi, the PM behind the launch! We did a Q&A with Naomi to tell you more about this important new feature and what launching it was like. Congrats on the launch! What was a memorable moment from the project? It was great to see how once the feature was launched, the results were immediate. For example, we had a driver that was attempting to use his debit card to get gas. Unfortunately, the card was declined. He ran out of gas and was stranded in an area he wasn’t very familiar with. He used the new Emergency Assistance safety feature to get assistance and suggestions, and as a result, a police officer came to his aide with a gas can. Once the vehicle got started, the officer thanked the Lyft driver for his services in helping keep drunk drivers off the road. It’s incredible to see how Lyft can be there for drivers when they need it most — and ultimately, making our communities safer. That’s an amazing story. How do you manage the responsibility of working on projects that impact user safety and security? I believe everyone at Lyft has a big responsibility. Our mission, after all, is to improve people’s lives with the world’s best transportation and that means giving people both social and economic mobility. It’s critical that we get this right. If a passenger isn’t able to request a ride, they might miss an important job interview. If a driver doesn’t get paid on time, they might not be able to make rent this month. It’s this responsibility that pulled me to Lyft in the first place, and specifically the Insurance & Safety team. The safety of our community is Lyft’s top priority, and it is our goal to make every ride safe, comfortable, and reliable. We design safety into every part of Lyft, and through every team — not just the Insurance & Safety team. Through partnership with other teams, we can ensure safety is a priority that PMs and designers consider when thinking through experiments or improvements to the core experience. Who did you work with to make the launch a reality? I worked side-by-side with a small team of engineers, user researchers, designers, Trust & Safety support leads, quality engineers, and marketers to launch this feature. The area we spent the most amount of time in was user research sessions. A key objective was to ensure that this feature is intuitive and easily accessible. We had multiple in-person usability testing and incorporated feedback from Lyft’s Driver Advisory Council, a group of Lyft drivers representing drivers in different regions of the country, to make sure it’s useful for drivers in case of an emergency. You mentioned working with user research. What is the most interesting thing you have learned about Safety? How language plays such an important role when it comes to safety. Most folks are reluctant to label unsafe experiences with the word “safety” or “unsafe”. Instead, they choose terms like “nervous” or “uncomfortable”. It made me revisit some of our current safety features and messaging to ensure we communicate in a way that is comfortable for people, especially when it comes to reporting safety incidents. What’s different about being a PM on Safety than a PM on other teams? With safety, we had to adjust our definition of a Minimal Viable Product (MVP), since there is a very little room for error. Especially in an emergency, we need to make sure we over-deliver on our promise to provide assistance to ensure we earn — and keep — that person’s trust. By setting a list of requirements for what it means to have a minimum viable safety product, we are able to launch features with both speed and quality. What do you think made Emergency Assistance a successful launch, and similarly, makes someone a successful PM here at Lyft? The key was to focus on ensuring we ship a feature with high quality. We had a clear definition of what that means for safety and multiple opportunities to gather feedback from stakeholders, partners, and our drivers to make sure we get it right. Post launch, it’s important to deep dive into the data to gather insights to guide future iterations. Pairing that with the feedback we got from drivers that used the feature during an emergency helped us uncover opportunities to take the feature from great to excellent. I know you’re very vocal and passionate about diversity at Lyft. What do you think makes it so important? The benefit of having diversity in leadership is that it ultimately creates a more diverse and inclusive organization, and that has a bigger impact on our company and users. One of the reasons I wanted to join the Insurance & Safety team is to bring a different perspective since there is not one definition of ‘unsafe’. The line between an unpleasant to unsafe experience is extremely subjective, and is conditional on a variety of factors — power imbalance, trauma, and discrimination to name a few. It’s critical to have a diverse team since they have been proven to come up with better solutions that have greater impact on our community. We’re hiring! Check out www.lyft.com/careers Stories from Lyft Engineering. 58 Thanks to Elaine Chow and Naomi Yarin . Lyft Product Management Product 58 claps 58 Written by Product Manager, Self-Driving at Lyft Stories from Lyft Engineering. Written by Product Manager, Self-Driving at Lyft Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-12-10"},
{"website": "Lyft-Engineering", "title": "running apache airflow at lyft", "author": ["Tao Feng"], "link": "https://eng.lyft.com/running-apache-airflow-at-lyft-6e53bb8fccff", "abstract": "Data Data Science Engineering Mobile Product Security By Tao Feng , Andrew Stahlman , and Junda Yang ETL is a process to extract data from various raw events, transform them for analysis and load the derived data into a queryable data store. Data engineers and scientists at Lyft build various ETL pipelines which run at a different set schedule to gain insight on topics ranging from the current ridesharing market to the experiences for driver/passenger, etc. A reliable, efficient, and trustworthy workflow management system is crucial to make sure these pipelines run successfully and deliver the data on its set schedule. Apache Airflow is a workflow orchestration management system which allows users to programmatically author, schedule, and monitor data pipelines. Lyft is the very first Airflow adopter in production since the project was open sourced around three years ago. Today, Airflow has become one of the most important pieces of infrastructure at Lyft which serves various use cases: from powering executive dashboards to metrics aggregation, to derived data generation, to machine learning feature computation, etc. In this post, we will share our experiences on how we run Airflow at Lyft. For context around the terms used in this blog post, here are a few key concepts for Airflow: DAG (Directed Acyclic Graph): a workflow which glues all the tasks with inter-dependencies. Operator : a template for a specific type of work to be executed. For example, BashOperator represents how to execute a bash script while PythonOperator represents how to execute a python function, etc. Sensor : a type of special operator which will only execute if a certain condition is met. Task : a parameterized instance of an operator/sensor which represents a unit of actual work to be executed. Plugin : an extension to allow users to easily extend Airflow with various custom hooks, operators, sensors, macros, and web views. Pools : concurrency limit configuration for a set of Airflow tasks. For other Airflow terminologies, please check out Airflow documentation for more details. The graph shows the Airflow architecture at Lyft: As illustrated in the above graph, there are four main architecture components: WebUI : the portal for users to view the related status of the DAGs. Metadata DB : the metastore of Airflow for storing various metadata including job status, task instance status, etc. Scheduler : a multi-process which parses the DAG bag, creates a DAG object and triggers executor to execute those dependency met tasks. Executor : A message queuing process that orchestrates worker processes to execute tasks. There are quite a few executors supported by Airflow. For example, the Kubernetes(k8s) operator and executor are added to Airflow 1.10 which provides native Kubernetes execution support for Airflow. At Lyft, we leverage CeleryExecutor to scale out Airflow task execution with different celery workers in production. Here we show how to deploy Airflow in production at Lyft: Configuration : Apache Airflow 1.8.2 with cherry-picks, and numerous in-house Lyft customized patches. Scale : Three sets of Amazon auto scaling group (ASG) for celery workers, each of which is associated with one celery queue: ASG #1: 15 worker nodes each of which is the r5.4xlarge type. This fleet of workers is for processing low-priority memory intensive tasks. ASG #2: 3 worker nodes each of which is the m4.4xlarge type. This fleet of workers is dedicated for those DAGs with a strict SLA. ASG #3: 1 worker node which is the m4.10xlarge type. The single node is used to process the compute-intensive workloads from a critical team’s DAGs. Numbers of DAGs / Tasks : 500+ DAGs, 800+ DagRuns, 25000+ TaskInstances running on Airflow platform at Lyft daily. There are nearly five hundred DAGs running daily on Airflow. It is crucial to maintain the SLA and uptime for Airflow. At Lyft, we leverage various technologies including Datadog, Statsd, Grafana, and PagerDuty to monitor the Airflow system. Previously, we had a production issue which caused Airflow not to schedule any task for an hour at Lyft. We didn’t have a good monitoring system to understand whether Airflow schedules tasks or not at that time. Hence we built the Airflow “canary” monitoring system which aims to treat Airflow as a black-box and verify that it schedules and executes tasks in a reasonable amount of time. If Airflow doesn’t schedule task within a threshold (10 minutes), the oncall will immediately get a page notification for the issue. We monitor Airflow overall system health in three aspects: Airflow scheduler and worker availability health check We use Airflow “ canary ” monitoring DAG in production which does: A connection check with a simple SQL query (e.g.” SELECT 1”) for all the critical data sources including redshift and Postgres, etc. A celery queue check by scheduling a dummy task to every queue. The “ canary ” DAG helps the oncall to answer the following questions: how long it takes for the Airflow scheduler to schedule the task (scheduled execution_time — current_time). how long it takes for celery worker to pick up the task. how long the task runs. Airflow UI / Web server availability We monitor the Airflow web server health check endpoint and trigger a page notification if the numbers of healthy hosts are less than certain thresholds. Airflow Uptime for 7 days, 30 days, and 90 days The uptime is measured by 100% - %downtime. Airflow is down when either Airflow scheduler, workers, or the web server are down. Other important metrics for monitoring: Schedule delay: this metric as mentioned above gives us insight not only on the latency of the Airflow scheduler but also the end-to-end system overall availability. The numbers of tasks that are in queued state vs running state for every celery queue. The numbers of occupied slots for every celery worker. This stat shows whether the worker is fully occupied and needs to scale out. The numbers of DAGs and how long the Airflow scheduler takes to parse DAG. The multi-tenant isolation of the UI in Airflow 1.8.2 has documented limitations. It is hard for us to answer users’ questions like: “who paused my DAG? ”, “who marked the task for my DAG failed / success?” and “who changed the state of my DagRun?”, etc. We leverage the existing Airflow log model and Flask signal to implement an audit log for actions taken via the Airflow UI. It will send a UI signal which triggers a callback to log the related information (who did the action, what was the action) whenever a Flask UI endpoint is accessed. The above figure shows someone (removed from the picture) turning off a DAG named “ fact_user_messages ”. This feature helps us to answer these kinds of questions easily. We would like to customize the task instance model view UI panel based on its operator at Lyft. For example, we would like to display a Qubole (3rd party hive computation platform) query link for the QuboleOperator, show the Kibana logs for all the operators, link to the internal data portal table detail page for the HiveOperator, etc. The above graph shows that we provide a Kibana log link for the Airflow task instance in the UI panel. The feature is very generic and allowed developers to customize and associate various links with the operators. We have contributed and submitted the feature back to upstream. The Graph View tab in the Airflow UI is great for visualizing dependencies within a DAG. However, some DAGs at Lyft have dependencies on other DAGs, and the Airflow UI doesn’t provide a good visualization of these inter-DAG dependencies. Our intern, Tirath , built a tool that visualizes the DAG lineage/dependency and allows a user to filter the upstream/downstream dependency for a given specific DAG. It looks for inter-DAG dependencies expressed via ExternalTaskSensors to compute the dependency graph and leverages Airflow web view plugin for display. In the future, we will enhance the tool to show the DAG state in DAG dependency graph in real time which helps data engineers to understand any ongoing issues related to their DAG. One pain point we have with Airflow at Lyft is that it takes a very long time to load the UI for certain DAGs. We have hundreds of DAGs running in production, some of which have hundreds of tasks. The default Airflow UI loads the DAG tree view with past 25 DagRuns for all the tasks’ information. This quite often triggers UI timeout which prevents the users from seeing their DAG’s execution details. Although we could certainly rewrite the UI for better performance, we found that we could simply reduce the numbers of DAG run for display from 25 in default to 5. This helps to significantly reduce the page load time. We contributed the change back to upstream which allows the user to modify default_dag_run_display_number for the number of DagRuns for display in the Airflow configuration file(A irflow.cfg ). Airflow provides various configurables to tune the DAG performance. At Lyft, we suggest users tune the following variables: Parallelism : This variable controls the number of task instances that the Airflow worker can run simultaneously. Users could increase the parallelism variable in the Airflow.cfg . We normally suggest users increase this value when doing backfill. Concurrency : The Airflow scheduler will run no more than concurrency task instances for your DAG at any given time. Concurrency is defined in your Airflow DAG as a DAG input argument. If you do not set the concurrency on your DAG, the scheduler will use the default value from the dag_concurrency entry in your Airflow.cfg . max_active_runs : Airflow will run no more than max_active_runs DagRuns of your DAG at a given time. If you do not set the max_active_runs on your DAG, Airflow will use the default value from the max_active_runs_per_dag entry in your Airflow.cfg . We suggest users not to set depends_on_past to true and increase this configuration during backfill. Pool : Airflow pool is used to limit the execution parallelism. Users could increase the priority_weight for the task if it is a critical one. Airflow scheduling latency could be measured by the delay between the time where the dependencies of a task are met and when the task actually starts. At Lyft, we tune the following configs to reduce the latency: max_threads : Scheduler will spawn multiple threads in parallel to schedule DAGs. This is controlled by max_threads with the default value of 2. The user should increase this value to a larger value (e.g. the number of CPUs where scheduler runs-1) in production. scheduler_heartbeat_sec : User should consider increasing scheduler_heartbeat_sec config to a higher value (e.g. 60 secs) which controls how frequently the Airflow scheduler gets the heartbeat and updates the job’s entry in the Airflow metastore. Developers from the community have contributed couples of fixes in Airflow upstream to further reduce the latency. We spend considerable efforts to improve Airflow’s reliability. Here are a few things worth mentioning: Source Control For Pools : We maintain the Airflow pool configuration in source control and review each team’s pool request with their estimations on the max task slots. The updated pool configuration is applied in runtime for Airflow. Integration Test For DAG : We have integration tests running in Continuous Integration phase, which do checks to ensure Airflow best practices including a sanity check on all the DAG definitions; a start_date parameter check to guarantee all DAGs have a fixed start_date; a pool check to ensure there is no unused pool and a check to ensure pool specified in any DAG actually exists, etc. Secure UI access : We disable write access on a couple of important UI ModelViews (e.g, PoolModelView, VariableView, DagRunModelView) on Airflow. This is to avoid users accidentally modifying Pool, Variable, and DagRun tables in the Airflow metastore from the UI. In this post, we shared how we operate Airflow at Lyft. We described: How the Airflow architecture looks like in general How we monitor Airflow to maintain high SLA How we customize Airflow for Lyft use cases How we improve Airflow performance and reliability in production In the future, we are going to focus on improving Airflow security for the multi-tenant environment. We also plan to have a second blog post to share how we build various toolkits for Airflow to boost internal ETL developer productivity at Lyft. Special thanks to all the team members on the Lyft Data Platform team, especially Maxime Beauchemin, Tao Feng, Andrew Stahlman, Junda Yang, Max Payton, Chao-Han Tsai, Alagappan Sethuraman, and Jinhyuk Chang for their contributions on maintaining Airflow’s SLA at Lyft and Shenghu Yang, Yuko Yamazaki, Prashant Kommireddi, and Guy Bayes for their guidance. Thanks to Maxime Beauchemin and Mark Grover for the review. Este artículo también está en español: eng-espanol.lyft.com. Stories from Lyft Engineering. 1.98K 10 Big Data Apache Airflow Etl Engineering Data 1.98K claps 1.98K 10 Written by Apache Airflow PMC and Committer | Co-creator @Amundsen Stories from Lyft Engineering. Written by Apache Airflow PMC and Committer | Co-creator @Amundsen Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-05-27"},
{"website": "Lyft-Engineering", "title": "what i learned about cross functional collaboration from being a pm at lyft", "author": ["Hadar Dor"], "link": "https://eng.lyft.com/what-i-learned-about-cross-functional-collaboration-from-being-a-pm-at-lyft-9bfb77c747ab", "abstract": "Data Data Science Engineering Mobile Product Security Hey everyone! I’m Hadar. I am the Product Manager for our Cities Team, responsible for keeping Lyft compliant with regulations and proactively building relationships with the cities we operate in. Outside of work, I’m passionate about music and social impact endeavors to economically empower people across the world. Before Lyft I was a PM at Postmates , Kiva.org , and Quixey . Before that I was a student at UCLA, where I founded LA Hacks . I decided to join Lyft for two reasons: I believe that in the first decade or so of your career your #1 priority should be optimizing for personal growth, and I’ve found that Lyft has some of the best mentorship of any tech organization out there. My long-term goal in life is to maximize my social impact, and I resonate strongly with Lyft’s goal to economically empower people who choose to drive with us. Part of my role includes collaborating across many, many teams here at Lyft. Here are some of my thoughts on tips and strategies for successful cross-team collaboration: Who my team collaborates with The Cities team works closely with about 15 cross-functional teams across Lyft. That means several hundred people across Lyft HQ’s day-to-days are impacted by the tools that our team ships! These teams span legal and compliance, various operations teams, customer support, and sibling product teams in Driver and Marketplace. Why I think cross-functional collaboration is so important When most people think of what PMs do, they think of leading a team of engineers, designers, and data scientists to ship a technical product. While that’s of course a big part of what PMs do, I want to also draw attention to how important cross-functional relationships are in the success of your product. Incorporating the diverse perspectives of cross-functional teams is necessary when building the best possible solution for our users. Every team at Lyft has a unique perspective on our users as a result of their unique touch points with them, and also has wide variety of backgrounds and perspectives to draw from when thinking about about all the different factors that go into designing the best possible experience for our Drivers and Passengers. We’re all one team. Without customer support, our users would get lost and leave when anything went wrong. Without our operations teams, our users wouldn’t have high-touch experiences to guide them towards making the most of our service. Without Lyft’s legal and policy teams being the experts in how to ensure we’re always compliant with regulations across where we operate, we wouldn’t be able to do our part in being good citizens in the cities in which we operate. The list goes on, and I’ve seen time over time how these teams are the most valuable make or break point for our users’ experience. My philosophy around good collaboration boils down to three things: Communication: Optimize for oversharing to avoid things you ship negatively impacting partner teams Be concise — optimize for the medium, length, and amount of context your audience will be most receptive to Have regular meetings with stakeholders to build relationships and build consensus around what our shared goal is moving forward Documentation: Write down everything: notes from 1:1s and team meetings, actionables, etc. Have one concise and centralized doc for every initiative Give your stakeholders ample time to view, suggest updates, and prepare before you ship Feedback: Misalignment and miscommunication will happen Get regular feedback from your stakeholders to understand how you can better facilitate Record it, internalize it, act on it, and get continuous feedback to make sure it’s working Here are some best practices that help me keep our cross-functional team on track: Make sure we’re all aligned under the same goal, same way of quantifying it, and same methods for keeping track of progress towards it Recurring 1:1s with leaders of the teams I collaborate with most to make sure we’re on the same page on strategy Recurring syncs with stakeholder teams to make sure everyone on the team is aligned on how we’ll achieve our goals and how we’re prioritizing our projects Have a well-maintained doc for every person you do 1:1s with, every cross-team meeting you lead, and every initiative your team is a part of. Use this to maintain a record of what was talked about and what action items remain to take care of Here’s how I evaluate if everyone is already collaborating well when joining a new team It all comes down to (1) if the team members are happy with how collaboration is going, and (2) if the team is producing the results that are necessary to move forward in the right direction. When I join a new team, I meet with everyone on it to get their take on the above. They’re my customers and any process we add to improve collaboration is a product meant for them. And it doesn’t end there — I’m always re-evaluating with folks how we can continually improve collaboration! Tips for resolving conflict among a large group of stakeholders Misalignment and miscommunication will happen. It’s inevitable. When a lot of people are on the same team and care a lot about achieving something good for our users, conflict can and will arise on how to best achieve that. Ultimately, conflict among stakeholders usually stems from a lack of clear understanding of goals and a lack of alignment on a framework for decision-making. Clear those up, and things should run much more smoothly! I’ve found the following checklist to be helpful when resolving conflicts in a team: Align on goals and success metrics: What problem are we trying to solve? Which users are we serving and what do they need? How do we know we’re succeeding as a team? What metric(s) are we optimizing for? Shared framework for decision-making: Are we all using the same framework to decide what the right thing to do is? If not, let’s first align on a framework, and then apply it to the situation at hand. Shared context: Does everyone in the conversation have all of the context that they should have to make the right decision? Everyone should have access to all the information that everyone else in the conversation has. Writing all of this down in a central place is crucial to make sure everyone aligns on the same path forward as well. Learning collaboration skills First of all, learning how to better collaborate is a never-ending journey! There’s always more to learn and grow in being a better team collaborator, as for any other PM skill :) Getting to where I am today came from joining teams, making an effort to create a state of good collaboration, getting feedback from teammates on how it’s going, and getting mentorship from other PMs in my organization on how to uplevel my cross-functional collaboration skills. And most importantly, making mistakes and learning from them! For follow-ups, check out these resources PM articles/books I think are worth reading: pm.hadardor.com Frameworks I use to approach other things as a PM: frameworks.hadardor.com Email me at hdor@lyft.com We’re hiring! lyft.com/careers Stories from Lyft Engineering. 302 1 Thanks to Ryan Lane . Product Management Lyft Product Collaboration Cross Functional Teams 302 claps 302 1 Written by Currently career hiatus. Formerly Product @Lyft, @Postmates, @Kiva, startups. hadardor.com Stories from Lyft Engineering. Written by Currently career hiatus. Formerly Product @Lyft, @Postmates, @Kiva, startups. hadardor.com Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-01-29"},
{"website": "Lyft-Engineering", "title": "empowering personalized marketing with machine learning", "author": ["Beatrice Girard"], "link": "https://eng.lyft.com/empowering-personalized-marketing-with-machine-learning-fd36e6bdeca6", "abstract": "Data Data Science Engineering Mobile Product Security Personalization is one key component of modern customer engagement programs. At Lyft, it contributes to sustainably growing markets and building a unique relationship with drivers and passengers. The ways to personalize the Lyft experience are endless. There is indeed an amazing diversity of passengers in terms of needs, preferences, expectations and past experiences. Taking this diversity into account is essential to send relevant communications (emails, coupons, ride passes…), deliver valuable experiences and improve customer retention. In this post, we will go through an applied example of solving a personalized marketing problem using machine learning and optimization techniques. The first step of most applied ML/Optimization work consists in understanding the business side of the problem. What is the underlying business question? Which resources are available? What are the key metrics? As an example, let’s assume that a business partner just sent the message below to some data/research scientists at Lyft. Formulation 1 Hey there! We’re going to have too many drivers on the road next week, we want to keep them busy. Can you find me good ways to get more passengers? And tell me in advance how efficiently we will do it? A critical aspect of a scientist’s job consists in reformulating that message into a concrete tractable personalized marketing problem that we can solve using machine learning and optimization techniques. Sending personalized offers to some of our passengers can be a good direction to address this need. Marketing offers can be price-related (coupons, ride passes, subscriptions etc.) but also experience-based (service upgrade, lower ETA, additional services etc.). But let’s keep things simple and focus only on coupons. The different resources available to solve this problem are then: An inventory of offers (different types of coupons we could send) A pool of passengers with different profiles Some investment budget We can now work on defining key metrics. Here is John, Lyft’s passenger in NYC, who received a coupon from Lyft. With this coupon on his account, John is likely to take more Lyft rides than what he would have without. This lift is called “Incremental Rides” . However, we only observe John with a coupon. To estimate the incrementality, we need to estimate the number of rides that John would have taken without a coupon on his account i.e. the counter-factual. On the other side, sending coupons has a cost for Lyft. Passengers receiving them don’t pay the full price of the ride. Lyft is covering the difference so that drivers are not impacted. The cost of sending a coupon to John can be written as: Note that we could use more refined return and cost metrics, taking into account the additional profit generated by incremental rides, interferences with the marketplace … In the end, we have a much more defined problem: Formulation 2 Which offer from the inventory [Resource] should we send to which passengers [Resource] to maximize [Business Need] the number of incremental rides [Metric] given some budget constraints [Metric, Resource] and keep control over variance [Business Need] ? How machine learning and optimization techniques can help solving this question? Let’s go step by step. First, we need to understand how key metrics move when we give different groups of passengers different marketing offers from our inventory. We start by building a training dataset by randomly giving out offers to these different groups of passengers and tracking our key metrics. Then, we can build some machine learning models (Tree-based models or neural networks) to predict incremental rides and cost, given passenger features and offers characteristics. This is a hard problem in itself and requires consequent feature engineering and modeling work. These models can be at the individual level or at the segment level (groups of passengers with similar reactions or features). In the following steps, we will assume that we have predictions at the segment level. Once we have predictions, we can optimize the outcome by solving the following problem: Simplifying the expression using matrices, we have: Depending on budget and population constraints (i.e. parameter N ), it is not systematically the combinations segments/variants with the lowest Cost per Incremental Rides that will be chosen. These formulations don’t take into account the uncertainty inherent in our predictions and optimization. However, human’s behavior can vary quite a lot. Passengers don’t have the same schedule every week, they are not always in the same mood, they don’t react exactly the same way in the same situation… The only thing that we can actually predict is the probability of someone having some behavior. For example, we cannot predict with 100% confidence that sending a coupon to John will generate 2 incremental rides and cost $2. However, we can say that it is not very likely to generate 100 incremental rides at no cost. In other words, matrices R and C follow a joint distribution of probability . For a given segment of passengers receiving a coupon, incremental rides and cost are following some distribution like the one below: Why do we care about this randomness? The thing is that we don’t want to hit our budget and volume targets only “in expectation”. We want to be as close as possible every time. There is business value in being able to accurately predict the outcome, even if this implies losing slightly on efficiency. Less or more incremental rides than forecasted can damage the marketplace, falling far from the budget isn’t good for the business as well. We have to take this stochasticity into account and rewrite the optimization problem: In other words, we are relaxing the budget constraint to be able to reduce variance around cost and incremental rides. Switching from Formulation 3.2 to 4 has an impact on the shape of both cost and incremental rides distributions (< C , A > and < R , A > distributions): we have some additional bias but less variance around the mean. To summarize the entire pipeline, we have: Defined metrics and available resources Acquired data Built some predictive models for incremental rides and cost Built an optimization program managing variance However, this is not the end of the story. Unexpected seasonality trend or significant changes in user behaviors can happen. If we never refresh training datasets or retrain models, predictions can quickly be far off actuals and model performances can drop. This is why it is critical to log experimental results and keep learning from them. Note that the data coming from targeting experiments is different than the data coming from drops on random population samples. The distribution of covariates (user features) can completely differ due to the targeting logic applied. We then built a way to remove this targeting bias (distribution matching) and keep exploring the feature space (explore/exploit strategies). This allows us to have a sustainable learning cycle. At a higher level, using machine learning and/or optimization tools for marketing projects can be powerful and personalization models can add significant value to experiences, products and communications. Personalization is a promising research field and Lyft Science is hiring! You can read more on open research and data science roles here If you enjoyed this post, don’t hesitate to share! To learn more, check out other Lyft’s Data Science posts. Special thanks to Keith Henwood, Su Wang, Cam Bruggeman, Varun Pattabhiraman, Alex Wood-Doughty, Hector RdB, Thibault Martin, Alok Gupta and Jose Abelenda for their insightful reviews and edits. Stories from Lyft Engineering. 777 5 Thanks to Ryan Lane . Targeting Machine Learning Data Science Personalization Customer Engagement 777 claps 777 5 Written by Research Scientist at Lyft Stories from Lyft Engineering. Written by Research Scientist at Lyft Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-12-04"},
{"website": "Lyft-Engineering", "title": "what to expect when interviewing as a pm at lyft", "author": ["Dan Barak"], "link": "https://eng.lyft.com/what-to-expect-when-interviewing-as-a-pm-at-lyft-fd13634ca381", "abstract": "Data Data Science Engineering Mobile Product Security This post is direct and personal, so let me introduce myself. My name is Dan Barak — I’ve been at Lyft for almost 2 years, supporting the Growth Platforms teams, after spending more than 4 years at Facebook. I’m also very passionate about recruiting and interviewing and am actively working to make these processes better at Lyft. At Lyft, we view hiring as one of the most important and leveraged responsibilities PMs have. Our goals are to both hire great talent, as well as provide the best experience for candidates. That probably sounded a little too vague and corporate-y, so let’s bring it down to earth: We want you to succeed — the interviewer is on your side! Hard to believe? Not really… your success usually means the conversation is stimulating and interesting (we’re not here to check boxes) and we’d love to work together. You should know what to expect. This post is the first step and we have awesome recruiters that will guide you through the process. Moreover, each interviewer will open the conversation with an overview of what this interview is about and their expectations. If you feel you don’t know what to expect, please let your recruiter know. This isn’t a loophole to ask for the question ahead of time, but nice try :) The interview is mutual. This process is a representation of us as a company and we hope you like what you see. Deciding which company to work for the next several years of your life is an important moment for you and we want you to make an informed decision about how the next phase of your career. Lyft’s Core Values Bear with me… you should already know I’m not about empty slogans :) You’ll get plenty of chances to prove you embody the latter two, but start by being yourself! I cannot stress this enough — we’re not looking for people that fit a mold and diversity is our secret superpower when it comes to creating great products people love and use effectively. One way to practice this is by expressing honest opinions. I once interviewed someone who took hundreds of rides with Lyft and claimed our app was perfect, with nothing to improve on… that’s either not being thoughtful or not expressing your real thoughts. Here’s an easy sell: being your authentic self means you have one less thing to stress over and focus on, leaving you free to think about the problems at hand more clearly. Harder sell: yes, you definitely want to be accepted to all the positions you’ve applied for and choose between them, but you also don’t want to spend the next few years pretending to be someone you are not. Nobody wins in that situation! General interview structure and guidelines We’re trying to get the most signal about your ability to be a successful PM at Lyft in a short amount of time, so using that time efficiently is very important. That means, for me personally, I probably won’t chat you up as we get started, since I find I get less meaningful signal during “small talk”. I will try to put you at ease by clearly explaining what we’re about to do and acting like a human being that cares about you. Each PM has their own style, but remember that each of them wants to make the most out of your mutual time. A word about unconscious bias It’s assumptions and shortcuts our brain makes when we don’t have enough information, and it’s something we go to great lengths trying to prevent, including dedicated in house training sessions. For example, if you and I have the same hobby, my mind might play tricks on me (“this person is like you and that’s good”) and add signal that has nothing to do with you being a good PM. We’re don’t believe in “greater minds think alike, greater minds think like me”. Diversity across the board is our secret power, remember? Interacting with your interviewer Let’s start with a reminder — your success is our success. And a successful interview means we both got the signal needed to help us make a better decision. Since time is short, we might cut you off (as gently as possible) if we’ve either gotten the information we needed regarding the specific rubric we were aiming for, or the conversation is getting a bit off track and we need to bring it back to the focus. Please don’t see this as rudeness, but as us attempting to make the most out of our time together. For most of our interviews you don’t need any domain expertise. Of course, if you’re applying for a senior PM of payments infra and claim 15 years of experience in the field, we’ll test that, but that’s a special case. If any understanding of the problem space is assumed or needed, the interviewers will help with the setup and context so you’re on equal grounds with everyone else. It is a good idea to have used ride sharing at least once (we do provide you with credits to get to our office) and have a very general idea about the complexity of marketplaces. The interviewer will ask you questions and might offer pushbacks or guidance as is required. Remember my tip about being yourself? Your aim is not to please the interviewer, make them feel smart, or anything of the sort. If you think the guidance makes sense, by all means work together with them as you would with any fellow PM, but also feel free to stick to your guns and push back, with ample reasoning. Your interviewer cares much more about the “why” over the “what”, so treat them as you would a team member you’re trying to bring along with you on a journey, not as an interrogator you have to appease with the “right” answer. Finally, a few minutes before the end of your time, it’ll be your turn to ask us questions. Use that time to try to figure out if you would like to come work for Lyft and spend the next several years with people like your interviewer. As I’ve mentioned — we will not sugarcoat answers just to reel you in (and then mutually discover later it’s a mismatch). Ask us about challenges, culture, etc., but please don’t ask us about some niche detail of our product or roadmap — we might not be able to divulge that information, nor will it give you with the signal you need. Interview Roles Being a PM isn’t easy and requires many sets of skills, some more easily quantifiable than others and so we’re focusing on getting signal on everything we care about through several interview types. Product interview The questions we’re trying figure out during this interview are: Can you turn big ambiguous problems into great products? Do you have user empathy, UX chops, the ability to build an MVP? What can you expect? Usually a prompt about a problem space, often related to ride sharing and transportation, but equally likely about any other domain. You’ll play the PM leading this space and trying to decide if we should build anything here or leave it for others to solve, what would we build first and what would that teach us and finally what could be a long term vision. Remember the why trumps the what — there’s no correct solution that you have to get to in order to pass… it’s much more about showing us how you’d approach such a problem and lead a team to solve it. While starting at a high level, this interview can drill down into specific flows and interactions with the product. Lastly, yes, we know that like any good PM you’ll look at tons of data before embarking on the journey and you’ll do user research and focus groups — you don’t need to mention it and it’s fine to make assumptions that are supported by solid arguments. Analytics Interview The questions we’re trying figure out during this interview are: Will you use data to find and solve problems, prioritize correctly? Can you work well with engineers / analysts to run experiments, debug problems? What to expect? We’d like to see you’re comfortable with data and you use it regularly. If that’s to construct a useful dashboard, to understand what might be wrong with your product and to decide what to build next. You won’t need to write any sql in this interview, but will be expected to do some back of napkin math to estimate opportunities and make tradeoffs. It’s important to be specific in this interview and not hand-wavy. If you want to measure impact, please specify what metric exactly and how to measure it. Assume that some of the people you interact with are either junior, or can’t necessarily read your mind and hence accuracy is of the utmost importance. Leadership Interview The questions we’re trying figure out during this interview are: Can you build, motivate and support a team? Can you deal with ambiguity? Get through tough times and learn from failure? What to expect? While this interview is less structured than the rest, it’s still a very important piece of the puzzle. You’ll probably get a mixture of questions about your work and interactions in the past, as well as some hypothetical situations that might arise. We already know you’re awesome as you’ve made it this far, so while tempted to share everything from your past positions so we know you best, make sure you’re driving the right points across. First sign that you’re going a bit off track — you’re not coming up for air and your interviewer hasn’t been able to put a word in in 10 mins. Long monologues are usually not a sign of great communication skills… rather, try to make sure you understand what your interviewer is asking, answer concisely and validate you’re both on the same page. Please be truthful — we know everyone makes mistakes and we even celebrate those learning moments. Being able to bring up failures and how you dealt with them is a good thing. If you seem too good to be true, we’ll suspect you are :) That’s about it. If you have any questions, please get in touch with your recruiter. They will be delighted to help you with anything. Please let us know how we did at the end, regardless if the process continues or not. We truly believe interviewing is the most leveraged thing we can do for the future of Lyft and want to constantly get better. Looking forward to seeing you roam our halls! We’re hiring! Apply here . Stories from Lyft Engineering. 460 2 Interview Interviewing Lyft Product Product Management 460 claps 460 2 Written by Cofounder & Chief Product Officer @ Stackbit, Ex Facebook, Lyft. Stories from Lyft Engineering. Written by Cofounder & Chief Product Officer @ Stackbit, Ex Facebook, Lyft. Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-10-08"},
{"website": "Lyft-Engineering", "title": "growing scientists into manager roles", "author": ["Ricky Chachra"], "link": "https://eng.lyft.com/growing-scientists-into-manager-roles-ce846e5eaf7b", "abstract": "Data Data Science Engineering Mobile Product Security The following is an adaptation of an invited post that I recently published on the blog of Domino Data Lab . The cross-posting is being made with their consent. TLDR: I joined Lyft over three years ago as an individual contributor (IC) on the Data Science (DS) team and transitioned into Science management about four months ago. Along with some personal reflections on my journey at Lyft, this post contains insights that will be helpful to scientist ICs interested in transitioning into management and for companies looking to home-grow their promising data/research scientists into managers. When I joined Lyft in June 2015 as their eighth Data Scientist, Lyft was a company of about 350, housed in a small building in San Francisco’s hip Mission District. It is now well more than 3,500, with staff presence in several major American cities. Seeing both myself and the company work through various stages of our ongoing maturation, I had a distinct vantage point from where I have drawn the following reflections about growing ICs into managers. What first drove me to Lyft is also what keeps me here today, more than three years later. We share many of the same character traits that are best summarized as being friendly, fast-growing, socially and ethically conscious, farseeing, and driven to excellence. I’m proud to confess that after interviewing at Lyft in March 2015, I felt such an attraction that I immediately dropped other companies out of consideration. Where I was in my personal life in mid-2015 could also be said of how Lyft manifested then: we were both relatively junior in the social strata, not decisively sure about our raisons d’être , less certain about our near-term prospects, and still learning to stand in this chaotic world. Now 2018 is almost over, and Lyft and I still mirror each other: we’re more confident, no longer “junior”, our networks have expanded, we have a say in important matters, a clarity in our purpose, and the roadmaps to getting where we’re going. In retrospect, it’s quite incredible that right at the start of my role as a DS individual contributor, I was placed in the highly visible Marketplace team and entrusted to single-handedly drive the development of our dynamic pricing algorithms. These algorithms respond to and regulate the crucial demand-supply balance on our platform and many company executives and general managers pay careful attention to them. Empowered by supportive colleagues, brimming with ideas and determined to make an impact, I had my Lyft -off into the honeymoon period with all engines on full power. Unbeknownst to me, turbulence was brewing behind the scenes in senior and cross-functional manager circles. Within a few months, I came to be in the middle of challenging debates about metrics, project priorities, and serious staffing issues on engineering and product. Several events unfolded that taught me a lot about myself and others, informed my career decisions, and rapidly evolved my views on the practice of management. To keep-up with the bursting headcount growth, some of the most brilliant ICs in data science and engineering transitioned into management. It was the most rational thing to do in the given context, something any fast growing startup would do. In retrospect, though, we failed to appreciate that excellence in technical abilities doesn’t necessarily translate into excellence in management. Today, no one at Lyft would deny that management is a distinct field that is more akin to the arts. Understanding management comes from having experience, formal education, mentorship, a keen observation of role models in action, and self-study. Seeing how manager actions were not always positively affecting my team’s performance, our well-being, and career progression, I promised myself that I would chart my own path to find what it takes to excel in management. Management was not completely foreign to me. Prior to joining Lyft, I was an in-house management consultant within the Chief Analytics Office at IBM. Growing further in this direction seemed like a natural progression. Yet, when I focused on management within the broader field of data science, a bigger can of worms opened. If you look at the rate at which the data scientist population is continuing to climb, you can’t help but realize that this explosive growth must have sounded alarm bells even among seasoned executives. After all, is there another new profession in recent history that has scaled as quickly? The advances in data science have brought us a panoply of choices on multiple fronts. From the ever increasing means of data capture, storage and modeling to algorithms and complex ethical matters, executives have many serious choices to make. However, two factors, the relative infancy of this profession and the uniqueness of every company’s situation, preclude outright resolution of problems in this space by the so-called method of “following best practices” ─ we can’t just imitate what everyone else does. Several iteration rounds may pass before norms start to take hold, if they ever do, causing companies to find solutions to their problems de novo and in situ . That’s all good, but who will lead the search for solutions as the scramble to outcompete continues? Companies that are looking for visionary DS managers are already turning over the first of many stones on the problem trail. There is an industry-wide shortage of DS managers. We felt this shortage acutely at Lyft where, due to rapid growth, the DS IC headcount more than quintupled between mid-2015 and late-2017 while the manager count remained stationary, resulting in a staggering average IC:manager ratio of 15:1! In practical terms, managers, not due to any fault of their own, were spread too thin. Weekly one-on-one meetings felt like extended standups; there were hardly any career discussions or time for two-way feedback. If you asked a manager (or even an IC), what they thought was the purpose of manager:IC relationships, I’m not sure you would be satisfied with the spectrum of answers. If you think about it, could a relationship ever be wholesome until all parties have reached a higher-level understanding of its purpose? Lyft has come a long way in how we go about management now. With more eyes than ever on the details, several committed people are driving rapid improvements in our processes. To say the least, we are increasingly more bottoms-up and geared to accelerate IC-growth. As the leaders at Lyft brought an increased focus on filling DS manager positions, my resolve to transition to the management side was crystalizing. I didn’t have a great measure of confidence, so I started by taking little “risks” like speaking-up more frequently when I saw an issue and offering to help my manager whenever I noted he was getting overwhelmed. With the passage of time, I became increasingly self-trusting, thanks to the positive reinforcement I got from my peers. They listened to my views and they started encouraging me to consider the path to management! My interest in management had previously taken me to IBM, then remained dormant, so it was rejuvenating when it came right back. In an exceedingly supportive environment, I eventually transitioned to the management pathway at Lyft in July 2018. Here I share some insights from my transition journey with an intent to help companies looking to propel their business savvy ICs into management roles. It is also my hope that ICs interested in management will read and reflect on the pieces they need to put together as they chart their own path. By late 2017, when I was sure that I wanted to transition to management, I made my intentions known to my manager. He was supportive and identified two important steps prescribed in the transition protocol at Lyft. First, get a promotion as an IC so I could come to a level where I would be eligible to transition to manager. Getting promoted entailed demonstrating my abilities at the next level and summarizing my impact in a compelling promotion application. Having already been unsuccessful at getting promoted in early-2017, I was not taking anything for granted ─ Lyft has strict standards. I curtailed most of my social activities and worked harder with ongoing projects to cross the finish line ahead of the early-2018 review cycle. Since promotion decisions are made behind closed doors, I remained anxious until the news became official. Second, go through a management fitness assessment by entering a 10 week trial period as the acting manager of two direct reports. The company finalized its decision to move me into management based on the “strongly recommended” rating my evaluators gave me at the end. I spent a considerable amount of time working out how I would perform during the assessment period. My process is summarized below. I was fortunate to have a thoughtful, disciplined, and goal-oriented external mentor whom I met through Everwise , a subscription mentoring platform. I leaned on him to gain an understanding of what steps I should take to improve my chances of finding success in my transition to management. Following his recommendations, I read blogs he pointed me to, I wrote down my personal strengths and development areas, and reflected a great deal on my past experiences of being managed. I also interviewed two science managers at Lyft about their perspectives on what they were looking for in other managers. Before entering the assessment period, I distilled my understanding by identifying four areas that manager candidates should demonstrate strengths in: mentorship, interpersonal relationships, communication skills, and business skills. Although I am not aware of formalized fitness assessment metrics used at Lyft, it seems reasonable that evaluators would be looking at these four areas. Mentoring skills I owe a lot in my life to the great mentors I’ve had. At the same time, I’ve also experienced being on the receiving side of poor mentorship — — both, in academia and in industry. My standard of personal excellence vehemently rejects any compromise on this critically important matter. Early managers will likely have the responsibility of junior ICs as direct reports. For effective mentoring of junior ICs, DS managers must not only possess impactful IC skills at a level well above foundational, but also the ability to coach and groom raw technical talent. I was fortunate that in the three years of working as an IC at Lyft, I mentored two PhD students as interns, both of whom had strong impact in their projects. My interns gave great feedback about me to my manager which infused him with confidence to encourage me to pursue the managerial path. 2. Excellence in managing interpersonal relationships Numerous opportunities allowed me a close study of how my own morale and that of my teammates was affected due to actions resulting from the prevailing perspectives on manager-IC relationships. Through readings and reflections in my personal time, I came to distill the following understanding which I elaborated in a document that I shared within Lyft: To excel in management, individuals need to continuously develop the emotional intelligence that comes with an increasing level of self-awareness. At least four kinds of relationships in the workplace come into spotlight for managers: An empowerment-centered relationship with direct reports, a dependability-garnering relationship with leadership and other DS managers, a collaboration-fostering relationship with cross-functional managers, and most importantly, a strong self-relationship where any residual authoritarian tendencies are constantly negated. Purposeful working relationships can only be nurtured with clarified intentions. 3. Communicating effectively with the technical and non-technical audience Data science literature is rife with subtle terminology encompassing nuances in sundry experimentation and modeling methods, algorithm design, and metrics and their implications. Often times data science managers have to discuss their choices and work with non-scientists (product and engineering) so that meaningful tasks, realistic expectations, and roadmaps and sprint plans can be generated. In addition to communication skills, the ability to inspire a genuine interest in understanding different viewpoints is critical in this function. Once again, I was fortunate that, as often the only DS IC on a project team, I had regular and productive interactions with some of the most patient and outstanding product and engineering managers in the industry. It is my understanding that these colleagues also supported my candidacy to management. 4. Seeing the big picture, as it relates to business, and connecting the dots to the science roadmaps Beyond performance evaluations, promotion decisions, and compensation calibrations, data science managers attend various planning meetings wherein business strategies or roadmaps aligned to them are generated. Naturally, business-savvy DS ICs will be more engaged in such meetings and can have a stronger impact on a roadmap that they will help craft. Thanks to my long-standing interest in business, I had taken more than a handful of courses at Cornell’s business school along the way to earning my PhD in engineering. These courses helped me gain familiarity in common business frameworks and executive communication styles. My astute manager might have taken notice because a few times he had me attend important planning meetings on his behalf. Either managers will identify the ICs they see with management potential or interested ICs will step forward on their own once the company defines the skill requirement and the transition criteria. Grooming can be a long process, but it must reach a place of confidence for a high chance of success in an assessment period. As each individual starts at a different place in their maturation, managers, mentors, and ICs may want to co-create a personalized plan for skill development. For this to be possible in a systematic way, managers may have to purposefully create the environment or perturb existing processes to allow for grooming. My manager started asking me to own low-risk projects that provided me with some of the experience I now need as a manager. For example, I led the recruiting and staffing of our interns two years in a row and I was the ramp-up partner to some new hires. Lyft has significantly diversified since I started. Newer possibilities to engage budding managers are being formalized. In particular, two of our more recent initiatives we’re investing in, learning and development, and diversity and inclusion programs, offer opportunities for ICs to become well-rounded in company matters, whether or not they intend to pursue the management path. Once an individual’s candidacy is sufficiently established and the mutual interest in pursuing the transition to management is crystalizing, managers may consider pairing these candidates with current managers for mentorship. My external mentor was a critical resource in helping me focus on the right things at the appropriate time. Additionally, my manager connected me with a kind software engineering director who generously gave me valuable guidance and shared candid insights on management processes within Lyft. One can imagine how internal mentors can not only guide the mentees but also provide a pulse-check to the company’s leadership about how far along their mentees are in their personal development plan. As every company’s and every IC’s stories and journeys are unique, I surely do not expect the above reflections to apply out of the box. Nonetheless, they should stimulate conversation and movement to help create a healthy DS management environment. DS managers who were once ICs at the same company can undeniably provide the impetus to implement improvements in areas that they had felt lacking. They could also prove to be vital collaborators of managers hired from other companies who bring fresh perspectives and new ideas to the table. We’re hiring! Check out www.lyft.com/careers Stories from Lyft Engineering. 274 Data Science Management Career Advice Personal Development Reflections 274 claps 274 Written by Stories from Lyft Engineering. Written by Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-11-13"},
{"website": "Lyft-Engineering", "title": "meet andrew the pm behind lyfts recent transit launch", "author": ["Jennie Braunstein"], "link": "https://eng.lyft.com/meet-andrew-the-pm-behind-lyfts-recent-transit-launch-ceeb93655f5f", "abstract": "Data Data Science Engineering Mobile Product Security Lyft recently announced the launch of Nearby Transit in Santa Monica! This launch includes public transit information in the main app for passengers. This is a big step forward toward achieving our mission of improving people’s lives with the world’s best transportation. Read more about the launch in TechCrunch , Wired , and our Blog ! Meet Andrew, the Product Manager behind the launch. We had a conversation with Andrew to give you a peek into the product development process, and to show a bit about what it’s like to work as a PM at Lyft. Congrats on the launch! What was the best part? For me the best part of any launch is hearing stories from people using what we’ve built. For Transit Nearby, I distinctly remember one person mentioned that they found our transit integration useful because they were “pretty sure a bus was near here” but weren’t quite sure, and Lyft helped them explore the nearby routes. This case is especially interesting because it’s a use that we weren’t expecting. Who did you work with to make it a reality? The transit team — small, but growing — came together to make this a reality. Our designers, engineers, user researchers, and data scientists all collaborated to learn about what people need from public transit and how Lyft can help. A core part of this process included weekly user research sessions where we asked people to try out our latest prototypes and asked for feedback. We also brought in executive leadership because our feature is front and center on Lyft’s home screen. I’m looking forward to experimenting with new transit features and learn directly from our users what works and how we can improve. How do you think about launching something that — if used by existing Lyft users — could take away from our main business of car rides? Lyft isn’t only in the business of making the best ridesharing app. Rather, it’s in the business of providing great transportation. When viewed from this perspective, it’s essential for Lyft to help people take transit because transit is a core part of transportation. It’s a big change for us at Lyft to expand our offerings to include not just ridesharing but also transit (and scooters!) and this change is something I’m excited to help with. As a San Francisco resident, I’m sure you look for transit all the time. How did you remove your own personal biases and gather customer empathy from more perspectives than your own? It’s hard! This is where product design as a discipline comes in. Our team frequently reminds each other to not rely on our own experiences, but instead to listen and learn from the people we’re trying to help. This can mean bringing people into the office and showing them our latest ideas, and observing aggregate effects after we launch. It also means learning about transit systems across the entire country, rather than simply relying on what I use living in San Francisco. When we traveled to Santa Monica, we realized how important it is to catch the bus when the next one might not come for another 30 minutes, and in NYC we saw how impressive 24 hour service is and how weekend closures make step by step navigation more important even for transit experts. Where were you before Lyft, and how does Lyft compare? I got my start as an APM at Google, wondering why they chose me as someone with a BA from a small liberal arts college. Turns out, nobody cared about my background as long as I could be a sponge for learning. Google’s rotational program gave me a lot of responsibility but also enough support and guidance that I really learned how to be a good PM. Google is a lot bigger now than when I joined, and is working on so many things. It’s refreshing to be at a focused startup that’s just starting to branch out into more products. Why’d you choose Lyft? At some point it became inevitable that I’d join a transportation company. Maybe it was after I learned that building more lanes generally induces additional trips , bringing the road right back up to capacity. Or possibly it was after reading that a highway, railway, or river can cut a community off from its neighbors, making it less likely to be vibrant and safe. Or it could be because I enjoy weird types of transportation enough to pay for a bobsled down from the Great Wall. In any case, I picked Lyft over other transportation companies because it’s growing fast, focused on helping users, and is filled with really smart people. Booking time on your calendar, I see you’re very intentional about how you structure your days with meetings. Do you have any advice or tips for other busy folks to manage their time? I have a hard time context switching and I’m not good at multitasking, so it’s really important for me to carve out chunks of focused, uninterrupted time. I also have a bunch of other commitments outside of work so I need to spend my time wisely. Cal Newport has good thoughts and suggestions on how to structure your time for deep work. What do you think made it a successful launch — and similarly, makes someone a successful PM here? I’d be a bad PM if I didn’t mention that a product needs to solve a user need to be successful. Beyond that, it helps to think about who might feel a sense of ownership over the product and ensure that they’re onboard with whatever you’re shipping. It’s much better to include them from the beginning rather than have them find out when they read the article in TechCrunch. We’re hiring! Check out www.lyft.com/careers Stories from Lyft Engineering. 81 Thanks to Ryan Lane . Transportation Product Management Product 81 claps 81 Written by Product Manager, Self-Driving at Lyft Stories from Lyft Engineering. Written by Product Manager, Self-Driving at Lyft Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-11-06"},
{"website": "Lyft-Engineering", "title": "day in the life of a lyft pm meet lily pm of airports venues", "author": ["Lily Sierra"], "link": "https://eng.lyft.com/day-in-the-life-of-a-lyft-pm-meet-lily-pm-of-airports-venues-71ee44078004", "abstract": "Data Data Science Engineering Mobile Product Security I joined Lyft 1.5 years ago from LinkedIn! My team works on the driver and passenger experience at all airports and venues, the most operationally complex and congested pickup zones. On September 17, I took this selfie series to document my day as a PM at Lyft. I reach my desk with breakfast (cereal and water with lemon provided by Lyft). First, I write my to-do list for the day. I prioritize based on the number of meetings I have. Today, my calendar is quite full, so I will pick 3 tasks. All are analysis reports for experiments or upcoming projects. I will spend most of the morning head down in SQL. Early morning, I also check the birthday calendar. I do not want to show up to a meeting or see someone in passing who is celebrating and I do not know! Before reaching my desk, I commuted 40 min from the Marina on Chariot. During that time, I checked email/slack and reviewed my calendar. I try to reach inbox 0 at least 1–2 per day. It’s quite challenging when you receive around 100 emails a day! This is wellness week at Lyft! I found this bag of goodies on my desk this morning. I will try to book a team yoga class this week so we can all bond. I finish my 3 analysis tasks in time for standup! Standup is a time in which every engineer shares what they did yesterday and are doing today. The point is to make sure everyone is working on the most important tasks and reveal any blockers to completing them. Usually standup is done standing up in a circle (hence the name). However, my team does standup via Slack because we have a team member who is hard of hearing. Pictured here is Arun writing his slack update. Usually I don’t squat at his desk while he does this, but I’ll make an exception today. Normally, I eat lunch at my desk while working or with my team. I prefer eating with the team because I get to check in and make sure everyone is having a fantastic day. Today, I have a lunch meeting with some of my ops partners. We are going to talk through metrics for an upcoming project. We get free lunch at Lyft, so I stop by our kitchen to pick up food on my way to this meeting. This is my manager, Alexis. We have a weekly 1:1 to talk about how I can be more effective and happier at work. She unblocks me and gives me feedback on how to do a better job. Feedback is a must if you want to improve. This is the weekly core passenger leads meeting. Here we share learnings across the organization. There are a lot of ideas and insights I can apply to airports, so I listen for them. Repurposing or adjusting work from another team to solve an issue at airports is a great ROI. This is my engineering partner, Tim. This is my most important relationship at work. I end every meeting with Tim asking how I can help him be happier or more effective in the upcoming week. Today we are planning our team’s upcoming sprint. A sprint is a two week period of work for the engineers. We prioritize work for each engineer and review high priority bugs that we may want to slot into the sprint. During this meeting, an engineer on my team named Joseph pings me. The analysis I did this morning allowed him to discover a pretty bad airport bug. He is almost done pushing the fix. Success! Before leaving early, I end the day with a product kickoff and a design review. I then get back online around 8pm to wrap up a few to-do’s from my afternoon meetings on my couch. This job is quite flexible in terms of hours spent in the office. However, you can’t leave things hanging because it will inevitably limit your team’s work the next day. Lyft is hiring its first class of APMs to start fall 2019. Apply here . Stories from Lyft Engineering. 312 Lyft Product Product Management 312 claps 312 Written by Stories from Lyft Engineering. Written by Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-09-27"},
{"website": "Lyft-Engineering", "title": "application security in a devops environment", "author": ["Chris Steipp"], "link": "https://eng.lyft.com/application-security-in-a-devops-environment-53092f8a6048", "abstract": "Data Data Science Engineering Mobile Product Security It seems like every AppSec vendor pitch talks about how you can shift security “to the left” and they can help you transition to “DevSecOps”. When I hear these pitches, I’ve often thought to myself, “I don’t think that word means what you think it does.” While it’s great that security is embracing DevOps-style engineering (yay!), and it’s great that many vendors are thinking about how their tools fit in these environments (yay!), I don’t see a lot of the discussion about what DevSecOps looks like when it’s done well . When I joined Lyft’s Security Team, surviving (let alone thriving) in their DevOps environment was a challenge, but working in this environment taught me a number of things about doing AppSec in a DevOps environment. There is plenty of advice for making your AppSec team function better in a DevOps environment, but this article is specifically about the characteristics of Lyft’s AppSec program that I feel support our DevSecOps efforts well. If you work on an AppSec team, maybe our experiences can help you think through how you work with you DevOps engineering teams. If you’re a vendor and your product doesn’t support these, then let’s delay talking until they do. A vendor recently described Lyft’s engineering process as “extreme DevOps”. We have a lot of teams working on different products and features. Each team owns their services and are responsible for meeting SLA’s for availability and response time. Teams have almost complete control over how their services are built and run, as long as they maintain their SLA. Most teams use this freedom to develop fast, and are often deploying new features several times a day. With this freedom, teams have the responsibility of securing their services, and ensuring security issues are fixed within an established timeframe. Adding a traditional AppSec program to this would be labor intensive. When building the AppSec program at Lyft, we had to re-think how we engage teams, and develop tooling to automate integrating security throughout the development flow. When looking at the projects in Lyft’s AppSec program that have been successful, a couple of themes stand out. Everything has to be measured Security’s input needs to be timely and respect the developer’s time We need continuous feedback loops between processes If you look at all of the activities that we’re doing, we don’t embody these themes in everything that we do, but we’ve had enough successful examples of each that I believe they’re worth sharing. Tools and processes need to be measurable. Not only do we need to collect the measurements, we need people watching those metrics. Some metrics can be watched by the AppSec team, but for many it’s far more effective when the the development teams monitor the metrics and are held responsible for maintaining a reasonable threshold. At Lyft, each team maintains one or more dashboards for each of the services they run. This shows metrics such as the service’s error rates, response time, and the percentage of time the service has been within it’s SLA for services that rely on it. The Security Team is currently rolling out metrics to our service dashboards tracking out of date patches, with an alarm to page the team when security patches are left unapplied. Giving the teams visibility into the risks lets them prioritize and schedule their patching, instead of relying on the security team to monitor patch levels. There are many vendors who can show patch levels across your fleet. Most of those tools do a much better job analyzing packages on the system and figuring out what patches are missing instead of our naive scripting. They produce prettier graphs. The problem with most systems is they display this data to the security team, and rarely support getting that information to the people who have the power (and responsibility) for patching those instances in a way that integrates with their workflow. If you’re a security vendor, please, support native exporting of your tools data to the tools that our engineers are in every day — wavefront, grafana, elasticsearch. Or let us write all the data into S3 so we can ingest it into our standard audit pipeline. We also need to scope reports on that data to the appropriate teams, so please, support slicing data on AWS tags, ASG names, etc. When giving security input to an engineering team, if the input is not given at exactly the right time there’s a good chance it will just be filtered out as noise by the members on that team. With the speed of development and the velocity of change in our environment, engineers have to digest a firehose of information. Whether it’s all-engineering emails about changes to a service template, changes to the process for deploy a particular job, or best practices that teams have figured out and want other teams to adopt, engineers are bombarded by (good and helpful) information from other teams every day. To be productive as a developer, you have to filter out a lot noise. The security team telling you it’s cybersecurity awareness month, so please don’t xss or fall for phishing, becomes noise. Engineers need to be reminded about cross-site scripting when they are writing new frontend code, and about phishing when they are reading emails from questionable sources. One place where I saw a significant gap in getting timely information to our developers was during the pull request (PR) process. We have static analysis tools running against PR’s in github that must all pass before the PR can be merged. But often those tests take 10–15 minutes to run. By the time our static analysis tools fail the build, the developer is often off working on another task, or working through code review with a peer. To make things worse, the UX for discovering why a Jenkins test failed isn’t intuitive for new engineers. This resulted in engineers asking the security team on Slack why a test was failing (interrupting flow for both the developer and the security team member who needed to answer their question), and a mean time to fix of over an hour. Worse, if the engineer didn’t understand the results, they sometimes would force merge the PR under the assumption that it was a false positive or they could fix it later. Seeing this, we built a system (LASER) at Lyft to quickly give non-blocking security feedback on PR’s. We try to give feedback within 30 seconds of the developer opening a PR or pushing a commit. This way the feedback is present before their peer looks at the PR for code review, and the developer is notified of the comment before they transition to another task. The comment from LASER gives a summary of the issue, with links to more information in case they aren’t familiar with the security issue that was found. This resulted in the average fix time dropping to 7 minutes. In addition to having tools that are fast, the results need to be very accurate so the developer’s time is respected. Every false positive wastes an engineer’s time (even if it’s only 7 minutes). Yes, this means higher false-negative rates, but if we’re stopping the most common issues with no marginal work for the security team, then the security team is freed to work on better surfacing those issues in a fast and accurate way in our tooling. The best tools are ones that both address an existing set of issues, and allow us to improve our entire AppSec process at the same time. What does this look like at Lyft? The security team tries to interact with the owner of a service at 13 points in their development process. For that to be possible with a relatively small AppSec team, those processes are highly automated and manual work is prioritized based on risk. When implementing the automation, we specifically looks for ways that the outputs can be used as inputs into other automated processes. We use a self-assessment questionnaire that lets teams report what user data they are storing and where. This provides automated feedback to the developer based on their answers, to prevent those mistakes as they implement their service. Certain characteristics automatically flag the service for deeper review by the Security Team. We also use the results to update our data map and inform how we prioritize that service for review and external security assessment. When vulnerabilities are found, we look for ways to implement high signal rules in our scanning tools to detect these prior to deployment in the future. Which brings up another issue for vendors — if your scanning tool doesn’t allow us to modify and write new rules, then your tool is significantly less useful. Most AppSec tooling and processes provide data that is useful input into other processes, but thinking through how that will be done in advance has been useful for ensuring that when we make investments in one aspect of our program, we’re improving multiple aspects of our program simultaneously. This would be a nice, popular medium post if I promised that by doing these 3 things, you’ll be super successful and someone will probably give you a unicorn. The reality is that AppSec is a lot of hard work, and there’s a reasonable chance I’m wrong about a lot of things. Please leave feedback in the comments, and I hope we can all learn more together! Interested in working in an environment like this? Lyft is hiring! Apply through our application system , or drop me a note at csteipp@lyft.com . Stories from Lyft Engineering. 1K 5 Thanks to Ryan Lane . Software Development Devsecops Application Security Security Engineering 1K claps 1K 5 Written by Stories from Lyft Engineering. Written by Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-02-01"},
{"website": "Lyft-Engineering", "title": "fingerprinting fraudulent behavior", "author": ["Hao Yi Ong"], "link": "https://eng.lyft.com/fingerprinting-fraudulent-behavior-6663d0264fad", "abstract": "Data Data Science Engineering Mobile Product Security You’re a Lyft driver and you’ve just accepted a ride. You start making your way to the pickup location and suddenly you get a call. “Hello Alex, I’m Tracy calling from Lyft HQ. This month we’re awarding $200 to all drivers with a rating of 4.7 stars and above, and I just wanted to congratulate you for being an awesome 4.9 star driver!” “Hey Tracy, thanks!” “No problem! And because we see that you’re in a ride, we’ll dispatch another driver so you can park at a safe location… Alright, your passenger will be taken care of by another driver. Before we can credit you the award, we just need to quickly verify your identity. We’ll now send you a verification text. Can you please tell us what those numbers are?…” At this point, you’ve just given up complete access over every last cent in your driver account without even realizing it. Posing as Lyft support, a particular kind of scammer would request for a pickup and call the driver that accepts the ride. Polite and professional, the scammer would introduce himself as a Lyft HQ representative and then congratulate the driver on having been selected for a monetary award, ostensibly for being an outstanding driver. To credit the award to his account, the driver would have to verify his identity and provide account credentials. For added effect, the scammer would claim that it’s an important call and “re-dispatch” the ride for the passenger. This apparent administrative privilege is possible since the scammer is really the passenger and can simply cancel the ride. To the uninitiated driver, unfortunately, this elaborate display of authority on top of a well-rehearsed script is often very convincing. Past this point, the scammers would have access to any hard-earned money that hasn’t been cashed out from the account. What follows would not surprise anyone familiar with the concepts of social engineering and account takeover . Analytically, we quickly learned to identify suspicious accounts through telltale signs in their user activity. For instance, the scammers’ accounts would exhibit unusually high driver contact and passenger cancellation rates with few completed rides. There were many variants of the exact history of user activity, of course, as these fraudsters changed their tactics over time. Their varied behavior made it challenging to codify their user activity into simpler, structured features that clearly distinguish good from bad users. To the experienced risk analyst, however, the patterns are obvious. There are “behavior fingerprints” that resulted from their modus operandi that didn’t — and perhaps couldn’t — fundamentally change. In this post, we reveal the practical motivations for a paradigm shift in Fraud Research Science work from pure, hardcoded feature engineering to a more model-centric approach over the past year. This newer approach focuses on modern machine learning techniques such as deep learning that expands the types of sources we can work with and helps us better capture the predictive properties of our signals. We then dive into one of our latest production machine learning models that detect behavior fingerprints in the fraudulent users’ account activity. We end by sketching out some of the current directions we’re looking into at Lyft Fraud. At Lyft, fraud decision-making is split between business rules handcrafted by analysts and machine learning models developed by research scientists. These business rules and machine learning models form the backbone of our detection system that trigger pre-authorizations and identity challenges targeted at blocking fraudsters. To power these decision-making tools, our team puts a lot of effort into analyzing the behavior of fraudulent users and distilling the signals they leave behind into hand-engineered features. But hand-engineering features is hard. To be precise, what’s hard isn’t generating a bunch of features. Rather, what’s hard is hand-engineering features that are robust: predictive not just in the short term but also in the medium and long term. Like diseases, it’s often easier to devise features that detect the symptoms rather than the cause of the symptoms. For instance, unusually high driver contact and cancellation rates might together form a good business rule that initially detects many of the con artists’ accounts as described in the introduction. But in our case, these scammers quickly learned to call our drivers’ switchboard-assigned phone numbers using alternative phone numbers to escape detection. To be effective against an adaptive adversary, it’s important to develop robust features that look at things fraudsters find hard to control or change. At its core, any counter-fraud measure worth its salt is designed to irrevocably drive the fraudster’s operational costs up to the point where the fraud vector becomes economically unsustainable. In the example above, the high driver contact rate features wouldn’t have driven up the operational costs because they simply shifted to using a burner phone separate from the ones used to create their passenger accounts to call the drivers. What we needed was a way to capture the sequence of behaviors that exposes the recurring pattern of cancellations after the requested rides are dispatched amidst all the other account activity. Designed right, features should be robust — unaffected by variance in the general fraud pattern since the fraudsters’ modus operandi remains the same. In order to escape detection, the scammers will have to do something drastic, such as actually going through with the ride without cancellation. While a couple of obvious methods come to mind, such as using stolen credit cards or doing coupon fraud, both would incur much greater cost. But easier said than done. Robust features require rich sources of signals that capture behavioral patterns that aren’t easy to defeat. The “classical way” to do this is to ingest as diverse a set of sources as possible into something like a logistic regression model with some interaction terms involved. An example of this sort of feature engineering is hinted above: we can take, say, the harmonic mean of the cancellation, contact, and (for good measure) the ride non-completion rates. Perhaps we can even throw in a time-based rolling window to make sure that we’re not susceptible to “incubated accounts” with good ride history. If it’s not yet apparent that designing robust features is hard, recognize that this is but one fraud vector of hundreds by dozens of highly adaptive fraud rings that actively target Lyft’s various product lines. To improve our fraud decisioning, we started focusing on more modern, powerful modeling methods in the past year . For instance, we shipped gradient-boosted decision trees (GBDT) ensemble models that vastly improved our performance due to the decision tree’s inherent ability to capture interactive effects between multiple features. That meant that we could spend more time exploring feature sources that weren’t directly predictive of fraud but improved our models in concert with our existing feature set. Hand-engineering features that were independently correlated with our fraud labels — necessary for, say, a Naive Bayes classifier — thus became less important than finding the right combination of features. Our success and migration to GBDT models was the start of our pursuit of better machine learning modeling approaches that made the most of our signals offered. More recently, we’ve shifted our attention to neural networks that were even more powerful and could gracefully work with far richer streaming data sources. It wasn’t that we weren’t aware of these data sources — often, our risk analysts were already poring over things like user activity logs and financial transaction histories in manual account reviews. We’ve even crafted some pretty predictive features around them for our GBDTs. The issue was that with most “shallow learning” methods, we’ve had to compromise on losing part of the information when transforming them into the structured features that work with these methods. For instance, even though GBDTs can capture the interactions between the cancellation, contact, and ride non-completion rates without the need for something like the harmonic mean, we couldn’t express the temporal relationship between these events. Part of the promise of neural networks was their ability to extract even more of the information inherent in these signals by working directly with “less processed” features in their more natural, sequential forms. To handle complicated sequential signals, we explored various neural network architectures that gave the model dynamic temporal behavior for a time sequence. In other words, we wanted a deep learning model that was able to update its belief about a user as more information streams in. And while we did initially think of alternative methods, such as n-grams to capture particular action subsequences and hidden Markov models (HMMs), they didn’t seem appropriate. In the former case, the number of n-grams to capture important subsequences would be the permutative complexity of all sequences — too expensive and difficult to maintain as we constantly improve our product. In the latter case, there isn’t an obvious way to sidestep the non-Markovian nature of user activity with respect to whether a user is fraudulent. For instance, if a user exhibits suspicious log-in activity early on that isn’t strictly indicative of fraud, it’s not obvious how to preserve that information as we observe more user activity. Reviewing the literature about modeling sequential data quickly pointed us to deep learning as the state of the art method. It also seemed at the time to be the most practical way to marry sequential features with our existing structured features given our earlier work on natural language processing (NLP) tasks with our Support Experience team. Practically, it was also easier for serialization, etc. E.g. , running an HMM on top of a GBDT model. Our existing model serving infrastructure would have to change significantly to accommodate arbitrary ML model stacks. To provide intuition into how neural networks work and insight on how we work with neural networks, we dive deep 🤓 into how we use one of the richest signal sources for fraudulent patterns: the user’s activity log. Specifically, the activity log is a temporally ordered sequence of user actions taken on our app along with their various metadata. These user actions range from ride request button presses to map-magnifying screen pinching. The action metadata include the duration of action, the time elapsed since the previous action, and the force applied on the phone screen by the user. Being one the most voluminous event streams we have at Lyft, it is impractical to take the classic approach of handcrafting features from it and we had to turn to a deep learning approach that benefitted from the scale of the available data. And that meant finding the best neural network architecture for our use case; i.e. , architecture engineering. As with most applications of deep learning, our approach was largely empirical. To find the best performing model, we took heavy inspiration from surveys and recent papers on deep learning techniques for NLP and searched over thousands of neural network architectures using automated cross-validation jobs. The search consisted of small changes such as specific activation functions and embedding dimensions to larger ones such as the order of the network layers and specific architectures proposed . In the end, we settled on a neural network that maps user actions to feature embeddings and a convolutional-recurrent architecture with an attention mechanism. Fancy. 😎 User action embeddings are dense vectors that encode the semantics of each specific user action. Our usage is inspired by word embeddings commonly used in NLP applications, such as GloVe embeddings and word2vec . Like word embeddings, each dimension in the embedding space encodes some property of the set of all user actions. For instance, one dimension could encode how likely an action is related to user log-in and another could encode how much keystroke input is needed for the action. In our case, we noticed that “similar” ride cancellation-related interactions are clustered together away from the ride request one when doing a t-SNE visualization on our top 50 user actions. These semantic clusterings also acted as a sanity check against our training pipeline. The 1D convolutional network (ConvNet) forms the second component of our neural network. True to its namesake, the 1D ConvNet is built around the idea of 1D convolutions where trainable filters are convolved with the sequence of embedded user actions. Convolutional filters with learnable parameters help extract similar local features across multiple locations and encode subsequences of user actions that together form more meaningful local interactions. For instance, a single ride request action doesn’t mean much by itself. But when considered together with repeated ride cancellations and requests that precede it, the subsequence of repetitive user actions paints a much more suspicious picture of the user. One way to think about this idea is: user action subsequences are to user activity logs as word phrases are to sentences. Another way to intuit ConvNets on user activities are how they operate if we learned user action n-gram embeddings, where n is the size of an analogous convolution filter. They both try to encode the semantics of consecutive user actions. But compared to the n-gram embedding, processing the single user action (1-gram) with a convolution layer with n-filters reduces the parameter size. This reduction is because we don’t need to learn an embedding for every unique n-gram, which also means we need enough observations of all possible n-grams. Considering the interaction of a small number of n-filters with, say, fully-connected layers can help us capture the same amount of information as a one-to-one embedding mapping without high sample complexity. To further improve our sample efficiency, we stack multiple convolutional layers such that each learns abstractions of user action subsequences in a hierarchical fashion. This approach allows our network to be even more expressive with fewer parameters than simply using a large convolutional layer after an embeddings layer. In traditional sequence-analyzing models (think HMMs), the probability is usually conditioned on a window of preceding elements. To make model training tractable, these models often make simplifying assumptions such as the Markov assumption. These models typically achieve better performance with higher order n-grams, some Laplace smoothing , and backing off to lower order n-grams when the higher order ones haven’t been observed. This classical approach usually requires a lot of n-grams and huge memory resources. For instance, in Heafield et al.’s state of the art work for NLP applications, “[u]sing one machine with 140 GB RAM for 2.8 days, [they] built an unpruned model on 126 billion tokens.” This approach would not have been practical for us. 😅 Recurrent neural networks (RNNs) are one way to condition the probability on all the previous elements in a sequence using a parametric (and somewhat blackbox-ish) function. Very (very) roughly, the RNN “memorizes” the sequence of user actions observed so far and saves it as a hidden state within the “memory cell.” As new actions are sequentially ingested, the cell considers the hidden state together with the current user action to update its “memory.” To obtain the “probability,” the cell considers the hidden state together with the current user action and outputs an estimate. Intuitively, when the inputs from the preceding ConvNet are passed into the RNN in our neural network, it determines how much of the information about user action subsequences should be retained for future consideration. It allows us to efficiently encode a temporal relation between earlier subsequence embeddings with later ones. To further improve on the RNN component, we experimented with a few ideas and augmented it with an attention mechanism . Our behavior fingerprinting neural network is implemented as a stack of the embedding layer, ConvNet, and RNN in that order on Tensorflow through the Keras interface. We concatenate the RNN’s output with the structured features and pass it through fully-connected layers that returns a softmax multi-class output that determines the probability assigned to each possible fraud user segment. On a per-model basis, we found that the addition of the behavior fingerprinting module to our production structured features-only neural network produced a relative lift in recall of over 40% for the same precision with respect to all fraudulent users. Historically, we’ve always had to first identify the fraudulent pattern and, in turn, use that to train our models. Today, not only are we fingerprinting fraudulent behavior, we’re also working on learning good user behavior to detect when someone is deviating from it. To that end, we’re developing models that uses a semi-supervised version of the generative adversarial networks algorithm to detect what are anomalous user embeddings. At a high level, we’re looking at ways to automatically encode human intuition about what’s not fraudulent in a user account. As opposed to our older approach of purely building discriminative models, we’re working on a generative model of the good user distribution. We indirectly sample fraudulent users from the complement of the good user distribution and use it to train a discriminative model with our existing true fraud targets. This approach is similar to what is described in Dai et al.’s work on the BadGAN . The hope is that understanding good behavior better can help us use it to protect and even reward good users. If you enjoyed this post, follow and recommend! And while we’re big fans of deep learning ( in the right context ), that’s not all that we do. To learn more, check out our other Research Science and Lyft Fraud posts! Interactions in fraud experiments: A case study in multivariable testing Stopping fraudsters by changing products What’s in a name? The semantics of Science at Lyft From shallow to deep learning: A Research Scientist’s journey through hand-coded regressors, pickled trees, and attentive neural networks As always, Lyft is hiring! If you’re passionate about developing state of the art machine learning models or building the infrastructure that powers them, read more about our Research Science and Engineering roles and reach out to me ! This post would not have been possible without the help of Yaniv Goldenberg, Vinson Lee, Patrick LaVictoire, Cam Bruggeman, Josh Cherry, Ryan Lane, Elaine Chow, and Will Megson. Many thanks! Stories from Lyft Engineering. 579 2 Thanks to Josh Cherry and Elaine Chow . Machine Learning Engineering Fraud Data Science Deep Learning 579 claps 579 2 Written by Research Scientist at Lyft Stories from Lyft Engineering. Written by Research Scientist at Lyft Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-07-28"},
{"website": "Lyft-Engineering", "title": "announcing cni ipvlan vpc k8s ipvlan overlay free kubernetes networking in aws", "author": ["Paul Fisher"], "link": "https://eng.lyft.com/announcing-cni-ipvlan-vpc-k8s-ipvlan-overlay-free-kubernetes-networking-in-aws-95191201476e", "abstract": "Data Data Science Engineering Mobile Product Security Lyft is pleased to announce the initial open source release of our IPvlan-based CNI networking stack for running Kubernetes at scale in AWS. cni-ipvlan-vpc-k8s provides a set of CNI and IPAM plugins implementing a simple, fast, and low latency networking stack for running Kubernetes within Virtual Private Clouds (VPCs) on AWS. Today Lyft runs in AWS with Envoy as our service mesh but without using containers in production. We use a home-grown, somewhat bespoke stack to deploy our microservice architecture onto service-assigned EC2 instances with auto-scaling groups that dynamically scale instances based on load. While this architecture has served us well for a number of years, there are significant benefits to moving toward a reliable and scalable open source container orchestration system. Given our previous work with Google and IBM to bring Envoy to Kubernetes , it should be no surprise that we’re rapidly moving Lyft’s base infrastructure substrate to Kubernetes . We’re handling this change as a two phase migration — initially deploying Kubernetes clusters for native Kubernetes applications such as TensorFlow and Apache Flink, followed by a migration of Lyft-native microservices where Envoy is used to unify a mesh that spans both the legacy infrastructure as well as Lyft services running on Kubernetes. It’s critical that both Kubernetes-native services as well as Lyft-native services be able to communicate and share data as first class citizens. Networking these environments together must be low latency, high throughput, and easy to debug if issues arise. Deploying Kubernetes at scale on AWS is not a simple or straightforward task. While much work in the community has been done to easily and quickly spin up small clusters in AWS, until recently, there hasn’t been an immediate and obvious path to mapping Kubernetes networking requirements onto AWS VPC network primitives. The simplest path meeting Kubernetes’ network requirement is to assign a /24 subnet to every node, providing an excess of the 110 Pod IPs needed to reach the default maximum of schedulable Pods per node. As nodes join and leave the cluster, a central VPC route table is updated. Unfortunately, AWS’s VPC product has a default maximum of 50 non-propagated routes per route table, which can be increased up to a hard limit of 100 routes at the cost of potentially reducing network performance. This means you’re effectively limited to 50 Kubernetes nodes per VPC using this method. While considering clusters larger than 50 nodes in AWS, you’ll quickly find recommendations to use more exotic networking techniques such as overlay networks (IP in IP) and BGP for dynamic routing. All of these approaches add massive complexity to your Kubernetes deployment, effectively requiring you to administer and debug a custom software defined network stack running on top of Amazon’s native VPC software defined network stack. Why would you run an SDN on top of an SDN? After staring at the AWS VPC documentation, the CNI spec, Kubernetes networking requirement documents, kube-proxy iptables magic, along with all the various Linux network driver and namespace options, it’s possible to create simple and straightforward CNI plugins which drive native AWS network constructs to provide a compliant Kubernetes networking stack. Lincoln Stoll’s k8s-vpcnet , and more recently, Amazon’s amazon-vpc-cni-k8s CNI stacks use Elastic Network Interfaces (ENIs) and secondary private IPs to achieve an overlay-free AWS VPC-native solutions for Kubernetes networking. While both of these solutions achieve the same base goal of drastically simplifying the network complexity of deploying Kubernetes at scale on AWS, they do not focus on minimizing network latency and kernel overhead as part of implementing a compliant networking stack. We developed our solution using IPvlan , bypassing the cost of forwarding packets through the default namespace to connect host ENI adapters to their Pod virtual adapters. We directly tie host ENI adapters to Pods. In IPVLAN — The Beginning , Mahesh Bandewar and Eric Dumazet discuss needing an alternative to forwarding as a motivation for writing IPvlan: Though this solution [forwarding packets from and to the default namespace] works on a functional basis, the performance / packet rate expected from this setup is is much lesser since every packet that is going in or out is processed 2+ times on the network stack (2x Ingress + Egress or 2x Egress + Ingress). This is a huge cost to pay for. We also wanted the system to be host-local with minimal moving components and state; our network stack contains no network services or daemons. As AWS instances boot, CNI plugins communicate with AWS networking APIs to provision network resources for Pods. The primary EC2 boot ENI with its primary private IP is used as the IP address for the node. Our CNI plugins manage additional ENIs and private IPs on those ENIs to assign IP addresses to Pods. Each Pod contains two network interfaces, a primary IPvlan interface and an unnumbered point-to-point virtual ethernet interface. These interfaces are created via a chained CNI execution. IPvlan interface: The IPvlan interface with the Pod’s IP is used for all VPC traffic and provides minimal overhead for network packet processing within the Linux kernel. The master device is the ENI of the associated Pod IP. IPvlan is used in L2 mode with isolation provided from all other ENIs, including the boot ENI handling traffic for the Kubernetes control plane. Unnumbered point-to-point interface: A pair of virtual ethernet interfaces (veth) without IP addresses is used to interconnect the Pod’s network namespace to the default network namespace. The interface is used as the default route (non-VPC traffic) from the Pod, and additional routes are created on each side to direct traffic between the node IP and the Pod IP over the link. For traffic sent over the interface, the Linux kernel borrows the IP address from the IPvlan interface for the Pod side and the boot ENI interface for the Kubelet side. Kubernetes Pods and nodes communicate using the same well-known addresses regardless of which interface (IPvlan or veth) is used for communication. This particular trick of “IP unnumbered configuration” is documented in RFC5309 . For applications where Pods need to directly communicate with the Internet, our stack can source NAT traffic from the Pod over the primary private IP of the boot ENI by setting the default route to the unnumbered point-to-point interface; this, in turn, enables making use of Amazon’s Public IPv4 addressing attribute feature. When enabled, Pods can egress to the Internet without needing to manage Elastic IPs or NAT Gateways. Kubelets and Daemon Sets have high bandwidth, host-local access to all Pods running on the instance — traffic doesn’t transit ENI devices. Source and destination IPs are the well-known Kubernetes addresses on either side of the connect. kube-proxy : We use kube-proxy in iptables mode and it functions as expected — Kubernetes Services see connections from a Pod’s source IP, as we loop traffic back through the requesting Pod using policy routing in the default namespace following kube-proxy DNAT resolution. kube2iam : Traffic from Pods to the AWS Metadata service transits over the unnumbered point-to-point interface to reach the default namespace before being redirected via destination NAT. The Pod’s source IP is maintained as kube2iam runs as a normal Daemon Set. Our design is heavily optimized for intra-VPC traffic where IPvlan is the only overhead between the instance’s ethernet interface and the Pod network namespace. We bias toward traffic remaining within the VPC and not transiting the IPv4 Internet where veth and NAT overhead is incurred. Unfortunately, many AWS services require transiting the Internet; however, both DynamoDB and S3 offer VPC gateway endpoints. While we have not yet implemented IPv6 support in our CNI stack, we have plans to do so in the near future. IPv6 can make use of the IPvlan interface for both VPC traffic as well as Internet traffic, due to AWS’s use of public IPv6 addressing within VPCs and support for egress-only Internet Gateways. NAT and veth overhead will not be required for this traffic. We’re planning to migrate to a VPC endpoint for DynamoDB and use native IPv6 support for communication to S3. Biasing toward extremely low overhead IPv6 traffic with higher overhead for IPv4 Internet traffic seems like the right future direction. Our stack is composed of a slightly modified upstream IPvlan CNI plugin, an unnumbered point-to-point CNI plugin, and an IPAM plugin that does the bulk of the heavy lifting. We’ve opened a pull request against the CNI plugins repo with the hope that we can unify the upstream IPvlan plugin functionality with our additional change that permits the IPAM plugin to communicate back to the IPvlan driver the interface (ENI device) containing the allocated Pod IP address. Short of adding IPv6 support, we’re close to being feature complete with our initial design. We’re very interested in hearing feedback on our CNI stack, and we’re hopeful the community will find it a useful addition that encourages Kubernetes adoption on AWS. Please reach out to us via GitHub, email, or Gitter. cni-ipvlan-vpc-k8s is a team effort combining engineering resources from Lyft’s Infrastructure and Security teams. Special thanks to Yann Ramin who coauthored much of the code and Mike Cutalo who helped get the testing infrastructure into shape. Interested in working on Kubernetes? Lyft is hiring! Drop me a note on Twitter or at pfisher@lyft.com . Stories from Lyft Engineering. 663 5 Thanks to becca p . Engineering Kubernetes Infrastructure AWS 663 claps 663 5 Written by Software Engineer @ Lyft Stories from Lyft Engineering. Written by Software Engineer @ Lyft Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-12-21"},
{"website": "Lyft-Engineering", "title": "from shallow to deep learning in fraud", "author": ["Hao Yi Ong"], "link": "https://eng.lyft.com/from-shallow-to-deep-learning-in-fraud-9dafcbcef743", "abstract": "Data Data Science Engineering Mobile Product Security One week into my Research Science role at Lyft, I merged my first pull request into the Fraud team’s code repository and deployed our fraud decision service. No, it wasn’t to launch a groundbreaking user behavior activity-based convolutional recurrent neural network trained in a semi-supervised, adversarial fashion that challenges a user to prove her identity — it would be a couple of years before that. Embarrassingly, it was to remove a duplicate line of feature coefficients in a hand-coded logistic regression model rolled out a little less than a year before. This small bug exposed a number of limitations of a system built primarily for a different type of usage — that of business rules that encapsulate simple, human-readable handcrafted logic. In our old worldview, models were simply extensions of business rules. As our models grew larger and features more complicated, model and feature definition inconsistencies between offline prototypes in Jupyter Notebooks and the production service occurred more frequently. These inconsistencies culminated in months-long back-and-forth cycles between Engineering and Research Science over feature definitions and ultimately blocked production. More worryingly, as we started exploring more modern, expressive machine learning libraries and frameworks like XGBoost and Tensorflow, it became clear that we needed drastic improvements to both the engineering stack and the prototype-to-production pipeline. The redeeming point of this ordeal? That our model was robust to inflated feature values, which is important when faced with fast-adapting adversaries. But the overriding thought at that time was, how can we improve our machine learning infrastructure? In Fraud, we’re primarily interested in classification algorithms that distinguish between good and fraudulent users on the platform. Despite the advent of more modern methods, logistic regression is still very much the bread-and-butter classifier of many industries. From a prototyping perspective, they are easy to implement and train, backed by theory, and amenable to mathematical analysis. From a practical standpoint, they are highly interpretable to even the layperson. For instance, the regression coefficients can be interpreted as how much a feature correlates with the likelihood of fraud. That makes diagnosing prediction errors easy: the key contributing features would have the largest summands in the logit. In terms of performance, a small team that constantly develops better features and maintains a good retraining pipeline will beat any huge team that manually handcrafts and manages hundreds of business rules. At Lyft, a feature engineering-focused framework around such “classical” machine learning methods has worked well for a long time. It allowed us to keep a relatively simple codebase that was easy to iterate on, and we logged each prediction’s feature importances to debug and evaluate performance. More importantly, our simple models gave anyone on the team the ability to quickly prototype new features without a steep learning curve to the underlying technology. However, the simplicity of our linear models came at the cost of performance — we weren’t able to capture all of information that our features provided through linear correlations. Any company that eventually grows large enough will invite second glances from fraudsters. Lyft is no exception. As the type of fraudsters that we attract grew in sophistication, we found it progressively harder to engineer features for the logistic regression model. Intuitively, the logistic regression model evaluates the information encoded in its features as a kind of weighted logical disjunction. To effectively capture any kind of higher-order interactions between features in the model, we’d have to hand-engineer them and add them as new features. For instance, it’s suspicious for a Lyft user with little ride history to suddenly start spending a lot on expensive rides. On the other hand, fraudsters “incubate” their accounts for a long while with cheaper rides before taking expensive rides in a short span of time. Encoding that type of complicated logic in a logistic regression model entails nontrivial feature acrobatics. We needed models that can capture such feature interactions more naturally. After exploring several types of logistic regression and decision tree ensemble models, we settled on the gradient-boosted decision trees (GBDT) model trained on the popular XGBoost library given its ease of use and efficiency. At the theoretical level, we knew that the GBDT is more powerful than logistic regression. One instinctive way to understand the difference between the two is to visualize their decision boundaries. The logistic regression decision boundary is simply a hyperplane on the feature space, which means good features must exhibit strong linear relationships with the likelihood of fraud. For GBDT, the decision boundaries are smoothed versions of a collection of higher-dimensional boxes, which allow us to encode more complicated feature interactions. Indeed, we found that our GBDT had a relative precision gain by over 60% compared to the previous model for the same operating recall and feature set. Based on impact alone, it would have been something like gross negligence not to replace our models with GBDTs. But there were a couple of complications. For starters, ensemble models are generally harder to interpret and analyze compared to the simple logistic regression, and we needed to find viable methods to do both for practical reasons. While we sacrificed immediate model interpretability, we were able to find good replacements in boosted trees-specific feature importances and model explainers . The former are akin to the magnitude of regression coefficients and help us prioritize which features to productionize. The latter help us determine the features most influential for specific predictions and answer the (oft-feared) question of, why was model X wrong? More importantly, we didn’t have a good way to productionize our GBDTs. As seen in our opening example, even simple hand-coded logistic regression models are prone to bugs. Approaching GBDTs with thousands of decision trees and complicated branching logic would be a nightmare. What we needed was a simple and reliable way to serialize a prototype model on a Jupyter Notebook and load it onto a production system. We evaluated tools like PMML but found that most lacked support for bleeding-edge models and couldn’t capture finer things like feature encoding and transforms in model pipelines. In the end, we built a library that utilizes the standardized scikit-learn API and pickle-based serialization. We knew of the many dangers of pickling and were thus extremely wary of the issues that may arise. But being even more wary of having to implement our own serialization scheme from scratch, we put additional guardrails around package requirements in a simple attempt at model versioning. Presently, we use Tensorflow deep learning models in production because of their performance and their ability to work with signals that are hard to engineer features from. For instance, GBDTs do not gracefully handle sequential inputs like the histories of transactions, rides, and user activity on our app. (We’ll discuss more of our latest models in an upcoming post.) Along the way, we’ve had to face other production issues and make adjustments to our internal serialization libraries and infrastructure. We also moved model execution and feature serving out of the original fraud decision service and built services around them that serves other teams’ needs. These changes pave a path for how we had long thought about elegantly handling model and package versioning. In our library, we didn’t quite solve the package dependency versioning problem, which results from a mismatch between our development and production environments. That meant that we couldn’t easily upgrade our “monolithic” decision service if we wanted to use newer package versions because our production models were based on older ones. I recall posing a naive question to the Fraud engineering manager back in early 2017, is it possible to spin up an EC2 instance with pip-frozen requirements for each model? Today, partly in response to our modeling needs, we’re developing a more modern, container-based model execution that we believe puts a seamless prototype-to-production ML process within reach. But suggestions and questions like these didn’t come naturally from research scientists back then. My foray into what may be more appropriately termed ML engineering in other companies started when it gradually dawned on me that there was a sharp lack of understanding on both Engineering and Research Science about each others’ needs. When I first joined Lyft, we had a siloed, “throw it over the wall” mentality where the scientist essentially stops work after the prototype feature set and model are built (hence the opening bug). For a while, it worked because the features were relatively simple. As our adversaries evolved and our features necessarily became more sophisticated, cracks begun to appear in that process. By the time I stepped foot into the company, “Feature War Rooms” (often in all-caps) to debug implementation inconsistencies had become a regular occurrence. It was then that I abandoned the old working model of handing off all of the feature and model production work to engineering. Learning how the system worked wasn’t exactly trivial and I quickly became that annoying friend to the Fraud Eng leads. But learning our microservice’s capability and limits was valuable. Beyond writing my own features in the existing infrastructure, my working knowledge allowed me to contribute in designing better infrastructure. For instance, I redesigned our holdback and experimentation framework to measure the impact of our system in a hierarchical fashion. Previously, all our measurements were rule-based and it was impractical to evaluate our system on the whole. With today’s framework, we can easily evaluate our system at the counter-fraud challenge and individual rule/model levels. I also contributed by building our first asynchronous model execution “trigger group.” Unlike most of our models that run synchronously with strict execution time SLAs due to the product flow, I recognized that there were places where asynchronous execution unlocked the use of more complicated features and models. Having a research scientist with good system knowledge helped guide and accelerate the engineering development process. Additionally, the change in work scope reduced miscommunication between different roles and freed up engineers from rote feature implementation to focus more on, appropriately, building better platforms. We’ve come a long way from copy-pasting regression coefficients. Over the past two years, we’ve broken away from a monolithic fraud decision service that combined feature ingestion and preprocessing, model deserialization and execution, and counter-fraud challenges. Today, we’re using and contributing to systems that provide the same team-agnostic services to a host of other teams that need the same sort of ML infrastructure. But consolidating our ML infrastructure is just the first step. From automating feature “back-simulation” such that anyone has access to a trusted library of feature values to train on historical cases, to capitalizing on Dryft’s powerful feature expressivity , to containerizing machine learning models as hinted above, we’re building foundational blocks that ease the prototype-to-production workflow. Armed with these tools, we’re developing automated feature generators that feed into model training, modular neural network layers that can be reused for different fraud problems, and even potentially bad ideas like protocols that allow us to collaboratively train and execute models across teams without direct access to the input features. If you enjoyed this post, follow and recommend! Also, don’t be fooled into thinking that Research Science is only about ML or that Fraud Engineering is only about backend infrastructure; check out our other Research Science and Lyft Fraud posts! Interactions in fraud experiments: A case study in multivariable testing Stopping fraudsters by changing products What’s in a name? The semantics of Science at Lyft As always, Lyft is hiring! If you’re passionate about developing state of the art machine learning models or designing the infrastructure that powers them, read more about our Research Science and Engineering roles and reach out to me ! This post was not just mine. Many thanks to Mike Ross, Steven Liu, Nick Chamandy, Josh Cherry, Will Megson, Yanshan Lu, Gil Arditi, Yaniv Goldenberg, Chris Elion, Ryan Lane, and Elaine Chow for reviews and edits! Stories from Lyft Engineering. 1.4K 4 Thanks to Ryan Lane and Elaine Chow . Machine Learning Engineering Data Science Deep Learning 1.4K claps 1.4K 4 Written by Research Scientist at Lyft Stories from Lyft Engineering. Written by Research Scientist at Lyft Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-07-13"},
{"website": "Lyft-Engineering", "title": "the challenges behind rolling out security updates to your docker images", "author": ["Andrey Falko"], "link": "https://eng.lyft.com/the-challenges-behind-rolling-out-security-updates-to-your-docker-images-86106de47ece", "abstract": "Data Data Science Engineering Mobile Product Security Since joining Lyft as a software engineer in August, I was pleased to learn that Lyft has automated security updates to Docker images. The crux of the solution is to generate pull requests to child docker images when parent images change. The pull requests cascade down to children of those child images until all images are up to date. This process begins at base image creation and completes when all images are updated. In this post, we will dive into the problem deeper and talk about the challenges your organization will likely face when rolling out such automation. You will learn about how to overcome several challenges and discover additional opportunities for improvement. Base images in the Docker ecosystem are images that don’t have any antecedents. In other words, these are images that are built “FROM scratch” — as in the required declaration that you make in your Dockerfile. Bits are normally injected into such images via some sort of bootstrapped chroot. You have to start somewhere after all! Taking a peek at how the maintainer bootstraps is always a good idea. Below is an example of what a professionally bootstrapped Dockerfile looks like. Bootstrapping your own base images can be a challenging task. You need to generate a chroot, pull a minimal set of packages with their dependencies from a Linux distribution of your choice, and then create a tar archive that can be loaded into your scratch image. Usually, distributions provide their users with tooling to make this process easier such as debootstrap or alpine-chroot-install . If you have strict air-gap security requirements whereby you cannot pull anything from public docker registries, you can use your distribution’s bootstrapping tools to automate making your own base images. The advantage that you get is that you can pull distribution packages from your own trusted sources. Taking the scratch approach adds additional cost because you might to need to hire a team that pulls together the contents of these secure sources. In essence, you need to invest time and efforts to redo what distributions already do for us. To avoid reinventing the wheel, you can use public images. However, you should take precautions to make sure that those images are safe to use. How can you find out if a public image is safe to use? Here’s a good starter checklist: Does the vendor actively announce security advisories? Are the images updated regularly? Do the vendor’s images depend on any images from other vendors? Generally, the answer to the first two questions should be “yes” and the last one “no”. If the last question on the checklist is “yes”, then you will need to evaluate if that dependent — or parent — vendor fits the same checklist. You will also need to know if new images from the parent vendor cause the images from child vendor to be updated. Once you’ve picked a trustworthy vendor, things get a little tricky. The vendor controls the cadence of updates and versioning scheme of the images that they publish — and they are definitely not going to send you pull requests to bump your images! Even if they were, your images might be in private repositories, and thus off limits to any of their automated tooling that can trigger your builds. In order to wrest back control over versioning, Lyft creates intermediate base image repositories that depend on a public image. This also allows us to build continuous integration (CI) phases that scan the incoming image for vulnerabilities, upload it to our own registry, and add any additional things we’d like all child images to have. We enforce some common best practices such as adding “dumb-init” and “LANG=C.UTF-8” in our intermediate repositories. We recommend taking steps to ensure that your application images cannot pull from public registries in your CI pipelines. If your developers want to depend on a public image they should go through extra reviews. The next problem that needs to be handled, whether you are using an intermediate repository or directly depending on a public image is determining if the parent image changed. If the parent image changed, that likely means that there is a security update and we should kick off the docker image update cascade as described in the introduction. At Lyft, we kick off our intermediate image pipeline on a schedule. This keeps things simple and reduces that time that our software is left vulnerable. We considered two approaches to improve on running at a schedule. The first approach is to have software that polls a descriptive tag such as “latest” and checks if the digest of that tag changes. If a change is detected, we kick off the intermediate image pipeline. This allows us to account for when the vendor has to release security patches out of cycle; perhaps when there is a high-risk vulnerability. The problem with this approach is that security updates don’t always come to the packages in our base image. For example, suppose we install Java in a child image. If the Ubuntu base image doesn’t have security or other updates for a long while, Java will not get updated. There is nothing that will trigger a build of the image. We have the same problem with descriptive tags for upstream Docker images that we have for installing apt packages within downstream images. We can of course also depend on the vendor’s security announcements that usually come in via emails. Ideally, by the time those emails are sent, there is a new image that we can pull into our intermediate image. However, that is not always the case for every distribution. Distributions often prioritize their package repositories over their Docker images. If the images do not get updated, you should consider placing an update command in your intermediate base image — — e.g. “apt-get upgrade”. Adding the extra upgrade in your Dockerfile hurts because it can double the size of your image. However, it is more important to lean towards the side of caution to ensure you have packages installed with security updates. We determined that the best balance is achieved by kicking your base image pipelines off at a schedule. That way we guarantee that all downstream images pull the latest packages from upstream package repositories and we don’t have to worry about distribution’s public Docker images lagging behind. Another challenge is that the security announcements are usually very granular to a package name level, so you’ll need to map package names to whether they exist in the image or not. Perhaps we can work with Docker image vendors to have security announcements targeted at docker images specifically and have them update them consistently when any of the packages contained within change. In summary, there are three possible starting points for your organization: Build your base images from scratch Create intermediary images that are based on the public ones Rely directly on publicly available base images Here is a chart that maps the pros and cons of these starting points: The other challenge that you’ll likely run into in a big Engineering organization is when service owners don’t merge pull requests generated by tools such as dockerfile-image-update in a timely manner. The longer it takes people to merge those changes, the longer things remain vulnerable in production. One way to resolve this problem is to automatically merge pull requests that have been left open. The timer starts when the pull request is made and service owners can see when the merge will happen in the pull request comments. You might face a challenge where the pull requests cannot be merged because they are haven’t passed tests. We’ve considered force merging the pull requests in order to signal to developers that they need to prioritize security updates. If they are in a crunch they can always revert the auto-merged pull request or simply close it before it gets merged. For now, we only force merge in a small number of cases where the Docker images are used for non-production purposes. We have covered two challenges that we have run into. The first is how to safely make use of public images. The second is how to keep security updates moving forward. Driving the cascade further to deployment systems is the next big body of work. Imagine how much toil would be eliminated if your Kubernetes pod.yaml could be safely updated and deployed for each freshly updated image. Exploring this aspect is a topic for a future blog post. I have recently talked at Jenkins World 2018 about this topic and will be talking at All Things Open and Devoxx about the same later this year. As always, if you know of tooling or ways we can solve automated security update challenges better, please let us know! Big shout out and appreciation goes out to Anthony Sottile who provided feedback and edits for this post. Also great thanks to Aneesh Agrawal and Brian Witt for designing, maintaining, and scaling the update system at Lyft. Interested in working in an environment like this? Lyft is hiring! Apply through our application system , or drop me a note at afalko@lyft.com . Stories from Lyft Engineering. 185 Thanks to Rebecca Powell . Docker Dockerfiles Engineering Security 185 claps 185 Written by Stories from Lyft Engineering. Written by Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-10-23"},
{"website": "Lyft-Engineering", "title": "stopping fraudsters by changing products", "author": ["Sam King"], "link": "https://eng.lyft.com/stopping-fraudsters-by-changing-products-452240f2d2cc", "abstract": "Data Data Science Engineering Mobile Product Security Lyft’s whole app experience is geared towards getting new users from the App Store or the Play Store to their first ride as quickly as possible. This streamlined process is great for our users, but presents an ever-present problem — how do we prevent bad actors from abusing our lightweight onboarding process? This medium post is on joint work from the Fraud and Identity teams @ Lyft . Key contributors include Sam King , Will Megson, Steven Liu , Mike Ross, Ryan Choi , Ryan McGowan , Glen Robertson , Adam Wushensky , Donald Chen , Helen Lau, and Siwei Shen . Our overall goal is to prevent account takeover, stop scripts and automated apps from accessing our servers, and minimize financial loss due to fraud. We focused our efforts on identifying three fundamental properties: Identifying automation or non-human traffic. Identifying a returning human — that is, detecting the same human we have seen on our app previously. Establishing individual ownership over payment instruments. All signals that we collect are an approximation of one of these fundamental properties, and when we’re looking for new signals, we cross-reference them against this list. In our architecture, there are three key components: client instrumentation to collect more data silently, a server-side algorithm that decides if an action in the product is suspicious, and a user challenge to collect more information in cases where the algorithm decides that the action (signup, login, or ride request) is suspicious. This figure shows how we collect additional information to decide if a login attempt is suspicious. In the client, before the user logs in, we pass back to the server a number of signals to try to confirm that the login attempt is coming from the same human that we encountered previously. If our detection algorithm determines that this login attempt is suspicious, we put the user through a login challenge to allow them to prove their identity. Examples of identity challenges include retyping their email address or by entering their driver’s license number. If we can confirm that this person is who we think they are, then we log them in. If we can’t, then we give them an escape hatch where as a last resort they have the option to create a new account. In this type of architecture, the detection algorithm and user challenge are complementary. An idealized, perfect detection algorithm detects all of the bad actors every time. An idealized, perfect user challenge identifies humans precisely, without adding friction to the flow. If you have the perfect detection algorithm or the perfect user challenge, you don’t need both. But in practice, neither is perfect, so we use both to correct for mistakes in the other. A low friction challenge enables security engineers to be more aggressive in their detection algorithm because they know that it will adversely affect fewer good users. Conversely, with high precision detection algorithms, security engineers can use challenges that add more friction but more cleanly stop malicious users. Over the last two years, we have shipped user challenges on login, signup, ride requests, passengers calls, driver calls, updating payment methods, and driver pay outs. Based on our experiences, we have developed five principles that we follow when designing new challenges. Specifically, in priority order: Never lock out good users. Craft the entire user experience around your false positives. Mobile-first design. Challenges should be hard for fraudsters. Numbers always trump philosophies. The first tenet of designing good user challenges is to never lock out good users. Under no circumstance should you design a user challenge that results in a legitimate user not being able to make progress. The second tenet is to create the entire user experience with a focus on the good users who we accidentally catch, not on the bad actors. This principle is most relevant when thinking about the overall presentation — don’t accuse your good users of doing something wrong. Be clear about the fact that you made a mistake and apologetic about asking them for more information. This figure shows a message from Facebook when they block a Messenger message. It is descriptive and gives users a clear path to let Facebook know if it was a mistake. The third tenet is that Lyft a mobile-first company and should design challenges with the assumption that all of the challenges will be served on a mobile phone or tablet. Pragmatically, this means avoid asking users to enter anything on a keyboard, but scanning something with their camera or clicking on a button are good because they minimize awkward and painful interactions. Our fourth tenet is obvious: make challenges difficult / impossible for fraudsters — but the point is that it’s the fourth principle; all things good user-centric priorities supercede it. Our fifth tenet might be the most important one: numbers trump philosophies and principles. If you have an idea, just ship it and see if it works. Overall when we issue a challenge, some of our false positives will stop using the app. Fortunately, only a small percentage of our users will ever see a user challenge, and we will do everything in our power to maximize the likelihood of good users to pass. However, fundamentally, when you add friction you lose users. Challenges are difficult to design because they are highly specific to the product. Designers of challenges need to be able to come up with asymmetric information that is easy to share with good users but hard for the fraudsters to learn. This section discusses challenges from Facebook, Google, and Lyft to explain the asymmetric information or idiosyncratic feature of the product that the challenge exploits. We speculate on the asymmetric information that the designers of these challenges used, but we describe the asymmetric advantage we exploited when designing and implementing Lyft’s login user challenges. This figure shows ReCaptcha v2 from Google where they ask users to find all of the pictures of turkeys out of a set of nine pictures. We believe that the reason this challenge works for Google is that they have a massive tagged data set via Google Image Search — something that would be nearly impossible for bad actors to reproduce. This figure shows how Facebook can ask you to identify your friends in a picture to verify your identity if they detect a suspicious login attempt. We believe that the reason this challenge works is that Facebook has a huge team of PhDs who work on scalable facial recognition algorithms, and they know who your friends are. This technical sophistication and social information would be difficult for an attacker to apply broadly. This figure shows a Lyft login challenge, where we ask people if they are an existing user. The key to this challenge is that we have an escape hatch: we always let people create new accounts. Lyft has a Growth team dedicated to making it easy to create a new account and take a ride, so we use this smooth onboarding experience as a way to maximize the number of good users who pass our challenge. From an account security perspective, creating a new account is acceptable because it prevents the wrong person from gaining access to someone else’s account, but this escape hatch is something that would never work for Google or Facebook because there is so much data tied to one’s email or social network account. In general, Lyft has a number of asymmetric advantages that we exploit when designing user challenges. These advantages include: Lyft is a mobile-first and primarily mobile company. Lyft is a consumer app. Creating new accounts is low cost for users. The Lyft product uses location data first class. Passengers register payment methods with Lyft to take rides. Drivers have a detailed on boarding process that includes interacting with humans. We know who our good users are after they have used the service for a few months. The Lyft use case is constrained by physics (users take rides in a car). Whenever we think about adding security to the Lyft platform, we consider these advantages and look for opportunities to exploit them — like in the challenge above, where we exploit the fact that creating new accounts is easy and that we’re a mobile first company. Login challenges are product flows that we show after the user authenticates successfully. If we see a successful login that our algorithms flag as suspicious, the Lyft app redirects the user to an extended version of our login flow that asks them for more information. Conceptually, you can view login challenges as a form of opportunistic two-factor authentication, where we only use the second factor when our algorithms detect something suspicious. Our challenge flow is based on information that the user should possess or know without setting up two-factor authentication explicitly. We designed and implemented two login challenges: a “enter the last four digits of one of the credit cards associated with your account” challenge and a “retype your email address” challenge. We designed the four digit challenge to prove that the person who is attempting to login owns one of the credit cards associated with the account. We designed the email retype challenge to use the email address we have associated with the account as a shared secret. In this system, we use the same detection algorithms for both populations, but challenge some of the users with the “CC last 4” challenge and others with the “retype email” challenge. This table shows our overall results, where the main metric we use to evaluate our challenges is churn — defined as the percent of users who fail to take a ride within two weeks after seeing a login challenge. The first surprising result was that such a low percentage of users who saw a login challenge churned. Although we are unaware of any published numbers around user churn and login challenges, based on our experience with social networking and internet companies, we were expecting 20% — 30% of the users who saw a challenge to churn. There are a number of plausible reasons for this difference, including the fact that ride sharing apps are quite different than email providers or social networks, but we believe that the difference is in the two escape hatches we provide for users based on our asymmetric advantages: login with Facebook and create a new account. For login with Facebook, as a consumer company we can assume that people are comfortable with social networks, and we can use this as an additional signal to identify the same human we saw previously. For creating a new account, Lyft’s overall on boarding flow is lightweight and highly optimized, so we can enable one to simply create a new account and unlock their ability to take rides without risking the security of any existing accounts. The second surprising result was that the retype email challenge performed better than the CC last 4 challenge. Before we deployed these systems, we had assumed that the numeric keypad was a more mobile-first experience and that users would be able to pass the challenge with less friction. However, we discovered that credit card companies will sometimes update credit card numbers — while still honoring existing tokens — so Lyft would have stale data. Also, for people who don’t have the last four digits of their credit card memorized, the CC last 4 challenge ended up being a possession authentication factor where they needed to have their credit card on them when they try to login. In contrast, people are likely to have their email address memorized, so despite the more onerous alpha numeric keypad, people were able to get through the retype email challenge at a higher percentage. The end result is that Lyft uses the retype email challenge exclusively for this class of login challenges, adhering to our principles. Although the CC last 4 is a mobile-first flow (Principle 3), due to complications around knowing credit card numbers we were effectively locking out some users (Principle 1). The biggest gains Lyft has made in fighting fraud and improving our account security is through our work on building sound distributed security systems, enabling quick and precise responses to active attacks, manufacturing clever signals, and understanding and changing the product to fend off the bad actors. This foundation serves as the base to enable humans to more effectively identify and stop bad actors, and provides the crucial signals needed to power machine learning algorithms and business rules that predict malicious behavior. Some people believe that machine learning is all one needs to fight fraud, but in our experience, without these basics in place first, the fraudsters have a fundamental advantage. Interested in having big impact by reducing fraud and improving security at a massive scale? Lyft is hiring ! Stories from Lyft Engineering. 1.4K 1 Thanks to Ryan Lane . Fraud Identity Security Product Security Engineering 1.4K claps 1.4K 1 Written by Inventor, tinkerer, engineer, hacker, and hater of fraud and fake accounts. Currently at Bouncer + CS prof @UCDavis. Formerly @lyft, @Twitter and @IllinoisCS. Stories from Lyft Engineering. Written by Inventor, tinkerer, engineer, hacker, and hater of fraud and fake accounts. Currently at Bouncer + CS prof @UCDavis. Formerly @lyft, @Twitter and @IllinoisCS. Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-12-14"},
{"website": "Lyft-Engineering", "title": "how to deal with the seasonality of a market", "author": ["Marguerite Graveleau"], "link": "https://eng.lyft.com/how-to-deal-with-the-seasonality-of-a-market-584cc94d6b75", "abstract": "Data Data Science Engineering Mobile Product Security At Lyft we want to ensure that in the next few weeks there will be enough drivers so that all the passengers will be able to get to their destination in time, but also that there will be enough passengers so that drivers will be able to work when they want to, i.e that the market will be balanced, to provide the best transportation to everyone. Lyft has built many tools and bonuses to incentivize drivers and passengers to use Lyft more often or at specific times. But can we predict a few weeks in advance when we will need to launch this machinery, and if it will be enough to close the gap between drivers and passengers? How can we predict the daily demand and supply a few weeks in advance ? Before starting to predict a raw time series, we need to understand how people ride and drive, and what affects their patterns of behavior; what we call seasonality. Only then will we predict the underlying evolution of the trend, the overall growth of driver hours and passengers ride requests. The seasonality patterns are a result of various phenomena: whether it is the work / home / week-end balance, or recurrent holidays or events, or even season effect (referring to summer / winter), each needs to be dealt with differently. We don’t move around a city the same way during the weekend and during the week. Depending on which city you live in, the effect of weekly seasonality varies, but typically there will be more rides on the weekend than during the week. Evaluating precisely the effects however, can be tricky. The seasonality is defined as a multiplicative coefficient relative to the underlying trend (the long term evolution). rides = trend * (1 + seasonality) Equivalently, we can work with the logs of the times series, which allows us to work with additive seasonality instead of multiplicative seasonality. log(rides) = log(trend) + additive seasonality The easy first go at evaluation of the weekly seasonality is to guess the trend (by a rolling average over the past 7 days for example), and to average the weekly effects. However this approach may fail or be inaccurate: First the time series is not affected only by weekly seasonality: drops or peaks can be due to holidays, or an unexpected weather event like a big snow storm Second, the weekly seasonality can vary over the years. The people using Lyft three years ago are different from the current users, and may use it for a different reason, for example starting to use it regularly as a daily commute means. Instead, we use a very common and powerful model, the Kalman Filter. Kalman Filters are a powerful tool used to evaluate the hidden state of a system, when we only have access to measurements of the system containing inaccuracies or errors. It bases its estimation on the past prior state, and the current measurements. For example, it can be used to estimate the position of a car based on its GPS signal. The position of the car at time t is a combination of its prior estimates of position and speed at t-1 , and of the current GPS measurements of position (which can be inaccurate or contain random errors). For more details on Kalman Filters, check out this class . With the notation from the figure above, Kalman Filters will be defined as follow: With those definitions, Kalman Filters can be applied to a car movement as we have just described, but also to the weekly seasonality of a time series. For each day of the week, we suppose the observed value in day can be decomposed between the level of the given week (the trend as described in the above graph), and the specific seasonality of the day in this week. After defining the problem, its state transition and observation model, the Kalman Filters can be tuned. This means fitting the unknown parameters of the model. The Kalman Filter is tuned iteratively by going through the time series, therefore it needs one more thing before being tuned: a first guess of the initial state, for example, the the observation during the first week. After tuning the Kalman Filters, it can be used to evaluate the state of our system every week, giving us the additive weekly seasonality. By removing the seasonality, we obtain the weekly-deseasonalized time series: As we can observe, the peaks during the week-end are very well taken into account. But some peaks and downs are still unexplained: those are due to either holidays, local events, or random weather conditions. Although a snowstorm can not be forecasted weeks in advance, we should have a pretty good idea of how Christmas is going to look like, based on what happened in the past years. Now that we got the weekly effects out of the way, we can focus on the next holidays and events in each city, Halloween is coming up pretty fast, and before that students are starting classes again in big college cities, which is going to drive the numbers up. Once again here we look for the additive seasonality coefficients describing how much each recurring phenomena is affecting our time series: log(rides) = log(trend) + additive seasonality Halloween and classes starting actually have a very different effect on people’s behavior in terms of transportation. On one hand, Halloween’s effect, similarly to a marathon or other holidays, is very narrow in time: it is concentrated on the Friday and Saturday nights when people use Lyft to go celebrate and come back home safely. On the other hand, students going back to school have a long term effect over several months, and opposite to the Summer when the population of a college city drops. Modeling holidays and yearly effects is based on the very basic idea that they will impact the time series in the same way year after year. Using the definition above, we will fit seasonality components based on the definition above, using the residual Z(t) = log(rides) — log(trend) . Punctual events are modeled by a few data points, whereas yearly seasonality (winter vs summer, classes starting, etc) are modeled by one time series described by a Fourier decomposition. The linear model is fitted by minimizing the least square errors, with a quadratic penalty. Where H is the matrix containing the time indicators and Fourier decomposition. You will recognize an optimization problem very similar to a ridge regression. However, a few tricks were added. Indeed, each time series is very different from another, and we need more flexibility than a regular ridge to truly adapt to the time series we are modeling: H can be modified ( Hreg ) to account for the decreasing impact of seasonality with time, due to the growth of the market W is a diagonal matrix; when the diagonal value is set to 0, it basically cancels some term in the regression. It is very useful when we want to ignore a period in time, for example during a hurricane Lambda , finally, is a penalty term that can be adapted to each holiday, for example putting a bigger penalty on the yearly effects than on the holidays. After setting all the parameters, and fitting the seasonality coefficients, we obtain the holidays and yearly seasonality effects. A big part of understanding seasonality, is knowing what to expect. The local teams in each city know what events affect Lyft’s users, their input is key to ensuring that seasonality catches all the recurrent changes in the markets. After incorporating both weekly and yearly seasonality, the residual time series represent the underlying evolution of our market. Now that we have removed the effects of seasonality, we can focus on forecasting the evolution of the trend. First of all, the trend is computed by smoothing the deseasonalized time series. To do so we will use a Kalman Filter again, modeling the trend and its growth. Forecasting the trend can then be done using directly the estimates from the Kalman Filters. But the trend of the market are affected by many other factors: the number of new drivers / passengers, marketing campaigns, change in pricing, etc. A Machine learning model is not particularly well equipped to deal with time series, and other techniques are used to compute seasonality. However, it can do a great job at incorporating external indicators into a model, that’s why we built a machine learning model to predict the evolution of the trend. Stay tuned on the Lyft blog for part 2. of the article, with more details on the trend computation and trend forecasting. Predicting holidays and seasonal impacts is key to understanding the evolution of the markets, as much as predicting its trend. It allows Lyft to anticipate unbalanced market conditions, and make plans to avoid those situations, using all the different tools our growth teams have built. If you enjoyed this post, follow and recommend! To learn more, check out our other Science posts! Fingerprinting fraudulent behavior Interactions in fraud experiments: A case study in multivariable testing Stopping fraudsters by changing products What’s in a name? The semantics of Science at Lyft From shallow to deep learning: A Research Scientist’s journey through hand-coded regressors, pickled trees, and attentive neural networks As always, Lyft is hiring! If you’re passionate about developing state of the art machine learning models or building the infrastructure that powers them, read more about our Research Science and Engineering roles and reach out to me! This post would not have been possible without the help and work of Su Wang. Many thanks! Stories from Lyft Engineering. 736 9 Data Science Timeseries Kalman Filter Seasonality 736 claps 736 9 Written by Stories from Lyft Engineering. Written by Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-11-14"},
{"website": "Lyft-Engineering", "title": "lyft android tech talks fall 2017", "author": ["Kathy Ma"], "link": "https://eng.lyft.com/lyft-android-tech-talks-fall-2017-8fc6501ca161", "abstract": "Data Data Science Engineering Mobile Product Security Pierce Johnson , Ryan Tempas , and I recently spoke at our Android Happy Hours as well as Droidcon SF & NYC 2017 to share our work with the broader tech community. We’re proud to represent the Lyft Android team and to share some of the fun and interesting projects we’ve worked on. Check out the recaps of each of our talks below! Shuttle was launched earlier this year as a beta program in San Francisco and Chicago. It aims to be a sustainable and affordable commuting solution — through building density along popular routes and filling higher occupancy vehicles, this feature can unlock lower per-ride price points for passengers and increase vehicle efficiency for drivers. In this talk, I dive into the technical and product challenges of building this new product on Android while working with in a cross-functional team (whose responsibilities include passenger and driver experiences, route generation, matching, dispatch, engagement, growth, and overall strategy). In his talk, Pierce presents an overview of integrating with the Lyft Amp, a Bluetooth LE embedded systems hardware device developed by Lyft and used by Lyft drivers. The device connects to the Lyft Driver app to display unique animations and information to drivers and passengers on two LED screens. The Amp can greet a passenger when the driver arrives at the pickup location, display the ETA for the ride’s destination, display the Seahawks colors on game days, and much more! While the product itself is unique to Lyft, the core problems and challenges of building embedded systems into Android applications are not. Embedded systems programming is unfamiliar to most mobile developers, but may become a larger part of our lives in the ever expanding network of connected devices. His presentation dives into development processes and architectural components and decisions required for integrating bluetooth-enabled hardware into an Android app by using Lyft’s own embedded systems device — Lyft Amp — as an example. Originally Lyft was a single app where users could switch between driver and passenger modes to give and receive rides. There were initial advantages to having one app with multiple modes, but after time this bloated the app size and significantly slowed the creation and release of new features. Splitting these two modes (one into a Passenger app and one into a Driver app) could be done many ways, but we settled upon using the modularization of various aspects of the code for the task. Ryan discusses the architecture and execution of this modularization choices made by Lyft in order to release the standalone Driver app. Interested in joining the Lyft team? We’re hiring! Drop us a note at android@lyft.com. Stories from Lyft Engineering. 110 Thanks to Ryan Lane . Lyft Android Engineering Mobile 110 claps 110 Written by Android Engineer @ Cash App Stories from Lyft Engineering. Written by Android Engineer @ Cash App Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-01-26"},
{"website": "Lyft-Engineering", "title": "getting to know engineers in data at lyft", "author": ["Sarah Gonsalves"], "link": "https://eng.lyft.com/getting-to-know-engineers-in-data-at-lyft-5ae6a57a8725", "abstract": "Data Data Science Engineering Mobile Product Security Meet Allison and Temo , two Software Engineers on the Data Platform team. We asked them to share about their work and experience at Lyft. Allison is a full-time software engineer for the Data Platform team at Lyft and is also a former intern. She primarily works on Amundsen, an open-source data discovery and metadata engine. What’s been the most challenging part of this job? Any memorable tasks/projects which were challenging? I would say the hardest part of my role is organizing my work in terms of priority. The impact on end users internal to Lyft is not the only factor our team must consider in order to determine how important an issue or feature is, but we also have to take into account how the Amundsen community will be impacted, and how we can prioritize our work internally to align with external asks as well. There is so much interesting work to do in both spaces, so it is difficult to strike a balance sometimes. What part of your role do you like the most? I really enjoy how collaborative my work is, and how the culture at Lyft really encourages engineers to work together in solving problems. If I have looked at a problem long enough and I am unable to fix it, I’ll message another person in my team or jump on a quick call to brainstorm and come to a solution. Given that we are working remotely, this has become really similar to just going over to someone and asking them to help you think through something as you write ideas on a whiteboard, which helps maintain some sense of closeness and trust with my coworkers. Since I work on Lyft data discovery platform Amundsen, which is open source, collaboration is not confined to my teams and other internal teams. Given that Amundsen has been adopted by 29 companies so far, my role allows me to collaborate with talented engineers from our community of 1,000+ people. I love being able to present ideas or problems to my coworkers internally, and also being able to get extra support from the community slack channel. This is also a unique opportunity for us to learn how other companies use the Amundsen product to serve their needs, which are often different to those at Lyft. This challenges us to consider not only how we can build a great product for internal use, but how new features and changes we make could potentially impact the diverse use cases of all adopters of the product. How was your transition from being an intern to full-time? Any words of advice for future interns? Although I joined a different team than the one I interned with, the transition was very smooth. We have a lot of practices that are used by pretty much all engineering teams at Lyft, so onboarding was not as daunting as it was the first time around. It also helped that as soon as I joined my new team, all of my more senior peers made sure I got onboarded at a pace that aligned with my level of familiarity with Lyft’s processes and the technology we used. I went from a backend role in the ETA team writing Go code to a full-stack role in the Amundsen team using TypeScript, React, and Python. I would say my assigned mentor and other engineers in my team made it really easy for me to get familiar with the services and technology we use every day. My best advice for interns or new grads joining Lyft is to rely on everyone around you. The reality of our industry is that people often change teams and move around within the company, so you never know how long you will be working with these amazingly talented people. Set up 1:1 meetings with them, ask them to help you problem solve and review your work, ask for help with short and long term goal setting for your career. Everyone I have ever met at Lyft is willing to help you grow, and in my opinion that is the best part about Lyft’s work culture. Onboarding during a lockdown: Since you joined remotely what advice would you give other folks joining now? When I onboarded as an intern, I was in-person, and comparing that to my remote onboarding experience, I would say it was not all that different, other than the fact that I was doing all of the process at home. The one thing that can be really hard when you are remote is feeling comfortable with asking questions. As weird as it feels to “bother” people on Slack with your questions, this is an irrational fear you have to get over. You probably would not hesitate to raise your hand or tap someone on the shoulder in real life to ask them a question or ask for help if you are stuck, so just think of a Slack message as the work from home equivalent. What does your day to day job look like? What tools/technologies do you use the most? Any favorites? It feels like there is never a boring day at work. When I am not addressing Lyft internal tickets for Amundsen, I can always pick up an issue from our public github repository, or look at the community slack to see who needs support and how I can help. On an average day I’ll start by looking at Slack to see if I have been mentioned on anything urgent, and going through emails. Next I’ll look at any PRs I have open or that I was asked to review and figure out what’s left to do on those. If I have no work in progress currently I will look at Jira to see what tasks I have queued or look at our backlog and pick up a ticket that needs to be done. Through the day I am also constantly monitoring the Lyft and Amundsen slack channels for questions or urgent issues. My team has four services we work on so in a week I can jump from working with React and TypeScript to Python. I also spend a lot of time investigating the root cause of issues in the backend, which many times requires collaboration with other teams that own data resources we surface in Amundsen. As far as meetings go, we do stand-ups three times a week, and sprint planning every two weeks. If we are collaborating with designers on a feature we will have design sync ups once a week to track the design process and give them feedback. We also have external community meetings once a month where we get to hear from other companies and how they use Amundsen and we also announce breaking changes and new features. Temo is a full-time software engineer for the Data Platform team at Lyft and also a former intern. During his time at Lyft, he has worked on a host of different things. What’s been the most challenging part of this job? Any memorable tasks/projects that were challenging? For me it’s finding that balance between being proactive and waiting. Sometimes with larger projects or ambiguous problems, it’s hard to balance when to try and do what you have in mind, and when to discuss and follow a different approach. Especially, when you are new to an area, or are surrounded by experts in the field, it’s hard to listen to your own voice. This balance between taking initiative and following guidance is still a challenge for me, especially with time and others’ priorities involved. As for memorable projects, the Data Governance and Data Security tasks were pretty challenging but pretty memorable. There were many unknowns, different ways to tackle them, and pivots such as understanding the data that we have, determining what or who owned it, understanding the lineage, working with legacy systems, and ensuring we did not break our current pipelines. It was great seeing how not only the Data org but the company came together to get it rolling. What part of your role do you like the most? I’ve always been interested in understanding how our data workflows work end to end. Something I really like about my role is how you can get involved in the different parts of the data pipeline, and understand it at our scale. From how we emit a datapoint to the systems we use to process it to how data customers use such data, you can learn and get involved in different projects along the pipeline. I really value having the opportunity to not only understand this holistically, but how we can keep improving it. One thing you wished you knew before joining Lyft and the Data Org? Someone recently told me that you’d be surprised how supportive folks are for you to follow what excites you. Whether it’s a new proposal, different area, or new team, don’t be afraid to say it and try it. You never know what doors open. Without having prior knowledge of this, I’ve been fortunate enough to have experienced this. I’ve been able to contribute to projects close and far to my main area that I was curious about, and currently I’m exploring the streaming data side after working on batch processing. How are you managing collaborating with team members while working remotely? Any advice to someone starting out remotely? Honestly it’s been a challenge, I started on a different project when everyone was remote. For me, two of the hardest collaboration problems to tackle are communicating progress and getting to know your team. I’m working on improving my written communication in JIRAs, resolution notes, and documentation. Being remote, this helps with transparency, and understanding the progress or blockers folks have. And a second challenge is getting to know your team, and how they work. It’s not as easy to do this as over lunch or a coffee walk in the office. I really recommend scheduling time with your teammates, and folks you really wanna meet sooner rather than later. Some of the most fulfilling conversations and learnings I’ve had are during these chats. It’s always awesome hearing people’s stories, their paths and life outside of work, and to just check in with them outside of stand-up. It’s great — not only to interact with others during these isolating times — but also build rapport and work better with your team. As always, Lyft is hiring ! If you’re passionate about developing state of the art machine learning/optimization models or building the infrastructure that powers them, read more about them on our blog and join our team. Stories from Lyft Engineering. 168 Lyft Data Diveristy And Inclusion Data Engineering Intern 168 claps 168 Written by Stories from Lyft Engineering. Written by Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2021-05-05"},
{"website": "Lyft-Engineering", "title": "a day in the life of a lyft data scientist", "author": ["Diversity in Science at Lyft"], "link": "https://eng.lyft.com/a-day-in-the-life-of-a-lyft-data-scientist-ffe6651f138b", "abstract": "Data Data Science Engineering Mobile Product Security COVID-19 has brought many changes to daily life, and the routines of our Lyft Data Scientists have been no exception. We sat down with Andy , Faten , Garrett , and Yibei for their takes on working at Lyft during the pandemic. Tell us a little bit about what you do at Lyft, and what a typical day looks like? Andy : I’ve been a data scientist at Lyft for around one year, around the same time my son was born! I work on the request flow part of the rider app, basically which ride-types we recommend to the rider in certain situations. In caring for my child , sleep is sometimes hard to come by, but both Lyft and my manager handle this very gracefully and are understanding of my situation. I usually get up early to put my son into daycare. I highly value these conscious moments before dropping him off — when he is still very alert and smiley. After coming back home, there is still half an hour for some exercise that I selfishly claim to combat screen fatigue. I usually sign on around 9AM with no meetings for 1–2 hours. I check emails, Slack, prepare for presentations, evaluate overnight models, or focus on some more fun projects during this time. In the early afternoon I have a couple of 1–1s and team meetings which require preparing and documentation. Ad-hoc conversations with stakeholders around data or business issues also tend to happen during this time. The late afternoon is when I work on long-term projects. For me it’s good focus time. I pick up my kid around 4:45–5PM, go for a walk with him, and tuck him into bed. I generally wrap up with a couple of hours of coding during the evening — there are usually no distractions then so it’s a great time for heads down work. That generally caps off my day at Lyft, and I follow this schedule during the week; weekends are work-free time and that is certainly something I like about Lyft’s culture. Faten : I work on the Revenue Operations Science team, where we create tools and analyses to help stakeholders make operational decisions rather than work on a consumer-facing product. A typical day of mine starts with a lot of analysis, usually for tool building, debugging, or upcoming operational decisions. I also often put together slide decks for stakeholders and cross-functional teams, meet with engineers and other collaborators to share project progress, and provide guidance on the tools I’ve developed. This is my first job in industry . Before Lyft, I wrapped up my Ph.D. in Neuroscience and pivoted into data science last summer. There is a high meeting load, but I can see their value. This definitely requires good time management skills and active planning of heads-down time to ensure that I’m pushing projects forward. Garrett: I am a scientist with the Lyft Transit Bike and Scooters line of business, where I support Lyft’s hardware team. What I do on a daily basis varies depending on where our team is in the hardware product development cycle. Are we launching a new product? Are we planning the next-generation hardware? Analysis, modeling, presenting, meeting with stakeholders: it just depends on the day and what stage we are at. In addition, our team is unique in that a good amount of our time can be spent doing hardware engineering . Most of our team has some background in hardware, and I did a Ph.D. in experimental physics before becoming a data scientist. If we have an issue that comes up in testing or in the field, we’ll work to debug it and help change the design. As such, when we’re first launching a product or discovering an issue, data scientists are very valuable to the engineers because we can give them data about what’s actually happening with hardware in the field. We will also help in the initial hardware design process by applying driven frameworks to a hardware product’s business case, product requirements, and engineering architecture. The hardware team consists of 50+ engineers in specialty disciplines that all work together, and we try to bring discussions to resolution with data, whether on the product side or the engineering side. There’s a lot of talking with cross-functional partners in addition to doing analyses, building dashboards, and building models. Yibei : I am a part of the Trip Experience group. From request to drop-off, we look at cancellation problems across the whole rider journey. Last winter, I made a decision to work remotely from Hawaii , to get a respite from the cold of New York. The COVID situation at the time was better managed, and there were a lot more things to explore. I’ll speak to my typical day in the context of my time there: There was a 5-hour time difference between Hawaii and NYC, meaning quite early mornings. I learned to adjust my time to load a lot of meetings towards the afternoon in Eastern time, and colleagues were adaptable to these changes. I also did my ad-hoc analyses and responded to slack channels in the mornings, and used the afternoons as the focus time for activities such as deep dives, experimentation analysis, and strategic thinking. After work, I was able to have dinner with friends outside, and there was always a lot to do during the weekends. I came back to NYC in early 2021, but I loved my time in Hawaii. How do you feel about working remotely during COVID? How does COVID change your daily work? Andy: Working from home this past year has added a significant amount of flexibility, which has been extremely helpful as a new parent. I also believe data science is not necessarily a 9-to-5 job — sometimes you work on a project, do something else, take a break then come back and work some more. I’ve been very happy with my work situation so far — I feel that a lot more gets done in my current role than my previous positions, and every interaction I’ve had with colleagues has been very structured and well organized. To share a fun story, due to COVID, the onsite interview was done remotely, and between interviews I was very nervous so I did some pacing through my garden to destress. I stepped on a wasp — barefoot — which was a bit unfortunate. Thankfully nobody noticed and I still got the offer; a small price to pay. Faten: I started this job remotely, so I don’t have a sense of what it was like before COVID. I feel that I’ve been able to be efficient while working alone, as I can focus on projects for hours at a time, and less time is lost context switching, but it can take longer to get questions answered and it is harder to onboard remotely. The biggest pain point has been feeling disconnected from coworkers, as we are not all in the same physical space with each other. Hanging out with team members virtually does help with this! I have two cats, and working from home with them is pretty fun. Sometimes my cats jump onto my desk and walk over my keyboard, sending an unintelligent message over Slack, or editing my SQL query. They are a great source of stress relief. Garrett: Our team has managed well remotely, but there are certain tasks that are best served in-office, so we go in from time to time. Hands on time with hardware is really important. As I used to commute with Caltrain, the first thing I had to do was convince my wife that I’d be safe to commute in. Another difficulty in COVID times is that people tend to schedule a lot more meetings virtually, so I have to figure out how I’m going to get to the lab, which meetings I can postpone, and which ones I’ll take from the lab (which can get loud with machinery). Lyft has strong safety protocols once in the office, but getting there safely has been the bigger challenge. As a COVID-times anecdote, one day I went into the office to help build bikes for an internal beta. We set up a 20-step assembly line, and my goal for the day was to go from steps 3–9. I did less than half of that because every time somebody came in that I hadn’t seen in person in about a year, we’d catch up a little bit. Yibei: I am still relatively new to the company, as I joined five months before COVID happened. I switched teams during that five months, so I was still in an onboarding period when COVID started. Between COVID, joining a new team, and covering two pods within Trip XP, work was challenging in the beginning. When COVID hit, we had the incremental difficult task of positioning the business for a successful rebound. However, I felt extremely supported by my manager and the other members of my team. In general, I definitely miss having face time, as when people meet remotely, people tend to be very busy, jumping in and out to their next meetings and adhering to the agenda. In Hawaii, there were many things to explore over the weekend, and less COVID, so it felt safer to go outside. I found that overall, moving to Hawaii made me work more efficiently during the daytime, because my stress level decreased. Q) What tools/skills or past experiences help you succeed as a data scientist at Lyft? Andy: One thing that I think really helped me is being curious and excited about what I am working on. This helps me stay focused, and drives me to ask good questions that further my understanding. Another thing I try to do is not jump to conclusions; as a data scientist it’s important to always be able to back up your arguments with data and that’s something I try to evangelize. In my past experience as a physicist I worked in small teams in various labs. This taught me that it’s super important to proactively help as much as you can, because it creates a kind environment that everyone wants to continue working in — especially if it’s hard sometimes. It seems that at Lyft I have found the same helpfulness being propagated, which creates an extremely welcoming environment. Faten: The biggest skills that carried over from my previous experience would be forming data-driven hypotheses and turning an open-ended problem into an actionable project with explicit success criteria. Another skill that has been helpful is the ability to communicate science to lay audiences; my volunteer experience giving scientific talks to a general audience in graduate school helped me a lot. Garrett: I think people in data science tend to be very focused on using methods to answer a problem. However, my impression about what’s most valuable for data scientists is to focus on the problem and to provide a solid answer, no matter what the data looks like or the tools at your disposal. This includes even documenting our opinion when might not have sufficient data, from the viewpoint of how we approach it as a data scientist. A Data Scientist with partial data or a framework alone can bring a new perspective to many conversations and decisions. Yibei: First, leave time to breathe, and get to know your coworkers on a more personal level. Chat about non-work-related stuff! We tend to be very efficient and technical in terms of solving the meeting agenda items, but sometimes we might overlook the importance of relationship and trust-building. And that also means that when we have different opinions on the project, we should feel free to disagree and commit. Second, make sure you’re keeping a good balance between work and life. As we’re all working from home, there is a tendency to fill commute and entertainment hours with more work hours. If you don’t remind yourself to keep these hours to yourself, you can evangelize unhealthy behaviors. The day-to-day can become repetitive, so find something that breaks you out of that cycle, gives you something to look forward to, and in turn makes you more efficient. Q4) Any parting words or advice that you can give to an aspiring Lyft data scientist? Andy: For the interview, I think it’s important to really nail what are some of the key metrics, specifically for the transportation sector. It’s also a good idea to brush up on statistics before the interview! Faten: Based on what I have observed so far, know your SQL and python, know your stats, and practice presenting clearly and succinctly. It will also help if you are able to adapt to the fast pace and constant changes at Lyft by turning projects around quickly while maintaining high quality work, and then iterating on them to get to a more final, thorough product. My advice for interview preparation is presenting everything in the context of business decisions, keeping in mind questions like ‘why does this matter for the company and customers’, and ‘what is the incremental value of doing this’. Garrett : I would encourage people to think about the problems and the framework, and use your business and engineering judgement. Think through how you can contribute even when you don’t have great data, thinking less about the tools and more about the problems that need to be solved. Yibei: The Science org at Lyft has been spending a lot of time building a strong community among scientists. Efforts originating from the Diversity and Inclusion Group and Science Learning and Development Councils have been very helpful in fostering a sense of belonging and inclusion. If you are interested in joining our incredible team of Data Scientists, check out our Careers page! A huge thanks to the following for their great contributions to this post (in alphabetical order): Ava Li, Carol Zong, Farshad Majzoubi, Michael Zhou, Simran Mirchandani, Tim Xu, Xin Wang Stories from Lyft Engineering. 111 Thanks to Lyft . Data Science Work From Home Lyft Workplace Diversity And Inclusion 111 claps 111 Written by Stories from Lyft Engineering. Written by Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2021-04-20"},
{"website": "Lyft-Engineering", "title": "chaos experimentation an open source framework built on top of envoy proxy", "author": ["Kathan Shah"], "link": "https://eng.lyft.com/chaos-experimentation-an-open-source-framework-built-on-top-of-envoy-proxy-df87519ed681", "abstract": "Data Data Science Engineering Mobile Product Security Services are bound to degrade. It’s a matter of when, not if. In a distributed system where there are many interdependent microservices, it is increasingly difficult to know what will happen when a service is unavailable, latency goes up, or when the success rate drops. Usually, companies find out the hard way when it happens in production and it affects their customers. This is where Chaos Engineering helps us. Chaos Engineering is the discipline of experimenting on a system in order to build confidence in the system’s capability to withstand turbulent conditions in production. By regularly experimenting with service degradation in a controlled production environment, we can preemptively validate our system's resiliency and uncover issues. As Lyft grew, we quickly realized the importance of Chaos Engineering. Since all the communications between Lyft services run through Envoy Proxy , it seemed like a great choice to leverage it for running chaos experiments like fault injection. Lyft previously performed fault injection experiments using Envoy’s runtime (disk layer). Engineers ran a CLI command that generated runtime files locally. Once runtime files were committed to GitHub, they got deployed by writing to the local file system of a cluster of hosts. Envoy read these files into memory and injected faults into the requests. Engineers had to repeat the same process when they wanted to terminate the fault. This worked well when Lyft was at an early stage and performed one-off experiments to prepare for a surge in traffic on days like Halloween, New Year’s Eve, etc. However, this had its drawbacks when running at scale. High touch Many times, engineers needed to perform multiple steps in their experiments, such as injecting faults for 1% of requests and then slowly increasing to ensure safety. This involved multiple commits to GitHub, and this process was very cumbersome. Long time for faults to be injected Once the runtime changes were merged, runtime deployment took a few minutes to complete. This could be risky when the experiment caused a real production issue and the engineer wanted to terminate the experiment right away. Poor insight into all active faults It was very difficult to find out how many faults were running at any given time. This info is very important to know when looking at all experiments from a bird’s-eye view. External dependency — GitHub This process had a dependency on GitHub. If GitHub were to go down (which could happen since it’s a service, after all), the active faults would persist in the system for a long, undefined period of time. This was a big drawback and could jeopardize our business. The Chaos Experimentation Framework (CEF) is an open-source framework built on top of Envoy Proxy in Clutch . Clutch made perfect sense for integrating the CEF since we could leverage built-in features like rich and responsive UI, role-based access controls, audit logging, database layers, stats sinks, etc. The CEF provides a powerful backend and user-friendly UI which gives high confidence to engineers when they perform experiments. The backend injects faults within a few milliseconds after starting the experiment from the UI. This is achieved using Envoy Proxy’s xDS APIs to transmit fault configuration rather than relying on deploying configuration on disk to every machine. It was designed to quickly inject faults without any dependencies. With the CEF, we have already seen several benefits at Lyft: Self-serve This framework is fully self-service, and Lyft engineers can perform fault injection experiments on their services quickly with the click of a button. Integration with CI/CD pipeline We run experiments on all deployments to ensure that deploying new code will not affect a service’s resilience to failures. This ensures that resiliency is validated regularly across the system. Ensure client resiliency Although we work to ensure the resiliency of our service mesh, at Lyft it’s equally important to make sure our mobile clients have a fallback plan when things don’t work as expected. Usually, product flows on mobile clients are tested under ideal conditions, i.e., the “happy path”. However, flows in non-ideal paths are hard to test in the QA environment. By regularly running fault injection experiments on our mobile client endpoints, we can minimize the chance that our customers are affected when a backend service is degraded. Faster service tier auditing Lyft services are categorized in different tiers (Tier 0 — Tier 3) based on how business-critical each service is. With Tier 0 being the most important, we inject faults in Tier 1 services that have Tier 0 downstream dependencies. With this method, we are simulating the situation where one specific Tier 1 service is experiencing a degradation, and observing how this degradation affects that service’s Tier 0 downstream dependents. Ideally, there should be no hard dependency on a Tier 1 service from Tier 0. However, we discovered some situations where this was not the case. With the CEF, this tier auditing is faster than ever. Validating fallbacks to third party external services Usually, it’s very hard to test failures from external services (like Mapbox, Google Maps, DynamoDB, Redis, etc) until they happen. With the CEF, Lyft engineers have been proactively validating fallback logic in case external services become degraded. Keeping observability and configuration up-to-date By running periodic fault injection tests, engineers preemptively tune service alarms, timeouts, and retry policies. Along with these tune-ups, they also make sure their services’ stats and logging provide clear indications of root causes when issues arise. “The Chaos Experimentation framework is very simple and straightforward to use. It allows service owners to determine the resilience of their microservices.” — Testimonial from a Lyft Engineer There are two major components in the framework — the backend server and the Envoy xDS management server . The backend server is responsible for all the CRUD operations of the experiments. It stores experiments in the tables of its Postgres database. The other component that the framework ships with is an xDS management server. The management server consists of Extension Configuration Discovery Service (ECDS) and Runtime Discovery Service (RTDS) APIs of Envoy Proxy. Either of the xDS APIs can be used to perform fault injection experiments: With RTDS, one can make changes to runtime-specific faults. Conversely, ECDS allows for changes to the entire fault filter to perform any custom experiments. When Envoy in the mesh boots up, it creates a bi-directional gRPC stream with the management server. Below is a code snippet for an RTDS config in Envoy: The management server polls Postgres at a regular cadence to get all of the active experiments. It then forms a runtime resource with a TTL that is sent to its respective clusters. Hence, the propagation of faults only takes a few milliseconds. The framework itself is very resilient. It automatically terminates experiments when the success rate of service drops beyond a configured threshold. Additionally, in the case where the management server itself becomes degraded, all experiments are automatically disabled without any intervention from engineers. The entire framework, like Clutch and Envoy Proxy, is config-driven. One can choose to use ECDS or RTDS, tune the polling duration to Postgres, provide runtime prefixes, tune resource TTL times, etc. There is still a lot of work to be done in this space to prevent system degradation from affecting customers. Here are some of our ideas for future improvements: Scheduling of experiments Scheduling would allow us to run experiments 24/7 and provide more confidence that the system is up-to-date and resilient. Real-time stats (per second) with Envoy’s Load Reporting Service API To perform more aggressive experiments, there needs to be a tight metric-driven feedback system to ensure that we can quickly terminate experiments before they affect our users. Squeeze experiments Squeeze tests would allow us to route additional traffic to a particular host in a given service and help determine the maximum number of concurrent requests that can be served by that host in a cluster. Based on squeeze experiments, engineers can set the scaling threshold of the service and its circuit-breaking threshold . Enhanced Redis support The framework already provides basic support for injecting Redis faults. However, more advanced features would allow for injecting faults based on a certain set of Redis commands and performing latency injection or connection failure experiments. If you’re ready to get started or contribute, check out these resources: Documentation Code in Clutch Join Lyft — apply here Chaos Experimentation, an open-sourced framework, would not have been possible without the contributions and hard work from many engineers at Lyft including Alexander Herbert, Ansu Kar, Bill Gallagher, Daniel Hochman, Derek Schaller, Don Yu, Gastón Kleiman, Ivan Han, Jingwei Hao, Jyoti Mahapatra, Miguel Juárez, Mike Cutalo, Rafal Augustyniak, Snow Pettersen, and Vijay Rajput. Special thanks to Patrick Sunday, Martin Conte Mac Donell, Matt Klein, Polly Peterson, Michael Rebello, and Pete Morelli. Stories from Lyft Engineering. 236 1 Thanks to Miguel Juárez and Michael Rebello . Chaos Engineering Envoy Proxy Open Source Engineering Fault 236 claps 236 1 Written by Software Engineer at Lyft Stories from Lyft Engineering. Written by Software Engineer at Lyft Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2021-05-19"},
{"website": "Lyft-Engineering", "title": "lyftlearn ml model training infrastructure built on kubernetes", "author": ["Vinay Kakade"], "link": "https://eng.lyft.com/lyftlearn-ml-model-training-infrastructure-built-on-kubernetes-aef8218842bb", "abstract": "Data Data Science Engineering Mobile Product Security Authors: Vinay Kakade, Shiraz Zaman In a previous blog post, we discussed the architecture of Feature Service , which manages Machine Learning (ML) feature storage and access at Lyft. In this post, we’ll discuss the architecture of LyftLearn, a system built on Kubernetes, which manages ML model training as well as batch predictions. ML forms the backbone of the Lyft app and is used in diverse scenarios such as dispatch, pricing, fraud detection, support, and many more. While the modeling technique used by each team is different, a common platform is needed to simplify the development of these models, parallelize model training, track past training runs, visualize their performance, run the models on schedule for retraining, and deploy the trained models for serving. We built LyftLearn to achieve these goals. To satisfy the diverse use-cases of ML at Lyft, we followed the following design principles. Fast iterations : What differentiates ML model development from the rest of software development is the need for fast iteration. An ML Practitioner needs to quickly evaluate different approaches towards a problem and zoom in on the most promising one. A system that does not support fast iterations isn’t optimal from the perspective of ML modeling. No restriction on modeling libraries and versions : The field of ML is fast-evolving, with innovation happening in multiple directions and new capabilities frequently getting added to the established modeling libraries. Teams at Lyft use diverse modeling libraries such as sklearn, LightGBM, XGBoost, PyTorch, TensorFlow among others — and different teams use different versions of them. LyftLearn should not restrict the modeling libraries or the versions that can be used, except for rare exceptions due to security reasons. Layered-cake approach : We have customers who need programmatic access via an API, customers who prefer configuring and creating training jobs via a CLI, and customers who prefer interacting with a GUI. We follow a layered-cake approach where all these modes of access are available, with CLI and GUI generally forming a layer on top of the functionality provided by the API. Cost visibility : ML training usually forms a big part of the infrastructure cost. The users using the system should know exactly the cost of a training run so that they can decide whether the cost is justified based on expected business impact. Ease of use : Given the wide adoption of ML across Lyft, we can’t afford to have users needing to go through complex onboarding. The system should be self-serve even for implementing the advanced aspects of ML such as distributed training and hyperparameter tuning. Additionally, it should be integrated with (a) existing data sources to get training data, (b) existing model serving solutions to deploy trained models. We describe the architecture across various components, namely Model Development , Training, and Batch Prediction , User Dashboard , Image Build, and the underlying Data & Compute infrastructure. Users can develop the ML model either in a hosted Jupyter notebook environment or a hosted R-studio environment or locally in their favorite editor. If using a hosted environment, the user needs to go to the LyftLearn homepage to select the hardware configuration (such as the number of GPU or CPU cores and memory) and a base image to start with. The system provides a wide selection of base images for common modeling techniques used at Lyft. Teams can create their own custom images as well. The user can then do the development within the notebook and install any additional dependencies. The user can also connect this remote environment with a Git repository so that the changes can be tracked over time. Once satisfied with the model code, the user can Save Model , which saves a new container consisting of the model code and the additional dependencies overlaid on the base image. The user also needs to specify a version while saving (this could be SHA for the corresponding Git commit) to track changes to the model code over time. Note that once the user makes the selection of hardware configuration and the base image, the notebook environment is created using the underlying LyftLearn Kubernetes cluster, and this operation takes only a few seconds. The fast spin-up of a new environment helps increase the speed of iteration critical in the model development process. To save cost, notebooks that aren’t used for a few hours are auto-saved and auto-terminated. And, since the spin-up is fast, it doesn’t cause degradation of the user’s experience. The user can also choose to develop the model locally in their favorite editor. In that case, the user can use a CLI to specify the model code and dependencies and then save the model. Once a model container is saved, the user can run training jobs using the same. The jobs can be configured and scheduled programmatically using the LyftLearn API or manually using CLI or GUI. The model container takes hyperparameters and configuration parameters — and the training jobs can be run in parallel for different sets of these. For example, a common scenario is to train (using the same model container) a different model for each of the granular geographies Lyft operates in. This can be easily achieved by having a geographical region as a configuration parameter to the model container, have the model code query different training data based on the region parameter, and then train a model specific to that region. The training jobs run as Kubernetes jobs on the underlying Kubernetes cluster and can be scheduled to run periodically to retrain the model at a regular frequency. LyftLearn supports this parallelization via Flyte , Spark , or Fugue . After training, there are two ways a model could be deployed for predictions: (1) the model could be deployed as a service and called by another online service for point predictions, such as a pricing model that is called for every ride request in the Lyft app, or (2) the model could be scheduled for periodic batch predictions on large batches of data, such as an incentives model which is scheduled weekly and determines incentives for the passengers for the particular week. We have built a separate system for the models that need point predictions, and we use LyftLearn itself for parallelizing and scheduling models that need batch predictions. Users can see all the models along with their corresponding versions and their past training and batch prediction runs on GUI. For each run, the user can access the corresponding logs and the model performance metrics. Once a model is developed, users can opt to deploy their model to the production serving layer and manage the complete lifecycle through the GUI. In the Model Development section, we discussed how a user selects hardware configuration and a base image before starting development. LyftLearn provides a selection of base images with common modeling techniques, as well as users can add their own by invoking an Image Build , which is essentially a wrapper over docker’s image build. Teams typically create their own base image or extend one of the existing base images with team-specific libraries, and it is used for all the models developed by that team. To get the above user-facing functionality to work, we rely on the following components: LyftLearn Kubernetes Cluster : Kubernetes forms the backbone of the computing infrastructure of LyftLearn, and the notebooks, as well as training and batch-prediction jobs, run on Kubernetes. The cluster is optimized for interactive development and long-running jobs. The primary reasons for choosing Kubernetes are: (1) we can package the model code and its dependencies as containers, enabling teams to use different modeling techniques and their versions, (2) starting a new LyftLearn environment takes only a few seconds, enabling fast iterations. Intermediate Storage : We mount AWS Elastic File System as a Kubernetes volume for each user. Users typically use this to store intermediate data files. Training Data, Model Metadata, Container Storage : The training data comes from Lyft’s data warehouse and is queried using Hive, Presto, or Spark. The metadata of models (such as ownership information, past runs, metrics) is stored in AWS RDS Aurora. The base images, as well as model containers, are stored in AWS Elastic Container Registry. We reviewed the architecture of LyftLearn, a large-scale system for model development, training, and batch predictions. The primary design principles followed in LyftLearn’s architecture are support for fast iterations, no restriction on supported modeling libraries and their versions, and a layered-cake approach enabling the system to be accessed programmatically via API, CLI, or GUI. LyftLearn has wide adoption and is used by dozens of teams to build hundreds of models every week. Huge thanks to Han Wang, Anindya Saha, Drew Hinderhofer, Eric Yaklin, Andy Rosales-Elias, Willie Williams, Saurabh Bajaj, Mayank Juneja, Narek Amirbekian, Jason Zhao, Gil Arditi, Erik Vandekieft, Craig Martell, and Balaji Raghavan for making this work possible. Interested in working with us? Check out our current job openings ! Stories from Lyft Engineering. 218 3 Machine Learning Model Training Kubernetes Software Development Data Science 218 claps 218 3 Written by Stories from Lyft Engineering. Written by Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2021-05-12"},
{"website": "Lyft-Engineering", "title": "speeding ahead with a systematic approach to web performance", "author": ["Mihir Mathur"], "link": "https://eng.lyft.com/speeding-ahead-with-a-systematic-approach-to-web-performance-282b6cf8ae2", "abstract": "Data Data Science Engineering Mobile Product Security From replaying shared rides on a map, to solving physical safety problems in real-time, to managing a fleet of thousands of bikes and scooters, to viewing the trajectories of autonomous cars— frontend services at Lyft support a plethora of diverse use cases. As Lyft has grown over the past decade, so has the complexity of our business needs. However, the one core requirement for fulfilling these diverse needs is the necessity of high-performance web applications. We sat down with our senior and staff engineers who are well versed with the history of frontend engineering at Lyft to understand how we have navigated performance challenges and built an ecosystem to support a myriad of use-cases needing performant web applications. By looking back at our journey of building 100+ high-performance frontend microservices at Lyft, we’ve distilled our learnings into what we call the Hierarchy of Web Performance Needs — a system that can strategically identify the most impactful performance needs of an organization building web applications. In this post we’ll describe this framework and give a glimpse of our ever-evolving frontend performance stack. This framework could help engineers who know the best practices for web development but have grappled with questions such as: How do I quantify and measure the business impact and prioritize performance improvements over feature development? What kind of performance tools/techniques do I use or build first? How do I influence the culture of an entire organization to value performance? The popular adage, “If you can’t measure it, you can’t improve it”, holds true for web performance. About 6 years ago, one of the first web performance investments made at Lyft was capturing, viewing, and analyzing RUM performance data. The way it worked was by running a script in each web app that would use the (now deprecated) Performance Timing API to record timing of events such as requestStart , domLoaded , and domInteractive. This data was sent asynchronously to a server side analytics tracking endpoint using Navigator.sendBeacon() on page load and then stored in our data warehouses for analysis. Last year, we started recording the new standard of web performance metrics: Google’s Core Web Vitals . To easily record these metrics across our array of applications, we created a wrapper around the web-vitals library that sends metrics to our analytics endpoint and can be used like this: Once we collected all this data, we needed tools to examine it and gain meaningful insights. We use two third party tools for this purpose: Grafana : For real-time observability of metrics and for integrating with alerts. Mode : For advanced analysis of trends of data (stored in our Hive clusters) using custom Presto queries. Equipped with tools to capture and analyze performance data, one might know some actionable steps that could improve a metric. But simply knowing action items for improving performance metrics in an organization with lots of feature work on the roadmap is often not enough. How does one get buy-in from their team and leadership for dedicating time and engineering resources to improve system performance? One approach Lyft takes is joining the performance data with key business metrics. Articulating a performance hypothesis in the format: “increasing <perf_metric> by X% would increase/decrease <business_metric> by ~Y%” to stakeholders can help in prioritizing performance work. However, forming such hypotheses is easier said than done. First, engineers should be equipped with data: both for performance and the business. Second, the causal relationship between a performance metric and a business metric may not always be clear. The first problem can be solved by having a culture of data transparency. One tool that can help in democratizing data access to all internal stakeholders is Lyft’s open-source data discovery engine Amundsen . For the second problem, it may help to think about how users will be affected if some performance metric was to significantly improve. Can they do more in less time? Are they more likely to revisit the site? Will they recommend the app to more people? Some of the most impactful performance work that can be done, especially for consumer web products, is making web pages load as quickly as possible. A study recently found that slow-loading web pages caused a significant increase in users’ blood pressures and led to stress . Moreover, forming a performance hypothesis that ties a page load metric (eg., Largest Contentful Paint, Speed Index, Time to Interactive) to business value (eg., sales $, sign ups, cost savings) can be relatively straightforward since there are several examples off of which hypotheses can be based. For instance, in 2015 when we had an Angular 1.3 frontend, a project was undertaken to make Lyft’s driver sign up page load faster. The hypothesis was that a reduction in page load time would lead to increased driver sign ups. A series of improvements such as the introduction of webpack for bundle splitting, moving to React for faster rendering (among other reasons), enabling server-side rendering, and excess CSS removal were introduced. These improvements led to a reduction in page load time from over 2 seconds to a few hundred milliseconds, which led to a 9% uptick in driver sign ups at the end of the funnel. At Lyft, we do several load-time optimizations such as: Server Side Rendering, code-splitting our apps, Brotli compression for serving static files, pre-fetching content, dynamically loading expensive Javascript, using fall-back web fonts, setting up libraries to tree-shake , among many others. Most of these optimizations happen at the build system level. Despite all the optimizations that each frontend service inherits, we sometimes mess up. For example, we’ve had cases where duplicate libraries get packaged into different JS bundles of the same app. Thanks to our tooling, we are able to keep an eye on regressions. For instance, we record bundle sizes during each build, and use webpack-bundle-analyzer to periodically audit the bundles of our services and shave off as many kilobytes of Javascript as possible. While load time optimizations are often easy to relate to business metrics, the hypothesized impact might not always materialize. For example, hoping to improve conversion rates of one of our user facing pages, we A/B tested a server-side fix that lowered Time to First Byte (TTFB) by 100ms at p50 and over 3 seconds at p95. Conversion rates on that page barely budged. Even though we shipped that change, we learned that load-time optimizations may not always move the business needle. With some of the optimizations mentioned above, a website would feel snappy during load. But to truly delight users, every possible interaction on an application should feel instantaneous (i.e., there should be visible responses to each input within 100ms ). We strive to create such experiences. However, having buttery smooth and fast interactions is difficult if an application is data or compute intensive. One such example is the app used by our support agents for resolving problems for riders and drivers. This heavyweight application provides an interface for real-time chat and phone communication coupled with a CRM for quickly sifting through multiple users’ rides, payments, or support history concurrently. Another example of a client-side compute intensive frontend service is our internal application for operations teams to manage fleets of bikes and scooters. Users need to quickly zoom in and out while viewing a lot of information overlaid on a map. These services, among many others, are used by each internal user for several hours a day. Therefore, enabling fast interactions is vital to their productivity. A few things we do to improve run-time performance: Tighten the long-tail of API call latencies: By examining latency data for each API call, the slowest requests to the server can be prioritized for improvement. Batch Requests: The number of egress requests can be significantly reduced by batching. For example, metrics and logs can be sent to an analytics endpoint in batches instead of individually. Smart Data Fetching: Use GraphQL or paginated APIs for fetching data and prioritize data fetching for above-the-fold components. Memoizing: Store frequently rendering components in memory usin g React.memo() and the useMemo hook . Profiling components/interactions: Dissect the rendering, scripting, and painting time of complex components or interactions to figure out the best optimizations. React profiler and Chrome DevTools are great tools for run-time performance profiling. These profilers, along with a solid foundation of measuring and monitoring tools that let users record any run-time metrics and visualize over time, can help to pinpoint the slowest parts of an application and make prioritization easier. Once web pages load blazingly fast and every interaction completes with lightning speed — what should be the next focus? The next step is to ensure that every new application built by the company inherits instrumentation for performance monitoring, has fast load speeds, and has great run-time performance––all with minimal effort from engineers. This is at the top of the pyramid because one can condense the learnings from all the solved performance problems and extract them into the build system or into reusable primitives. A new engineer with very little experience could then automagically write high performance frontend code using the primitives. Furthermore, in an ideal performant web infrastructure, it should be very hard to merge non-performant code. This can be achieved through a combination of abstractions, processes, and education. Some of the building blocks of our performant infrastructure and culture are: lyft-node-service : Our NextJS-based infrastructure makes it easy to spin up a new frontend service with most of the performance optimizations built in. Lyft Product Language Design System : Our design system and the accompanying library of unified high-performance, accessible UI components that provide building blocks for performant applications. Plugins and Migrations: We’ve built an internal plugin system that lets engineers easily share functionality with other frontend services across Lyft, so a performance optimization implemented by one team (e.g. image compression) could easily be distributed to every other service. Migrations are jscodeshift scripts that can apply changes or upgrades to all our frontend services. For instance, a performance change that needs a library to be updated or some code edits, can be applied in an automated way to all services so that from engineers’ perspective, the change is not breaking. This talk by Andrew Hao explains Plugins and Migrations in more detail. Frontend Performance Force: A working group of engineers from different teams who want to level up Lyft’s web performance. This group identifies new performance areas to focus on, shares learnings, creates educational resources, and strives to build a culture that prioritizes performance at Lyft. The hierarchy of web performance needs presented here is a heuristic for prioritization based on our learnings at Lyft. A question one might ask is: Could the pyramid be inverted? I.e., prioritize building a shared performant infrastructure and tooling first, then load-time and run-time optimizations, followed by measurement and monitoring? One of the problems with not investing in measurement tools first is that prioritization would not be rooted in data, and it would be harder to justify business value. Moreover, it can be challenging in engineering resource-constrained organizations with lots of feature work on the roadmap to embed performance features in the shared infrastructure (if there is one). Further, one of the benefits of prioritizing individual run-time and load-time optimizations over performant infrastructure work is that an organization can learn which optimizations are important enough to be extracted into the infra layer. However, one caveat in the hierarchy we presented is that if a web application has a captive audience (eg. internal tools, enterprise software), then run-time optimizations can be prioritized higher than load-time optimizations. When performance is an afterthought, slowness can silently kill businesses . We’ve learned that performance needs to be thought about from the beginning, and a hierarchy of needs can help with prioritizing performance improvements that should be made. If you’re interested in working with us on building high performance software or solving other complex transportation challenges to create the world’s best transportation service, we’d love to hear from you! Visit www.lyft.com/careers to see our openings. We’d like to thank Andrew Hao and Eric Bidelman for providing ideas and proof reading this article, and Michael Rebello for editing it! A shoutout to Joshua Callendar, Ryan Jadhav, and Xiaotian Huo for providing some of the examples mentioned. Stories from Lyft Engineering. 251 Thanks to Eric Bidelman , Andrew Hao , Eric Smith , and Michael Rebello . Web Development Front End Development Engineering Lyft Web Performance 251 claps 251 Written by Inquisitive guy on a journey through the world of tech | Software Engineer at Lyft | UCLA’19 ’20 | mihirmathur.com Stories from Lyft Engineering. Written by Inquisitive guy on a journey through the world of tech | Software Engineer at Lyft | UCLA’19 ’20 | mihirmathur.com Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2021-04-28"},
{"website": "Lyft-Engineering", "title": "frontend at lyft an overview", "author": ["Fernando Augusto López Plascencia"], "link": "https://eng.lyft.com/frontend-at-lyft-an-overview-f934c1524370", "abstract": "Data Data Science Engineering Mobile Product Security At Lyft we have the enormous privilege of working with a product that impacts millions of people all around the US and Canada. Most people are familiar with Lyft through our apps, which are the main way our users give and request rides, rent a vehicle or take rides on bikes and scooters. There’s also the infrastructure that allows that to happen, mainly in the form of thousands of backend microservices. But did you know that there is a sizable portion of web frontend development as well? While it might come as a surprise, frontend development is an integral part of what makes Lyft possible. Most of our frontend microservices are built on a Typescript-heavy stack; these services, for example: Power lyft.com and affiliate websites, Provide services to our corporate partners, allowing them to manage their fleets through the Lyft Network (such as our partnership with Hertz, which allows you to rent a car to drive with Lyft ), Allow the management of internal resources through easy-to-use interfaces that add reliability to our operations, Empower our scientists to understand and improve how Lyft runs through data tooling, Show dashboards through custom UX / UI to support activities such as indicator monitoring, searches and decision making. All of the above need to be delivered with a high degree of quality. Frontend software development is a complex solution space, where a number of factors have to be taken into consideration, such as scalability, resiliency, consistent user experiences, heterogeneous client capabilities, realtime experiences, with code that runs on both server and browser environments. Accomplishing these goals will provide our riders and drivers with the best transportation experiences. To make things happen reliably, quickly, and at the same time provide an awesome developer experience, we invested in infrastructure tooling early on. When I joined Lyft, in mid-2016, we were no more than 25 frontend engineers. Our frontend infrastructure was simpler, too, but it was already evident that the infra needed to become more scalable. Thus, we started migrating from an Angular-based monolithic approach for all frontend internal tools to a truly distributed network of React-on-Node services to host those tools. The transition from having dozens to hundreds of microservices was made possible by the use of shared, centralized libraries, such as our build system package, called Frontend Build. This allowed us to create a new service from scratch on top of Express , with preconfigured defaults for Webpack , and connected to the Lyft microservice mesh through Envoy . This also helped us better support recommended technologies and standards, such as Redux for state management and Jest for testing. Another shared piece of code, in this case for styling, were a set of classes that followed the atomic css philosophy, allowing everyone to create “Lyft-looking” interfaces by composing a set of well documented classes. By this time, an internal portal to share React components was also created, using lerna and storybook . Each component becomes its own package in an internal namespace, which means each component is trivially easy to include in other frontend projects across Lyft. Frontend Build kept evolving as the frontend environment did. For example, when the time was right, we stopped endorsing Redux as a default, in favor of a React Context -heavy approach. Still, developers and teams truly owned the ever-increasing number of services they created. This meant we were able to provide the best experience for each particular case, whether it was about data exploration, data visualization, or support resolution for our drivers and passengers through specific flows. Just four years later we’re almost a hundred frontend developers, distributed in six offices across two continents. The distributed approach was right for Lyft, but has also introduced some challenges. For example, having teams working independently on their own services meant that each service could diverge greatly from others. Due to this, we were at a point where upgrading Frontend Build was a complex task and, in some cases, a real showstopper — and not doing it could expose us to a number of issues, including potential security risks. It was time for a change, one that would still support a wide variety of use cases, streamlining upgrades, yet not leaving the developer out of the driver’s seat. The solution came in 2020 with @lyft/service, a new infrastructure platform based on Next.js . Adopting Next.js means the infrastructure is, in many cases, automatically upgraded, but the developers and teams still get to retain control on configuration: most Lyft-specific integrations have migrated to a plugin system, and external packages are still supported. (For more on how this migration happened, read “Changing Lanes: How Lyft is Migrating 100+ Frontend Microservices to Next.js” ) In the meantime, our friends in the Design org, who were facing similar challenges, created the Lyft Product Language (LPL) design system, which now brings visual consistency to all our external mobile and web apps. Paired with this, in 2019 the first version of the LPL was created for frontend as a library called CoreUI. This has now replaced our old, atomic css classes. Being a port, CoreUI makes the implementation of interfaces much easier, since it’s based in the same language in which the designs were made. Also, CoreUI uses styled components for embedding CSS, which provides for much better modularity and smaller bundle sizes. Examples of what can be found in CoreUI are: typography, colors, inputs, buttons, dropdowns, icons, and layout components. Today, almost a hundred different frontend-specific microservices exist in the Lyft ecosystem, owned by a variety of teams. Engineering at Lyft is vertically integrated. Instead of having teams organized by work function (e. g. a Frontend team, a Backend team…), every team instead owns a portion of the Lyft product. For example, my team is called Pricing: we make ourselves responsible for providing accurate pricing for all rides, and the algorithms around that. This is a multidisciplinary effort, of which I, as a frontend developer, am also a part. With this organization, a challenge is to effectively be in touch with other frontend developers. Collaboration needs to happen just to make things work: for example, every pull request has to be reviewed by another frontend developer. In my case, as the sole frontend developer in my team, I require to build relationships across teams just to make my work happen. This is why, as a distributed organization, Frontend at Lyft puts special emphasis on community and communication. Here are just some of the ways we achieve this: We have a number of org-wide events, such as a monthly All-Hands meeting, where everybody — from the most senior members to the most junior ones — are encouraged to talk about what they care about. We have a shared calendar where everybody that does frontend can check out what events are happening. There’s a #frontend Slack channel for general questions, plus a number of specialized channels for inquiries about particular topics. Slack is a great way to get help fast, request peer reviews, look at tech issues others have encountered in the past, and so on. We hold informal events as well, such as a monthly meetup called “Frontend and Friends,” with the sole purpose of connecting with each other. The frontend organization also started its own mentoring program, which greatly helped reduce siloing and allowed individuals to support each other technically and professionally. This program has recently grown to include all of Engineering. An internal portal exists to document all of the generally accepted frontend standards at Lyft. Another one lists all of the frontend services and how they align with these standards. At the center of it all is our Frontend Working Group, a voluntarily-run organism that coordinates work to standardize and advance our technologies and internal standards, to advocate for the career and growth of frontend engineers, and to ensure we have a real community — a particularly tough challenge during the COVID pandemic. The Frontend Working Group has executive sponsors and representatives from across all of Lyft’s business lines. It consists of a central group and three grouplets (Tech, Career and Community), with each grouplet working independently on a number of work streams. The Tech grouplet works on topics such as accessibility, state management, performance, etc. The Career grouplet concerns itself with topics such as: upward mobility, education for calibrations, etc. Finally, the Community grouplet is responsible for initiatives and events that foster a sense of connectedness. Any frontend engineer at Lyft can be a part of the Frontend Working Group. Apart from the above, we have a dedicated Frontend Infrastructure team, which exists as part of the general Infrastructure vertical and whose sole mission is to support and improve experience of the frontend developer. The members of this team are responsible for the creation and maintenance of Frontend Build, @lyft/service and all of the other pieces of software around it, along with documentation, help systems and developer assistance. Having a dedicated team for this important work has been a key for having a quickly iterative improvement process on frontend tooling and the software development processes themselves in all of the organization. Frontend work at Lyft is important, diverse, and exciting. While our organization has become robust during the years, we are still very flexible and anybody that contributes can have the chance to create real impact. For a couple of examples: A couple of frontend engineers proposed recently the internal adoption of xstate as a state machine engine; this is now recognized as an optional part of our state management stack. A group of engineers is tackling low-hanging web performance fruit in our tech stack. Another group is leading an internal initiative to rebuild end-to-end testing using the Cypress framework. Another group of engineers is driving GraphQL adoption across the organization by building tooling and internal support. For a couple of public examples of our work, you can check out: the Lyft website , with info on all things Lyft; our ride web page , where you can book a ride now, no app needed; our open-source projects with frontend components, such as Clutch , our extensible platform infrastructure manager; also, projects formerly owned by Lyft and donated to the open-source community, such as Amundsen , a data discovery and metadata engine, or Flyte , a machine learning and data processing platform. We are always looking for more collaborators eager to build Lyft together. If you want to be a part of this story, please consider joining our team ! Thanks to Andrew Hao, Jamie Cohen, Alex Ung, and Sheng Hong Tan for their collaboration in this article. This article is also available in Spanish: eng-espanol.lyft.com Stories from Lyft Engineering. 303 Thanks to Lyft . Frontend Front End Development Software Development Interview Lyft 303 claps 303 Written by Stories from Lyft Engineering. Written by Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2021-03-10"},
{"website": "Lyft-Engineering", "title": "running atlantis at lyft", "author": ["Nish Krishnan"], "link": "https://eng.lyft.com/running-atlantis-at-lyft-b95c7fa51db1", "abstract": "Data Data Science Engineering Mobile Product Security By: Nish Krishnan and Sarvar Muminov For the past couple of years, Terraform has been gaining traction as an easy-to-use, well-supported and flexible provisioning tool of choice in the industry and as such, has also been gaining increasing popularity within Lyft. While we initially chose to forego adoption of Terraform as our general configuration management solution in favor of stateless configuration , updates to Terraform’s language constraints and featureset made us reconsider. As individual teams started using Terraform more and more, we found a number of centralized platforms starting to gain popularity at the company. There was a homegrown Jenkins solution that was difficult to maintain, providing only the benefits of a central version of terraform but lacking any of the security features that a critical platform should contain. There were a number of teams using a commercial product offering for Terraform automation and management, which was a great solution for its primary functions. However, this didn’t quite fit the needs of Lyft as whole so we chose to leverage Atlantis instead. Atlantis is an Open Source Terraform Automation platform designed to be run as part of a version control provider’s pull request (PR) workflow. It gives us: Workflow customization on a per-team basis, Centralized permissions and binaries Audit logs in the form of VCS pull requests Apply guardrails such as PR approvals and mergeability Opportunities to iterate and contribute features/bug fixes upstream At Lyft, we manage our own Kubernetes stack which we leveraged to run Atlantis as well. We run Atlantis as a statefulset singleton with a persistent volume containing Atlantis’ state — PR state, plan files, terraform binaries etc. Running this on a mature infrastructure stack gives us a number of things for free including, logging, stats, deployment setups in addition to network access to our other Lyft services when we need it. We run two ingress configurations for Atlantis: Internal only allows traffic from within our company’s VPN. This is used to load the static Atlantis app, providing details such as PR locks currently held and allowing force unlocks, global apply locks etc. Public only allows traffic on /events for VCS webhook integrations. This endpoint is secured using webhook secrets. Instead of using webhooks directly, Atlantis is registered as a Github Application , which can be installed to any repository in our organization. This allows us to onboard repositories to Atlantis easily, since permissions are managed at the app level This gives us a higher rate limit threshold than webhook configuration Since we also manage repository setups through code, we can enforce certain repository level settings for those with the Atlantis app installed. The one with the biggest benefit has been enforcing atlantis/apply as a required status check for Atlantis-managed repositories to prevent developers from merging changes before applying them. As a platform team, we had to strike a balance between providing flexibility to our customers and limiting maintenance overhead for special cases. The default approach is to use a managed configuration option that abstracts the Atlantis configuration file from developers by integrating with existing Lyft service configuration tools. For non-standard cases, we provide an option to bypass the managed approach and directly create the Atlantis configuration file. Service Manifests are used at Lyft, to define service-specific metadata such as on-call information, code repository, deploy pipeline configuration etc. Lyft’s reliance on manifests made it an easy choice to be used for infrastructure orchestration as well. This led to faster adoption due to familiarity, provided a standard for the underlying Atlantis configuration, and reduced the amount of work required to educate our customers. Using the managed approach, only necessary fields are surfaced to the customer which are used to generate repo level configuration using pre workflow hooks . We implemented pre-workflow hooks to function similarly to custom workflows in that a predefined script can be executed. However, there are several differences between them: Execution occurs before a workflow on each command Hooks can only be defined in the server side repo configuration Errors are non-blocking and are transparent to the end user A generated Atlantis configuration looks as follows: With this setup, customers don’t have to think about the various configuration options for a given Atlantis project. Autoplan is enforced to catch errors at the time a pull request is opened. Terraform versions are managed transparently, allowing non-invasive upgrades. Finally, a single workflow can be enforced across all projects, in our case one which provides a standard for Terraform backends. We enforce S3 as the backend in our managed Atlantis workflow. S3 provides a number of security features such as versioning , encryption and replication in addition to the access control benefits provided by AWS. We ensure that only Atlantis and Atlantis operators have access to these states for compliance purposes. We also use DynamoDB as a lock provider. On top of what Atlantis already provides at the pull request layer, this provides an additional blanket of security when doing out-of-band terraform operations (e.g., state manipulations). Given that we enforce a managed workflow by default, we are able to dynamically generate the backend for a given project by leveraging a combination of custom run steps and override files . Prior to running terraform init the managed_backend workflow runs the following script to create a backend override file: A couple things to point out: BASE_REPO_OWNER , BASE_REPO_NAME , and PROJECT_NAME are all provided to the run step by Atlantis TERRAFORM_STATES_LOCKS_TABLE , TERRAFORM_STATES_BUCKET are global environment variables we’ve defined in our image. There are a number of advantages to overriding the backend at runtime: We can conveniently swap the backends as necessary, We can enforce a schema for partitioning our data allowing for easier management and future automation Offloads the complexity of correctly defining a backend from developers. But the biggest benefit is, It just works. At Lyft, apply requirements are used to require that all pull requests are approved by another engineer and all of its status checks are passing. This ensures that we are meeting existing compliance standards defined for our code deployments. Although a PR review works well for detecting issues, automation is more scalable and can be used for managing more nuanced risks. We implemented a policy checking command to run a predefined set of policies against the plan output as an additional layer of security and safety. This enables the team, as operators, to enforce best practices and mitigate common security risks. Additionally, policies can be used to detect changes that might be harmful and require secondary approval from a domain-specific team. Policies are written using conftest , a wrapper around Open Policy Agent(OPA) , which makes it easy to evaluate structured files. Some of our use cases for this step include: Checking for any destructive changes to critical infrastructure(VPCs, DNS records, datastores, etc). Denying resource provisioning through unauthorized modules. Finally, in the event of a code freeze, or a bad Atlantis deploy, we added an API endpoint to lock all applies through the platform. This allows us to quickly and temporarily halt infrastructure provisioning without bringing down Atlantis. Lyft infrastructure gives us service level metrics by default, but we’ve added a number of application level metrics to Atlantis to track failures, successes and latencies across each command (Plan/Apply/Policy Check), and Github API calls. Metrics are emitted using gostats , a wrapper around statsd . In addition to metrics, we’ve added structured logging to Atlantis to replace its own logging implementation, a wrapper around the default go logging library. Structured logging is useful for a couple of use cases: Processing log files for analytics (e.g., aggregations) Querying logs across multiple dimensions in a consistent fashion This second point in particular, is what we were looking for. We wanted a central place we could look to see all the changes that Atlantis has made in the past X period of time. This is super useful when we are diagnosing on-going incidents. Additionally, we also need to be able to query this data e.g. filtering by repository, pull request number, project or any combination of the three. Since our logs are ingested by Elasticsearch and searchable using Kibana , defining structured logs allows us to use a flexible query DSL to search to our heart’s content. We’ve been running Atlantis with this setup for 6 months now. Since then, our team has contributed a number of features upstream, has become core maintainers of Atlantis and has scaled the platform to operate on 2000+ Terraform projects across the company. We have multiple teams contributing Terraform modules and conftest policies in an effort to streamline infrastructure orchestration around a pre-defined standard. Pre-workflow hooks — partially released in 0.16.0 , with full functionality in 0.17.0-beta Policy checking — released in 0.17.0-beta Structured logging — coming soon Global Apply lock — coming soon If you’d like to try out Atlantis for your team, you can download the latest release . Note: we are currently running our own fork of 0.17.0-beta in production . To read more about Atlantis check out: https://www.runatlantis.io . Interested in working with us? Check out our open roles today! Stories from Lyft Engineering. 190 2 Thanks to Lyft . Atlantis Terraform Engineering System Architecture Open Source Software 190 claps 190 2 Written by Senior Engineer @ Lyft Stories from Lyft Engineering. Written by Senior Engineer @ Lyft Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2021-03-30"},
{"website": "Lyft-Engineering", "title": "flyte joins lf ai data", "author": ["Anand Swaminathan"], "link": "https://eng.lyft.com/flyte-joins-lf-ai-data-48c9b4b60eec", "abstract": "Data Data Science Engineering Mobile Product Security A little over a year ago on January 7th, 2020 Lyft open sourced and announced Flyte — its core platform for orchestrating Machine Learning and Data Processing Jobs. Today we are elated to share that Flyte is joining the Linux Foundation AI & Data ( LF AI&Data ), as its 25th hosted project. 2020 has been a tough and long year for many organizations around the world, but at the same time it has been a year in which Flyte made its slow and steady march towards maturity. In this post, we will go over the history of Flyte and its journey to the foundation and how we believe it will continue to progress. Flyte was born in late 2016 out of the need to deliver models for a core product at Lyft — Estimated Time of Arrival (ETA). This product had solved problems unique to Lyft’s ETA needs…or so we thought: Large amounts of historical data had to be used to train a set of ensemble models. The output artifacts could be as complex as a generated map or a trained model. Backtesting a model and validating the hypothesis was a requirement. Once in production, retraining the model frequently was required. Multiple models were developed, tested, and deployed simultaneously. Our largest cause for slowdown was often the need to procure, manage and build new infrastructure, which may not be useful if the model did not work out. As we developed a solution for ETA, we quickly realized that other internal teams were interested in using a similar platform. This led to a new set of challenges and goals from the maintainers’ point of view like: Agile management and development of the platform. Ability to deploy upgrades and changes to the platform without affecting the users. Ability to monitor the platform and provide insights for the operators to troubleshoot. Clearly separating cases in which the platform failed vs the user failed. Providing a single hosted experience for the entire company — which meant built-in multi-tenancy. Providing elastic scalability, which would grow as the demand for the platform grew. In late 2017, we launched our v1 of Flyte at Lyft, using a cloud available scheduler (AWS Step Functions). This enabled us to make a dramatic impact on the organization and adoption grew exponentially. At the same time we started talking with other companies and realized that orchestration was a common problem. We also recognized that building such a platform would benefit greatly from a large vibrant community. As the usage of the initial version grew within Lyft, we started noticing problems with scale and usability. AWS Step Functions made it hard for us to add new features natively and we identified the need to build a container native scheduling engine. The learnings from our previous iterations resulted in Flyte. The team has been continuously and incrementally developing Flyte over the last few years and the solution was scalable and reliable enough to serve the needs of large organizations. As we felt that the product had matured, we open sourced it in early 2020 and began our journey into building an ambitious open source project. Since the beginning of our open source journey, Spotify has been a foundational partner for Flyte. Their involvement meant Flyte had to be portable across clouds right out of the box. Thus Flyte is designed to work across clouds and has been battle tested on AWS and GCP. As the events of the year 2020 unfolded, the Flyte core team was focused on ensuring that the total cost of running Flyte at Lyft was low and the open source onboarding experience suffered. By mid 2020, Flyte was powering more than 1 million pipelines at Lyft, across the ETA, Pricing, Mapping, Driver Engagement, Growth, and Map generation teams, and growing. Around this time, we received contributions from the open source community that dramatically improved Flyte. Freenome , a biotech company pioneering work in early cancer detection improved our “Getting Started” user experience. Spotify built JAVA/Scala SDK for Flyte which validated the true polyglot power of Flyte’s declarative, specification based model. In the first year of open sourcing we realized that one of the crucial components of Flyte was the python SDK — flytekit . Foundations of flytekit were built in early 2017 and since then the user interface remained almost consistent, with minor modifications, to ensure backwards compatibility for existing users. This interface was built pre-python-3 and was cumbersome to extend. By the end of 2020 the core-team completed a major revamp of flytekit — which introduced an innovative typing engine based on python3 type hints and made it possible to express complex ideas succinctly. Flytekit now works completely locally without needing a Flyte backend for most development scenarios and makes it possible to customize the DSL for your use cases. We encourage you to checkout flytekit and the tutorial of how to use, extend and leverage it in your projects. Flyte started with the vision of truly unifying systems, projects and workflows with an extremely intuitive and approachable design language. From the onset we realized that Flyte was an ambitious and enormous project that could only work with a vibrant, thriving and helpful community. We are very lucky to have amazing partners who are responsive and contribute back to Flyte. Going forward, we want Flyte to become the conduit for collaborating across myriad open source projects. To achieve this it is essential that Flyte be perceived as a truly open platform so that we can break the silos and truly help our users. This is sorely missing in the current open source landscape. To make this happen, Lyft decided to contribute Flyte to the Linux Foundation (AI & Data chapter). This we feel is a giant step forwards towards our goal of simplifying the life of all the hardworking ML and Data Engineers. With the solid foundation in place, we will accelerate the featureset and integrations in Flyte. This includes multiple projects that make Flyte incredibly useful for organizations, both small and large. Some of these projects include: Event Egress — Flyte now supports egressing events to your desired pub-sub channel. Platform builders and users can subscribe to these events and react to them. Flytekit for builders — Flytekit provides the necessary tools to write customized DSL’s, which can provide a more tailored experience for certain use-cases. Reactive pipelines — Flyte workflows can react to external data and events generated in other workflows. Performance and scalability improvements — Ability to run concurrent workflows with thousands of nodes, scalable to multiple clusters with minimal deviation from ideal performance. Improved UI and UX — Improved interface to visualize, track and compare experiments, executions and artifacts. Gitops using flytectl — Most user facing Flyte entities are completely versioned, but certain core entities, like projects, domains etc are not versioned. The Flyte community has been working on flytectl, which intends to be a simple CLI for interacting with Flyte and provide a gitOps interface for managing non-versioned administration entities. Integrations and more integrations — ML Ops observability tools, DataFrame scalers ( Vaex, etc), Modern compute frameworks (Flink, Ray), customized and specialized type extensions (TFData etc), data correctness and quality plugins (Pandera, Great expectations etc), interaction with SQL engines and more. Contributing Flyte to LF AI & Data is just the first step in the journey. We invite all of you to checkout Flyte at https://flyte.org , drop us a line or bring in new ideas for collaboration. Interested in working with us? Please see https://www.lyft.com/careers for openings. Stories from Lyft Engineering. 303 AI Machine Learning Data Processing Data Orchestration Kubernetes 303 claps 303 Written by SWE Lyft Stories from Lyft Engineering. Written by SWE Lyft Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2021-03-18"},
{"website": "Lyft-Engineering", "title": "ml feature serving infrastructure at lyft", "author": ["Vinay Kakade"], "link": "https://eng.lyft.com/ml-feature-serving-infrastructure-at-lyft-d30bf2d3c32a", "abstract": "Data Data Science Engineering Mobile Product Security By Vinay Kakade and Shiraz Zaman Machine Learning forms the backbone of the Lyft app. Some examples of ML at Lyft include deciding on the optimal way to match drivers and passengers, deciding how to price a ride, distributing coupons and incentives to riders and drivers, detecting fraud, route planning, and automating support. The ML models for these use-cases need features that are computed via batch jobs on the data warehouse or via event streams. Additionally, regardless of the way these features were computed, they need to be made available via batch queries for model training and via low-latency online inference. For example, consider a model (say, Cancels Model ) that predicts whether a particular ride could be canceled when a ride request is made in the Lyft app. The model may need the Cancels History of the requesting user over the last year as a feature, which would be computed only by running a batch job on the Hive data warehouse which has this historical data. Another feature for this model could be Estimated Price for Current Ride , which would be computed in real-time based on the user’s actions in the app via an event stream such as Kafka. Additionally, (a) historical values for both the features need to be available via batch queries for training the Cancels Model, and (b) current values for both the features need to be available as a point lookup for inference once the model is deployed. We built a service for ML applications at Lyft, unsurprisingly called the Feature Service, to make all features available for both training and low-latency online inference, regardless of whether they were computed via batch queries or via event streams. The Feature Service is a battle-tested mission-critical service that hosts several 1000s of features across a large number of models, serves millions of requests per minute with single-digit millisecond latency, and has 99.99%+ availability. We share its architecture in this article. The Feature Service consists of Feature Definitions , Feature Ingestion & Processing, and Retrieval . Given the familiarity of SQL amongst ML Practitioners at Lyft, we decided on SQL as the language for feature definitions. The SQL is expected to have one column designated to be an entity ID (which is an identifier for the business entity such as a driver, passenger, or ride), and the rest of the columns are features. The complexity of feature definitions ranges from querying a single table to a few 1000s of lines of SQL comprising complex joins across multiple tables along with transformations. Multiple features can be grouped together into a feature group (aka entity type ) for ease of managing a large number of features. Features are versioned, which is especially important as feature definitions go through several iterations during ML model development. We keep versioning at the feature level rather than at the group level so that feature iterations are quicker. In case the version is omitted in the feature definition, the default feature version is 1. In addition to the SQL feature definitions, we need users to provide metadata in JSON which consists of the feature group, feature name, version, owner, the data type of the feature, validation information, and operational information such as the team to alert when feature generation is broken or produces invalid data. An example of feature metadata for features gh6_no_show_rate and gh6_total_rides , with the entity type ride, is below. For features defined on batch data, we run scheduled feature extraction jobs using Flyte . The frequency of the run can be controlled per feature, and the job executes the SQL against Lyft’s Data Warehouse and then writes to the Feature Service. For features defined on streaming data, the ingestion job happens as a custom Flink job utilizing in-house technology to run SQL queries in the stream. The job executes SQL against a stream window and then writes to the Feature Service. The service supports GRPC and REST endpoints for writing and reading features. When the server receives an add or update request for a feature value, it validates the feature value against the feature metadata and then writes the feature value in DynamoDB, where each row corresponds to the most recent feature value for a particular feature. From DynamoDB, the feature values are replicated to Hive and Elasticsearch . We use Redis as a write-through cache for both feature values and feature metadata to increase read throughput, and update the cached feature values while writing new feature values as well. Additionally, given the nature of callers to the service which themselves could be distributed, we use DynamoDB conditional checks to implement optimistic locking, so that the write request with the latest timestamp wins in case of a conflict. The service supports reads via a batch-get call, where a user can ask for a number of feature values for given entities. This causes reads first from a Redis cache, and then from DynamoDB in case of a cache-miss. The read path is highly optimized for low latency and high throughput, with no locking involved. For model training, we need to read a large number of feature values together, say for the past year. In these cases, the replicated data to Hive is used. The features are stored as tables in Hive, which are then queried by the training process directly. Thus, all the four use-cases of features computed via batch jobs or event streams being made available to both training and online inference are satisfied as shown below: Online-Offline Parity: Note that in the above model, a feature is defined only once, and both training and serving systems use the same feature definitions, along with the same validations and transformations. Thus, feature definitions, validations, and transformations used for training and serving stay in sync. That said, given the replication lags involved, the feature values seen by training and serving systems are eventually consistent (and not strongly consistent) — with replication delay within tolerable limits of ML applications at Lyft. Modern ML applications compute features based on both batching and streaming data, and these features should be available in batch mode for model training as well as in point-lookup mode for online inference. In this article, we described the architecture of Lyft’s Feature Service, which makes features available both in batch and point-lookup mode regardless of how they were computed. Our Feature Service was built in Q4 2017, and it has since been widely adopted by a number of Lyft teams for a variety of ML models such as fraud detection, driver dispatch, location projections, growth platforms, pricing, customer support among many others. Acknowledgments Huge thanks to Willie Williams, Alex Jaffe, Nitin Aggarwal, Gregory Fee, Arup Malakar, Ravi Kiran Magham, Dev Tagare, Craig Martell, and Balaji Raghavan for making this work possible. Interested in working with us? Please see https://www.lyft.com/careers for openings. Stories from Lyft Engineering. 305 Thanks to Michael Rebello . Machine Learning Data Science Features Infrastructure Distributed Systems 305 claps 305 Written by Stories from Lyft Engineering. Written by Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2021-03-16"},
{"website": "Lyft-Engineering", "title": "announcing the mobile native foundation", "author": ["Keith Smiley"], "link": "https://eng.lyft.com/announcing-the-mobile-native-foundation-a289ec63b60a", "abstract": "Data Data Science Engineering Mobile Product Security Today, we’re excited to launch the Mobile Native Foundation . Backed by The Linux Foundation , along with Airbnb , Capital One , Corellium , Elotl , Flare.build , GitHub , GogoApps , Haystack , Line , LinkedIn , Microsoft , Peloton , Robinhood , Sauce Labs , Screenplay , Slack , Solid Software , Spotify , and Square , the Mobile Native Foundation provides a place to collaborate on improving technologies and processes related to shipping large-scale mobile apps. Like many companies, Lyft discovered that platform vendors did not solve all of the problems we faced as our mobile team grew from a dozen engineers to hundreds of active contributors. We needed guidelines, libraries, and tools to improve our developer experience and the quality of our apps. This vacuum has led companies to invest in huge projects, shipped in silos, duplicated across the industry. We have implemented custom UI frameworks, flexible architectures, replacement build systems, and new networking stacks countless times. This realization led us to found the Mobile Native Foundation. We hope to foster a diverse and inclusive community that encourages collaboration and builds innovative libraries and tools to move the industry forward. To achieve this goal, the Mobile Native Foundation, independent from any one company, hosts public discussions and open source projects related to a wide variety of issues companies face. To kick us off, Lyft is contributing multiple projects including Kronos , index-import , and set-simulator-location . We’re excited to see what other projects the community brings. Get involved to help improve mobile development for everyone! Stories from Lyft Engineering. 1K Mobile iOS Android Infrastructure Open Source 1K claps 1K Written by iOS Engineer at Lyft Stories from Lyft Engineering. Written by iOS Engineer at Lyft Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2021-03-02"},
{"website": "Lyft-Engineering", "title": "elasticsearch optimizations at lyft", "author": ["Stefan Zier"], "link": "https://eng.lyft.com/elasticsearch-optimizations-at-lyft-b555dc020932", "abstract": "Data Data Science Engineering Mobile Product Security By Stefan Zier and Vinay Kakade At Lyft, we use an in-house Feature Service to store batch and streaming features used by ML models, making them accessible in both offline mode (for training) and online mode (for inference). The service replicates these features in Elasticsearch to enable advanced queries. For Growth products, we use Elasticsearch to power internal tools like our Customer Data Platform, which enables teams to define user cohorts based on myriad features. Under the hood, these cohorts are translated to Elasticsearch queries for use in targeting millions of customers in marketing campaigns. Elasticsearch also plays an integral role in allowing teams to assess the current state of a user and their membership to specific cohorts so that product experiences can be enhanced with real-time decision making. In 2020, new Health & Safety and real-time initiatives proved our previous Elasticsearch SLAs challenging to maintain. With an influx of different use-cases every week, we quickly outgrew our existing cluster, latency increased, and AWS costs soared. In this article, we share the various ways we measured and altered our interactions with Elasticsearch to ultimately decrease latency and costs. Our Elasticsearch cluster utilizes a long-lived index , where a document’s ID is known before the time of indexing and any document updates are made directly to the document source. There are roughly 300 million documents in one index with close to 4000 fields in its mapping. For each Lyft user, we have one document where the document ID matches the user’s ID. A document’s field mapping will be made up of feature names and values (e.g., user’s ride count, current region, whether or not the user has incentive credit, etc.). To efficiently reach customers faster and more consistently, we first identified the types of Elasticsearch requests we were making and then started to performance test/tune our cluster: Elasticsearch Scroll API: Powers our marketing campaigns. The Scroll API provides us with paginated lists of users belonging to a specific cohort. Elasticsearch Count API : Used in determining “membership” to a specific cohort (ex. is UserA a San Francisco-based rider with at least 5 rides?). Additionally, the Count API helps determine the total number of users in a cohort. A scroll request will return a paginated list of documents, sorted by relevance , where each index contains the document ID (user’s ID) and all associated fields. In our case, we did not require sorting and only needed document IDs in the response, not the entire field mapping, to begin marketing campaign execution. Using Apache JMeter for performance testing on a sandbox cluster that mirrored our production index setup, we learned: If document order does not matter, sort by _doc . When possible, omit _source entirely or return only required fields with stored_fields . The following is a sample query used in performance testing. The query aims to aggregate unactivated riders with an expiring incentive that don’t have Lyft Pink: We observed the average latency for this query was ~2.7s. Then, we tested the query without returning all document fields by setting _source to False : Here, we observed latency dropped to ~1.2s. As expected, latency improved significantly with the reduction in response size, however, sorting with _doc in addition to disabling _source yielded even greater results: With these changes latency for Elasticsearch Scroll requests dropped to 307ms. If your documents in Elasticsearch are large, then assembling many of them into memory might take a significant amount of time. After omitting _source and sorting by _doc in production, we found scroll latency immediately improved from >1s to 100–300ms, on average. Thousands of times every second, the Elasticsearch Count API helps us determine a user’s membership to a specific cohort. In essence, we execute a query with a similar structure to the scroll request above however, we append a match_phrase clause containing a user’s ID. Given our cluster is organized such that there is one document per user ID, these count queries will return 0 (user is not in the cohort) or 1 (user is in the cohort). Using the same query from the Scroll API testing in the previous section, we would add a match_phrase clause in the body for the Count API like so: As described in Elasticsearch Scalability , each index of documents is split into shards and, per the Elastic Count API documentation, a single count query operation will be broadcasted across all shards. In our case, the cluster had 20 primary shards, meaning one count request would execute the query with 20 search operations. As previously mentioned, data in the cluster is stored on a long-lived index, meaning we are aware of the document’s ID before the time of indexing. Given our query above is only interested in a single document, there’s no reason to run the query on all shards. Instead, we can simply route the query to one shard where the document is located, costing us only 1 search operation. To avoid extraneous search operations, when possible: Use the _routing field so a query is resolved to one shard Use terminate_after so query execution stops early when you’ve accumulated enough documents you’re interested in. When testing the _routing field in JMeter, we did not notice an immediate latency improvement in a single query’s execution time however, we did see significant changes in reduced load when rolled out in the production cluster. Since count queries resolve to one shard now, search rate dropped and CPU utilization noticeably improved. Consider fetching only the fields you need and sorting by _doc (if possible) in Elasticsearch Scroll requests while making use of _routing and terminate_after in Elasticsearch Count requests. These simple changes yielded performance improvements that ultimately helped us reduce cluster resources while ensuring we maintain SLAs. Since Elasticsearch performance is largely based on a variety of factors (document size, search operation rate, document structure, index size, etc.), it is recommended you test with tools like JMeter to accurately measure performance and tune to your needs. Acknowledgments Huge thanks to the following team members for their work in making this possible: Janani Sundarrajan, Leon Verdin and CD Wad Interested in working with us? Check out our open roles , today! Stories from Lyft Engineering. 391 1 Lyft Elasticsearch Data Science Engineering Optimization 391 claps 391 1 Written by Stories from Lyft Engineering. Written by Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2021-02-23"},
{"website": "Lyft-Engineering", "title": "cdmx", "author": ["Chris Lambert"], "link": "https://eng.lyft.com/cdmx-c087b515cc6f", "abstract": "Data Data Science Engineering Mobile Product Security Lyft was founded in 2012 by Logan Green and John Zimmer to improve people’s lives with the world’s best transportation. Available to approximately 95 percent of the United States population as well as select cities in Canada, Lyft is committed to affecting positive change for our cities by promoting transportation equity through affordable rides, bikeshare systems, electric scooters, and public transit partnerships. Behind every successful Lyft product you’ll find the world’s best talent, bringing their unique experiences into the innovation process. As we grow our workforce, we put a great deal of care into where we expand our footprint, keeping our business and the local community in mind. That is why we are thrilled to announce the launch of Lyft’s very first Latin American office in one of the world’s most important cultural and financial centers: Mexico City. The new CDMX office will be another home for our engineering teams, tapping into the wealth of entrepreneurial and innovative talent from Mexico’s finest institutions. While we have no immediate plans to offer rideshare in Mexico, CDMX is well poised to be our primary hub for all of Latin America, where team members will be able to collaborate from similar time zones with colleagues across North and South America and enjoy direct flights to our offices around the world. What began as a small team of 5 talented Software Engineers in early 2020 is already blossoming into a world-class Engineering location that we are excited to expand over the coming years. We hired 48 team members in 2020, with plans to triple that number by the end of 2021. CDMX engineers are working hand-in-hand with the US-based teams and leaders to solve complex problems and bring more innovation to our product lines. Many have joined our Fleet team, with ambitious goals to help move Lyft toward an increasingly electric and autonomous vehicle future. Lyft’s Fleet business is responsible for the strategic investments that pave the road for Lyft to operate fleets of the future at scale and with hyper-efficiency. We are currently operating one of the largest vehicle fleets in the US, under our Express Drive and Lyft Rentals programs, as well as building a nationwide network of Vehicle Services. We are looking forward to growing our engineering footprint and supporting the business in other areas over the coming months, such as Rideshare, Mobile, Product Management, Design, and Data Science. We are so grateful to participate in the thriving technical ecosystem in Mexico City. We envision a world where cities are more accessible. Where transportation and tech bring people together, instead of apart. We see the future as community-driven — and it starts with you. If you’re attracted to Lyft’s mission of improving people’s lives through the world’s best transportation, we would love to have you consider joining our team! Este artículo también está en español: eng-espanol.lyft.com Stories from Lyft Engineering. 339 Thanks to Brian Salomaki . Cdmx Mexico City Engineering Office Engineering Ciudad De Mexico 339 claps 339 Written by Stories from Lyft Engineering. Written by Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2021-01-27"},
{"website": "Lyft-Engineering", "title": "gotchas of stream processing data skewness", "author": ["Rakesh Kumar"], "link": "https://eng.lyft.com/gotchas-of-stream-processing-data-skewness-cfba58eb45d4", "abstract": "Data Data Science Engineering Mobile Product Security Using stream processing as a data processing technology and the frameworks designed to support it has become ubiquitous in recent years, as it unlocks huge potential in building business-critical systems. Within the Marketplace team at Lyft, we use the Apache Beam ( Flink Runner ) streaming processing framework to power our feature engineering and model execution orchestration jobs. Over the past couple of years, we have built and scaled several pipelines to process realtime events at very low latencies. To share some stats, we generate ~100 features for ~3 million geohashes per minute (~400 billion features per day). To power some of our critical products that require primetime (it is dynamic pricing to control demand and supply in any given region) or identify hotspots, we redesigned our architecture to rely on event-driven systems to provide realtime insights. Through that journey, we ran into our fair share of obstacles trying to scale the platform. The process of identifying the root causes and solving such issues gave us insight into how to design a pipeline that scales seamlessly. We are starting this series to cover gotchas that everyone should be aware of while designing a data-intensive pipeline. Each post will cover unique types of issues and their potential solutions. For our first post, we are going to focus on the issue of Data Skewness and how it impacts the performance of the pipeline. It doesn’t matter how highly performant the distributed framework is, a pipeline’s efficiency and throughput are always challenged. Some signs of this issue are higher CPU and memory utilization on a few of the nodes even though the average resource utilization is way lower (figure 1). Such an imbalance can cause lower throughput and higher end-to-end latency. Pipelines can fail and enter into crash loops. In rare cases, one may notice an intermittent end-to-end latency fluctuation without any noticeable change in incoming event traffic (figure 2) . Identifying the root cause in such cases is tricky. Knowledge on shape and spread of data distribution can help initially, but adapting to the changing times will require having a rich set of tools. When data is at rest, like in a SQL system, it’s easier to slice and dice to help understand the causes of skewness. However, in stream-processing, data is in flux and thus one has to fall back to effective observability tools like metrics and logs. Most of our pipelines are based on Flink , hence why the section below is Flink heavy. However, one can find similar monitoring tools for their streaming engines or frameworks. Flink metrics There are a ton of metrics available around task and operator levels to help one find more information. Of these metrics, input and output watermarks and the number of records ingested by task/operator can provide more precise information to determine data skewness. The graph below (figure 3) is created based on Flink metrics. As one can see, there are a couple of tasks in certain nodes processing a much higher number of records as compared to other nodes. This indicates that there is skewness in data. CurrentInputWatermark and currentOutputWatermark metrics would also give a similar graph that confirms certain nodes are falling behind and causing backpressure. Flink dashboard The Flink dashboard provides rich information on velocity and volume of data at a subtask level for each operator. This is really helpful to quickly identify the issue. Below is one screenshot for reference (figure 4) . The top two subtasks are processing a significantly higher numbers of records than the others, and thus clearly the source of the skewness. The same pattern is evident from the number of bytes received by the subtasks. Custom tools The above toolsets are general-purpose tools and may not work at all times. In some special cases, one has to come up with custom tools for root cause analysis (RCA). In one instance, our Flink metric dashboard alerted us to higher latencies on a few tasks. Nothing looked suspicious from the metrics in the dashboard, which was intriguing. As they say, “Once a chance, twice a coincidence, and thrice a pattern.” After observing multiple alerts, we had to choose the route of extensive data logging to avoid a cardinality explosion of Flink metrics as part of RCA. We came up with an ad-hoc logger and logged event name, region, and relevant ids in a JSON format. We then pulled the log file from the specific task manager that caused the backpressure. We had to clean up the log and fetch only those specific to the time window in question. Next, we loaded it into PyNotebook for interactive data exploration. The Python DataFrames library helped us slice the data on various data dimensions (event name, region, etc). We realized that a specific event was noisy. In some special scenarios, a large number of these events were being routed to the same task causing data skewness and thus resulting in backpressure. We had to go back to the drawing board and redesign the pipeline to evenly distribute the data. As a ridesharing platform, we need to process data based on cities/regions for some of our products. Densely populated regions generate more data. We key the data using the city name in order to group the data that belongs to the same region. A couple of unfortunate nodes are always assigned some big regions, causing data skewness in the system. We check data distribution across the shards and identify such issues early in the data exploration phase. We also include this information in our tech-specs so that we can design the pipeline to handle it properly. If you have found the root cause, you have already won half the battle. To win the next half of the battle, you have to redesign the pipeline so that you can evenly redistribute the load across all tasks/operators. There is no silver bullet to solve this issue since every application and its data characteristics are different. However, few general good practices can have a huge impact. Reduce Noise to Signal Ratio The streaming engines orchestrate operator executions and data transfers from one node to another. Most of the time, data transfer takes a significant amount of time, especially when the pipeline processes billions of events per minute. Reducing the volume of data being transferred increases the efficiency of the pipeline. Ideally, we have the data validation, filters, and data trimming as the first step of the pipeline. This reduces the noise to signal ratio. We aggressively do this to reduce the overall data transfer in the system. In some cases, pipelines ingest a huge amount of data even though they are not relevant for processing. For example, input events might have many attribute fields, but only few of the attributes are used in the pipeline. In such cases, it’s ideal to aggressively drop unused attributes to reduce total data size, thereby reducing the overall data transfer within and across nodes. Coupling this with efficiently serializing the data over the wire using Protocol Buffers significantly reduces data volume (by 20% in our pipeline!) . Distributed logic Since pipelines are generally distributed, one can devise a way to distribute processing logic as well. In some of our pipelines, data is aggregated for a region. We distribute the logic in 2 stages. In the 1st stage, we partition the data based on geohash and aggregate it. In the 2nd stage, we repartition the data based on region and combine the result of all the geohashes in the given region (figure 5) . In this case, we distribute the heavy lifting work to more nodes and then combine the intermediate result. This allows us to eventually distribute the computation and also increase the overall throughput. There is a disadvantage to this approach: by shuffling the data twice we introduce the risk of more data transfer within the network, which can reduce the pipeline’s performance. This approach is beneficial for scenarios where there is heavy data processing involved per shard and shuffling of data has negligible overhead as compared to processing data. Salting Most of our feature generation and processing is region-based, and this creates a huge data skewness especially for regions with heavy vehicle traffic. We salt our keys based on geohash and process the data first. But in some cases, some geohashes are also hot and they themselves can cause data skewness. We use key salting to redistribute that data further. Once most of the processing is done, we remove the salt (geohash) and re-key the stream based on the region so that we can join the data downstream. For better results, one can also combine this technique with a distributed logic solution. As shown below, we are using random.choice to add salt to the key before processing the data (figure 6) . This evenly distributes the data. Once the processing is done, we remove the salt from the key for joining the data downstream. Salting the key with a geohash works best for our problem domain. Choose the salting mechanism carefully, and make sure that it evenly distributes the data. Reshuffle Streaming frameworks have GroupBy or CoGroupBy operations. Such operations require accumulating data with the same key on one partition so that they can join the data. This operation requires shuffling data that internally sends data across different nodes just to make sure that it ends up on the right partition. This operation is network heavy and causes an increase in latency. Generally, streaming systems (especially Flink) try to fuse different operators into one to avoid data shuffle. This is just a general optimization trick. While investigating a bottleneck issue, we noticed that some of the Kinesis shards were skewed and affected throughput. We realized Flink wasn’t fully utilizing the available resources because the upstream didn’t have a good partitioning key and Flink was combining different operators. So we added reshuffling to explicitly break fused operators and redistribute the data to underutilized nodes. This simple trick increased the throughput and decreased the end-to-end latency. Keep in mind that reshuffling is a double-edged sword that shouldn’t be used everywhere. Use it when it is absolutely necessary. Sometimes a pipeline may handle most of the traffic in sunny-day cases, but that does not guarantee that one won’t run into any data skewness issues in the future. Discovering such issues proactively can save you from big headaches, particularly when the pipeline is in production and the issue is discovered during peak load. To avoid this, design and run a load test against the pipeline before going live. This can surface issues and provide an opportunity to fix them before rolling out to production. Also, add observability for key components of the pipeline so that it can be monitored when it’s in production. We used learnings from various issues caused by data skewness to establish a better understanding of data that helped us design more robust pipelines. We discussed different techniques to identify root causes and potential solutions. Some of the general-purpose solutions include trimming events, salting the key, and shuffling the data. Stay tuned for the next installment in this series, where we’ll discuss the deployment orchestration of streaming pipelines. As always, Lyft is hiring! If you’re passionate about developing state of the art machine learning models and building the infrastructure that powers them, join our team . Stories from Lyft Engineering. 396 Thanks to Eric Smith , maghamravi , Lyft , Prem Santosh , and Michael Rebello . Apache Beam Data Processing Streaming Pipeline Apache Flink 396 claps 396 Written by Hi, I’m Rakesh, Staff Software Engineer with extensive experience in highly distributed & scalable systems. Stories from Lyft Engineering. Written by Hi, I’m Rakesh, Staff Software Engineer with extensive experience in highly distributed & scalable systems. Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2021-02-16"},
{"website": "Lyft-Engineering", "title": "how we foster mask wearing with our health safety policy", "author": ["Preet Anand"], "link": "https://eng.lyft.com/how-we-foster-mask-wearing-with-our-health-safety-policy-decf74188ce8", "abstract": "Data Data Science Engineering Mobile Product Security In June, we introduced our Health Safety program to set a policy that reinforced public health best practices. We continue to do more work in this area to help keep our community safe and fight COVID-19. In this post, we’d like to share with you how we have approached maximizing compliance on our platform. The purpose of a policy is to set clear rules and expectations. The goal is maximum compliance with the smallest amount of additional effort exerted from the community. In the case of Health Safety, successful compliance with our policy means that people can transport themselves more safely during the pandemic and protect their communities. These are the steps we took to establish our policy and increase compliance: Clearly stated the policy and required affirmative acknowledgement. Frequently reminded all users of the policy. Validated compliance via user reports and checks. Provided corrective feedback for those who didn’t comply. Penalized those who repeatedly failed to comply with our policies. The first two are our educational efforts and the last three are part of our compliance program. Each step is critical because it informs the community, gets their buy in, and makes it clear that this policy is enforced. Unsurprisingly, we have seen that enforcing the policy reduces repeat offenses in a meaningful way. This graphic lays out how we have brought these steps to life. Overall, our mask compliance is significantly above the national average. We are always improving, but we have a strong baseline. Our approach is informed by the following principles: Most people will comply with sensible a policy if it’s clear. Repetition facilitates memory. Memory facilitates compliance. We need to solve for those who won’t comply to ensure trust in the policy. Compliance can be motivated via deterrence. Deterrence is based on the perception of possible punitive actions. There needs to be actual punitive actions, both to validate perception and to minimize repeat violations of the policy. While principles #1 and #2 inform the strategy for most people (education), #3–6 are why we have checks, feedback, and punitive enforcement. We would like to penalize as few people as possible, and when we have to, to do so fairly*. This is the inverse of a growth funnel — we hope to reach compliance without having to get to the punitive step. However, in line with our goal of prioritizing the safety of our entire Lyft community, we have to penalize bad behavior in order to create deterrence and to remove those who won’t ever comply with our policies. Safety is critical to healthy communities, and now more than ever. We hope that sharing our approach inspires ideas to help other communities as well. *The fairness aspect is important, and we make sure to build our systems to minimize any fraudulent reports. Stories from Lyft Engineering. 142 Thanks to Michael Rebello . Covid 19 Crisis Public Health Lyft Health Safety 142 claps 142 Written by My life mission is to perpetuate free will. Helped make 911 smarter (6k+ lives saved) and Lyft even safer (20M+ people protected). President of Snug. Stories from Lyft Engineering. Written by My life mission is to perpetuate free will. Helped make 911 smarter (6k+ lives saved) and Lyft even safer (20M+ people protected). President of Snug. Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2021-02-03"},
{"website": "Lyft-Engineering", "title": "how lyft discovered openstreetmap is the freshest map for rideshare", "author": ["Clare Corthell"], "link": "https://eng.lyft.com/how-lyft-discovered-openstreetmap-is-the-freshest-map-for-rideshare-a7a41bf92ec", "abstract": "Data Data Science Engineering Mobile Product Security Mark Huberty & Clare Corthell, Lyft Mapping Lyft Mapping study shows crucial OpenStreetMap road attributes are fresh and high quality in 30 North American cities, as compared to groundtruth. L yft moves people — from home to work, work to play, play to rest, through cities and beyond . Maps play a critical role, helping Lyft figure out where drivers and riders are, how best to connect them, and estimate how long it will take to get to the destination. Lyft Mapping is built on top of OpenStreetMap . This global map database is used by millions of people around the world, for combatting climate change , tracking agricultural land use , disaster recovery , refugee response , academic research, and much more. After 16 years of growth, OSM is now commonly used by many companies to power applications like logistics platforms, social media , and gaming . OSM is now the biggest crowdsourced repository of human geospatial knowledge. But is this map suitable for supporting the rideshare experience? Is it the best option available? Can Lyft support the OSM community and contribute to making the map better? Though we had a strong intuition that OSM offered a complete road network, we didn’t know how well the map matched the real world — so we ran a study. After three months, thousands of miles driven, and a lot of skilled work from our data curation team, we’re happy to report positive findings: OpenStreetMap has a very high-quality road network in 30 large North American cities. The OpenStreetMap community can be credited with maintaining the map at a reliably fresh standard in these areas. This survey design is potentially valuable for any study of map quality relative to groundtruth. In this post, we’ll review how we did this (see the paper ), the findings, and takeaways for Lyft and the mapping community. Measuring the entire map — whether a road exists, is annotated correctly, is up-to-date — is simply hard. Lyft operates in over 300 markets in the United States and Canada, from dense, old urban areas like New York City, to sprawling suburban metropolises like Phoenix or Los Angeles. To get the ground truth required to assess map quality, we might send a surveyor to every intersection and record whether something is correct. The US Census Bureau does exactly this — in fact, it hires so many people every 10 years that it singlehandedly changes the unemployment rate . But this approach could be incredibly slow and expensive. We needed a survey design that balanced our desire for regional specificity with cost and logistical feasibility. We knew that we needed a sampling-based approach to make this tractable. But a pure random sample wouldn’t work; sending people to randomly-sampled intersections around a city would be incredibly time consuming. Instead, we looked to public health and remote sensing for a solution. Health researchers often face a similar problem of how to send a limited number of survey workers to homes for evaluation of disease prevalence or health outcomes. They solve this with cluster sampling : Sample spatial units , such as a city block Sample households from that block This two-stage process simplifies life for survey workers. A survey taker can go to one city block and visit multiple households at a time, maximizing the information they get from one trip. This sort of survey design isn’t as statistically efficient as a pure random sample, but what it lacks in purity it makes up for in logistical simplicity and cost effectiveness . We mimicked this methodology and added a twist: remote sensing . Rather than sending surveyors in person, we partnered with Mapillary (now part of Facebook) to collect high-quality imagery from our spatial samples. A team of Lyft Map Data Curators then used these images to study whether OSM matched the real world. With the curator-reviewed map in hand, standard statistical techniques gave us our answer — based on the sample, how good was OSM quality, and how much did it vary by region? The survey package for the R statistical programming language provided all the tools to estimate nationwide and regional quality based on our sampling design. Detailed estimates follow at the end of this post, and in our public paper . This process of sampling, rapid imagery collection, and curation allowed us to study all of North America in three months , completed in March 2020. We found that core features of OpenStreetMap roads are correct more than 95% of the time relative to what exists in the real world. Data critical to safe navigation, such as left turn restrictions, are correct more than 85% of the time. Nationwide, these estimates are precise to within 5% sampling uncertainty. The regional uncertainty varies more based on region-level dynamics, visible in the figures at the end of this post. As is said in Mapping, perfection is unattainable; the map goes out of date the moment it is published , because the real world is always changing. But as of March 2020, OSM map data showed only minor differences from the real world. *See full list of cities in data plots below These findings are encouraging, because they show that Lyft is running on a map that accurately represents reality. This gives us greater confidence that we typically won’t predict a Lyft route with an illegal turn, or driving the wrong way down a street. At the same time, it narrows in on areas of opportunity. OpenStreetMap is in maintenance or “ Gardening ” phase of map curation for many features and geographies. This means that issues arising are usually recent changes to the real world, rather than gaps in the extent of map. Discrepancies with groundtruth are often due to recent changes rather than longstanding unmapped features. This study shows 30 cities in maintenance mode for the features noted. Areas for investment remain including onramp signage and lane annotation in specific cities. This is useful information to help mappers, including the Lyft Mapping Curation Team, direct their efforts to improving the map for everyone who uses OpenStreetMap. Studies like this help us narrow focus and build stronger programs for finding errors. Sample-based field surveys are both fast and less costly than a groundtruth census. The approach of sampling + remote sensing could be useful for further studies on different tags, geographies, or use cases. We hope others will be encouraged to use these methods. Sensor networks combining imagery (like Mapillary) and telemetry (fleets like Lyft) can improve OpenStreetMap , in safe and anonymized ways, by surfacing errors and changes. For example, see How Lyft Creates Hyper-Accurate Maps from Open-Source Maps and Real-Time Data . As mappers know well, sensors are a valuable tool for everyone to use to improve maps — and their accelerating ubiquity is making improvement very accessible. Keeping the map fresh and up-to-date is a matter of finding the needles in the haystack . Evidence beyond this study (such as the title image) show that the community is tracking construction, natural disaster impacts, and new changes (like new bike lanes !) and mapping them as they happen. These findings are valuable in themselves — they tell us that Lyft runs on a great map. As OpenStreetMap has demonstrated, we should never underestimate the potential of crowd collaboration. With every edit, addition, modification, and discussion, the community of editors and organizations have created a complete map of North American cities in OpenStreetMap. As Lyft Mapping continues research on how to continue collaborative contribution to OpenStreetMap, we’ll look further afield beyond these cities — stay tuned! If you want to be a part of fine-tuning our complex mapping systems, join us ! Visuals below are fully documented in the paper: bit.ly/lyftosmqualitystudy Turn restrictions matter for ride share for two simple reasons: we don’t want drivers making illegal turns, and we don’t want to expose passengers to unsafe routes. Unlike roads, we can’t see them in satellite imagery, nor find them in municipal road data released by most cities. Mapillary’s ground-level imagery proved critical to understanding OSM completeness here. Ride Safely! Stories from Lyft Engineering. 870 2 Mapping Technology Rideshare Lyft Data Science 870 claps 870 2 Written by The Arctic, Maps, Data. @clarecorthell Stories from Lyft Engineering. Written by The Arctic, Maps, Data. @clarecorthell Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2021-02-09"},
{"website": "Lyft-Engineering", "title": "pulse stats apis from envoy mobile", "author": ["Jingwei Hao"], "link": "https://eng.lyft.com/pulse-stats-apis-from-envoy-mobile-6db71c6a2f22", "abstract": "Data Data Science Engineering Mobile Product Security Early this year, we published a deep dive after the v0.2 release of Envoy Mobile. A lot has happened since then: Envoy Mobile is now enabled in the production Lyft apps and the open source project recently joined the CNCF . Today, we are excited to announce Pulse, Envoy Mobile’s stats solution. In February, we published a blog post detailing how Envoy Mobile enabled a new range of end-to-end observability of our network . Pulse is a continuation of this work. Real-time observability is essential for server-side development. There are existing industry standard open source solutions, such as Prometheus and StatsD. Service owners are accustomed to instrumenting and monitoring their services with various metric types like counters, gauges, and histograms. In comparison, on mobile, observability has conventionally focused on crash reporting and event tracking: Crash reporting : Apps typically use third party reporting tools focused on crashes and exceptions, such as Crashlytics or Bugsnag. Pros: These tools usually report events with relatively low latency (usually minutes-level). Cons: The reporting is only focused on crashes and exceptions. Additionally, these tools are usually not configurable or extensible. For instance, it’s difficult to integrate them with alarm systems like PagerDuty. Event tracking : Many apps use in-house or third party systems to gather analytics events like user interactions, diagnostic data, etc. for different purposes. Pros: compared with the crash reporting systems, the analytics events systems usually allow for custom structured data. This affords performing ad-hoc queries to gather insights about specific features of the apps. Cons: data usage is relatively high (compared with crash reporting). Therefore, data reporting resolution is usually in the longer-than-minutes range. In order to reduce the time for anomaly detection and to save developers’ time on triaging issues in different areas, Pulse provides a set of easy to use APIs for mobile engineers to report time-series data (think data points) from their apps . These time-series then get populated into observability systems and are used to render real-time stats. At Lyft, we recently started integrating real-time stats into our clients. Using these stats, we were able to build a dashboard that tracks app crashes and enable a corresponding set of PagerDuty alerts. Previously, engineers relied solely on the Bugsnag UI and fished for crashes for specific releases. It was manual and time consuming. Time to root cause issues was delayed by our inability to know immediately when our apps were not behaving as expected. Now, when the app crash metric spikes, our mobile on-call engineers get paged and immediately start acting on the incident. A few weeks ago, the app crash metric spiked for Lyft’s rider app. Thanks to an alert driven by these stats, the on-call engineers were paged immediately. The engineers were able to act on the issue quickly and merged a hot-fix for the incident, and were able to minimize the potential impact. Another valuable aspect of real-time stats is monitoring specific areas of the Lyft apps. Pulse’s stats APIs are flexible and can be applied to any area of the codebase. For example, if the count of taps on the “Request a Ride” button (a relevant interaction for the Lyft rider app) suddenly spikes or drops, it likely suggests a problem worth investigating. Currently, Pulse supports two types of stats: Counter and Gauge. Counter: a value that can be incremented. Many mobile apps use analytics for tracking occurrences of an event. Conceptually, this practice acts as a counter. Taking the example from the previous section, traditionally when a user tapped the “Request a Ride” button, an analytic event was emitted. With Pulse, the app instead has a “request_ride” counter that is incremented every time when the user taps the button. Gauge: a value that can be incremented or decremented. Gauge stats are less common in mobile development compared to Counters. A simple example is a gauge to report the amount of network connections in flight for an app at a given time. The amount of network connections in flight is a useful metric to observe in real-time: when there is an anomaly (a sudden spike or a drop) for this metric, it most likely means there is something wrong. There is a third type of stats we plan to support, it’s Histogram. Histogram: a histogram samples observations and counts them in configurable buckets. For example, reporting request durations or time-to-interact on fresh app starts. If you want to read more about Pulse’s APIs and how to use them, check the documentation , along with some example apps for both iOS and Android. Client stats are received server-side by a gRPC service based on StatsD . The service expects a list of time-series stats serialized as a standard Prometheus MetricFamily . The service then flushes the stats to Lyft’s internal observability systems, where they’re used to populate dashboards and alarms. More details on the full end-to-end system are described in a previous observability blog post . We are currently working on adding support for Histogram stats to Pulse’s APIs, and are also adding the ability to tag specific stats. With tagging, developers will be able to enrich their stats with different dimensions of information (for example, by adding app_foreground or app_background tags to the app_crash stats mentioned above, providing more insight into the conditions during which the app crashes happen). Our goal is to make time-series metrics not only a tool for mobile development, but a necessity: it should become second nature for development teams to consider what stats they want to monitor on their mobile clients to track performance. We’ve already begun piloting Pulse at Lyft with other feature teams, and are excited to share more of our discoveries as we make progress on Pulse. In the meantime, feel free to try it out , contribute , or review our roadmap ! We would like to thank the following teammates for their work and contributions: Michael Schore, Jose Nino, Alan Chiu, Michael Rebello, Rafal Augustyniak, Don Yu, and Miguel Juarez Stories from Lyft Engineering. 183 Thanks to Miguel Juárez , Don Yu , and Michael Rebello . Stats Envoy Proxy Mobile Logging Networking 183 claps 183 Written by Stories from Lyft Engineering. Written by Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-12-19"},
{"website": "Lyft-Engineering", "title": "changing lanes how lyft is migrating 100 frontend microservices to next js", "author": ["Andrew Hao"], "link": "https://eng.lyft.com/changing-lanes-how-lyft-is-migrating-100-frontend-microservices-to-next-js-42199aaebd5f", "abstract": "Data Data Science Engineering Mobile Product Security By Josh Callender and Andrew Hao In 2019, Lyft’s frontend architecture needed a reckoning. We were growing quickly as a company, and new teams were creating new software systems daily. At that point in time, we were generating new frontend services from a service generator template — complete with a copy of our bespoke, zero-config frontend build platform. Having such an easy means of service creation led to an explosion of new services with heterogeneous code built upon our React-based frontend architecture. At the same time, we were running into headwinds trying to maintain our own frontend platform — an internal set of Webpack configurations, ESLint libraries and framework code — and finding ourselves bogged down troubleshooting cryptic build errors and generally finding our productivity sapped by such support requests. Because codebases began to diverge (as they do in microservice architectures), our developers found the task of upgrading to new versions of our frontend platform to be time-intensive and frustrating. With over 100 frontend services and nearly as many frontend engineers, it was clear something needed to be done in order to ensure that our platform was maintainable for Lyft’s growth. We sat down and named some of the core issues we were facing: Drifting infrastructure: New platform releases did not see uniform uptake, leaving us with a long tail of services that were left on older platform versions. With time, our frontend infrastructure began to drift, leading to maintainability and code complexity issues. Keeping the entire service fleet up to date is hard : The responsibility of upgrading each service fell upon our product engineering teams, who were often busy and overbooked. This led to services falling behind on security and performance updates. Proliferation (and divergence) of infrastructure code : Each service implemented frontend infrastructure (like Redux, or server-side rendering) in its own special way according to its own needs and team preferences, leading to heterogeneous implementation of common app patterns. Performance bottlenecks: As new technologies like dynamic imports and other bundle size optimizations became available, frontend services that had not been upgraded to our latest platform began to lose out on performance wins offered by newer platform updates. Common tasks are hard to apply at scale : Tasks that would normally be simple were difficult to apply at scale. For example, if we wanted to introduce styled-components to our service bundles, we would need to manually go into each service and add it in its own special way for each service’s implementation approach. Lack of standardization : Sharing code is very difficult due to our heterogeneous codebases. Our engineers must often reinvent the wheel when implementing patterns and modules instead of leveraging shared code and libraries. We made the decision to turn to the open-source community to find a batteries-included framework that would solve these headaches for us. After evaluating different platforms, we landed on Next.js ! We liked: Its batteries-included, opinionated philosophy would help unify the divergent architectures of our platform. Its executable wrapper that allowed us to move all central application concerns behind a module interface and remove our need to maintain our own build system architecture. Its strong open-source ecosystem, friendly community, and solid documentation really sold us on the future growth and trajectory of the platform. We could have just pulled Next.js off the shelf and asked everyone to use it as-is, but we still needed to solve a couple more problems. Two problems remained unsolved with Next.js out of the box. First, we needed to automate platform migrations for the future. We needed the ability to write easy-to-run and bulletproof service upgrades, and to be able to apply these at scale. To solve this, we designed a migration service with jscodeshift that allows us to ship and run migrations that automatically update service code when upgrades are run. This means that any future breaking changes in our platform will come with automatic codemods that upgrade code in the host app. This also means that we can open pull requests to upgrade services across the fleet without product engineering intervention. We needed a way to code share. We wanted to build an extensible application architecture that would allow developers to write plugins to introduce different state managers and packages with as little configuration or glue code as possible. We designed a plugin service around Webpack Tapable that would allow any of our developers to inject shared Lyft packages into server middleware and the client React app to accomplish different tasks in our ecosystem — from GraphQL clients, Mirage mocking support, UI component libraries to shared libraries around metrics and logging. One doesn’t just go and upgrade 100 services on their own — we needed to validate and understand the pain points from our product engineering teams before we committed to our design. We interviewed engineers from around the company to learn about their challenges and pain points with the current platform and collect feedback about whether our new design would solve them. We kept teams updated with the progress of the new stack in our internal frontend guild all-hands meetings. The entire process was transparent and developer-centric from start to finish. We named our new platform @lyft/service and identified a small group of developers that would be involved in alpha testing platform milestone releases. As our platform continued to mature, we expanded this audience to larger groups of teams that gathered for half-day migration workshops. Doing these sessions really helped us to build a community where teams worked together to learn the Next.js architecture, help each other fix issues, and understand more context to why we made the design decisions we did. We ran into a few roadblocks that emerged only after we began migrating services in beta sessions. For example, we had assumed that it was possible to migrate all our applications from React Router to the default Next.js filesystem-based router . However, due to the very specific ways that React Router was implemented in our services, we found it was nearly impossible to easily codemod these routes. Instead of migrating away from React-Router, we built a feature that allowed our engineers to preserve their existing React Router router and migrate to the Next.js router one route at a time. A migration to @lyft/service is incredibly easy to run. A service owner simply invokes one command: $ npx lyftsrv upgrade And our codemods go to work and safely upgrade code in-place. Once that’s done, most of the heavy lifting is complete! Of course, loose ends must also be addressed by each service owner, like: Fixing unit tests Integrating with the new Next.js router (or using our implementation of React Router) Upgrade related packages that may need manual intervention (like usage of mobx or Redux). On average, the work needed from running the migration scripts to tying up loose ends takes a matter of days. Today, @lyft/service runs nearly 40% of our frontend fleet, and we’re accelerating its adoption quickly. We have seen incredible feedback for this new platform, including the following wins: Reduced the dev feedback loop (time from code change to browser update) by 350ms . Removed 845kb of bundle size (in our boilerplate app). Removed 10,000 lines of infrastructure code from each service. Migrating to this new platform will continue paying off in the future, as: New upgrades are as simple as NPM module upgrading @lyft/service and running a migration CLI command. Because infrastructure code is fully encapsulated behind a package (and a suite of plugins), migrations require far less surface area than they did in the past. Migrations can be done automatically with automatically-opened PRs across the fleet, requiring far less product team intervention, and all services can receive the latest and greatest that the Next.js community has to offer. For more details about our migration, watch Josh’s talk on our migration process at Next.js Conf 2020! If you’re interested in working with us on the next-generation stack (pun intended) we’ve outlined here, solving complex transportation challenges to create the world’s best transportation service, we’d love to hear from you! Visit www.lyft.com/careers to see our openings. We would like to thank the following teammates for their work and contributions to our new platform: Daniel Kempner, Elad Ossadon, Derek Louie, Dustin Savery, Guy Thomas, Kim Truong, Jordan Patton, Jose Padilla, Martín Conte Mac Donell, Shekhar Khedekar, Adam Derewecki, Andrew Oh, Ryan Jadhav, Evan Madow, Derek Schaller, David Andrus, Beto Dealmeida, Ashley Yiu, Moon Jang, Alex Ung, Alexandre Smirnov, and Marcos Iglesias Stories from Lyft Engineering. 782 Thanks to Michael Rebello . React Nextjs Lyft Frontend Microservices 782 claps 782 Written by Engineering @lyft, @carbonfive alumni, co-founder @Wejoinin. Trail runner and coffee lover. Stories from Lyft Engineering. Written by Engineering @lyft, @carbonfive alumni, co-founder @Wejoinin. Trail runner and coffee lover. Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-11-30"},
{"website": "Lyft-Engineering", "title": "building a gateway to flyte", "author": ["Katrina Rogan"], "link": "https://eng.lyft.com/building-a-gateway-to-flyte-474b451b32c8", "abstract": "Data Data Science Engineering Mobile Product Security By: Katrina Rogan & Ketan Umare At the beginning of the year we introduced Flyte , a production-grade, orchestration platform for data and machine learning workflows. Today Flyte has grown to host more than 9,000 workflows, over 20,000 tasks and over a million workflow executions per month on the platform, while serving 400+ users (at Lyft). This post dives deep into one of the most critical components of Flyte which has made this possible. Flyte was built at Lyft across multiple iterations and was built from the start to leverage existing open source systems. However, we realized there was no single open source project that met our requirements. Some features that were important to us included, First-class data awareness and lineage tracking Centralized, hosted and multi-tenant platform for the entire company Ability to run disparate workloads written using varied frameworks, that would evolve with the users needs Ability to efficiently maintain, extend and administer Flyte for all our users. Most importantly, we valued: agile development, continuous deployment and a low maintenance overhead for our small team. This was made possible by using a Cloud-native architecture and designing the control-plane separately from the data-plane. Flyte Admin is the central stateless service that is the control plane of, and gateway to, Flyte. This post will dive into the design goals behind Flyte Admin, an architecture overview, and provide a couple examples of how Flyte Admin has made Flyte extremely adaptable within Lyft. One API : Flyte Admin hosts the inventory of all the business logic in the company. As a whole, Flyte should be accessible to all users through various modalities like the Console (UI), CLI, programmatic executions and set the foundation for building other differentiated platforms on top of Flyte Decoupled execution and elastic scalability : We wanted to decouple the data-plane that executes Flyte workflows and tasks from the management and observability of these executions. We also wanted to make sure that we could easily add increased capacity, capabilities and clusters without any downtime for our users. Central management of policies, quotas and resources : It was important that any multi-tenant platform we designed provided a mechanism for managing users including generic whitelist/blacklist mechanisms and infrastructure provisioning in one, centralized service. Retrieve results and debugging information : Flyte handles reliability and infrastructure but users still need tools to iterate on their code. Flyte Admin is the easily accessible store for all execution-related data. Extensive visibility and observability : Flyte Admin also powers additional tools for developers on Flyte. These include workflow execution notifications, tracking execution progress in real-time and monitoring scheduled executions. Flyte Admin is implemented as a stateless, simple gRPC service, which also surfaces a REST api using the grpc-gateway generator. The service is built with the excellent gorm ORM to leverage any relational datastore. Database access patterns are optimized to reduce unnecessary joins and large transactions. Flyte Admin serves as the communications hub for Flyte. It communicates with third-party cloud tools for asynchronous scheduling and notifications, generic blob storage and storing artifacts in a persistent database. Key, implementation features include: Persistent, data storage : User data refers to tasks, workflows and their executions . Durably storing this data and exposing it through the single API allows our users to discover and share task definitions and re-use them. Reliability and performance : If an issue affects the data plane, execution progress may be halted but users can still retrieve data artifacts and queue executions for launch. Likewise if a single, rogue user accidentally hammers the Flyte Admin service this does not affect in-progress executions belonging to other users. Centralized resource management : Flyte Admin ships with the ability to configure a variety of workflow registration and execution parameters. All of these attributes are based on a hierarchical set of tags including user projects and workflows. These logical configurations are separate from those of the data plane. This allows Flyte Admin to operate as a single brain, at once aware of its users and capable of modifying the Flyte deployment around them. Authentication : To track the requests to set configuration parameters and other API calls, Flyte Admin ships with support for OpenID Connect authentication and audits all user actions. Flyte Admin makes it possible to quickly extend the capacity of Flyte by horizontally scaling at a Kubernetes cluster level. This has helped us with: Uptime : We can isolate multiple use cases by criticality. By supporting tiered service levels, we can continue to deploy and enhance Flyte, without affecting the quality of the service for critical use-cases Seamless, runtime updates : Flyte Admin makes it possible to dynamically and transparently migrate our users onto different execution settings. At Lyft we’ve scaled Flyte executions across multiple kubernetes clusters, some of which are dedicated to latency-sensitive clients. Execution placement across these clusters is easily managed by configuration exposed in the Flyte Admin API. Cluster bootstrapping : Adding execution clusters is simple. Flyte Admin creates and syncs common configuration with various Flyte clusters out of the box so that per-user namespaces are configured with no manual intervention. This includes configuring secrets, quotas, service roles and more. The simple templating system allows Flyte administrators to configure their deployment as they need. Single source of truth : Admin is the centralized repository for execution data. There’s no need for users to hunt down which cluster an execution ran on or where logs have been uploaded. Resource arbitrator : Flyte Admin acts as the centralized gatekeeper for features and resources. This includes quotas to throttle large workloads and tunable options to block unauthenticated workflows from executing. Flyte continues to evolve to support not only Lyft but our partners too. Upcoming Flyte Admin features include: Authorization : to complement the already existing authentication support Event Sinks : Currently workflow notifications are supported on completion for Email, Slack and PagerDuty but we plan to support more generic event sinks including cloud message queues Granular, priority-based scheduling : Use a quality of service designation to manage workflow scheduling Full-fledged multi-cloud support : Currently Flyte Admin has been tested on AWS and GCP but we plan to incorporate and test Azure support too. Previous blog post introducing Flyte The Flyte repo on Github Flyte Admin control plane overview and contributor docs Flyte Admin service how-to If this work excites you, join us ! Stories from Lyft Engineering. 285 Flyte Orchestration Control Plane Mlops Science 285 claps 285 Written by Stories from Lyft Engineering. Written by Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-11-18"},
{"website": "Lyft-Engineering", "title": "iam whatever you say iam", "author": ["Alex Chantavy"], "link": "https://eng.lyft.com/iam-whatever-you-say-iam-febce59d1e3b", "abstract": "Data Data Science Engineering Mobile Product Security This post was co-authored with Andrew Johnson @SecPrez . Last year in March (this was 2019 in case you forgot; doesn’t that feel like forever ago?), we open sourced Cartography , our Python tool that consolidates technical assets and the relationships between them in a graph database. Using graphs helps us visualize and reason about security problems in a very powerful way. One such problem is understanding cloud permissions relationships: we needed an answer to the question “who has permission to read and write to my sensitive data resources?” In the cloud, this is not always a straightforward problem! Lyft is an AWS shop, and AWS’ access control mechanism is called IAM . It determines which Principals (e.g. users, groups, and roles) may perform which Actions on which Resources (e.g. storage buckets, compute instances, etc). IAM is powerful, highly customizable, and can work across account boundaries. IAM can be easy to configure incorrectly, and mistakes here can enable adversaries to easily move around and perform malicious actions in your environment. Cartography ingests AWS IAM data and represents it in graph form like this: AWS roles, users, and groups have policies attached to them, which determine the Actions they are allowed to perform or not perform against a defined set of Resources . IAM can get very complicated: you can specify advanced clauses like NotAction (which determine what a Resource can’t do) or NotResource (which determine the resources this statement does not apply to). Further, you can use the * character to have a policy apply to objects that match a given text string. A principal’s resulting access is determined by all the policy statements mapped to it. As a motivating example, we wanted to quickly see which principals had root or “root-like” privileges in our environment. An IAM policy like this allows the equivalent of root privileges to all principals it is attached to because it allows any action to be performed on any resource: Cartography uses a Neo4j graph database, so we can use this query to search for these principals: If you stare at the query long enough, it makes sense: we look for PolicyStatements that are attached to AWSPolicy nodes that are attached to AWSPrincipals where * is set as both a Resource and an Action . The result can look something like this. If you try this yourself, you might be in for an unpleasant surprise if you aren’t expecting any principals to be highly privileged: It’s great that we have the data, but it’s cumbersome to need to remember all the rules of IAM policy evaluation to answer this question. It would save us a lot of time to be able to simply ask “who has permission to read from my storage buckets?” or “who has permission to run queries on my DynamoDB tables?” With all of this data in the graph, earlier in April of this year we thought that it’d be a great idea to evaluate IAM policies offline so that we could determine a given principal’s resulting accesses (for those familiar with Windows security, this calculation might remind you a bit of RSOP ). We called this feature Resource Permission Relationships. We started by including 3 built-in RPRs with Cartography: As seen above, our plan was for Cartography to automatically map AWS principals to the resources that they can access! These mappings would be specified in a permission_relationships.yaml file , and you can read how to configure this here . To understand how this works, we’ll walk you through the above picture’s CAN_READ example: Cartography will search all AWSPolicyStatement nodes that allow the S3:GetObject permission and find all AWSPrincipal nodes attached to these statements. Cartography also verifies that no other AWSPolicyStatement nodes deny the S3:GetObject permission. Then, Cartography will search for all S3Bucket nodes in the current AWS account (since ResourceType is specified as S3Bucket above) and draw a graph edge labeled CAN_READ from the AWSPrincipal to the S3Bucket . In summary, we have taken the path (:AWSPrincipal)-->(:AWSPolicy)-->(:AWSPolicyStatement{effect:\"Allow\", resource:\"s3\", Action:\"S3:GetObject\"}) , evaluated it against S3Bucket nodes, and simplified it to draw an edge from the principal to the S3 bucket like this: (:AWSPrincipal)-[:CAN_READ]->(:S3Bucket) . The questions “what does this principal have permission to do?” and “who has access to my resource?” can now be queried at scale as they are precomputed and stored in the graph. Let’s verify who has read or write privileges to a sensitive s3 bucket: Result: Since group expansion has already been calculated, this is the resulting set of roles and users that can access the sensitive S3 bucket. Only identity-based policies are evaluated at the moment, we plan to add resource policies in a future update. Finding out an individual user has access to is also simple, as Cartography syncs Okta identity data: You can also query for aggregate data over your entire graph. For example, who in your organization has access to the greatest number of S3 buckets? At the time of development, we knew of other great open-source projects that dealt with IAM like policyuniverse and Cloudsplaining , but we did not know of anything that performed offline evaluation in the way we wanted. We found that the closest project to our idea was NCC Group’s PMapper (neat!), but PMapper focuses on returning a single query answer and doesn’t yield as much of a full, explorable picture to the extent that Cartography does. There are more benefits to having this logic in Cartography itself, which we will cover in the next scenario. You might have noticed that we only included 3 RPRs in the previous section regarding S3 buckets and DynamoDB tables, and you might be wondering why we didn’t perform this policy expansion logic for all principals and for all resources so that the graph would be as complete as possible. This is infeasible because AWS has a lot of built-in IAM policies that may not even apply to any principals in your environment. Running this calculation for everything is wasteful so we opted instead to include some sample RPRs in the default permission_relationships.yaml file and allow you to customize this to your needs. You can copy the process described in the next section to do this. More recently, we had a task to monitor the IAM principals that have admin access to AWS Redshift instances. This was a simple 6 line change to our permission_relationships.yaml file (and you can see the PR here ): This short config searches the graph for policy statements that allow any one of the four above actions ( redshift:* , redshift:CreateClusterUser , etc). Then, we find which AWS principals in the current account are attached to these statements. Finally, we draw a link from each principal to all RedshiftCluster nodes in the current account of the form (:AWSPrincipal)-[:CAN_ADMINISTER]->(:RedshiftCluster) . Our rationale here is that if an identity is able to perform any one of those four actions on a Redshift cluster, then we consider that a so-called “Redshift admin” and we want to draw a relationship from the identity to the cluster so that we can quickly query for them. The result looks like this where the yellow AWSGroup below has the redshift:* action attached to it, which allows it to perform any action on all Redshift clusters. You can draw your own resource permission relationships by copying our examples to your own yaml file and specifying its absolute path in the Cartography command-line interface’s --permission-relationships-file argument. We have established mappings from AWS principal to sensitive Redshift resources, but as mentioned above in our Related Work section, this is still slightly duplicative of PMapper’s functionality. Why even build this into Cartography? Now that we have enriched the IAM data in the graph, we can use Cartography’s Drift Detection feature to let us know via Slack alerts whenever the list of Redshift admins changes, and that we should investigate why this list changed. We’ll blog on the details of Drift Detection in a future post if there’s interest (and once we dig ourselves out from under of the pile of other wonderful ideas we want to build), but as a teaser, the result looks like this: We’ve shared Cartography at several security conferences over our first year and a half as an open source project, but this is the first time that we’ve blogged about it here on the Lyft Engineering blog. The problem of understanding IAM permissions lends itself well to a graph-based solution, and we’re just scratching the surface of what we have planned. There are lots of ideas we have around engineering a more reliable data sync , speeding up the sync process , making our plugin framework even more newcomer-friendly , and making the data useful for more people . More than just providing data, we’re excited about using the graph for both offensive and defensive automated actions: imagine having a robot army running around your environment using the graph as a map and fixing all the security problems that it finds. There are many possibilities to use this idea to its full potential and we look forward to building out this platform together with the open source infosec community. We hope that you’ve found this post informative and that you feel inspired to check out our tool at https://github.com/lyft/cartography . If you have questions or want to reach out, please say hi in channel #cartography on the Lyft OSS Slack or at our monthly public community meeting . Join us building open source tools to solve security problems at scale; Lyft Security is hiring ! Stories from Lyft Engineering. 167 Security Cartography AWS Security Engineering Security Engineer 167 claps 167 Written by Software Engineer, Security at Lyft Stories from Lyft Engineering. Written by Software Engineer, Security at Lyft Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-11-05"},
{"website": "Lyft-Engineering", "title": "creating a culture for learning development at lyft for data and science", "author": ["Alexis"], "link": "https://eng.lyft.com/creating-a-culture-for-learning-development-at-lyft-for-data-and-science-fd22ab1defd6", "abstract": "Data Data Science Engineering Mobile Product Security By Alexis Weill and Martin Yamane Learning and Development (“L&D”) is a broad term that encompasses any initiative aimed at improving employees skills and knowledge so that they may be more effective in the workplace. To help facilitate opportunities for Lyft employees to grow and succeed, Lyft has created an L&D council for the Data and Science organization with the goal of helping employees to acquire the skills to be most effective in their roles. There are a few reasons Lyft is investing in developing Learning and Development opportunities: It helps recruit the types of employees that are interested in self-development, It develops employees’ skill sets which increases company productivity, It increases employee retention as our team members want to keep growing professionally. It is a win-win situation for the company and its employees. In 2019, we realized there was an opportunity to bring together disparate elements of Learning and Development within the Data and Science organization under one umbrella. This offered two main advantages: Elevate the visibility of existing programs so more team members could benefit from them. Allow us to take a more holistic approach to Learning and Development and identify existing gaps to develop programs to address those. Our general approach has been to leverage the wealth of knowledge that our team members already have across the organization to benefit everyone. Having seen similar efforts decay over time, we spent some to time to develop the right structure to ensure that this effort would be self-sustaining. We structured the council as follows, inspired by our Diversity & Inclusion program : An executive sponsor Two co-chairs that are responsible for setting the vision, selecting workstream leads, running the council day-to-day, and representing the council Four workstreams, with two to three leads for each workstream Volunteers organized in initiatives within each workstream All positions entail six to twelve month commitments — this ensures that we have continuity in the short to medium term but also that we provide opportunities for more voices to be involved. We created four workstreams to focus on the following areas of opportunity: The mission of this workstream is to uplevel team members’ technical skills. Our current offerings are multi-week courses, single-session training, and bi-weekly talks on specific projects. They are developed and delivered by team members. The courses cover a wide range of subjects such as experimentation and data visualization, primarily relying on online resources for general learnings and team members for Lyft-specific teachings. The training sessions are focused on specific skills on internal know-how on a specific topic or tool. The mission of this workstream is to uplevel team members soft and business skills. The workstream puts together monthly, employee-led panel discussions to go over topics such as “managing up”, “providing feedback” or “thinking like a product manager”. Volunteers are also piloting a Toastmasters club for individuals to practice presentation & public speaking skills. This workstream covers opportunities that are hands-on or individualized, with more opportunities for one-on-one interactions. We currently offer 3 programs: The mentorship program pairs employees with a peer or manager to provide them with mentorship/coaching to help them think through their careers, provide advice, etc. The shadow program provides an opportunity for employees to learn more about different roles on different teams or at different levels. For example, a Data Scientist may decide to shadow a ML Engineer or a Science Manager, or simply learn more about the challenges faced by other Data Scientists. Lastly, the leadership lunch series provides an opportunity for team members to interact with leadership in a casual, small-group setting. This workstream improves processes and policies around career development for the team. The work ranges from developing a set of “shared agreements” between team members and their managers on how to best work together to providing documentation on career development (e.g., clarifying Lyft’s career pathways, providing career growth principles, and sharing a sample promotion packet). The first step to kick-starting Learning and Development at your company is to understand how team members want to develop, as well as what leadership considers the most important skills to cultivate among employees (we noticed that they often differ). This helps demonstrate value quickly by focusing on content that addresses the most sought-after areas of development. This can be as simple as sending out a survey: when we were first getting started, our workstream leads sent out surveys asking Lyft employees to rank different ideas for programs and courses as well as suggest new ones. “If you want to build a ship, don’t drum up people together to collect wood and don’t assign them tasks and work, but rather teach them to long for the endless immensity of the sea.” (attributed to Antoine de Saint-Exupéry) Putting together a team that shares the council’s vision is key to have motivated members that deliver great impact. Team members are often passionate about a specific aspect of the council, so it helps to match them with their top area of interest. One challenge is that you will be asking volunteers and workstream leads to commit to additional work on top of their regular obligations. Executive sponsorship and buy-in from the top helps — if contributions to the wider team are valued, it aligns incentives for everyone. One approach is to add those activities in team members’ career pathways: the volunteers get credit for helping out and the whole team benefits. Like any other initiative, prioritization is key. After determining the areas team members want to develop in, you should create a roadmap of the L&D program you plan to launch. For each initiative, you will need to think through the idea end-to-end to ensure its feasibility within a reasonable timeframe. Ask yourself: How much effort needs to be put into developing the materials for this offering? Will this cost money? If so, how do we get the budget? Can we leverage existing materials or past efforts to avoid duplicative efforts? How much more effective will the participants be in their role after completing this program? After you have answered these questions, you can determine which ideas are worthwhile and which ideas to scrap. Content is only as good as the impact it has on the team: it is important to communicate early and often to make sure that team members are aware of the various opportunities available to them. You can expand the reach of any piece of content by recording it, as this allows you and the team to focus on creating new content instead of spending time reproducing existing content. We also found it helpful to make all the content available in one central place. One side benefit of having compiled all this great content is that it is great for new hire onboarding too! Just like any new feature launch or program change, you will want to track how your new L&D initiatives are performing. It is the best way to determine the value of the new content. Tracking metrics such as attendance rates, program completion, and net promoter score (NPS) for your initiatives is key to ensure that you are on the right track. If programs are getting low attendance or NPS, you should question whether the offering should be continued or modified. Setting up the right systems ensures that your L&D efforts lasts well past your tenure. There needs to be a formalized process for recruiting new team members and for transitioning work. Additionally, as we discussed earlier, incentives should be aligned so that employees feel like their time spent creating valuable content is valued by their manager and the company. Building a Learning & Development council has been an extremely rewarding experience that has benefited many at Lyft, both from working on the council and learning from our programs. A year in, we are happy to say that we are seeing high attendance for our efforts and have continued adjusting our programming based on ongoing feedback. In 2021, new team members will lead the council and continue to build upon the culture of Learning and Development for Lyft. While we created this council to meet Lyft’s L&D needs, the same strategies can apply for any company looking to increase their team’s skill set. We hope this guide has motivated you and given you the necessary action plan to start your own Learning and Development initiative! Thanks to the following people for their helpful comments (in alphabetical order): Akshat Jain, Alya Abbott, Sarah Morse, Simran Mirchandani Thank you to the L&D council leads as well (in alphabetical order): Ken Chew, Lei Tang, Mehrnaz Abdollahian, Noriko Kakihara, Or Cohen, Rahul Kadian, Ricky Chachra, Yunjie Zhao Lyft is hiring. Check out our open roles . Stories from Lyft Engineering. 148 Thanks to Lyft . Learning And Development Data Science Learning Corporate Culture Organizational Culture 148 claps 148 Written by All things data Stories from Lyft Engineering. Written by All things data Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2021-01-13"},
{"website": "Lyft-Engineering", "title": "envoy mobile joins the cncf", "author": ["Mike Schore"], "link": "https://eng.lyft.com/envoy-mobile-joins-the-cncf-99aee4bdb32e", "abstract": "Data Data Science Engineering Mobile Product Security Just over a year ago, we announced Envoy Mobile’s initial OSS preview . Today, we have some exciting news to share: Envoy Mobile is officially joining its parent project Envoy as part of the Cloud Native Computing Foundation (CNCF)! From the outset, we made the commitment to develop Envoy Mobile as an open source project. Our belief in its potential to break new ground as a model for mobile networking and recognition of the vibrant community that has grown around Envoy made open source the clear choice for us. In the past year, it has been immensely rewarding to interact with the community as we’ve moved from the earliest proof of concept stages to an experimental and then production solution, fully in the open. Today is an important milestone, as we are privileged to join projects like Envoy, Kubernetes, gRPC, and more under CNCF stewardship. The success of the Envoy project came about because it provided a common foundation to a network of diverse and polyglot systems. Deploying Envoy means one achieves consistent observability, configurability, and extensibility across the fleet, regardless of any unique characteristics of the individual services. Envoy’s model of active/passive health-checking and eventually-consistent routing configuration provides stability and reliability in the face of unpredictable network and software faults. The Envoy Mobile project began with a question: Why do we treat mobile devices differently from the nodes in our backend infrastructure? Even though businesses run critical software on both, one is treated as core infrastructure, and the other is treated as a discrete, external client. The default assumption has historically been that the two components are fundamentally different. While there surely are differences, many of the challenges experienced by mobile clients resemble problems we had previously solved on the server with Envoy. Achieving visibility into the health and performance of mobile applications is an ongoing struggle for many organizations. Similarly, custom runtime/dynamic configuration solutions are the norm for apps deployed at scale. And given that both Android and iOS must generally both be supported, sharing code is difficult. Moreover, the network conditions faced by mobile clients are even more unpredictable and fault-prone than those in backend infrastructure. We had leveraged Envoy as a common solution to these problems on the backend. Could Envoy become the solution for mobile clients as well? Envoy Mobile came about when we decided, “Absolutely.” “Envoy has become the universal programmable data plane for application networking. We are excited to see Envoy Mobile bring the benefits of Envoy to the mobile ecosystem. We now have the ability to deliver programmability to client applications such as mobile apps enabling true end-to-end cloud-native services.” — Anna Berenberg, Distinguished Engineer, Google Cloud Before we embarked on this journey, we spent some time evaluating other possibilities. There are a number of capable mobile networking libraries with a strong following. But a key differentiator Envoy delivers is that it provides a common layer for all networking across all platforms within a distributed application. Ultimately, we came to the conclusion that the only way we could achieve that same advantage on our mobile clients would be to use Envoy itself. Envoy, of course, wasn’t designed to be an in-process library, let alone one run within a mobile application. But a number of its design decisions made it remarkably adaptable to this new purpose — from its largely single-threaded codebase, to its buffer management model. We’ll share more on this in a future post, but for an in-depth look at how we adapted Envoy to run as a library instead of a server, see this talk we gave at KubeCon and EnvoyCon last year . Envoy’s modern architecture and design gave us a notable head start on proving out the viability of our idea. When we first published the Envoy Mobile repository, the library was little more than a working prototype. Envoy ran and could serve requests on both iOS and Android. We had an ambitious roadmap, but rather than execute on the project behind closed doors and share it only once we had a production-ready solution, we made the deliberate decision to open source not only our incipient project, but also the roadmap itself. Our belief is that the ideas behind Envoy Mobile have the potential to fundamentally change how organizations view and interact with mobile clients and IoT devices. Sharing that vision and the earliest version of the project was in part our commitment to that belief, and in part a pledge to the community that we wanted to work to realize these benefits for everyone. A year on, and Envoy Mobile is compiled into the Lyft apps and dispatches our production traffic. The library has first-class bindings and support for Swift, Kotlin, Objective-C, and Java (with Python in the works!). We’ve reached the point where we now have a real foundation in place to build the future of mobile networking. “Envoy’s community growth and myriad use cases continues to exceed my wildest expectations. While Envoy has had a profound effect on how server-side cloud-native distributed systems are built, mobile clients and IoT devices have all of the same problems as their server-side counterparts including observability, fault tolerance, load balancing, and configuration. Envoy Mobile moving into the CNCF will accelerate the adoption of Envoy as an end-to-end cloud-native networking platform, enabling more robust deployments of complex distributed applications. I couldn’t be more excited.” — Matt Klein, creator of Envoy With a unified foundation and a common network abstraction in place across clients and backend services, features can be created once and used everywhere . We have big plans for the next chapter of Envoy Mobile’s development. A very abbreviated selection includes: Strongly-typed API generation — allowing APIs to be defined with IDLs such as protocol buffers and leveraging code generation to eliminate boilerplate and abstract away transport. Policy-based support of networking features selected via API/IDL annotations — including caching, retries, deferred APIs, streaming, push, and prioritization. Advanced protocol support and network optimization — support for QUIC, HTTP/3, DNS alternatives, and custom protocol extensions, with intelligent connection weighting and selection. xDS — leveraging Envoy’s configuration discovery APIs to dynamically configure via the same control plane as the traditional mesh. To see more about what we have planned or see opportunities to contribute , our roadmap is open source as well! The timing is right for Envoy Mobile to join the CNCF. This new, neutral home will make it easier for us to collaborate with and accept contributions from other organizations. It will also align us more closely with Envoy’s development cycle, allowing us to execute shared CI test coverage, and upstream features with tighter integration between the two projects. With this step, we look forward to working with the CNCF — and all of you — on redefining how we think about client-server communications. Stories from Lyft Engineering. 127 Thanks to Michael Rebello . Cloud Computing Open Source Envoy Proxy Engineering Mobile 127 claps 127 Written by Stories from Lyft Engineering. Written by Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-11-17"},
{"website": "Lyft-Engineering", "title": "decomposing network calls on the lyft mobile apps", "author": ["Don Yu"], "link": "https://eng.lyft.com/decomposing-network-calls-on-the-lyft-mobile-apps-c352de8e5e77", "abstract": "Data Data Science Engineering Mobile Product Security When Lyft was first developed, it was built using a monolithic server architecture. Within this architecture, all mobile clients relied on a single endpoint for fetching all data pertaining to the user and their ride (the “state of the world”): /users/:user_id/location . Using a 5 second polling loop, the client would send the server the user’s current location information, and then receive “the state of the world” back in what we called the Universal Object. This Universal Object was exactly like the name suggests — it included nearly everything the app needed to render a screen. Need to know whether a driver is online? Check the Universal Object. Need to display the driver’s earnings? Also in the Universal Object. Need ride stop information? You get the point. By introducing a “state of the world” polling loop, Lyft was able to iterate quickly when building new features, since product teams could piggy-back on this polling loop by adding new fields to the Universal Object. Additionally, page loads were seamless and the client apps didn’t need complex data management because all user or ride data was always present. This architecture made a lot of sense when Lyft was first starting out and iteration speed was the biggest priority, but it led to tech debt and numerous resiliency issues as we scaled our user base and transitioned our servers to a microservice architecture in 2017. A single “universal” endpoint introduced a single point-of-failure, and did not leverage the extensibility and independence of microservices. Through a joint effort across various teams spanning 13+ engineers, we were able to decompose the Universal Object into many isolated endpoints. There were numerous server and client benefits that motivated this project: Benefit #1: Improved reliability because the new APIs allow for partial availability and failure isolation. The Universal Object (UO) polling loop did not have resource isolation, and a single incorrect field on a small portion of the response could prevent the client from parsing the entire payload, thereby leading to a blocked user experience. By decomposing the UO and introducing resource isolation, new endpoints were able to map to a single core data model and remain independent from each other. For example, a bug in the microservice that serves user profile information would no longer be able to prevent the driver from receiving route stop information. Benefit #2: Easier to triage user issues. By simplifying the number of downstream service dependencies that were needed to construct a single endpoint’s response payload, we were able to create a far simpler debugging interface server-side. Prior to migrating off of Lyft’s monolith service, the client-facing API did not decompose the Universal Object into smaller pieces, which led to engineers having to investigate dozens of different microservices to root cause a bug on a specific field of the Universal Object. By decomposing the “state of the world” endpoint to match the microservice architecture, engineers only needed to investigate 1–2 microservices. Benefit #3: Reduced costs by polling different resources at different rates and transitioning to push. Different resources update at different rates. For example, the ETA to the next stop changes far more frequently than the user’s profile information. When we decomposed the Universal Object into separate resources, our clients were able to request updates from different APIs at different rates by adjusting their polling intervals. This saved a ton of compute time on the microservices that previously had to perform redundant calculations every time the Universal Object was requested. The reduced costs from this change apply to both hosting costs on Lyft’s side and bandwidth costs for our users. Throughout these experiments, we have seen shorter latency on network requests as payload sizes decreased. Overall client bandwidth costs also decreased because the client app was able to leverage push streams, poll at varying rates, or switch to a single-fetch for non-updating payloads. One of the core goals of the project was to avoid breaking the mobile apps or blocking other product teams from feature development. Here are the best practices we followed to ship a mobile app refactor of this size. Best practice #1: Create client-side abstractions. When we started client-side work, we wanted to make sure that other developers could start working on top of the new data models immediately. To that end, we first started mapping the existing Universal Object into the various “decomposed” data models on the client before we split the endpoints. At the same time, we deprecated the existing data model fields to prevent new usages. This approach gave client engineers time to adjust to the new paradigm, build new features on top of it without worrying about the switch to “decomposed” endpoints, and made it possible to start testing a client-only decomposition before the underlying APIs were ready server-side. Best practice #2: Shadow server-side. As we built out the new APIs to replace the Universal Object, we needed to ensure that there were no mismatches between the payloads on the new and legacy APIs. For example, the address of a route stop fetched from the decomposed /v1/routes/stops endpoint would need to exactly match the route stop address fetched from /users/:user_id/location. To that end, we shadowed 1% of all production traffic and tracked mismatch counts on a Wavefront dashboard to monitor potential bugs during rollout. Best practice #3: Experiment rigorously on the new endpoints. We launched 28+ A/B experiments over the last year to confirm that the decomposition did not break any important user flows or cause harm to business metrics. While these experiments slowed down rollout, running isolated experiments helped the team uncover numerous edge cases that affected a smaller percentage of users. Lyft has already started to enjoy the fruits of the decomposition effort. The new decomposed endpoints have a reduced p50 latency of <120ms compared to the p50 latency of >200ms on calls to the original /users/:user_id/location endpoint. One of the biggest impacts from decomposition came from moving new ride request info off of the Universal Object, which ended up reducing the time between when Lyft matches a driver to a ride on our backend and when the driver is notified of the match by over 20%. While Lyft still maintains the legacy endpoint to support older app versions, newer versions of the rider and driver mobile apps no longer fetch the Universal Object. However, decomposing network calls might not always be the right choice. After decomposition, there is no longer a single source of truth and thus the client cannot assume that it always has the same state of the world as the server. There could be lag across the different decomposed polling streams, leading to one piece of information being more up-to-date than another (i.e., the driver’s location versus the ride’s status). Also, there is additional client complexity as engineers need to hit the specific endpoint that has the data model they need rather than access a field on a state-of-the-world object. A continuous “state of the world” polling loop could be the right architecture when your product is in its early stages, as was the case for Lyft in the past. However, decomposition became a necessary adaptation as Lyft’s user base grew exponentially and stability became the highest priority. Even if decomposition is not the right architectural decision at the moment, it is important to keep the trade-offs top-of-mind when considering the future scale of an application. We hope that our journey decomposing the Universal Object at Lyft will help readers improve the design/scalability of their client apps. Don Yu, Sarah Mazur, Daniel Duan, Pierce Johnson, and the many other engineers involved in this project work on different teams at Lyft (Resilience, Core Services, Client Architecture, Driver App Platform, and more). If you’re passionate about creating resilient client apps or tackling interesting scaling problems like this one, read more about them on our blog or apply to join our team ! Stories from Lyft Engineering. 426 Thanks to Rafal and Michael Rebello . Mobile Client Networking Decomposition Microservices Engineering 426 claps 426 Written by Mobile developer currently working @Lyft. Previously @Facebook, @Betterment. Stories from Lyft Engineering. Written by Mobile developer currently working @Lyft. Previously @Facebook, @Betterment. Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-11-11"},
{"website": "Lyft-Engineering", "title": "what the heck is gevent", "author": ["Roy Williams"], "link": "https://eng.lyft.com/what-the-heck-is-gevent-4e87db98a8", "abstract": "Data Data Science Engineering Mobile Product Security gevent is a coroutine-based cooperative multitasking python framework that relies on monkey patching to make all code cooperative. Gevent actually draws its lineage from Eve Online which was implemented using Stackless Python which eventually evolved into eventlet which inspired gevent. Just like Eve Online, gevent is full of skullduggery and confounds outsiders. There’s a lot in there, so let’s break it down. Coroutines split up programs into blocking and non-blocking work. Frontend developers are familiar with callback hell — each of those callbacks is a coroutine. In the above example, the functions are coroutines - they do some work, call a blocking function with a callback for what to do when that blocking function finishes. Importantly coroutines have to explicitly yield control for another coroutine to run which allows us to more easily reason about where we have to be concerned about interleaving (more on this later). coroutines are not threads! A coroutine is also known as a greenlet in gevent. Cooperative Multitasking is a method used to enable a computer to handle more concurrent work than there are workers (e.g. cores on your computer). Cooperative Multitasking dates back to the dawn of timesharing — Windows 3.1 relied on cooperative multitasking to allow multiple applications to run at the same time on a computer with 1 core. The main downside, then and now, is cooperative systems rely on tasks to…well..cooperate. In a preemptive multitasking system like modern operating systems, the OS understands priorities and how much time each task has run for and can interrupt long running process to allow another to run. No such fairness mechanism exists for a cooperative system. Monkey patching is a technique for modifying behavior at runtime for the standard library or third party libraries. Monkey patching is generally considered a bad practice as it can lead to hard-to-understand bugs and requires the monkey patches to be updated with the underlying library being patched. In the case of gevent — monkey patching has to be the absolute first thing a process does — otherwise libraries will get a handle to the real, blocking implementations. Below we have a full example using gevent - we first monkey patch the standard library which then magically makes time.sleep cooperative - instead of blocking the CPU it yields control for at least the sleep time. No other code changes were required. gevent avoids callback hell by monkey patching the standard library, automatically creating coroutines - effectively resumption of work at the yield point. A useful corollary is how C# generates async/await code - it effectively breaks the function up every time there's an await . We’ll run this code using gevent.spawn to simulate concurrent requests. These coroutines are cooperative - notice we never see anything else run between first step and second step because control was never yielded. gevent isn't all bad. Coroutines are substantially lighter weight than threads or processes - we can run tens of thousands of greenlets on a host that could run hundreds of threads. gevent works with almost every third party library with no code changes - twisted , tornado , even asyncio all require using libraries that play nicely with their event loops for any blocking I/O - in contrast gevent works with requests out of the gate. gevent promises \"Add these two lines of code to your project and it's magically higher throughput\" and it mostly delivers. Unfortunately it’s not all sunshine and rainbows with gevent . gevent can lead to real correctness problems by interleaving code in unexpected ways. If a library is using native code we can still face blocking. Lastly while gevent optimizes for increasing throughput, it can lead to real latency problems. But this post is already getting quite lengthy — so we’ll go over correctness problems and how to avoid them in part 2 and responsiveness issues in part 3. If you want to play around with this code yourself, you can download this as a Jupyter Notebook ! This is part of a 4-part series. We suggest you also check out Part 2: Correctness , Part 3: Performance , and Part 4: Applying Learnings to Deliver Value to Users . Lyft is hiring! If you’re interested in improving peoples lives with the world’s best transportation, join us ! Stories from Lyft Engineering. 205 Thanks to Shiv Toolsidass and Garrett Heel . Python Performance Asynchronous Programming Languages Gevent 205 claps 205 Written by Stories from Lyft Engineering. Written by Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-10-16"},
{"website": "Lyft-Engineering", "title": "gevent part 2 correctness", "author": ["Roy Williams"], "link": "https://eng.lyft.com/gevent-part-2-correctness-22e3b7998382", "abstract": "Data Data Science Engineering Mobile Product Security As we talked about in part 1 , gevent makes all code magically cooperative, but not all code was written to be cooperative. This can lead to really subtle problems in production that are incredibly hard to debug — code that works in tests but breaks under load. Take the code below — let’s imagine we have an in-memory database of account balances and we send money in a circle. At the end, there should be no difference in balances — everyone should have $100. So far, so good. We have a unit test that passes and even after 100 concurrent requests we still have everyone at the correct balance. Now imagine we want to introduce some observability and track how much money is moving around. We want to maximize utilization of our service so our stats library uses gevent. compute_new_balances seems like the correct place to instrument, so we add a stats call. We make the change and then run our unit test again to ensure it's safe. Perfect! We introduced a stats change, tests passed, how dangerous can it be? Let’s run this bad boy in production… Uh oh….everyone’s supposed to have the same balance at the end of this but now everyone’s wildly off. Our customers are going to be really disappointed (or really happy!). How did this happen? Our Bank code was proven to be correct with unit tests. The core of the issue is our Bank code was written assuming serial, uninterrupted execution - send_money assumed the underlying database wasn't modified during between when we read the value and when we wrote it. This assumption was correct until a stats call was introduced - now a change in code I was calling into broke that assumption. This is the crux of why gevent causes bugs — magically interleaving code that wasn’t meant to be interleaved causes bugs. This isn’t unique to Python — this is a super common problem in threaded languages, for example Java’s HashMap is not thread safe. When faced with problems like this frequently the desire is to reach for locks — rarely is that the correct solution. Locks lead to even more performance and correctness problems and should be used as a last resort. One thing PHP got right was “no shared state” — it’s nearly impossible to accidentally share state between requests like this. The first step of mitigation is to avoid shared state whenever possible. In the above example — we should either create a new Bank per request or use an external database. Each request should be stateless . If that’s not possible we should ensure that all state updates are consistent. In this case, the core of the problem is the assumption the read and write are consistent and the underlying db did not change in between. A more correct way would be to accumulate transactions in a log, replaying them when we request the database. Appending to the list is safe as we are never updating the underlying database. If avoiding shared state isn’t possible, we should identify our critical sections and ensure no switching occurs when we are reading from/writing to this shared state. gevent has a utility gevent.util.assert_switches which does the opposite of what we want, but can be coerced into what we need: gevent has all of the correctness downsides of threads - code is hard to reason about as engineers have to consider all possible inter-leavings. It's easy to write bugs that don't reproduce in unit tests and bite us in production. gevent is incredibly powerful, but we have to be aware of these downsides and avoid them if at all possible. PHP has millions of problems, but shared state isn’t one of them! If you want to play around with this code yourself, you can download this as a Jupyter Notebook ! Play around with code and see the problems for yourself! This post is heavily inspired by Unyielding by Glyph Lyft is hiring! If you’re interested in improving peoples lives with the world’s best transportation, join us ! Stories from Lyft Engineering. 33 Thanks to Shiv Toolsidass and Eric Smith . Python Asynchronous Performance Bugs Gevent 33 claps 33 Written by Stories from Lyft Engineering. Written by Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-10-15"},
{"website": "Lyft-Engineering", "title": "dynamic pricing to sustain marketplace balance", "author": ["Davide Crapis"], "link": "https://eng.lyft.com/dynamic-pricing-to-sustain-marketplace-balance-1d23a8d1be90", "abstract": "Data Data Science Engineering Mobile Product Security By Davide Crapis and Chris Sholley Dynamic pricing is the main technology that allows us to maintain market balance in real-time. If we were able to perfectly plan for the future we wouldn’t need this technology, but in reality rider demand is quite volatile and often unpredictable. Our dynamic pricing algorithm is called PrimeTime (PT). Over the past few years, we went through several iterations of PT, continuously improving its capabilities. This blog post will walk through common marketplace problems we have observed in practice and then show how successive iterations of PT have helped us solve these problems and substantially improve the experience of riders and drivers on our platform. When a demand spike happens that we did not predict, there may be more riders requesting rides than there are drivers on the platform to service them, and the market can spiral in an unhealthy state of extreme undersupply in which there are no more drivers available. When the demand spike is less severe, the market may be in a state in which there are just enough drivers on the road to service all ride requests. This is still problematic because the few drivers that remain available are geographically dispersed and subsequent riders will experience very high wait times. We call this scenario undersupply . At the other end of the spectrum there are oversupply scenarios, in which small improvements to rider wait times are accompanied by low driver utilization. Normally, these happen because there are too many drivers on the platform relative to rider demand, but these can also happen if prices are too high and are artificially suppressing ride requests. This is a bad experience for both riders and drivers, hence the need to design our dynamic pricing algorithm to avoid both oversupply and undersupply. The main problem that motivated the adoption of dynamic pricing in ridesharing is that in cases of extreme undersupply, there are no drivers available to service demand. If you use ridesharing today, most likely you take service availability for granted. Perhaps you worry about how long it’s going to take your driver to get there and how much time you have to prepare, but you expect there will always be a driver around to get you where you want to go. It was not always like that. In the early days of Lyft, service unavailability was a common problem, in part because of a smaller network and lower density of rides, but also because there was no dynamic pricing mechanism to regulate market imbalances. The initial implementation of the pricing model hinges on the application of the basic pricing principle that the price-response curve is decreasing in price. In situations of undersupply, increasing price will suppress demand and bring the market back towards balance. But by how much should we increase the price? Our first attempt was based on the idea that as drivers accept requests, fewer drivers are available to accept new requests. Roughly speaking, if the request rate exceeds the drop-off rate, eventually there won’t be any more drivers available. So, a shrinking number of available drivers is a good signal that the market price is actually higher than the current price. The question is how to map the number of available drivers to a market price. We turned to Queuing Theory , which is a 100-year old area of mathematics that studies waiting lines. Specifically, we can model our marketplace as a system with a fixed number of total drivers ( c ), riders opening the Lyft app according to a Poisson process, and rides being completed with exponentially-distributed drop-off times. If a rider opens the app and there are no available drivers, the rider experiences unavailability, and cannot participate in the marketplace. Such a system is known as an M/M/c queue and has a well-studied relationship between the expected percent of drivers which are busy (i.e. “utilization”) and the probability that a rider will face unavailability upon opening the app and have to wait, as shown below: This stylized model matched our observed data quite well. Using this relationship, we could target a level of availability and allow the market price to rise if we believed the driver availability would fall below the target. We could also measure the benefits of scale, meaning more drivers and more riders (observe that as we increase the number of drivers, c , in the chart above, the marketplace can operate at higher levels of utilization with the same driver availability). Unavailability of service is a terrible rider experience that can have many bad consequences, with riders being stranded and not being able to go back home or being without access to essential services. If Lyft cannot offer a reliable transportation platform, riders will ultimately stop using it and choose other platforms or other modes of transport. Seeking to improve the consistency and reliability of service, we invested in several rounds of development after a successful initial launch of PT. In PTv0, we assumed that riders responded the same way to price changes at all times. In reality, riders have different transportation options depending on their time and place, and therefore we should assume they have different responses to price changes depending on their time and place. We found that without studying the rider response to price changes more carefully, we would fall below or exceed our availability target. The key component of PTv1 is estimating the price-response and using it to calculate the price increase that is necessary to maintain availability at virtually all times. For illustration purposes, we are going to use an exponential function to represent average conversion as a function of price C ( p ) = α ⋅ exp(− β ⋅ p ), where α and β are parameters to be estimated. We use this form because we want to capture decreasing conversion elasticity with respect to price. In the simplest implementation of PT, this function would be estimated off-line from historical data. In reality, one needs to control for other factors and we have developed several models and experimental techniques to get accurate and unbiased estimates of conversion functions. In PTv1, a conversion function similar to the above was used to compute a look-up table that would return the desired price multiplier given a target availability, a number of riders, and a number of drivers in a given location at a given time. This led to significant improvements in service availability. The first versions of dynamic pricing allowed us to mitigate the extreme undersupply case and avoid unavailability. This is a fundamental problem and source of the worst user experiences. However, there can still be problems in less extreme undersupply cases. The service might be available but with long waiting times, which is a bad experience for both riders and drivers. For example, a situation in which we might be able to match all riders with drivers but with very high ETAs (Estimated Time of Arrival for the driver) can be quite common in practice. It has been studied in economic literature as the Wild Goose Chase problem, and refers to drivers being matched to riders who are very far away and spending too much of their driving time “chasing” them. In order to solve this problem, we needed a better algorithm that allowed us to suppress demand beyond the point where supply equals demand instantaneously, in order to improve how many rides happen over time. The first component of PTv2 is to learn the relationship between mean ETAs and the number of available drivers. For illustration purposes, we are going to use a power function which can be derived by standard Spatial Poisson Process theory plus some simple assumptions on the ETA distance metric: ETA ( a ) = √ ( 𝜋 / ( 𝛾 ⋅ a )), where 𝛾 is a parameter to be estimated and a = s − r is the number of available drivers in the relevant area (the number of total drivers minus the ones serving ride requests). Again, we need to estimate this relationship, controlling for other factors that might affect ETAs. Using our conversion function, we can express ride requests as a function of number of interested riders d and price p as r = d ⋅ C ( p ) , and the average ETA as a function of price. We can now use prices to do ETA targeting. In other words, suppose that we wanted to attain an average ETA of x (perhaps we have another model that tells us that x is the efficient ETA), then we can compute a price that enforces this average ETA and depends only on our target, market conditions, and estimated model parameters. In our simple example, this has an analytic form: p ( x , s , d , α , β , 𝛾 ) = [log( α ⋅ d ) − log( s − 𝜋 / ( 𝛾 ⋅ x ²))] / β. Market conditions can change quickly, and if the model does not faithfully represent the state of the market we will be making sub-optimal decisions. To solve this challenge, we added a second component to PTv2 which leverages on-line machine learning techniques to update the relevant market parameters very frequently. This led to significant improvements in average ETAs and gave us confidence that we can maintain market balance on behalf of riders and drivers. A Lyft region is a dynamic and interconnected network of locations with many “local markets” continuously coming online and evolving over time. You may have never thought about it, but your decision to request a ride as a rider or accept a ride as a driver may affect the experience of other riders and drivers in nearby areas quite significantly, and even affect other riders and drivers in a completely different part of town half an hour later. Our algorithm needs to be able to compute prices that balance the entire network of markets in near real-time. For that, we needed to move past the heuristics we used in previous versions and instead use an optimization framework that allows us to flexibly specify objectives and efficiently compute prices. PTv3 solves a constrained optimization problem to maintain market balance across the network of markets. Using the notation from our illustrative examples we can sketch the problem as maximize f ( p ) subject to M ( d ∘ C ( p )) ⪯ Ms − m ( x ), with p̲ ≤ p ≤ p̅ . The matrix M encodes the topology of the ridesharing network: distances between locations, reachability, etc. The constraint is a set of matrix inequalities, or marketplace balance conditions, that ensure that the desired service levels x are maintained in every relevant subnetwork ( m ( x ) specifies available driver targets at the subnetwork level). The objective can be adapted to our goal. For example, we may want to maximize throughput of rides, in which case f ( p ) = d ∘ C ( p ), or we may want to maximize throughput of rides including the discounted future impact of a current price change, in which case we set f ( p ) = d ∘ ( C ₀( p ) + δ C ₁( p )). In reality, the problems we solve are based on models with many more features and parameters, and we have developed a suite of mathematical and computational techniques to solve these problems efficiently and at high frequency. We also need to ensure quality, and we are constantly monitoring and improving accuracy of the models and goodness of the solutions. Hopefully this post provided helpful insights on some of the key challenges in balancing ridesharing marketplaces and some of the exciting technology we have developed to tackle them. There are still plenty of open problems, from estimation and optimization bias on the algorithms side, to efficient input data and model streaming on the infrastructure side. We are constantly experimenting with new products to unlock efficiencies in how we run the marketplace (check out our Marketplace Experimentation series: Part 1 , Part 2 , and Part 3 ). If this work excites you, join us ! Stories from Lyft Engineering. 456 Thanks to Eric Smith and Michael Rebello . Data Science Ridesharing Pricing Algorithms Software Development 456 claps 456 Written by Stories from Lyft Engineering. Written by Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-11-11"},
{"website": "Lyft-Engineering", "title": "gevent part 3 performance", "author": ["Roy Williams"], "link": "https://eng.lyft.com/gevent-part-3-performance-e64303fa102b", "abstract": "Data Data Science Engineering Mobile Product Security Gevent’s sweet spot for performance is network-bound workloads. Coroutines allow us to efficiently interleave other CPU work while waiting on the network instead of just waiting for results. As we discussed in part 1 : gevent uses coroutines for multitasking and coroutines are not threads. They run completely in user space, from the kernel’s perspective there is still a single task. Coroutines rely on user level libraries for scheduling via an event loop — in gevent’s case the default is libev . Coroutines are significantly lighter weight than real threads so a process can run significantly more of them, and switching between coroutines can also be much faster as there's no requirement to thunk into kernel space. In exchange for being higher speed and lighter weight, coroutines require more thought and work to be done in user space. Since coroutines are not threads, there aren’t any interrupts as with real threads. If an OS thread is hogging too many resources the OS will interrupt it for another task to run, keeping things as fair as possible. At the end of the day the machine only has so many CPUs though and we have to deal with those real physical limitations — coroutines are not useful for parallelizing CPU-bound work, they’ll probably make things worse, but are incredibly useful for parallelizing network-bound work. Let’s imagine we have an endpoint that needs to fetch 3 resources, does some computation, and returns. The simplest way to write this would be to fetch all 3 resources serially, compute the result, and return: Great! This works as expected. Let’s see how long it takes: Hmm — not bad, but it can probably be faster. This workload appears to be totally dominated by network time, so it’s probably a good candidate to be sped up with coroutines! Few workloads are as clean as above though — usually our servers do some amount of compute, some amount of I/O, even if that compute is just serialization/deserialization of requests. Let’s create a benchmark where we vary the amount of CPU work, Network-bound work, and the number of coroutines we have running in parallel to simulate concurrent requests. This benchmark does half of its CPU work, then makes a network call, then does the remaining CPU work. You’ll find the code for this benchmark in the last cell of the Jupyter Notebook attached. First, let’s run a workload that does 5ms of CPU work and 55ms of networking, with 1 greenlet as a start: So far so good — our RPS is roughly what we would expect from a 60ms workload and we have a really tight bounding of response times. The only downside is that utilization number — we’re only using 8.24% of our CPU! Let’s throw more green threads at this task since we’re mostly dominated by I/O: Amazing! A nearly linear speedup with the number of green threads, and now we’re using 41% of our CPU instead of 8%! Well if 5 green threads was good, 50 must surely be better, right? Uh Oh….while our throughput is 11.6x higher than the baseline (a number tantalizingly close to 12, or 1/(1-(55/60)) but more on that later) our response times have skyrocketed! How can it be the case that the p50 time for a workload that takes 60ms is 188ms? Probably the GIL or garbage collector, right? Let's try to debug - let's dump who was doing what during our longest request. We'll use a different mix to more simply show the problem ( + indicates the start of work on a request, - indicates that work finishing) Interesting — when running with 5 green threads we see that the issue is for the longest request (Request 3) 150ms of network time “took” 228.47ms. Looking at the details we see that there was 25ms of CPU time scheduled for 8 other requests — helping us blow past our 150 network time. The first 6 times CPU time was scheduled it was OK as our request wasn’t done…but every additional time CPU work was scheduled we were starved. The issue is as the number of concurrent requests increases the likelihood more work gets scheduled than can be effectively be parallelized increases. Let’s see what this looks like in the limit by plotting the throughput speedup vs. # Green Threads and Response Time vs. # of Green Threads: We see that the speedup asymptotically approaches 12, but after ~11 concurrent requests p50 response times start to go up. Let’s revisit that 1/(1-(55/60)) number - this come from Amdahl's Law which states the maximum speedup of a task by parallelization is 1/(1-p) where p = {Parallelizable Ratio of Workload} . In our example here 55ms out of 60ms can be parallelized, so p = 0.92 Our use case here is slightly different than Amdahl’s law — in the case of Amdahl’s Law we throw more machines at a problem to incrementally reduce the cost of the parallelizable portion. With gevent and cooperative multitasking after 1/1-p workers the amount of time the parallelizable portion takes goes to zero as we can run all our CPU work while the network tasks are running. Even though we focused on Python and Gevent, these issues aren’t limited to gevent — they would be just as applicable to NodeJS, Asyncio, or any other cooperative multitasking system — if a cooperative system attempts to handle more simultaneous requests than it can response times will degrade. For putting this into practice, however, we will focus on gevent and gunicorn . In practice, this means we would like to minimize the number of concurrent requests a given process is handling — any processes handling too many concurrent requests will degrade response times. With this in mind, there’s a few things to be aware of: epoll unfairly routes requests , favoring higher PIDs. This can quickly lead to a situation where one process is handling too many requests, running super hot while the others are barely scheduled. This is exacerbated by a bug in gunicorn that causes a worker to accept as many connections as it can upon waking up. Pulling in this fix leads to more fair routing. Setting SO_REUSEPORT on the listening socket also improves fairness . This flipped from always being on to being opt-in in gunicorn 19.8 . Services should consider manually specifying --reuse-port in their gunicorn configuration. Gunicorn allows services to specify the maximum number of concurrent requests a worker will services with the — worker-connections flag. This defaults to 1000 which is almost certainly too high for most services (for example, services would have to do 1ms of CPU work, 1s of network bound work per request for this to be correct). Consider tuning this down to a more appropriate number using tools like tracing to determine how long the service is blocked on network vs. consuming CPU. A more proper number is {network_time}/{cpu_time} - for example if a service does 5ms of CPU work and 55ms of waiting on network for a typical request it should likely tune --worker-connections=12 - anything higher and response times will degrade. Optimize CPU time — Reducing CPU increases the {network_time}/{cpu_time} naturally allowing the service to handle more concurrent requests. Reduce how often critical code yields — Yielding more often to the event loop will reduce predictability of responses since there are now more opportunities for other requests to run and slow down the running request. Below we run the same benchmark, except instead of 1 55ms network call per request there's 5 11ms network calls per request. We see response times degrade faster as the number of green threads grow. Note that all of the above optimizations really only help services that are running somewhat hot. If each host is only ever serving a single request at a time this won’t help — concurrency isn’t an issue in that case. That said, the issues mentioned above are important blockers to running hotter — if a service saw response times degrade at higher CPU try making the above optimizations and try running hotter again. Once again, you can download this blog post as a Jupyter Notebook! I encourage you to play with the benchmarks to help understand how performance degrades as you try to do more work in greenlets. Lyft is hiring! If you’re interested in improving peoples lives with the world’s best transportation, join us ! Stories from Lyft Engineering. 48 Thanks to Garrett Heel . Python Performance Asynchronous Web Development Gevent 48 claps 48 Written by Stories from Lyft Engineering. Written by Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-10-16"},
{"website": "Lyft-Engineering", "title": "how lyft predicts your destination with attention", "author": ["Hyungjun Lee"], "link": "https://eng.lyft.com/how-lyft-predicts-your-destination-with-attention-791146b0a439", "abstract": "Data Data Science Engineering Mobile Product Security By Hyungjun Lee, Jacob van Gogh This blog post details how Lyft predicts riders’ destinations when they open the app. Destination prediction enables Lyft to provide relevant location suggestions, allowing riders to set their destination with a single tap instead of typing out search queries, and thus making this part of the ride hailing experience effortless. We tackle the destination recommendation problem using the rider’s historical rides. The main idea is to limit candidate recommendations to addresses where the rider has previously taken a Lyft ride to or from. Within this candidate set, we use an attention mechanism (discussed in more detail below), to determine which locations are most relevant to the current session. Ride destinations are highly personal — each location has different meanings to different riders. For example, a rider’s home is a highly probable destination for that rider, but would not be relevant to most others. We therefore need to tailor predictions specifically for each rider. We achieve this personalization by restricting the candidate destinations to the locations that appear either as the origin or the destination of a previous ride taken by the rider, filtered by a physical proximity measure. (For example, a rider probably isn’t taking a Lyft to their home in San Francisco when they’re in New York on a business trip). An added benefit of this approach is that it limits the number of candidate destinations to a reasonable size — classification problems with many candidate labels generally require a candidate generation step for computational reasons. In a naive approach, there is an extremely large number of destinations a rider can theoretically travel to. In order to predict which of the previous ride origins and destinations a rider is traveling to, we use various forms of attention . Let’s first review the basics of attention. Consider a sequence of vectors ( keys ), K , where the goal is to create a score for each individual vector to use for weighting purposes. Imagine a separate vector ( query ), q , which is the same dimension as the keys. One way to score all of the keys is to then take the dot product similarity between each key and the query. We can then normalize these scores to get a standard weighting, which we then apply to a sequence of values , V (which must be the same length as our keys, and can even be the keys themselves). Our simplified attention would then be constructed as follows: What if we want to generate more than one set of weights for our keys? There’s nothing preventing us from stacking a set of queries into a matrix, Q . Finally, research has shown that the performance of this scheme is improved, particularly on longer sequences, by scaling the weights by the square root of the length of keys / values¹, n, giving us the following attention function: We enhance the capabilities of attention by utilizing multi-head attention. As the name suggests, multi-head attention has multiple heads, the outputs of which are concatenated to yield the final multi-head attention output. Each head can be trained to focus on a particular context, instead of a single big attention mechanism being trained to focus on everything. Specifically, each head applies linear transformations to the query ( Q ), key ( K ), and value ( V ), and applies attention to the transformed matrices: where W^Q_i , W^K_i , and W^V_i are trainable weights defining the linear transformations. The output of the multi-head attention is the concatenation of the outputs of each head: where H is the number of heads. The final output of the model, which represents the probabilities that each location in the candidate set is the destination of the current session, is the weighted average of the historical origins and destinations represented as a one-hot encoded vector generated by an attention layer. The inputs to this final attention layer are: Query: A vector representing the current ride context. Key: A sequence of vectors representing the context of each historical ride. Value: A sequence of one-hot encoded vectors of historical origins and destinations. To obtain the current and historical ride contexts that serve as the query and key for the final attention layer, we send the raw feature vectors from the current session and the historical rides jointly through a series of joint self-attention layers. The joint self-attention layers help frame the “meaning” of each ride in the context of the rides around it. There are four separate attention mechanisms in the joint self-attention layer: one self-attention for each current and each historical ride context, and two cross-attentions between the two. In self-attention, the query, key, and value are all provided by the same context. In cross-attention, the query comes from one context and the key and value from the other. We add a skip connection on top of the attention layers to obtain the intermediate output, as shown above in the diagram. Finally, the intermediate output is sent through a pointwise feedforward layer with another skip connection to yield the final output: We assume that the ride will originate from the rider’s current location, and the ride will be requested shortly after the app is opened. Thus, the raw ride features are: Latitude and longitude of the rider’s current location. Request time (i.e., the current time). Request time, latitude and longitude of origin and destination of the rider’s historical rides. We first send these raw ride features through pointwise feedforward layers. For historical rides, there are two separate pointwise feedforward layers to extract the raw context vectors for the origins and destinations, respectively. The results of these pointwise feedforward networks are fed through two successive joint self-attention layers described above to yield the query (current ride context) and the key (historical ride contexts) for the final attention layer. At this point, we append an additional output class to the historical origin/destination set to account for the possibility that the desired destination of the current session is not in the historical set. A corresponding key vector consisting of all 0s is also appended to the keys. The resulting query, key, and value are sent through the final attention layer to yield the current session destination probabilities. The diagram below shows the complete view of the network: This model is trained on 16 million pre-COVID rides for 50 epochs. COVID-19 has drastically changed the ride patterns of Lyft riders. When evaluating this model’s performance, it was important to ensure that the model provided high-quality predictions regardless of the types of rides passengers were taking. We analyzed the performance of the model on a dataset from early 2020, before the US started sheltering in place, and on one from when the effects of COVID-19 were the strongest. One particular metric of interest to us is top 2 accuracy, which measures the percentage of rides in which the correct destination is in the model’s top 2 predictions. Our top 2 suggested destinations are highlighted in the app, so performance in these spots is especially important. Our model improved the top 2 accuracy by 8% over the existing methodology. Lyft conducted a two-week, user-split experiment across all regions comparing this new model to the previous one. Riders who were provided suggestions from the new model used them to set their destinations 3.5% more often than users still on the old model. It’s always exciting when the performance improvements seen in the offline model translate to the live system, but the work doesn’t stop here. This 3.5% increase isn’t as large as the improvement we saw offline. We believe there are optimizations we can make to the way the suggested destinations are displayed that can help us get closer to the offline results. The team is always striving to leverage our vast amounts of data to improve the Lyft experience. If you’re interested in joining, please check out our job listings . ¹ For intuition, assume that each element of the query and key follows IID Standard Normal distribution. Then, each ( q ᵢ * k ᵢ) has mean 0 and variance 1 for each i , and hence, q・ k = ∑ᵢ( q ᵢ * k ᵢ) has mean 0 and variance n . Thus, for large n , | q・ k | ~ sqrt( n ) >> 1, and the gradient of softmax( q・ K ) is ill-behaved. By dividing the dot product by sqrt( n ), we ensure that the variance of the arguments of the softmax is close to 1, leading to a better gradient behavior. Stories from Lyft Engineering. 354 Thanks to Michael Rebello . Lyft Deep Learning Attention Data Science Engineering 354 claps 354 Written by Stories from Lyft Engineering. Written by Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-11-12"},
{"website": "Lyft-Engineering", "title": "amundsen 1 year later", "author": ["Tao Feng"], "link": "https://eng.lyft.com/amundsen-1-year-later-7b60bf28602", "abstract": "Data Data Science Engineering Mobile Product Security On October 30, 2019, we officially open sourced Amundsen , our solution to solve metadata catalog and data discovery challenges. Ten months later, Amundsen joined the Linux foundation AI (LFAI) as its incubation project. In almost every modern data-driven company, each interaction with the platform is powered by data. As data resources are constantly growing, it becomes increasingly difficult to understand what data resources exist, how to access them, and what information is available in those sources without tribal knowledge. Poor understanding of data leads to bad data quality, low productivity, duplication of work, and most importantly, a lack of trust in the data. The complexity of managing a fragmented data landscape is not just a problem unique to Lyft, but a common one that exists throughout the industry. In a nutshell, Amundsen is a data discovery and metadata platform for improving the productivity of data analysts, data scientists, and engineers when interacting with data. By indexing the data resources (tables, dashboards, users, etc.) and powering a page-rank style search based on usage patterns (e.g. highly-queried tables show up earlier than less-queried tables), these customers are able to address their data needs faster. At Lyft, we knew that we built an amazing product that was essential to improving productivity after consistently observing 700+ weekly active users (WAU) and high CSAT score after Admunsen’s launch. We are also thrilled with the project’s growing user community outside Lyft, including users from, ING, Square, Workday, Asana, iRobot, Gusto, Edmunds.com, and many more. Though Amundsen is still in the LFAI incubation phase, here are several factors we’ve perceived that drive adoption: Amazing community : as mentioned, the number of community members in Amundsen’s slack workspace has grown to more than 900+ at the time of publishing. When new members join and ask questions, the community is super helpful to answer questions they’ve encountered and solved before. In addition, Mark Grover, our former product manager, has done a great job organizing a monthly community meeting to conduct knowledge sharing among different users about their user cases, adoptions, learnings, and different amazing contributions, etc. Open Governance model and new RFC process : with the hosting of the project in the LF AI Foundation, we introduced a new process for project governance and a RFC process for proposing new features. We believe these processes will help convert new and existing users into contributors and create opportunities to collaborate with other LFAI hosted projects. Rich connectors on heterogeneous resources for integration : Currently, Amundsen supports three main types of resource entities: dataset, user, and dashboard. As different companies utilize different data warehouses and BI tools, various connectors are required to fetch the resource entities. The community works together to contribute a rich list of connectors for integrations that benefits others. The full list of connectors can be found here . Pluggable architectures : The persistent layer of Amundsen is pluggable and works with the following metadata stores: 1) Neo4j integration is shipped by default; 2) Apache Atlas, another metadata framework, which supports role-based access control with Apache Ranger, and can integrate with and leverage Amundsen’s functionality. The team from ING has done an amazing job on the integration which is shared in their blog post ; 3) Gremlin back stores : many graph stores (e.g AWS Neptune) are supported by the Gremlin language. The team from Square is driving an initial integration with RFC and implementation . Extensible Metadata for other functionalities : Amundsen initially serves the data discovery use case. However, the model is easily extensible for other use cases: 1) Tracking user privacy or security . More details in the Square engineering blog post ; 2) Integrating data quality with Amundsen: More details in the Edmunds engineering blog post . Modern frontend stacks : the Amundsen frontend is a React Application written in TypeScript, leverages Redux for managing its application state, and follows the “Ducks” modular approach for code structuring. Redux-Saga was introduced as middleware to effectively manage the side effects of asynchronous requests on the application state. In addition, our stellar UX designer Knowl Baek is crafting the best UX experience for users. We believe that a great UX experience combined with a modern stack is essential to power users’ discovery experience. Amundsen is open-sourced on Lyft’s github , used by more than 20+ companies, and has a community of 900+ people in the slack workspace. However, in order to sustain contributions from existing community members, we need a neutral holding ground for the project. Thus we started to do research on contributing Amundsen to a foundation that could help share the maintenance load and help us grow. Linux foundation (LF) is one of the largest non-profit foundations to host open-source software with over 350 hosted projects and over 1400 corporate members. The LF offers a lot of flexibility around hosting a project, the choice of license, the governance structure, etc. In some cases, for large projects, the project can even be setup with its own foundation (e.g. prestodb , delta lake ) or the project can join an existing LF umbrella foundation (CNCF, LF AI, etc). The LF supports projects in a variety of ways (marketing, infrastructure, legal, community, events, compliance, etc) and recently, more data projects are being hosted in the LF under the LF AI. Amundsen is already listed as part of the LF AI landscape . Hence, we would like Amundsen to be hosted by LF as well. LF AI is an umbrella foundation under the LF focused on AI, DL, and data-related projects. Though the name AI doesn’t do justice to data, LFAI is inclusive of data. All data projects within LF are in the process of moving to LF AI. There are growing numbers of popular projects in the industry that are currently hosted in LF AI. Moreover, LF AI also provides various services for hosting projects. LF AI allows you to bring your own governance which significantly reduces maintenance overhead. In summary, we believe LF AI is a good and appropriate foundation for us to host Amundsen. Overall the project has been growing at a steady pace and with LF AI incubation, it will help unlock the next stage of project growth with wider adoption in the industry. There are many improvements and new features that the community is working which could be found in this roadmap doc. If you are interested in implementing any of the features, please propose it as RFC here. Please check out our latest website , the document homepage , and the Github repo . We look forward to the project’s continued growth and success as part of the LF AI Foundation. As always, Lyft is hiring! If you’re passionate about developing state of the art machine learning/optimization models or building the infrastructure that powers them, read more about them on our blog and join our team . Stories from Lyft Engineering. 505 Thanks to Lyft . Metadata Data Discovery Linux Foundation Amundsen Open Source 505 claps 505 Written by Apache Airflow PMC and Committer | Co-creator @Amundsen Stories from Lyft Engineering. Written by Apache Airflow PMC and Committer | Co-creator @Amundsen Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-10-06"},
{"website": "Lyft-Engineering", "title": "applying gevent learnings to deliver value to users part 4 of 4", "author": ["Shiv Toolsidass"], "link": "https://eng.lyft.com/applying-gevent-learnings-to-deliver-value-to-users-part-4-of-4-36ad932deea8", "abstract": "Data Data Science Engineering Mobile Product Security Earlier this year, Lyft launched a feature to allow users to compare bike/scooter modes against rideshare and transit modes. The goal of this project was to provide users with all the upfront information required in order to plan a bike or scooter journey, including the ETAs, routing and pricing information. After releasing the feature to our users, we noticed a glaring problem in the UX — bike and scooter modes would often flicker in and out , leading to a confusing end user experience. This post outlines how we used our gevent learnings from Posts 1 , 2 , and 3 to root cause and fix the problem. This post is relevant to anyone leveraging latency-sensitive Python microservices that have mixed CPU and I/O workloads. A 10,000 foot view of the problem When a Lyft user enters an origin and destination in the Lyft app, mobile clients send a request to our backend services, asking them to generate a set of ‘offers’ for all the transportation modes that Lyft offers in the user’s region. An ‘offer’ comprises details of the mode pertinent to the user’s journey, notably the cost, ETA and route. Our backend services compute ‘offers’ for each Lyft mode in parallel — we set a hard cap of 500ms on the time it takes to generate each offer to ensure that the end user experience is snappy. If an offer cannot be generated within these response time constraints, the offer is disregarded and the ones that have successfully been generated are returned to users. Clients poll our backend services every couple of seconds in order to keep this list of offers fresh for a user. We found that bike and scooter offer generation was taking longer than 500ms in some cases, which meant that on some client polls, our backend services rendered bike and scooter offers, whereas on subsequent polls, bike and scooter offers failed to generate within the response time constraints and were disregarded. To give you a sense of how bad this problem was, 0.5% of bike/scooter offers were failing to be generated due to timeouts, putting us at 2.5 nines of reliability — not great for one of the highest-value user experiences in the Lyft app. Our p95 latencies were doing pretty well and consistently below 450ms. However, our p99 latencies were as high as 1 second in some Lyft regions (2x our p95s latency). p999 latencies of bike and scooter offer generation were an astonishing 1.5 seconds (well over 3x our p95 latency). The question after digging into this data was ‘why is there such a high variance between our p95, p99 and p999 latencies?’ To answer this question, let’s dig deeper into how a bike or scooter offer is generated. The backend service responsible for generating Lyft offers, (let’s call it offerservice ) computes each offer in parallel. In order to compute the price of bike and scooter offers, offerservice , a Go microservice, makes an upstream request to pricingservice, a Python microservice. Here’s an ASCII data flow diagram of how a bike and scooter offer is generated: Mobile client (iOS/Android) -> offerservice (Go) -> pricingservice (Python) -> upstream microservices (Python + Go) You may be wondering why we’ve explicitly called out which languages our backend microservices are written in. This is an important detail to keep in mind and it’ll become apparent why shortly. When digging into upstream service request latencies of pricingservice , we noticed that some calls were reported as taking ~110ms by the upstream service, but pricingservice ’s instrumentation reported latencies as high as 5.3 seconds , a ~35x discrepancy. Clearly, the delta in these two sets of latencies was unaccounted for and we needed to figure out where it was being spent. We made use of distributed tracing to debug where the delta was being spent (to learn more about how Lyft set up distributed tracing, you can check out this talk ). Using our distributed traces, we found that upstream requests from pricingservice were being queued and were spending a long time waiting around. We suspected that this might have something to do with the way that pricingservice ’s web server was scheduling requests. As mentioned earlier, pricingservice is a Python microservice which leverages Flask as its web application framework. Gunicorn is the web server used to front Flask and gunicorn uses gevent to schedule multiple requests at the same time. Let’s recall that Python’s global interpreter lock effectively makes any CPU bound Python program single threaded . So, how do gunicorn and gevent serve thousands of incoming requests per second with this constraint? gevent spins up a greenlet (also known as green thread) for every request received by the microservice. These greenlets are scheduled to be run using an event loop using the cooperative multitasking framework we talked about in Part 1 . A typical greenlet for a request (let’s call this greenlet A) might do some work on the CPU, make an upstream service call (which involves blocking I/O) and finally perform some more work on the CPU. While performing blocking I/O, i.e the network request in this case, the greenlet will yield control to the event loop. Gevent can then find another greenlet (let’s call this greenlet B) to schedule on its event loop. Once greenlet B is done, if greenlet A has finished its blocking I/O, the event loop schedules greenlet A to be run again. Greenlet A completes its CPU work and is taken off the event loop. The event loop now finds the next greenlet to run. The key takeaway is that when a greenlet makes a network call, we wait not only for the call to complete, but also for the event loop to once again run our greenlet . If we run too many concurrent greenlets, CPU work for other requests can “block” our request from running. Let’s illustrate with an example (imagine that each cell represents a 10ms time slice). Our networking infrastructure would measure the latency of Request 7’s network call as 70ms, however, the application would measure it as 180ms since we waited an additional 110ms to get scheduled on the event loop. CPU work for other requests “block” our request from running. In the above example, running 3 concurrent requests would be nearly perfect: Now, how does this relate to pricingservice and the high response times we were seeing earlier? pricingservice houses hundreds of different endpoints that have varying levels of CPU and I/O workloads . Some endpoints are CPU intensive, whereas some (such as the endpoint that prices bike and scooter offers) are I/O heavy (since they call several upstream services). The greenlets for CPU intensive endpoints in pricingservice were blocking greenlets for I/O heavy requests from being run. This was leading to the bike and scooter pricing endpoint’s greenlets being queued up and waiting longer to be scheduled by the event loop. To solve this problem, we decided to run this endpoint on dedicated application service processes so that we’d get dedicated gunicorn processes to field our I/O heavy bike and scooter pricing requests. These requests and their greenlets would no longer need to contend with CPU intensive greenlets from other types of requests. Using tooling built by our infrastructure teams, we were able to run our bike and scooter pricing endpoints on dedicated processes and we pointed downstream services at these dedicated hosts for all bike and scooter pricing requests. Once we rolled these changes out, we got exactly the results that we were hoping for! Timeouts for requests to our bike and scooter pricing endpoint stopped immediately, which led to the flickering we described earlier stopping. Reliability of bike and scooter pricing requests leaped from an average of 2.5 to 4.5 nines — a massive improvement! The key takeaway for readers is that running a high throughput Python service (which leverages gevent) that runs mixed CPU and I/O workloads might lead to CPU intensive requests starving I/O intensive requests. We recommend employing distributed tracing to dig into request lifecycles and employing the recommendations from this gevent series to improve your Python service’s latencies and reliability. Hopefully you’ve found this 4-part series on gevent to be useful and we hope that you’ll be able to leverage these learnings to deliver value to your users like we were able to. This post wouldn’t have been possible without help from Roy Williams, David Quaid and Justin Phillips. Additionally, a huge thanks to Garrett Heel, Ilya Konstantinov, Daniel Metz, and Jatin Chopra for their advice and for reviewing this post. Lyft is hiring! If you’re interested in improving peoples lives with the world’s best transportation, join us ! Stories from Lyft Engineering. 65 Python Gevent Programming Bugs Engineering 65 claps 65 Written by Trying my best to not take down prod @ Lyft. Stories from Lyft Engineering. Written by Trying my best to not take down prod @ Lyft. Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-11-12"},
{"website": "Lyft-Engineering", "title": "diving into data science", "author": ["Diversity in Science at Lyft"], "link": "https://eng.lyft.com/diving-into-data-science-7a24a08a2332", "abstract": "Data Data Science Engineering Mobile Product Security Meet Alya , Mike , and Troy ! They are some of our talented Data Scientists who joined the DS space through less traditional paths. In this Q&A, they open up about how they got to this point in their careers, and share some advice to others looking to make the switch. Q) How did you get here? How do you feel this path equipped you with the skills necessary to succeed in Data Science? Until I moved to California from Boston in 2014, I was unfamiliar with the term “Data Science.” I picked up the skills of a data scientist by following my interests and learning as I went along, and was lucky to find that those skills were in high demand. I did my PhD in theoretical linguistics, intending to go into academia. My thesis was on syntactic phenomena in Uyghur, Russian and Faroese, and had nothing to do with Natural Language Processing (NLP). However, linguistics is a tiny field with a very tough job market. I ended up working at MIT Lincoln Lab after graduation, doing applied research in crowdsourcing, NLP and machine learning. Lincoln Lab hires based on the ability and willingness candidates to figure things out, which a PhD is great preparation for. As I didn’t have a lot of relevant experience to draw on while at Lincoln Lab, I was constantly learning and developing. While relevant prior experience in my work was minimal, it was an amazing place to learn. After a couple of years at MIT Lincoln Lab, I moved from Boston to California and started looking around for what to do next. This is when I realized that in the Bay Area, what I had been doing at Lincoln Lab was called data science (in 2014, the term hadn’t made it to Boston yet!). I took a job as a data scientist, gradually transitioning from an individual contributor (IC) to management. I feel incredibly lucky to have found an area with so many fascinating problems and opportunities. Q) Are there things that surprised you or really challenged you once you made the transition? What surprised me most when I moved into industry was just how much collaborative effort is required in order to ship a product. Even something that seems like a “small” feature with a data science model at its core can require contributions from Product, Engineering and Design in addition to Data Science, as well as collaboration with other teams. This is very different from the research work I saw in my PhD and at Lincoln Lab, where one person — or a small handful of technical experts — would complete a project end-to-end. It gave me a sense for why companies emphasize communication skills so much for highly technical roles. Q) What about our Data Science organization drew you to Lyft? Even before I started interviewing at Lyft, I had heard about the collaborative, mission-driven culture at the company, and the interview process was a great way to learn more. The focus on making a positive impact attracts people who are passionate about what they do; it’s fun being at a place where people work hard and help each other make things happen. From the interviews, I learned about the huge variety of data science problems that Lyft tackles. It’s been a great place to work, with so many smart people from a variety of backgrounds to learn from. Q) Lastly, what is the best advice you could give to someone hoping to pivot into Data Science? Find ways to get both theoretical and practical experience. Figure out a problem that excites you that could be solved with data science tools and try to solve it. Actually make it work, but also read enough theory to understand why it’s working — what’s happening under the hood? Why did some approaches you tried fail? Throughout my career, the main resource I’ve turned to is the people around me. Whenever you’re trying to do something for the first time and are feeling unsure, talk to people who’ve done it before! This applies to technical work, but also to non-technical skills like project/time management or navigating relationships to get something done at the company you’re at. People are generally very generous with sharing their experience; even if you don’t know them well, just ask! Q) How did you get here? How do you feel this path equipped you with the skills necessary to succeed in Data Science? My path to working as a data scientist at Lyft started with my fascination with using and improving public transit. The data science aspect came later. I was born, raised, and currently live in, Brooklyn. I started commuting by subway in high school, and have never owned a car (nor do I plan to, despite having two small children). After early career experiments around different applications of software engineering, I sought a way to marry my technical skills with personal priorities around dense cities and the many economic, social, and cultural benefits they bring. I honed in on Public Transportation (known as Transit at Lyft) as an industry to focus on, found the Transportation program at MIT, and was able to dust off the mathy side of my brain while doing data-driven research with large transit agencies in New York and London. I ended up working in Product and Engineering for NYC’s real-time bus tracking system and then on the largest bikeshare systems in America. That is, not much data science per se . When Lyft was opening its office in NYC, I was invited to interview to build and lead the Data Science team there as a function of (I believe) my academic training, deep transportation industry background, and cross-functional leadership experience. The work at Lyft - especially as Lyft has gone all-in on transit, bikes, and scooters - has proven to be a perfect way to leverage those skills. What I learned in my study of the rich literature around ‘traditional’ transportation systems modeling turns out to apply directly to rideshare and micromobility. My experience in previous roles helps me collaborate effectively with our cross-functional partners, which is a key requirement for any Data Scientist. Even my time at a century-old public sector infrastructure and operations bureaucracy helped me learn how to navigate large organizations, to be deeply empathetic to people with very different responsibilities, and to communicate my technical work in terms others can easily understand. Q) Are there things that surprised you or really challenged you once you made the transition? While it seems obvious in retrospect, I was surprised how important the analytical work to drive human decision-making at Lyft was. I had imagined that the lion’s share of Data Science work was to build the automated models in the product, but I have come to really appreciate how critical Data Science can be for driving strategy, product roadmap, and tactical business decision-making. Q) What about our Data Science organization drew you to Lyft? Before the Data Science organization, I was first attracted to Lyft because it shares my personal mission around transportation for livable cities . Then, the fact that Lyft is pursuing these goals through the use of technology and data played to my technical strengths. I felt that I had a tremendous amount to learn from the company, while being able to provide a unique perspective given my background in other transportation contexts. Coming to Lyft from outside the broader Tech industry and as a not-so-young person (with kids), I was a little afraid that I might be an outlier on the team. However, the very first piece of content in day 1 of onboarding highlighted how many of the 90 people in the onboarding class talked about their children. It immediately made me feel right at home . Q) Lastly, what is the best advice you could give to someone hoping to pivot into Data Science? At Lyft we expect Data Scientists to “continuously earn a seat at the table through thought leadership and deep collaboration.” We are knowledge-workers in the purest form (i.e. creating knowledge), so none of the work matters if our colleagues don’t consume, understand, and believe it. This is about both which work you do — which problems are you going after to drive the most benefit — and also about how you communicate the results. Double down on thinking about your audience, understanding their needs, and delivering your work and insights in a way that speaks to them. Q) How did you get here? How do you feel this path equipped you with the skills necessary to succeed in Data Science? My path to Data Science was a little circuitous but I think it prepared me well. After studying Computer Science and Economics in undergrad, I worked at a quant hedge fund where we used Python pandas every day. I didn’t know it at the time but a whole new field called Data Science would soon be created around such tools. After a year, I left finance to work as a backend software engineer at a startup. I learned a lot about software development processes there. Throughout this time I built (and still build) software tools, which is where I learned about the importance of analytics in building software that users love. While I enjoyed building software, I also liked finding insights in data. I found I wanted to influence — and even make — higher level product or strategic decisions, so I left my software engineering job and spent a year “exploring.” During that year, I freelanced as a software engineer with data chops, tried to start a company with friends (twice, neither worked out), and explored both Product Management and Data Science job openings. By now Data Science had exploded in popularity. I decided that Data Science was a good fit for me, and here I am! Q) Are there things that surprised you or really challenged you once you made the transition? When I first transitioned into Data Science, I was most surprised by how much Data Scientists (at least product-oriented ones) rely on SQL and A/B testing on the job. For example, while I was familiar with programming languages like Python and had used pandas a lot, I never had to learn SQL before becoming a Data Scientist. SQL wasn’t taught in my Computer Science classes in college. Looking back, all the SQL practice I get every day has really shown me how being fluent in SQL helps you become much better at both thinking about and wrangling data. Even though I minored in statistics and understood the theory behind A/B testing, it was nothing like on-the-job experience and learning from my coworkers about the important details of actually applying A/B tests to a product. Examples include the need to define both primary and secondary or “guardrail” metrics, doing power analyses where sampling occurs over time to estimate A/B test length, and why changing variant allocations or shocks like marketing campaigns in the middle of an A/B test can bias results. Q) What about our Data Science organization drew you to Lyft? I really liked how data-driven Lyft seemed to be, given the Data Science blog posts I read from Lyft and also what I learned from my Lyft interviews and interviewers. Data Science is treated as a critical function here. We have internal tools and even teams dedicated to democratizing data and data-driven decisions, and a culture that not only sees data as something to report on but also as a competitive advantage, core to both the product and to our strategic decisions. What put Lyft above other companies in my mind was the inclusive and diverse environment that the company cultivates through action. For example, this blog, and this specific post, is a manifestation of just that because it showcases people from diverse backgrounds — in not only gender, ethnicity, age, etc. but also life paths — and their unique insights. Q) Lastly, what is the best advice you could give to someone hoping to pivot into Data Science? My advice for someone hoping to pivot into Data Science is to prepare early and well. Being a Data Scientist requires a lot of different kinds of skills, both technical and non-technical. Understand what Data Scientists do and develop and practice those skills. Demonstrate those skills through your current day job. And if that’s not possible, create separate opportunities for yourself to showcase those skills — like through doing side projects — and put them on your resume. Make it easy for your future employer to have confidence that you can do the job well. Of course, interview performance is critical too, so prepare well for that. Reach out to data scientists and talk to them about their experiences and the broader interview structure. Brush up on relevant topics and find some practice problems online. You can even record yourself explaining your answers and giving responses, then make adjustments to how you want to present your problem solving process, your background, etc. We hope these team members’ stories inspire you to pursue Data Science if that is where your passion lies, irrespective of your background! If you are interested in joining our incredible team of Data Scientists, check out our careers page ! A huge thanks to the following people for their great contributions to this post (in alphabetical order): Eric Smith, Polly Peterson, Sarah Morse, Simi Mirchandani Stories from Lyft Engineering. 216 Thanks to Lyft . Data Science Nontraditional Transportation Inclusion Diversity In Tech 216 claps 216 Written by Stories from Lyft Engineering. Written by Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-08-28"},
{"website": "Lyft-Engineering", "title": "building accessible web experiences at lyft", "author": ["Antonio Clarke"], "link": "https://eng.lyft.com/building-accessible-web-experiences-at-lyft-185ad9328c6f", "abstract": "Data Data Science Engineering Mobile Product Security tl;dr: Test your web apps with both keyboards and screen readers, learn to love the WAI-ARIA guidelines, and always use accessible HTML elements when possible. At Lyft, we have a large and diverse group of riders and drivers, and it’s important for us to aim to build experiences that are accessible to as wide of an audience as possible. In this blog post, we’ll dive into how we build accessibility into our web apps through a case study of a new feature on our web platform for riders . Lyft users have the ability to create email reports of their rides, for use-cases such as personal budgeting or business expensing. In our native app, this is done on the following screen: For the MVP of this feature on our web platform, we opted for a mobile-first approach that maintained an identical UI with our app for small screens and simply scaled the UI for larger screens. At Lyft, we work with a modern front-end tech stack featuring React Hooks and TypeScript, so the implementations shown in this post will echo that. Note: This post will focus primarily on the accessibility aspects of this component, and a basic understanding of both TypeScript and React Hooks will greatly help in understanding this post. This screen features a multi-selectable, infinitely scrollable list of all of the user’s past rides. Setting aside the infinite scroll implementation (which deserves a blog post in itself) we’ll start with the base for building the multi-selectable list for mouse users . Assuming we have a RideListEntry component that renders the details of a single ride, our list of rides could look something like this: Next, we need to keep track of the currently selected rides, which we will store with React’s state . Because this state is used not only for indicating which rides are currently selected, but also for showing a summary at the bottom of the screen with the number of rides selected and their total cost, we lift its state up to their parent component and then pass it down with props. Our updated component ends up looking like: Finally, to make rides selectable, we receive an additional prop — a function that takes a Ride as input and updates the list of selected rides accordingly. Because all of the styling is already handled in the RideListEntry component, and <button> elements tend to come with unwanted, unaesthetic styles, we may be tempted to add a simple wrapper <div> around each of them and call it a day: With the click handler added, our code works! Kind of. Although we can now select rides to generate a report, none of them are able to be navigated to or selected by the keyboard, making this feature impossible to use with the keyboard . The first step in improving our experience is doing this type of testing and recognizing when you have a problem. Key Takeaway #1: Test your web apps with both keyboards and screen readers. We’ll focus on keyboards for now and get to screen readers later. The problem lies in using a <div> element for our wrapper click-handler element. By using a <div> to simplify our styling, we miss out on a variety of accessibility features that a <button> gives us for free, most significantly in this case, keyboard navigation. For many common of interactive interfaces, such as dropdown selectors, forms, and buttons, the browser’s default elements do a lot of heavy lifting behind the scenes, so take advantage of them whenever possible. Use them whenever possible. Key Takeaway #2: Always use accessible HTML elements when possible With that in mind, let’s try to correct our earlier faux-pas and use a <button> with a bit of CSS to improve its appearance. At Lyft, we use styled-components , and a simple implementation might look something like this: With just a few lines of CSS, we now have all of the benefits of a <button> with the styling we want! Score! Now that we’re using a <button> we can navigate through the list of rides using the Tab key and can select and de-select them using the Enter key, great! We can call it a day, now right? Not exactly. Because this list uses infinite scroll for all of the user’s rides, we’d have to Tab through every ride we’ve ever taken to get to the “Send report” button. Yikes! Alternatively, we can use Shift + Tab to navigate backwards through the entire page until we reach the Send button, but that also isn’t a great user experience. It looks like maybe the <button> element won’t work for us after all. Well, at least we tried. At this point, we might wonder how the keyboard interaction should work for this module in general. Clearly navigating through the entire list with Tab isn’t the way to go, but how do we decide what is? When building an interactive user interface and unsure how to handle keyboard interaction, look no further than the WAI-ARIA authoring practices . This document features rich and detailed explanations for building web applications accessible to both keyboards and screen readers. What we’re interested in specifically is the section on design patterns and widgets . The third key takeaway from this guide is to: Key Takeaway #3: Learn to love the WAI-ARIA guidelines The first time reading this document, you might be overwhelmed. Which of their listed design patterns should I be following? Sometimes the answer is obvious — for example, “Button” and “Radio Group” are fairly self-explanatory, however others like “Spinbutton” and “Treegrid” might not be. To get started on the right track, we can let Google do some heavy lifting for us. We know that we’re working on a multi-selectable list of items, so let’s see what shows up with a search for that. Since the “Listbox” design pattern showed up, let’s take a look at it. It is described in the WAI-ARIA document as follows: A listbox widget presents a list of options and allows a user to select one or more of them. A listbox that allows a single option to be chosen is a single-select listbox; one that allows multiple options to be selected is a multi-select listbox. A multi-select listbox sounds like just what we need! Now we can read the keyboard interaction sub-section of the Listbox section and identify the ideal case for keyboard interaction: When a multi-select listbox receives focus: If none of the options are selected before the listbox receives focus, focus is set on the first option and there is no automatic change in the selection state. If one or more options are selected before the listbox receives focus, focus is set on the first option in the list that is selected. Down Arrow: Moves focus to the next option. Up Arrow: Moves focus to the previous option. Space: changes the selection state of the focused option. Based on the WAI-ARIA document, there’s a lot of keyboard interaction we have to build, and these are just its base recommendations — there are many more key combinations it suggests supporting! As mentioned earlier, default HTML inputs should be used whenever possible, and this added complexity highlights the first pain point of building your own custom inputs: Pain Point #1 of Custom Inputs — Keyboard Support Focus is an indicator of the element that is currently able to be interacted with by the keyboard and is typically moved by pressing the Tab key, as we saw with <button> elements earlier. However, based on our learnings from the WAI-ARIA document, we don’t want all of our elements to be focusable at once, so our first step is going to be sacrificing our <button> and going back to a <div> Now we’re back where we started and none of our rides are reachable with the keyboard. Enter tabindex tabindex ( or tabIndex in React) is an HTML attribute used to determine an element’s position in the order of focusable elements. It accepts a numerical value, but generally only values of 0 or -1 should ever be used . The simplest way of using tabIndex is with a value of 0, which simply adds the element to the page’s natural tab order, whereas a tabIndex of -1 makes an element programmatically focusable , but not part of the page’s tab order. And with that, all of our rides are tab-able… trapping us inside the infinite scrollable list and re-introducing the same problem we had with our <button> As outlined by the WAI-ARIA document, rather than each ride being tab-able, only one ride should be tab-able and the rest should be reachable using the Up and Down arrow keys. Let’s start with only making a single ride tab-able and keeping track of the tab-able ride with the State hook . Next, we listen for the Up and Down arrow keys and update the state of the currently tab-able ride, making sure not to ignore the Down key if we’re at the end of the list and the Up key if we’re at the start of the list. Note: We store all of the key codes in a constants file for easy re-use throughout our web app. If you don’t have something similar within your codebase, you can find the key code on this list . One “gotcha” here is that although we’ve updated the tabIndex, changing the ride which is tab-able, we haven’t actually moved focus from the current ride. To actually update the currently focused element, we’ll need to call the .focus() method of its underlying DOM node directly. In React, DOM elements are typically accessed using refs . Because we need to be able to programmatically focus on any of the rides, we create a ref for each ride and call its .focus() method in our onKeyDown handler. Now we can press Tab to enter the list of rides, navigate amongst them with the Up and Down arrow keys, Shift + Tab out of them, and then Tab back to the previously focused ride. Note: this implementation deviates slightly from the WAI-ARIA document, which recommends setting focus on the first selected element when tabbing back into a listbox. Lastly, let’s add the ability to select rides with the Space key per the WAI-ARIA document. It’s as simple as adding an extra case to our onKeyDown handler and passing the currently focused ride into our onRideSelect handler. With keyboard support working well, let’s give our component a run-through with a screen reader and see how it performs. For the purposes of this post, we’ll be using VoiceOver (VO), which is included in macOS by default. Some basic VO commands: Command + F5 — Starts VO Ctrl + Option + Left Arrow / Right Arrow — Navigates through content Apologies to all non-Mac users, but you should be able to follow along with a Windows-supported screen reader. Navigating through the page with VO, this is what we observe: Oh no! Does that behavior look familiar? Just like when we started with every ride being included in the page’s tab order, we have to walk through every ride with the screen reader to get to the “Send” button. This walkthrough highlights the second major pain point with building your own custom inputs. Pain Point #2 of Custom Inputs — Screen Reader Support Thankfully, with a little effort we can make a big improvement in our screen reader experience using ARIA roles and attributes. How many developers have seen an aria-* attribute in a codebase before but not really understood what it did? Or perhaps have tangled with a linter warning telling you to add a role to your interactive <div> ? I confess, both of those statements applied to me before my work on accessibility at Lyft. If you feel overwhelmed, don’t worry! Once again, the WAI-ARIA document comes to the rescue with its sub-section on WAI-ARIA roles, states and properties for the listbox design pattern we’ve been following. The basics are listed below: 1. An element that contains or owns all the listbox options has role listbox . 2. Each option in the listbox has role option and is a DOM descendant of the element with role listbox or is referenced by an aria-owns property on the listbox element. 3. If the listbox is not part of another widget, then it has a visible label referenced by aria-labelledby on the element with role listbox . 4. The element with role listbox has aria-multiselectable set to true . 5. All selected options have aria-selected set to true . 6. All options that are not selected have aria-selected set to false . This might seem like a lot at first, but with just a few lines of code in this file, we can handle all of this! The code All that’s left is making sure the ID referenced by the aria-labbeledby attribute matches ID used by the listbox’s label — in this case, the page’s header. Our header lives in a separate component, and we update its ID below: With the ARIA attributes added, let’s try walking through this component with VO again. With all of these changes: The entire list of rides is its own widget in the screen reader that we can enter / exit Each ride in the list announces its position and the list and whether or not it is selected The list itself announces how many rides are selected followed by the details of those rides Is it perfect? No. Is it significantly better than what we started with? Yes! With keyboard and screen reader support handled, let’s go back to the most common use-case: interactions with a mouse. Looking back to our GIFs of click interactions, there’s something out of place: Can you see it? There’s no hover state to indicate that these rides are selectable! When we hover over the buttons, the cursor changes to indicate they are clickable, but nothing similar happens for the rides. This brings up the last, but easiest to solve, pain point of building custom inputs: Pain Point #3 of Custom Inputs — Mouse Support Going back to our styled-components implementation from earlier, we add the cursor’s hover state to our selectable <div> Whew, that was a lot. After all of our effort, we have successfully: Built our feature for the base-case of mouse support Supported keyboard users Supported screen reader users We built a much more accessible feature than we started with! We learned some key takeaways along the way, and they’re worth reiterating: Key Takeaway #1: Test your web apps with both keyboards and screen readers. Key Takeaway #2: Always use accessible HTML elements when possible Key Takeaway #3: Learn to love the WAI-ARIA guidelines The caveat to #2 is that sometimes (like in this case) there is no accessible HTML element that does exactly what you want it to do, and when that happens, be wary of the pain points of building custom inputs: Pain Point #1 of Custom Inputs — Keyboard Support Pain Point #2 of Custom Inputs — Screen Reader Support Pain Point #3 of Custom Inputs — Mouse Support There were a lot of steps in this blog, so let’s recap the process we followed today: Built the feature itself Tested it with a mouse, success! Tested it with a keyboard and realized it wasn’t usable Tried using a native HTML element, but <button> also didn’t fit the use case Identified the ideal state for keyboard interaction from the WAI-ARIA authoring docs Implemented key interaction for focus management and key handlers Re-tested it with a keyboard Tested it with a screen reader and realized it wasn’t usable Identified the recommended ARIA roles, states, and properties from the WAI-ARIA authoring docs Implemented the ARIA roles, states, and properties Re-tested it with a screen reader, success! Re-tested it with a mouse … something’s not right. Added hover states Re-tested it with a mouse, and we’re good to go! I’ve found that when making interactive widgets accessible in web apps, my process often ends up looking quite similar to this. Give it a try on your next web app and see how it goes! Thanks to Joanne Deng for proof-reading this post and to Eric Bidelman for reviewing the PRs related to this post. If you are interested in an internship with Lyft and making more accessible products for all, check out our careers page ! Stories from Lyft Engineering. 294 React Accessibility Front End Development Web Development Internships 294 claps 294 Written by Stories from Lyft Engineering. Written by Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-09-02"},
{"website": "Lyft-Engineering", "title": "a new real time map matching algorithm at lyft", "author": ["Marie Douriez"], "link": "https://eng.lyft.com/a-new-real-time-map-matching-algorithm-at-lyft-da593ab7b006", "abstract": "Data Data Science Engineering Mobile Product Security By Marie Douriez, James Murphy, Kerrick Staley When you request a ride, Lyft tries to match you with the driver most suited for your route. To make a dispatch decision, we first need to ask: where are the drivers? Lyft uses GPS data from the drivers’ phones to answer this question. However, the GPS data that we get is often noisy and does not match the road. To get a clearer picture from this raw data, we run it through an algorithm that takes raw locations and returns more accurate locations that are on the road network. This process is called map-matching . We recently developed and launched a completely new map-matching algorithm and found that it improved driver location accuracy and made Lyft’s marketplace more efficient. In this post, we’ll discuss the details of this new model. Map-matching is a process that maps raw GPS locations to road segments on a road network in order to create an estimate of the route taken by a vehicle. At Lyft, we have two main use cases for map-matching: At the end of a ride, to compute the distance travelled by a driver to calculate the fare. In real-time , to provide accurate locations to the ETA team and make dispatch decisions as well as to display the drivers’ cars on the rider app. These two use cases differ by their constraints: in real-time, we need to be able to perform map-matching quickly (low latency) and with the locations available up to the current time only. At the end of a ride however, the latency requirements are less stringent and the whole history of the ride is available to us (allowing us to work “backwards” from future locations). As a result, End-Of-Ride Map-Matching (EORMM) and Real-Time Map-Matching (RTMM) are solved using slightly different approaches. In this post, we will focus on algorithms used for Real-Time Map-Matching. A bad map-matched location leads to inaccurate ETAs, then to bad dispatch decisions and upset drivers and riders. Map-matching thus directly impacts Lyft’s marketplace and has important consequences on our users’ experience. There are several main challenges to map-matching. First, as Yunjie noted in his blog post , the location data collected from the phones can get very noisy in urban canyons (where streets are surrounded by tall buildings), around stacked roads, or under tunnels. Those areas are particularly challenging for map-matching algorithms and are all the more important to do correctly since many Lyft rides happen there. Beyond noise and road geometry, another challenge is the lack of ground truth : we don’t actually know the true locations of the drivers when they are driving and we have to find proxies to evaluate the accuracy of our models. Finally, the performance of the map-matching algorithms relies on the quality of the road network data . This problem is being solved by another team within Mapping at Lyft; see Albert’s blog post to learn how we make our internal map more accurate. We won’t go into all of the techniques for solving map-matching, but for a review of common approaches, please refer to this study by Quddus et al. [1]. A nice way to frame the problem is to use state space models . State space models are time series models where the system has “hidden” states that cannot be directly observed but give rise to visible observations. Here, our hidden states are the actual positions of the car on the road network that we are trying to estimate. We only observe a modified version of the hidden states: the observations (raw location data). We assume that the state of the system evolves in a way that only depends on the current state (Markov assumption) and further define a hidden-state-to-hidden-state transition density and a hidden-state-to-observation density. A commonly used state space model for map-matching is the discrete-state Hidden Markov Model (Newson & Krumm [2], DiDi’s IJCAI-19 Tutorial [3], Map Matching @ Uber [4]). In this system, we generate candidates by looking at the closest points on the road segment and use the Viterbi algorithm to find the most likely sequence of hidden states. However, the Hidden Markov Model (HMM) has several limitations: It is relatively inflexible to different modeling choices and input data It scales badly (O(N²), where N is the number of possible candidates at each state) It can’t cope well with high(ish) frequency observations (see Newson & Krumm [2]) For these reasons, we developed a new real-time map-matching algorithm that is more accurate and more flexible to incorporate additional sensor data. Let’s first review the basics of the Kalman filter . (Read how Marguerite and her team used Kalman filters to estimate the seasonality of a market in this blog post .) Unlike the discrete-state HMM, the Kalman filter allows for the hidden state to be a continuous distribution. At its core, the Kalman filter is simply a linear Gaussian model and models the system using the following equations: Using these equations, the Kalman filter iteratively updates its representation of the system’s state using a closed form predict-correct step to go from step t-1 posterior to step t posterior. One limitation of the Kalman filter, however, is that it can only handle linear problems. To deal with non-linearities, generalizations of the Kalman filter have been developed such as the Extended Kalman Filter (EKF) and the Unscented Kalman Filter (UKF) [5]. As we’ll see in the next section, our new RTMM algorithm uses the UKF technique. For the rest of the post, the technical differences between the Kalman filter and the UKF don’t really matter: we can simply assume that the UKF works like a standard linear Kalman filter. Let’s now describe how our new RTMM algorithm works. We will refer to it as a Marginalized Particle Filter (MPF). At a high level, our MPF algorithm keeps track of multiple “particles”, each representing a position on a trajectory on the road network, and runs an unscented Kalman filter conditioned on each of these trajectories. To be more precise, let us define the following objects: An MPF state is a list of particles. A particle represents one possible road position of the car on the map, associated with some probability. Each particle has 4 attributes: A probability p ∈ [0,1] A trajectory (i.e. a list of intersections from the map) A mean vector x = [d v]’ where d is the position of the car on the trajectory (in meters) and v is the car’s velocity (in meters/second) A 2x2 covariance matrix P representing the uncertainty around the car’s position and velocity We update the MPF state each time we receive a new observation from the driver’s phone in the following way: If the previous MPF state has no particle (for example, if the driver just logged in to the app), we need to initialize a new one. The initialization step simply “snaps” the GPS observation onto the map and returns the closest road positions. Each particle’s probability is computed as a function of its distance to the observation. At the next update (new observation), we iterate through our state’s (non-empty) list of particles and perform two steps for each particle. First, the trajectory extension step finds all the possible trajectories from the particle’s current position on the road network. Second, we loop through these new trajectories and on each of these, we run our UKF and update the newly created particle’s probability. After these two nested loops, we end up with a new list of particles. To avoid keeping track of too many of them in our MPF state, we simply discard the most unlikely ones ( pruning ). The downstream teams can then decide to use the most probable particle from the MPF state as the map-matched location or can directly exploit our probabilistic output (e.g. to create a distribution of possible ETAs). To recap, the Marginalized Particle Filter maintains a set of particles representing possible positions of the car on trajectories and each particle is updated using the Kalman filter algorithm. The new algorithm provides not only location but also speed estimates and uncertainty. In practice, we have observed that it yields more accurate map-matched locations than the HMM, in particular in downtown areas and around intersections where errors would lead to very inaccurate ETAs. After experimenting with this new real-time map-matching algorithm, we found positive effects on Lyft’s marketplace . The new model reduced ETA errors, meaning that we could more accurately match passengers to the most suited driver. It also reduced passenger cancels, showing that it increased passengers’ confidence that their drivers would arrive on-time for pickup. We’re not done yet, though: there are many ways that we plan to improve this model in the coming months. We’re going to incorporate data from other phone sensors, such as the gyroscope, which will allow us to detect when drivers turn. We also plan to take into account the driver’s destination (if they have one, e.g. when en route to a pickup) as a prior. Indeed, another strength of the Marginalized Particle Filter is that it allows us to easily add these new types of information to the model in a principled way, and it is a good foundation on which we can continue to make the Lyft experience a little more seamless for our passengers and drivers. Special thanks to the entire Locations team for helping us put this model into production! As always, Lyft is hiring! If you’re passionate about developing state-of-the-art quantitative models or building the infrastructure that powers them, read more about our Science and Engineering roles and reach out to us . Este artículo también está en español: eng-espanol.lyft.com [1] Quddus M., Ochieng W. & Noland R., “Current map-matching algorithms for transport applications: State-of-the art and future research directions,” Transportation Research Part C: Emerging Technologies, 15(5), 312–328, 2007. [2] Newson P. & Krumm J., “Hidden Markov map matching through noise and sparseness,” Proceedings of the 17th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems GIS 09, 336, 2009 [3] DiDi’s IJCAI-19 Tutorial: Artificial Intelligence in Transportation (slides 28–40) [4] Map Matching @ Uber [5] E. A. Wan and R. Van Der Merwe, “The unscented Kalman filter for nonlinear estimation,” Proceedings of the IEEE 2000 Adaptive Systems for Signal Processing, Communications, and Control Symposium (Cat. №00EX373), Lake Louise, Alberta, Canada, 2000, pp. 153–158. Stories from Lyft Engineering. 1.3K 2 Thanks to Kerrick Staley , Nicholas Chamandy , Smorse , and Michael Rebello . Data Science Data Engineering Mapping Geolocation 1.3K claps 1.3K 2 Written by Stories from Lyft Engineering. Written by Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-11-12"},
{"website": "Lyft-Engineering", "title": "2020 computer vision conferences cvpr", "author": ["Peter Ondruska"], "link": "https://eng.lyft.com/2020-computer-vision-conferences-cvpr-655de22a6714", "abstract": "Data Data Science Engineering Mobile Product Security ECCV is just around the corner, but before that, let’s take a look at the interesting papers we saw at CVPR. The Conference on Computer Vision and Pattern Recognition (CVPR) is one of the top tier computer vision conferences, and was held virtually in June this year. Over 1,467 papers were presented, marking its largest edition to date. Read on for our hand-picked list of papers most relevant to self-driving. Point-GNN: Graph Neural Network for 3D Object Detection in a Point Cloud Paper from Carnegie Mellon University — Graph neural networks (GNNs) are establishing themselves as state-of-the-art architecture for many tasks due to its flexibility and performance. In this work, the authors propose using GNNs for object detection in point clouds and show that these can achieve competitive performance with regard to current best solutions. Predicting Semantic Map Representations From Images Using Pyramid Occupancy Networks Paper from University of Cambridge — Birds-eye-view (BEV) image representation of scenes is a flexible way to capture context around the vehicle for ground motion prediction and driving decisions. This paper proposes constructing these representations directly from raw camera inputs. RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds Paper from University of Oxford — The authors propose a new way to segment 3D point clouds that performs up to 200x faster than current best methods while also improving accuracy. PnPNet: End-to-End Perception and Prediction With Tracking in the Loop Paper from Uber ATG — Authors show that detecting objects from sensors and tracking their position over time are two related problems that if solved jointly lead to improvement in both. MotionNet: Joint Perception and Motion Prediction for Autonomous Driving Based on Bird’s Eye View Maps Paper from Rutgers University — This paper is similar to the one above, but achieves a similar point without the need of semantic map information. Train in Germany, Test in the USA: Making 3D Object Detectors Generalize Paper from Cornell University — Authors of this paper study the ability of detectors to differentiate between datasets collected in different regions. They show that accounting for varying vehicle sizes in different regions significantly boosts this ability. Evaluating Scalable Bayesian Deep Learning Methods for Robust Computer Vision Paper from ETH Zurich — Adding uncertainty quantification to the outputs of machine learning-based perception systems makes them more dependable. This paper compares different methods to measure epistemic uncertainty. VectorNet: Encoding HD Maps and Agent Dynamics From Vectorized Representation Paper from Waymo — This paper proposes the use of graph neural networks instead of convnets to model motion forecasting tasks. This leads models that perform an order of magnitude faster. Exploring Data Aggregation in Policy Learning for Vision-Based Urban Autonomous Driving Paper from University of Tubingen — Training end-to-end neural networks to drive a car can be achieved by first collecting sample driving data from experts, and then further querying expert supervision on demand . This work improves the DAgger schema, establishing a new best result on CARLA benchmark. Attentional Bottleneck: Towards an Interpretable Deep Driving Network Paper from Waymo — This paper combines visual attention mechanisms with neural networks trained to drive the car. This allows us to understand what the network is paying attention to when making decisions. D3VO: Deep Depth, Deep Pose and Deep Uncertainty for Monocular Visual Odometry Paper from TUM and Artisense — This work presents the new state-of-the-art in monocular visual odometry (VO). This is achieved by formulating three key elements of the VO pipeline as machine learning problems and employing neural networks to model each. Learning Multi-View Camera Relocalization With Graph Neural Networks Paper from UISEE Technology — Authors model camera re-localisation using a combination of CNNs and graph neural networks. This allows propagation of information between map and query images to determine pose of query images. As a result, this method outperforms other learning-based camera localization methods. Bundle Adjustment on a Graph Processor Paper from Imperial College London — The presented system shows that Bundle Adjustment, one of key components of SLAM, can be solved efficiently using a message passing algorithm. Moreover, it can be implemented efficiently using graph processors, resulting in significantly faster performance compared to usual CPU solvers. SurfelGAN: Synthesizing Realistic Sensor Data for Autonomous Driving Paper from Waymo — This paper introduces methods to simulate camera and lidar measurements from existing lidar + camera scenes, but from a different vehicle position. To achieve this, the authors propose a new GAN network to fill the gaps and remove noise. LiDARsim: Realistic LiDAR Simulation by Leveraging the Real World Paper from Uber ATG — Unlike the paper above, this paper presents a procedural way to synthesize new unseen lidar scenes. To achieve high-realisticity, the paper combines existing real-world observations, as well as a way to realistically synthesize new objects and corresponding lidar measurements. Scalability in Perception for Autonomous Driving: Waymo Open Dataset [link] Dataset from Waymo — This dataset contains 1,150 lidar + camera scenes of 20 seconds each with annotations from diverse locations. The dataset is aimed to support the study of generalisation performance for perception and tracking tasks across different regions. nuScenes: A Multimodal Dataset for Autonomous Driving [link] Dataset from nuTonomy — The dataset contains 1,000 scenes of 20 seconds each, with data from 6 cameras, 5 radars and 1 lidar, paired with human annotations and semantic maps. The database is aimed to support development of multi-modal perception and tracking systems. BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning [link] Dataset from UC Berkeley — The dataset contains 100,000 dash-cam videos of 40 seconds each from diverse locations, with one fully-annotated frame per video. The dataset is aimed to support development of visual perception systems. Mapillary Street-Level Sequences: A Dataset for Lifelong Place Recognition [link] Dataset from Mapillary — This dataset contains 1.6M images from different locations across the world. The dataset is aimed to support development of lifelong visual localisation systems. Stories from Lyft Engineering. 13 Cvpr Av Level 5 Autonomous Lyft 13 claps 13 Written by Head of Research at Lyft Level 5, self-driving, augmented reality, entrepreneur, www.ondruska.com Stories from Lyft Engineering. Written by Head of Research at Lyft Level 5, self-driving, augmented reality, entrepreneur, www.ondruska.com Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-08-20"},
{"website": "Lyft-Engineering", "title": "improving kubernetes cronjobs at scale part 1", "author": ["Kevin Yang"], "link": "https://eng.lyft.com/improving-kubernetes-cronjobs-at-scale-part-1-cf1479df98d4", "abstract": "Data Data Science Engineering Mobile Product Security At Lyft, we chose to move our server infrastructure onto Kubernetes, a distributed container orchestration system in order to take advantage of automation, have a solid platform we can build upon, and lower overall cost with efficiency gains. Distributed systems can be difficult to reason about and understand, and Kubernetes is no exception. Despite the many benefits of Kubernetes, we discovered several pain points while adopting Kubernetes’ built-in CronJob as a platform for running repeated, scheduled tasks. In this two-part blog series, we will dive deep into the technical and operational shortcomings of Kubernetes CronJob at scale and share what we did to overcome them. Part 1 (this article) of this series discusses in detail the shortcomings we’ve encountered using Kubernetes CronJob at Lyft. In part 2, we share what we did to address these issues in our Kubernetes stack to improve usability and reliability. Users of Kubernetes CronJob Anyone building a platform on top of Kubernetes Anyone interested in running distributed, scheduled tasks on Kubernetes Anyone interested in learning about Kubernetes usage at scale in the real-world Kubernetes contributors Insight into how parts of Kubernetes (in particular, CronJob) behave at scale in the real-world. Lessons learned from using Kubernetes as a platform at a company like Lyft, and how we addressed the shortcomings. Basic familiarity with the cron concept Basic understanding of how CronJob works , specifically the relationship between the CronJob controller, the Jobs it creates, and their underlying Pods, in order to better understand the CronJob deep-dives and comparisons with Unix cron later in this article. Familiarity with the sidecar container pattern and what it is used for. At Lyft, we make use of sidecar container ordering to make sure that runtime dependencies like Envoy, statsd, etc., packaged as sidecar containers, are up and running prior to the application container itself. The cronjobcontroller is the piece of code in the Kubernetes control-plane that reconciles CronJobs A cron is said to be invoked when it is executed by some machinery (usually in accordance to its schedule) Lyft Engineering operates on a platform infrastructure model where there is an infrastructure team (henceforth referred to as platform team , platform engineers , or platform infrastructure ) and the customers of the platform are other engineers at Lyft (henceforth referred to as developers , service developers , users , or customers ). Engineers at Lyft own, operate, and maintain what they build, hence “ operat- ” is used throughout this article. Today at Lyft, we run nearly 500 cron tasks with more than 1500 invocations per-hour in our multi-tenant production Kubernetes environment. Repeated, scheduled tasks are widely used at Lyft for a variety of use cases. Prior to adopting Kubernetes, these were executed using Unix cron directly on Linux boxes. Developer teams were responsible for writing their crontab definitions and provisioning the instances that run them using the Infrastructure As Code (IaC) pipelines that the platform infrastructure team maintained. As part of a larger effort to containerize and migrate workloads to our internal Kubernetes platform, we chose to adopt Kubernetes CronJob* to replace Unix cron as a cron executor in this new, containerized environment. Like many, we chose Kubernetes for many of its theoretical benefits, one of which is efficient resource usage. Consider a cron that runs once a week for 15 minutes. In our old environment, the machine running that cron is sitting idle 99.85% of the time. With Kubernetes CronJob, compute resources (CPU, memory) are only used during the lifetime of a cron invocation. The rest of the time, Kubernetes can efficiently use those resources to run other CronJobs or scale down the cluster all together. Given the previous method for executing cron tasks, there was much to gain by transitioning to a model where jobs are made ephemeral. Since adopting Kubernetes as a platform, developer teams no longer provision and operate their own compute instances. Instead, the platform engineering team is responsible for maintaining and operating the compute resources and runtime dependencies used in our Kubernetes stack, as well as generating the Kubernetes CronJob objects themselves. Developers need only configure their cron schedule and application code. This all sounds good on paper, but in practice, we discovered several pain points in moving crons away from the well-understood environment of traditional Unix cron to the distributed, ephemeral environment of Kubernetes using CronJob. * while CronJob was, and still is (as of Kubernetes v1.18), a beta API, we found that it fit the bill for the requirements we had at the time, and further, it fit in nicely with the rest of the Kubernetes infrastructure tooling we had already built. To better understand why Kubernetes CronJobs can be difficult to work with in a production environment, we must first discuss what makes CronJob different. Kubernetes CronJobs promise to run like cron tasks on a Linux or Unix system ; however, there are a few key differences in their behavior compared to a Unix cron: Startup Performance and Failure handling . We begin by defining start delay to be the wall time from expected cron start to application code actually executing. That is, if a cron is expected to run at 00:00:00 , and the application code actually begins execution at 00:00:22 , then the particular cron invocation has a start delay of 22 seconds. Traditional Unix crons experience very minimal start delay. When it is time for a Unix cron to be invoked, the specified command just runs . To illustrate this, consider the following cron definition: With this cron definition, one can expect the following output: On the other hand, Kubernetes CronJobs can experience significant start delays because they require several events to happen prior to any application code beginning to run. Just to name a few: cronjobcontroller processes and decides to invoke the CronJob cronjobcontroller creates a Job out of the CronJob’s Job spec jobcontroller notices the newly created Job and creates a Pod Kubernetes admission controllers inject sidecar Container specs into the Pod spec* kube-scheduler schedules the Pod onto a kubelet kubelet runs the Pod (pulling all container images) kubelet starts all sidecar containers* kubelet starts the application container* * unique to Lyft’s Kubernetes stack At Lyft, we found that start delay was especially compounded by #1, #5, and #7 once we reached a certain scale of CronJobs in our Kubernetes environment. To better understand where this latency comes from, let’s dive into the source-code of the built-in cronjobcontroller . Through Kubernetes 1.18, the cronjobcontroller simply lists all CronJobs every 10 seconds and does some controller logic over each. The cronjobcontroller implementation does so synchronously, issuing at least 1 additional API call for every CronJob. When the number of CronJobs exceeds a certain amount, these API calls begin to be rate-limited client-side . The latencies from the 10 second polling cycle and API client rate-limiting add up and contribute to a noticeable start-delay for CronJobs. Due to the nature of cron schedules, most crons are expected to run at the top of the minute ( XX:YY:00 ). For example, an @hourly cron is expected to execute at 01:00:00 , 02:00:00 , and so on. In a multi-tenant cron platform with lots of crons scheduled to run every hour, every 15 minutes, every 5 minutes, etc., this produces hot-spots where lots of crons need to be invoked simultaneously. At Lyft, we noticed that one such hot spot is the top of the hour ( XX:00:00 ). These hot-spots can put strain on and expose additional client-side rate-limiting in control-plane components involved in the happy-path of CronJob execution like the kube-scheduler and kube-apiserver causing start delay to increase noticeably. Additionally, if you do not provision compute for peak demand (and/or use a cloud-provider for compute instances) and instead use something like cluster autoscaler to dynamically scale nodes, then node launch times can contribute additional delays to launching CronJob Pods. Once a CronJob Pod has successfully scheduled onto a kubelet , the kubelet needs to pull and execute the container images of all sidecars and the application itself. Due to the way Lyft uses sidecar ordering to gate application containers, if any of these sidecar containers are slow to start, or need to be restarted, they will propagate additional start delay. To summarize, each of these events that happen prior to application code actually executing combined with the scale of CronJobs in a multi-tenant environment can introduce noticeable and unpredictable start delay. As we will see later on, this start delay can negatively affect the behavior of a CronJob in the real-world by causing CronJobs to miss runs. It is good practice to monitor the execution of crons. With Unix cron, doing so is fairly straight-forward. Unix crons interpret the given command with the specified $SHELL , and, when the command exits (whether successful or not), that particular invocation is done. One rudimentary way of monitoring a Unix cron then is to introduce a command-wrapper script like so: With Unix cron, stat-and-log will be executed exactly once per complete cron invocation, regardless of the $exitcode . One can then use these metrics for simple alerts on failed executions. With Kubernetes CronJob, where there are retries on failures by default and an execution can have multiple failure states (Job failure and container failure), monitoring is not as straightforward. Using a similar script in an application container and with Jobs configured to restart on failure, a CronJob will instead repeatedly execute and spew metrics and logs up to a BackoffLimit number of times on failure, introducing lots of noise to a developer trying to debug it. Additionally, a naive alert using the first failure from the wrapper script can be un-actionable noise as the application container may recover and complete successfully on its own. Alternatively, you could alert at the Job level instead of the application container level using an API-layer metric for Job failures like kube_job_status_failed from kube-state-metrics . The drawback of this approach is that an on-call won’t be alerted until the Job has reached the terminal failure state once BackoffLimit has been reached, which can be much later than the first application container failure. Non-negligible start delay and retry-on-failure loops contribute additional delay that can interfere with the repeated execution of Kubernetes CronJobs. For frequent CronJobs, or those with long application execution times relative to idling time, this additional delay can carry over into the next scheduled invocation. If the CronJob has ConcurrencyPolicy: Forbid set to disallow concurrent runs , then this carry-over causes future invocations to not execute on-time and get backed up. A more sinister scenario that we observed at Lyft where CronJobs can miss invocations entirely is when a CronJob has startingDeadlineSeconds set. In that scenario, when start delay exceeds the startingDeadlineSeconds , the CronJob will miss the run entirely. Additionally, if the CronJob also has ConcurrencyPolicy set to Forbid , a previous invocation’s retry-on-failure loop can also delay the next invocation, causing the CronJob to miss as well. Since beginning to move these repeated, scheduled tasks onto Kubernetes, we found that using CronJob out-of-the-box introduced several pain-points from both the developers’ and the platform team’s points of view that began to negate the benefits and cost-savings we initially chose Kubernetes CronJob for. We soon realized that neither our developers nor the platform team were equipped with the necessary tools for operating and understanding the complex life cycles of CronJobs. Developers at Lyft came to us with lots of questions and complaints when trying to operate and debug their Kubernetes CronJobs like: “Why isn’t my cron running?” “I think my cron stopped running. How can I tell if my cron is actually running?” “I didn’t know the cron wasn’t running, I just assumed it was!” “How do I remedy X failed cron? I can’t just ssh in and run the command myself.” “Can you explain why this cron seemed to miss a few schedules between X and Y [time periods]?” “We have X (large number) of crons, each with their own alarms, and it’s becoming tedious/painful to maintain them all.” “What is all this Job, Pod, and sidecar nonsense?” As a platform team , we were not equipped to answer questions like: How do we quantify the performance characteristics of our Kubernetes Cron platform? What is the impact of on-boarding more CronJobs onto our Kubernetes environment? How does running multi-tenant Kubernetes CronJobs perform compared to single-tenant Unix cron? How do we begin to define Service-Level-Objectives (SLOs) to communicate with our customers? What do we monitor and alarm on as platform operators to make sure platform-wide issues are tended to quickly with minimal impact on our customers? Debugging CronJob failures is no easy task, and often requires an intuition for where failures happen and where to look to find proof. Sometimes this evidence can be difficult to dig up, such as logs in the cronjobcontroller which are only logged at a high verbosity log-level. Or, the traces simply disappear after a certain time period and make debugging a game of “whack-a-mole”, such as Kubernetes Events on the CronJob, Job, and Pod objects themselves, which are only retained for one hour by default. None of these methods are easy to use, and do not scale well from a support point-of-view with more and more CronJobs on the platform. In addition, sometimes Kubernetes would just quit when a CronJob had missed too many runs, requiring someone to manually “un-stick” the CronJob. This happens in real-world usage more often than you would think, and became painful to remedy manually each time. This concludes the dive into the technical and operational issues we’ve encountered using Kubernetes CronJob at scale. In Part 2 we share what we did to address these issues in our Kubernetes stack to improve the usability and reliability of CronJobs. As always, Lyft is hiring! If you’re passionate about Kubernetes and building infrastructure platforms, read more about them on our blog and join our team ! Stories from Lyft Engineering. 345 4 Thanks to Polly Peterson . Kubernetes Infrastructure Cronjob Lyft K8s 345 claps 345 4 Written by @yangkeIO | Software Engineer at Lyft Stories from Lyft Engineering. Written by @yangkeIO | Software Engineer at Lyft Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-08-05"},
{"website": "Lyft-Engineering", "title": "how we learned to improve kubernetes cronjobs at scale part 2 of 2", "author": ["Kevin Yang"], "link": "https://eng.lyft.com/how-we-learned-to-improve-kubernetes-cronjobs-at-scale-part-2-of-2-dad0c973ffca", "abstract": "Data Data Science Engineering Mobile Product Security This is Part 2 of a two-part blog series on Improving Kubernetes CronJobs at Lyft. If you haven’t already, checkout Part 1 . It became clear that Kubernetes CronJobs out-of-the-box were not going to be an easy to use, drop-in replacement for running repeated, scheduled tasks. If we wanted to move all of Lyft’s crons onto Kubernetes confidently, we needed to not only address the technical shortcomings of CronJobs, but the human experience of using them as well . Namely, we needed to: Listen to our developers to understand what questions they wanted answered about their crons: “Did my cron run?” ( “Did the application code execute?” ) “Did it run successfully?” “How long did the cron take to execute?” ( “How long did it take the application code to execute?” ) Scale platform support by making Kubernetes CronJobs easier to reason about, their life cycles well-understood, and the platform / application boundary clear. Instrument our platform with built-in metrics and alerts to reduce the amount of bespoke alarm configurations and duplicated cron wrapper scripts that developers need to write and maintain. Build tooling to make it easy to not only recover from failures but test new CronJob configuration as well. Fix long-standing, technical issues in Kubernetes like the TooManyMissedStarts bug that require manual intervention to remedy and cause an important failure scenario ( when startingDeadlineSeconds is missed ) to silently fail. We solved these problems by: Exposing observability that not only enables developers to debug their CronJobs, but allows platform engineers to define and monitor Service Level Objectives (SLOs) as well. Adding a tool to make ad hoc invocations of a CronJob easy in our Kubernetes stack. Fixing those long-standing issues inside Kubernetes itself. We instrumented our Kubernetes CronJob stack with the following metrics that are emitted for all CronJobs at Lyft: started.count — This is a counter that is incremented specifically when the application container of a CronJob invocation starts for the first time . This answers the question: “Did the application code execute?” {success, failure}.count — These are counters that are incremented when a given CronJob invocation reaches a terminal state (when a Job has finished running and the jobcontroller no longer tries to execute it). These answer the question: “Did it run successfully?” scheduling-decision.{invoke, skip}.count — These are counters that expose the decisions the cronjobcontroller makes when invoking a CronJob. In particular, skip.count helps answer: “Why is my cron not running?” and is parametrized by the following reason labels: reason = concurrencyPolicy — The cronjobcontroller skipped invoking a CronJob because doing so would be a violation of its ConcurrencyPolicy . reason = missedDeadline — The cronjobcontroller skipped invoking a CronJob because it has missed the invocation window defined by .spec.startingDeadlineSeconds . reason = error — This is a catch-all for other errors encountered when trying to invoke a CronJob. app-container-duration.seconds — This is a timer that measures the wall-time of the application container. It answers the question: “How long did it take the application code to execute?” This timer deliberately does not include time taken to schedule a Pod, startup sidecars, etc., which are part of what the platform team owns and is encompassed by start delay. start-delay.seconds — This is a timer that measures start delay. This metric, when aggregated across the platform, enables platform engineers to not only quantify, monitor, and tune the performance of the platform, but also begin to define SLOs for things like start delay and maximum cron schedule frequency. With these metrics, we were then able to create default alerts that notified developers when: Their CronJob did not run when it was supposed to ( rate(scheduling-decision.skip.count) > 0 ) Their CronJob failed ( rate(failure.count) > 0 ) Developers no longer need to maintain their own alerts and metrics for crons on Kubernetes as the platform provides them built-in. We adapted kubectl create job test-job — from=cronjob/<your-cronjob> to our internal CLI tool that every engineer at Lyft uses to interact with their services on Kubernetes to make it simple to invoke their CronJob ad hoc in order to: Recover from intermittent CronJob failures. Reproduce and debug run-time failures at a time that is not 3:00 AM (a more convenient time when you can inspect CronJob, Job, and Pod events in real-time) instead of trying to catch it in the act. Test run-time configuration when developing a new CronJob or migrating an existing Unix cron without waiting for the cron schedule to pass by. We fixed the TooManyMissedStarts bug so that CronJobs would no longer get “stuck” after 100 missed starts in a row. In addition to removing the need for manual intervention, this patch also allowed us to actually monitor when startingDeadlineSeconds is exceeded . Hats off to Vallery Lancey for designing and writing this patch as well as Tom Wanielista for help come up with the algorithm. There is currently an open PR in Kubernetes to upstream this patch. One of the tricky parts about implementing an alert on missed cron invocations is dealing with cron schedules ( crontab.guru is tremendously helpful for deciphering these!). For example, consider a cron schedule like: To instrument this cron, you might increment a counter metric every time the cron finishes (or use a cron wrapper ). In your alerting system, you would then write a conditional query that says, “Look back through a 60 minute window, and alert me if the counter increased by less than 12”. Problem solved, right? What if instead you had a cron schedule like: Now you need to get fancy with your query, or maybe your alerting system has some features that allow you to only be alerted during “business hours”. Regardless, these examples illustrate that coupling the cron schedule to the alert definition has several downsides: Changing the cron schedule means changing the alert. Some cron schedules require complex time series queries to replicate. Crons that don’t start exactly on time will require some amount of “grace period” to be built in to the query to minimize false positives. #2 alone makes generating default alerts for all crons on a platform a very difficult task, and #3 is especially pertinent to distributed platforms like Kubernetes CronJob where start delay is non-negligible. Alternatively, there are solutions that use dead man switches , which still requires coupling the cron schedule to the alert, and/or anomaly detection algorithms, which require learning expectations over time and thus don’t work immediately for new CronJobs nor changes to a cron schedule. Another way of looking at the problem is to ask: what does it mean when a cron is supposed to run but hasn’t? In Kubernetes, barring bugs in the cronjobcontroller or your control-plane being down (the latter of which should be very obvious if you are monitoring your cluster correctly), this means that the cronjobcontroller evaluated the CronJob, determined (according to the cron schedule) that it needed to be invoked, yet still deliberately chose not to . Sound familiar? This is exactly what our scheduling-decision.skip.count metric captures! Now, we only need to check for changes in rate(scheduling-decision.skip.count) in order to alert someone that a CronJob was supposed to run but hasn’t. This solution decouples the cron schedule from the alert itself, which yields several advantages: No need to re-configure alerts when cron schedules change. No complex time series queries and conditionals. Easy to generate default alerts for all CronJobs on the platform. This, combined with the other time series and alerts mentioned previously, helps paint a more complete and easier to understand picture of a CronJob’s state. Due to the complex nature of CronJob life cycles, we needed to be precise about where in our stack we added instrumentation in order to accurately measure this metric. This boiled down to capturing 2 timestamps: T1 : When the cron is expected to run (as dictated by the cron schedule). T2 : When the application code actually begins executing. Then, start delay = T2 — T1 . For T1 , we added some code to the cron invocation logic in the cronjobcontroller itself to write the expected start time as a .metadata.Annotation on Job objects that the cronjobcontroller creates when invoking a CronJob. Then we can consume it directly from any API client by issuing a basic GET Job request. T2 is trickier to get right. Because we are interested in capturing the tightest bound of start delay, we want T2 to be the first time the application container starts running. If instead we recorded T2 at any application container start (including restarts), then our start delay would include application code execution time as well. To accomplish this, when we detected that the application container for a given Job transitioned to Running for the first time, we wrote another .metadata.Annotation to the Job object, essentially creating a distributed lock so that future application container starts for a given Job would be ignored, and only the timestamp of the first start would be recorded. Since rolling out these features and bug fixes, we’ve received a lot of positive feedback from our developers. To summarize, developers using our Kubernetes CronJob platform: No longer need to maintain their own bespoke monitoring and alerts. Can have high confidence that their CronJobs are working, and will be alerted when they are not. Can easily recover from failures and test new CronJobs in this environment using our ad-hoc CronJob invoking tool. Can understand the performance of their application code (using the app-container-duration.seconds timer metric). Additionally, platform engineers now have another dimension ( start delay ) for measuring the user experience and performance of the platform. Finally (and perhaps the biggest win), by building richer observability to make CronJob state easier to reason about, developers and platform engineers can now debug issues together while looking at the same data, and more often than not, developers can diagnose and solve issues themselves all-together using the tools the platform provides. Orchestrating distributed, scheduled tasks is hard. Kubernetes CronJobs are just one of many ways of doing so. While they are far from perfect, CronJobs can work at scale if you are willing to invest the time and effort into adding observability, understanding how they can fail, and building what your users need. Lyft is hiring! If you’re passionate about Kubernetes and building infrastructure platforms, read more about them on our blog and join our team ! Note: There is an open Kubernetes Enhancement Proposal (KEP) to address the shortcomings of CronJobs and graduate them to GA. Big thanks to Rithu John , Scott Lau, Scarlett Perry , Julien Silland, and Tom Wanielista for their help in reviewing this blog series. Stories from Lyft Engineering. 237 3 Thanks to Polly Peterson . Kubernetes Infrastructure Cronjob K8s Engineering 237 claps 237 3 Written by @yangkeIO | Software Engineer at Lyft Stories from Lyft Engineering. Written by @yangkeIO | Software Engineer at Lyft Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-11-12"},
{"website": "Lyft-Engineering", "title": "announcing clutch the open source platform for infrastructure tooling", "author": ["Daniel Hochman"], "link": "https://eng.lyft.com/announcing-clutch-the-open-source-platform-for-infrastructure-tooling-143d00de9713", "abstract": "Data Data Science Engineering Mobile Product Security By Daniel Hochman and Derek Schaller Today we are excited to announce the open-source availability of Clutch , Lyft’s extensible UI and API platform for infrastructure tooling. Clutch empowers engineering teams to build, run, and maintain user-friendly workflows that also incorporate domain-specific safety mechanisms and access controls. Clutch ships with several features for managing platforms such as AWS, Envoy, and Kubernetes with an emphasis on extensibility so it can host features for any component in the stack. The dynamic nature of cloud computing has significantly reduced the adoption cost of new infrastructure. The CNCF Landscape currently tracks over 300 open source projects and 1,000 more commercial offerings. Although organizations can rapidly adopt these projects and vendors, each new technology comes with its own set of configuration, tooling, logs, and metrics. Allowing developers to quickly and safely make changes throughout the stack requires significant upfront and ongoing investment in tooling, which most organizations fail to account for. Therefore, while new infrastructure is increasingly easy to adopt, it is difficult to scale the management of new components, especially as the complexity of the overall platform and the size of the engineering team grows. Clutch solves this problem by enabling infrastructure teams to deliver intuitive and safe interfaces for infrastructure management to their entire engineering organization. Clutch is the result of a year-long development cycle to address deficiencies in Lyft’s developer experience and tooling. At its core, Clutch is made up of two main components. The Go backend is designed to be an extensible infrastructure control plane, bringing together a patchwork collection of systems behind a single protobuf-driven API with common authorization, observability, and audit logging. API definitions for the backend automatically generate clients for the frontend via protobuf tooling. The React frontend is a pluggable and workflow-oriented UI allowing users and developers to create new features behind a single pane of glass with less code, less prior JavaScript knowledge, and less maintenance. What differentiates Clutch from other solutions in the developer tooling space? At the outset of the project, we did a thorough analysis of the existing tools before building our own. Our main goals for the tooling we intended to adopt were: Reduce the Mean Time To Repair (MTTR) infrastructure when responding to an alert. Engineers were spending too long reading through runbooks and navigating complex tooling when responding to pages while on-call. Eliminate accidental outages when performing maintenance tasks. Significant outages have occurred as the result of a user missing a warning in a runbook or even modifying or deleting the wrong resource altogether, e.g. one they thought was unused but had significant traffic and usage. Enforce granular permissions and audit all activity in a common format. Some permissions are too broad because the vendor access controls do not support fine-grained control. Also, while we were collecting audit logs from various tools for security purposes, it was hard to distill that data into actionable insights on how we could improve our tools. Provide a platform that greatly eases the development of future tools. At Lyft’s scale, projects with a large scope are rarely successful if they don’t account for contributions outside of the immediate team. We don’t have enough resources to build every feature that Lyft needs, much less support it. We started by looking at the shortcomings of the available vendor UIs. Vendor tools are slow (and in some cases dangerous) due to a lack of specialization. They require unnecessary steps to perform common tasks and present a superset of the necessary information. There are generally few guardrails beyond simple access controls. The result is that an operator may perform an action that seems innocuous but actually degrades the system. On the other hand, they may be unfamiliar with the tool such that it results in delayed remediation. Ideally, engineers are only on-call every four to six weeks. It’s easy to forget how to use a tool, especially considering the possibility of going multiple on-call cycles without needing to interact with a particular system. The net outcome of relying on vendor tooling is high cognitive load due to fragmentation and information sprawl. The net outcome of relying on vendor tooling is high cognitive load due to fragmentation and information sprawl. In contrast, a vendor-agnostic tool like Clutch can unify disparate systems with a clear and consistent UX and offer specialized functionality to perform common tasks with as few clicks and as little training as possible. Next, we turned to the open-source community. We found that the scope of open-source infrastructure management tooling is still usually limited to a single system and not designed for extensive customization. This does not address the problem of cognitive load and fragmentation. Also, while there are other frontend frameworks for building consoles, none of them incorporate an integrated backend framework with authentication, authorization, auditing, observability, API schemas, and a rich plugin model. There is a popular continuous delivery platform that addresses many of the same overarching issues as Clutch (e.g., lowering MTTR, user-friendly UI). However, it requires significant investment in running many microservices and migrating applications to a structure different from our own. Clutch’s backend simplifies feature development by integrating the core functions listed above for free on every API endpoint. It also deploys as a single binary requiring little operational investment. Finally, we wanted a platform that we could invest in as an organization, thus requiring it to be easy for other internal teams to understand and build on. Clutch offers an integrated and guided development model that makes feature development a straightforward process. In addition to the first-class backend features, Clutch’s frontend offers unique abstractions for state management and multi-step forms that make frontend development easier for infrastructure teams without a lot of JavaScript experience. For a detailed analysis on how other tools measure up to Clutch, see Comparisons to Other Tools . Envoy Proxy was created at Lyft. Today, it’s one of the most popular proxies, deployed at many large internet companies and continuously advancing the standard for cloud networking. Our team has learned a lot from maintaining Envoy alongside the larger community. One of the most popular topics discussed among Envoy users is the state of control plane development , specifically how to systematically integrate a wide range of disparate components such that Envoy can effectively route and report on network traffic. This is directly analogous to Clutch, which integrates disparate infrastructure systems behind a uniform API. Clutch adopts many of Envoy Proxy’s core patterns that emerged from years of work on network control planes. Clutch adopts many of Envoy Proxy’s core patterns that emerged from years of work on network control planes. Like Envoy, Clutch is configuration-driven , schema-driven , and leverages a modular extension-based architecture to make it work for a wide variety of use cases without compromising maintainability. Extending Clutch does not require forks or rewrites, and custom code can easily be compiled into the application from a custom public or private external repository. These same patterns that enable organizations large and small with unique tech stacks to converge on a single proxy solution will hopefully enable similarly unique organizations to converge on Clutch as an infrastructure control plane. Additionally, Clutch ships with authentication and authorization components. OpenID Connect (OIDC) authentication flows for single-sign on, resource-level role-based access control (RBAC) via static mapping, and automatic auditing of all actions with the ability to run additional sinks for output, e.g., a Slackbot. Clutch also has features to mitigate the potential for accidents. Guardrails and heuristics normally documented in runbooks can be implemented programmatically. For example, we never allow a user to scale down a cluster more than 50% at a time since this behavior has historically led to accidental outages during normal maintenance. In the future, we plan to fetch CPU and other usage data to display alongside the cluster information, even going as far as to limit the lower bounds for scale down if we determine that it is likely to cause an outage. By implementing guardrails and heuristics directly into the tool, we avoid the need to rely solely on training and runbooks to prevent accidents. Clutch ships as a single binary that contains the frontend and backend, making it trivial to deploy. Many changes can be achieved via configuration rather than recompiling a new binary. Other systems that offer infrastructure lifecycle tooling require a complicated set of microservices or migration to an opinionated way of managing and deploying applications. Clutch is meant to complement existing systems rather than replace them. Clutch is powered by a Go backend and React frontend. It provides full-featured frameworks for backend and frontend development. All components in Clutch are composable, allowing for partial use of the framework offerings or completely custom features. This component-and-workflow architecture allows a developer with limited frontend experience to replace clunky tooling or command-line scripts with a clear and easy-to-use step-by-step UI in under an hour of development time . Clutch’s frontend packages offer components to easily build step-by-step workflows with a consistent and seamless user experience, including: DataLayout , a workflow-local state management component that handles user input and data from API calls. Wizard , for presenting step-by-step forms to users. Custom Material UI components, for displaying rich information with minimal code in a consistent manner across workflows. Clutch’s backend relies heavily on generated code from protobuf API definitions. Protobuf tooling also generates frontend clients which keeps the backend and frontend in sync as APIs evolve. Components on the backend include: Modules , implementations of the code generated API stubs Services , for interacting with external sources of data Middleware , for inspecting request and response data and applying auditing, authorization, etc. Resolvers , a common interface to locate resources based on free-form text search or structured queries Resolvers are one Clutch abstraction we hope will make a big impact on the way features can be abstracted to multiple organizations. Resolvers are easily extended with custom resource location code, allowing operators to locate resources (such as K8s pods or EC2 instances) by the common name(s) that the organization is accustomed to rather than the terse canonical identifier. For example, if developers call their application `myService-staging`, it’s easy to add code that will interpret such a query as the structured elements `${application_name}-${environment}`. Furthermore, the frontend automatically generates user input forms from the backend definitions. With one line of code on the frontend: The following form is rendered: Configuring additional search dimensions on the backend will automatically reflect in the rendered form on the frontend. Before Clutch, Lyft engineers relied on a hodgepodge of command line tools, web interfaces, and runbooks to perform simple tasks. The most common alerts at Lyft required as many as six different sources of information to resolve: the alert, other service dashboards, the runbook, other sources of documentation, the vendor console or scripts, and configuration settings. As Lyft scaled in terms of team, product, and stack, we realized that the tools did not keep pace. We had no path forward to solve these problems with the existing frameworks. This led to the development of the first iteration of Clutch. Over the past year, Clutch has enjoyed an incredible rate of internal adoption, both in usage and development. Thousands of otherwise risky operations related to infrastructure management have gone through Clutch, each one representing the potential for an accident or delayed incident mitigation causing loss of trust from our drivers and riders. Seven internal engineering teams are already planning to add new features by the end of 2020 at the time of this writing, with at least half of those targeting open source. Engineers (including our wonderful interns) have been able to develop meaningful functionality with limited guidance. Most importantly, we’re finally able to see a path forward for delivering our internal platform through a single pane of glass, making Lyft infrastructure a product that meets our customers’ needs rather than a patchwork collection of systems and tools. We have received a lot of positive feedback internally, for example: “I’m so happy that this exists because otherwise I would still be waiting on the tab to load in the cloud provider’s console.” More specifics on Clutch at Lyft can be found in the Lyft Case Study article. Throughout the journey of building Clutch, the product has evolved and our external and internal roadmaps now encompass the entirety of the developer experience at Lyft. Our long-term vision involves building a context-aware developer portal. Not only providing a set of tools to developers, but presenting the most relevant tools and information as soon as the user lands in the portal. Upcoming features include: Envoy UI , giving users a real-time dashboard to monitor their distributed application’s network behavior and configuration. Chaos testing , integrating with Envoy to allow scheduled fault injection and squeeze testing with automatic abort criteria. Auto remediation , responding to alerts automatically with the appropriate Clutch action. Safety enhancements , including escalation capabilities, challenge modals, and two-phase approvals. Additional infrastructure lifecycle management capabilities , viewing the state of a cluster to find outliers, perform long-running maintenance tasks. Service health dashboard , providing feedback to developers on the state of their service (e.g. code coverage, cost, active incidents) using configurable reporting mechanisms. Generalized configuration management , allowing users to manage complex configuration via a guided UI or otherwise reflect changes in infrastructure as code declarations. Topology mapping , associating users with the services they own and showing them relevant data and tools on the landing page. For more details on upcoming projects visit our roadmap . Clutch has had a significant impact on Lyft’s developer experience, allowing the infrastructure and other engineering teams to deliver tooling as a polished product rather than an afterthought. Teams are super excited about contributing new features internally, and engineers at Lyft love using the platform: “The usability is super awesome and improves the overall quality of life for engineers at Lyft.” Join us! We think Clutch has the same outstanding potential for other engineering teams both large and small. We look forward to working with the community on additional plugins and core features in order to build a rich ecosystem. Our goal is to give every team and every tech stack access to first-class cloud-native tooling and reduced cognitive load. All contributions are welcome, from ideas to implementations, and we’re happy to help you get off the ground with your first feature. To learn more about Clutch, contribute, or follow along: Visit our website and documentation at https://clutch.sh . Check out the code at https://github.com/lyft/clutch . Join us on Slack @lyftoss/clutch . Follow us on Twitter @clutchdotsh . Lyft is hiring engineers to work on Clutch. Apply here . Clutch would not have been possible without the contributions and hard work from many engineers at Lyft including Sindhura Tokala, Matthew Gumport, Scarlett Perry, Ryan Cox, Shawna Monero, Bill Gallagher, Gastón Kleiman, Paul Dittamo, Mike Cutalo, Rafal Augustyniak, Alan Chiu, Kathan Shah, Ansu Kar, Tony Allen, Amy Baer, Stephan Mercatoris, Susan Li, Jose Nino, and Matt Klein. Special thanks to Patrick Sunday, Martin Conte Mac Donnell, Polly Peterson, Michael Rebello, and Pete Morelli. Thanks again to all of those directly involved in the project and those who have provided us with guidance and support throughout this process. We can’t wait to see where the open-source community will take the project. Onward! Este artículo también está en español a eng-espanol.lyft.com . Stories from Lyft Engineering. 932 Infrastructure React Golang AWS Kubernetes 932 claps 932 Written by Software engineer at Lyft. Stories from Lyft Engineering. Written by Software engineer at Lyft. Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-09-10"},
{"website": "Lyft-Engineering", "title": "technical program management tpm principles lyft", "author": ["John Walpole"], "link": "https://eng.lyft.com/technical-program-management-tpm-principles-lyft-9134becd2222", "abstract": "Data Data Science Engineering Mobile Product Security I am fortunate enough to be on Lyft’s Team to deliver rider and driver delight as we seek to reconnect people through transportation and bring communities together. I’ve worked on many of Lyft’s critical projects, including splitting the app for drivers and passengers, our IPO, expanding to Canada, Bikes & Scooters, and many more. Recently I was asked to think about some foundational principles that I have gathered after nearly 4 years at Lyft and 13 years working in TPM. With the help of some amazing mentors ( Amy Farrow , Peter Morelli , Carlos Whitt , Amish Kagalwala , Sgreene , Nicholas Muldoon , and Nate VanDusen ) as well as the Lyft TPM management team this post is where I’ve landed. This is the first in a series of posts on how Lyft TPM operates to deliver for our teams & customers. With the principles in place, I will then focus on optimizing flow and demonstrate how we applied both the principles and flow to one of Lyft’s biggest programs as a real-world example in a future post. The principles below are the foundation for a high performing TPM organization and they are the same principles that enable us to deliver on Lyft’s mission to improve people’s lives with the world’s best transportation. 1. Customer obsession We know our customers well, we are engaged with them and get feedback from them frequently and consistently. We know their use cases, pain points, as well as their dreams, wishes, and desires. Our customers and partner teams believe we understand their needs and have their best interests in mind. Customers include riders, drivers, partners, cities, and employees. We thrive to delight our customers! 2. Ownership We take full responsibility, accountability, and ownership of programs and portfolios to deliver customer delight throughout the entire project lifecycle — from ideation to ship, and maintain. We start with the business outcome, value proposition, and high-level view and then dive deep into the details, leaving no stone unturned. We do this by: Setting clear and measurable OKRs and driving continual accountability against those OKRs Agreeing and iterating on working agreements for our programs and portfolios to ensure we identify customers and stakeholders Implementing DACI (Driver, Approver, Contributors, Informed) framework Continually grooming a healthy product backlog Producing and iterating on product / technical / science specifications Limiting the work-in-progress (WIP) to optimize flow Aligning on standards ahead of time including a definition of ‘done’, test strategy, experiments & rollout plans Maintaining program health with a defined schedule and shipping cadence, gantt tracking (where appropriate) for super large programs and using execution tracking ( JIRA and/or GSheets ). We regularly check progress and team health against program health Documenting a clear communication plan with escalation paths for effective risk management Holding regular retrospectives to learn and iterate as we go Our sense of ownership expands beyond initial execution. We take pride in having clear & crisp documentation to ensure visibility, accountability, alignment, and ease of scaling — something that has become even more important to support remote working across large distributed multidisciplinary teams. We utilize data, goals, and metrics to measure progress and adapt. Our communication is succinct, accurate, and backed with data; from one-pagers to status reports we have a clear call to action. When a program ends (ships/canceled/disbanded), ownership of the features and services shipped as part of the program is transferred to the run the business team, nothing is orphaned or dropped. 3. Technical depth A healthy TPM organization has a deep understanding of our focus areas and spends time understanding the problem space, ensuring requirements and technical specifications are complete, and fully addressing the program objectives. It’s important to be curious, pressure test assumptions, experiment, and ship as soon as possible to validate those assumptions. We respectfully challenge prioritization to ensure the most critical problems are addressed first. We take an all-encompassing view; we see the forest and understand the relationship between the trees. Through our deep understanding of the technical space, we help identify new opportunities and dependencies to ensure the right teams are included in programs and portfolios upfront to ensure they are part of the planning process from start to finish. We dig deep into datasets, build dashboards, and understand the details of the program specifications, code, tool, and processes thoroughly. 4. Teamwork, alignment, and cross-functional collaboration We make relationships a priority. We deliver by empowering and inspiring our colleagues, rather than pushing to hit deadlines. We create working agreements to facilitate healthy discourse, listen, and use inclusive discussions to resolve conflicts and bring teams closer together. The end goal is always to deliver delight for our riders, drivers, partners, cities, and employees. We lead by example on everything we do to inspire confidence, demonstrate competence, and run a tight ship. We do this by getting multiple teams aligned on one cross-functional shared goal as we strive to understand the diverse perspectives of our program and portfolio of teams. We are force multipliers: we bring people together to deliver way beyond their own capabilities. We celebrate success together and when problems come up, we ask, “ how can I help? ” We work with teams in conflict or struggling with a program goal to deliver clear recovery paths and align our teams, partners, and leadership on the recovery plan. We get our hands dirty and muck in to help. 5. Growth mindset In everything we do, we create and grow psychological safety so we, the teams we work with, and partners can learn and grow. We are obsessed with retrospecting to drive continuous learning, improvement, and our own professional growth. We all have both mentors and mentees to make ourselves and others great and hold each other to a high bar. We don’t want to let our teams down. We stay humble and constantly ask for constructive feedback. 6. Simple & scaleable Lyft TPM teams take complex problems and break them up and dilute them down into simple, elegant, and scalable solutions. Plans change, so we must be adaptable and agile to business needs. We achieve this through iterative delivery, learning, automation, and experimentation by shipping to get data and feedback allowing us to quickly adjust. We start small with P0 prototype(s) and then MVP (s) to prove our hypotheses and then build on them week-after-week adding in P1s and P2s to reach full capability. We embrace lean principles and agile methodology to keep it simple and ship small increments quickly — when we fail we fail fast, learn, and change course. 7. Predictable We manage to impact & outcomes. We keep the trains on the track and on time to spot risks ahead of time which allows us to respond and avert issues before they happen. When risks do materialize, we use a combination of diverse ideas from the team and data to present a clear call to action to get delivery back on track. We do this by: Building diverse and inclusive program teams Limiting the amount of work in progress (WIP) at once Breaking down work into small vertical chunks to support iterative shipping to prove our assumptions Carrying little technical debt Maintaining 100% test passing Always listening to our customers We have tenacity and grit to work relentlessly to make programs and portfolios successful. We are meticulous planners, but we understand that the only constant is change and frequently update our plans based on learnings and changing business needs. 8. Portfolio approach We take on complex, cross-functional initiatives, and company OKRs to deliver the most value against the highest business priorities. Large, complex, and highly cross-functional programs are where we add 10x the value. At Lyft we exercise good judgment in terms of when to work on outside projects which align with company values, whether giving team members an opportunity to volunteer with an organization they believe in or to work on exciting Open Source projects. 9. Fun ! We fully utilize our talents working on interesting and challenging problems in an effective system, working on a great team with teammates that we like, to make customers happy. This means we have much more fun at work! As a result, we are motivated, more dedicated, and much more productive. Coming soon: a post about how I used these principles to optimize flow and deliver on a large cross-functional program at Lyft. I would love to hear your feedback and thoughts: @JWalpole ! As always, Lyft is hiring! If you’re passionate about developing state of the art products and services or building the infrastructure that powers them, read more about them on our blog and join our team . Stories from Lyft Engineering. 414 Tpm Lean Agile Flow Agile Transformation 414 claps 414 Written by 🇬🇧🇬🇬🇺🇸🏳️‍🌈 @lyft @twitter @microsoft | tech investor | dog lover 🐶 | School of Hard Knocks and University of Life | #blacklivesmatter | Miami Beach 🏖 Stories from Lyft Engineering. Written by 🇬🇧🇬🇬🇺🇸🏳️‍🌈 @lyft @twitter @microsoft | tech investor | dog lover 🐶 | School of Hard Knocks and University of Life | #blacklivesmatter | Miami Beach 🏖 Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-06-30"},
{"website": "Lyft-Engineering", "title": "how lyft does remote learning differently", "author": ["Jen Gilbert"], "link": "https://eng.lyft.com/how-lyft-does-remote-learning-differently-d57fdb19dde7", "abstract": "Data Data Science Engineering Mobile Product Security I manage the Tech Learning team at Lyft. Until recently, my team onboarded newly hired SWEs and other technical employees at Lyft HQ. Our students spent up to two weeks with us in our dedicated classroom and in our coffee-shop-style learning lounge, learning about Lyft’s architecture, software development life cycle, reliability best practices, and so on. When shelter-in-place took effect on March 15, 2020, we found ourselves pivoting to a remote program under tight deadlines. We only had a few days to design a remote program before the next group of new hires arrived … we threw it together, welcomed our first remote cohort of ten students, and watched the feedback roll in at the end. “This is without exaggeration the best onboarding experience I have had,” one student said. The program was a hit, with overwhelmingly positive feedback. Remote learning is notoriously unenjoyable. But our version succeeded for the same reasons our in-person onboarding succeeds: we were already obsessed with building automated, hands-on learning experiences, and those translate well to a remote format. To understand the remote experience, it helps to first familiarize with our live experience, which isn’t a traditional presentation-based onboarding program: it’s challenge-based, and its delivery combines online learning with live classroom interaction. Technical people enjoy thinking critically and solving problems, and we built our entire program around that idea. While live onboarding does include social elements and lightning talks, 90% of it is delivered as a series of interactive self-paced challenges. For example, in one of our most popular challenges, pairs of students are given 15 minutes to translate a long text description of our data architecture into their own whiteboard diagram. In a more traditional onboarding program, such a diagram might simply be shown to the audience on a slide. Even when working independently, our students always know: which scenario they should imagine themselves in, the problem we want them to solve, and which docs or other resources will help them accomplish that task. Most of the challenges are short, lasting 5 to 10 minutes, with the longest ones taking up to an hour. It’s impossible for a human facilitator to dole out each student’s next challenge manually throughout days of onboarding. So, even in our live classroom experiences, we deliver our challenges in an online platform called Qualtrics. We use Qualtrics to collect each student’s challenge answers along the way. The question format is mostly open text or similarly flexible inputs (like uploading a hand-drawn diagram), which offers accountability while avoiding the oppressive formality of standardized testing. It feels more like a conversation than an exam, and this casual approach signals our trust in our students. We find that our students value accountability as long as it’s offered respectfully, since they understand that upbeat extrinsic motivation makes hard work easier. Our online challenges are designed for live classroom use, including social elements like paired challenges and “get to know your neighbor” prompts. These elements encourage friendly conversation and teamwork. You can often hear chatter in the classroom as people work together to solve a puzzle or draw a diagram. When we suddenly needed to build a remote version of onboarding, most of the pieces were already in place. Qualtrics allows conditional content tweaks based on the incoming URL, so we were able to make modifications without having to maintain a full duplicate instance of onboarding. We just needed to: Set up a unique URL for the async version of onboarding. Record the lightning talks in lieu of live guests, and weave those in. Remove the pairing prompts so that every challenge set is a solo set (but we’d love to find a remote-friendly way to mix pairing back in!). Assign an oncall facilitator to our Slack channel for icebreakers, troubleshooting, and friendly banter. That’s it — no awkward videoconferences required. Our format sets higher expectations for students than “look awake on camera,” while also offering them the adventure of figuring things out for themselves. As mentioned above, our students seemed to genuinely enjoy the remote experience, giving it an average rating of 4.84 out of 5. It’s tough to do much better than that, as even the most amazing products on Amazon can attest. One student said, “Big thumbs up to the entire virtual onboarding experience. I might have enjoyed virtually onboarding more than in person onboarding.” But the remote version does have its drawbacks versus the live experience: our students’ progress through the challenges is consistently slower in isolation, and students are more likely to wander away, requiring more reminders in order to complete the full program. None of our program’s drawbacks are surprising to anyone who has ever attempted to stay disciplined while wearing their pajama pants all day. In the short term, these anticipated drawbacks were acceptable to us. But now that business travel is uncommon and we’re expecting our remote program to fill longer-term gaps, we’re finding creative ways to increase engagement and accountability. The likely answer is a concept just as unpopular as remote learning: the dreaded group work. We’ve committed ourselves to offering a fresh take on it, and we’re considering everything from building virtual escape rooms to awarding bonus swag to the entire cohort after they pass a given checkpoint. We know one thing for sure: the answer isn’t more videoconferencing. Watching a presentation from home just doesn’t feel like attending in person, even if students muster the willpower to close their other tabs as instructed. Staying focused in that passive scenario is exhausting. Instead, we want to continue to find more medium-appropriate ways to deliver the most important criteria of a live classroom experience: People feel seen. People feel accountable. People feel immersed. The praise from our students so far is encouraging, but we’re excited to do more. Scalable education was a challenging frontier even before COVID-19, so we know that any improvements we can make in our approach can deliver enormous impact far beyond our current circumstances. What we offer is far from perfect, but we’re very excited about the progress we’ve made, and the opportunity for innovation that now lies before us. If you are interested in joining our team and seeing our cutting-edge onboarding for yourself, check out our careers page ! Stories from Lyft Engineering. 146 1 Thanks to Polly Peterson . Onboarding Process Onboarding Onboarding Tools Remote Learning Covid 19 146 claps 146 1 Written by Stories from Lyft Engineering. Written by Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-05-27"},
{"website": "Lyft-Engineering", "title": "building a seamless experience to request a ride for others", "author": ["Glen Robertson"], "link": "https://eng.lyft.com/building-a-seamless-experience-to-request-a-ride-for-others-b80f09db6aa2", "abstract": "Data Data Science Engineering Mobile Product Security By Glen Robertson and Tak Cho Amy is outside a busy supermarket, where she has purchased essential food supplies during the shelter-in-place order. She’s waiting for a Lyft ride home, which was requested by her friend, Deb, who wants to help cover her costs. Her driver, Marc, is a few blocks away. He attempts to call the rider, Amy, to ensure she is ready outside. However, Deb, the ride requester receives the call. After a game of phone tag between all three of them: Marc confirms with Deb that Amy is ready outside. He tries to identify Amy among the sea of people waiting for rides, but the name and the photo in the app is Deb’s. Drivers are lining up behind Marc while he looks for her, and he is being pressured to leave the pickup area. As Lyft has continued to grow, we have observed more riders unofficially requesting rides for friends and family. This use case became apparent when analytics revealed rides where the pickup or drop-off locations differ drastically from the requester’s location. It became clear that we needed to officially support this experience to solve the identity problem, improve the pickup experience, and most importantly enhance the safety of our riders and drivers. That’s why we built the ability for riders to request rides for their friends. This post discusses the challenges we encountered while building this experience into the Lyft apps and how we addressed them: developing the user experience; managing the ripple effect of a new ride request feature across teams within the company; and ensuring a reliable rollout during COVID-19. The ability for the rider to be able to see the ride details and track the driver is an important part of ensuring a smooth experience. When the requester requests a ride for a friend or family member, we want to allow both the requester and the rider to be able to monitor and control the ride synchronously in their own app. Allowing a ride to be synchronized between the requester’s and rider’s apps was non-trivial: There was an existing assumption that the ride has one user who performs three roles: requesting, paying for, and taking the ride. The requester and the payer are the same, but the rider is now different. We needed to break this assumption, while allowing existing functionality to still work for the rider. We changed the way the app fetches active rides for the requester, adding rides that the requester has requested for friends. This allows us to show the ride in the requester’s app as well as in the rider’s app. We had to add new secondary indexes into our databases so that we could query these new active rides without affecting performance. We modified server-side code that controlled which actions are allowed or what UI components should be rendered on the app for the requester and the rider. Only the requester should be able to add tips, but only the rider should be able to rate the driver and exchange text messages and/or calls with the driver. To build the feature, we had to change screens throughout the ride request flow. We needed requesters to be able to select another rider on the pickup and drop-off selection screen. We also needed to update the rate and pay screen to prevent the requester from rating the driver, and prevent the rider from tipping the driver. These screens were owned by different teams, so we had to collaborate to ensure there were no conflicts with other projects being worked on in parallel, and no negative consequences of our changes. Building the functionality on the server side required modifying 21 microservices across 16 teams. Fortunately, there is a high level of standardization in how they are built. Microservices at Lyft are built in Python and Go. Each codebase has the same overall directory structure, and uses largely the same libraries to provide APIs. A Lyft microservice can be launched in a development environment without any specific knowledge of how it works. APIs are configured in a single repository, and automatically generates code for the iOS and Android codebases to consume. Most importantly, engineers across other teams are supportive and welcome contributions from other teams. Our collaboration extended well beyond engineering. Some of the teams we worked with across the company include: integrity, legal, data science, data engineering, design, UX research, marketing, customer support, and communications. We had to think through what would happen when we break the assumption that the requester is always the rider: Growth accounting and user reporting: we created a new scenario where the requester are different users, requiring us to re-evaluate existing reporting. Fraud: we had to think through how this would impact our fraud protections, and what new fraud vectors might we introduce. We revisited existing protection mechanisms and made updates so that those are evaluated with the correct user (requester or rider). Legal: we needed to ensure that this feature complied with applicable privacy laws as well as our privacy practices. Support: our support agents had to handle new types of support tickets. We had to update our support tools to show the requester and rider were different, and train support agents to handle these new cases. For example: only the requester who paid for the ride should be able to receive concessions from a payment issue. Experimentation: existing rider metrics were derived from the rider’s perspective. We had to add new metrics for the requester’s perspective, and ensure that any effects with existing metrics were understood. Answering these questions required a concerted effort across the company. As shelter-in-place orders have taken effect across the U.S. during COVID-19, the stakes for safe and reliable rides, as well as the stability of our apps have become more important than ever. Getting riders to doctors for appointments or to grocery stores for essential food supplies are critical during this time. At the same time, the reduced traffic presented obstacles to our typical testing for shipping a new feature. We needed to roll out the feature much more slowly and monitor it more carefully, in case there were unforeseen negative effects. It was crucial that we deploy the new feature without causing any bugs or crashes in the app during the ride request flow. Key to this stability was a clean isolation of the feature’s components from critical code paths, and a high degree of testability. We also needed a reliable way to disable the feature in production in case something did go wrong. To achieve all of this, we built the feature with a unidirectional-flow oriented architecture. The rider selection component manages its own state: the selected rider ID. Once the requester has selected another rider: the selected rider’s user ID is sent to the ride request handling logic, which injects the rider ID into the ride request, and therefore lets the ride object be created with the correct payer, requester, and rider. The architecture includes safety flags by default, so the component can be easily disabled and hidden if it causes any issues. Stay tuned on the Lyft engineering blog for a future blog post covering the architecture in detail. While building for the rider’s experience, one challenge we encountered was handling riders whose apps are on older app versions. We do not force riders to upgrade their apps due to the high friction experience of having to re-download the app. We had to carefully adjust the existing APIs to enable the new functionality. This was feasible with existing server controls that we could leverage to toggle the visibility of the rating and tipping components, as well as showing an error if they try to update their destination. If the rider does not have the Lyft app installed, they are able to view a web based view of the ride: we were able to re-use our existing product that allows sharing ride details with others for this functionality. As Lyft has grown, it has become increasingly important to think ahead about what we may build next. When we build features to launch today, we must also be sure we are considering potential future use cases, so we pave the path for future engineers. The server controls were fortunately put in place by engineers who were designing for the future. In our case, we focused on building this product by creating a ride object with payer, requester, and rider all mapped to the correct users. This allows us to not only provide the ability to request rides for friends and family, but also prepare for future functionalities such as: Requesting a ride for multiple riders at the same time. Allowing the requester to follow one or more rider’s ride that they requested for. Enabling seamless communication between the ride requester, the rider, and the driver. There is more work to be done, but we hope that applying forward-thinking will help to pave the way for future work. Try out requesting a ride for a friend today in the iOS and Android Lyft apps. If you are interested in working on these problems, come join our team and help us to improve people’s lives with the world’s best transportation: check out our careers page ! Thanks to the following people who played critical parts in building this product (in alphabetical order): Adam Cmiel , Ajay Andrews , Akshay Patil , Alex Dailida , Alice Zhang , Alison Marlin , Amanda Schroder , Amy Schultz , Andrei Stasevici , Anton Tananaev , Ashwin Raj , Bomee Park , Brad Ellis , Brady Law , Brian Ng , Byron Wilkes , Caitlin Osbahr , Carolyn Buehler , Chris Martin , Chuanxin Hu , Cooper Smith , Daniel Sumstine , David Hildebrand , David Kwan , Delphina Yuen , Denis Nekliudov , Elias Ramirez , Eunice Joung , Gabriel Lanata , Gerald Lee , Glen Robertson , Gonzalo Larralde , Harita Yenisetty , Harith Khawaja , JP Simard , Jaden Choi , Jason Bridgemohansingh , Jeff Hurray , Julio Carrettoni , Kang Lee , Kyo Kim , Lauren Frederick , Leo Jiang , Liuyin Cheng , Marcel Ortiz , Melissa Hamilton , Mengying Yang , Meredith Guo , Michael Lodick , Morgan Holland , Neil Shah , Nick Ung , Patrick Barry , Preet Anand , Rebecca Shields , Reza Mostafavi , Robin Chou , Scott Berrevoets , Sean Shi , Shawn Shaligram , Shivendra Kishor , Stephanie Carpenter , Suresh Pragada , Tak Cho , Tory Nelson , Xiang Long , Xinwei Gao , Yvonne Wong Este artículo también está en español: eng-espanol.lyft.com Stories from Lyft Engineering. 539 1 Thanks to Polly Peterson . Identity Lyft Mobile Backend Engineering 539 claps 539 1 Written by Software Engineer at Lyft. I like Swift, Python, OSS, long walks on the beach. I also enjoy mastering the art of constructing biographical messages under 160 ch Stories from Lyft Engineering. Written by Software Engineer at Lyft. I like Swift, Python, OSS, long walks on the beach. I also enjoy mastering the art of constructing biographical messages under 160 ch Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-11-11"},
{"website": "Lyft-Engineering", "title": "translating lyft into spanish", "author": ["Zak Haitkin"], "link": "https://eng.lyft.com/translating-lyft-into-spanish-6f14cb37c7fd", "abstract": "Data Data Science Engineering Mobile Product Security This article is a translation. Original publication here . In the United States, the Latinx community comprises a large part of the population. Therefore, many of Lyft’s drivers and riders speak Spanish. However, before 2018, these users could only access the app in English. This issue prompted several of our engineers in 2018 to engage in discussions with other companies to learn more about the solutions they used in making their respective applications available in multiple languages. After these conversations, we at Lyft decided to do things a little differently than other technology companies. This solution allowed us to have more than two languages available in the app. We decided to start with Spanish, since, after English, it is the most common language preferred by Lyft drivers based on their mobile device settings. There are two types of platforms on which we store Lyft content for translation: client (in mobile apps) and on our servers. For both, a new type of data structure was created, the “translatable string,” which contains not only a string of characters but the context in which it will be used. Here is a very simple example: In this case, it would seem evident that we are looking to translate a single word, “red”, which alludes to the color of the car. It is not so complicated for the translator to understand, however, in more complex examples, the context will help him or her find the right translation for each situation. For both the client and the server, there are several elements that we have to define before showing our system designs. First, it’s important to note that we use a third-party translation management system (TMS) that handles the translations we send them. Then, we created an internal microservice in Lyft called “translation service.” This service handles interactions between the third-party system and Lyft’s mobile services and apps that need to translate a string of characters. This service takes care of three things: Extracting character strings that need to be translated. Sending those strings to the TMS. Distributing the translations to the microservices or clients (mobile apps) that need them. This is the overall solution we’ve implemented at Lyft. However, certain specific considerations will change depending on whether the translations are delivered to microservices or mobile (client) applications. In mobile apps, there are a lot of strings, but the advantage is that a large part of them are constant. This makes the design of the mobile solution a little simpler. Here you can see the diagram that explains the design that’s used to translate the strings for the client apps: In this case, many of the strings to be translated are dynamic because they may be stored in a database or in a configuration file on the servers. This makes the solution in this case a little more complex. Here, apart from the translation service and the TMS, there is also a configuration repository containing translations called “translationdata.” This data is downloaded to the servers of the services that subscribe to it by way of the configuration of the service that needs it. This design allows translations and changes to be made when the data changes dynamically. This can be seen in the following diagram: When we performed the first tests of our translation systems, we realized that some strings in Spanish did not fit within the available screen spaces for certain buttons, since this language tends to use more characters than English in order to express an idea. To fix this problem internally, Lyft created a way to control the language in the app’s alpha settings so that you can change the app’s language to check different strings and how they are displayed on the screen. In the following example, we can see the above with the screenshots I took from the alpha version app on my mobile phone. When the first translations came out, many Spanish-speaking employees had the opportunity to test the Lyft apps in Spanish and see what errors were present in some of the translations which failed in the contextualization process, as well as make suggestions to correct them. For example, one of the errors I found the first time I used the app in Spanish was that on the promotions page, whereas in English it said “Free rides”, in Spanish it said, “Viajes libres” (free as in speech) instead of “Viajes gratis” (free as in products). At Lyft, these changes were crucial to the services we offer. They could not have happened without the efforts of key team members such as Dan Sullivan, Julien Silland, Ava Zhang, Kris Gellci, Brian McConnell, Ryan Grasell, Zak Haitkin, Alex Atencio, Antonio Luna, Jonathan Schoonhoven, Rahul Phalak, Dan Barak, among others. Lyft is hiring. If you want to be a part of our team, check out lyft.com/careers . Original article by Miguel Molina . Translated to English by Zak Haitkin. Stories from Lyft Engineering. 189 Localization Translation Transportation Internationalization Localization Service 189 claps 189 Written by Stories from Lyft Engineering. Written by Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-07-10"},
{"website": "Lyft-Engineering", "title": "introducing lyft engineering en español", "author": ["Fernando Augusto López Plascencia"], "link": "https://eng.lyft.com/introducing-lyft-engineering-en-español-abe1975e3e27", "abstract": "Data Data Science Engineering Mobile Product Security Este artículo fue publicado originalmente el 31 de marzo 2020 en eng-espanol.lyft.com . Two of Lyft’s core values are “Be Yourself” and “Uplift Others.” Many of us, members of the tech organization at Lyft, pride ourselves in having Spanish as our first or second language. It is in the spirit of being ourselves and supporting the Spanish-speaking community that today we proudly present the Lyft Engineering Blog En Español . This blog will not only be the home of translations of relevant articles from the English blog, but also original content, written and published in Spanish. This effort has come to fruition for a number of reasons: Even though the quantity of technical literature in Spanish is increasing, it’s still far from being as abundant as it is in English. At Lyft, we want to do our part and increase access to specialized literature in Spanish. Lyft is a pioneer in transportation network technology in North America. We believe that we have much to provide to the discourse regarding these topics. By launching this blog we want to create a space to share our experiences as a company and use them to help better serve the Hispanic community. We recognize that the tech industry has systemic problems around issues of diversity. At Lyft we believe not only in diversity, but in belonging: that it is the equal right of all to have a chance to be a part of the industry, using their origins and culture as a strength instead of a weakness. This blog will be another one of our constant efforts to promote diversity and belonging in our industry and, through that, in this specific case, the empowerment of the Latinx and Hispanic community around the world. Even though our content will be predominantly technical, we also want to approach the population who is not. Lyft has the mission to improve people’s lives with the world’s best transportation — and that means all people. We want to provide our ideas and start a conversation with the Spanish-speaking general audiences; the Spanish blog will be a platform to do this. Our goals may sound lofty, but our third core value is “Make it happen”. Every big journey starts with the first step. Today we are very happy to make this blog a reality and to take a step forward with you, our readers. We believe Lyft truly is an excellent place to work. Diversity and belonging are at the center of who we are. If this resonates with you, check out our open positions and come join us! ¡Bienvenidos! (Welcome!) Stories from Lyft Engineering. 107 1 Spanish Engineering Announcements Lyft Inclusion 107 claps 107 1 Written by Stories from Lyft Engineering. Written by Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-11-12"},
{"website": "Lyft-Engineering", "title": "hashing and equality in python", "author": ["Roy Williams"], "link": "https://eng.lyft.com/hashing-and-equality-in-python-2ea8c738fb9d", "abstract": "Data Data Science Engineering Mobile Product Security Don’t override __hash__ and __eq__ to force objects to hashable. Use immutable objects instead. Dictionaries and Sets are some of the most common data structures, used heavily for their O(1) lookup times. This O(1) look is enabled by hash functions which have the following properties: If a == b then hash(a) == hash(b) If hash(a) == hash(b) , then a might equal b If hash(a) != hash(b) , then a != b Dictionaries and sets, regardless of language, use these assumptions to enable fast O(1) lookups. This post will focus on Python, but the lessons are applicable regardless of language. Python dictionaries are quite advanced and optimized — for the details see this great talk by Raymond Hettinger , but to understand why these properties are important we will go over how they work at a high level. Call __hash__ on the key to compute the hash of the key. If the key is not hashable raise a TypeError Store (hash_value, key, value) in an array at location hash_value % len(array) . If the array requires resizing, re-use the previously computed hash_value s to re-insert all previously stored values. Call __hash__ on the key to compute the hash of the key. If the key is not hashable raise a TypeError Look in hash(key) % len(array) for an entry with a matching hash_value . If one exists — check for equality, first by identity, then by calling __eq__ . To have functionally correct dictionaries, a few things follow from this: __eq__ and __hash__ must agree - equal objects must have equal hashes. __hash__ must never change. The hash of an object is never re-computed once it is inserted. Objects that implement logical equality (e.g. implement __eq__ ) must be immutable to be hashable. If an object has logical equality, updating that object would change its hash, violating rule 2. dict , list , set are all inherently mutable and therefore unhashable. str , bytes , frozenset , and tuple are immutable and therefore hashable. Let’s first dive into how objects work if we don’t implement __hash__ and __eq__ . By default, equality and hashes are based on the identity of an object. The current implementation is id >> 4 (we should never rely on this implementation detail). Dictionaries and sets assume that if an object has an equivalent identity to an object in a set or dict the objects are equal (e.g. we assume an object is always equal to itself). This is an important optimization since __eq__ functions can be expensive. __hash__ is only called once when an object is inserted into a dict or a set . Note we never see the hash for a or b called again even though we've forced the set to resize by inserted 10,000 more items and we've retrieved a logically identical object. This is why it is absolutely critical the __hash__ of an object never change over its lifetime. This works out of the gate for object identity (if id(a) == id(b) we know a == b ). Should the __hash__ change we can get into a situation where dictionaries and sets no longer work and have extremely hard-to-find bugs. So far — so good. We’ve outsmarted Python and really forced that dictionary it didn’t want to hash to be hashable! But what happens if we mutate d - the underlying dictionary Wat? I thought an object was always equal to itself? Why is x in the list but not in the set which contains the exact same objects? Same for a logically equivalent object? Recall — all of the optimizations that enable dictionaries and sets to be super fast with O(1) lookups require hashes to never change. By fighting against the language we've introduced the opportunity for really subtle, confounding bugs. I’m glad you asked!! If we really want these objects to be hashable we can achieve this with immutability! There’s a few built-ins we can leverage, namely tuple and frozenset , and some third-party-ish libraries we can use - immutables and attrs . While both of these are third party libraries, both have strong analogs in the Python standard library (immutables may be part of the standard library in 3.9 as it's used as part of asyncio contextvars and attrs had been mostly duplicated in the standard library with dataclass ) . If we have a list we want to hash, we can convert it to a tuple to ensure it's immutable: If we have a set we want to hash, we can convert it to a frozenset to ensure it's immutable while still maintaining O(1) lookups: We’re going to use two high quality third party libraries, attrs and immutables attrs supports frozen objects which can be treated as immutable and therefore are hashable. As bonus points it gets rid of a ton of boiler-plate code! For dict we're going to use immutables.Map . immutables.Map is an immutable Map, and therefore Hashable. Any mutations create a new map all together, but leaves the underlying map unchanged. It's significantly higher performance than deepcopy . Fighting against the language to try to force objects to be hashable is both slow and unsafe — wasting money, hurting response times, and damaging reliability. We’ve gone over some of the tools you can use here to make objects hashable when you need to while preserving correctness and performance. We used Python as an example here, these problems and contracts aren’t limited to Python — they’re fundamental to how data structures like hash maps or hash sets work. This blog post is also available as a Jupyter notebook you can download and play with the examples yourself! If you find yourself fighting against the language stop and think if this is really the correct thing to be doing. If you’re interested in joining the team at Lyft, we’re hiring! Feel free to check out our careers page for more info. Stories from Lyft Engineering. 166 Thanks to Ryan Lane . Python Programming Programming Languages Lyft Hashing 166 claps 166 Written by Stories from Lyft Engineering. Written by Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-04-15"},
{"website": "Lyft-Engineering", "title": "building a design systems library with figma scripter", "author": ["Alex Lockwood"], "link": "https://eng.lyft.com/building-a-design-systems-library-with-figma-scripter-c046df0a895c", "abstract": "Data Data Science Engineering Mobile Product Security Over the past few months, I’ve been helping Lyft build the initial version of our Figma design systems library, which contains all of the colors, text styles, and components used by designers across the company. As we worked on the project, we began to notice patterns of time-consuming tasks, some examples being: Generating helpful descriptions for our 500+ color styles. Optimizing each of the 900+ icons in our icon library. Finding and deleting private components that were no longer being used. Eventually we became curious as to whether we could automate some of these processes. We discovered a Figma plugin called Scripter that allowed us to write quick-and-dirty scripts in TypeScript using the Figma Plugin API . It has significantly sped up our workflows ever since. In this blog post, I want to share a few examples of how we’ve used Scripter to help build our design systems library in Figma and automate tasks that would have otherwise taken hours or even days to complete. Figma allows users to add descriptions to their styles that show up as tooltips when designers hover over them in the style picker. One thing we wanted to do was update the description of each of our color styles to be the color’s RGB hex code, making it easily accessible for designers. These descriptions are easy to add and edit in Figma: simply right click the style, select ‘Edit Style,’ and update the description. However, our design system consists of over 500 color styles, and updating each individually would have taken hours. One of the first scripts we wrote was designed to automatically generate these tooltips. For each color style, the script analyzed the style’s color, generated its corresponding RGB hex color code, and set it as the style’s description. Like many design tools, Figma allows one to optimize icons by ‘flattening’ them into a single layer. We’ve found it to be good practice to flatten icons before publishing them for a few reasons: Flattened icons result in less noise in the layers panel. Icons that contain less layers are more efficient to render in Figma, especially in mocks that are heavily dependent on our icon library. Flattening icons after they have been published breaks overrides that designers have applied. That said, our icon library currently consists of over 900 icons, so flattening each one independently through the Figma UI was unrealistic. Fortunately, the Figma Plugin API provides the figma.flatten() function that can help automate the process! Note that this script makes a few important assumptions to be aware of. Specifically, it assumes that (1) every component in the file is an icon, (2) each icon contains a single color, and (3) the icons don’t use masks. The figma.flatten() function may not work as one expects if one of these conditions aren’t met. Most of the components in our design systems library are created using atomic building block components . For example, each of our button components share the same internal background component, which is private to the file and makes our buttons easier to update and maintain. When Auto Layout was released , we rebuilt several of our components from scratch and many of these building block components became outdated and were no longer necessary. Unfortunately, Figma doesn’t provide a built-in mechanism for searching for local instances of a given master component, which made it difficult for us to confirm whether or not some of these components were still being used in the file. To solve this problem, we wrote a script! It allows us to select a component to delete and takes care of removing the component if it’s private and unused in the file. These are just a few of many scripts we’ve written to build our Figma design systems library. Here are some other examples: Generating color styles. We wrote a script that parses a JSON file containing all of the colors in our color spectrum and created a corresponding Figma color style for each using figma.createPaintStyle() . This saved us hours of work that would have otherwise been spent manually creating hundreds of color styles through the Figma UI. Searching for layers that aren’t using our color styles. We want all of our components to make use of our color spectrum styles rather than hardcoded hex colors. To verify that this is the case, we wrote a script that searches files for color usages that are not linked to a Figma color style. We determine if a layer has a fill color by analyzing its fills property, then verifying the fill color is linked to a Figma color style by checking that the layer’s fillStyleId property is non-empty. Check out this script for an example. Building custom selections. Figma provides numerous tools for selecting multiple layers at once. However, sometimes we want to create a selection satisfying our own custom set of criteria (e.g. selecting all instances of the currently selected master component). Fortunately, building selections is relatively easy to automate in a script. We do this by traversing the current page’s layer hierarchy, creating a list of layers you want to select, and assigning that list to the figma.currentPage.selection property. Cleaning up layer names. Every once in a while we run a script that cleans up our layer names by removing trailing whitespace, fixing capitalization mistakes, adding missing spaces before and after ‘/’ characters, etc. This has significantly reduced the number of accidental typos we’ve made, and has ensured that naming remains consistent across our libraries. Writing scripts has been essential for accelerating up the development of our Figma design systems library, and has also made working on the project a lot more fun! We’d love to hear ideas and stories about how others automate their workflows in the comments below! And if you’d like to work with me, or any of our amazing designers and engineers here at Lyft — we’re hiring ! Stories from Lyft Engineering. 2.2K 6 Thanks to Michael Rebello . Figma Design Systems Typescript Design Plugins 2.2K claps 2.2K 6 Written by Stories from Lyft Engineering. Written by Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-03-10"},
{"website": "Lyft-Engineering", "title": "what is data science at lyft", "author": ["Simran Mirchandani"], "link": "https://eng.lyft.com/what-is-data-science-at-lyft-4101a69be028", "abstract": "Data Data Science Engineering Mobile Product Security By Simran Mirchandani and Thibault Martin Throughout this post, we will discuss our various (and newly rebranded!) Lyft Data Science roles, explain how Data Science fits into the rest of the company, and describe some unique challenges that our Data Scientists tackle. If you are interested in learning about our work and how we approach it, read on! Lyft has assembled a team of 200+ Data Scientists with a variety of backgrounds, interests, and expertise in order to make the best possible decisions, and thus build the best possible product. We organize the team based on the typical output they produce: Decisions and Algorithms. Data Scientist, Decisions: “Data Science for Humans” Decisions Data Scientists influence decisions made by executives, product managers, engineers, ops/business teams, and other stakeholders. They utilize a deep understanding of the business to develop decision frameworks that drive alignment on the most impactful problems and solutions. Data Scientist, Algorithms: “Data Science for Machines” Algorithms Data Scientists develop models that power internal and external production systems. They typically apply a combination of optimization, machine learning, and inference methods to design, improve, and monitor models and systems. Many components of day-to-day work are the same across all Data Scientists, such as identifying product or business opportunities, surfacing data insights, designing experiments, and interpreting their results. The strength of the team comes both from these common expectations as well as from a diverse spectrum of complementary skills . The distinction between Decisions and Algorithms helps us hire and staff based on business need: influencing human decision-making or writing high-quality production models. However, the archetypes are not intended to be restrictive. Depending on the need and people’s ability to execute or desire to learn new skills, Data Scientists are encouraged to explore both spaces. Teams at Lyft are organized by focus area, each of which includes members from multiple functional areas of expertise (Product, Engineering, Data Science, Design, Business/Operations). The various functions sit together and work hand-in-hand towards common roadmaps and targets. The goal of this structure is to clarify ownership, streamline communication, and enhance cross-functional collaboration. Here’s a sneak peek at what each focus area works on and how Data Science contributes. This area is responsible for the underlying systems that decide how to dispatch and price each ride. Marketplace is all about trade-offs and efficiency: making the best use of our resources to keep drivers, riders, and the company happy. Some example projects are: Designing a model to assign drivers to riders across a city in the most time- and cost-efficient manner. Determining which riders should be matched in a Shared ride such that all riders have positive experiences, while also making the ride profitable. Leveraging pricing and incentives to balance supply and demand. This group focuses on acquiring and retaining riders by building high-quality end-to-end products and experiences. We do this by: Measuring the quality of riders’ experiences, and developing and testing frictionless products and features to improve their experiences. Analyzing past behavior to understand riders’ needs and preferences, and designing memberships and other programs to better retain riders. Optimizing rider acquisition and engagement funnels through targeted models and marketing campaigns. This area focuses on ensuring that we grow, retain, and engage our driver base by providing a best-in-class product experience. We do this by: Developing new pay structures that ensure drivers receive a compelling value proposition of consistent and predictable earnings. Designing and testing new product features to help deliver the most intuitive, distraction-free, and safe driving experience. Optimizing the driver acquisition and onboarding funnels, building models that identify drivers at risk of churning, and creating user segmentation that enables customized product and marketing features. This group owns the growth/profitability goals for Rideshare, and accordingly builds plans and sets market-based strategies. Some focuses are: Building the diagnostic tools that improve our understanding of the business, for example through anomaly detection, attribution, competitive intelligence, and reporting. Building investment plans under various scenarios that evolve with changing conditions, for example through forecasting and optimization. Creating decision making frameworks to optimize our investments in our riders, drivers, and our marketplace, for example through causal attribution and valuation modeling. This area helps to build and measure the quality of Lyft’s map, map data, and the numerous services built on top of it. A few of the problems that Data Science tackles are: Providing the most accurate inputs (travel times, travel distances, driver and rider locations, pickup and drop-off spots) to downstream algorithms. Establishing a framework for measuring map accuracy, which includes bug detection for new updates, inferring missing map elements, and establishing application-specific quality criteria. Contributing to Lyft’s routing technology in order to generate probable and optimal routes between two points in real-time. The group’s focus is on applying cutting edge data science models and frameworks to our billions of dollars of variable costs such as payments, support, fraud, and insurance. Example projects include: Building a telematics platform to convert raw phone sensor data into signals like aggressive braking or phone use so that we can help drivers improve their driving abilities and reduce accident rates. Optimizing our routing of transactions across different payment providers to balance costs with long-term value. Identifying and classifying the root causes of negative experiences so that we can predict and prevent them for our users. Lyft Business focuses on building scalable commute, courtesy, and travel management solutions to help organizations reduce costs and streamline their ground transportation needs. Data Science work includes: Driving product decisions by developing frameworks and tools to understand gaps in our current product experience and identifying opportunities for improvement. Developing techniques and ROI frameworks to identify opportunities to partner with organizations (like in the airline or hospitality spaces) to provide high-quality travel experiences and loyalty programs. These areas bring scientific and research-based expertise to help solve some of the most complex problems at Lyft and in the broader transportation-as-a-service industry. We collaborate with partners to develop transformative solutions to high-risk yet high-reward problems. Recent efforts include: Delivering novel solutions for real-time driver supply, driver earnings, and Shared rides efficiency. Leveraging state-of-the-art statistical, machine learning and econometric techniques to improve Lyft’s ability to measure the causal impact on the marketplace of various levers like pricing and new product launches. Pairing existing economics literature with econometrics and statistics tools to explore research questions such as: How should we price a new mode? What impact do we think new regulations will have on the business? How do new roads affect travel speeds? Fleet includes Express Drive (Car Rentals for Drivers), Lyft Rentals (Car Rentals for Consumers), and Driver Service Centers . Data Science helps improve Fleet’s growth and profitability. Some examples are: Building models to determine optimal pricing to balance demand, supply, and driver risk. Developing frameworks that minimize operational costs by identifying risky drivers, improving fleet forecasting, and optimizing retail locations. Improving renter retention and engagement by identifying opportunities to improve the funnel and product experience. This area develops and supports all of the non-car products at Lyft. Some of the unique focuses are: Improving operations by establishing metrics of operational performance and developing mathematical models that optimize on-street product rebalancing, battery-swapping, and maintenance processes. Building multimodal routing algorithms and modeling user preferences by integrating transit and micromobility to ensure Lyft users get the right type of ride at the right price at the right time. Analyzing data sent by the best-in-class vehicles (Lyft- designed and manufactured bikes and scooters!), and developing fusion algorithms with sensor data to ensure the vehicles are ridden and parked safely. This group helps accelerate the rollout of self-driving vehicles onto the Lyft platform. Some sample projects are: Measuring the progress of our self-driving technology on the road and in our simulated environment. Determining which autonomy features to prioritize for rideshare use cases, and growing self-driving pilots in scale by determining optimal locations. Building models to determine the availability of self-driving vehicles for our riders to maximize the utilization of the fleet. We hope this gives you insight into the Data Science team at Lyft, and the myriad of problems we get to solve day-to-day. The varied expertise of our team paired with the company structure that allows us to work so closely with other functions ensures that we approach every decision with rigor and logic. If you are interested in joining our incredible team of Data Scientists and improving people’s lives with the world’s best transportation, check out our careers page ! A huge thanks to the following people for their great contributions to this post (in alphabetical order): Akshat Jain, Alexis Weill, Alya Abbott, Ankit Syal, Elizabeth Stone, Eric Smith, Josh Cherry, Mark Grover, Mike Frumin, Neil Chainani, Nicholas Chamandy, Quang Nguyen, Sarah Morse, Varun Madduri, Varun Pattabhiraman, Venkat Devireddy Stories from Lyft Engineering. 799 Thanks to Mark Grover , Nicholas Chamandy , and Eric Smith . Data Science Data Organizational Culture Machine Learning Lyft 799 claps 799 Written by Stories from Lyft Engineering. Written by Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-04-27"},
{"website": "Lyft-Engineering", "title": "building an adaptive multi tenant stream bus with kafka and golang", "author": ["Xinyu Liu"], "link": "https://eng.lyft.com/building-an-adaptive-multi-tenant-stream-bus-with-kafka-and-golang-5f1410bf2b40", "abstract": "Data Data Science Engineering Mobile Product Security Back in the 2000s, SOAP/WSDL with ESB (Enterprise Service Bus) was the dominant server-side architecture for many companies. Since the 2010s, microservices and service mesh technologies have grown wildly and thus became the de-facto industry standards. Service meshes promote point-to-point communications between 2 arbitrary parties and eliminate the role of a centralized message bus or load balancer. (Lyft retains a centralized bus used by many core services to schedule asynchronous HTTP requests). A message bus is an architecture that enables separate services to work together in a decoupled fashion via synchronous or asynchronous messaging. Nowadays, using a message bus as an integration design pattern has arguably faded away in favor of service meshes, whilst it remains strong in the data streaming arena. At Lyft, hundreds of microservices are backed by numerous databases. New tables are being created constantly to support features and demands of our fast-growing business. Online tables hold critical and time-sensitive data for serving real-time requests from end users. Because of this, running analytic queries against OLTP (Online Transactional Processing) datastores to satisfy data mining and analysis requirements does not suit our needs well. The Change Data Capture (CDC) pipeline is a design in which database change events are captured without interfering OLTP. These events are routed near real time to various destinations, such as Hive tables, where analytic queries can be safely conducted offline. To CDC stream consumers, losing events or malforming the ordering often yields unpredictable consequences. A classic example of this would be a table row update prior to the insertion. With the tenet of data correctness and freshness, Kafka was central to the architecture in our redesign efforts for Lyft’s CDC pipeline. As a stream-processing platform, Kafka guarantees not only message ordering per partition, but also supports at-least-once and exactly-once delivery semantics. During a downstream sink outage or slowdown, Kafka acts as a buffer. It absorbs back pressure and obfuscates the existence of back pressure to event producers. Since this is a lossless stream-processing platform, this back pressure results in message delivery delays. We set out to replace the existing CDC pipelines with a universal “CDC Event Bus” in order to house all of Lyft’s CDC streams and potentially other future stream routing requirements. Our design and implementation of CDC event bus targeted the following features: 1. Multi-tenant support . The bus will house hundreds of CDC pipelines. Adding a new pipeline should be effortless and require little overhead. 2. Pipeline isolation . A broken or blocked pipeline should have zero impact on its neighbors. 3. Autoscaling . Traffic on CDC pipelines varies drastically throughout the day. Additionally, new pipelines continuously onboard. CDC Kubernetes deployments must be resilient to accommodate the changes and automatically scale up/down. 4. Minimal redeployment . Kafka client rebalancing is an expensive operation that leads to message delays and duplication. Rebalancing takes place when the CDC is redeployed or a new pipeline is added. The CDC should automatically detect and apply other configuration changes without restarting Kafka consumers. 5. Adaptive architecture . The CDC framework should be agnostic to the concrete event source and sink types. It should support various event sources, (e.g., AWS DynamoDB , Aurora , RDS ). Meanwhile, sink adapters should be modularized, allowing for easy integration with Firehose , Elasticsearch , Kafka , Kinesis , gRPC , and other downstream consumers. 6. Disaster Recovery. Data from Kafka topics should be backed up in AWS S3 via Firehose . Command line tools are expected to ease replay of any pipeline between 2 arbitrary timestamps. Our CDC event bus is comprised of 2 subsystems: the tailer service and the adapter service. The tailer subscribes and delivers database change events as-is to designated Kafka topics. (Note that every Kafka topic corresponds to a table in a database.) The adapter, on the other hand, does the heavy lifting work. This includes tasks such as event filtering, transformation, stream routing, and error handling. These two subsystems are decoupled by Kafka topics. The tailer actually encapsulates a set of services, each for a different event source type. Examples include DynamoDBTailer, AuroraTailer, etc. By leveraging cloud services like AWS DynamoDB Streams and Kinesis Client Library library , we greatly simplified the architecture of the tailer services. The adapter service accommodates most of the features listed above, and we’ll dig into it more in the sections below. Load balancing and auto scaling After starting on the system design, we quickly realized the challenge of proper load balancing. Traffic (QPS) on database tables varies drastically. For our CDC pipelines, some route hundreds of thousands of messages per second, while others route only a fraction of that. A cost-efficient design would be able to allocate resources proportional to the traffic throughput and avoid hotspots in Kubernetes deployments. A single Kubernetes deployment per CDC pipeline/table would be too expensive in terms of hardware and operational overhead. It is desirable to have a single deployment serving numerous CDC pipelines. Furthermore, Kubernetes autoscaling should function properly when messaging load changes. As a common practice, declaring more partitions on a Kafka topic increases its overall throughput. Partitions can be processed by multiple consumers in parallel. Running on different hardware (VMs or pods), these consumer instances are not subject to the limit of a single network interface card (NIC). But how do we load balance among these Kafka consumers? Kafka offers 2 load balancing (LB) options, a Kafka-managed LB and a client-managed (self-hosted) LB. With a client-managed LB, consumers must specify the targets they’re subscribing to. The complexity of this approach is prohibitively high, given that the following tasks all need to be handled: detecting the liveness of consumers, making sure only one consumer is associated with a topic partition at any moment, handling networking hiccups, defending zombie processes, and more. In essence, it is a scheduling problem requiring advanced tooling and skills to tackle. With Kafka-managed load balancing, Kafka groups its consumers using a consumer group tag. Every consumer instance is tagged with a single consumer group value. With a RoundRobinAssignor (or StickyAssignor ) configured, Kafka evenly assigns topic partitions to each group’s consumers. Our approach was to make all deployed consumers (pods/containers) identical to Kafka. Put simply, all consumers are part of the same consumer group and subscribe to the same set of topics. When a consumer joins or leaves the same group, Kafka shifts partitions among the available consumers. (To be precise, rebalancing is coordinated by a coordinator broker and conducted by the consumer group leader.) A greedy algorithm 1. Let’s start with the simplest scenario: the CDC adapter service connecting to a single Kafka cluster. From Kafka’s perspective, the adapter container instances are identical. Thus, each container (or pod) is allocated in a round-robin fashion with near equal amounts of topic partitions by Kafka. Upon receiving an AssignedPartitions event, the container inspects the configuration settings, figures out which CDC pipeline it is affiliated with, instantiates required resources (such as a Firehose sink adapter), and spins up a dedicated goroutine to process the event stream. The more partitions are assigned, the more goroutines are launched. In contrast, when a RevokedPartitions event is received, the container quits the corresponding goroutine gracefully without disrupting neighbors. 2. Single consumer groups don’t work well with stream fanout (pub/sub) cases. With fanout, one data stream is delivered multiple times to different destinations, each with a distinct checkpoint managed by Kafka. Fanout is taken care of by declaring multiple consumer groups, so that these pipelines are processed by distinct consumers and their checkpoints are maintained separately. As an example, topic 2 and topic 12 are fanned out twice in the chart below. For clarity, an endpoint in the configuration settings maps to one consumer group in a Kafka cluster. 3. Once the limit/quota of a single Kafka cluster is exceeded, future topics are provisioned in a new Kafka cluster. Again, this is taken care of by consumer groups. 4. At this point, each Kubernetes pod has received a near equal amount of topic partitions. Assuming workload on all topic partitions is at the same scale, Kubernetes Horizontal Pod Autoscaler (HPA) should work nicely. Nonetheless, workload varies drastically in accordance with message size, throughput, and transformation logic. To refrain from CPU or memory hotspots, we leveraged Kubernetes deployment partitions. Topics with similar workloads are grouped under the same Kubernetes deployment. The FACET: live property in the above configuration settings dictates the Kubernetes deployment. The chart below describes this best effort approach. Threading model In the CDC adapter service, each stream partition is processed by a designated goroutine with a buffered channel. A master goroutine dispatches messages received by a single consumer into these channels. Once a channel is over half full due to sink slowdown or outage, a pause command is sent to Kafka to prevent channel’s overflow: kConsumer.Pause([]kafka.TopicPartition{topicPartition}) . Left unpaused, this would block the master goroutine. It is due to the buffer in cgo Kafka client , we can’t wait for the channel to be completely full before sending out the pause command. Once a channel occupancy is down to 1/8, the paused topic partition is resumed by calling kConsumer.Resume([]kafka.TopicPartition{topicPartition}) . Essentially, it throttles a specific stream partition, so that other streams sharing the same Kafka consumer are not impacted. It is worthwhile to mention that these goroutines are tracked by a state machine, which provides excellent observability into the lifecycle of individual stream partitions. Dynamic configuration In order to minimize Kafka client rebalancing churn, the service captures configuration changes and applies most of them on the fly without restarting Kafka consumers. Without scanning the entire configuration tree for possible changes, we leverage a Merkle Tree to quickly pinpoint updates in the configuration settings. Batch engine We discovered that in many cases, the sink was the bottleneck of our CDC pipeline. Leveraging bulk/batch API calls on a sink greatly improved its throughput. Often, a sink returns 429 , 400 , and 500 error codes, which must be handled explicitly to prevent data loss. When a bulk write fails, every message in the batch is retried sequentially until a bad one ( 4XX except 429 ) surfaces. A 4XX error initiates a human-intervention step in the workflow. On the other hand, we backoff and retry on transient errors ( 429 and 5XX ). This batch engine design along with a built-in rate limiter also brought in an unexpected financial benefit: with mitigated 429 throttling events, we were able to downscale many over-provisioned Elasticsearch clusters at Lyft. We were inspired by open source batch engines . Modularized sink adapters Most components in the CDC adapter framework are agnostic to the concrete event source and sink types. In the form of Golang function currying , new sink adapters can be quickly developed and deployed. Stateless transformation and filter functions can be declared as chains in the CDC pipeline configuration settings. CDC throughput During performance testing, the CDC adapter service routed 4 million messages per minute with an average message size of 1.5KB. Such a level of workload was easily handled by 5 Kubernetes vCPU and 10 GB memory in total. Our ultimate goal is to make this infrastructure into a self service system by providing tools (CLI and GUI) for engineers to auto-provision resources and quickly launch new CDC pipelines. With enhancements to the tooling, we aim to further alleviate the onboard overhead for a better user experience. In addition, a full stream validation job is being developed. Leveraging AWS Athena and Firehose , the job will on-demand validate data loss with very little overhead on a live stream. We believe it will eliminate all unintentional data drops and bring our service quality to the next level. As always, Lyft is hiring! If you’re passionate about developing state of the art machine learning/optimization models or building the infrastructure that powers them, read more about them on our blog and join our team. Stories from Lyft Engineering. 304 Thanks to Michael Rebello . Merkle Tree Golang Streaming Kafka Load Balancing 304 claps 304 Written by Stories from Lyft Engineering. Written by Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-02-19"},
{"website": "Lyft-Engineering", "title": "how to build a diversity inclusion program that lasts", "author": ["Smorse"], "link": "https://eng.lyft.com/how-to-build-a-diversity-inclusion-program-that-lasts-12d7c1e42445", "abstract": "Data Data Science Engineering Mobile Product Security By Sarah Morse , Prachi Sharma , and Josh Cherry The importance of diversity within the workforce is well established at this point, yet much of the tech industry struggles to build diverse teams. Diversity matters not only for ethical reasons, but also from a business perspective: diverse teams outperform non-diverse teams , and teams led by diverse leaders are more likely to propose novel ideas that lead to innovation . Yet so often, as companies grow, they fail to recruit and retain individuals from diverse backgrounds: the bias to action that fuels Silicon Valley frequently leads to recruiting fast rather than well. When companies reach the point of maturity when at last they can pause and reflect, it is often to realize that they’ve built a culture and team that does not represent their user base. In late 2018, despite multiple previous attempts to recruit diverse talent to Lyft’s Data Science team, we had failed to make substantial inroads. Our efforts had been at times unsustainable, at others fruitless. We had grown our team significantly in five years — from a scrappy group of ten to a mature Data Science org — but only around 20% of us were from target groups (women, black/African American, and LatinX). In October of 2018, we — a small group of data scientists — created the Diversity and Inclusion Group (DIG) as an answer to the difficult question of how to build a sustainable initiative centered around recruiting and retaining diverse talent within Data Science. We credit the structure of DIG for its impact and staying power. Here’s how DIG works: DIG is led by two co-chairs who are responsible for overseeing the various projects that volunteers are working on and for representing DIG to the rest of Data Science and our partners. Volunteers are broken into several small sub-teams, called workstreams, that focus on a specific initiative (e.g. conferences, internal events, etc.). Each workstream has two co-leads and 2–10 volunteers. Workstreams have defined missions and establish roadmaps to achieve those missions. DIG co-chairs and workstream co-leads meet every two weeks for a Leads Meeting to discuss progress and address blockers. Every six months, DIG goes through a transition process to select new co-chairs. co-leads, and volunteers. At the end of the transition process, leads and volunteers create new roadmaps for the upcoming term. Since October 2018, DIG volunteers have delivered significant impact in our efforts to recruit and retain diverse talent to the Data Science team and promote a culture of inclusivity. In 2019, we increased the percent of our team from a target background from ~20 percent to 32 percent. We held our first ever Lyft Data Science Panel in April of 2019 to recruit diverse talent, and have since hosted several more meetups with that goal. The following October, we hosted a Data Challenge to recruit university students to internship positions, and were pleased to see the majority of our finalists come from a diverse pool of students. In addition, we launched a mentorship program that ~40% of Data Science has participated in, represented the Lyft Data Science org at multiple conferences (including Grace Hopper and AfroTech ), created a suite of reference materials on inclusivity best practices, held informal gatherings to establish stronger connections within Data Science, and built strong relationships with Lyft’s recruiting and centralized I&D teams. The success we had with DIG in 2019 inspired the creation of three parallel Data Science team initiatives — Learning & Development, Onboarding, and Knowledge Share — which closely mirror the structure and processes of DIG. Accomplishments may be impressive when you string them together in paragraph form, but the reality is that we are constantly learning. To save others from the same long road of discovery, we’ve coalesced a year’s worth of lessons into a set of principles that we hope will help jumpstart I&D efforts across Tech and other industries. Here are Lyft Data Science’s 10 principles for building a successful grassroots Diversity and Inclusion group: Create a decentralized reporting structure that empowers volunteers. DIG needed to be approachable and sustainable. We accomplished that by structuring DIG so that every individual had a well-defined and reasonable scope, and so that the success of one workstream didn’t depend on another. Potential volunteers could be sure of the commitment they were signing up for (essential in a workplace where urgent tasks crop up daily). The advantages of a decentralized structure were made clear when one of our workstreams was blocked on a project for months. While the leads of that workstream worked to unblock their team, their volunteers were able to pivot to other tasks. Meanwhile, the efforts of our other workstreams continued unimpeded. 2. Keep participation open, and communication transparent. Anyone at any level is welcome to participate in DIG. All are welcome (volunteer or not) to attend the bi-weekly Leads Meetings and listen in or offer ideas as well as to attend meetups and events hosted by DIG. The Data Science team is continuously reminded of DIG via org-wide communications. By publicizing DIG’s efforts, we motivate volunteers by emphasizing the importance of their work, hold the group accountable for producing results, and turn “DIG” into a household name that all Data Science team members recognize. At Lyft, we did this via a Biweekly DIGest that highlighted recent efforts, upcoming events, and calls-to-action. 3. Let the work your group does evolve. Over the course of the year, we began to understand the impact of DIG’s various efforts, and we took those lessons and adapted our focus accordingly. We shifted volunteers and spun up new workstreams as the Data Science team grew and participation in DIG grew with it. Enabling workstreams to dissolve and new workstreams to be created keeps existing volunteers engaged, inspires new volunteers to join, and democratizes the process of defining the group’s focus. 4. Define a recruitment process for the group. As we ramped up DIG, we outlined a process to sign up volunteers and transition people in and out of roles. DIG formally recruits new volunteers every six months and determines where to allocate them based on changes to workstreams. A defined recruitment process with term limits for leads ensures that new people consistently have an opportunity to lead and that a single person doesn’t become essential for the success of a particular workstream. 5. Track progress to hold volunteers accountable. Having a clear process to track progress holds people accountable, inspires effort, and creates an efficient way to communicate updates. DIG uses a tracker that highlights owners for projects, biweekly progress, and blockers. Workstream leads provide updates in the tracker that are then reviewed in the biweekly Leads Meeting. Regular updates keep volunteers and leads accountable for executing and ensure that potential blockers are surfaced quickly. 6. Find an executive sponsor who cares. Having an executive sponsor (Director+) matters for the success of an initiative like DIG. From the start, we had to align with Lyft’s broader I&D team, coordinate efforts across Data Science and recruiting, and secure a budget for events we wanted to attend or host. Exec level folks have the ability to push things along when blockers arise, and are more easily able to connect relevant parties across a company as big as Lyft. In addition, execs can push to have participation in the group recognized more formally (an advantage we’ll discuss in more depth below). 7. Build partnerships with key orgs — especially Recruiting. One of the main goals of DIG is to recruit diverse talent. Tackling that challenge has meant identifying new sources of potential candidates, prioritizing the most promising recruiting opportunities, and operationalizing new recruiting processes. To do this, we needed a strong partnership with Recruiting — Recruiters have specialized expertise that Data Scientists lack, and having them on our team meant that we could leverage their experience to make better decisions. We asked recruiters to volunteer for DIG as leads or workstream participants, and were fortunate to have several Recruiters join. They play a critical role in determining which conferences to attend, what kinds of events to host, and informing our strategy by leveraging information about the hiring pipeline. In addition to Recruiting, we also developed lines of communication with our internal I&D team, the People team (HR), and our Legal team. When we are uncertain about the best path forward, we can turn to these partners to draw on their experiences in I&D and hiring. For example, when looking at the source of the increase in our team’s diversity from 20%-32%, we saw that the majority of that increase was driven by an increase in the number of women on the team. With help from our partners in I&D we were able to expand our focus in 2020 to target conferences and plan meet-ups that would better enable us to recruit black/African American and LatinX data scientists, as well as female data scientists. (While COVID-19 has temporarily stalled these plans, we fully intend to pick up where we left off when we are safely able to). 8. Ensure that the group’s success doesn’t rest on one person’s shoulders. We set DIG up to thrive in the absence of any single person. People leave the company, get busy with other projects, or have things come up in their personal lives. To be successful, a group needs to be able to withstand any temporary or permanent personnel changes. In DIG, we wanted to preclude any one person from being essential for the success of the group or a workstream. For example, a decentralized reporting structure means that if one workstream is in flux when someone leaves, the rest of the group can still make progress. A defined recruitment process ensures that new volunteers step up to fill gaps. Documenting progress in a formal tracker means that new people can easily pick up from where others left off. Co-leads for any leadership position means that if one lead has to step back, their co-lead can pick up the slack. Term limits ensure that no single person can become too important. Six month transitions ensure that we are constantly updating our documentation, adding a layer of protection against institutional knowledge living with a small group of people. 9. Reward participation in your I&D group. People will volunteer, and volunteers will engage if they know that their reporting structure values this work and that participating will reflect well on them when review cycles come around. Ideally, people volunteer because they agree that diversity and inclusion are important — and many do. But relying on that alone runs the risk of putting the burden of effort on those most affected by a lack of inclusivity and diversity. Incentivizing participation means that people who may not prioritize inclusion or diversity otherwise take up the mantle and work to improve their teams. The Lyft Data Science career pathways explicitly states that investment in our team is expected, and DIG is one way to meet that expectation. 10. Have fun with it! Diversity and inclusion work is important, but it can also be emotionally draining to continuously work to create more diverse and inclusive teams. It’s helpful to intersperse the more taxing, long-term work with lighter projects that everyone can enjoy and use to recharge. Our culture-focused workstream organizes events that the entire Data Science team enjoys: low-key social gatherings, informal presentations where a few Scientists can present on something outside of work they are passionate about, book clubs, etc. These events are well-attended and allow DIG volunteers and the broader Data Science organization to enjoy the community we’re building. Ultimately, people participate in things they enjoy — making DIG fun has kept people volunteering term after term. DIG is an ever evolving group; we’re constantly learning from our successes and failures. One thing we’ve learned is that partnerships are invaluable — they enable idea sharing, efficient recruiting, and a culture of collaboration and support on difficult topics. If your organization has a data, data science, or other decentralized I&D group, we would love to hear from you, share ideas, and partner on events. Please reach out to us at diversity-in-science@lyft.com — we look forward to improving diversity and inclusion together! If you are interested in joining our incredible team of Data Scientists, check out our careers page ! Stories from Lyft Engineering. 294 Thanks to Michael Rebello . Diversity In Tech Diversity Data Science Data Team Building 294 claps 294 Written by Stories from Lyft Engineering. Written by Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-05-22"},
{"website": "Lyft-Engineering", "title": "mentoring myself ep 1", "author": ["Hallie Lomax"], "link": "https://eng.lyft.com/mentoring-myself-ep-1-29280b39215c", "abstract": "Data Data Science Engineering Mobile Product Security Exploring the idea of attempting to “be the person you needed when you were younger” through comic form. I dropped out of college after taking only a few introductory computer science courses, so I’m what a lot of people might refer to as “self-taught.” All of the practical software engineering knowledge I had prior to landing my first job was learned either during internships or while pulling all-nighters at random weekend hackathons. While having a “non-traditional” background can be a fun ice breaker, early on it can also come with an almost crippling paranoia that you don’t have what it takes to succeed in the professional world. I couldn’t tell you how many hours I wasted in the beginning trying to figure things out on my own because I was afraid of revealing where my knowledge gaps were, only to later find out that someone just…forgot to give me critical background information. It took me a while to realize it but, as a junior developer, I was only putting myself at a greater disadvantage by not asking for help when I needed it. Now that I’ve been at this for a number of years, and have helped on-board and been a mentor to people with a wide range of experiences — from interns, to new grads, to some who’ve been at this a lot longer than I have — I’ve realized the power in asking for help. Unless you’re starting your own company (which I tried to do once), there’s always going to be context you have to gain. Resources you’ll need to find. People you’ll need to grab coffee with. On the other end, being an effective mentor requires a lot of self-awareness. It’s understanding what information you have that someone else wouldn’t naturally, and being able to bridge that knowledge gap. For this reason, I personally try to avoid using buzzwords and acronyms when talking to new people, and provide as much context as I can (with links to code, internal documentation, tutorials, etc.) when putting together starter tasks for new hires. I’ve also found it can be incredibly useful to schedule lunches and coffee meetings for them with some other people you know — both on and off your immediate team! Since I can’t go back in time and actually help Lil Hallie navigate the professional world, I’m making it my mission to help people like her that I meet moving forward! If you’d like to work with me, or any of our amazing mentors here at Lyft — we’re hiring ! If you’d like to read about other fun times I’ve had at work over the years, feel free to check out my personal webcomic series ! Stories from Lyft Engineering. 838 5 Thanks to Polly Peterson . Mentorship Engineering Tech Software Engineering Webcomics 838 claps 838 5 Written by Stories from Lyft Engineering. Written by Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-11-12"},
{"website": "Lyft-Engineering", "title": "scaling driver compliance across lyft", "author": ["Oleg Krogius"], "link": "https://eng.lyft.com/scaling-driver-compliance-across-lyft-b9ac69724f6e", "abstract": "Data Data Science Engineering Mobile Product Security A passenger opens the app, requests a ride, and just a few minutes later there is a car with a friendly driver in front of them. They may know this driver went through a background check and other vetting; however, many might not realize all the complexity in the on-boarding process. Driver and vehicle requirements vary substantially across markets (and even at very proximate locations) due to varying market dynamics and government regulations. In this blog post, we’ll discuss how this evolved and how we enforce compliance today. Many years ago, when Lyft was much smaller, prospective drivers would upload their driver’s license, insurance, and then come in to be interviewed in person by one of our co-founders. If they passed the interview, their account would be updated with a approvedDriver=true boolean flag. When Lyft expanded, an enum field was added storing their region. Drivers would also have approvedDriverRegion=SFO (or LAX ). As Lyft expanded to hundreds of other markets across North America, requirements started to evolve further. Some markets, like New York City, are heavily regulated and the local governing agency does bulk of driver and vehicle validation. In other markets, local regulators want to do additional driver screening or vehicle licensing (while Lyft performs its usual checks). Lyft collects dozens of different documents and performs a variety of different checks, depending on the driver’s home base. Regulation is increasing every day. More locations need fine-grained controls. While the legacy system mostly functioned, adding additional complexity was becoming more and more difficult. Region codes work well by centralizing configuration geofences across multiple teams. Different teams can manage their varied configuration (e.g. prices, driver requirements, ride types available) in a single place. Changes to region geofences immediately reflect all configuration. Conversely, splitting or merging regions impacts dozens of teams that depend on configuration and corresponding analytics, making it a challenging task. Some desirable region subdivisions vary based on the use case. It may make sense to think about Bethesda, MD; Washington, DC, and Arlington, VA as the same market for pricing. However, once each state (or district) starts regulating ride-sharing more actively, compliance requirements are better aligned to state boundaries. To compensate for this, additional systems were built on top of the usual driver on-boarding system to enforce requirements. A driver could become approved in a market, but then get filtered from select locations (e.g. airport or some other venue) by dispatch logic. Having this logic live in multiple places made it very difficult to understand all requirements and how they were enforced. Many driver documents expire at midnight and thus there are a few times daily when tens of thousands of documents expire at the same instant. A cron to deactivate drivers or vehicles takes time to execute. This means either extra expiration checking logic, or a trade-off in early deactivation versus increased risk. Large amount of custom business logic was needed to keep up with Lyft’s hyper-growth and expansion, keeping a high driver funnel conversion rate. This translated into hundreds of endpoints that could mutate driver state. If just one endpoint mutated state in a particular way but neglected to update the boolean could mean incorrect results. In addition to business logic, there were technical challenges. One of the database systems used at Lyft at that time would acknowledge a write after writing to its in-memory cache, but before persisting to disk. There were times when data would become inconsistent. Regular audits can help identify inconsistencies, and otherwise help confirm the system is functioning as expected. Some of these are internally driven; some others are done at regulator request. However, increasing audits scale linearly in work required. Furthermore, it’s hard to discover new issues quickly. It was clear that a single system responsible for identifying and enforcing all requirements was necessary. It would enable operations to clearly see requirements and help drivers, compliance to understand what is being enforced and where we might have gaps, as well as engineering in reliably applying those rules. First step to enabling such a system was to centralize all requirements. For requirement storage, we went with our existing regions service that is used across the company for storing geospatial configuration. Teams define maps and configure JSON applicable to a geofence (a shape). Additionally, each map can have shapes on separate layers, and the service takes care of merging layers to produce a flattened map. This worked well for this scenario; however, we intentionally did not want to reuse market shapes. Instead most configuration would be on newly defined geofences, typically aligned with geopolitical boundaries. This way some requirements applicable across the country could be on one layer, state-specific requirements could be on another layer, and local differences like venues (e.g. airports) could be on a a different layer. In most cases, these would be additive, but we preserved support for a more-local layer to adjust a requirement from a more-general layer. A shadow flag enables testing requirements (gather stats, but not to actually surface or block based on these). New rules can become effective on a particular date, allow a grace-period for some drivers/vehicles, and grandfather drivers/vehicles created before a particular date. Most requirements are satisfied by records — typically documents. For example, a driver might need a document of type=driverLicenses that is unexpired. Additionally, in Seattle, there may be another rule that specifies drivers must have a subtype=WA driver’s license, or (as a substitute requirement) have a record indicating they are in the military. Other requirements are satisfied by metadata — vehicles must have ≥4 doors, ≥5 seatbelts, and have a non-temporary license plate specified (after some grace period). We’ve made some revisions to our schema since then, to accommodate interplay between pickup and drop-off locations and cases where drivers and vehicles needs to be evaluated together. Next step after having all requirements in a single location was to calculate compliance in one location. We contemplated two different approaches — pre-calculate many booleans in advance or avoid pre-calculation entirely. Double Down on Legacy Approach? (No) The previous boolean and enum system is inherently a cache of a compliance results. Natural evolution of this would be to store multiple boolean and enum pairs for a given driver and vehicle, add tooling for generating and maintaining a large number of booleans, and improve tools that automatically update this. Perhaps, if a driver signs up in Seattle , we calculate the top-N nearby places to them that may have different requirements — let’s say for example, Seattle , Bellevue , SeaTac , Tacoma , Olympia , Everett , Bellingham , Wenatchee . We could then store a collection of approval booleans for a specific driver. Some logic could be wired up to keep values up to date, perhaps whenever anything about the driver changed (any documents, any background checks, any consents, etc). Recalculating selectively might save some computation, but we could run into issues where something substantial doesn’t trigger a recalculation — therefore, over-calculating is safer. We would also need to come up with logic about what to do when a requirement geofence changes — for example, we might need to create new booleans, calculate them, then gradually transition them into use. But the number of booleans would need to grow much larger. PickupSeaTac vs DropOffSeaTac. Perhaps different booleans for different ride types (non-emergency medical transport requires an additional background check). This soon becomes a very complicated system. Calculate in real-time as needed? (Yes, simpler is better) The question details are as important as the answer. Where is the action taking place, what is the action, when is the action taking place? Instead of trying to pre-calculate all answers that might be relevant (and keep those cached values accurate), we decided to instead build a light-weight calculator to be called whenever a result was needed. If a driver wanted to go online at a location and drive, this calculation would be run as part of the go online attempt. If a passenger requested a ride, dispatch would look for nearby drivers and call this calculator with a list of prospective drivers to filter eligible matches for that trip. If a user wanted to sign up as a driver, we could leverage this to show them all the requirements they need to fulfill. We need to create a new service to store driver documents, implement a new service to perform compliance calculations (aggregating requirements, documents, and metadata), and to roll this out without disrupting the business. Drivers going online is part of a “golden path” for Lyft (in addition to ride matching and completing a ride). Any dependencies enabling this are categorized as tier-0 and need to meet stricter reliability requirements. When this project started, drivers and vehicles were already decomposed into a tier-0 service, supply . However, driver documents were in an existing monolithic service. They needed to be decomposed into a new service (called supplyrecords ). Usual steps of spinning up a new datastore, dual writing data, backfilling existing data, performing shadow dual-reads, and gradually shifting over traffic applied well. Additionally, this enabled some other wins in reliability, security, and maintainability. We also made the choice to store other records such as consents, manual deactivations (e.g. from our Trust and Safety team), and records of passed background checks within supplyrecords . By doing this, we would have a single tier-0 source of truth for all records we would need to make decisions (rather than trying to cache data from several higher-tier services). New stateless compositional service was created to perform calculations. Our starting idea was a GetMissingRequirements(driverId, vehicleId, lat, lng) function. It would query all underlying services for data ( regions for requirements at a particular location, supply for driver & vehicle metadata, and supplyrecords for documents). It would then filter requirements based on applicability and determine what was unsatisfied. If nothing was missing, the request is compliant. This was implemented in Go, to minimize latency for dispatch matching. To minimize call volume, some requests are batched or streamed. Additionally, some short-lived caches are used to further improve performance and reduce load from the underlying services. We then needed to build some internal tools. These tools would visualize all requirements within an area, show how a particular user meets requirements, and enable editing requirements. Lyft has thousands of operations staff. Correspondingly, we worked with our ops training teams to come up with videos, documents, and other materials on how to use these tools. The service also emits pass and fail stats by requirement and location, events for an audit trail, and some logging for service health monitoring. We then created dashboards to monitor pass and fail rates across the platform and to alert us of any changes. Once the service and tooling were ready, next step was establishing a rollout schedule. With that schedule, we would configure requirements in a handful of states, turn on dark reads (just comparing pass rates), communicate with stakeholders, and turn on blocking using the new system. This would run in parallel with the legacy system — legacy system would enable users to sign up, become drivers, upload whatever documents. New system would additionally check, in real time, whether those drivers and vehicles are compliant. We then proceeded to enable this across the entire platform. Now, every go online attempt and every ride match pairing are evaluated in real time against this compliance checker. Finishing the rollout enable cleaning up and deprecating the legacy system. This post is reflecting back on just one project we’ve accomplished. You can see how this translates into many follow-up projects. And while some of these are completed or underway, many are still remaining. Corresponding obligatory note — Lyft is still in its infancy, and there are numerous unsolved challenges in reshaping how we think about transportation. Join us! Stories from Lyft Engineering. 294 Thanks to Mark Grover . Microservices Compliance Lyft Architecture Rules 294 claps 294 Written by Engineer @Lyft Stories from Lyft Engineering. Written by Engineer @Lyft Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-02-24"},
{"website": "Lyft-Engineering", "title": "observability for the missing hop", "author": ["Jose Nino"], "link": "https://eng.lyft.com/observability-for-the-missing-hop-6688c6f3911a", "abstract": "Data Data Science Engineering Mobile Product Security Envoy’s guiding vision has always been: The network should be transparent to applications. When network and application problems do occur it should be easy to determine the source of the problem. The second part of this vision has played an important role in Envoy’s increasing presence in modern day distributed systems. Envoy has a best-in-class, comprehensive suite of observability data points. From distributed tracing, to logging, to time-series metrics, data from Envoy allows an operator to understand their system. At Lyft, time-series metrics emitted by Envoy enable engineers to explore different system conditions. For instance, they are able to see the concurrency that a service has at a given time, or the amount of healthy nodes for a particular service, or latency at p95. The possibilities seem almost endless. While this plethora of metrics is incredibly powerful, I would argue that the most powerful aspect is consistency: engineers are able to use the same metrics and dashboards to observe all of Lyft’s systems, regardless of which microservice they are interested in. Since Envoy runs at every hop of the network, every hop of the network emits the same metrics! (For more details on Lyft’s Envoy dashboards, please take a look at this blog post ). There is one hop in Lyft’s network mesh that did not enjoy the same consistency in observability: the hop from our mobile clients to the edge of our server infrastructure. This is an important hop in our network — perhaps even the most important. In the words of our blog post announcing Envoy Mobile: Three 9s at the server-side edge is meaningless if the user of a mobile application is only able to complete the desired product flows a fraction of the time. If we are unable to observe this hop, we are unable to improve reliability and resilience for our mobile clients. Among the many reasons expanded upon in our previous blog posts , observability for this hop was a big reason why we decided to invest in Envoy Mobile. By giving our mobile clients the same observability stack as our other hops, we could obtain metrics that were consistent with ours server infrastructure . We announced the release of Envoy Mobile v0.2 in November. In that announcement, we mentioned that Envoy Mobile had replaced the networking libraries in Lyft’s alpha rider app and that we were going to build observability integration into Envoy Mobile. The rest of this blog post describes how we executed on this and in order to start taking advantage of Envoy’s time-series metrics on our mobile clients as of early December. One of Envoy’s most powerful features is all of its extension points — probably only second to its best-in-class observability. Envoy can be extended to add more filters (both at L4 and L7), new transport sockets, and new health checkers. Most pertinent to our work on observability is the fact that Envoy has an extensible stats sink interface. This means that developers can create new ways to expose Envoy’s time-series metrics by creating new stats sink implementations. One of the stats sink implementations is the “ metrics service ” stats sink. This implementation encodes metrics in Prometheus’ protobuf format and sends them over gRPC to a remote service. This is a fitting model for Envoy Mobile because we can’t install a stats collector on phones, nor can we scrape metrics from a known endpoint. Lyft’s Envoy Mobile metrics pipeline looks like this: On the left we have the mobile clients running Envoy Mobile as their network library. Envoy’s stats configuration allows us to configure two important aspects for the mobile environment: Stats flushing interval: given battery and data concerns in mobile environments, we must finely tune flushing intervals over longer time periods. In the long term, we would like to change flushing to be based on OS level signals like app backgrounding or termination. Metric emission filtering: Envoy emits hundreds of different metrics, and not all of them are useful in a mobile environment. Envoy allows us to configure an inclusion_list of metrics tags , as well as which metrics families to emit. Moreover, Envoy has a very useful property in its bootstrap configuration, the node property . This field is used to identify an Envoy instance. In Envoy Mobile, we use it to tag the mobile client with useful information like which OS it’s running on (i.e., “Android” or “iOS”). In the future, we could send even richer information such as the client release version, the OS version, the user-id, etc. The reason this metadata is useful is because it can be used in a metrics aggregation service to enrich the metrics collected from our clients. The Metrics Service is a gRPC service we built to receive metrics from the clients (and is represented by the red block above). At its most basic, the Metrics Service receives metrics from clients on a gRPC stream, transforms them to statsd format (using Lyft’s gostats library ), and emits them to statsd aggregators. Using the rich metadata from the node field, the Metrics Service can also perform online aggregation based on runtime parameters. For instance, if we would like to aggregate metrics based on a specific client release to track down a bug, we can do that! Very powerful mechanisms can be built on top of this on-demand aggregation to keep costs low at steady-state, while allowing us to tap into a mine of data when it is useful. After the metrics are aggregated, they are available to be plotted via the same front ends described in Lyft’s Envoy dashboard blog post . This consistency allows service owners to work with tools they are already accustomed to. One of the most powerful moments I personally had in the last six months of working on Envoy Mobile was when I was able to plot the following two graphs side-by-side: These two graphs are plotting the same Envoy metric ( upstream_rq.count i.e., network traffic volume coming into Lyft’s platform), over the same time period. The important difference is that the top graph is plotting metrics coming from our mobile clients (Envoy Mobile), and the bottom one is plotting metrics coming from our edge infrastructure (Envoy Proxy). The top graph shows us the missing hop for the first time! Moreover, we have access to this data via the same metrics names as the rest of our infrastructure. This consistency goes a long way in reducing the cognitive load of the server engineers that operate infrastructure at Lyft, and we are now able to extend it to our mobile engineers as well. It is truly powerful to be able to see complementary graphs like the ones above. The dashboards that we are now able to build give us insights that we did not have before, and we were able to do this by leveraging Envoy Mobile and Lyft’s existing metrics infrastructure. We built a transparent and observable network experience that finally includes mobile clients. This is just the first of many examples of the possibilities we are unlocking by using a unified network primitive across all platforms, with Envoy Mobile on our mobile clients and Envoy Proxy on the server. It highlights the fact that we can start reaping benefits by leveraging pre-existing infrastructure thanks to Envoy and Envoy Mobile’s flexible extension points and their plethora of observability data. If you are interested in reading learning more about Client Networking at Lyft or Envoy Mobile, please head over to the additional resources section in the Envoy Mobile docs. Envoy Mobile is being built completely in open source, with publicly available milestones . If you find this work interesting, please come join us: open up an issue for a feature you’d like to see, or (even better) a PR! Este artículo también está en español . Stories from Lyft Engineering. 118 Thanks to Michael Rebello . Envoy Proxy Observability Time Series Data Cloud Native Networking 118 claps 118 Written by Engineer @lyft Stories from Lyft Engineering. Written by Engineer @lyft Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-06-25"},
{"website": "Lyft-Engineering", "title": "lyfts journey through mobile networking", "author": ["Michael Rebello"], "link": "https://eng.lyft.com/lyfts-journey-through-mobile-networking-d8e13c938166", "abstract": "Data Data Science Engineering Mobile Product Security In 5 years, the number of endpoints consumed by Lyft’s mobile apps grew to over 500, and the size of our mobile engineering team increased by more than 15x. To scale with this growth, our infrastructure had to evolve dramatically to utilize new advances in modern networking in order to continue to provide benefits for our users. This post describes the journey through the evolution of Lyft’s mobile networking: how it’s changed, what we’ve learned, and why it’s important for us as a growing business. The early iterations of the Lyft apps used commonly known networking frameworks like URLSession on iOS and OkHttp on Android. All of our APIs were JSON over RESTful HTTP. The workflow for developing an endpoint looked something like the following diagram, where engineers hand-wrote APIs on each platform based on a tech spec: In this world, the burden of consistently writing an API implementation across 3 platforms (iOS, Android, and server) was placed on individual engineers. With a small team and limited codebase sizes, this was an acceptable first approach. However, as our teams started to get larger and our velocity increased, our communication complexity also increased and led to more errors. For any given resource, there were numerous potential points of failure in this implementation process, each stemming from a lack of consistency between platforms and resulting in problems we’ve all likely encountered: An engineer on one platform could write a typo in a JSON mapping. One platform might assume a field is optional while another treats it as non-optional. Different mobile clients may handle HTTP status codes and/or response bodies differently. A change might be made on one platform in a future iteration of an API, but not on others. In an effort to reduce handwritten code and subsequent programming errors, members of the Android team began adopting YAML-based Swagger for defining APIs. With Swagger came an in-house code generator that created Java files containing functions that wrapped calls to OkHttp (similar to what Retrofit provides, but without the annotation processing). Android engineers ran this generator locally to create type-safe API methods which could be imported into the codebase. This change meant the workflow between platforms became asymmetrical: Swagger made it easier to integrate new APIs on Android and provided some documentation for the APIs exposed to mobile, but we still had similar inconsistency problems as those mentioned previously: No code generation for server or iOS. YAML definitions (and thus Android’s implementations) could differ from the server and/or iOS implementations since there were no strict API contracts between platforms. As the sole consumers of the YAML definitions, Android engineers became the de facto maintainers, and the definitions became outdated. While Swagger generation attempted to drive consistency and documentation, it didn’t provide enough value since not all platforms were able to leverage the tool. In order to maintain a high bar of consistency for APIs, we needed a single source of truth which all platforms could utilize. As the number of services and APIs grew, it became clear that we needed a single source of truth for APIs that provided guarantees around the behavior of any given API on every platform. We envisioned a central collaboration space where any engineer could add an API definition from which consistent interfaces and implementations would be generated for our services and mobile clients. The goals for this project were ambitious: Create one canonical source of truth for every API definition, respected across all platforms. Generate ergonomic, consistent, and testable interfaces in each of our supported languages for every definition. Abstract away the networking implementation details from the generated interfaces, allowing for the underlying implementations to change and evolve over time. We chose to adopt protocol buffers (“protobuf”) as the Interface Definition Language (“IDL”) that would be adopted across the company for the following reasons: Simple and descriptive language for API contracts. Open-source generators are available for most languages. Writing new generators is possible by using protoc-gen-star . Backwards compatible with our existing JSON APIs (with some tweaking), enabling easy migration. Supports an optimized binary format which could be adopted in the future. Through our IDL system, we aimed to unify the workflow for all engineers regardless of the platform they worked on, and to provide a common collaboration ground for them: Protobuf allows for defining RPCs (Remote Procedure Calls, basically endpoints) that accept and return messages (models) over gRPC (a thin message-based streaming protocol built on top of HTTP). However, we needed to support all pre-existing Lyft APIs that use JSON over RESTful HTTP, and wanted a way to build future functionality on top of protobuf that was available to mobile clients and transparent to engineers. To accomplish this, we opted to write custom code generators for Swift and Kotlin. These code generators transform the protobuf language’s AST (Abstract Syntax Tree) provided by protoc-gen-star (an open-source protoc plugin developed by Lyft) to create models and endpoint definitions which can then be consumed by mobile apps. By writing our own custom generators, we were able to embed additional metadata using protobuf annotations, which enabled back-porting existing RESTful JSON HTTP APIs. Our protobuf definitions look something like this: Using this simple protobuf definition, our mobile code generators produce a few different things: Models created by the generators called DTOs (Data Transfer Objects) represent a structure that is common across platforms. Engineers are expected to convert these to/from application-level models for use in their product code, allowing UI/business logic to be separated from the API format. Note: Although the examples below are in Swift, the corresponding Kotlin APIs match 1:1. The generated APIs include a protocol/interface for the service and its corresponding RPC declarations, allowing them to be easily mocked for testing. A production-ready implementation of the interface is also generated. These interfaces and implementations utilize Rx patterns for the generated APIs, making combining/transforming them quite simple: Each RPC is exposed as a function that returns an observable containing a native Result type with either a successful response or a typed error. When the observable is subscribed to, the API call is performed, and the result of the call is emitted over the Rx stream. If the stream is disposed by the consumer before it completes, the API call is canceled automatically. We’ll get more into the implementation details of how the API call is actually performed by the networking stack later in this post, but it’s important to note that these details are hidden away behind the interfaces of the generated APIs. This decision was key to unlocking future improvements to the transport layer. Lastly, mock classes conforming to the same interfaces are generated and compiled into separate modules that are only visible to test targets. These classes make it easy to mock out and test features that consume the generated APIs. The generated client is consumed by product code and looks something like this: Using an IDL with centralized definitions and automated code generators established a high bar of consistency between different platforms, made consuming APIs very simple, and created a clear line between the interfaces and their implementations . In other words, it abstracted away how network calls are executed and paved the way for future improvements to the transport layer. Once we established strict API contracts and clear abstractions through IDL, we considered: How can we change what data is being sent between the client and server in a way that benefits our users? Since protobuf supports an optimized binary format for serializing models, we decided to try it out using an A/B test. To do so, we employed content negotiation as a way to “upgrade” server responses to protobuf if both the client and server support it for a given resource: The above diagram outlines the following workflow: Client sends a JSON request and indicates it knows how to read both protobuf and JSON responses for a given endpoint using the Accept header. If the service receiving the request can also read protobuf, middleware running on the service “upgrades” the request from JSON to protobuf using the IDL definition. If not, the original JSON is passed through. The service responds with protobuf, indicated by the Content-Type header. If the client originally indicated it was capable of reading protobuf responses, the middleware passes the response through. If not, the response is converted back to JSON. Client is able to take advantage of the reduced protobuf payload size. This workflow enabled us to migrate resources to protobuf definitions independently on the client and server while iteratively taking advantage of the benefits provided by the binary format. In doing so, we saw reduced response payload sizes (sometimes cutting down size by > 50%!), improved success rates, and faster response times on our larger endpoints. Best of all, product teams were able to take advantage of these improvements without any changes to their code , thanks to the IDL abstraction in place. After deploying content negotiation, the team started asking: Considering that our IDL abstraction and infrastructure allows us to experiment with its implementation, what if we were to change how we send data in addition to what we send? On the server side, we use Envoy proxy to relay all network traffic into, within, and out of our service mesh. Envoy supports a myriad of protocols and provides a rich feature set that is used throughout our infrastructure to provide consistent behavior between our hundreds of microservices (i.e., auth, protocol negotiation, stats, etc.). We envisioned a world where Envoy could run on our iOS/Android clients, bringing the power and consistency of the project to mobile and essentially making them simply another node on our service mesh. This would enable us to write core functionality once and share it across both mobile and our services (for example, performing auth handshakes entirely within Envoy), and would allow us to deploy new protocols like QUIC before they’re officially supported by OS providers. Earlier this year, we announced Envoy Mobile — an iOS/Android library with the goal of bringing Envoy’s functionality to mobile apps. Since then, we’ve begun testing the library in pre-release versions of the Lyft rider app. We believe that Envoy Mobile will not only become a key piece in Lyft’s networking infrastructure, but that it will revolutionize how we enable network consistency across platforms throughout the industry. All of the work on Envoy Mobile is taking place in open source, and we’ll be publishing more detailed posts about it in the near future. If you’re interested in contributing to Envoy Mobile or testing out the library, you can check out the project on GitHub . Throughout Lyft’s mobile networking journey, we were able to recognize the value of consistency across platforms and languages, and iterated on our infrastructure to abstract away networking implementation details while making it easy for engineers to interact with APIs. These changes allowed the organization to scale, and provided clear benefits to our end users by optimizing how they communicate with our network. In the near future, we plan to open source the code generators described in this post. Additionally, we’ll be continuing our efforts to push Envoy Mobile to production. We invite you to check out our roadmap or open a PR if you’re interested in contributing to the library! This post provided a high-level discussion of APIs at Lyft, but there are numerous topics that we plan to publish more about in the future, including: Deep dives into Envoy Mobile as it evolves. How we enabled streaming and push using IDL. Building protobuf generation for mobile. If you’re interested in joining the team at Lyft, we’re hiring! Feel free to check out our careers page for more info. Stories from Lyft Engineering. 617 1 Thanks to Mark Grover and Jose Nino . Mobile Envoy Kotlin Swift Engineering 617 claps 617 1 Written by Stories from Lyft Engineering. Written by Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-11-12"},
{"website": "Lyft-Engineering", "title": "envoy mobile v0 2 deep dive", "author": ["Jose Nino"], "link": "https://eng.lyft.com/envoy-mobile-v0-2-deep-dive-7ed262cfdf93", "abstract": "Data Data Science Engineering Mobile Product Security In November we released Envoy Mobile v0.2 . In the accompanying blog post we detailed the features that the library supported and announced that we had replaced the networking libraries in Lyft’s alpha rider app with Envoy Mobile. In this blog post, I want to expand on the technical aspects of the v0.2 release and take a technical deep dive of Envoy Mobile’s current architecture. Many companies, Lyft included, have realized the benefits of deploying Envoy across their backend infrastructure: consistent behavior, dynamic configuration, and best-in-class observability, among others. Envoy handles every network hop in the data center. From the network edge, into the service mesh, even when egressing the VPC , Envoy takes care of all the network traffic. Envoy has done a great deal to help us achieve high reliability server-side: having a universal network primitive that is performant, reliable, configurable, and extensible has made the network largely transparent to server engineers. And importantly, Envoy’s best in class observability has made it increasingly easy to debug issues when they arise. The realization that brought us to Envoy Mobile was that solving these problems in the data center was not enough , and over the last 6 months we have worked toward a v0.2 release where we could start to reap the benefits of a truly universal network primitive. From our v0.2 announcement: Envoy Mobile v0.2 is a fundamental shift in how mobile clients use Envoy. Envoy Mobile now provides native Swift/Kotlin APIs that call through to Envoy directly (rather than using Envoy as a proxy), which apps use to create and interact with network streams. This release includes a variety of new functionality: 1.HTTP request and streaming support. 2.gRPC streaming support through a built-in codec. 3.Automatic retries using Envoy’s retry policies. 4.Programmatic, typed configuration for launching the Envoy network library. Let’s see how the library was designed from the ground-up to achieve the functionality described above. This section will discuss: The build system and how it relates to the overarching organization of the library. The lifetime of a request through the Envoy Mobile Engine and how we leverage existing Envoy constructs to ensure networking performance, reliability, and DRY code. Hopefully this deep dive is not only informative and interesting, but also entices you, the reader, to jump on board and start exploring the library! There are two main organizational dimensions in the Envoy Mobile project: API layers, and threading contexts. This section will describe both of them. API Layers At a high level, Envoy Mobile is a project in three parts: Platform code: all the code that is written for either Android/JVM (in Java and Kotlin) or iOS (Objective-C and Swift). A fundamental goal of the library is cross-platform consistency. Therefore, the code in this layer is largely identical between both platforms. Bridging code: common C types that allow us to bridge from the platform specific constructs down to the C++ codebase. Note that this layer can also one day enable us to write bindings for other languages like Python, Rust, etc. (hint hint wink wink). Native code: written in C++ and interfaces directly with the rest of the Envoy codebase. This layered architecture is closely tied with how we build the library. Envoy itself is built with Bazel , which already has good tooling for multi-platform/multi-language projects, making it an excellent fit for building Envoy Mobile. Getting this set up was not trivial, and much of the initial effort of the project was put towards successfully compiling for each of the required languages and platforms. After completing the initial setup, things have worked reasonably well, most of the time. The diagram below shows how the Bazel targets map over from the tiered architecture discussed in the previous section: On the left, in red, we have platform-specific targets for iOS and Android. In the middle, in blue, are the targets for the bridging code. And lastly on the right, in green, is the native C++ code (both the core of the library, and Envoy itself, as a dependency). This gives us an overview of one fundamental dimension of the project: how we organized the library and thought about its API surface. Threading Contexts The other foundational concern was how we took something that was meant to be run as a multi-threaded process and ran it in a single threaded context within a sandboxed mobile application. In other words, running Envoy as an Engine, rather than a process. This dimension gave us the threading context in which the library had to be divided: The application threads that interact with the Engine and issue network calls. The main Envoy thread which runs the Engine. The callback threads where the Engine surfaces callbacks when responses are issued (whether from over the network or generated by the Engine). If we layer these threading concerns on top of the API layers of the library, we get a handy-dandy matrix that allows us to explain the lifetime of a request and the library components that make it happen: Using the mental model described above, we can explore the lifecycle of network I/O in Envoy Mobile. Envoy Mobile exposes a top level Client interface (iOS/Android ) that allows the application to start the Envoy Engine and provide it with a configuration. Although this Client is held in the application thread, the Engine is run in a separate native C++ thread — the Envoy main thread. It is worth noting that Envoy itself contains mostly single-threaded code. When Envoy is running as a server, its main thread is responsible primarily for lifecycle tasks and bookkeeping. When a listener receives network events on a socket, its tasks are handled in worker threads. If a worker needs to interact with the main thread, it uses an event dispatcher interface largely built on top of a libevent implementation (more information on Envoy’s threading architecture can be found in this blog post ). Lastly, if a worker thread needs to issue network I/O off-band, it does so via an AsyncClient (code here) . In Envoy Mobile, we leveraged these existing Envoy constructs (cross-threading dispatch, and the AsyncClient ). By hooking the Engine into the Envoy’s main thread event dispatcher and AsyncClient s running in the context of the main thread. The result of this approach is that all the work is dispatched from the application threads to the Engine’s main thread and out to the network via existing mechanisms in Envoy. This means that we are harnessing Envoy Mobile into the battle-tested and hardened code paths of Envoy itself. Like Envoy, Envoy Mobile considers streaming as a first class construct and is designed to work seamlessly with persistent streams while providing convenient traditional APIs. This forward-thinking approach ensures flexibility within the library and a path to unlocking new streaming protocols like QUIC in the future. This is largely evident due to the fact that we leveraged Envoy’s AsyncClient , but also by the streaming-based API exposed in Envoy Mobile’s bridge layer here . When the application creates and dispatches a request via Platform level APIs, Envoy Mobile transforms that request into a new HttpStream that lives in the Bridge layer. Before the Engine can pass handling of those API calls down to Envoy’s main thread (and thus to C++ memory constructs), we have to safely manage the ownership and lifecycle of memory passed by the application down to memory that will be handled by Envoy. This was another case where Envoy Mobile was able to leverage pre-existing functionality in Envoy. Envoy has an elegant buffer abstraction . Buffers are composed of buffer fragments that have callbacks that indicate when a buffer has been drained and the underlying memory is no longer needed. Perfect! This allowed us to directly tie platform memory management schemes to the lifecycle of memory usage in core Envoy code without any special bookkeeping — all in a platform-agnostic fashion. To illustrate this example we can take a look at passing a Java ByteBuffer through Envoy Mobile. The ByteBuffer is converted to envoy_data in the following code: Importantly, note that there is no copying being done; envoy_data is wrapping the memory of the ByteBuffer directly. Additionally, it attaches a static release function (which deletes the global ref captured here), and the Java object as the type-erased context. The envoy_data is then given to Envoy as an unowned Buffer fragment which, as described above, gets released when Envoy is done with the wrapping Buffer. The effect that this creates is that Envoy Mobile avoids unnecessary buffer copies even when passing memory blocks across threading and API boundaries — all without complicated bookkeeping. Once the memory barrier is bridged and Platform-specific constructs are transformed into Envoy constructs (e.g from envoy_headers in the Bridge layer to Http::HeaderMap in Envoy’s codebase), Envoy Mobile uses the event dispatcher to inject HTTP events into the AsyncClient . This integration is an instance where some work was done in upstream Envoy to fully enable this solution. We envision this type of collaboration to continue to flourish out of the partnership between Envoy and Envoy Mobile. This gives us a fully functional solution in the outbound direction, where the application issues network I/O via Envoy Mobile. Once the request is sent to the network and a response is received, Envoy Mobile needs to traverse in the inbound direction to surface the response back to the Platform layer. Envoy’s AsyncClient fires an array of callbacks that were easily leveraged in Envoy Mobile’s codebase — another instance where Envoy Mobile’s deliberate design allowed us to utilize existing mechanisms in the Envoy codebase. To prevent arbitrary user-provided Platform code from running and potentially blocking Envoy’s main thread, Envoy Mobile uses Platform-specific dispatching mechanisms (Grand Central Dispatch on iOS and Executors on Android) to fire callbacks on application threads using Platform-specific lambdas. This presented another interesting challenge: C function pointers (remember the Bridge layer is written in C) have no facility to capture state as true lambdas do. To facilitate mapping to Platform-specific callback abstractions, Envoy Mobile captures Platform-specific context in a type-erased manner. This allows Envoy to hold that context in a way that is memory-agnostic of its contents. Upon dispatch, that context is used to reconstitute the high-level Platform lambdas supported by the API, thus allowing Platform-rich handling of HTTP callbacks. For instance, when iOS builds its envoy_http_callbacks , it assigns static callback functions that are capable of reconstituting the type-erased context and calling the Platform-given callbacks on a Platform dispatch queue. For example: Note that this static function reconstitutes the void* context into a defined type known to the Objective-C code. The code is then able to get the Platform-specific EnvoyHTTPCallbacks object which has the user-provided dispatch queue, and the user-provided block to execute. Lastly, we made a deliberate design decision in the library to avoid synchronization between the outbound and inbound directions (with the exception of stream cancellation state, which is checked atomically before callbacks dispatch to the application). All the management of the request and response state is delegated to the Engine, and Envoy Mobile enforces contracts in its internal APIs to keep these paths independent. This results in a dramatically simpler implementation in the library itself as well as in external code consuming the library. Just like that, we have made a full trip around the lifecycle of HTTP traffic in the Envoy Mobile library: As you can see in this blog post, Envoy Mobile is built using deep integration with the Envoy codebase, and leverages a lot of the technologies built into Envoy. Although Envoy Mobile already takes advantage of Envoy’s AsyncClient , this is just the beginning. One of Envoy’s most powerful features is the concept of L7 filters. These filters are written to operate on HTTP-level messages without knowledge of the underlying protocol version. This feature has allowed complex deployments server-side, enabling functionality like health checking, rate limiting, or even fronting storage primitives like DynamoDB. Since this project’s onset, we have been working toward solutions that open the doors for doing similarly complex operations on mobile clients agnostic of the architecture — solving problems like deferred requests, OAuth, and compression in one place across both mobile platforms. We believe Envoy’s L7 filter system is the best place to accomplish this. One of the issues we opened in our fully public v0.3 roadmap was to work on Envoy Mobile’s filter chain support. In order to extend Envoy Mobile to have access to L7 filters, we needed to replace the Async Client-based direct API for one that harnesses Envoy’s HTTP connection manager. This is exactly what we did in late December by exposing an Envoy API listener, and leveraging that in Envoy Mobile. Excitingly, with those two PRs we were able to fully test Envoy’s L7 filters in Envoy Mobile, starting with the dynamic proxy filter which gives our mobile APIs even more flexibility as Envoy Mobile does not need to resolve DNS addresses a priori. This is just a taste of the power that we will be unlocking with the next version of Envoy Mobile! This blog post dove deep into the internals of Envoy Mobile v0.2. It is our goal to develop Envoy Mobile out in the open and to distribute a rich set of materials which other parties in the industry can start leveraging, building upon, and collaborating on. Here are a few other resources that compliment this post: If you prefer video: a lot of the content in this blog post was shared in two talks Mike Schore and I gave at EnvoyCon and KubeCon NA 2019. If watching us explain the material is easier than reading about it, feel free to head on over! This blog post compliments Michael’s blog post about Lyft’s mobile networking journey very well. In his blog post he takes us through a brief history of API design at Lyft, and how we have evolved mobile APIs over the last few years. In other words, how we focused on the “what” that we transmit over our APIs, and how that culminated in Envoy Mobile: an opportunity to focus on “how” we transmit over our APIs. The Envoy Mobile repo , where we host our open source roadmap . We do this to encourage you to actively participate in this project! Stay tuned, we have a lot more content coming over the next few months, from how we are able to observe the network using Envoy Mobile metrics, to a deeper dive into the filter chain integration teased above! If you’re interested in joining the team at Lyft, we’re hiring! Feel free to check out our careers page for more info. Stories from Lyft Engineering. 128 Thanks to Michael Rebello . Networking Envoy Proxy Swift Kotlin Mobile 128 claps 128 Written by Engineer @lyft Stories from Lyft Engineering. Written by Engineer @lyft Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-02-04"},
{"website": "Lyft-Engineering", "title": "kronos android easy ntp", "author": ["Amelia Riely"], "link": "https://eng.lyft.com/kronos-android-easy-ntp-d44fd6ece9c5", "abstract": "Data Data Science Engineering Mobile Product Security Based on a sampling of over 5 million sessions, over 3% of Lyft’s Android users have device clocks skewed by over one minute. That’s more than twice as many as the 1% we see on iOS. Think about what that means for our systems and users if we depended on device timestamps for analytics, offline events, locations or displayed times. Data loses order and meaning; drivers get dispatched for rides that no longer make sense geographically; users might see estimated arrival times that look like they’re in the past. We are announcing Kronos-Android , an open source Network Time Protocol (NTP) synchronization library, which provides a trusted clock on the JVM. This post about Kronos for iOS goes into detail about our decision to derive a “sane time” from NTP. Unlike the system’s clock, the time reported by Kronos is unaffected when the local time is changed while your app is running. Instead, Kronos stores accurate NTP time along with a delta between the NTP time and the system uptime. Since uptime increases monotonically, Kronos isn’t affected by device time changes. Calling KronosClock.getCurrentTimeMs() will return the local time based on the last known accurate time + delta since last sync . Include the following in your build.gradle file: Obtain a Kronos clock instance that is synchronized with NTP servers. Replace usages of with If the NTP server cannot be reached or Kronos has not yet been synced, getCurrentTimeMs() will return time from the device or other fallback clock and trigger syncInBackground() . If you’d rather control the fallback, you can use getCurrentNtpTimeMs() , which returns null instead of falling back. Since it relies on system uptime, Kronos detects and requires a new sync after each reboot. Kronos comes with a set of default configurations that have worked well for us. You can customize the configuration by using AndroidClockFactory.createKronosClock() with a set of optional parameters described in the README . Kronos provides access to the Kotlin-only base library, Kronos-Java , for usage with non-Android modules. Kronos-Java depends on an externally provided local clock and cache. The Android library abstracts away the creation of the clock by extracting the Android system clock from a provided Context . It abstracts away the creation of the cache by using SharedPreferences . To use Kronos-Java, include the following in your build.gradle file: Check out the library on Github . Lyft is hiring! If you’re interested in working on projects like this and shaping the future of transportation, apply to join our team. Please comment with your favorite quote about time — I missed the opportunity to include any in this blog post. I was entertained by many of these . Stories from Lyft Engineering. 205 2 Thanks to Leo Kwong , Martin Conte Mac Donell , Aaron Rosenberg , Megan Kercher , Ryan Lane , and Alex Rafter . Android Ntp Kotlin Open Source Mobile 205 claps 205 2 Written by Dev at Lyft. Opinions are my own. Stories from Lyft Engineering. Written by Dev at Lyft. Opinions are my own. Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-01-27"},
{"website": "Lyft-Engineering", "title": "meet charlotte a pm on lyfts autonomous vehicles team", "author": ["Jennie Braunstein"], "link": "https://eng.lyft.com/meet-charlotte-a-pm-on-lyfts-autonomous-vehicles-team-f7200d3fb826", "abstract": "Data Data Science Engineering Mobile Product Security Before getting into the autonomous vehicles space, I had worked in data science for 5 years as a technical individual contributor. Three years ago, I started specializing in data-related products for autonomous vehicles and decided to stay in the AV space since then. I am now the Product Manager for Mapping at Lyft’s autonomous vehicles team ( Lyft Level 5 ), supporting engineers from Palo Alto, Munich, and London. Though mapping is unrelated to data science, I can still leverage my data skills to design metrics and measure product performance. Having a data science background is not uncommon in the AV field. You would be surprised by the rich diversity of job skills that the AV space requires. Other than software development and machine learning, specialities such as aviation, UX, fleet operations, and infrastructure, are also very common skills seen across our teams. AV is probably the only space where you can accomplish three things via one job: taking on the most cutting-edge technical challenge (for professional development), improving efficiency for getting from point A to point B (building a valuable product), and saving people’s lives through safer transportation (for a mission-driven profession). You rarely find all three in other industries. Many other technical products either cannot promise a commercial launch or do not relate as closely to our daily lives. Working as a Product Manager for AVs is intrinsically interesting. You have to accept the fact that the product you build is as hard as aerospace engineering, but somehow the “rocket” needs to be launched to the public in a safe, timely manner, which entails extremely careful product roadmapping and prioritization. You would know that the “rocket” is not there yet when your autonomous vehicles still struggle with making left turns. You would know that your rocket cannot “land on the moon” when your driving trajectories still require some fine-tuning. The impulse to overcome these challenges is what keeps me up at night and wakes me up in the morning, everyday. Lyft is well-positioned to launch autonomous vehicles at scale because it enjoys the luxury of its large fleet from Lyft RideShare. There are many ways through which AVs can benefit from human drivers. Mapping is a notable example. Maps are consumed by both human drivers and autonomous vehicles but in different ways. Human drivers need general navigation from maps, whereas autonomous vehicles need high-precision road features from maps. At Lyft, teams from both Level5 and RideShare collaborate actively to maximize the value from one common map. Lyft provides the full freedom for teams to explore how humans and robots can cooperate. One other reason why I chose to work at Lyft: culture. Building an AV is a marathon without a clear course map. That’s why company culture plays a critical role in AV development. Lyft’s culture is ‘quality over speed’. Quality is the foundation for safe operation of AVs. We also value collaboration over individualism. Building AVs requires teamwork across software, hardware, operations, and many other functions. At Level 5, decisions are typically made with all stakeholders in the room so that all considerations can be taken into account. Ultimately, it’s the culture, rather than the tech, that brings talent together to deliver reliable autonomous vehicles. Autonomous vehicle products are fundamentally different from traditional products in many ways. Firstly, the AV product life cycle is much longer, sometimes a year, sometimes even three to five years, because every product feature has to go through in-depth research, pioneering prototyping, rigorous testing, and thorough validation. Each phase can take months or even years. Adding a brand new layer to maps, for example, can take a long time to implement. Product Requirement Documents (PRDs) can become obsolete every time product goals shift. More likely than not, teams are unable to deliver any tangible products in a long period of time due to technical difficulties. Successful PMs need to break down large products into small milestones so that the incremental improvements can be celebrated along the way . Some incremental improvements can be iterated rather quickly. One code change in the morning may fix a disengagement during a test drive in the afternoon. Nevertheless, teams need to be constantly reminded that not all technical solutions are worth exploring. PMs help engineers focus on long-term solutions without compromising short-term deliverables. Some quick wins could simply be red herrings. Patience is a prerequisite to becoming an accountable PM in the AV space. Secondly, there are no prior product best practices that you can borrow from the industry. Unlike conventional consumer products, no one has successfully launched level-4 autonomous vehicles to the public at scale. Our Open Platform team has enabled Lyft to become the only company that has delivered a large volume of autonomous rides with with a safety driver at the wheel. Being the first comes with challenges. As a PM, you get to define your own standard, even when it is wrong. This is what makes being an AV PM such an exciting job. Trial and error is common in developing product roadmaps, but acknowledging and embracing failures will make your team and products stronger in the long run. Take mapping as an example. There is no industry standard for creating high-definition maps for autonomous vehicles. Therefore, we created one that we believe can set the team up for success. Besides trial and error, there are many other ways product managers can adopt to build product roadmaps. Autonomous vehicle PMs also learn their skills through attending industry conferences, talking to thought leaders in academia, and reading the latest research papers. To foster mutual learning, PMs are also responsible for sharing internal knowledge with external stakeholders. Lyft Level 5 just recently launched a valuable dataset to the public with the hope that the community can work together to design best practices for autonomous vehicles. Lastly, you need to manage your customers when there are no real customers yet. In the traditional product space, products have clearly defined user groups from which product managers can receive instant feedback. In the autonomous vehicles space, however, your customers are your vehicles and the engineers who build them, since you do not have real riders yet. Establishing a smooth internal feedback loop is critical while your product remains in the R&D phase. Within the feedback loop, both upstream and downstream stakeholders are equally important. Unlike developing traditional online products, to deliver an autonomous vehicle product, even one small task can still involve a handful of engineers across multiple teams. Way too often, some stakeholders’ feedback is neglected, which leads to product failures at a later stage. Managing stakeholder relationships is an art for AV PMs. You succeed or you fail together. PMs always have to revisit their mental stakeholder relationship map to ensure all feedback is taken into consideration. It may sound surprising, but Lyft’s hiring committee has made it crystal clear that prior AV experience should not be a hard requirement. Lyft believes that new hires can gain deep knowledge in autonomous vehicles once they join the team. Lyft’s collaborative culture makes employee onboarding extremely easy. That said, PMs are expected to speak the technical language with their engineers. They live and breathe by how well they understand the technology and should never stop learning new things. This is largely how AV PMs turn themselves into high-performing employees over time. Although prior AV experience is not a hard requirement, some high-level knowledge about how AVs work is still preferred. Some general ideas about AVs can help candidates understand the various products in the AV space and anticipate how to manage their future stakeholders with diverse technical backgrounds. Most candidates learn about the industry by following some recent news about AVs, but be mindful that most news articles are written by folks who do not necessarily work in the AV space. Try to think deeper than what you see at the surface level. For example, many articles talk about how frequently different autonomous vehicles disengage during public road tests, but the fact that disengagement is a shallow metric has already been acknowledged by many AV experts. PM candidates are encouraged to build new metrics to accurately measure AV performance. In addition to what is written in job descriptions, candidates are also expected to master product prioritization skills. Due to lengthy product life cycles, teams constantly look for guidance from their PMs with regard to what to build next week as opposed to what to build next year. A team is healthy when its engineers rarely struggle with prioritization . If you want the experience of creating a product from 0 to 1, then joining the AV space would definitely be a wise career choice. Product management for AVs is fundamentally different from traditional product management. You get to contribute to industry best practices since you are one of the first few players in the space. Lyft provides the perfect culture and platform for teams to develop best practices and encourages every employee to become an AV thought leader. You could be next. Shout out to our Mapping team at Level 5, without whom this blog post wouldn’t have existed. Our Mapping engineers are the heroes behind the scenes. AVs can drive with peace of mind thanks to high quality maps. Please come join us! Lyft AV is hiring! Our Mapping team is hiring. Our Product team is also hiring! Check out lyft.com/careers . We have open roles across Palo Alto, London, and Munich. Read more about Lyft Level 5 here . Stories from Lyft Engineering. 258 1 Product Management Product 258 claps 258 1 Written by Product Manager, Self-Driving at Lyft Stories from Lyft Engineering. Written by Product Manager, Self-Driving at Lyft Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-01-29"},
{"website": "Lyft-Engineering", "title": "announcing omnibot a slack proxy and slack bot framework", "author": ["Ryan Lane"], "link": "https://eng.lyft.com/announcing-omnibot-a-slack-proxy-and-slack-bot-framework-d4e32dd85ee4", "abstract": "Data Data Science Engineering Mobile Product Security We’re pleased to announce the initial open source release of omnibot : a Slack proxy, and Slack bot framework. We’re also releasing a python receiver library and a simple example service, eventbot , to show how the proxy works end-to-end. Over time Slack has added a number of APIs for writing integrations. The Real Time Messaging (RTM) API was the primary API in the past, and now the Event Subscription and Web APIs are recommended; however, full Slack integration functionality is spread across a number of APIs like the incoming webhook, slash command, and interactive component APIs. This has been a point of confusion for Lyft engineers new to writing Slack bots. Choosing the right API to use was closely tied to the use-case of the bot, and unfortunately the upfront choice could ultimately be wrong, as requirements change. For example, when creating a simple bot, the easiest choice is the RTM API, because getting started with it is quick, and it doesn’t require accepting web requests from the outside world. Unfortunately, if the bot needs slash command support or interactive components, it’ll be necessary to accept web requests from Slack. Also, if the bot needs to be highly available (HA), the RTM API won’t work, because only a single instance can be connected at a time to the RTM API. Changing from the RTM API to some of the other APIs is nearly a full rewrite, making this a painful early mistake. Lyft has a lot of bots, which were implemented in a number of different ways. The majority of them used the RTM API. Some used a combination of the incoming webhook APIs and the RTM API. Some used the Web API only. None of the bots that accepted commands or reacted to messages were HA. omnibot was introduced to bring consistency to how we approach Slack in our infrastructure. To facilitate this, omnibot is opinionated. It only supports parts of the Slack API that can be used in an HA manner (like the Event Subscription, Slash Command, Web, and Interactive Component APIs). What it supports is explicit so that it’s guaranteed to work consistently and reliably. Functionality that we don’t yet support gets added as new bots need it. We put effort into making our preferred patterns easy to use, to encourage engineers to be consistent in using those patterns. Ultimately, omnibot is a Slack-specialized HTTP proxy. We point all Slack apps at omnibot for event subscriptions, slash commands, and interactive components. omnibot routes those events to configured callbacks, whether the callbacks are within omnibot, or in another backend service. From the opposite side, backend services can call Slack’s Web API for any configured team and bot via omnibot’s wrapper APIs, as long as omnibot’s authorization configuration allows that service to do so. This behavior makes our Slack functionality very flexible. We can choose to send a specific event to one backend service, or multiple. We can move functionality for a bot between services. We can move functionality of bots between bots. We can move slack notification logic between services transparently and easily. Multiple services can act as a single bot without needing to have access to that bot’s credentials. We have a large number of backend services that interact with Slack. Many of them need to do common actions, like notify a set of users. The backend services know the users by email address, but not by Slack ID. Slack has a users.lookupByEmail function , but it’s ratelimited at 50 requests per minute. omnibot implements some wrapper APIs for functions like this to provide cached responses, allowing us to avoid fetching and caching this data in multiple backend services. This caching also makes bots more reliable, as it’s less likely that bot owners will get ratelimited for common issues. When receiving events from Slack APIs, it’s often necessary to parse data in the events, then make API calls to look up information about users, channels, etc. using the data. Prior to delivering events to callbacks, omnibot pre-parses the events and injects extra data into the them, so backend services can avoid common work. It caches information as it’s fetching it, which reduces ratelimits and speeds up execution of calls, even across bots in a workspace. Thanks to omnibot, we detect and alert on Slack outages prior to end-users noticing issues. On rare occasion, we’ve detected Slack API outages before Slack realized they were having issues, and were able to notify them. This is because of the rich instrumentation included in omnibot. omnibot outputs stats that make it easy to determine what part of the system is having issues, or if Slack itself is having issues. Stats are sent for common functionality and per-bot activity. Outputted logs are specific to bots, with tracing information to trace individual logged events through callbacks (including into backend services, if those services log the same tracing information). The majority of functionality in omnibot is modular, making it easy to extend and customize. Events are routed into omnibot and matched against a set of configured handlers. If a handler matches an event, it sends it through one or more configured callbacks for that handler. A handler can match against multiple bots, and can be sent to multiple callbacks. Calls into omnibot’s APIs go through a modular authentication and authorization layer (authnz). By default omnibot currently ships with authnz modules for whitelisting endpoints, and for allowing access to endpoints based on a header value (for envoy/istio style header auth). Prior to omnibot, Slack bot ownership at Lyft was a free-for-all, which lead to bots breaking when employees left the company. This happened because Slack’s apps are tied to the user that originally installed them. Since omnibot centralizes the Slack app management to a single service, it also makes it possible for a single team to then manage all the apps. If someone from that team leaves, we can properly prepare, re-install all the apps as another user, and update all the app credentials at that time. As a bonus, app credentials are also isolated to a single service, lowering the attack surface for credential issues. Since we introduced omnibot a couple years ago, it’s been adopted by more than 20 bots, each of which now also have multiple environments for testing and deployment. All of our most-important bots, like our deployment and employee directory bots, are running through omnibot at this point. Some quotes from developers writing bots using omnibot: “[omnibot] makes it very easy to make Slackbots in the Lyft workspace” “I can’t believe how easy it was to make a bot using omnibot. This is awesome!” “I just wrote my first omnibot bot a few days ago and it was super straightforward” For how to install, configure, and use omnibot, see the omnibot docs , and checkout the eventbot service to see how to implement a bot with a medium level of complexity. omnibot may not currently support the features you need for the bot you’re writing. We’re happy to take contributions to add support for features you need. Check out our contribution docs for how to get started. Interested in open source work and having a big impact? Lyft is hiring ! Drop me a note on Twitter or at ryan.lane@lyft.com . Stories from Lyft Engineering. 258 Thanks to Michael Rebello . Slack Engineering Open Source Bots Automation 258 claps 258 Written by Security Engineer and Open Source Team Lead at Lyft Stories from Lyft Engineering. Written by Security Engineer and Open Source Team Lead at Lyft Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-12-11"},
{"website": "Lyft-Engineering", "title": "introducing flyte cloud native machine learning and data processing platform", "author": ["Allyson Gale"], "link": "https://eng.lyft.com/introducing-flyte-cloud-native-machine-learning-and-data-processing-platform-fb2bb3046a59", "abstract": "Data Data Science Engineering Mobile Product Security By Allyson Gale and Ketan Umare Today Lyft is excited to announce the open sourcing of Flyte , a structured programming and distributed processing platform for highly concurrent, scalable, and maintainable workflows. Flyte has been serving production model training and data processing at Lyft for over three years now, becoming the de-facto platform for teams like Pricing, Locations, Estimated Time of Arrivals (ETA), Mapping, Self-Driving (L5), and more. In fact, Flyte manages over 7,000 unique workflows at Lyft, totaling over 100,000 executions every month, 1 million tasks, and 10 million containers. In this post, we’ll introduce you to Flyte, overview the types of problems it solves, and provide examples for how to leverage it for your machine learning and data processing needs. With data now being a primary asset for companies, executing large-scale compute jobs is critical to the business, but problematic from an operational standpoint. Scaling, monitoring, and managing compute clusters becomes a burden on each product team, slowing down iteration and subsequently product innovation. Moreover, these workflows often have complex data dependencies. Without platform abstraction, dependency management becomes untenable and makes collaboration and reuse across teams impossible. Flyte’s mission is to increase development velocity for machine learning and data processing by abstracting this overhead. We make reliable, scalable, orchestrated compute a solved problem, allowing teams to focus on business logic rather than machines. Furthermore, we enable sharing and reuse across tenants so a problem only needs to be solved once. This is increasingly important as the lines between data and machine learning converge, including the roles of those who work on them. To give you a better idea of how Flyte makes all this easy, here’s an overview of some of our key features: Flyte frees you from wrangling infrastructure, allowing you to concentrate on business problems rather than machines. As a multi-tenant service, you work in your own, isolated repo and deploy and scale without affecting the rest of the platform. Your code is versioned, containerized with its dependencies, and every execution is reproducible. All Flyte tasks and workflows have strongly typed inputs and outputs. This makes it possible to parameterize your workflows, have rich data lineage, and use cached versions of pre-computed artifacts. If, for example, you’re doing hyperparameter optimization, you can easily invoke different parameters with each run. Additionally, if the run invokes a task that was already computed from a previous execution, Flyte will smartly use the cached output, saving both time and money. In the example above, we train an XGBoost model using the dataset provided here . The machine learning pipeline is constructed in Python and consist of the following four tasks, which align with a typical machine learning journey: Data preparation and test validation splits Model training Model validation and scoring Computing metrics Note how each task is parameterized and strongly typed, making it easy to try different variants and use in combination with other tasks. Additionally, each of these tasks can be arbitrarily complex. With large datasets, for example, Spark is more preferable for data preparation and validation. Model training, however, can be done on a simple model coded in Python. Lastly, notice how we’re able to mark tasks as cacheable, which can drastically speed up runs and save money. Below, we combine these tasks to create a Workflow (or “pipeline”). The Workflow links the tasks together and passes data between them using a Python based domain specific language (DSL). Every entity in Flyte is immutable, with every change explicitly captured as a new version. This makes it easy and efficient for you to iterate, experiment and rollback your workflows. Furthermore, Flyte enables you to share these versioned tasks across workflows, speeding up your dev cycle by avoiding repetitive work across individuals and teams. Workflows are often composed of heterogeneous steps. One step, for example, might use Spark to prepare data, while the next uses this data to train a deep learning model. Each step can be written in a different language and utilize different frameworks. Flyte supports heterogeneity by having container images bound to a task. By extension, Flyte tasks can be arbitrarily complex. They can be anything from a single container execution, to a remote query in a hive cluster, to a distributed Spark execution. We also recognize that the best task for the job might be hosted elsewhere, so task extensibility can be leveraged to tie single-point solutions into Flyte and thus into your infrastructure. Specifically, we have two ways of extending tasks: FlyteKit extensions : Allow contributors to provide rapid integrations with new services or systems. Backend plugins : When fine-grained control is desirable on the execution semantics of a task, Flyte provides backend plugins. These can be used to create and manage Kubernetes resources, including CRDs like Spark-on-k8s, or any remote system like Amazon Sagemaker, Qubole, BigQuery, and more. Flyte is built to power and accelerate machine learning and data orchestration at the scale required by modern products, companies, and applications. Together, Lyft and Flyte have grown to see the massive advantage a modern processing platform provides, and we hope that in open sourcing Flyte you too can reap the benefits. Learn more, get involved, and try it out by visiting www.flyte.org and checking out our Github at https://github.com/lyft/flyte . Este artículo también está en español: eng-espanol.lyft.com Stories from Lyft Engineering. 934 4 Thanks to Ryan Lane and Mark Grover . Machine Learning Kubernetes Data Science Orchestration Data Processing 934 claps 934 4 Written by Product manager for Flyte.org @Lyft. Previously @Google Search, Android. Curious about ethics and decision-making. Stories from Lyft Engineering. Written by Product manager for Flyte.org @Lyft. Previously @Google Search, Android. Curious about ethics and decision-making. Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-03-31"},
{"website": "Lyft-Engineering", "title": "meet val software engineer at lyft", "author": ["Sherin Thomas"], "link": "https://eng.lyft.com/meet-val-software-engineer-at-lyft-4f0c5486dc17", "abstract": "Data Data Science Engineering Mobile Product Security Valeria Martinez joined the Data organization at Lyft as a full time Software Engineer, exactly one year ago, but her time at Lyft actually began almost 2 years ago when she joined the Apprenticeship Program. On the anniversary of her career at Lyft, we decided to get to know her a little better and hear the story of her transition into tech, data and coding from the world of plants and microbes. Val’s unusual journey into tech began when she was studying Bio-Environmental Sciences at Texas A&M University. While working on a statistical analysis tool for her undergrad research, she developed an interest in software development and decided to join a coding bootcamp. Cut to the present day — after spending 9 months in the Software Engineering Apprenticeship program, Val is kicking butt in her role as a Software Engineer on the Observability team. A few weeks ago, my colleague Lakshmi and I did a Q+A with Val. Here are some of the highlights of our conversation. My interest in tech began when I was a teenager. The first time I actually touched code was when I was in middle school. I’m not sure if you are all familiar with MySpace, but it was THE social networking site before Facebook came around. My friends and I created a site dedicated to creating layouts and graphics and spent our summers adding content. I submerged myself in YouTube tutorials and learned how to use software programs like Photoshop, and learned the basics of HTML code. It was never something I considered as a career choice, this was really just something I just did for fun with my friends. By the time I got to high school, I developed other interests and began pursuing a career in the sciences. It wasn’t until my senior year of college, when I spent a year doing research in a plant pathology & microbiology lab that I realized that I wanted to pursue something else. I didn’t know what that ‘something else’ was, so I started thinking about what I’d enjoyed doing thus far. During my time in a lab, I mostly enjoyed working with data, I enjoyed being able to collect one data point, then 10, then 100, and being able to use statistical analysis software like JMP to create visual representations of that data. I thought back to my silly MySpace layouts site and determined that, I liked creating things through software engineering. When I decided to pursue Software Engineering I considered several options such a Master’s degree in CS, bootcamp, taking online courses through Khan Academy . Initially, I was skeptical about bootcamps, they’re not very popular where I’m from (Houston). But after speaking to my sister (who is an engineer) and other folks who went to bootcamps and now have successful careers, I grew more confident about joining a bootcamp. There are plenty of options in Bay Area and that is why I made the decision to move to the Bay Area from Houston. When I was starting out I did not know whether I wanted to work on frontend or backend , so I decided to study both and choose later. Fun Fact: during bootcamp I built an application called “Save a Paw” that allows users to search for pets by zip code and filter based on breed, gender and age. This was my first time using an API (the Petfinder API in this case) to build an app. I’d always dreamed of opening an animal shelter, so this was a perfect project that combined my love of animals and code. The Software Engineering Apprenticeship program is designed for engineers with a nontraditional professional background. It was 9 months long, where the first 6 months were spent learning the ropes — requirements gathering, writing tech specs, building software, executing on the project to bring it to a finish. We also collected metrics and data to verify that our features are making their intended impact. It is exactly like a full time job — with standups, meetings, pair programming sessions etc. After the 6 months were over, we did a 3 month internship embedded in a team of our choice. This gave us the opportunity to apply all that we learned in the last 6 months to build actual features for Lyft. This was extremely rewarding. Absolutely! The Apprenticeship Program is designed for people coming from different industries into tech. Anthony Velazquez (an amazing engineer here at Lyft) launched and runs the program. His sole purpose was to help us ramp up and be successful. There were three other members in my cohort. All of us were from different backgrounds. We bounced ideas off of each other, helped each other and learned a ton in the process. The primary criteria for successful conversion to full time roles was “growth.” They wanted to see our learning progression throughout the 9 months and how much we grew. The program helped me learn how things are done in the industry and convert my theoretical knowledge into real world industry applications which was invaluable. Thank you :) A lot of my excitement comes from the experience of building cool things and seeing a project come to life! I enjoy the early stages of a project when it is being conceptualized, working with designers, writing a tech spec and finally seeing all this come together in a tangible way. I get a great sense of gratification from that. My team owns all of the metrics and logging infrastructure at Lyft. It is one of the core teams here since all the other teams at Lyft need metrics, alerting and logging for their projects and we enable that. It’s pretty cool that I’m still able to work so closely with data — outside of a lab. My day usually starts with responding to code review requests and comments that people left on my code. I set aside around 20 to 30 minutes in the morning for that. After that I participate in the team “ standup ,” where we share updates, discuss what each of us are working on, ask for help with blockers and report progress. This is followed by lunch — we do team lunches at least once a week. After lunch I write code, do some feature work and make progress on my tasks for the day. I ask a lot of questions over Slack and in person if I need help with something or get stuck. All my teammates (and everyone at Lyft in general) are super helpful! Being a platform team, we also spend a lot of time answering technical customer queries related to our products. Customers in our case are other other engineers at Lyft. Occasionally I conduct technical interviews for prospective candidates. I really love getting the chance to talk to so many awesome people interested in learning more about Lyft. Definitely had doubts early on! There are not many people in tech who look like me — I also faced impostor syndrome in the beginning. Finding a community of women and Latinas in tech helped me immensely. I became part of an organization of LatinX in tech called “Techqueria ” Finding that support system was super valuable. They all had similar background as me and gave me a lot of support. My best advice to folks considering a career switch — find a support network! Find people who are in the same boat as you, who can offer guidance and allay your fears. Grumpy cat Dwight Tabs “ Educated ” by Tara Westover Working out, lifting weights, drawing (I like drawing animals), being around dogs! This year I participated in a panel discussion at the Tech Inclusion conference where some of my colleagues and I got to share insights on our transitions from unrelated career paths or unconventional backgrounds in tech. I used to be intimidated by large crowds and public speaking, but I pushed myself to do it and looking back at the girl I was two years ago, I feel really proud of having overcome my fear of public speaking. My generation is the first one in my family to go to college and growing up, I did not have much guidance on what career opportunities were out there. I ended up switching careers but am extremely proud of my journey into tech. It is never too late. Lyft is hiring for awesome software engineers from all backgrounds! Join us . Este artículo también está en español: eng-espanol.lyft.com Stories from Lyft Engineering. 267 Thanks to Polly Peterson and Valeria Martinez . Programming Lyft Engineering Culture Apprenticeship Inclusion 267 claps 267 Written by Engineering at Lyft, previously Twitter, Google. I write about tech interviews, technology and general musings. Occasionally I rant about stuff. Stories from Lyft Engineering. Written by Engineering at Lyft, previously Twitter, Google. I write about tech interviews, technology and general musings. Occasionally I rant about stuff. Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-04-14"},
{"website": "Lyft-Engineering", "title": "open sourcing amundsen a data discovery and metadata platform", "author": ["Tao Feng"], "link": "https://eng.lyft.com/open-sourcing-amundsen-a-data-discovery-and-metadata-platform-2282bb436234", "abstract": "Data Data Science Engineering Mobile Product Security By Tao Feng , Jin Hyuk Chang , Tamika Tannis , Daniel Won In a modern data-driven company like Lyft, every interaction on the platform is powered by data. The challenges that arise from complex data generation, ETL processes, and analytics make metadata significantly important. Moreover, the types of data resources are constantly increasing. At Lyft, these resources include SQL tables and views in Redshift, Presto, Hive, PostgreSQL, as well as dashboards in business intelligence tools like Mode, Superset, and Tableau. With growing data resources it becomes increasingly difficult to know what data resources exist, how to access them, and what information is available in those resources. Previously we introduced Amundsen — our solution for data discovery — from a high level product perspective. Over the past year since launching in production at Lyft, we’ve made all of Amundsen’s repositories public, explored various expansions and improvements, and established a growing open source community. Today, we are proud to officially announce that we are open sourcing Amundsen ( https://github.com/lyft/amundsen ), our data discovery and metadata platform. In this post, we discuss Amundsen’s architecture in depth, explain how this tool democratizes data discovery, and cover some challenges we faced when designing the product. At the end, we highlight some of the great contributions from our early adopters, and summarize our future roadmap. The graph below represents Amundsen’s architecture at Lyft. Amundsen follows a micro-service architecture and is comprised of five major components: Metadata Service handles metadata requests from the front-end service as well as other micro services. By default the persistent layer is Neo4j, but can be substituted. Search Service is backed by Elasticsearch to handle search requests from the front-end service. By default the search engine is powered by ElasticSearch, but can be substituted. Front-End Service hosts Amundsen’s web application. Databuilder is a generic data ingestion framework which extracts metadata from various sources. Common is a library repo which holds common codes among all micro services in Amundsen. Amundsen’s metadata service provides metadata about data resources. This metadata is displayed in the front-end service’s web application, and is also utilized by other services at Lyft. The metadata service currently supports three types of resources: Table resources include metadata such as the table’s name, descriptions, columns, relevant statistics, plus more. User resources represent individual users or teams and include metadata such as names, team information, and contact information. Dashboard resources (currently work in progress). Internally the metadata service connects with its persistent layer through a proxy and serves requests via REST APIs. By default, the persistent layer is powered by Neo4j, an open source graph database. The service can also be configured to use Apache Atlas — another metadata engine. This integration with Apache Atlas was the result of an early open source contribution. Amundsen models metadata entities as a graph which makes it easy to extend the model when more entities are introduced. All of the entities — such as tables, columns, and schemas — are connected via edges that represent the relationships between them. Each entity is also provided with a unique resource ID for identification. Lyft currently uses the Neo4j community version 3.3.5 to host metadata. We backup the Neo4j graph into Amazon S3 every four hours. Amundsen’s search service provides an API for indexing resources into a search engine and serves search requests from the front-end service. Similar to the metadata service, the search service can interact with different search engines through a proxy. By default the search service is integrated with ElasticSearch 6.x but it can also be integrated with Apache Atlas, which provides similar search capabilities with Solr. We currently support various search patterns for a more flexible search experience: Normal search: returns the most relevant results for the given search term and particular resource type. Category search: filters for resources where a primary search term matches a given metadata category (e.g. search for database:hive ), then returns results matching a secondary search term based on relevancy. Wildcard search: allows user to perform wildcard search over different resources. Amundsen’s front-end service is composed of two distinct parts: A React application for client side rendering. A Flask server for serving requests and acting as an intermediary for metadata or search service requests. The React application is written in TypeScript , leverages Redux for managing its application state, and follows the “ Ducks ” modular approach for code structuring. Redux-Saga was introduced as middleware to effectively manage the side effects of asynchronous requests on the application state. Babel is used to transpile our source code across different ECMAScript standards, Webpack is used to bundle the source code, and Jest is used for unit testing. The application also uses NPM for managing dependencies. Keeping the open source community in mind, Amundsen’s front-end application is highly configurable. There are feature flags and various configuration variables to account for the unique desires of different organizations. For example, at Lyft we ingest and index our employees as user resources. Realizing that not all organizations have a use case to integrate employee data, the features related to user resources are hidden by default in the application’s user interface but can be switched on with a feature flag. Databuilder is an ETL ingestion framework for Amundsen, which is highly inspired by Apache Gobblin . There are corresponding components for ETL (Extractor, Transformer, and Loader) that deal with record level operations. A component called Task controls all three components. Job is the highest level component in Databuilder that controls Task and Publisher. Job is also what the client should use to launch an ETL job. In Databuilder, each component is highly modularized. Using namespace based configurations and HOCON (Human-Optimized Config Object Notation) makes Databuilder highly reusable and pluggable. For example, a transformer can be reused within an extractor, or an extractor can be reused within an extractor. Extractor extracts record from the source. This does not necessarily mean that it only supports pull pattern in ETL. For example, extracting record from messaging bus makes it a push pattern in ETL. Transformer takes records from either extractor or from transformer itself (via ChainedTransformer) to transform records. Loader takes records from transformer or from extractor directly and loads them to the staging area. As loader is operated at the record level, it’s not capable of supporting atomicity. Publisher is an optional component. It’s common usage is to support atomicity at the job level and/or to easily support bulk load into the sink. The diagram below shows an example of how a databuilder job fetches metadata from Hive metastore and publishes that metadata to Neo4j. Each distinct metadata source could be fetched through a different databuilder job. At Lyft we use Apache Airflow as the orchestration engine for Databuilder. Each databuilder job will be an individual task within the DAG (Directed Acyclic Graph). Each type of data resource will have a seperate DAG since it may have to run with a different schedule. At Lyft we initially started with Amazon Redshift as our main data warehouse. Eventually we began migrating from Redshift to Hive. Over time the volume, complexity, and breadth of data grew exponentially and today we have in the order of a hundred thousand tables in Hive, and around a few thousand tables in Redshift. As Lyft grew, finding relevant data resources became more and more important, yet it also became a bigger challenge as the data landscape became increasingly fragmented, complex, and siloed. Amundsen now empowers all employees at Lyft — from new hires to those more experienced with our data ecosystem — to quickly and independently find the data resources they need to be successful in their daily tasks. Lyft’s data warehouse is on Hive and all physical partitions are stored in S3. Our users heavily rely on Presto as a live query engine for Hive tables as both share the same Hive metastore. Each query through Presto is recorded to a Hive table for auditing purposes. At Lyft, we would like to surface the most important or relevant tables to users. To achieve this, we leverage the Databuilder framework to build a query usage extractor that parses query logs to get table usage data. We then persist this table usage as an Elasticsearch table document. This information helps the search service surface the most relevant tables based on usage ranking from database access logs. Data ownership could be quite confusing for users. For example, lots of resources don’t have someone responsible for it. This problem is intertwined with the fact that data is hard to find, an issue that Amundsen tries to solve in a scalable way. A lot of the process for finding data consists in interactions between people. It is flexible but error prone and not scalable. It is time consuming unless you happen to know who to ask. Amundsen addresses this problem by creating relationships between users and data resources and tribal knowledge is shared through exposing these relationships. Below is an example of what a user profile looks like in Amundsen. There are currently three types of relationships between users and resources: followed , owned , and used . By exposing this information, experienced users will themselves become helpful data resources for other members on their team, and for other employees in similar job roles. For example, new hires can visit Amundsen and search for experienced users on their team, or for other employees who conduct similar kinds of analysis. The new hire will then be able to see which resources they can consider diving deeper into, or contact that person for more guidance. To make this tribal knowledge easier to find we also add a link to each user’s Amundsen profile on the profile pages of our internal employee directory. Furthermore, we have also started building out a notification feature which allows users to request further information from owners of data resources if existing metadata context is lacking. For example, if a table is missing a description a user can directly send a request to the owners of that table in Amundsen asking the owners to improve their table descriptions. Capturing the ABC (Application Context, Behaviour, Change) metadata for data resources makes users more productive. We would like to capture all metadata that is meaningful for each type of data resource. Here is an example table detail page which looks like below: There are two types of metadata from the above table page: Programmatically curated Description Data range or watermark Table stats Owners Frequent users Source code of the table Application (e.g. Airflow DAG) that generates the table Numbers of records for the table Manually curated Description Tags Owners Some metadata — such as descriptions, tags, and owners — can be curated both programmatically and manually. To ingest additional metadata into Amundsen, a developer needs only to leverage the databuilder ingestion framework. They can either reuse existing extractors , or build a new extractor and model based on the interface needed for accessing that metadata at its source. There are two types of approaches to index metadata into Amundsen — pull and push: Pull approach: Periodically update the metadata by pulling from the source via crawlers. Push approach: The sources (e.g databases) push metadata into a message queue (e.g Kafka), to which downstream sources can subscribe to and consume changes. When Amundsen was bootstrapped, we initially considered the push approach, however Lyft had only just started building a reliable messaging platform based on Kafka. As a result, we built the data ingestion library databuilder to use the pull approach to index metadata into Amundsen. We acknowledge that the pull approach it is not scalable, as different organizations within Lyft would prefer to push their metadata. The benefit of the push model is to allow the metadata be indexed in near real-time. As a result, we are currently moving into a hybrid model: organizations will be able to push metadata into the Kafka topic based on a predefined schema, while Amundsen’s databuilder still pulls metadata through a daily Airflow DAG. Amundsen currently doesn’t provide metadata versioning. When the Databuilder Airflow DAG finishes, the latest metadata is upserted into the graph. Initially we did not delete stale metadata, so when a table was deleted from a data store the metadata for this table continued to exist in Amundsen. This created some confusion for users so we added a separate graph-cleansing Airflow task into our workflow DAG to remove stale metadata. In the long term, we plan to add versioning to data sets, so users can see how a data set has evolved instead of simply looking at the current/latest snapshot of metadata Amundsen chose the graph data model to represent its metadata. Graph data model is not a common choice for most applications, but we believe it is a great fit for Amundsen as it deals with a lot of relationships among entities. A graph data model is to represent relationship between entity (vertex) and relation (edge). With this characteristic, it makes it easy to extend the model when more entities are introduced. In the graph data modeling, one of the design decisions that needs to be made is to decide whether to add new information as an entity’s property or as a new entity. In Amundsen, we decided to add a new entity as a default choice as it opens up an opportunity for relationship with other entities. In Feb 2019, we made Amundsen’s code repositories public as part of an open source soft launch. Since then we have been collaborating with awesome engineers from some early adopters at companies such as ING and Square. There are more users than just these organizations. We apologize for not listing all of them. Amundsen has support for some of the popular data warehouses in the industry including, but not limited to: Snowflake, Hive, Redshift, Postgres, Presto, etc. As of today, a few companies use Amundsen in production and 20+ people have contributed code back to Amundsen, including support for valuable features like new extractors for BigQuery and Redshift (Databuilder), support for Amundsen to integrate with Apache Atlas (metadata service & search service), and markdown support for description editing in the UI (front-end service). We have a thriving community of 60+ companies and 250 users at the time of this writing. We’d love for you to join our community . Moving forward there are many improvements that we are currently working and new features that we would like to tackle with the community. Our roadmap currently includes: Search & Resource page UI/UX redesign Email notifications system Indexing Dashboards (Superset, Mode Analytics, Tableau, etc) Lineage integration Granular ACL (access control) Further details can be found in Amundsen’s open source roadmap doc . Amundsen has been running in production at Lyft for over a year now with about 1000 weekly active users at Lyft internally. We have extremely high penetration rate (90+%) for technical roles like Data Scientist, Research Scientists and Data Engineers while also being used by business users like Marketing and Community associates. In the future, we will continue working with our growing community internally and externally to enhance the data discovery experience and boost users’ productivity. Last but not least, please join the community through our open source slack channel . We would like to thank many people for making Amundsen happen: Mark Grover who provides product leadership. Jin Chang, Tamika Tannis, Tao Feng, Daniel Won who are currently working on the project at Lyft. Shenghu Yang, Yuko Yamazaki, Prashant Kommireddi who provide engineering leadership. Matt Spiel who provides UI design support. Philippe Mizrahi, Alagappan Sethuraman, Junda Yang, Ryan Lieu who used to work on Amundsen at Lyft. Verdan Mahmood , Bolke de Bruin , Nanne Wielinga from ING who have worked with us since the inception of the open source journey. Many different people (e.g., Jørn Hansen , Alyssa Ransbury , Joshua Hoskins to name a few) from different companies who help the community continue growing. Data Portal team at Airbnb, who gave us a lot of inspiration and suggestions. As always, Lyft is hiring! If you’re passionate about developing state of the art machine learning/optimization models or building the infrastructure that powers them, read more about them on our blog and join our team . Stories from Lyft Engineering. 962 6 Thanks to Ryan Lane and Mark Grover . Metadata Data Discovery Amundsen Data Open Source 962 claps 962 6 Written by Apache Airflow PMC and Committer | Co-creator @Amundsen Stories from Lyft Engineering. Written by Apache Airflow PMC and Committer | Co-creator @Amundsen Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-11-06"},
{"website": "Lyft-Engineering", "title": "beyond l2 loss how we experiment with loss functions at lyft", "author": ["Mohamad Elmasri"], "link": "https://eng.lyft.com/beyond-l2-loss-how-we-experiment-with-loss-functions-at-lyft-51f9303f5d2d", "abstract": "Data Data Science Engineering Mobile Product Security Estimating expected time of arrival (ETA) is crucial to what we do at Lyft. Estimates go directly to riders and drivers using our apps, as well as to different teams within Lyft. The Pricing team, for example, uses our ETA estimates in order to determine the rider’s fare. The Dispatch team uses them to find the best matches between drivers and groups of riders. To provide reliable and accurate ETA estimates, the Mapping team built a sophisticated system that includes a machine learning ETA prediction model. The main component in any prediction model is the “loss” (or “objective”) function, and choosing the right loss function for the data can improve prediction accuracy. This post discusses a recently developed method that generates and compares loss functions that are suitable for calculating averages, such as the ETA. Loss functions can be thought of as trails through a mountain range that can lead to different peaks. Each trail varies in difficulty. The amount of training data is the amount of energy one has to hike up the mountain. The optimization method is the trail guide. The result of all this is a prediction model or a trained hiker. Therefore, to take any trail we need to consider the energy and have the right guide, just as for each loss function we need to consider the amount of training data and the optimization method we’ll use. The common approach when experimenting with loss functions is to create training, validation, and test datasets. Training models are run with different loss functions and evaluated on the test dataset with some evaluation metrics, in order to hopefully find a winner. To use the metaphor: Each day a hiker trains by mustering all their energy, choosing a trail and a guide to take them up the mountain, and stopping after they run out of energy. Trained hikers are compared based on how far they make it up the mountain. However, and this is a big “however”, how can we compare hikers that trained on different trails? In other words, are estimates from different loss functions comparable? In evaluating competing models, Gneiting (2011) argued that one should either: Fix and communicate a single evaluation loss to modelers; or communicate the desired target (i.e. the mean), such that loss functions that are consistent for the target to be used in training. Consistency is like identifying all trails that lead to the same peak, and thus the peak is the target. Formally, consistency with respect to a target means that the loss achieves its minimum value at the target. Mathematically, if the target is the mean, μ = E[ Y ], of some univariate random variable Y , a loss function L is consistent for μ if for any other value z ∈ ℝ. Why the different approaches? The two approaches enable us to be clear on what is being evaluated, since trails can lead to different peaks. This relates to the statistical concept of interpretability of the value being estimated: Are we estimating the mean, or the median, or some other property of the predictive distribution? If we know exactly the desired evaluation loss, then regardless of interpretation, it suffices to communicate it to modelers. This facilitates automated rank based decisions, since modelers have already acceded to the evaluation loss. Modelers have the choice to use this loss in training if desired. If interpretation is important, then it suffices to communicate the target (i.e., the destination peak) to modelers. For example, if prediction is used in derivative or secondary applications, then one might want to estimate the mean, since it is additive. Lyft’s ETA team trains our models to estimate the expected travel time, as opposed to the median, to preserve interpretability in stacked models. Since we already know the peak we want to reach, the most famous trail is the squared-error (L2) loss, defined as: That raises two questions. The first is intuitive: What are the other loss functions that are also consistent for the mean, and can they be better in some way? The other question is slightly non-intuitive. If two hikers trained on different trails leading to the same peak, we clearly cannot evaluate them on how far they perform on a single trail. The evaluators’ choice of trail might influence their ranking. If a hiker knows beforehand which trail they will be evaluated against, they might as well train on that trail to increase their competitiveness, going back to the first approach by fixing the evaluation loss. In practice, if we have another loss function other than L2 that is also consistent for the mean, how do we compare them? If we use a specific metric to evaluate their outputs, for example, the root mean square error (RMSE), can we ensure that RMSE is impartial to the choice of the training loss? Here n is the number of observations of y, and x is a prediction. In this view, moving beyond the L2 loss while preserving the target (i.e., the mean) not only requires a method to generate consistent losses for the mean, but also an impartial diagnostic tool for such losses; an evaluation metric that does not depend on any specific trail. The recent results of Ehm et. al. (2016) provide exactly that, a loss-generating method and an impartial diagnostic tool called the Murphy’s Diagram. In the next section I will briefly discuss the Ehm et. al. (2016) framework and show some consistent losses for the mean, and then briefly introduce the Murphy’s Diagram as a diagnostic tool. Later I’ll discuss the optimization method and some practical implementation issues of the generated losses, before finally presenting an example from our data. For a point prediction x of some y ∈ ℝ , Ehm et al. (2016) showed that any loss function that is consistent for the mean admits a mixture representation of the form: For a continuous non-negative function h(u) , where: It is called a mixture since we’re integrating over some value u that is between model prediction x and the true observation y , where each u has a weight defined by h(u) . lᵤ ( x,y) is called the simple loss, since it characterizes the family of all losses that are consistent for the mean. Let’s look at some known examples: L2 loss. Assume that x < y (the converse follows similarly), and let h(u) = 2 , we have: Binary cross-entropy. Let h(u) = 1/{u(1-u)} with 0<u<1. By conditioning on y=1 and letting p ∈ [0,1] (I am using p to indicate a probability prediction), we have: In the other case when y=0, we have: Now all we need is to define h(u) and then solve the above integral representation to generate other consistent losses for the mean. For example, the Patton (2011) family is generated by setting: Then, One can retrieve the L2 loss from Patton losses by setting b=2 and completing the square. Another example is the Exponential Bregman family : This is the result of setting h(u) = exp(αu) . The Bregman family nests the L2 at the limit when α → 0. Now that we are able to generate consistent losses for the mean, how should we compare them? The idea is similar. All losses are essentially the area under the graph of the simple loss function lᵤ(x,y) weighted by h(u) . Using the prediction of a model, we can approximate the area under the graph of lᵤ(x,y) at different values of u . If one model has a smaller area under lᵤ(x,y) then no matter what the positive weight h(u) is, the model will have a smaller evaluation loss than the other. Here we are also using consistent evaluation metrics (for example, the absolute loss |x-y| is not a consistent evaluation metric for the mean.) In practice, say we have models 1 and 2, with predictions xᵥᵢ , for events yᵢ, i∈ {1,… , n} and v∈ {1,2}. Then we can estimate the simple loss function at each u as: If s₁(u) < s₂(u) for all u∈ {xᵥᵢ, yᵢ}, then model 1 would beat model 2 under all consistent evaluation metrics. Why is it enough to evaluate only at those values of u ? The answer comes from the theory of convergence of empirical processes and it is out of the scope of this blog post. In brief, since we do not have any other prediction or true observation between the values we already have, then our best guess of the evaluation loss at the missing points is merely an average of the loss at the nearby observed points. Hence, it is enough to evaluate at the set of predictions and observations only. Plotting the values of sᵥ(u) against u is called the Murphy’s diagram, representing a visual diagnostic tool of competing models. For more information about the interpretation of Murphy’s diagram, please refer to Ehm et al. (2016) . The Murphy’s Diagram is an extra diagnostic tool a Data Scientist can carry in their tool set; it is not meant to replace all other tools. Now that we know how to generate losses and compare them, the next section focuses on practical implementation of the Patton and Bregman families of losses, and some issues that can be encountered in the training procedure. Recalling our mountain analogy, in order to begin hiking, if untrained, we’ll need a guide to help us navigate the terrain. Ideally, we would like the most experienced guide for the chosen trail. In practice, unfortunately, it is not always possible since some trails are difficult and require a great deal of energy. In other words, implementation issues arise if the optimization guide is not sufficiently trained for the chosen loss function and/or there is limited data. Machine learning optimization methods generally rely on the gradient descent algorithm, or a variation of it. The gradient descent requires some amount of tuning and regularization. It generally performs well in specific types of terrain, though not in all. Even though we are able to generate and compare losses as discussed in the previous section, implementing such losses can become challenging. The Patton and the Exponential Bregman families are asymmetric families of losses, except at the values where they represent the L2 loss. Patton losses are also restricted to the positive real line. Even though ETAs are strictly positive variables, in iterative optimization such as the gradient descent, the predictions can sometimes be negative. This can cause some instability issues. I find that the best way to understand loss functions is to visualize them and their gradients. The following figure shows the shape of Patton and Bregman losses, for different parameter values, chosen to be positive to satisfy the conditions of Patton losses. For predictions away from 0, Patton b=0 and b=1 have loss values much lower than L2. The closer the predictions are to 0, the larger their loss value become in comparison to the L2. For b=3 , the reverse pattern is observed, and the loss is much larger than L2 for large values. Bregman loss at α= -0.1 is flatter for larger errors and steeper for smaller ones. The Bregman family doesn’t require positivity. As for the gradient, we can see from the following figure that Patton b ∈ {0, 1} are extremely flat around the prediction. The signal becomes stronger closer to 0. However, when the true value is far away from 0, those losses are very flat which can slow down the convergence of the optimization process. For Patton b=3 , there is an inflection point just below 2. Such an inflection point can cause unexpected optimization problems, which we also faced when implementing with XGBoost and LightGBM packages. The Bregman family is more easily implemented, although the parameter α seems to tune best close to 0, i.e. closer to L2. In summary, the benefits of using Bregman or Patton losses are: They are consistent for the mean. They are asymmetric losses (except when they represent L2), and can be used in cases where practitioners want to penalize errors differently based on where they fall. They are scale invariant and thus can be useful in certain applications. For example, in all the Patton family (excluding b=2 ). However, they don’t come free of charge: They are still sensitive to outliers with varying degrees, since their derivative depends on the magnitude of the error. The absolute loss derivative depends only on the sign of the error. Some have inflection points close to 0 (for example b=3 ), which makes gradient based optimization complicated, as it requires some tweaking for saddle-point approximations. We’ll finalize this section with a toy example from our data. Using XGBoost , we implemented the Patton ( b=1 ) custom loss and compared it to the results of L2. Using training and validation sets, we trained the tree-based model and tuned the learning rate for each loss separately. The following figure shows the Murphy’s Diagram on the test set for both losses. We can see that Patton b=1 performed better in the medium prediction range and more similarly to the L2 at the extremes. Overall, the Patton b=1 loss is less than the L2 loss almost everywhere, implying that Patton b=1 should result in less error metrics in almost all consistent losses for the mean. The following table shows the ratio of Patton b=1 loss to the L2 when we evaluate them in RMSE and other Patton losses. For example, Patton b=1 achieved 2.25% less errors in RMSE compared to the L2, and similarly for all other consistent evaluation losses. We have briefly summarized a method that generates consistent losses for the mean of point estimates, and a diagnostic tool to compare them. The mean is only a special case, as shown by Ehm et al. (2016) , which also showed similar representation for other properties of the distribution(quantiles, for example). One thing to keep in mind is that this theory and construct only applies to point estimates — the multivariate case still has some theoretical issues to resolve. In building our ETA models, we have experimented with the Patton and Bregman families of losses alongside the classical L2, which has enriched our understanding of how the shape of those losses (as shown in previous figures) affects our business metrics. The tool presented also enables us to design our own losses based on desired impact on some metrics. On Lyft’s ETA team, we know which peak we want to reach. We have explored some trails to take us there. Not all trails are easy to hike, and we are looking for enthusiastic Research and Data Scientists to help us train more hikers and guides to navigate other trails, gaining more insight and expertise about this important topic. If you’re interested in joining, please check out our job listings. Ehm, W., Gneiting, T., Jordan, A., & Krüger, F. (2016). Of quantiles and expectiles: consistent scoring functions, Choquet representations and forecast rankings. Journal of the Royal Statistical Society: Series B (Statistical Methodology) , 78 (3), 505–562. Gneiting, T. (2011). Making and Evaluating Point Forecasts. Journal of the American Statistical Association , 106:494, 746–762, Patton, A. J. (2011). Volatility forecast comparison using imperfect volatility proxies. Journal of Econometrics , 160 (1), 246–256. Stories from Lyft Engineering. 228 Thanks to Ryan Lane , Albert Yuen , and Michael Rebello . Machine Learning Modeling Statistics Big Data Data Science 228 claps 228 Written by Stories from Lyft Engineering. Written by Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-12-23"},
{"website": "Lyft-Engineering", "title": "envoy mobile v0 2 ready for donuts", "author": ["Michael Rebello"], "link": "https://eng.lyft.com/envoy-mobile-v0-2-ready-for-donuts-a8aa852a900b", "abstract": "Data Data Science Engineering Mobile Product Security We’re excited to share that Envoy Mobile has evolved from a demo to a true mobile networking library that can handle Lyft rides. Even for donut runs! This summer, we announced Envoy Mobile — an iOS and Android library — with the goal of bringing the power and flexibility of Envoy to mobile apps. At the time, the open source repo simply showcased a proof-of-concept demo (where mobile clients proxied requests through Envoy running in the application process). Today, we’re releasing v0.2 of Envoy Mobile — the first alpha version of the library with direct APIs for performing network requests through Envoy. Envoy Mobile v0.2 is a fundamental shift in how mobile clients use Envoy. Envoy Mobile now provides native Swift/Kotlin APIs that call through to Envoy directly (rather than using Envoy as a proxy), which apps use to create and interact with network streams. This release includes a variety of new functionality: HTTP request and streaming support gRPC streaming support through a built-in codec Automatic retries using Envoy’s retry policies Programmatic, typed configuration for launching the Envoy network library Like Envoy Proxy, Envoy Mobile takes a “streaming-first” strategy and is designed to work seamlessly with persistent streams while providing convenient traditional APIs. This forward-thinking approach ensures flexibility within the library and a path to unlock new streaming protocols like QUIC in the future. Another fundamental goal of the library is cross-platform consistency, and the APIs provided by Envoy Mobile are 1-to-1 across iOS and Android. Each of these APIs is detailed on the documentation site , and there are demos available in Kotlin, Swift, Java, and Objective-C. This version of Envoy Mobile marks the first release of the library that Lyft has integrated into our apps. Envoy Mobile is currently being tested in pre-release versions of the Lyft rider app on both iOS and Android for all HTTP traffic. The first Lyft ride using Envoy Mobile was taken by the team a few weeks ago to purchase celebratory donuts. Since then, thousands of employee rides have been completed using the early version of the library! We’ve already started work on the v0.3 milestone! This is planned to be the first production-ready version of Envoy Mobile. In v0.3, we are building observability/stats integration, as well as L7 filter support both in the C++ core and natively in Swift/Kotlin . These filters will unlock the full power of Envoy’s filter chain, delivering on the promise of network consistency between platforms by centralizing previously duplicated logic from server-side filters and across iOS/Android apps in a single C++ filter leveraged by all platforms. (For example, imagine performing an OAuth handshake entirely through Envoy filters on both the server and mobile clients!) You can see the full list of features for our v0.3 milestone on GitHub here . As we continue to battle test the library at Lyft, we’d love to have other partners join us. Members of the core Envoy Mobile team will be speaking at several conferences over the coming weeks, and would be happy to meet up and chat. You can contact us here or contribute to the project by opening an issue or pull request. Stories from Lyft Engineering. 164 iOS Envoy Android Envoy Proxy Mobile 164 claps 164 Written by Stories from Lyft Engineering. Written by Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-11-05"},
{"website": "Lyft-Engineering", "title": "detecting stop signs and traffic signals deep learning at lyft mapping", "author": ["Deeksha Goyal"], "link": "https://eng.lyft.com/detecting-stop-signs-and-traffic-signals-deep-learning-at-lyft-mapping-75bac609c231", "abstract": "Data Data Science Engineering Mobile Product Security By Deeksha Goyal , Albert Yuen , Han Kim , and James Murphy Last fall, I was an intern on Lyft’s mapping team, where I worked on map making. I designed and productionized a deep-learning based algorithm that predicts where Traffic Control Elements (TCE), such as stop signs and traffic lights, are in the road network using purely anonymized driver telemetry data, such as speed and GPS traces. This work paved the way for us to infer traffic control elements and update our map in real-time. This project culminated in a paper ¹ that I recently presented at KDD’s 8th International Workshop on Urban Computing in 2019 held in Anchorage. Today, I am back full-time on our growing map-making team and working on similar exciting problems to create a hyper-accurate map from real-time data. In the same topic, my coworker Albert Yuen recently wrote a post ² on how we detect map errors using anonymized telemetry data. For Lyft, having high accuracy and coverage of traffic control elements in our internal map is valuable for multiple reasons: a) More accurate route ETAs : We can add a time penalty to go from one road segment to the next if there is a traffic control element. b) Dispatch : We can improve driver position prediction, which can help improve market decisions. c) Autonomous vehicles : With more TCEs, our autonomous driving team, L5, can plan the behavior of vehicles on the road more efficiently and reliably. We noticed that as drivers approach stop signs and traffic signals, they have different speed patterns. As the typical driver approaches a stop sign, they slow down and stop their car, verify that the traffic is clear and free from pedestrians crossing, and then resume driving. Figure 1 shows an example of the change in speeds as a driver approaches a stop sign: The driver starts with a non-zero initial speed. As the driver approaches the stop sign, their velocity decreases until it reaches around zero velocity in front of the stop sign. Then, the driver increases their velocity. Figure 2 shows how this fares in reality on anonymized driver speed readings. We found two main main driver patterns for traffic signals: When a driver first sees a red light, they slow down and stop in front of the traffic light. When the green light is activated, the driver accelerates again. This behavior is very similar to the stop sign case, as shown by the red stroke in the diagram on the right of Fig. 3. We may nevertheless expect that the stopping time in front of the traffic signal may be longer than the stop sign case. When the driver only sees a green light, the driver usually keeps near constant speed, as shown by the green stroke in the diagram on the right of Fig. 3. Figure 4 shows how this fares in reality on anonymized driver speed readings. This case encompasses many different types, including other road signs and intersections. However, most of the time, the drivers are simply not encountering signs. We therefore expect the velocity of the drivers to remain near-constant, as shown in Figure 5. Figure 6 shows how this fares in reality on anonymized driver speed readings. Inferring traffic control elements with telemetry data has been explored in the past by using ruled-based methods in a set of manually selected candidate locations, or by using more traditional statistical learning approaches, both supervised (random forest, Gaussian Mixture Models, SVM, Naive Bayes) and unsupervised (spectral clustering). However, these methods require extensive feature engineering which may not be straightforward because of noisy or sparse sensor data, e.g., the number of the times the vehicle is stopped and the final stop duration. They also do not necessarily generalize well across multiple different roads, cities, and regions due to differences in speeds, traffic light configurations, and dependence on knowledge about the road network. Instead, we used a computer vision approach that picks up on basic driver patterns that we can reasonably expect from any stop sign or traffic signal. This approach generalizes well for many other types of map features and across regions and is well equipped for updating Lyft’s map regularly based on real-time location traces. We used Lyft’s proprietary telemetry location data to look for different speed behaviors near each intersection type. By using the City of San Francisco’s open dataset for stop signs and traffic signals as ground truth, we were able to label bounding boxes around each intersection in SF. We collect GPS traces within each of these bounding boxes and then train a convolutional neural network (CNN) to pick up on the different patterns we expect from each traffic control element. We create bounding boxes over the end of each road segment, the intersection, and a little bit after the junction. In order to do this, we use the road network provided by Open Street Map (OSM). We filter out bounding boxes for junctions that have more than four segments as those cases are rare (around 0.01% in San Francisco). We also filter out bounding boxes for junctions that are too close to each other, so that we do not have any bounding boxes cover more than one junction. Even after removing these cases, we retain a coverage of junctions that is over 90%. We then label the bounding boxes using the City of San Francisco’s open datasets. They are labeled by checking which traffic control element in the dataset is closest to the junction in the bounding box. If there are none, then we label the bounding box as not having any traffic control elements (neither). For each bounding box, we collected driver telemetry data inside of them from 40 days in the summer of 2018 for San Francisco. These days are deliberately chosen to be different days of the week to ensure that we are not overfitting for traffic patterns on certain days. We then place each of these data points into the correct bounding boxes by aligning the bearing of the telemetry data and the bearing of the bounding box. This prevents collecting driver data in the opposite direction of traffic flow. It is likely that some data points would display low GPS accuracy because of tall buildings in San Francisco as well as low smartphone quality. Thus, we made sure to only collect data points with high GPS accuracy. The confidence of the signal comes with the phone’s telemetry readings and is calculated based on standard deviation of the noise of the GPS location. For each bounding box, we create a diagram over speed and distance from junction by applying a 2D Kernel Density Estimator (KDE) on the data with a Gaussian kernel function. The bandwidth of the KDE is determined by Silverman’s rule of thumb. At lower speeds, we are likely to see more location data points than at higher speeds because of the sampling rate. This leads to a noticeable amount of driver data points at speed zero at all distances from the junction. These points are not indicative of any driver pattern and add noise. In order to mitigate the effect of this noise, we normalize the diagram with a cube root and min/max normalization. These normalization steps moreover help with surfacing the driver patterns we are searching for. For the stop sign image, you can see a clear decrease and increase in speed as the driver approaches the intersection, which is at around 40 m in the diagram. At a traffic signal, you see the same behavior as at the stop sign (a ‘V’ shape), but you also see a straight line which indicates drivers going straight through on a green light. For the neither histogram, there isn’t much change in speed by the driver. We trained on VGG19 pretrained on ImageNet to classify the kernel density estimator images. We initialize with the weights from VGG19 pretrained on the ImageNet dataset and do not freeze any layers. We add an additional fully connected layer to the end of the network that has randomly initialized weights and outputs three scores. After hyperparameter tuning, our best model for the three classes has a validation accuracy of 91.26% in San Francisco. The validation and training accuracies in Figure 9 tend to stay around the same, suggesting that there is little overfitting. The learning rate (0.0001) that we landed on after hyperparameter tuning gives a steady decrease in loss (Figure 9). In order to evaluate how general the classifier is, we test the classifier in Palo Alto. Palo Alto is a good candidate because it is not as urban as San Francisco, so one might expect a drop in performance of the classifier. Moreover, in our dataset, San Francisco’s traffic control elements comprise around 40% stop signs, 20% traffic signals, and 40% neither. Palo Alto, on the other hand, is comprised of around 15% stop signs, 8% traffic signals, and 77% neither. We created bounding boxes around each intersection in Palo Alto and then used the afore-mentioned SF trained classifier on this data. These predictions are then compared against manual curation by human experts. With a confidence threshold of 90%, we are able to have a total accuracy of 96.654%. Overall, we have found that driver telemetry patterns are reliable and generalizable indicators of traffic control element types and that CNNs are effective in deciphering these patterns. We plan on extending this work to include other types of traffic control elements, combining this framework with other strategies for detecting road signage, and using this telemetry-based strategy for detecting other types of road network information. - Thank you Albert Yuen, Han Kim, Ferhan Elvanoglu, James Murphy, Karina Goot and many others on mapping for guiding me through this project and supporting me! Also, thank you to Alex Kazakova and her team for their help with data curation. Finally, thank you to the wonderful folks at Flyte. Learn more about Flyte, Lyft’s at-scale workflow infrastructure, and how you can use it here . I enjoyed working on this project and am excited to continue working on map-making. Deeksha interned at Lyft in Fall of 2018 and recently joined back full time after getting her bachelors and masters degrees in Computer Science from Stanford University. As always, Lyft is hiring! If you’re passionate about developing state-of-the-art quantitative models or building the infrastructure that powers them, read more about our Science and Engineering roles and reach out to us . [1] Traffic Control Elements Inference using Telemetry Data and Convolutional Neural Networks, Deeksha Goyal, Albert Yuen, Han Suk Kim, James Murphy, KDD UrbComp Workshop 2019, 2019. link [2] How Lyft Creates Hyper-Accurate Maps from Open-Source Maps and Real-Time Data, Albert Yuen. link Stories from Lyft Engineering. 505 2 Thanks to Albert Yuen and Ryan Lane . Machine Learning Deep Learning Computer Vision Data Science Engineering 505 claps 505 2 Written by Engineer at Lyft, formerly at @Stanford @Google @LightspeedVP Stories from Lyft Engineering. Written by Engineer at Lyft, formerly at @Stanford @Google @LightspeedVP Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-11-05"},
{"website": "Lyft-Engineering", "title": "securing apache airflow ui with dag level access", "author": ["Tao Feng"], "link": "https://eng.lyft.com/securing-apache-airflow-ui-with-dag-level-access-a7bc649a2821", "abstract": "Data Data Science Engineering Mobile Product Security By Tao Feng and Andrew Stahlman Apache Airflow is a workflow orchestration management system which allows users to programmatically author, schedule, and monitor data pipelines. Since its adoption at Lyft, Airflow has become one of the most important pieces of infrastructure at Lyft which serves various use cases: from powering executive dashboards to metrics aggregation, to computing derived data generation, etc. We shared our experiences running Airflow at Lyft in our last post . There are scenarios where users require fine-grained access control in Airflow. For example, some users don’t want their high priority workflows (directed acyclic graph or DAG in Airflow) accidentally paused by others; some users don’t want their DAG files with sensitive data to be seen by other users etc. Apache Airflow introduced a role-based access control ( RBAC ) feature in 1.10. This feature built the foundation to improve Airflow UI security. However, the RBAC feature comes with its own limitations as it only supports five static role types. Airflow’s DAG level access feature was introduced in Airflow 1.10.2 with additional enhancement in 1.10.3. In this post, we will discuss the implementation of DAG-level access control on how it extends RBAC to support access control at a DAG level. Then we will talk about how Lyft adopted the feature to handle visibility around highly sensitive data. Lastly, we will discuss potential future improvements for this feature. In this section, we will discuss how Airflow UI security works. For more details on Airflow internals, please refer to these other posts . Prior to 1.10, Airflow was built upon the flask-admin framework, which did not provide any access control functionality. In 1.10, Airflow switched over to Flask-Appbuilder (FAB), which provided the necessary security features to support RBAC. Before going into detail on RBAC, it is helpful to discuss the internal of FAB. FAB is web-based framework built on top of Flask, including security modeling, auto CRUD generation, and integration with different authentication mechanisms. It has a built-in security manager which is instantiated by the app to handle security operations. Under FAB, each user could have multiple roles, with each role granting certain permissions on a set of views and menus. The view menu is the core concept in FAB which is a Flask blueprint created automatically by FAB. Additionally, the view allows setting fine-grained view class level security that can be associated with the user role. Please take a look at the FAB doc for additional details. Airflow 1.10 ships with a set of roles by default: Admin, User, Op, Viewer, and Public. Only Admin users can configure or alter the permissions for other roles. Admin: users who have all the possible permissions, including granting or revoking permissions from other roles. Viewer : users who have read access to DAGs, but cannot modify the state of Airflow metastore. User: users who have DAG ownership with read and write access. These users can clear failed tasks, trigger new DAG runs, and mark tasks as success / failure etc. Op: Airflow Devops users who can modify Airflow configuration. Public: non authenticated users. The RBAC feature from Airflow 1.10 builds the foundation for a more secure Airflow UI access environment. But RBAC granularity is still too coarse, as it can only limit the users to all the DAGs or none of the DAGs. It cannot limit users to view only a subset of DAGs. Ideally, we would like for certain DAGs to be accessed only by certain roles. In the next section, we will talk about how we designed DAG level access control, which achieves the finer granularity access control that meets our requirements. Each user can be assigned with multiple roles, each role can be associated with a set of permissions on views. In the 1.10.0 Airflow RBAC model, each user could only have access to all the DAGs or none of the DAGs. To extend the existing RBAC model to support DAG level access, we introduced two new permissions( can_dag_edit , can_dag_read ) for each DAG, and a logical view( all_dags ) to represent all the DAGs. There are two methods to create a new role : by using the Airflow command line (e.g airflow roles — create role1 ) by the admin user through the Airflow webserver security tab There are two methods to assign a DAG permission to a given role: The DAG view permission(e.g c an_dag_read_$DAG for read access, can_dag_edit_$DAG for write access) can be assigned to a role through the security tab by the Admin. User can declare the read or write permission inside the DAG file as shown below: Internally, FAB provides a built-in security manager (SecurityManager) to support the security management. We built a custom Airflow security manager, subclassing the FAB’s built-in security manager in order to support DAG level permission access. The Airflow security manager will initialize and create the read / write security permission for each DAG whenever the airflow webserver is bootstrapped or the DAG Bag is updated. It also allows the possibility to build a custom security manager for different companies to support their own secure policy. The following documents the security manager flow: Create the two permissions(read, write) for every DAG in here . Initialize all the predefined roles permissions in here . For example, assign all_dags view with read and write permission to the User, Op, Admin role Initialize the necessary permissions for all the DAG roles in here . The reason is because all the DAG roles still need to access other UI views like different menu tabs. Clean up the outdated permissions from the FAB tables. For each URL routing, it will be added a new DAG level access decorator named has_dag_access . The has_dag_access decorator conducts the following checks: If the role of the user has can_dag_edit write permission on all the DAGs( all_dags view), the URL is accessible for the user. If the role has the write permission over the DAG, the role has both read and write permissions. If the role has can_dag_edit permission on the given DAG, the URL is accessible for the user. If the role has can_dag_read permission and the URL only requires read permission, the URL is accessible for the user. Access is denied for the given user if none of the above is true. DAG level access control has become a real need at Lyft in the last couple of years. For example, datasets on HR Financial data are highly sensitive and restricted to only specific Lyft employees. Our existing Airflow cluster could not accommodate these sensitive workloads. In early 2019, we built out a dedicated cluster leveraging DAG level access control to support these secure use cases. The DAG level access feature is a natural fit to support the secure multi-tenant use case: Each team maintains their own own source-code repository which contains their team’s DAG definitions. Common utilities and Airflow operators are centralized in a “standard library” for ETL. Each team has the option to maintain their own fleet of Airflow workers and have full control over their IAM permissions and Confidant credentials. Alternatively, they can be deployed to a shared fleet which is maintained by our team. Each team at Lyft has their own RBAC role, which is defined in source control and automatically provisioned via “ airflow roles — create”. Each team has their own Airflow Celery workers ASG. To onboard, each team needs to declare their DAG repo inside our central DAG infrastructure devops repo, which manages the deployment of our Airflow infrastructure. In order to streamline setting roles for users, each team also needs to define a role for their team members inside a shared but source controlled yaml file. Once the file is defined and updated, the role will get created and assigned to the given users when the Airflow infrastructure is deployed. Then the onboarding team members can start writing ETL. To ensure the team’s DAGs can only be accessible by the onboarding team members, access control with the specified role needs to be defined in each of the DAG files. By default, each user is assigned the public role, which couldn’t access any DAGs. Some DAGs require access control while some DAGs don’t. One way is to create a default DAG role and advise users to set those non sensitive DAG’s default access control on it. In the future, we could improve this process to be more smooth. Currently this new cluster has been running for about 6 months. And we are planning to move those existing users from the existing legacy cluster to this multi tenant Airflow cluster. In this post, we shared the internals of Airflow DAG level access feature and how Lyft leverages it to support high confidentiality use cases. In the future, we plan to: Improve the feature to be production-ready. Migrate the existing DAGs from the monolith cluster to this new multi tenant cluster. Integrate the feature with Lyft internal ACL service which will map the FAB role to internal ACL service role. Improve the onboarding experience especially on how to support the user group. A huge thanks to the data processing experience team who helped in supporting and improving Airflow infrastructure at Lyft. Thanks to Junda Yang for helping out this multi tenant Airflow effort at Lyft. Thanks to Maxime Beacuemin and Joy Gao and the community for providing all the gracious and thoughtful review feedback on the initial design and implementation . Thanks to Prashant Kommireddi , Sean Shi and Mark Grover for reviewing this post. Last but not least, thanks to the rest of the Apache Airflow open source community for all the support. Please join the OSS slack channel to get involved in the community. As always, Lyft is hiring! If you’re passionate about developing state of the art machine learning/optimization models or building the infrastructure that powers them, read more about them on our blog and join our team . Stories from Lyft Engineering. 186 4 Thanks to Mark Grover . Airflow Data Etl Orchestration Security 186 claps 186 4 Written by Apache Airflow PMC and Committer | Co-creator @Amundsen Stories from Lyft Engineering. Written by Apache Airflow PMC and Committer | Co-creator @Amundsen Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-10-22"},
{"website": "Lyft-Engineering", "title": "how lyft designs the machine learning software engineering interview", "author": ["Hao Yi Ong"], "link": "https://eng.lyft.com/how-lyft-designs-the-machine-learning-software-engineering-interview-bbbb9fc8bb28", "abstract": "Data Data Science Engineering Mobile Product Security Lyft’s mission is to improve people’s lives with the world’s best transportation and it’ll be a slow slog to get there with dispatchers manually matching riders with drivers. We need automated decision making, and we need to scale it in a way that optimizes both the user experience and the market efficiency. Complementing our Science roles, an engineer with a knack for practical machine learning and an eye for business impact can help independently build and productionize models that power product experiences that make for an enjoyable commute. A year and a half ago when we began scouting for this type of machine learning-savvy engineer —something we now call the machine learning Software Engineer (ML SWE) — it wasn’t something we knew much about. We looked at other companies’ equivalent roles but they weren’t exactly contextualized to Lyft’s business setting. This need motivated an entirely new role that we set up and started hiring for. Most companies are open about the expectations for the role being interviewed for, the interview process, and preparation tips. Most have recruiters give verbal tips, some provide written guides, and fancy ones hire entire studios to produce feature films. One thing you probably won’t find, however, are notes about how they came up with them. Admittedly, we’re no exception. As a candidate, it’s easy to get a foot in the door and be evaluated by an interviewer. As an interviewer, it’s easy to get thrown in a room and be asked to evaluate a candidate. What’s not easy is for an organization to design and scale an interview process while ensuring its consistency and reliability for the candidates, the interviewers, and the hiring managers making the hard choices. In a break from convention, this post lays out the motivations for our interview design and a set of principles that guide our iterative approach to it. To illustrate our approach, we focus on the ML SWE “interview loop” and dive deeper into how we applied these principles to the modeling onsite interview. We address the key components to our interview loop design in each of the following sections: Defining problems, Harmonizing scale, Uncovering talent, and Tracking progress. Before diving into the actual design principles for an interview loop, we need to understand the motivation for the loop. The motivation comes from what we want out of the role, which in turn helps us define what we should look for in a candidate. The following are three questions that we need to address: What are Lyft’s challenges (and can a specific role help)? What should the role be with respect to the organization’s goals? What are the desired skills, knowledge, and talents given the expectations for the role? We’ve motivated Lyft’s challenges introduced the role of the ML SWE above. What’s left is to define the necessary ingredients for what a successful hire looks like vis-a-vis (3) in Lyft’s context: Skills acquired through practice, Knowledge learned through study and personal experience, and Talents that make each candidate unique. The desired skills and knowledge are simple to define and test for; e.g., skills needed to prototype a model and knowledge of ML theory, concepts, and libraries. You can gain it from study, practice, and experience. We won’t belabor it here but let’s be more specific about what we mean by the talents care about. Our desired talents are recurring patterns of thought, feeling, and behavior that can be productively applied in the context of Lyft’s ML SWE role. What we’re looking for here is a bit more complicated than simply work done in the past by a candidate. Faced with the same stimuli, people react and behave differently. When we look for role and values fit, we do mean just that. Beyond skills and knowledge, will a candidate’s unique way of responding to the problems thrown up in Lyft’s business context help that candidate succeed? So while conventional wisdom might suggest it, we’re not always looking for the Michael Jordans of machine learning (be it I. or J.). The narrow sort of talents associated with celebrated excellence can be important but in most cases the interviewers are listening for predictive clues of how a candidate will react when posed Lyft-specific problems on the job. (We found Gallup’s First, Break All the Rules useful in explaining distinctions between skills, knowledge, and talents through their data-driven approach.) Since Lyft’s business context changes (sometimes rapidly when going public), our desired talents change and so, too, must our interviews. In yesteryear’s context, we may value a candidate’s comfort and excitement with ambiguous business problems because there are plenty of low-hanging fruits to pick. Tomorrow, we may value the clarity of mind in organized problem solving more because as our product matures the biggest business opportunities lie in ensuring smooth interactions between product features. Iterating on the interviews is an important part of recognizing change and ensuring that the role stays relevant. The section title is thus an extended pun on the different types of definition problems we face in our interview design: Lyft’s high-level problems that stem from its business context, The perceived problems that an ML SWE role should readily tackle, and The practical problems that candidates respond to in an interview to help interviewers learn about them. Once we can comfortably address these problems, we think about scaling the interview. In particular, how can different interviewers and hiring managers consistently and reliably assess role and level fit across diverse candidates? Most of this post focuses on what goes on between the interviewer and the candidate. That said, there are a couple of layers of the interview process that we need to peel away to set context for how we actually design the problems and evaluations that help us look for talent. At a high level, a good interview design harmonizes all of these components. First, candidates on the ML SWE loop go through Lyft’s hiring review. The review is a regularly scheduled session for a committee to study candidates with an unbiased perspective and decide whether to hire them. Working alongside the review committee is a separate panel of interviewers that provides technical feedback. This feedback is designed to help the committee decide if there’s a fit and, if so, the candidate’s technical level. At first glance, this review process may seem cumbersome. Examining the checks and balances more carefully, however, we notice that they are intentionally introduced to put friction on the hiring process. Having a consistent review committee unifies standards and eliminates bias. Second, the loop consists of a technical phone screen followed by a series of onsite interviews in no particular order. The phone screen determines if the candidate’s working knowledge of fundamental ML concepts and basic coding skills would allow them to succeed on site. The onsites are a set of interviews to further test the depth of the candidate’s skills and knowledge in practical job requirements for the role. Additionally, the onsites also provide opportunities for candidates to reveal their talents when faced with various technical challenges. These two layers of the interview process are dynamic. When I joined Lyft, the hiring review didn’t exist and we simply had debriefs conducted with the interview panel and the hiring manager. It wasn’t efficient. When we first rolled out the ML SWE loop, we simply tacked on a couple of modeling interviews to the standard SWE loop. As our understanding of the ML SWE’s role at Lyft evolved, we rapidly learned that the loop needed to be better contextualized to the role. We introduced a committee to revamp the loop. And being part of the committee requires grasping how these pieces intertwined with the questions that interviewers ask and candidates answer. Pairing a candidate with an interviewer lets us test for the candidate’s skills, knowledge, and talent. Skills are taught and mastered. We even offer internal “mastery courses” for machine learning. Knowledge is picked up through experience and personal learnings. Both are important and part of the bar and leveling criteria we set for any hire. To decide if there is a fit , however, simply validating a candidate’s skills and knowledge is insufficient. We need to know if her recurring patterns of thought, feeling, or behavior match the role. As much as we hate to admit it, interviews that last barely an hour are blunt tools for teasing out the talents that are relevant to the role. It’s tricky to design questions and ask them in a way that reveals consistent behavior across multiple interviews and interviewers. It’s even trickier if you consider the confounding effect of having practiced for such interviews: is it practiced interview behavior or natural behavior? At Lyft, we regularly discover problems that no one has ever solved before. As much as we appreciate the time and effort put into preparing for the interview, interview skills aren’t equivalent to on-the-job performance. In scientific parlance, interviews have low statistical power and a high sampling cost. Nevertheless, here are some design principles we found useful in sharpening our tools. To illustrate, we discuss how these principles apply to the design of the ML modeling onsite interview. The goal of the interview is to predict how candidates naturally perform when placed in Lyft’s business context. To that end, our interviewers design and draw from a pool of diverse problems amenable to a variety of potential solutions. This way, candidates aren’t forced to study any specific topic to piece together a good response. Further, it reduces the likelihood that candidates can practice for them. In the context of the modeling onsite, we ask open-ended problems with sufficient business and problem context such that the candidate can clearly identify an ML-based approach to solve it. After all, we make no attempt to hide the fact that we’re hiring an ML SWE. A key ingredient to a good interview is therefore introducing the implicit problem constraints by way of context early without biasing the candidate to any one right answer. Interviewers polish their problems and calibrate them against trusted peers before using them in formal interviews. In the modeling interview, we want to test how the candidate turns a vague business problem described in prose into a well-defined ML-amenable problem; e.g., regression or classification. The interviewer’s focus is on the candidate’s spontaneous response. So while the interviewers provide business context and clarifications, they deflect and stress on the candidate driving the conversation. What’s important is how the candidate interprets and approaches the business problem. This way of deferring to the candidate to drive the conversation applies to design questions and experience interviews, too. Of course, the candidate’s response is confounded with experience. If the candidate isn’t comfortable with driving the conversation even when pushed, the interviewer offers guidance while noting what was offered. Again, there’s a fine line to draw between giving ample context without giving too much away. Part of designing the loop involves creating an interviewer onboarding process that aligns the interviewers on the specifics we should look out for. When we design the interview loop, we strive to be scientific without forgetting that a lot of it is an art that the interviewers need to learn and practice. To sufficiently evaluate candidates, the modeling problem must be ambiguous enough that there can be multiple problem definitions and different good ML approaches. In fact, the candidate should be inspired by the ambiguity to get creative. It is the interviewer’s job to assess whether a creative approach is valid or simply “too creative” and try to steer the candidate in a relevant direction. For instance, I once had to push a candidate to try a more obvious heuristics-backed ML approach for the immediate problem instead of formulating a generalized version of that problem as, say, a non-convex constrained optimization problem. The gravitation towards a more generalized, complicated solution is part of the talent we want to learn about but it’s also important to stay on track so that the talents are assessed in the right context. One way to ensure that the discussion stays on track is to know when to provide validation and steer the conversation. Even when we have an open-ended challenge, interviewers have to let the candidate know when to dive deeper and when to move on. Decomposing the broader problem into big, rough chunks of subproblems allows candidates to respond on their own terms while staying within the bounds of what we want to assess. In practical terms, we’re simulating various stages of working with an ML model at Lyft. We may want to think about problem formulation at the prototyping phase. We may also want to think about model evaluation at the productionization phase. These are broad enough categories that candidates can offer clues to their recurring thought patterns while making sure all interviewers adhere to a consistent set of standards around skills, knowledge, and talents to uncover. Beyond mirroring real-world conditions, the interview is a good avenue to provide a feel of the types of problems we tackle at Lyft and establish brand. With that in mind, our questions are generally Lyft-inspired problems that candidates won’t find elsewhere. For consistency, each question is peer-reviewed before being published to an internal questions repository. Interviewers conduct informal interviews with colleagues and get them to sit in actual interviews as “shadowers” for feedback. The overarching goal is to maximize the discerning power of our interviews and minimize their predictive variance. Of course, interviewers will have objective and subjective feedback. It’s important that team members interview a candidate and think about whether the candidate and the broader Lyft organization match each other. The challenge is that our candidates are diverse and it’s impossible to maintain an exhaustive interviewer’s checklist of how they can fit in at Lyft. We rely on our interviewers, their training, and our internal calibration to help discern that. Once you have the key components of a loop, it’s crucial to maintain a solid feedback loop to measure the health of our interview funnel over time. As difficult as it is to obtain enough samples for statistical metrics, we strive to use data to decide if something is unhealthy. For instance, suppose that the healthy pass through rates were 50% for the phone to onsite and that that for onsite to offer was 25%. If the pass through rates from the phone to onsite were at 70% and our onsite to offer was 10%, there’s something very broken in our phone interview process. In this case, we look closer at the phone interviews, confer with the interviewers, and examine anomalous cases to establish broader trends. Any subsequent changes made are monitored pretty intensely to ensure that our rates stabilize. Loops for different roles and even the same loop at different times will yield slightly different baselines. For instance, a pipeline with fresh grads from a recruiting event probably has higher offer acceptance rates than one with senior candidates we’re sourcing. The recruiting team is represented on the loop design committee to offer guidance on what are reasonable numbers. Once the loop is “live,” the recruiting team regularly pulls these numbers and call out potential trends. Beyond the interview funnel, it may be of interest to track longer-term trends in the hired candidates’ performance. There are good reasons to be cagey about the interview design process. What if the information helps candidates game the interview? What if the things we’re looking for in our interview change over time? What if the design process is actually bad? Worse, what if folks realize that their LeetCode premium subscriptions won’t help for the ML SWE loop? Despite the what-ifs, being transparent about how we design interviews can improve our interviews. Call it enlightened self-interest: candidates invest time to talk to us and we mutually benefit from learning if there is a good fit. Even if there isn’t an immediate fit, positive experiences build brand and improves candidate sourcing. Maybe the candidate can reapply when the timing is better. Practically, hiring an engineer easily costs tens of thousands of dollars. By showing how we iterate on our interviews, we reveal what we truly care about and how we try to probe at them, hopefully adding to the virtuous cycle for the hiring pipeline. This post focuses on the ML SWE loop because I’m familiar with it. I’ve conducted over a hundred variants of it with candidates of all backgrounds since the inception of the loop. Having worked with the committee that defined the role and the recruiting team to introduce the loop, I’ve had a hand in designing and tweaking the interview goals, formats, and question banks for the set of specific skills, knowledge, and talents we want. More recently I motivated and onboarded more team members onto the loop to help expand our pool of interviewers. The other interview loops I’m on and the various interviews I’ve had with other companies are also good references. Ultimately, we want good candidates that match our role and values to succeed, and we wanted them yesterday. If you enjoyed this post, follow and recommend! For prior expositions on how we conduct interviews and how to succeed at them at Lyft, check out the following. Interviewing with Lyft Engineering What to expect when interviewing as a Product Manager at Lyft For a taste of machine learning problems we tackle at Lyft, be sure to check out the following. Fingerprinting fraudulent behavior Empowering personalized marketing with machine learning As always, Lyft is hiring! If you’re passionate about developing state of the art machine learning models or architecting the platforms and infrastructure that powers them, read more about our roles and reach out to me ! The content here is distilled from the hard work by the Lyft ML committee, the recruiting team, and our interviewers and their interviewees. This post would not have been possible without Karthik Subramaniam , Kendall Gruyere , Davide Crapis , Matt Green , and Mark Grover polishing it into what you see here. Stories from Lyft Engineering. 717 Thanks to Mark Grover and Matt Green . Hiring Machine Learning Interview Engineering Data Science 717 claps 717 Written by Research Scientist at Lyft Stories from Lyft Engineering. Written by Research Scientist at Lyft Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-10-22"},
{"website": "Lyft-Engineering", "title": "operating apache kafka clusters 24 7 without a global ops team", "author": ["Andrey Falko"], "link": "https://eng.lyft.com/operating-apache-kafka-clusters-24-7-without-a-global-ops-team-417813a5ce70", "abstract": "Data Data Science Engineering Mobile Product Security Earlier this year, the Streaming PubSub team at Lyft got multiple Apache Kafka clusters ready to take on load that required 24/7 support. The team’s operational burden for Kafka quickly started heading towards burn-out territory. On-call rotations started getting miserable because we’d get woken up at night due to failing hosts. Business requirements kept coming and requiring us to scale the clusters further. The more we scaled, the more we’d get woken up. To get our work life balance back and help the business expand, we took a month to create an automatic remediation system. In this post, we hope to share what we built and what we learned. As a Reliability Software Engineer at Lyft, I’m responsible for minimizing latency and data loss for all events sent from our mobiles and web services. To achieve this, we’ve been deploying our own Kafka systems for the past year. Adding Kafka to our ecosystem gives us major benefits: Ability to flush data from our services and sidecars fast enough to prevent data loss when those services die or get restarted. Ability to use Kafka’s acknowledgement feature to cleanly retry. Kafka’s throttling system gives us the ability to back-pressure producers smoother than than having to deal with API Limit errors. At the same time that we have to deliver on high reliability requirements, we also have to run with as much cost efficiency as possible. We do not have a global 24/7 operations team that can respond to hardware or software faults. We don’t run with 4x replication as a way to delay replacing hosts when they fail during business hours either. To keep costs low, we consciously built our clusters on d2.xlarge instances. We also made a decision to use the smallest available d2 instances. Smaller instances allow us to right size our clusters and also guarantee higher durability guarantees . The drawback of d2.xlarge instances is that they have higher failure rates due to us running a larger fleet of them. At our starting scale, we were seeing 3–4 failures per week. The failure rate grew linearly as we grew our clusters. The failures would also happen inconveniently at the dead of night. Human error occasionally was made at those late hours and it became very stressful — in fact, impossible — to keep our 99.99% availability target on track. We didn’t want to hire a larger team to respond to these failures. These were three types of failures that we would see. All of them had clear remediation actions: The failures that we wanted to automatically repair appear fairly cut-and-dry. If you can detect them, it isn’t rocket science to act on them in an automated way. Our first plan of action was to evaluate if solutions or frameworks already existed that would do most of the heavy lifting for us. We were looking for services that could be run with a small footprint, be reliable, and integrate with services we use, such as Pagerduty and AWS EC2. We looked at the following solutions: We were on a tight time-budget for this project, so building a solution that required us to stand up multiple components was out of the question. Every component that you stand up adds cost and increases your operational burden. We felt that the remediation system needed to be as stateless as possible and at least as reliable as the Kafka systems that we were targeting. We chose to create our own little service to get the job done. Here is a picture of service’s architecture: Note that despite all the boxes, version one of the service runs in a single JVM. We only need to run one copy of it at our current scale. As mentioned above, we’re expecting to see 3–4 failures per week, which means that the service will not need to be scale horizontally for some time. We optimized for getting the service out to production vs. trying to make it web-scale. The first thing that we’ll need to scale likely is the detectors part of the service. The entry point to our system are detectors. Detectors watch a resource for when it fails. Our initial inclination was to trigger remediations from either Pagerduty or the system that sends alarms to Pagerduty. We hit the following challenges with this approach: Modifications to the alarm system was risky. If we were to make a mistake, we risked not sending normal alarms to Pagerduty. Building a proof-of-concept with detection done this way thus required extra time that we didn’t have. If you intercept an alarm, you need to figure out how and when to page a human. The last thing we wanted to do was page a human and trigger a remediation at the same. The could cause confusion and in the worst case a human might start doing remediation while the machine is already doing it. Conversely, if we made a coding error and didn’t page the human, then we’d effectively be running blind. Our metrics pipeline has higher latency and lower reliability than our Kafka systems. For example, for a Kafka metric to make it out Wavefront , we first need our agent on the machine to send it to an aggregating sidecar, which then sends it to a proxy. It generally takes 1–2 minutes for a metric to fully make it to Wavefront. We then would need to poll it and determine what to do with those metrics, which could take another minute to two in order to exclude false trends or check other metrics to ensure we don’t start remediating things during a planned rolling restart. If Wavefront or networking is not working well with our metrics pipeline, then we lose the ability to remediate. A final problem was security: we’d have to go through the effort of provisioning tokens with the right access and managing access to them. What we chose to do instead is to have remediators take advantage of Apache Zookeeper ZNodes . Luckily for us, Kafka also uses Zookeeper ZNodes. Thanks to the Curator Framework , we created watches and wrote a little bit of logic to re-register them as nodes would fall in and out of the respective clusters. For the more difficult — but rarer — case where we suffer a partial network partition, we are planning to work on an agent that creates a separate set of ZNodes for us to watch. It is better explained with the following diagram: Note that we only need the second set of ZNodes to handle cases where network is only partially down. In our experience on AWS, we see our instances’ Kafka port stop receiving traffic, while the instance is still able to keep a healthy connection to Zookeeper alive. Kafka doesn’t take such a node out of the cluster. As explained in more detail in KAFKA-8702 , partitions that are leaders on the broker having this failure get under-replicated. When the afflicted broker is restarted, we suffer a period during which our partitions are in offline state due to the fact that we run with ISR=2 with three replicas. To avoid taking a hit on availability, we need to detect brokers that fail in this way as soon as possible and pull them out of the Zookeeper quorum. When our detectors detect a fault, they create an ailment object. This ailment object is persisted to a Kafka topic. We went through the trouble of doing this for the following reasons: If remediator crashes or gets restarted mid-way through doing work, it’ll come back and resume from where it left off. Kafka offsets are committed after remediation is triggered. This will be explained in more detail later. By writing to Kafka, we are essentially creating an immutable and ordered log of ailments. Ordering is important for remediation because we want to pick up remediations in the order in which they were detected, especially if ailments are being detected for the same object. The predictability of an ordered work queue alone allows us to worry about fewer things with writing our remedies. Scaling our detectors is made easier because detectors on run on separate machines and write to the same Kafka topic. Latency is very low from when an ailment is sent to when we are able to poll for it and start working on it. The only disadvantage of using Kafka as a persistence layer for us is that we have a circular dependency: Kafka needs to be alive for us to remediate Kafka. We mitigate this by running multiple Kafka clusters. Most of the clusters that remediator works on are not where remediator’s Kafka topic resides on. We also have service protections such as quotas turned to prevent another use case from bringing down the cluster that remediator is using and vice versa. Remedy coordinator consumes ailments from Kafka, determine what type it is and farms it out to respective remedy workers and validators. Remedy coordinator and is a reactive consumer that treats each ailment type as a state. Examples of ailment states are: “DOWN”, “WAITING_REBOOT”, “MISSING”, and “WAITING_PROVISIONING”. Most hosts will start with an ailment detected of “DOWN”. The remedy worker that processes hosts that are down, will take a look at the host’s system status and test networking. If it is indeed down, it will try to restart the host or kick off a provisioning task if the host doesn’t exist anymore. After the task is kicked off, we kick off another ailment and persist it to our Kafka topic. Our remedies are generally split into an interface that has two functions: “invokeOperation” and “awaitOperation”. The intention is that any remediation operation should be followed up by waiting for it to successfully complete. The distinction is important for two major reasons: We want to commit our offset back to Kafka as soon as we’ve completed an operation. This minimizes the probability that remediator will crash mid-way through and cause a remedy to be re-invoked when it comes back up. We want to minimize running a remediation invocation for the same resource. Most of the remediations that we invoke, ideally should be validated somehow. The validation can take some time. For example, a reboot might take a few minutes. If reboot doesn’t succeed, then it might be time to either cause another reboot invocation or re-provisioning to be attempted. Most crucial though, waiting separately allows us to send metrics out to avoid alarming humans that our cluster is degraded. The final piece of remediator is our integration with our metrics and alarming systems. Every remedy outputs a “timeRemainingMins” metric that provides a count-down to operators that might be looking our dashboards. The countdown is displayed on our under replicated partition time series chart. In theory, it can be plugged into any other chart. Each remediation validation, as a general guideline, has a maximum timeout. In the example above our remediation is allowed to take eight minutes. If it exceeds eight minutes or fails to fire altogether, we allow our alarm to fire and page a human. The advantage of doing it this way is that we only needed to add a second condition to our existing alarm. Essentially, our alarms look like this: whatever-your-previous-under-replicated-partitions-alarm-was and if(exists(ts(“kafkaremediator.brokerId.*.cluster.<cluster name>.serviceMetrics.remediationProgressMinsRemain.statistic.value.gauge.value”)), all(<m, default(1h, 0, ts(“kafkaremediator.brokerId.*.cluster.<cluster name>.serviceMetrics.remediationProgressMinsRemain.statistic.value.gauge.value”)) = 0), 1) Note that the above is a Wavefront query, which happens to be the metrics and alarming system we currently use . In English, we are checking for presence of the “remediationProgressMinsRemain” metric. If the metric exists and so long as it is non-zero, then we don’t alarm. We tie it in our our previous under-replicated partitions alarm so that it stops firing when remediations are in progress. Before unleashing remediator on our clusters, we found the need to add a couple of kill-switches. The first was to add a DRY_RUN configuration flag. This allows us to do a quick re-deploy of remediation to force it to run in a mode where it still detects problems, but doesn’t execute the tasks. We bump the offset, but don’t do any operation invocation or validations. We view remediator’s logs to see what it would have attempted to do. The second kill switch we added is one that checks Zookeeper for presence of nodes that indicate if a cluster rolling restart is in progress or not. Our cluster restart automation writes state to Zookeeper to help it coordinate an orderly restart. This will be worthy of a future blog post. Remediator checks for presence of this state and will not queue any ailments until the restart is complete. In the case of unsuccessful restarts, we don’t remove state from Zookeeper and have a human clean the state up after investigating what went wrong. Remediation is essentially turned off in that case. We have a separate alarm that pages us if a restart hasn’t completed within thirty minutes or so. Due to time constraints, the remediator is currently only targeting Kafka, so as a result the full architecture isn’t fully implemented or proven for other services. If and when we are able to build the fully generalized solution, we’ll gladly open source the framework including some of the detectors and remedies. Some things we’ve done or plan to do make this easier to generalize: The integration with the monitoring system will cut off at metrics emission. We chose to use micrometer to allow other metrics emissions systems to be used. We plan to use Kafka partition keys to order ailments on a per-cluster or per-resource level. This will allow us to run more remedies concurrently without messing up queue ordering. We’ve added some basic unit-test level checks to avoid cyclical remediations, but plan to add additional guard-rails for when cyclical remediations are warranted. Right now, remediator is written in Java and requires that all remediations and watchers are written in Java. We hope to allow some parts, which as remedies to be written in other languages. We’re always looking for ways to improve, so if you see any holes in our solution or have a better system to recommend, we’d love to hear from you. Thank you to my teammates who make up the Streaming PubSub team: Jason Carey, Ying Xu, Dev Tagare, Yash Kumaraswamy, Kailash Dayanand, and Packy White. Thank you to Alex Berghage, Daniel Hochman, Ian Shape, Istvan Marko, and Chandni Jain for helping out with the initial research and letting me bounce ideas off you. Thank you Ravi Sharma for building a strong reliability team and giving us space and encouragement to experiment and push the envelope. Interested in working in an environment like this? Lyft is hiring! Apply through our application system , email data-recruiting@lyft.com , or drop me a note at afalko@lyft.com . Stories from Lyft Engineering. 271 1 Thanks to Mark Grover . Kafka Remediation Automation Stateful Applications Engineering 271 claps 271 1 Written by Stories from Lyft Engineering. Written by Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-09-25"},
{"website": "Lyft-Engineering", "title": "sensor data in localization wi fi", "author": ["Yunjie Zhao"], "link": "https://eng.lyft.com/sensor-data-in-localization-wi-fi-329ea84db959", "abstract": "Data Data Science Engineering Mobile Product Security By Yunjie Zhao , James Murphy , Yuanyuan Malek , Kyle Madsen , Joe LaBarbera , Chris Woo Sitting at my desk at Lyft HQ which is located right next to the Mission Creek Channel, I pulled out my phone and opened a mapping app, only to see an odd yet unsurprising view — the blue GPS dot had me placed in the water (Figure 1). While GPS (Global Positioning System) works well under a clear sky, the accuracy varies in places like urban canyons, stacked roads, tunnels, or anywhere indoors. Lyft’s mission is to improve people’s lives with the world’s best transportation. Inaccurate location signal can lead to a cascade of undesirable experiences, which adds friction to achieving that mission. For example, a rider may rely on their smartphone to set their pickup spot. When it fails to reflect their actual location, the pickup experience degrades and causes frustration for both the rider and driver in the form of elevated pickup time, increased rider-to-car walking distance, rate of driver-rider phone calls, and elevated cancellation rates. On the driver side, incorrect positioning may stimulate a chain of unfavorable reactions too, causing suboptimal rider matching, low utilization and earning, and convoluted navigation. The list goes on for bikes, scooters and any other location-aware components on Lyft’s platform. Location! Location! Location! To address these challenges, Lyft’s Mapping team has been working on models and algorithms that utilize sensor data collected by any off-the-shelf smartphone. This sensor data includes bluetooth, accelerometers, gyroscopes, barometers, etc. In this post, we will dive into how Wi-Fi can help the location process and improve the in-app experience for our drivers and riders. Picture a typical Lyft experience: a rider in downtown New York City orders a mocha latte at the coffee shop on the street corner, expecting to call a Lyft to catch their morning meeting. As they open their app, the blue dot on the screen is a few blocks away from their actual location (note: riders often assume Lyft knows where they are). The rider can either drag the pickup pin manually to the correct location (assuming they are familiar with the area), or turn back to the barista and ask for an exact address, all while juggling the latte in the other hand. In the urban environment, the Global Positioning System (GPS) or Global Navigation Satellite System (GNSS) is constrained by many factors, being particularly affected by the multipath effect (see Figure. 2 for an illustration). The multipath effect occurs when GNSS signal is reflected off an object, like the wall of a building, causing the signal to travel slightly longer than it would otherwise to reach the receiver directly (i.e., triangle inequality). This extra travel time in radio propagation, although very small, causes the device to calculate an incorrect position. Of course, the impact of this noisy data leads to a negative experience for the rider trying to arrive at their destination on time and with ease. The fact that the rider was in a particular coffee shop should actually help provide some context on their position. That is exactly the type of information Wi-Fi can bring to the location process. From Wikipedia , “Wi-Fi is a family of radio technologies commonly used for wireless local area networking (WLAN) of devices”. Radio-based indoor positioning is not a new idea [Ref 1–4]. Quite a few researchers have demonstrated its value in indoor positioning via either model-based or fingerprint-based methods in the past two decades. One major constraint, however, is that most of these methods require previously-constructed radio maps or exhaustive site surveys, making them impractical for Lyft’s use cases. A site survey, for example, normally consists of two steps: First, the researchers map out the indoor floor plan of the place they are interested in localizing. Then, the surveyors sweep the floor by marking their positions and the corresponding “Wi-Fi fingerprints”, and ingest them into a database. At the inference time (i.e. when the prediction is made), the query scans this database, finds the most relevant Wi-Fi fingerprints, and predicts the location. Unfortunately, the survey effort is so time-consuming and labor-intensive that it is not viable to scale up to an entire city building by building. To address these limitations, the Mapping team has been working on a solution with minimal curation needs. At a high level, the algorithm is composed of a Wi-Fi map learning phase and an inference phase. The focus of the learning phase is to build up belief in Wi-Fi’s location, with the requirement that it has to work with little manual curation in the loop. At inference time, we can leverage the learned Wi-Fi map to answer a query like “ where is rider X?” , conditioning on the Favorite Coffee Shop WiFi signal being observed . Like road networks and map elements, Wi-Fi is one of many layers on Lyft’s path to model the physical world in an accurate, fresh, and rich manner. It represents the aggregated belief of Wi-Fi access point locations extracted from past observations. Depending on the specifics of the applications and use cases, we have defined two types of Wi-Fi maps, namely Point Estimate Map and Probability Map . Wi-Fi Point Estimate model the location of the physical Wi-Fi router. Like the input data in many machine learning problems, the raw Wi-Fi observations are distorted by noise along the data collection process. Just to name a few, the sources of noise include but are not limited to: Heterogeneity in both the sender and receiver (i.e., a spectrum of phone and Wi-Fi router manufacturers, hardwares, range, power level and other technique specifications). Unsynchronized timeline for Wi-Fi scans and GPS readings. Complex urban environment and moving objects, obstacles and barriers that interfere with the radio propagation. Inaccurate positioning (especially in the urban areas that we care about). In addition to handling noisy readings, we have to address a few other technical challenges ranging from scalability and sparsity to lack of ground truth. For example, say there are millions of WiFi access points in a city, and each access point has thousands of observations. How do we extract the useful signals from the billions of noisy observations? How do we evaluate the model’s performance, considering that we don’t have access to the true location of the access point itself? Although the Point Estimate is conceptually straightforward to reason about, it does not directly answer the original question we started with at the beginning of the post (where is the rider, given their device is scanning a specific Wi-Fi signal?). We could make assumptions on signal propagation and power loss as a function of distance, derive the distance, and apply trilateration/multilateration. However, this approach makes strong assumptions on the radio propagation process, and is still quite complex to compute since it requires fitting a few parameters per access point. Instead, we went back to the drawing board and reconsidered the fundamentals. What matters is not the exact location of the Wi-Fi access point, it is merely a means to an end. More importantly, we care about the device location, provided the extra information that a certain Wi-Fi access point is in the proximity. Therefore, we can apply Bayes’ theorem and derive the posterior probability. p : probability x : device location y : scan WiFi access point The unknown in the equation is the likelihood p (y|x) . Circling back to the original example, the likelihood describes the probability of a device scanning Favorite Coffee Shop Wi-Fi, conditioned on a specific rider location. There are a variety of methods on the shelf to fit this probability, and one of them is the Gaussian process . The Gaussian process is a good fit for the problem because it is reasonable to assume continuity in probability on the two-dimensional surface and the similarity between points in the proximity (i.e. kernel function). Plus, it not only predicts a point estimate, but also returns the uncertainty information “for free”. Following the steps above, we gathered all the ingredients for the learning process and effectively generated a Wi-Fi map. Leveraging its knowledge, we may start to answer some interesting queries at inference time. We can use this data to add context to the rider position, utilizing more information than latitude and longitude alone. We refer to this problem as “rider location”, and it can take the form of places of interest, pickup stands at the airport, entrances at the shopping mall, stretches of road segment/curb info, etc. Figure 4 and Figure 5 demonstrate examples of how Wi-Fi may facilitate the pickup spot selection process in urban environments and at the airport, where: Blue dot and shaded circle represent the GPS location and uncertainty. WiFi icons are the learned locations for the access points scanned by the device. Pink pins are the predicted pickup spots. Since adopting the Wi-Fi-derived features in the model, we’ve seen 7 percentage points of improvement in rider location accuracy, and an average of 6 percentage points in airport pickup accuracy across most major airports where Lyft operates. Moving forward, we are actively working on adding additional sensor data to the location technology stack. Special thanks to everyone in Lyft’s Mapping team, especially Karina Goot , for editing the post. This work would not be possible without their feedback, collaboration, and support. Yunjie Zhao is a Research Scientist working on the Location Accuracy Team in the Mapping Org at Lyft. Over the past year, he has worked on prototyping and developing applications of sensor data within Mapping. As always, Lyft is hiring! If you’re passionate about developing state-of-the-art quantitative models or building the infrastructure that powers them, read more about our Science and Engineering roles and reach out to us . Bahl, P., Padmanabhan, V.N., 2000. RADAR: An in-building RF-based user location and tracking system. In Proc. IEEE INFOCOM. Yang, C. and Shao, H.R., 2015. WiFi-based indoor positioning. IEEE Communications Magazine, 53(3), pp.150–157. Mok, E. and Retscher, G., 2007. Location determination using WiFi fingerprinting versus WiFi trilateration. Journal of Location Based Services, 1(2), pp.145–159. Nandugudi, A., Maiti, A., Ki, T., Bulut, F., Demirbas, M., Kosar, T., Qiao, C., Ko, S.Y. and Challen, G., 2013, November. Phonelab: A large programmable smartphone testbed. In Proceedings of First International Workshop on Sensing and Big Data Mining (pp. 1–6). ACM. Stories from Lyft Engineering. 791 1 Thanks to Michael Rebello . Wifi Gps Lyft Engineering Data Science 791 claps 791 1 Written by Research Scientist @ Lyft Stories from Lyft Engineering. Written by Research Scientist @ Lyft Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-11-12"},
{"website": "Lyft-Engineering", "title": "making long term forecasts at lyft", "author": ["Hamed Hasheminia"], "link": "https://eng.lyft.com/making-long-term-forecasts-at-lyft-fac475b3ba52", "abstract": "Data Data Science Engineering Mobile Product Security Authors: Hamed Hasheminia, Arman Jabbari, and Varun Madduri At Lyft, like many other companies, we need to make accurate short and long-term forecasts. Some of the metrics that we need to accurately predict are number of driver hours provided by drivers in different regions — i.e our supply side of the business — and also number of rides taken by riders in different regions, i.e. our demand. We have several internal tools that we use to make forecasts. One of the tools that we use to make long-term forecasts of up to 52 weeks is a cohort based model. In this blog, we are going to briefly explain how our cohort based model works. We call specific users who completed their first ride or provide their first driving hour in a specific region and in a specific week, a cohort. For instance, all the passengers who used their Lyft app for the first time in the 2nd week of 2018 are considered in a cohort. It is not hard to assume that there are differences among different cohorts. As an example, the sub-population of a college town who start using the app in September are different from an East Coast city where a subset of the population start using the app in a specific week of a cold winter. The goal of our cohort based model is to: Forecast the behavior of each observed cohort and use it to project how many rides are taken or driver hours are provided within a specific cohort Forecast the behavior of the cohorts that are yet to be seen. Aggregate all the projected rides and driver hours to make forecasts for both the demand and supply side of our business. To build up the simplest model for each cohort, let’s assume the users of each cohort are homogeneous. In other words, let’s assume each user who starts using our app at a specific region, r , and a specific time T, on average takes R(r,T) rides a week. A(r,T) is the number of users who belong to this cohort. y(0,r,T) = A(r,T)R(r,T) is the total number of rides that all riders take at the inception week of their cohort. The churned (or inactive) users are the category of users who do not take any ride in a specific week. At the inception of the cohort we do not have any churned user. This is simply because we would not have counted a user in a specific cohort if they had not had their first ride in the first week. Additionally, we assume that through time, active users churn our platform at the rate of λ per week and churned users are resurrected (i.e resume taking ride on the app) at the rate of μ per week. The churned users who stay in their churned state will not take any rides. We also assume that active users will take an average of R(r,T) rides per week if they remain in the active user state. Without loss of generality, let’s assume R(r,T)=1. Now, let’s freeze the time at t periods after inception of a cohort. By assuming R(r,T)=1 , at time t , the total number of active users and hence the total number of rides are Yt . Active users leave their active-user-state at the rate of λ. At time t , the total number of churned users are Y0-Yt which will resurrect at the rate of μ. The total number of active users at Δ t period after t can be calculated as: It is relatively straightforward to show that We call the above equation, the retention curve . Where: Since both the churn rate and resurrection rate are positive, therefore: Here are some of the conclusions of this set-up: 1.When the resurrection rate is zero, μ=0 and the churn rate λ is positive, steady state is zero. Therefore, at the steady state, when the system reaches its equilibrium, we expect the total number of rides made by the users of the specific cohort to eventually reach zero. 2.When the churn rate and resurrection rate are the same, i.e. λ= μ , at the steady state, the number of rides is half of the initial number of rides, Y0. 3.When the synergy of the system is high, i.e. λ+ μ is high, the system reaches the steady state faster than the case that the synergy of the system is low. Here is the relationship between the resurrection rate and the parameters of the retention curve: If we decide to share information — i.e. use any statistics from an old cohort (users who used their app for the first time at time T , and in region r ) to a young cohort (users who used their app for the first time at time T’>>T and in region r’ ), our data should be on the same scale. One of the ways to scale the data is dividing driver hours and rides by number of activations at each cohort— i.e total number of users in each cohort. It is important to consider the difference between activation and active users in a cohort. We call the total number of users in a cohort (active or inactive), total number of activations, whereas in the above setting, the group who are using our app at a specific time are called active users and the ones stopped using our app are called inactive users. Deciding how to scale the data turned out to be an important design decision. It helped us to generate or impute observations when necessary from some cohorts to others. To evaluate the robustness of the exponential function, we generated simulated data with a normal noise. Each time a new dataset is generated, the exponential function was refitted on that dataset. Figure 2, summarizes the results of the robustness evaluation on the exponential function. The exponential function is not only robust to noises, but also fits our data very fast and efficiently when a reasonable initial point is provided. The contours of the exponential function fitting problem are depicted in Figure 3. As it is shown in Figure 3-a, Residual Sum of Squares (RSS) in regime B is less than RSS regime A . Figure 3-b is zoomed on area A , and Figure 3-c is zoomed on area B. Figure 3-b shows minimizing RSS is not convex as there are several local optimal points. Figure 3-c shows that having a roughly good initial point can result in finding the global optimal solution efficiently. To check how well our model can be fitted on real data, we first needed to deseasonalize our data using the coefficients estimated in this article . We were hopeful that by adding cohort level (or any combination of region and tenure of the cohort) indicator (or dummy) variables to the exponential functions and trying a fixed effect type model, we will be able to estimate a model that defines all retention curves simultaneously. This practice resulted in unsatisfactory prediction errors. We noticed that if we solely focus on cohort observations that are older than a few weeks, the exponential retention curve fits almost perfectly on the cohort observations. It is appeared that in the first several weeks, the actual retention curves deviate substantially from the exponential functional form. We soon realized the difference between the two is mainly due to heterogeneity of the riders/drivers in the very early weeks of using our app. For example, there are portion of riders or drivers who decide to leave our app after an initial promotional offer. Or as another example, imagine a tourist who visits San Francisco and installs our app and uses it for a week or two when she is in San Francisco. This rider is completely different form a San Franciscan who installed our app at the same time as the previous tourist did. In order to enjoy using the properties of exponential functions and at the same time capture the swift change of the retention curve in the first few weeks, we used a Spline type model. We decided that instead of identifying the retention curve solely with one function, define them by two different functional forms and seamlessly connect them at a knot. The model that we used, guarantees continuity to two order of derivatives at the knot. As will be illustrated, once two functions are joined at a knot with continuity of two orders of derivatives, it will not be possible for a human being to visually discern the location of the knot. Beside the cosmetic effect of joining two functions using different levels of continuity, such a practice reduces degrees of freedom of the model. Below is the functional form we use to model retention curves: At first glance, it may look like we are adding a lot of parameters. However due to imposing continuity assumptions on the functions, first order derivative of the functions, and also the second order derivatives of the functions, we are only adding two degrees of freedom to the original exponential function. In other words, the following constraints must be satisfied while estimating the parameters of the model: Although fitting the Spline type model is harder than the exponential model, they show very similar properties. We designed a two-level optimization method for finding the optimal parameters. As it is shown in Figure 5, in the first step, a gradient based method is used to find the optimal solution. Then the cohorts whose fitted parameters are drastically different from all other cohorts are flagged as outliers. In the second step, based on the information provided by the fitted parameters in the first step, a better initial point as well as lower and upper bounds on the parameters are estimated. At the second step, the gradient based method is used again to find the optimal parameters of the flagged cohorts with respect to the added guardrails. Ensuring continuity at the knot of the Spline functions limits us with the choices that we have in dealing with heterogeneity. Only very fine-tuned treatment of both functions of the Spline can guarantee continuity at the knot. To benefit from the Spline functions and also capture heterogeneity that exists among cohorts, and at the same time, being able to decompose the problem into smaller subproblems and getting benefits from parallel computing, we decided to develop our own new methodology. The new method focuses on estimating retention curve of each cohort separately. The model consists of 3 hyper-parameters that are shared among all regions and all cohorts. Later, we are going to discuss this model in detail. The parameter estimation of each cohort-level Spline is very fast. In our new model, we estimate the parameters of every cohort and then project them into the future. For instance, the following cohort curve is estimated using the Spline type model explained above. It turns out our decision to move away from fixed-effect type models and focusing on cohort level estimations added significant level of complexity to our estimation methodology. It is easier to explain some of the complexities associated with this type of modeling using an example. Imagine that you are interested in estimating the parameters of a cohort curve that was initiated just 2 weeks ago. How can we use two data points to estimate 5 parameters? Below we explain how we solve this problem. We first categorized the cohorts into three buckets: Mature: cohorts that have enough observations that we can reliably estimate their retention curves with their own observed data. The estimated parameters of these cohorts are reliable enough that, if needed, can be used to generate auxiliary data. Young: cohorts that have number of observations, yet, not enough observations to reliably trust their estimated parameters. For very young cohorts — young cohorts younger than a certain age — we increase the accuracy of the predictions significantly by adding auxiliary (or imputed) observations coming from mature cohorts. We add such observation to these very young cohorts and estimate the parameters of their retention curves by fitting the retention curve on their real and auxiliary data. This is actually what roughly happens when we deal with fixed effects models and a class has only a few observations. In that case, all the parameters of such a model are estimated by other classes, and observations in this class only has an infinitesimally small effect on the estimates of the main parameters. They only affect their own class-specific parameter(s). Unseen: cohorts with no observations. The first question that needs to be answered is how many observations are enough to categorize a cohort into the mature category? This is the first hyper-parameter in our model. In the following table, the O s represent the observed values and the X s represent unobserved values. Red X s represent generated observations from mature cohorts. The cut-off point between very young cohorts — i.e. cohorts that are too young and can benefit from auxiliary observations — and the rest of the young cohorts, cohorts that do not benefit from generated observations, is another tuning parameter in our model. We decided to treat such observations, i.e. Xs in red, as missing values and decided to impute the values of each. Number of different imputation methods exist in the literature, the most promising being Multiple Imputation methods, where the user imputes multiple values for each case to effectively generate m different datasets and then run machine learning models on each dataset and finally pool the results. We noticed that, this method can drastically increase the computation time, so much so, that we may lose the ability to effectively run long-term backtests in a reasonable time. Due to the computational load of this method and the fact that we had over 1000 of such cohorts, we decided to use the Stochastic Regression imputation method. Basically, in this method we use the estimated value of the parameters of mature cohorts and add an error term to the forecasted values to impute for missing values. For instance, to impute the value of one observation that is marked as red in a 12-week-old cohort, we estimate the parameters of average values of recent mature cohorts and use it to estimate the value of missing value for the 12-week-old cohort. We finally add a random error to such generated observations. By imputing the missing values of young cohorts, not only do we use the actual observations of a young cohort but we also carry information from the more mature cohorts to them. This was one of the primary reasons for why we initially decided to focus on rides per activation or driver hour per activation as our retention curves. For instance, if we have an unusually high number of activations in a week, we can still use number of rides per activations as the metric for a cohort that has lower number of activations. Now, we need to tune three important hyper-parameters: The first hyper-parameter is the cut-off point between mature and young cohorts. This is used to generate observations for very young cohorts. So, the higher this number, the higher the bias and the lower the variance. This hyper-parameter is labeled as “maturity” in the following figure. The second hyper-parameter is the cut-off point for the very young cohorts category that we generate data for. For example, the model may decide to only generate data for young cohorts that are younger than 20 weeks. The higher the cut-off point, the more the bias and the less the variance. This hyper-parameter is marked as “extension” in Figure 8. The last hyper-parameter is the error we add to the generated values. Usually, in the imputation literature, the error rate that is considered is the same rate as the error of the regression model. However, this practice may not be suitable for our modeling purposes due to the fact that despite the similarity of our model to the imputation literature, our framework has its own characteristics. Again, tuning the amount of error balances bias and variance. Adding low level of error to the estimated values will result in similar shapes, i.e. lower variance and higher bias. Finally, we needed to estimate the shape of the unseen cohorts. To do so, we considered average shape of the young cohorts and project that for unseen cohorts. For the activation level of unseen cohorts we used a simple time-series model. The only case remained to be discussed is the case of regions with no mature cohorts. To help imputing missing values for such regions, we developed a separate machine learning model that can identify the similarity of the shapes of cohorts in different regions. We simply pick the most similar region and use the mature cohorts of that region to impute observations for young cohorts of the new region. Now we can add the values of cohorts and make long-run forecasts. In this post, we discussed one of the forecasting methods that we use at Lyft. The cohort based model is used to make long-term forecasts of up to 52 weeks and is used in our planning. We shared the Spline type model that we use to estimate retention curves and explained how we separate cohorts into different categories and discussed about the methods that we use to estimate retention curves of each type of cohort. Our method is tuned by 3 hyper-parameters. After hyper-parameter tuning of our proposed model we estimate every single retention curve and make long-term forecasts in few minutes. Beside being used in our long-term forecasting, the retention curves are used in LTV estimation. As always, Lyft is hiring! If you’re passionate about developing state of the art machine learning models or building the infrastructure that powers them, read more about our Research Science and Engineering roles and reach out to us! Stories from Lyft Engineering. 770 Thanks to Ryan Lane , Matt Green , and Mark Grover . Machine Learning Long Term Forecast Cohort Based Models Data Science Data 770 claps 770 Written by Stories from Lyft Engineering. Written by Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-10-04"},
{"website": "Lyft-Engineering", "title": "how lyft creates hyper accurate maps from open source maps and real time data", "author": ["Albert Yuen"], "link": "https://eng.lyft.com/how-lyft-creates-hyper-accurate-maps-from-open-source-maps-and-real-time-data-8dcf9abdd46a", "abstract": "Data Data Science Engineering Mobile Product Security By Albert Yuen, James Murphy, Sumanth Ravipati, Deeksha Goyal, Han Kim, Adithya Hemakumar, Milo Han, Alex Ung, Clare Corthell At Lyft, our novel driver localization algorithm detects map errors to create a hyper-accurate map from OpenStreetMap (OSM) and real-time data. We have fixed thousands of map errors in OSM in bustling urban areas. Later in the post, we share a sample of the detected map errors in Minneapolis with the OSM Community to improve the quality of the map. Lyft’s mission to build the world’s best transportation relies on its inherent geospatial capabilities. For example, driver and passenger geolocations must be precisely known in order to efficiently pair drivers and passengers. We also need precise knowledge of the road network to compute efficient routes and accurate estimated time of arrival from current driver position to pick-up point, and from pick-up point to drop-off point. Moreover, meticulous understanding of the road network is crucial to correctly compute the distance travelled by the drivers. These technical challenges require a team with a strong geospatial expertise. Lyft’s mapping team provides a rich, fresh, and accurate model of the physical world, and how our users move around within it. We enable: Generating optimal and infer probable routes of drivers to passengers Making accurate time and distance prediction Localizing drivers, passengers and vehicles Building a knowledge base of physical places Inferring map features Our internal map of the road network is based on OSM, which has been built and improved over the years by the open source community. More recently, larger organizations (such as Apple, Microsoft, Mapbox, Mapillary, Facebook, Uber, Grab, Lyft, etc.)¹ have also worked to improve the map. Akin to Wikipedia as an open-source encyclopedia, OSM as an open-source map may contain missing or erroneous data for several possible reasons. Old roads may have never been mapped, new roads may not have been mapped yet, previously closed roads may be reopened, roads may be digitally vandalized, buildings may be non-existent, turn restrictions may be erroneous, road directions may be incorrectly labeled, and so forth. As OSM is a source for our basemap, we need to monitor its quality and accuracy. Upon detecting map errors in OSM, we work with our Data Curation Team to fix them in OSM. This can be done using our proprietary data. Before discussing map error detection, it is necessary to have an understanding of what map-matching is. At Lyft, we geo-localize drivers from the sensors embedded in their smartphones. This includes a GPS receiver that receives sparse (due to battery constraints) and often noisy (due to urban canyons ) locations. If we do not have any understanding of the road network, we can only employ a free-space tracking algorithm such as a Kalman Filter, as shown in Fig. 1. Drivers would therefore not be localized on the road network. However, OSM provides us knowledge of the road network. Taking both a sequence of sparse and noisy GPS traces and a map of the road network as input, map matching algorithms can infer the most accurate trajectory on the road network, as shown in Fig. 2. An example of a map-matching algorithm is the one based on Hidden Markov Models (HMM) developed by Newson and Krumm². The quality of the map is essential for accurate map-matching. Not all map features in OSM are useful for Lyft. While hiking trails in OSM delight my outdoorsy self, this is not a crucial map feature for the Lyft app. The most important and fundamental map features for Lyft are those that characterize the building blocks of the road network: the existence of road segments, the directions of road segments, and the existence of turn restrictions. At Lyft, we distinguish two types of road network errors: The road network errors that trigger map-matching issues and routing issues, denoted Type I map errors The road network errors that mostly trigger routing issues, denoted Type II map errors Because Type I map errors are the only ones that trigger map-matching issues, we can detect them by finding where driver localization is failing on the road network. Figure 3 shows a case where map-matching fails to reconstruct the correct trajectory of the driver. Because Type II map errors are the only ones that triggers routing issues without triggering localization issues, we can detect them by finding where on the road network routing is failing while localization is not failing. In the following two examples, the dotted line is a road segment that does not exist in the physical world, and yet, this road is mapped in OSM. Assuming the GPS locations are not too sparse and not too noisy, the extra road segment in OSM does not cause map-matching failures, as displayed in Fig. 4. However, the extra road segment causes shortest/fastest path calculations to be wrong, resulting to routing issues, as displayed in Fig. 5. The map errors related to the existence of the road segments, the direction of road segments, and the existence of turn restrictions can be categorized using this framework, as displayed in the following Tables 1, 2 and 3 for road existence, road direction and turn restriction. Road existence: Road directions: Turn restrictions: When the map is wrong, Kalman Filters perform better than traditional HMM map-matching algorithm. However, the map is often right. We developed an algorithm based on a semi-interacting multiple model (sIMM) in which an off-road Kalman Filter is run in parallel to an on-road HMM map-matching algorithm. When the map is correct, our algorithm uses the output of the HMM, while the Kalman Filter is preferred when the map is wrong, as explained by our paper here³. Note that a particle filter could easily substitute the HMM in this framework. At Lyft, the output of the Kalman Filter — the off-road locations — are used to detect Type I map errors, which encompasses missing roads, roads in OSM that are set to the wrong one-way direction, and turn restrictions that should not have been mapped in OSM. (In some sense, those are the features that over-constrains our routing graph). Figure 6 shows an example of how our sIMM filter employing an off-road Kalman Filter and an on-road HMM operates. There is a missing road at the middle, and a driver travels through that missing road, as shown by the GPS traces. The correct section of the map (represented by the red path on the left) is used by the HMM to compute the initial trajectory of the driver. In the portion of the map where the road is missing (represented by the green path), our system detects that the map is wrong and switches to the off-road Kalman Filter mode. Eventually, as the map is correct again, the algorithm switches back to the on-road HMM mode, as shown by the red line on the right. The output of the sIMM filter when the map is wrong can be used to detect Type I map errors. By leveraging weeks of anonymized Lyft’s driver locations when they are connected to the Lyft app, and making a large-scale plot of off-road trajectories, we can highlight areas where we observe Type I map errors. At Lyft, we found that most of the Type I map errors are due to missing roads (although we have observed a few incorrectly labelled road directions and turn restrictions that trigger Type I map errors). We used the python package datashader and our internal visualization tool to render the off-road trajectories. We made raster tiles of those off-road trajectories in order to speed up the loading of the detected map errors from the off-road trajectories. Here, raster tiles are more appropriate than vector tiles. Loading vector tiles would be similar to recomputing each off-road trajectory poly-line for display as we scroll around the map while loading raster tiles does not require recomputing, at the expense of less interactivity — it is difficult to add metadata on raster tiles. Figure 7 showcases off-road locations in yellow-red around the MSP airport in Minneapolis. An analysis of an (outdated) satellite imagery shows that roads were probably recently built and that are now navigable by cars. OSM is not up to date yet as those roads are not mapped yet. Figure 8 showcases off-road locations in a parking lot in Minneapolis. The parking lot building is mapped in OSM. However, because the aisles of the parking lot are not mapped, routing within the parking lot is not possible, which triggered the off-road locations. Figure 9 showcases off-road locations near the Northtown Mall in Minneapolis. Accurately mapped roads at venues such as malls are crucial to Lyft as their absence prevents smooth pick-up experiences. We have nevertheless observed that our Type I map error detector does not perform well on wide roads, when the GPS accuracy is poor, or when drivers do not observe the road network. This is because this would activate the off-road mode even though the map is correct. Wide roads are mapped as a single line with no thickness in OSM, even though there is a road width tag, which is often unfilled. This is the main reason why our sIMM algorithm does not perform well in the case of wide roads. GPS accuracy is particularly bad in urban canyons when high density of tall buildings corrupt GPS readings due to multi-path or occlusion. Even when the road network is correct, if, for example, a driver decides to ignore a turn restriction, the algorithm will generate off-road locations. Those off-road locations unfortunately falsely indicate that the map is wrong. Using our map error detector, we have fixed and contributed thousands of critical Type I map errors in OpenStreetMap, hoping that it will be beneficial for the OSM community. Furthermore, in order to get more feedback from the community and see if a larger release would be useful, we are releasing a sample of our detected Type I map errors in Minneapolis in MapRoulette (see Fig. 10). Check them out here ! At Lyft, mapmaking and map quality assessment is central to our business. Our burgeoning map-making team is tackling the routing-based Type II map errors as well as other types of map inferences, such as traffic control elements (check out Deeksha Goyal’s work during her internship at Lyft⁴). Stay tuned for more exciting mapping blog posts! If you enjoyed this post, follow and recommend! To learn more, check out our other Science posts . As always, Lyft is hiring! If you’re passionate about developing state-of-the-art quantitative models or building the infrastructure that powers them, read more about our Science and Engineering roles and reach out to us . We would like to thank the entire mapping organization for their contribution/support to this work, Alex Kazakova and Spencer Jaquish for leading the Data Curation Team, as well as the copy editors Ryan Lane, Julien Silland, Andrew Hao and Mark Grover for their feedback. Many thanks! [1] Corporate Editors in the Evolving Landscape of OpenStreetMap , Jennings Anderson et al., ISPRS Int. J. Geo.-Inf., 2019. link [2] Hidden Markov Map Matching Through Noise and Sparseness , Paul Newson and John Krumm, In Proceedings of the 17th ACM SIGSPATIAL, 2009. link [3] Map matching when the map is wrong: Efficient on/off road vehicle tracking and map learning , James Murphy, Yuanyuan Pao, Albert Yuen, arXiv, 2018. link [4] Traffic Control Elements Inference using Telemetry Data and Convolutional Neural Networks, Deeksha Goyal, Albert Yuen, Han Suk Kim, James Murphy, KDD UrbComp Workshop 2019, 2019. link Stories from Lyft Engineering. 1.6K 5 Maps Mapping Engineering AI Data Science 1.6K claps 1.6K 5 Written by Research Scientist in Mapping at Lyft Stories from Lyft Engineering. Written by Research Scientist in Mapping at Lyft Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-09-17"},
{"website": "Lyft-Engineering", "title": "how to solve a linear optimization problem on incentive allocation", "author": ["Shujing Wang"], "link": "https://eng.lyft.com/how-to-solve-a-linear-optimization-problem-on-incentive-allocation-5a8fb5d04db1", "abstract": "Data Data Science Engineering Mobile Product Security By David Shmoys and Shujing Wang At Lyft, scientists solve all kinds of optimization problems. While solvers can come in handy, there are times when complicated or large-scale problems can be simplified and solved with customized and efficient algorithms. In this post, we will show how to use linear programming duality to transform and solve an incentive allocation problem. Incentive allocation is a common theme at Lyft. For example, we send riders promotional coupons and send drivers bonuses, in the hope of optimizing objective functions such as maximizing incremental rides or driving hours. In this context, the main constraints are (1) a total budget constraint on how much we can spend on the incentives, and sometimes (2) a requirement that each rider or driver can be only given at most one incentive. Such allocation problems can be easily formulated as integer programming problems, which can be further relaxed to be linear. Although such problems have simple structures, their problem size can easily grow large (e.g. there can be many incentives, or a large population of riders). So we wonder: is there a fast and efficient way to solve this problem that can out-perform open-source solvers? The answer is yes. In this particular use case, we develop an algorithmic idea based on duality. Problem Formulation For simplicity, let us consider a scenario of giving out coupons to riders. We define the following: We can then formulate the problem with a linear programming relaxation (P) as follows, where the integrality constraint of the variables is omitted. We know that we are going to get at most one variable with a fractional solution (nice property of LP) in this relaxation, which is acceptable in exchange for the efficiency of solving this problem: Here, we have preprocessed the data so that we may assume Coupons eliminated in the preprocessing step are dominated by at least one coupon in the feasible set. In other words, the cost, value and efficiency of feasible coupons for each given rider are in increasing order. We leave the mathematical proof to interested readers (the idea is that if this relationship is violated, there is always a feasible perturbation in the optimal solution for which you can make the solution even better, leading to a contradiction). From the primal to the dual To solve the linear programming relaxation, we will first solve its linear programming dual . We define the following dual variables: Then the dual (D) is as follows: Let D( λ ) denote the dual linear program (D) for a fixed value of λ ≥ 0 . In considering this LP, note that the optimization problems for each rider i = 1, . . . , m, is an independent (trivial) problem: we simply set and hence the optimal value to D( λ ) is to solve (D), we simply need to find λ that minimizes this expression (E). Solving the dual LP We show how the compact form of the dual given as finding λ to minimize (E) provides a direct way to solve this LP. For now, focus on one rider i and the corresponding term in (E). We observe the following: Therefore, we want to create a list of breakpoints for rider i by checking each of the intersections of of the k ᵢ lines, starting with the intersection of j =1 and j =2. The key point here is that we only keep breakpoints that are monotonically decreasing. After this process, we may assume without loss of generality that each rider i , who is now left with gᵢ ( gᵢ ≤ k ᵢ) incentives, has a list of eligible coupons and breakpoints that satisfy The figure below provides a graphical explanation, and we leave the mathematical proof to interested readers. Now consider what we have learned about the objective function (E). For a fixed rider i , the corresponding term in the summation gives rise to a piecewise linear function with the gᵢ breakpoints. Hence, in considering the sum in (E), we again get a piecewise linear function with the union of these breakpoint values (over all choices of i ) as the resulting breakpoints. Let 0 = ρ₀ < ρ₁ < ··· < ρn be the union of these breakpoints. Between any pair of breakpoints, the function D( λ ) is an affine function (i.e., it is of the form αλ + β ); furthermore, D( λ ) is convex with the slopes in these pieces are an increasing function of λ . For λ sufficiently large (bigger than ρn ), the slope is clearly C ; at λ = 0 , the slope is which we can assume is negative. Otherwise, we would simply assign each rider their maximum value coupon (which is also their maximum cost coupon), since that does not exhaust the budget, and hence is clearly optimal. In all other cases, there is a breakpoint ρ* for which the slope for λ < ρ* is negative, and for λ > ρ* is positive. In that case, D( λ ) is minimized when λ = ρ* . The Dual LP optimization algorithm At a high level, the algorithm to compute the optimal dual LP solution works as follows: maintain the breakpoints as m sorted lists, and iterate through the entire set of breakpoints in a descending order, computing the current total slope, until the first point at which the slope switches sign. Note that iterating the breakpoints in a descending order makes the algorithm a (relatively simple) extension of the case when selecting the greatest bang/buck choice for each rider exhausts the budget (and is, again, clearly optimal, where each rider except the one that uses the last bit of budget is guaranteed an integral assignment). For each rider we keep a pointer to the largest element in the sorted list of breakpoints that has yet to be processed. We start with an active set of breakpoints, one for each passenger i = 1, . . . , m , initially equal to vᵢ ₁ /cᵢ ₁(which is that passenger’s greatest bang/buck value). We initialize the current slope, which is for λ greater than all breakpoints, equal to C , and introduce the notation that cᵢ ₀ = 0 , i = 1, . . . , m , for a latter use (though this doesn’t correspond to any coupon option).We keep the currently active breakpoints in a priority queue, and in each iteration, extract the maximum remaining one. Suppose that the current breakpoint extracted corresponds to coupon j* for rider i* . We know the slope of the dual objective function greater than this breakpoint, and want to compute the slope just less than it. To do so, we need to reflect the change from the old contribution to the slope for i* to its new one: the new one is option j* , and the old one is option j* − 1 . If we let let OLD denote the cost of coupon j* − 1 for rider i* and let NEW denote the cost of coupon j* for rider i* , then the change of cost is OLD-NEW. To replace the breakpoint extracted for i* , we insert the next breakpoint of i* into the priority queue (if there is one available), and the algorithm continues to the next iteration. We continue to iterate until we have decreased the slope from C to below zero. It is possible that multiple riders could have the same “next” breakpoint, in which case we extract the next maximum in an arbitrary order. We update the slope after each extraction in the same way, and possibly we will have reached the termination point (because the sign becomes negative) after processing some of these common values, but not others. This will give rise to multiple optimal solutions for the primal LP. From optimal dual to optimal primal LP solution We now have obtained the optimal dual solution. We have already assumed that the slope of D( λ ) at λ = 0 is negative, and hence λ* > 0 . To compute the optimal primal solution, we take advantage of the complementary slackness conditions for primal and dual LP optima, which gives us the following: The case in which each breakpoint corresponds to a distinct unique rider is easy to understand. In this case, the optimal value λ* for (D) is the breakpoint corresponding to exactly one rider i* , and is between breakpoints for every other rider. Thus, for all other riders other than rider i* , there is a unique maximum j(i) attained in the term corresponding to i in the sum (E); hence, the corresponding option must be set equal to 1 for that rider. This argument can be extended to the case when the optimal breakpoint corresponds to multiple riders. The correctness of the algorithm follows directly from showing this pair of primal and dual linear programming solutions are both feasible for their respective optimization problem, and satisfy the complementary slackness conditions. Summary of the algorithm Recall that the algorithm produces a fractional solution in which at most one passenger is not given an integral assignment. In the algorithm below, we assign the more expensive coupon for the rider at the breakpoint and obtain an solution that is just fractionally more valuable than the optimal LP solution. Putting it all together, the pseudo code for solving the dual LP to get the primal coupon allocation looks like the following: Conclusion In this post, we have described an elegant way of solving a linear optimization problem with a budget constraint. In practice, the speed and efficiency of this tailored algorithm outperforms those of open-source LP solvers more than tenfold. On the flip side, there are obvious limitations (maybe clever extensions?) of the algorithm because of its rigid dependence on the problem structure. Most importantly, by showcasing to you how fundamental knowledge in optimization (without fancy blackbox methods) can help to solve problems we face at Lyft, we hope that you feel inspired to learn more about optimization, and about Lyft! If you enjoyed this post, follow and recommend! To learn more, check out our other Science posts. As always, Lyft is hiring! If you’re passionate about developing state-of-the-art machine learning/optimization models or building the infrastructure that powers them, read more about our Science and Engineering roles and reach out to us. We would like to thank Kedar Thakkar, Lei Ding, Alex Wood-Doughty and Jose Abelenda for their contribution/support to this work. Also, this post would not have been possible without the help of Matt Izant, Hao Yi Ong, Mark Grover, Ryan Lane and Alex Rafter. Many thanks! Stories from Lyft Engineering. 637 2 Thanks to Mark Grover . Machine Learning Linear Programming Data Science 637 claps 637 2 Written by Research Scientist at Lyft Stories from Lyft Engineering. Written by Research Scientist at Lyft Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-09-03"},
{"website": "Lyft-Engineering", "title": "the first few weeks of lyfts inaugural associate product manager class", "author": ["Jennie Braunstein"], "link": "https://eng.lyft.com/the-first-few-weeks-of-lyfts-inaugural-associate-product-manager-class-359ecc7aba19", "abstract": "Data Data Science Engineering Mobile Product Security Meet Rafah , one of 6 APMs (Associate Product Managers) in Lyft’s first APM class! In this Q&A, Rafah discusses why she chose to start her career in an APM program, what the first few weeks of the program have been like, and gives tips to interested candidates on how to prepare for APM interviews. The Associate Product Manager (APM) program is an 18-month rotational program designed to build future product leaders. It’s comprised of three six-month rotations across different areas of the business, and is an incredible opportunity to foster strong product management skills, learn about different verticals of Lyft’s business, and build a strong network as an early PM. How exciting you’re one of six candidates chosen for Lyft’s first-ever APM class! Why did you choose to start your PM career as an APM rather than as a PM? I chose to pursue an APM program for a few main reasons: the breadth of experience, mentorship, and leadership opportunities. Junior PMs are typically placed on one team and learn product management within the scope of that team, which is a great way to dive deep into a type of product management. However, through talking to PMs and internships, I’ve noticed that every team has a slightly different definition of what a good PM looks like. For my first PM role, I’m optimizing for experience across a range of different product management styles. A rotational PM program not only provides exposure to many flavors of product management, but also the opportunity to practice skills needed to succeed in different areas. One team may require me to be more data-driven, another team may need more of an intuitive pulse on our users, and yet another team might require more technical knowledge. The best PMs have the ability to flex all these different muscles, and I think an APM program is a great place to learn to do so. Having experience across multiple teams helps in decision making, gives me a broader understanding of the company as a whole and helps me understand the different ways Lyft makes decisions, creates goals, and maintains cohesiveness. For example, even if I’m shipping a rider product, I can draw on my rotational experience to think about how this product affects our drivers. This allows me to be a strong cross-functional PM, since I’ll know how teams across Lyft work. I personally learn best from hands-on experience with people I respect, so mentorship was a key factor in my decision. As APMs, we have both rotational managers, who stay with us for the duration of our rotation and are there to manage us on a day-to-day basis, and an APM manager, who manages us throughout the 18 months and mentors us through our growth. At a company like Lyft, where the culture of mentorship and learning from peers is very much alive, I’ve felt an extraordinary level of support and mentorship from other PMs. Whenever I’ve reached out with a question, I’ve expected a long delay or to be low on their very busy priority list. Instead, I’m always met with a, “I’d absolutely love to meet you. Please find some time on my calendar” — and they mean it. They prioritize our meeting and come to the meetings fully present and ready to help. This is by far one of the most amazing parts of being an APM, and especially at a place like Lyft, where there is so much to learn from the people around me. Lastly, I loved that Lyft’s APM program was intentionally designed to build product leadership. The focus on leadership means that not only do we meet people and think about day-to-day PM tasks, like writing product specs or leading a team of engineers, but we’re also thinking about macro industry factors, strategy, and executive presence. I’ve loved the balance between these two and the opportunity to foster my leadership at an early point in my career. What have your first few weeks in the program been like? My first few weeks have been amazing! I’m writing this towards the end of week four, and I already feel like I’ve had the opportunity to meet people, feel welcomed, become very educated by leaders across our organization, and received enough guidance to make a meaningful impact on my team. Our first week was training and onboarding. It began with a general company onboarding, where we learned about our mission and values, and met new people across the organization. It was amazing to feel the commonalities between a colleague in finance and another in marketing, and to know that we’re all working towards the same goal: creating the world’s best transportation. We then moved on to technical onboarding, where we met people all across the organization to learn about Lyft’s organizational structure and meet energetic leaders who could explain their team’s role, importance, and involvement in our projects. It was amazing to see how excited leaders were to talk about their work and pave the way for potential collaboration. The last step was product onboarding, my favorite part of the week. Our program director brought in senior level executives to tell us about everything: the industry landscape, the ways we determine pricing, leadership, understanding our core business metrics, and how PMing is different at Lyft. We frequently ran out of time in these sessions because we were brimming with questions, and the product leaders were so excited to share. My favorite part was that everyone who presented to us could answer every question because they were the people who had implemented these ideas and thought about our questions. Everyone left the room with a personal relationship, more intellectual curiosity, and new enthusiasm for another area of the business. Throughout the week, I got to hang out with my best friends at Lyft: the APMs! It was a week full of many laughs, intellectual curiosity, and far too many snacks. I felt settled knowing I have a group at the company that I can reach out to for advice on how to run a meeting, a set of eyes on an important doc, or even just lunch recommendations. We’ve already had many adventures, like having an intense Spikeball tournament on Angel Island, exploring the Mission’s best Mexican food, and indulging in one APM’s addiction to Salt & Straw. In the second week, I finally joined my team, and my amazing rotational manager (who also started her PM career as a rotational APM!) guided me along in feeling adjusted and understanding what I was there to do. She gave me a detailed brief on our problem space, where I’ll be working, and why it’s important. It was special to know that she put so much thought into what it’d be like to be a fresh grad on a team, and she provided more than enough support to ensure I was settled and set up for success. I learned so much every day that week, and I felt like my voice was immediately valued on the team. One special moment that stuck out to me was that on my second day, my APM manager asked me about my opinion on our marketing strategy in a room of senior product people, conveyed her genuine excitement that I had joined, and made it clear that my opinion was valuable the moment I walked in. So far, Lyft has been a place where I have felt constantly welcomed and supported, but also challenged and excited to learn. Do you have any tips for any candidates who may be reading this on applying for jobs right after school & what to expect from the interviewing process? My biggest suggestion while looking for APM programs is to understand what matters to you, because this helps you focus your energy towards efforts that will actually pay off. The way I approached this was to come up with six factors (in no particular order) that I wanted to prioritize in my role: The company’s mission The company’s culture The company’s standing within the industry The job’s location The role Compensation. I ranked these factors based on their importance to me, and to stress test this ranking, I actually came up with scenarios that represented the ranking, and tried to see myself in those scenarios. I then ranked every company I interviewed with on each of these factors on a scale of 1–10. It quickly became clear which companies aligned with what I really wanted, and cut through a lot of the noise surrounding the recruitment process. Maybe your factors are different, and maybe this method doesn’t work for you. That’s totally fine! It doesn’t matter how you do it — it matters that before you go into a whirlwind of interviews, you know what your end goal is. On a similar note, research both the role and the company that you’re interested in. This should be easy, because hopefully you’re excited about the opportunity to interview. I found that calling PMs and asking educated questions — making sure to do research on their company beforehand and asking questions that only they are qualified to answer — led to really rewarding intellectual conversations. However, it’s important to note that there’s no point in networking for the sake of networking; it’s a missed opportunity, and never as interesting as when you ask questions you care about the answers to. Another key suggestion I have is to practice interviewing. If you’ve ever heard an incredibly intelligent person talk about something they’re passionate about, you know that even the smartest people can sometimes struggle with breaking down their thoughts. A key skill to have in a PM interview, and as a PM in general, is to slow down and over-communicate. PM interviews require you to detail your logic, and to state and validate the most seemingly basic assumptions. That’s a skill — and one that takes practice. In my experience, I found interviewers cared more about my ability to structure my thoughts and work through a problem than the outcome of the thoughts. Another great thing about practice interviewing is that you understand what it’s like to be on the other side of the table. This helps you understand your interviewer’s perspective and learn to provide them with structure and succinct answers to help them focus; but, it also helps you see interesting ways that other talented applicants approach problems and makes you rethink your own approaches. There are a variety of Facebook groups that set calendars up to match applicants to practice interview — check them out. In terms of preparing specifically for PM questions, I found that it was helpful to learn to explain my experience in a succinct, non-jargon way. I’d test my ability to talk about any professional experience in 1–1.5 minutes, and I’d ask friends outside of the industry to listen, to ensure that my explanation was easy to follow and as simple as possible. Lastly, and most importantly, find your voice. Hear all of your answer in a way that reflects the way you think, not the way you think interviewers want to hear it. Being authentic ensures that you end up somewhere you’re actually excited to be, and that a company is excited to have you for who you are. Rafah is one of six of Lyft’s first class of APMs! Here’s our entire inaugural APM class: Rafah is an APM whose first rotation is on Open Platform, Lyft’s platform for autonomous vehicles. She graduated from Northwestern with a degree in Computer Science, Design, and Entrepreneurship. She’s a foodie, and is on the hunt for San Francisco’s best burrito! Taylor is working on Lyft’s Growth Team. He graduated from Harvard University in 2017, and spent two years founding a healthcare tech company before joining the Lyft family! In his free time, Taylor’s a singer/songwriter. Mayank started at Lyft with his first rotation on Consumer Rentals, Lyft’s approach to rental cars (currently in beta). He graduated from Penn State with a degree in Computer Science and Entrepreneurship. He loves to travel, and visits a new country every year! Neeka is on the Pricing XP team. She graduated from UC Berkeley with simultaneous degrees in Materials Engineering and Business Administration, with a minor in Entrepreneurship & Technology. Fun fact: she’s raced competitively on a Formula 1 track several times (and won first place!) as part of her vehicle engineering team at Berkeley. Phil is an APM working in the Marketplace Experience team, where he tries to improve the user experience of the shared modes available to Lyft users. Before that, he was the Product Manager for the Data Map & Discovery team, helping Lyft, a company that grows fast in number of datasets and employees to make sense of its data. Previously, Phil worked as a product manager for a Fintech startup in Berlin, as well as an M&A Analyst for an Investment Bank in Paris and served as an Officer in the French Air Force. He is a graduate from École Polytechnique in Paris and Columbia University in NYC. Lyft is hiring for our next stellar class of APMs! Recruitment closes in mid October. Learn more here . Stories from Lyft Engineering. 210 1 Product Management Product Lyft 210 claps 210 1 Written by Product Manager, Self-Driving at Lyft Stories from Lyft Engineering. Written by Product Manager, Self-Driving at Lyft Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-09-25"},
{"website": "Lyft-Engineering", "title": "presto infrastructure at lyft", "author": ["Puneet Jaiswal"], "link": "https://eng.lyft.com/presto-infrastructure-at-lyft-b10adb9db01", "abstract": "Data Data Science Engineering Mobile Product Security Early in 2017 we started exploring Presto for OLAP use cases and we realized the potential of this amazing query engine. It started as an adhoc querying tool for data engineers and analysts to run SQL in a faster way to prototype their queries, when compared to Apache Hive. A lot of internal dashboards were powered by AWS-Redshift back then and it had data storage and compute coupled together. Our data was growing exponentially (doubling every few days) this required frequent storage scaling as well. With storage coupled with compute, any maintenance, upgrade required down time, and scaling nodes made querying extremely slow (as massive data moves across nodes), we needed a system where data and compute were decoupled, thats where Presto fit nicely into our use case. A pipeline to store event data in Parquet format had already been setup and data was being accessed via Hive. Adding Presto to this stack was icing on the cake. Now thousands of dashboards are powered by Presto and about 1.5K weekly active users are running a couple of million queries every month on this platform. As of today we have 60 PB of query-able event data stored in an S3 based data lake and about 10 PB of raw data is being scanned every day using Presto. Following are the charts showing the timeline of presto usage growth. In the above chart we are looking at raw data scan volume per day. We have seen daily raw data scan volume growing 4X in last 4 months. Lyft users consume data using query tools such as Apache Superset, Mode, Looker, Tableau, Jupyter notebooks, and some internal ML tools that build and improve application specific machine learning models. All these tools connect to presto via Presto Gateway , queries sent by these clients are load-balanced across multiple Presto clusters uniformly. With the gateway we perform zero downtime upgrades, transparent to the users/apps querying through these clients. We have forked prestosql/presto under lyft/presto and created the lyft-stable branch (adding additional cherry picks if needed). We have a private repository where we use saltstack and aws-orca scripts to roll out deployments to our environments. In this private branch we have added additional dependencies specific to our environment and have added integration tests that run on every update or pull request. We have dockerized this repo and use the Docker container as a development environment. On every new commit, we trigger the integration tests against the dev environment through Jenkins. Here are the various components we use while releasing Presto to production. Lyft’s presto query-log plugin: We have added a query logging and blocking plugin based on Presto-Event listener framework. We use EventListener to intercept when new query lands and we block a few types of queries that we deem harmful for system (eg. some tools actively cache column names and query system.jdbc.columns or catalog.information_schema.columns and cause additional overload on the system). We log query stats in both queryCreated and queryCompleted events, this helps us analyze success rate, latencies, various types of failures and their root causes etc. Presto UDFs: We let our users add custom UDFs based on their needs if a function addressing the use case is not available in Presto. Some users have added custom geo functions based on their use cases. Python-based stats collection: We are using datadog-agent lib and have added multiple checks to collect system/jvm/process stats metrics and push to statsd. We have statsd collector that collects and pushes the metrics further to Wavefront. We have setup various alerts and get paged through pagerduty when there is a threshold breach. Test suits: Before we roll a new presto version to production, there are multiple types of tests we run to ensure the quality of a release candidate. Following are the type of tests we run to ensure quality of each release. a. Integration test suite — Runs Table-CRUD and UDF tests against each build, launching devbox docker container in Jenkins build task. b. Replayer test suite — After rolling a release candidate to pre-prod, we launch day-long test with an older days worth queries, replaying in the same way as the queries had executed in the past. We feed the query log using our event logging pipeline and compare the performance with previously logged data. c. Verifier test suite — Similar to replayer, we use presto-verifier to run static set of queries and run against old vs new versions. We move to this version if the performance results have not degraded. 5. PrestoInfra: collection of configurations and scripts to build and release deploy-able artifacts and saltstack scripts to rollout environment specific deployments. 6. PrestoProxy: a customized presto-gateway with lyft specific routing rules and overrides. We are running multiple presto clusters sharing load through presto-gateway , each 100+ worker nodes. The node specifications are as follows On each cluster, we have set 25 max query concurrency and 200 max query queuing with each running 4 queries max and 20 queries queuing per user. Each query has 30 min max run time limit and 10 minutes max execution time limit. We rotate the entire cluster once in a day to avoid long old gen GC build up. We let each query scan up to 1 TB max with each worker node loading up to 10 GB data. Below are exact configurations to achieve these settings. In Presto gateway coordinators can be assigned a routing group each, setting X-Presto-Routing-Group header would route that request to one of the clusters under that routing group. We have one cluster which runs with extended memory limits assigned nolimit routing group. User has to add a comment ` — higherlimit` in the query as a hint to indicate resource heavy query and presto-gateway routes that query to the cluster with higher resource limit. We are running Java 11 on presto nodes. Java 11 has parallel GC for old gen phase, reducing the collection time significantly. Following are the JVM configuration we are running presto processes with. With Java 11 upgrade, we were able to bring down Old gen GC pause in the order of a couple of seconds, where with Java 8 it was peaking in the order of 400 seconds crippling the service every once in a while. value for max_heap_size_mb is (0.6 * node_memory_mb) which is 114 GB on worker nodes. Despite all the optimizations, presto consumes host resources over time and from our experiences we have learned that the longer the service runs, the slower it becomes. Old gen GC pause time increases with time as more and more queries execute on the cluster also we have noticed that a newer cluster yields better query performance as compared to an old cluster. Keeping that in mind, we have designed the infrastructure to recycle all the nodes in each cluster once every 24 hours. We use PrestoGateway’s deactivate API to disable the cluster 30 minutes before scheduled shutdown time and activate the cluster 30 minutes after the scheduled start time through a cron providing no-downtime maintenance. Since we have set 30 min max run time per query ensuring no query loss during these operations. We are using aws-orca scripts to trigger AWS’s ScheduledActions to bring up or down entire presto cluster on a given schedule. This is the salt script for shutting down a cluster at 5:20 PM and bringing up back on 7PM Pacific Time. Presto-Gateway is a stateful load-balancer, proxy and router for multiple presto clusters, it provides transparent access to underlying presto-backend without changing the protocol. It started as a proxy service project to expose presto coordinators to external BI tools like Mode and Looker in a secure way. As the query volume increased, we faced more frequent outages as single cluster could not handle the load. We soon realized that we need multi cluster setup. So we dedicated one cluster to each such query tool. Though it reduced the frequency of outages and incidents, we still had not achieved zero downtime maintenance and noticed that one cluster is relatively free when other was suffering with high queuing. At that point we decided to implement a true proxy load balancing router. Since the BI tools were external, they were accessing presto infra via an agent which is deployed in our network, these agents were unable to follow HTTP-redirects, so we needed to implement a true proxy router and load-balancer. We used JettyProxy which is a proxy-server that allows binding custom filters and plugging proxy-handler. We implemented routing rules in a Proxy Handler which lets us intercept HTTP Requests and response allowing to inspect query, source, headers etc and based on that we may choose a backend host to serve the query request. There are two types of requests sent from clients 1. New query submission request, 2. Followup request for a previously submitted query. PrestoGateway proxy handler caches the query id to backend mapping so that it can stick the followup requests to the backend which ran the original query. Presto Gateway has 3 components: BaseApp — It provides boilerplate code to add/remove plug-able components through yaml configuration and has inbuilt metrics registry module making it easy to emit custom metrics for the apps built on top of this. ProxyServer — Its a library built on top of jetty proxy which provides a proxy server implementation with a plug-able proxy-handler. Gateway — This component acts as container for proxy-server and plugs in ProxyHanders to provide proxy, routing and load balancing functionalities. It also exposes a few end points and UI to activate / deactivate backends and query history view for recently submitted queries. We are using lombok to reduce a lot of boiler plate code such as getters/setters/logger/equals etc speeding up the development process and the app is built using dropwizard framework. We started with one Presto cluster and as the usage grew, we kept on adding more worker nodes to support the higher compute demand. As we added more worker nodes, to utilize the full potential of cluster, the query concurrency setting had to be raised and that required presto-coordinator restart causing a down time. After introducing the presto-gateway, we went multi-cluster mode using gateway as load-balancer. The entire presto infrastructure usage is not uniform throughout the day and the nature of our workloads have been bursty, so we over provision the infra so if there is a burst of queries, infra should be able to absorb it gracefully. To optimize cost we implemented dynamic scaling. We looked at the incoming query volume rate and noticed that usage is usually higher during business hours. After implementing backend activate/deactivate API at gateway we were already able to perform no-downtime upgrade, deployments and maintenance. We took this to the next level and added scheduled down time for half the number of clusters during non-business hours. 30 minutes before triggering the shutdown, we disable the cluster using Gateway APIs, since we have set 30 minutes max-run time for any query ensuring no query is penalized in this process. Following chart shows how number of nodes vary with time. Cutting 50% infra during non-business hours resulted 30% cost savings overall. This allows Presto to read data from a Google sheet so that small dimensional data can be added into a query. We announced this feature in Presto Summit 2019 and contributed it back to open-source . Currently gsheets connector plugin has following restrictions. All the sheets have to be shared with service account user at least with view access if sheet is not public. First line of sheet is always considered as header having all column names. All columns are resolved with type VARCHAR. Gsheets API’s rate limit — 100 calls per 100 seconds (unlimited for the day) will be applied if google project account does not have billing enabled. To avoid this users may choose higher value for cache config property — sheets-data-expire-after-write in presto-gsheets configuration. Following are the steps to set up presto google sheets plugin: Step 1. To use this, user has to generate service account JSON credential in Gsuite-console, after creating a project and granting read access to Google sheets API. This credential file should be stored and provided in the sheets.properties as credentials-path . Service account user id will be in credentials file, user should share all the google sheets with this service account user to read the sheet in presto. Step 2. Now create a metadata sheet and to add table to sheet id mapping. Share this sheet with service account user and note down the sheet id. Add this sheet id in sheets.properties as metadata-sheet-id config property. Step 3. Now add the table name and corresponding sheet id (after sharing the sheet with service account user) in the metadata sheet and you should be able to query from your table in presto. Step 4. Happy querying ! This feature is currently in beta at lyft and has become pretty popular since we rolled it out and about a couple of hundred users already running thousands of queries every day on 50+ google sheets as table. There are many challenges users face when developing a SQL based workflow or pipeline. Query browsers provide error and suggestions to fix syntactical errors (bad sql or wrong UDF usage) after executing the query. It takes some time for users to figure out and resolve such errors through multiple iterations and degrades the user experience. We implemented Explain Type (Validate) queries in Presto and send these explain queries as user types in sqlLab in Superset ahead of actual query execution and the returned explain plan captures the syntactical errors along with validity of columns, tables and udf signatures. This performs deep query validation without actually executing the whole query improving the overall querying experience by eliminating debugging time involved in writing complex sql queries. Apache Superset — pre execution deep query validation. We have contributed these features back to the open source. It is important for all the teams at Lyft to make data driven decisions and it has been Data Platform team’s mission to make data at the heart of all decisions made at Lyft. Presto is a key component for us to achieve this mission. In the last couple of years we have been primarily focused on scaling the infrastructure, reducing data arrival latency, improving user querying experience while being cost-effective at the same time. A huge thanks to the rest of the Data Platform Infra team who helped in improving, scaling and supporting Presto Infrastructure at Lyft. Also, thanks to the rest of the PrestoSQL open source community for helping and supporting us through the development. Please check this page on how to get involved in the project . The Data Platform team is actively looking for talented and driven engineers, data engineers, scientists, product managers, and marketers to join our team . Learn about life at Lyft and visit the careers section for the latest openings. Stories from Lyft Engineering. 605 2 Thanks to Andrew Hao . Presto Scaling Data Big Data Analytics Analytics 605 claps 605 2 Written by Software Engineer Stories from Lyft Engineering. Written by Software Engineer Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-09-24"},
{"website": "Lyft-Engineering", "title": "what i learned transitioning from consumer to data platform product management", "author": ["Allyson Gale"], "link": "https://eng.lyft.com/what-i-learned-transitioning-from-consumer-to-data-platform-product-management-b984101c8dd1", "abstract": "Data Data Science Engineering Mobile Product Security Hi all! I’m Ally, the product manager for Flyte, Lyft’s workflow orchestration platform that makes it easy for us to do machine learning and batch compute at scale. Outside of work, I play music, volunteer at SFJAZZ and explore the Bay by bike. Before Lyft, I worked at Google where I went directly into product management after college through their Associate Product Manager (APM) program. While there I had a heavy focus on consumer products, working on Google Flights, Fit and Search. Before Google, I studied computer science at the University of Washington. When I decided to make a change, I thought deeply about what I wanted in my next move. Still early in my career, I wanted to use this opportunity to sample a different flavor of product management. There are so many different types of PMs and I believe your early career is best spent trying out a variety to see where you flourish most. Since my background heavily focused around consumer, I decided to dive into the data platform and machine learning worlds to round out my experience and return to my technical roots. I’m grateful that Lyft was willing to hire me into an entirely new space, trusting me to learn the industry-specific chops on the job. While I was excited to try something new, part of me was apprehensive. Not only was data platform an unfamiliar space (with many new acronyms to learn), but I worried that the work would be so fundamentally different as to risk it not being transferable. After being in the role for almost a year now, however, I realize that the nature of product management remains unchanged. Only subsidiary elements like the typical profile of your users differ, making this transition expansive to my career, rather than restrictive. The rest of this post will detail the similarities and differences I’ve noticed between consumer and data platform PMing, challenging the broad divide most assume exists while also highlighting the subtleties that make each unique. Similarities: Where Data Platform and Consumer PM Roles Overlap At the most fundamental level your job as a Data Platform PM is the same as any other product management role: to be the voice of the user. It’s still critical for you to immerse yourself in your users’ problems, craft a compelling vision to solve their most pressing needs and align this with business goals and strategy. In the consumer space this might look like “create the ability for users to share rides to make a more efficient marketplace and reduce emissions”. In the data platform space, this might look like “create an experimentation platform for product teams to rapidly test product iterations and drive launches with data”. With that, your key success metrics don’t stray far from those of a consumer-oriented product: adoption and satisfaction are key. Differences: Where Data Platform and Consumer PM Roles Diverge The most notable differences are: Who your user is How critical technical expertise is The nature of your impact Users As a Data PM, your users are technical: engineers, data scientists, machine learning researchers, etc. This affects how you weigh tenets like “ease of use”; for example, your users will likely prefer more knobs at the expense of simplicity and can generally handle a lower degree of polish. That said, your users are incredibly opinionated. Building a technology platform for technical users is akin to cooking a meal for a table of chefs: they’re incredibly discerning, will have opinions on how it’s made, and know exactly how the magic works. These users are also internal to your company, which opens up the opportunity to deeply understand their needs and contexts. Don’t know if your proposed feature will fully encompass their use case? Swing by their desk and ask. Since your users are your colleagues, you don’t have to rely solely on aggregated metrics and small-scale user studies. Instead, you can go directly to the source, develop strong relationships with them and ultimately an intuition for their needs. Technical Expertise Data Platform PM roles require greater technical depth than consumer. Whereas in consumer product you need to understand the underlying technology to gauge the difficulty and timeline of a change, as a PM for a data platform you need to intimately understand the technology in order to innovate at all. The problems your users face are fundamentally technical in nature (e.g. real-time access to data, take an ML model from prototype to production) and traditional to the field of computer science (e.g. scale, extensibility, reliability, API design). If you lack technical expertise you won’t be able to fully understand your users’ problems and will therefore be unable to innovate on their behalf. That said, it’s important to call out that a background in computer science is not necessary. What is required is technical understanding — and that can be attained through schooling, both formal and informal, as well as on the job. Nature of Impact As a data platform PM the nature of your impact is transitive but incredibly scaled. The transitivity comes from the fact that you’re building a platform that many product teams use to build features. So while you won’t be able to point to a specific consumer-facing feature and say “our team built that”, you can point to many features and say “our team enabled that”. This also means that you’re driving impact at a company level, fostering and accelerating the productivity of everyone around you. Flyte, for example, is a platform that enables developers at Lyft to build machine learning and batch pipelines at scale. Examples of user-facing features we power include critical experiences like the prices, ETAs, routes and suggested locations you see when requesting a ride. As another example, while Flyte doesn’t directly implement Lyft’s pricing functionality, our team made it possible for the Pricing org to launch a more performant pricing model to half of all Lyft sessions (and growing) last quarter. We don’t touch end-users directly, but our work has critical impact across the product. In Conclusion While the problems and users a Data Platform PM addresses are more technically angled to that of a consumer PM, the foundational elements of the role remain constant. At the core of both is still an unwavering devotion to your users, strong advocacy for their needs and the importance of iteration. Interested in trying out a role as a Data Platform PM? We’re hiring and looking for awesome PMs from all backgrounds! Check out www.lyft.com/careers for more information and instructions to apply. Stories from Lyft Engineering. 259 Thanks to Ryan Lane . Product Management Product 259 claps 259 Written by Product manager for Flyte.org @Lyft. Previously @Google Search, Android. Curious about ethics and decision-making. Stories from Lyft Engineering. Written by Product manager for Flyte.org @Lyft. Previously @Google Search, Android. Curious about ethics and decision-making. Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-06-06"},
{"website": "Lyft-Engineering", "title": "announcing envoy mobile", "author": ["Matt Klein"], "link": "https://eng.lyft.com/announcing-envoy-mobile-5c2067d9ade0", "abstract": "Data Data Science Engineering Mobile Product Security Today we are thrilled to announce the initial OSS preview release of Envoy Mobile , an iOS and Android client network library that brings the power of Envoy Proxy to mobile platforms. This is the beginning of a journey that we hope mobile developers around the industry will join us on. When Lyft originally announced Envoy in 2016, the project goal was simply stated as: The network should be transparent to applications. When network and application problems do occur, it should be easy to determine the source of the problem. Envoy proxy was initially built at Lyft to solve the networking and observability issues inherent in large polyglot server-side microservice architectures. Over the last two and a half years, much to our surprise and satisfaction, Envoy has become incredibly popular throughout the industry . Today Envoy is used by all major public cloud providers, countless end user companies, and a plethora of infrastructure startups that have recognized Envoy’s extensibility as a useful base with which to build vertical applications and services. For those not familiar with Envoy’s architecture and feature set, please see the links in the further reading section at the bottom of this post. As fundamental as Envoy has become in scaling both Lyft and other organizations’ distributed architectures, the reality is that three 9s at the server-side edge is meaningless if the user of a mobile application is only able to complete the desired product flows a fraction of the time. This may be due to a combination of network and application errors. Thus, in order to fully achieve the Envoy project’s goal of making the network transparent to applications, the service mesh and its inherent benefits (observability, consistency, etc.) must expand beyond the edge all the way to the mobile applications that are so critical to the end user’s experience. Envoy Mobile in conjunction with Envoy in the data center will provide the ability to reason about the entire distributed system network, not just the server-side portion. Three 9s at the server-side edge is meaningless if the user of a mobile application is only able to complete the desired product flows a fraction of the time. Whereas server-side Envoy proxy is a self-contained process that is meant to be deployed alongside a polyglot architecture , Envoy Mobile is distributed as a library meant to be compiled directly into client mobile applications. The library approach is required due to the practicalities of how applications are written and distributed on both the iOS and Android platforms. The high level goals of the library are discussed in the following subsections. Low-level networking improvements like HTTP/2, QUIC, QUIC to TCP failover, gRPC, DNS replacements, push/streaming, observability, analytics, state of the art TLS, retry and timeout policies, etc. take a tremendous amount of effort both to implement robustly as well as to make consistent across all of the Android and iOS versions in common use. Envoy Mobile will provide all of this functionality in a consistent cross-platform manner, especially when paired with Envoy running at the edge, similar to how server-side Envoy provides consistency to polyglot distributed application architectures. With the industry progressively moving towards specifying APIs via a strongly typed IDL such as protocol buffers , Envoy Mobile will standardize and abstract how mobile developers interact with IDL exposed endpoints. Via intelligent protobuf code generation and an abstract transport, both iOS and Android can provide similar interfaces and ergonomics for consuming APIs. Initially we are planning on focusing our efforts on Swift APIs for iOS and Kotlin APIs for Android, but depending on community interest we will consider adding support for additional languages in the future. Our ultimate goal is to make the low-level Envoy common C++ code an implementation detail that the average mobile developer does not need to be aware of. Instead, mobile developers will interact with high-level language specific APIs that encapsulate common concerns such as making API calls, analytics, tracing, etc. With protocol buffer’s powerful annotation/extension system, Envoy Mobile can add sophisticated cross-platform functionality in a simple and explicit way when using strongly typed IDL APIs. Examples of annotations that are planned on our roadmap include: Marking an API as offline/deferred capable Caching Priority Streaming Marking fields for exclusion both on the request and response in poor network conditions General Envoy policies such as retry and timeout specifications Much like Envoy’s use in a server-side service mesh, the goal is to push as much functionality as possible into the common core so as to avoid reimplementing it in every mobile application language. Our long-term plans include evolving the gRPC Server Reflection Protocol into a streaming reflection service API. This API will allow both Envoy and Envoy Mobile to fetch generic protobuf definitions from a central IDL service, which can then be used to implement annotation-driven networking via reflection. This model means that Envoy Mobile will not necessarily need to have prior knowledge of an organization’s APIs in order to provide enhanced cross-platform networking functionality. One of the reasons that Envoy has become so popular as a platform is its rich configuration discovery APIs which are collectively known as xDS (more information can be found here and here ). These APIs allow a distributed set of Envoys to be managed by an eventually consistent control plane . One of the long term goals of Envoy Mobile is to bring xDS configuration all the way to mobile clients, in the form of routing, authentication, failover, load balancing, and other policies driven by a global load balancing system. This will be an extremely powerful mechanism for bringing layer 7 / application networking concepts all the way to the mobile client. One day, it may even become possible to compile WASM code server-side, remotely distribute it to mobile clients, and remotely alter client behavior either to fix latent bugs or react more quickly to customer needs without needing to distribute a completely new client binary. This release is a proof of concept demo that shows Envoy compiled as a library and functional on both iOS and Android. Unlike when Lyft released Envoy in 2016, we are not providing a production-ready client networking solution. So why are we releasing it now? Based on discussions with organizations around the industry, it seems clear that there is a strong desire for a comprehensive cross-platform client networking solution similar to what this project proposes. We have decided that it is in the industry’s best interest to release now, even while our team actively works on the base feature set required for shipping Envoy Mobile as the default networking transport layer in Lyft’s mobile applications. We look forward to collaborating with mobile engineering teams around the industry to ensure that what we build is generally useful, even while continuing to iterate at high velocity. If we are successful in realizing our vision for the future of mobile client networking, we fully expect to grow the project’s maintainer ranks with engineers from around the industry, as well as ultimately migrate the project to a CNCF-owned vendor neutral location. In addition to the demo release which we encourage you to compile and play with, we are also open sourcing our roadmap and plan to develop Envoy Mobile entirely in the open. Please reach out to us via GitHub, email, or Slack if you would like to join us on our journey of defining the future of mobile client application networking. We look forward to hearing from you and collaborating with you! Announcing Envoy: C++ L7 proxy and communication bus Envoy: 7 months later Envoy joins the CNCF Envoy graduates! Envoy documentation Envoy Mobile documentation Este artículo también está en español: eng-espanol.lyft.com Stories from Lyft Engineering. 581 1 Thanks to Ryan Lane . Microservices Android iOS Networking Mobile 581 claps 581 1 Written by Engineer @lyft (https://mattklein123.dev/) Stories from Lyft Engineering. Written by Engineer @lyft (https://mattklein123.dev/) Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-03-31"},
{"website": "Lyft-Engineering", "title": "how to grow new user sign ups on android", "author": ["Kateryna Semenova"], "link": "https://eng.lyft.com/how-to-grow-new-user-sign-ups-on-android-56eefdf516f9", "abstract": "Data Data Science Engineering Mobile Product Security 2018 was a momentous year for the Lyft community! In September, we celebrated our 1 billionth ride and our service is now available to 95% of the US population. How did we manage to achieve this? This is all in service of our mission: improving people’s lives with the world’s best transportation. Let’s look at what we did as a first step to increase app installs and sign-ups. The first thing that we do at Lyft is to try to understand people’s transportation problems and needs. For people who have never tried our product, we ask what is important to them? How can we help them and what can increase apps installs and sign-ups? According to Google, each 6 MB increase to an APK’s size decreases install conversion by 1% . A significant amount of the decline in this conversion rate is not only due to people simply choosing not to install, but also the install not completing due to different factors. The download completion rate of an app with an APK size of around 10MB will be ~30% higher than an app with an APK size of 100MB. At Lyft we employ various approaches to reduce APK size: reducing the number of 3rd party libraries avoiding adding large resources such as videos; if you need to play a video, download them after installation using vector drawables instead of png assets; vector graphics are much smaller than raster images; checkout Alex Lockwood presentation about Vector workflows shrinking our code and resources with ProGuard These simple techniques can help keep APK size small. To reduce APK size even more you can try the Android App Bundle , a new app publishing format. It helps to build and serve APKs that are optimized for each device configuration. This results in a smaller app download for end-users by removing unused code and resources needed for other devices. After installing an application, the next most important step is sign-up. It should be simple and quick. At Lyft, we reduced the number of steps that it takes for people to sign up and use the app. Let’s look at some examples about how we simplified our sign-up flow: Problem: To log in/sign up we prompt a user to enter their phone number. To verify it, we send an SMS with a code which needs to be manually entered into the login screen. Some people had problems understanding where they should find the code. It is also not very convenient to switch between apps in order to find and somehow remember it. Solution: We decided to use automatic SMS verification with the SMS Retriever API, so people don’t need to enter the code manually: https://developers.google.com/identity/sms-retriever/overview Before this change, 4% of new users dropped off at this step. Now they don’t need to do anything: the verification step has been automated for them. Problem: People need to enter a lot of data about themselves, like: first and last name email profile picture phone number payment information Very often people make typos which leads to different problems, like unreachable phone numbers or email addresses, incorrect payment information. Any of these can prevent people from using your service. We wanted to automate this process and help people to reduce the total number of steps to sign up as well as fix typos in their data. Solution : The first solution that we employed is the Google credentials API . It helps to get a user’s info from their Google accounts. With one tap, users can fill in all the required information: About 70% of our users use this feature. But what about the other 30%? For that population, we benefit from the Autofill framework to help them fill out forms. It can automatically fill in account information as well as credit card numbers, which helps reduce number of payment errors. The Autofill framework is available starting with Android 8.0 (API level 26) and later. Most autofill services use heuristics that allow them to fill out screens in client apps without any optimization, but you can optimize your app to make sure autofill services can fill out its views correctly. Lastly, what about users with devices that have Android older than 8.0? We noticed that the most common problem for these users is having typos in the email addresses they entered, such as: gnail.com gmail.con yahoo.con To remedy this problem, we decided to provide suggestions to users as they type. To implement this, we use a subclass of EditText called AutoCompleteTextView , and specify text suggestions using an Adapter . Once people start typing their email we suggest the most popular domain names for autocomplete. This solution helped us to reduce number of typos in emails. Reducing the APK size and simplifying the sign-up flow will drive new-user acquisitions. Just remember to measure any change and prove that it’s working and helps people. Let’s look at Lyft’s UI evolution over the last few years: 2016 2017 2018 For every minor UI change made, we ran an experiment to make sure that it is demonstrably improving. Anything that didn’t seem to increase sign-ups was dropped. Everything that moved the needle became a permanent part of our application. Success in growing your app depends on understanding people and helping to solve their needs. I hope you’ve enjoyed this post. If you’re interested in hearing more, I gave a talk about helping new users adopt your app as well as helping other types of users as part of Lyft’s Android Tech Talk . We’re hiring! lyft.com/careers Special thanks to Ignacy Zulawski , Rocky Smith , Adrien Cadet , Artem Zinnatullin , Mark Wei and Adam Wolf. Stories from Lyft Engineering. 506 Thanks to Ryan Lane and Alex Rafter . Android App Development AndroidDev User Acquisition Lyft Mobile 506 claps 506 Written by Software engineer @ Lyft Stories from Lyft Engineering. Written by Software engineer @ Lyft Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-06-12"},
{"website": "Lyft-Engineering", "title": "move thoughtfully and dont be afraid to change things", "author": ["Sean Bolton"], "link": "https://eng.lyft.com/move-thoughtfully-and-dont-be-afraid-to-change-things-ab9fc4fd0cf2", "abstract": "Data Data Science Engineering Mobile Product Security On the support tech team at Lyft, we’ve been rebuilding our support systems to better stand the test of time and enable our growth. We chose to “move thoughtfully” and “not be afraid to change things” instead of the popular hacker mentality of “move fast and break things.” This strategy was fueled by a deep understanding of what challenges existed in the past, what is needed now, and what is required for the future. The result is a new generation of support infrastructure that is built to last and adapt to our business needs over several years. We focused on two areas: Routing, Storage, and Reporting Data Model Design Several years ago, Lyft offered support primarily via email through a third party tool/service provider. Gradually we added other channels for support such as phone and chat, along with other 3rd party providers for sending this communication. The system architecture was simple with one support channel and one 3rd party. However, each new type of support or provider of support drastically increased the complexity of the overall system. This was because each node in the system needed to know about the overall system. Beyond being overly complex, the implication of this architecture was that we did not maintain an internal source of truth for support interactions. We delegated support requests to 3rd party providers, which fundamentally limited our ability to deliver a customized, high quality support experience. We were not able to have easy application layer access to a given user’s support history so we could not use that data to understand the previous experience or current status of a user. To address this, we created a new system architecture that enables us to be more interoperable. We created a routing micro-service which acts as the entry point to all of our support infrastructure. In our new design, we maintain an internal database of support interactions which provides a complete picture of a customer’s support history, regardless of the underlying channel or provider. This architecture makes it easier to add new types of support and new support providers in a way that does not increase of complexity of the overall system. We delegate to the 3rd parties only to send the channel communication. As an added bonus, we introduced an abstraction layer to hide all provider-specific implementation details which enables other teams at Lyft to integrate into support without writing provider-specific code. Integrating with support can now be distilled into one simple API request: “contact this user via this channel about this issue.” An added benefit came out of this new core support system: it allows us to report more accurately, in real time, how the support system is being used, rather than waiting to sync data from 3rd parties or trying to merge data from various 3rd party formats. It sounds so simple and obvious in hindsight and that’s exactly why it took time to get there. One entry point to support tech that does: routing, storage, reporting, and abstracts 3rd party logic. Most off-the-shelf support platforms are very opinionated about the way they model their data. Whether through their data model or interface, the product defines how one can structure their support system. This is a good thing for most small/medium-sized companies because their requirements are simple enough to fit out of the box solutions. At Lyft’s scale, we need much more flexibility to differentiate our support experiences for a diverse range of situations. There are several common paradigms that exist in the support world: Ticket Most systems use a ticket architecture. A ticket has a set of metadata to define what it is about. Then there is usually a thread of conversation and events that happen regarding that ticket. For Lyft, this would look something like a generic driver payment ticket to cover all driver pay issues. Pro Simplicity, which is great for when your customer contact you once about a single topic and the topics tend to be broad. Con Can be overly simplistic and unnecessarily rigid for more complex situations that involve multiple issues, multiple mediums, and multiple people. Case (a.k.a. Issue) The case system acts an umbrella that connects multiple ticket-like interactions. You can contact the support team and they can track your progress toward resolving your issue as you move through multiple interactions. You can think about a service center needing to track the work done on your car by different mechanics. For Lyft, this would look something like having multiple support associates help you at different stages of your driver sign up process. Pro Able to group tickets by an overall issue, making it easier to track resolution of that issue across multiple tickets. Con Could add unnecessary overhead if your support needs are fairly straightforward. Continuous Thread This system does not try to categorize why people are contacting support, instead it models after a conversation where you can easily flow through different topics. In practice, this is something like a chat thread you may have with a friend. For Lyft, all support would be one thread of conversation, regardless of why you are contacting support. Pro The most simple. Easy to understand from the customer perspective as it’s one long conversation. Con A single thread can be confusing for a more complex business that needs to work with customers in multiple capacities or have multiple issues to solve simultaneously. Each one of these paradigms has a set of pros and cons, heavily dependent on use case. The Lyft support engineering team regularly visits our customer support locations to shadow associates. We get to experience first-hand how customers use Lyft and how associates use the tools we build. These visits help us build a deeper understanding of support use cases and gather details to inform our data model designs. At a very high level, there are a few main factors that surface among the diversity of support interactions. The below exist in every support interaction, at varying degrees of intensity: Frequency of response Accuracy of the identified reason for support Simplicity of resolution Information immediately available to provide a resolution Number of topics present in a single support request Number of topic changes that happen during resolution of a support request Necessity of specialty skillsets that require assistance from other internal teams By combining each of these factors, we can represent nearly every real-world support case. For example, a user contacts support presenting one issue but it is really about another issue (#2), this issue requires more information from the user (#4), and the complexity of the issue also requires assistance from other support associates (#7). The list of possible use cases is very long and not necessary to list out exhaustively. What these seven factors do tell us is that there is huge variability to the ways people may need support, the initial reason and complexity of issues often evolve throughout the support interaction, and imposing an opinionated data model upfront restricts the ability for the system to accurately model a support experience by the time it ends. These factors point out the crux of the problem with the current solutions. So let’s throw out everything we know about support systems and get creative. Given how people actually interact with support, how might we better design the engineering architecture for these support systems? To answer this question, we synthesized months of conversation with support associates, tool admins, and other engineering teams. The common thread was flexibility in the data model . On the most basic level, the data model needs to be able to accurately model any type of support interaction that may happen and continue to be able to accurately model the support interaction throughout the duration of the interaction. Our solution was: a flexible data model. This flexible data model is designed in such a way that via code, we can model any of the three paradigms listed above, as well as model any new type of paradigm we want to invent. In fact, we can run multiple paradigms at the same time, to solve for multiple types of business needs throughout the company. We can even transition between various paradigms. For example, we can transition between a ticket paradigm to a case paradigm. And we can do this all with the same underlying datastore so we have a single source of truth. Effectively, we built a system that can model any support software that exists today and any model we may invent down the road. The opinionated ideology of this new system is that it needs to be able to support multiple support data model paradigms. And it’s the ability to be able to easily move between paradigms and invent our own that makes this model so interesting. For the technical underpinnings of the flexible data model, we chose a Relational Database Management System (RDBMS) for three reasons: Consistency of data to be represented. Ease of being able to query across individual or multiple data fields. Flexibility in defining the relations between database tables. Our implementation includes both many-to-many relationships (via an id mapping reference table) and one-to-many relationships. There are three main concepts that define our database design and relations: case, interaction, and step. A case represents an overall topic. It has a many-to-many relationship to interactions. An interaction represents an instance of two parties being connected to each other. It has a one-to-many relationship to steps. A step represents an activity stream within a given interaction. The below diagram is an illustrative example of how these concepts can relate to each other. Given these three main concepts and the ways relations can be defined between them, let’s apply them to the three main paradigms discussed in the The Data Model Problem section. Ticket A ticket is a case and interaction combo, with the activity as steps. It is a one-to-one relationship between the case (topic) and the interaction (connecting two parties). The conversation about a ticket happens as step entries. Case This is most similar to the implementation we used. It is a one-to-many relationship between the case (topic) and the interaction (connecting two parties). The conversation about a case happens as step entries. This showcases the flexibility of the database design. It would use a one-to-many relationship between the case (topic) and the interaction (connecting two parties). The case would be thought of as the entire lifespan of the user. Each interaction would be thought of as the given medium (e.g. phone, chat, etc.) of communication. The conversation through a given medium happens as step entries. You can retrieve the entire step history for a given user across the interactions and display that as the thread, adaptive to whatever medium you may be using. These are just the three main paradigms. As we can see, the way the relations are defined between the different parts of the database model can be adjusted to represent the different main paradigms mentioned above. At Lyft, we have invented our own modification of the case paradigm that will fully utilize the many-to-many relationship available between cases and interactions. This will enable us to solve for all of the different types of business cases we uncovered during our research. It fits even our most complex situations that involve multiple issues, multiple support associates, multiple mediums, multiple time periods, and multiple types of resolutions. With this ability to model different paradigms, we will be able to tailor support for the different use cases within our business. We can truly create the paradigms that fit best for the experience we want to provide. This is not something any company that offers support services has historically been able to do. By gathering as much context as possible about the support infrastructure problem space, taking into account information within Lyft and external to Lyft, we were able to design a solution that solved our current needs while planning for the future. We will continue to learn and adapt this model. By being able to point to the same data source throughout the evolution of our support product offerings, we will be able to more rapidly develop, experiment, and not be tied to any particular paradigm that exists in the industry for modeling support data. At the end of the day, we want to provide the best support experience possible— one that makes you excited to stick with Lyft. Having technology that’s flexible enough to capture multiple support paradigms helps us to provide an ideal experience in every situation. Special thanks to Jonathan Como, Adam Pallin, Sahil Grover, Shah Mansoor, Shekhar Khedekar, and the entire Support Tech team for your collaboration in creating and implementing this new vision for the support architecture at Lyft. Interested in architecting high impact support platforms and redefining what is possible in the support tech space? Lyft is hiring ! Drop me a note or just say hi at sbolton@lyft.com . Stories from Lyft Engineering. 325 1 Thanks to Megan Kercher and Alex Rafter . Programming Technical Architecture Database Design Engineering 325 claps 325 1 Written by I’m a design and growth focused product maker on a mission to make the world feel more human. Learn more: http://seanbolton.me/ Stories from Lyft Engineering. Written by I’m a design and growth focused product maker on a mission to make the world feel more human. Learn more: http://seanbolton.me/ Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-06-24"},
{"website": "Lyft-Engineering", "title": "amundsen lyfts data discovery metadata engine", "author": ["Mark Grover"], "link": "https://eng.lyft.com/amundsen-lyfts-data-discovery-metadata-engine-62d27254fbb9", "abstract": "Data Data Science Engineering Mobile Product Security This post introduces the Amundsen project — its goals and users. To read more about technical architecture, see this follow-on post . In order to increase the productivity of data scientists and research scientists at Lyft, we developed a data discovery application built on top of a metadata engine. Code named, Amundsen (after the Norwegian explorer, Roald Amundsen ), we improve the productivity of our data users by providing a search interface for data, which looks something like this: Data in our world has grown 40x over the last 10 years — see the chart below from United Nations Economic Commission for Europe (UNECE). Unprecedented growth in Data volumes has led to 2 big challenges: Productivity — Whether it’s building a new model, instrumenting a new metric, or doing adhoc analysis, how can I most productively and effectively make use of this data? Compliance — When collecting data about a company’s users, how do organizations comply with increasing regulatory and compliance demands and uphold the trust of their users? The key to solving these problems lies not in data, but in the metadata. And, to show you how, let’s go through a journey of how we solved a part of the productivity problem at Lyft using metadata. At its core, metadata is a set of data that describes and gives information about other data . There are 2 parts to metadata — a (usually smaller) set of data that describes another (usually larger) set of data. 1. A describing set of data — ABC¹ of metadata Three broad types of metadata fit in this category: A pplication Context — information needed by humans or applications to operate. This includes existence of data, and description, semantics, tags associated with the data. B ehavior — information about how the data is created and used over time. This includes information about ownership, creation, common usage patterns, people or processes that are frequent users, provenance and lineage. C hange — information about how the data is changing over time. This captures information about evolution of data (for example, schema evolution for a table) and the processes that create it (for example, the related ETL code for a table). Capturing these three kinds of metadata and using them to drive applications is key to many applications of the future. ABCs of metadata is a terminology adopted from a paper on Ground by Joe Hellerstein, Vikram Sreekanti et al. 2. The data being described Now let’s talk about what data is being described by the ABCs above. The short answer is any data within your organization. This includes, but is not limited to: Data Stores — tables, schemas, documents of structured data stores like Hive, Presto, MySQL, as well as unstructured data stores (like S3, Google Cloud Storage, etc.) Dashboards/reports — saved queries, reports and dashboards in BI/reporting tools like Tableau, Looker, Apache Superset, etc. Events/Schemas — Events and schemas stored in schema registries or tools like Segment. Streams — Streams/topics in Apache Kafka, AWS Kinesis, etc. Processing — ETL jobs, ML workflows, streaming jobs, etc. People — I don’t mean a software stack, I mean good old people like you and me who carry data in our head and in our organizational structure, so information like name, team, title, data resources frequently used, data resources bookmarked are all important pieces of information in this category. This exact metadata can be used to make data users more productive by providing them the relevant metadata on their fingertips. At a 50,000 feet level, the data scientist workflow looks like the following. At Lyft, what we observed was that the while we wanted the majority of the time to be spent in model development (aka prototyping) and productionalization, a lot of the time was being spent in data discovery. Data discovery includes finding the answer to questions like: Does this data exist? Where is it? What is the source of truth of that data? Do I have access to it? Who and/or which team is the owner? Who are the common users? Is there existing work I can re-use? Can I trust this data? If they sound familiar, we feel you. The idea for Amundsen was inspired a lot by search engines like Google — in fact, we often think of it as “Search for data” within the organization. What we present below are mocks with fake data , to give you a sense for what using Amundsen feels like. Landing page: The entry point for the experience is a search box where you can type plain English to search for data, e.g. “election results” or “users”. If you don’t know what you are searching for, we present you a list of popular tables in the organization to browse through them. Search ranking: Once you enter your search term, you are shown search results as following. The results show some in-line metadata — description about the table as well the last date when the table was updated. These results get chosen by fuzzy matching the entered text with a few metadata fields — table name, column name, table description and column descriptions. Search ranking uses an algorithm similar to Page Rank, whereby highly queried tables show up above, while those queried less show up later in the search results. Detail page: Once you have selected a result of choice, you get to the detail page which looks like below. The detail page shows the name of the table along with it’s manually curated description. The column list along with descriptions follows. A special blue arrow by a column showcases that it’s a popular column, there by encouraging users to use it. On the right hand pane, you see information about the Behavior of the table. In other words, who’s the owner, who are frequent users and a general profile of the data to see how the count of records is changing in the table over time, and you see associated tags with the table. Information like descriptions and tags is manually entered by our users, while information like popular users is generated automatically by grazing through the audit logs. The bottom of the same page contains a widget for users to leave us any feedback they may have. Clicking on a column reveals more stats about that column like below. In the above, for the integer column, the stats show the count of records, null count, zero count, min, max, and average value over the last day of data, so data scientists can start to understand the shape of the data. Lastly the table detail page, also contains a preview button, which if you have access to view the data, would show you a preview from the latest daily partition of the data, like below. This preview only works if you have access to the underlying data. We often have to strike a balance between discovery and curation. For example, if your organization had only a small number of data sets, and each of them was manually crafted by a set of Data Engineers, and each table was well named, under a well defined schema, each field appropriately named and the schema evolved in sync with how the business evolved, then your need for discovery may not be as much in such a world. However, if you live in a organization, that grew too fast, with lots of data, it’s unlikely that curation and best practices for schema design on their own are going to make your users productive. Our approach is to have a combination of both. To have a discovery (aka search) system, while also adopting some best practices about names and descriptions about schemas, tables and fields. Another important balance to strike is between security and democratization. Discovery platforms like the one described above democratize the discovery of data to everyone in the organization, while the Security & Privacy team has a mission to protect and safeguard sensitive data across the organization. The question then becomes how do you balance these two seemingly competing needs? Our approach is to divide the metadata into a few categories and give different access to each of the categories. A good way of doing so is Existence and other fundamental metadata (like name and description of table and fields, owners, last updated, etc.) This metadata is made available to everyone whether or not you have access to the data or not. The reason is that in order for you to be productive, you need to know if such a data set exists and if that’s what you are looking for. Ideally you can figure out the fit using this fundamental metadata, and if it is what you are looking for, request access. The only rare exception here is if the existence of a table or a field reveals some privileged information like the countries you operate in, in which case, it’s better to fix the data model or security model and not do security by obscurity. 2. Richer metadata (like column stats, preview) This metadata is only available to users who have access to the data. This is because these stats may reveal sensitive information to users, and hence should be considered privileged. Amundsen has been super successful at Lyft, with really high adoption rate and Customer Satisfaction (CSAT) score. It has driven down the time to discover an artifact to be 5% of the pre-Amundsen baseline. Users can now discover more data in a shorter time, and with higher degree of trust. The future as we see it lies in nailing down productivity even further by adding more features, but more importantly in unlocking a new use-case through all the great metadata already available in Amundsen — the use-case of compliance. While GDPR and newer privacy laws like the California Consumer Privacy Act (CCPA) affect the treatment of data in many ways, their provision of user data rights is one of the most impactful. Organizations must manage ways to comply with exercise of these various rights, such as those to access, correct and delete certain data. These privacy laws typically provide for certain exceptions, such as the ability to keep certain information due to legal obligations, even in the face of a deletion request. Thus far, organizations have taken a varied number of approaches to becoming compliant. Some have established manual processes for resolving the data service requests that come in, while others have gone and quarantined personal data in one location/database, so user rights management becomes easier. However, those method may fail to scale — both as the organization and the amount of data and use-cases on it grows as well as when the number of incoming data service requests grows. One approach that scales is the one powered by metadata. It’s the approach where a tool like Amundsen is used to store, and tag all personal data within the organization. Such a metadata powered solution can help an organization remain compliant as the data and its use-cases or service requests grow. Currently we integrate with Hive, Presto and any other systems that integrate with the Hive metastore (e.g. Apache Impala, Spark, etc.). These are the upcoming items in our roadmap: Add people to Amundsen’s data graph, by integrating with integration with HR systems like Workday. Show commonly used and bookmarked data assets. Add dashboards and reports (e.g. Tableau, Looker, Apache Superset) to Amundsen. Add support for lineage across disparate data assets like dashboards and tables. Add events/schemas (e.g. schema registry) to Amundsen. Add streams (e.g. Apache Kafka, AWS Kinesis) to Amundsen. With large amounts of data, the success in using data to fullest lies not in data but in the metadata. Lyft has built a data discovery platform, Amundsen, which has worked really well in improving the productivity of its data scientists by faster data discovery. At the same time, there’s a lot of value a metadata driven solution can provide in the space of compliance, in tracking personal data across the entire data infrastructure. We should expect a lot of investment in that area in the future. Stay tuned for an upcoming blog post detailing the architecture of the data discovery application and the metadata engine that powers it! Thanks to Max Beauchemin, Andrew Stahlman, Beto Dealmeida for reviewing the post. To deep-dive into the technical architecture of Amundsen, see this follow-on post . Thanks to the engineers who made it possible (in alphabetical order): Alagappan Sethuraman , Daniel Won , Jin Chang , Tamika Tannis , Tao Feng , to Matt Spiel for design, and to the engineering and product leadership of Shenghu Yang and Philippe Mizrahi . Stories from Lyft Engineering. 1.8K 10 Thanks to Tao Feng , Daniel Won , and Ryan Lane . Big Data Data Discovery Productivity Lyft Amundsen 1.8K claps 1.8K 10 Written by Writer, Engineer, Poet (mark.thegrovers.ca) Stories from Lyft Engineering. Written by Writer, Engineer, Poet (mark.thegrovers.ca) Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-12-01"},
{"website": "Lyft-Engineering", "title": "meet adi an entrepreneurial pm at lyft", "author": ["Jennie Braunstein"], "link": "https://eng.lyft.com/meet-adi-an-entrepreneurial-pm-at-lyft-b5b05f8b3803", "abstract": "Data Data Science Engineering Mobile Product Security Adi Rathnam joined Lyft almost two years ago along with the rest of the team from Kamcord , the mobile live streaming company he co-founded. Through the course of an always exciting and sleepless six year entrepreneurial journey, Adi experienced everything from building a mobile gameplay recording SDK that was on 200M phones to multiple business pivots (one that was brilliant and one that wasn’t received as well) to raising funding by driving up and down Sand Hill Road with the cheapest car on the block. Here at Lyft, Adi is a Product Lead on the Driver Experience team. We did a Q&A with Adi to learn more about his entrepreneurial background and why it’s important for Lyft PMs to be passionate about building new things. Adi Rathnam (right) along with his Kamcord co-founder Kevin Wang (left), who is now an Engineering Manager at Lyft Congrats on your success with Kamcord and bringing the team over to Lyft! How did it feel? It was a complicated mix of extreme excitement for what lay ahead, pride over having persevered for so many years, nostalgia for everything we built together with a great group of people, and some sadness that while we achieved a lot of success at Kamcord, we weren’t able to take it to the heights that we had envisioned. How did the conversations with Lyft start and what made you choose us? One of my biggest learnings from Kamcord is that if you are looking for exponential growth (for your company and for yourself), you need to be working in a market that is growing very, very fast. In 2012 when we started Kamcord, we thought mobile gaming would become more hardcore and keeping growing like a weed. By 2017, it was clear that the mobile gaming and social space were not growing as fast as we had predicted, which in turn caused our business to plateau. We were in the process of exploring some new spaces when Carlos Whitt, an Engineering Director at Lyft, reached out to chat. Two things really struck out to us in the conversations we had with various leaders at Lyft: The positive effects that transportation-as-a-service could have on the future of our cities and the environment were very obvious to see. Further, with ridesharing accounting for less than 1% of the vehicle miles traveled in the US, the market opportunity was clear. Lyft had spent a lot of time meticulously building its great culture and we could see how everyone on the Kamcord team would fit in well here from our mobile engineers to our office manager to our product marketers. Keeping the whole team together was hugely appealing to us and Lyft provided the option where all of us felt most at home. Ultimately, we started getting more excited about helping advance Lyft’s mission versus taking another crack at zero-to-one product building. What were some skills you gained being a startup CEO that are very transferable to your current role and what are some adjustments that you have had to make? Two skills from being a startup CEO that have been useful in my current role have been: getting by with limited resources and selling ideas effectively across the company. Even as Lyft has doubled in size over the last year, it always feels like we have half as many people as we need. Being able to get stuff done despite being short-staffed is a hugely valuable skill in the PM arsenal. Life on frontlines selling to customers, potential candidates, and investors prepares you well to convince a number of teams to come together to work on the big bets. Truly solving any of the largest driver or passenger problems essentially mandates cross-team collaboration and people who can get this done effectively can have outsized impact. To prepare for a product role at a larger company, I reached out to a few friends of mine who had gone through a similar transition. The advice I heard over and over again was coming to terms with the fact that you are no longer a founder / CEO who has to constantly make company-defining bets. While you should definitely speak up when there is such an opportunity, it’s often more important to understand the company objectives really well and execute innovatively, quickly, and flawlessly towards those goals. A lot of people call Product Managers the mini-CEO of their product. Do you agree with that? I think it’s pretty dangerous for PMs to think of themselves as “mini-CEOs”, as it can lead to them acting like lone wolves who have all the great ideas. If startup CEOs should use the “let’s do it this way because I said so” card sparingly, PMs should almost never play that card. Not having the power a startup CEO may have doesn’t mean everything needs to be a perfect consensus either and PMs need to find a way to set a strong vision, put forth controversial ideas for debate, have a high degree of accountability for team success, and resolve conflicts, despite being on a level playing field with their peers in engineering, design, user research etc. Seems like you had a good sense for what you were getting into by working at small startups for 4 years prior to starting Kamcord. Do you think that being a PM is good training for starting your own company and being CEO one day? There is no one path to being startup founder or CEO and there is almost no way to truly prepare for it. So, regardless of experience, definitely take the plunge when you have a compelling idea in a fast growing space with a complementary set of co-founders :) That said, being a PM might be one of the best ways to prepare for being a startup CEO. Ultimately, a startup can’t get off the ground if it doesn’t have product-market fit and the day-to-day responsibilities of a PM — having an in-depth understanding of the customer problems, working cross-functionally across many disciplines, focusing on short-term execution and long-term vision are very similar to that of a startup CEO. The PM role varies from company to company. The role at Lyft is particularly conducive towards starting your own company down the road because PMs are given a tremendous amount of autonomy and ownership over the team’s strategy. Do you have any advice to budding entrepreneurs who aren’t in Silicon Valley and don’t have investor relationships? Apply to Y Combinator or another top incubator: Y Combinator and other incubators are a great way to get wholesale access to top investors with minimal effort. Spend a couple days to fill out the Y Combinator application form — regardless of whether you apply or not, doing a few revisions of this short application will force you to describe concepts concisely and likely give you more clarity into your business. The numbers game: Unfortunately, most investments are predicated on competition between investors. To create this competition, you need to reach out to a lot of people. As a rough order of magnitude, you want to meet with as many as 50 investors in a short window of time (about 2 weeks) to drum up enough interest to close a round. To do this, go to Crunchbase or AngelList and make a spreadsheet of every investor who might invest in your company. And try to get an intro to them by any and all means possible (LinkedIn, Twitter, university alumni resources, local tech meetups). And I’m sure the PM in you will nerd out at the idea of A/B testing your emails and pitches. Get some mentors: Reach out to every second / third degree connection for help. You’ll be surprised at how much help every successful founder or investor got and most are eager to pay it forward. Just keep your emails to them short and simple with a clear ask that they can process in a couple minutes. Lyft is hiring for awesome, entrepreneurial PMs! Check out our careers page Stories from Lyft Engineering. 103 Thanks to Ryan Lane . Product Product Management Lyft 103 claps 103 Written by Product Manager, Self-Driving at Lyft Stories from Lyft Engineering. Written by Product Manager, Self-Driving at Lyft Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-08-12"},
{"website": "Lyft-Engineering", "title": "meet sravanthi one of the pms behind lyfts new rider app", "author": ["Jennie Braunstein"], "link": "https://eng.lyft.com/meet-sravanthi-one-of-the-pms-behind-lyfts-new-rider-app-9bc61abeee31", "abstract": "Data Data Science Engineering Mobile Product Security Sravanthi joined Lyft 2.5 years ago as a Data Scientist and has transitioned into Product Management. She leads the Request team, which optimizes the ride request experience. That includes helping users get through the request flow quickly so they can get on their way, and helping users find the right Lyft ride to take (which is increasingly important as Lyft expands its range of offerings to bikes, scooters, and more ). Most recently, Sravanthi worked on last year’s launch of a new rider app — check it out on Wired , The Verge , and our Blog ! The new app also won a 2018 Material Design Award from Google. We did a Q&A with Sravanthi to tell you more about how Lyft PMs partner with design, research, and analytics to launch best-in-class user experiences. You transitioned to Product at Lyft — that’s awesome! What made you want to make the switch? What I enjoyed most in Data Science was using experimentation to inform product decisions — and this skill is equally important for PMs. But I came to realize there were many other parts of the product development process that I enjoyed just as much! As a PM, I get to see the work from beginning to end, which means pairing closely with everyone on the team. I work with people across different disciplines — researchers, data scientists, support teams, designers, engineers, quality engineers, marketers, and more — to identify and solve problems, including everything from brainstorming new solutions to dealing with edge cases. In the case of the new rider app, I also partnered with other PMs on the Rider team. By working with team members from so different disciplines, I feel like I’m always learning. And each of those disciplines brings its own perspective. One of a PM’s key responsibilities is to set a vision that teammates can align around and that takes into account all of those perspectives, and that’s a challenge that I personally enjoy. Typically you see Engineers transitioning into Product instead of Data Scientists — do you have a CS background? Compared to some other product managers, I don’t have a deep background in Computer Science: I minored in CS in college, but that didn’t expose me to mobile development. In general, PMs tend to be deep in one skill and have a broad understanding of many others. The main skill set I bring is my background in analytics and experimentation, but different PMs bring their own diverse experiences to the table — as designers, engineers, project managers, and more. You don’t need a CS background to succeed as a PM, particularly if there are other areas of expertise that you can contribute to your team! (In the case of the redesign, the engineers on the team were instrumental in initially getting the project off the ground.) And beyond technical skills, the most critical skill for a PM is communication. How do you use your background in analytics in your day to day in Product? Analytics is most critical at the very beginning and the very end of the product development process; data gives us a feedback loop to make sure our product is solving a user problem. At the beginning of the development process, I’m responsible for making sure the team is tackling the most impactful problems possible, which means setting metric goals (KRs) that measure how we’re tracking against higher-level objectives and then sizing incoming product ideas against those goals. And at the end, after we’ve built a new feature, we evaluate its success against our goals using experimentation. I log into our internal experimentation tool and our team dashboard at least once a day to check in on my team’s experiments. The new app looks amazing. How did you partner with design and research? Thank you! First, research: A lot of research went into getting the new app to where it is today. And by “research,” I mean everything from usability sessions and diary studies, where users reported on their experiences with the app, to digging through support tickets and using analytics to see how users engaged with the app. We initially tested the app with a small group of users, and the combination of qualitative and quantitative inputs helped us refine the experience before we launched more widely. Quantitative research helped us find areas of opportunity: for example, we saw that users had difficulty finding other ride types in an earlier version of the app. Qualitative research helped us more deeply understand the issue, and then validate solutions that we should build and experiment with. We talked to users in-person to get feedback on why the previous design made it challenging to discover the full list of ride types and to compare that to new designs that we were proposing. Users’ reactions to the new designs narrowed our list of potential solutions before we built one and again returned to the quantitative side by running a large-scale experiment. The results from that experiment gave us the final answer on whether our solution effectively solved the initial problem. Second, design: I’m very lucky to work with a team of brilliant designers. The key to partnering effectively across PM and design has been understanding where my design counterparts and I can best contribute to problem-solving. In most cases, it’s my responsibility to come to my design counterparts with clearly-defined and high-impact problems to tackle, and my partners on the design team are the experts on crafting an awesome user experience to solve these problems, including making sure our solutions are intuitive, elegant, consistent with our design language (which was created in tandem with the new app), and accessible for all users. It’s because of their work that the new app won a 2018 Material Design Award from Google! The new rider app was one of the biggest projects Lyft has ever undertaken. What were the best and worst parts of working on a project of such massive scale? Two personal favorites: Seeing the impact of the new app on Lyft’s users and mission. The new app makes it easier for users to compare across Shared and standard Lyft rides. As Lyft works to reduce congestion so that we can design cities around people, encouraging more users to share the ride brings us closer to our vision. Making it easier to compare across ride types also sets Lyft up to offer bikes, scooters, and other modes of transportation to users within the app. The amount of collaboration that went into the new app. Given the scope of the project, launching the new app not only required input from team members across different disciplines, but also required coordination by many other teams across Lyft. This is often tricky to get right, and it was great to see teams work together to make the new app happen, particularly as we got close to launch. The worst part: we were often eager to see how the features we shipped in the new app impacted users, but we had to wait for the app release cycle to see that impact. For those unfamiliar with app release processes, for each new update of the Lyft app, we choose to first beta test internally for a period, and then we submit to Apple and Google and wait for their approval. In the real world, for most companies, that means there’s a few weeks’ delay in getting your product out into users’ hands. This is totally normal, but we felt it more with this project given multiple teams depended on the changing results and how excited we were to see them! How was it seeing so many articles about what you worked on and getting positive feedback about the new app? Did you get to work with PR? It was great! After the long test period, it was exciting to finally get the chance to share the news with family and friends. It was also awesome to hear feedback from all kinds of people (Jack Dorsey even tweeted about the new app!). As we got closer to shipping, I collaborated with Lyft’s PR team to decide how to tell the story of the new app. I appreciated the PR team’s ability to focus in on the core problems the app was solving for users. The go-to-market process gave me a moment to step back and appreciate what the broader team had accomplished. We’re hiring! Stories from Lyft Engineering. 163 Lyft Product Management Product 163 claps 163 Written by Product Manager, Self-Driving at Lyft Stories from Lyft Engineering. Written by Product Manager, Self-Driving at Lyft Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-03-06"},
{"website": "Lyft-Engineering", "title": "save money offer a better experience it can be done", "author": ["Sean Bolton"], "link": "https://eng.lyft.com/save-money-offer-a-better-experience-it-can-be-done-2f3cdbc66c42", "abstract": "Data Data Science Engineering Mobile Product Security Everyone has experienced the tension between Finance and Product: saving money is often in conflict with providing customers with a better experience. But better customer service is not a zero-sum game. At Lyft, we found a way to reduce our costs AND provide a measurably improved customer service experience by enhancing our automation in support ticket handling. Support teams often come up with ingenious workflows using a very limited toolkit; they want to provide customers with the best experience at all cost. However these workflows can be error prone and inefficient because the right tools don’t exist. Resourcing engineering effort for these workflows can be an uphill battle. Organizationally, there can be a perception that support is an afterthought of the user experience; implicitly this mentality can be summarized by “if we make the perfect product then people won’t need support.” Imagine you overcome that hurdle. What happens when you take a product engineering team and have them focus on support tooling? A lot, actually. At Lyft, the biggest win in this regard has been the introduction of automating responses to easily solved tickets by intelligently (and automatically) understanding the context of the user and their support request. When a user contacts support, the system can give an immediate response and save the user from having to contact a support agent, leading to a better experience. Not only is this great for the user but it also save the company money. In Lyft’s case, this approach has already saved tens of millions of dollars per year. Keep in mind that these are guidelines, not strict rules. Each attempt at automating a support interaction should be deeply understood, there is no one-size-fits-all solution. Track anything that may help understand the context of the interaction; don’t simply surrender everything to your 3rd party support tools. Support ticket attributes (such as contact reason, contact resolution) can indicate why a user may be seeking support, and provide critical context for exploring and understanding support interactions. At Lyft, this information resides in warehoused views for complex queries and live views for system performance monitoring. Easily queryable data makes it simple to answer business questions and build predictive models for proposed changes to support interactions. Without data, you can’t answer the questions. Consider the hypothetical dashboard below, which helps us answer two questions: 1) How many support tickets come in about cancellation fees? and 2) How do agents resolve those tickets? Digging through historical ticket data can unearth some information about ticket resolution, but only by talking to the actual people handling the support tickets can we understand the complete picture. The ultimate goal is to understand the business logic and workflows used by support agents well enough to programmatically replicate resolutions with high accuracy. We spent many weeks talking to agents and working with the customer support team to document how tickets are resolved. When researching with agents, we would pick a subset of ticket types and then observe as they solved the tickets, asking them to talk through how they determined the resolution. Our aim was to understand all the data used to resolve each ticket. After these research sessions, we were able to develop a workflow that was comprised of a series of steps. Broken down into these steps, it’s easier to develop a programmatic solution. This step breakdown should be done with the involvement of the support team, to make sure the workflow is documented correctly. Using data from our ticket resolution dashboard above, let’s look at the resolution ‘inform user of coupon info.’ Given all we know from the dashboard is that the user submitted a ticket about a cancellation fee and the resolution was ‘inform user of coupon info,’ here is a breakdown of the steps an agent took to resolve this ticket. When understood as a set of steps, it becomes much easier to provide a programmatic solution. After learning the underlying human workflow for resolving a ticket and documenting the data associated with each step, the resolution steps can be programmatically automated. Prior to introducing automation, we were sending the support request directly to the 3rd party API. After introducing automation, we immediately return a response to the client if we can provide an automatic resolution before handing off the support request to the 3rd party API. Creating this intermediate layer provides two benefits: 1) faster response to the client because it runs before invoking an external API request and 2) lower chance of failure due to syncing 3rd party data after the initial user interaction. The automation code is specific to the reason a user may be contacting support, however, all support tickets follow the same general pattern. Continuing our example about a cancellation fee ticket resolved by informing the user of the coupon details, the code (in Python) would look something like the below. Now we have code that replicates the steps from our previous research in a simple, repeatable way. The user receives an immediate support response and agent resources can be focused on more complex problems. It is not enough to simply finish a project with the goal of providing a better experience. The customer’s experience needs to be measurably better. This is doubly important for support features, compared to any other product feature. If the user is contacting support they are undoubtedly experiencing a pain point with your product. The danger of automating any kind of interaction that was once handled by a human is that it could end up feeling (even more) frustrating for the end user. The underlying emotion behind this frustration is a sense of not being understood and not having needs met. The pain of this emotion is costly. At a human level, the brain is wired to easily recall painful experiences and it requires effort to overcome that association. At an economic level, it is expensive to win back an angry customer. This dilemma of automated technology having a negative impact on human experience is not just science fiction, like cultural echoes of HAL refusing to open the pod bay doors in 2001: A Space Odyssey. Examples of this dilemma fill our world. Everyone has been through a frustrating automated phone tree with no escape (e.g. “press 1 to …”) and no chance of solving your issue. We have seen automated voice/chat assistants go haywire (like Microsoft’s Tay in 2016) or generally misunderstand our needs (like the endless memes of Siri misinterpretations). These anecdotes of automation mishaps are consistently absent from dashboards, automated reports, or data queries, which is why it’s important to qualitatively validate the improvement. Ignoring qualitative data and focusing only on quantitative data can provide a false sense of success. The best solution is to draw qualitative insights from the quantitative data. Here are some popular ways to do this: Beta program — Get feedback from a highly engaged audience that cares deeply about your product. Create multiple groups to control audience size and gather more granular feedback. For example: internal beta, small external beta, public beta, etc. User research — Watch end users go through the experience of using your product. Experiments that includes free text input — This lets you test on a small subset of the population and gather unstructured, unfiltered user feedback. Human QA team — Just as DHH (David) from Basecamp discusses it is immensely valuable to have a human QA team, not just rely on automated testing. Ignoring qualitative data can impact the product further down the road in many ways: loss of customers, decreased lifetime value, and high spending on churned users. Regardless of how you gather qualitative data, it acts as a final gut check to ensure that your product changes are creating a better experience for your users. Interested in having a big impact by building external and internal support tools? Lyft is hiring ! Drop me a note or just say hi at sbolton@lyft.com . Stories from Lyft Engineering. 100 Customer Service Engineering Software Development 100 claps 100 Written by I’m a design and growth focused product maker on a mission to make the world feel more human. Learn more: http://seanbolton.me/ Stories from Lyft Engineering. Written by I’m a design and growth focused product maker on a mission to make the world feel more human. Learn more: http://seanbolton.me/ Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-11-14"},
{"website": "Lyft-Engineering", "title": "lyft marketing automation", "author": ["Ajay Sampat"], "link": "https://eng.lyft.com/lyft-marketing-automation-b43b7b7537cc", "abstract": "Data Data Science Engineering Mobile Product Security We take pride in our mission to improve people’s lives with the world’s best transportation. More than 50 million carbon neutral Lyft rides happen every month across the US and Canada — and we’ve barely scratched the surface in the potential for rideshare. Part of our growth is improvements in our acquisition process — like launching region-specific ad campaigns that increase awareness, and consideration of our multi-modal offerings. Coordinating these campaigns to acquire new users at scale has become time-consuming, leading us to take on the challenge of automation. Acquisition is typically led by a data-driven cross-functional team that focuses on scale, measurability, and predictability. You may have seen Lyft ads like these: Acquisition operates at the top and largest part of the onboarding funnel, through the various channels listed on the left. No two channels are created equal: we work with different partners, technologies, and strategies to make sure that Lyft is the top choice for consumers. Other teams at Lyft focus on different parts of the user journey to provide a world-class experience. A high-level view is shown below. Acquiring users at scale means making thousands of decisions each day, for each region where Lyft operates: choosing bids, budgets, creatives, incentives, and audiences; running tests; and more. Just keeping up with these repeated tasks occupies a great deal of marketers’ mindshare and can lead to suboptimal decisions. It’s expensive to the business and does not scale. By automating routine decisions, we can scale efficiently and create a data-driven learning system. This also lets marketers concentrate on innovation and experimentation instead of operational activities. Our goal: build a marketing automation platform to improve cost and volume efficiency while enabling our marketing team to run more complex, high-impact experiments. Requirements: Ability to predict the likelihood of a new user to engage with our product. Measurement mechanisms to allocate our marketing budgets across different internal and external channels. Levers to deploy these budgets across thousands of ad campaigns. Marketing performance data contributes to a feedback loop to constantly nourish the reinforcement-learning system. Here are examples of problems we needed to automate: Updating bids across thousands of search keywords. Turning off poor-performing display creative. Changing referrals values by market. Identifying high-value user segments. Sharing learnings from different strategies across campaigns. And so, we created Symphony — an orchestration system that takes a business objective, predicts future user value, allocates budget, and publishes that budget to drive new users to Lyft. The Symphony architecture consists of three main components: lifetime value (LTV) forecaster, budget allocator, and bidders. Our tech stack comprises Apache Hive, Presto, an internal machine learning (ML) platform, Airflow , and third-party APIs. A light front-end feeds in business targets and launches creatives. The architecture has a lot of moving parts and dependencies and requires rigorous logging and monitoring. We dive deeper into each component below. Understanding the potential value of a user is critical to every business. The goal of this component is to measure the efficiency of various acquisition channels based on the value of the users coming from those channels. Budget can then be allocated based on the expected value for users coming from a given channel and the price we are willing to pay in a particular region for those types of users. The above diagram portrays at a high level how we calculate a user’s expected LTV while accounting for supply and demand in our two-way marketplace. We try our best to predict LTV accurately, as it helps us set mid- to long-term strategic goals. Early in the user’s lifecycle, it is hard to get a sense of their retention, rides, or transaction value, so instead of attempting to measure LTV directly, we predict it from historical data. The forecast improves as the user interacts with our services. The benchmark here represents average expected LTV from a cohort of users. These forecasts feed into the budget allocator and help it decide the value of users that come from a specific set of campaigns. The budget allocator collects marketing performance data in conjunction with LTV forecasts. Budget allocations are done using Markov chain Monte Carlo (Thompson Sampling). A curve of the form LTV = a * (spend)^b is fit to the data assuming a & b come from distributions with their own parameters (e.g. a comes from a distribution with mean μa and standard deviation 𝜎 a ). Here’s the trick — we don’t try to estimate a & b directly as you would in standard regression. Instead, we estimate the parameters of their distributions: ( μ a, 𝜎 a) & ( μ b, 𝜎 b). Consequently, instead of drawing a curve with fixed a & b , each day we sample a different estimate of a & b from these distributions — naturally injecting a degree of randomness into our cost-curve creation process. This type of random searching may seem wasteful, but modest exploration is actually optimal in the long run. It helps us explore points in the curve we would have not normally considered to converge to a global optimum. The budget allocator sends each campaign’s allocation to the respective channel bidder for deployment. Bidders publish the final changes needed to serve an ad at the target price point. Bidders are made up of two parts — the tuners and actors. The tuners decide how to deploy the capital based on available levers (e.g keyword, title, value, bid type for Google Search), while also considering channel-specific context. The actors communicate the actual bid to the internal and external channels like Job Boards, Search, Display, Social, and Referrals through API integrations. Over the years we have built relationships with our partners who help get our product in front of the right audience. Each channel, based on its level of sophistication, supports different bidding strategies. Some popular strategies are listed below. We’re constantly experimenting to set each campaign’s bid with the right strategy and update cadence in an ever-changing digital media landscape. Bidders contain a lot of channel-specific nuances that help them make the best possible decision. The bidders also have some level of recency weighting and seasonality baked in to account for market volatility. The long-term success of marketing automation at Lyft depends on incorporating human feedback into our machine learning platforms. This is generally referred to as “human-in-the-loop” machine learning, and enables machines to work on automation-breadth problems while empowering human operators to focus on knowledge-dependent problems. Without good input from the humans driving the automation engine, the quality of the models will suffer (“garbage in, garbage out”). Without the cognitive overhead of manually updating bids or allocating budgets, we expect our marketing teams to more nimbly apply audience and creative changes to campaigns. They have more time and energy to: Understand our users and their interests Ideate new ad formats, messaging & channels Form hypotheses for big shots on goals We have many exciting ideas for the continued iterations to Symphony: Always-on experimentation Incorporating seasonal effects like weather & time of day Better marketplace context to inform our bidders Intelligent segmentation & personalization With Symphony, we are achieving a higher return on our investments while saving marketer hours. The system powers a growing ecosystem of more than 30 million riders and close to 2 million drivers (based on 2018 data). Marketing automation is still in its nascency at Lyft, and while these methodologies have helped us scale so far, we will continue learning and improving as we grow. We are excited for our bright machine learning and experimentation driven future. Many thanks to the team — Alex Armenta , Jared Bauman , William Borges , Anna Campanelli , Dongwei Cao, Carolyn Conway , Ismail Coskuner , Petros Dawit , Jared Gabor , Langfei He , Robert B. Kaspar , Antonio Luna , Patrick McGrath , Usman Muhammad , Jack van Ryswyck , Alejandro Veen , Vidya Vutukuru , Xing Xing , and our cross-functional teammates. The Growth team is actively looking for talented and driven engineers, data engineers, scientists, product managers, and marketers to join our team. Learn about life at Lyft and visit the careers section for the latest openings. Stories from Lyft Engineering. 2.7K 12 Marketing Automation Growth Engineering Data Science 2.7K claps 2.7K 12 Written by Engineering at Instacart | ex Lyft & Hitachi | ajay.digital Stories from Lyft Engineering. Written by Engineering at Instacart | ex Lyft & Hitachi | ajay.digital Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-06-26"},
{"website": "Lyft-Engineering", "title": "day in the life of a lyft pm meet mo growth engagement pm", "author": ["Mohit Moondra"], "link": "https://eng.lyft.com/day-in-the-life-of-a-lyft-pm-meet-mo-growth-engagement-pm-be4a9d5f5960", "abstract": "Data Data Science Engineering Mobile Product Security My name is Mohit “Mo” Moondra and I have been at Lyft for just over 3 years. In my time at Lyft I have exclusively focused on Growth and Engagement, with much of my work around helping drivers on our platform through incentives, a great app experience, and personalized relevant communications. It has been a wild ride growing the team from 6 people to over 90 cross functional team members! I can’t wait to see what’s next for Lyft! Because I’m up swimming in the mornings, I’m usually into the office by 9:30 AM. I often read a book on my commute in. These last few days I’ve been reading The Why Axis by John List whose pioneering work in economics field experiments has led to greater understanding of human motivation and decision making. I’m also proud to say that John List is one of the prestigious economists working with Lyft today and I love that I’m able to reach out to him for advice. After having a breakfast smoothie I’m off to meetings. Today is Monday so I have team planning for our 2-week sprint cycle, most of my 1:1s with my core team, and our Growth Engagement Leads meeting where we focus on long term planning and business updates. Here’s a quick pic after sitting down with some engineers to walk through a rollout of one of our new incentive services. It has been a complicated rollout, but we are in the home stretch. Part of what has made the project a great success has been writing a strong Product Requirements Document (PRD) and having a great engineering partner write a detailed Technical Specifications Document (TechSpec). This project is going to be a win because it will help push the boundaries further on our team’s technical capabilities AND allow us to serve our drivers better with more robust incentives. After lunch, I’m off to a few 1:1s with my team. On Mondays I set up time with my different cross functional teammates in Data Science, Product Marketing, Growth Marketing and Design. Besides catching up with each other, we focus on using the time to review upcoming priorities and timelines. As a PM at Lyft, you can be sure that launching and maintaining successful products requires communication and partnership across organizations! As one of the larger teams at Lyft, Driver Engagement is broken down into several focus areas with both long term and short term plans. Keeping a team of this size in sync can be challenging! The team leads (across all our orgs) meet weekly to review top of mind issues with the business as well as raise concerns on work and resourcing. This helps us all stay focused while also being aware of larger business trends and teams’ projects. As the afternoon draws to a close I take some time to catch up on emails and respond to any urgent notifications. I ran into some of my fellow Product Managers working in the kitchen and join them to catch up and finish some individual work. Sometimes being a PM means operating solo for much of your day, but I like to work around some of my smart colleagues when I can find the time (hoping some of those smarts rub off on me :D ) Some days I have dinner at work before heading home, but usually I head out after my last few meetings. Before signing off for the evening, I hop on to check a few emails. I take a look at the calendar for tomorrow and fill out some action items for our weekly Business Health meeting with our senior executives. Tuesday’s are all about the state of the markets, so I take a look at a few company KPI dashboards to be sure I’m well informed. Off to bed and back at it again in the morning! We’re hiring! Check out www.lyft.com/careers Stories from Lyft Engineering. 84 Thanks to Jennie Braunstein . Lyft Product Management Product 84 claps 84 Written by Product Manager @ Lyft, previously Sosh, always ready to eat Taco Bell Stories from Lyft Engineering. Written by Product Manager @ Lyft, previously Sosh, always ready to eat Taco Bell Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-04-08"},
{"website": "Lyft-Engineering", "title": "from four wheels to two", "author": ["RJ Marsan"], "link": "https://eng.lyft.com/from-four-wheels-to-two-403bcf1cbf59", "abstract": "Data Data Science Engineering Mobile Product Security In September of 2018, Lyft launched its first scooter service in Denver and Santa Monica. Creating a new service like this was a major undertaking, in part because we wanted our riders to be able to ride away with their existing Lyft app — no new downloads or setup required. Throughout the development process, we built towards a successful release by maintaining three key engineering principles: Stay simple and lean Reimagine over reinventing Launch what matters These principles helped us ship fast yet safely, enabled us to iterate quickly, and can help guide your product development as well. In order to successfully build new experiences, we had to lay a foundation for development that minimizes risk to the core product. One best practice we enlisted is to use a Feature Flag — a set of conditional checks which limit the visibility of a feature. They are an excellent way to control if users are exposed to a new experience or UI and to understand how a feature performs through A/B testing. Since feature flags are checked at runtime, however, they don’t provide inherent safeguards against side effects; a feature may not be visible, but if the code interacts with other layers of the app, like data or business logic, it may inadvertently affect the core user experience even when turned off. We took this principle one step further while building Lyft Scooters and utilized feature modules in addition to feature flags. Lyft’s mobile components are already modular to isolate critical parts of the product. By forcing ourselves to be compilation-level isolated, we were never tempted to use abstraction-breaking checks to support our feature outside our module. We were required to be explicit with how we interfaced with the rest of the codebase. For our initial launch, we settled on a single button on the home screen and a polling component. Minimizing the surface area connecting our experimental feature to the pre-existing app provided creative constraints to our user experience development while ensuring stability in our architecture. “Don’t polish your trophy case until you win some awards” Every new feature is a chance to start with a clean slate, and it’s often tempting to immediately build for scale. We all want our products to launch to massive fanfare and usage, but more often than not, the path to success for new features is slow and steady. With steady growth in mind, we designed our first architecture to support exactly what’s needed for our first product iteration, and nothing more. For Lyft Scooters, we cut out many features one might expect from a classic ride such as sharing ETA or setting a destination. The bigger the feature, the more likely it takes shape in unexpected ways. For engineers, that means we don’t know the optimal abstraction layers until we observe real world usage. As my coworker said: “ Put concrete walkways down where people have killed the grass by walking too much. ” We were tempted to represent the in-ride and post-ride experiences as a complex set of states, building in future-proof flexibility, but instead we modeled it as a single state machine with different UI screens corresponding to a unique server-driven state. We polled a single endpoint and felt confident that we could reasonably predict what the app would do in any given situation. By keeping the architecture simple, we ensured it could be built within the allotted timeframe. Discussions with product stakeholders about what our product could readily do were grounded in this architecture—and thus we could establish tradeoffs if they wanted something outside these limitations. Lyft Scooters offers people a different way to ride, but it still shares common app components and server infrastructure, including integration with the existing Lyft passenger app, user profile, payments and more. Even though these components were complicated to integrate, we chose to leverage this well supported infrastructure across the app. Our fastest and least regretful path to launch balanced all these integration options carefully. “Is it faster to rebuild this or reuse this, and what will we regret later?” This question comes up a lot when building new features: is it quicker to rebuild or reuse? When timelines are tight, we tend to rebuild hoping to stay small. When we anticipate a complex launch, we tend to reuse hoping to leverage existing scale. With Lyft Scooters, we designed the UI to reuse core elements from our existing experiences. As we built our feature, Lyft was investing in its Mobile Design System called the “Lyft Product Language.” The LPL is a comprehensive library of UI elements, designed and developed for usability, consistency and accessibility. It also gave our product designer clear guidelines for what we could easily reuse. We established that any design made with LPL components that respected their usage guidelines could be built magnitudes faster than custom designs. LPL-compliance supercharged our UI development time: we needed little-to-no instruction or follow-up from design over wireframes or specs, spacing and font sizing. Time intensive aspects of mobile UI development like loading, disabled states and accessibility were included in the LPL as well. “What can we build off of in this app?” Some components weren’t coded with reuse in mind, but our modular codebase allowed us to treat it like a toy box, repurposing existing elements for use in Lyft Scooters. For example, we needed a way for riders to verify their drivers licenses. Our product manager pointed out, “ Doesn’t the driver app have a way to scan drivers’ licenses? ” We discovered BarcodeView and, after a quick chat with the owner of the module, we were able to wrap the view in a UI Component that could be dropped into our feature in a matter of hours. It’s hard to imagine writing this feature from scratch in that timeline. Not all features can come to life with creative refactoring and reuse. The Scooters home screen featured a map of all nearby scooters, clustering when appropriate. The Lyft app abstracts away our map implementation, which means we didn’t have immediate access to Google map markers or their clustering utility. Implementing clustering the “right” way would have required a careful balance of exposing different aspects of the Google Maps SDK with our abstraction layer and finding a way to render our scooter markers in a separate pipeline from all other markers in our app. Instead, we bit the bullet and utilized a naïve implementation of map clustering. Quadratic complexity? The computer scientist in me was angry, but when the datasets are small enough, reasonable tradeoffs can be made in the short term without sacrificing the user experience. When choosing the “rewrite” approach, it’s important to be confident that the code will stay simple and easily explainable. In this case, the algorithm wasn’t perfect, but it worked reliably and quickly. Throughout the entire process of building Lyft Scooters we faced questions about the product that we didn’t have answers to (e.g. if users would understand how to reserve a scooter, if they would find our feature from the home screen, if they found it useful). Launching and iterating on our best guess was the way for us to answer these questions and measure the usefulness of what we were building. Smaller features have the luxury of A/B testing, iterative development, and alpha releases—but we had to understand what we could build with foundational UX research, without our internal “product compass.” We anticipated that our biggest insights would come first time users. To prepare for rapid iteration, we added as many hooks as possible so we could tweak the experience on the fly. Lyft’s mobile infrastructure was already built with reliable testing and configuration flexibility, and we relied on it to remotely customize the content and design. Leading up to the launch, we stress-tested the product. On launch day we were on the ground, asking users about their experience, internalizing their feedback, and iterating on what we observed. Five months later, Lyft Scooters are now live in nine markets! Countless days of code, sweat, and preparation has paid off, but these bets are never certain. Creating bold, new features inherently comes with risk and learning opportunities, but these challenges should never stop anyone from trying! Stay open and curious, and keep in mind the three principles: stay simple and lean, reimagine over reinventing, and launch what matters. Want to hear more? Listen to my talk at Lyft’s Android Tech Talk series in October 2018. We’re hiring! lyft.com/careers Stories from Lyft Engineering. 979 1 Thanks to Alex Rafter , Elaine Chow , Rebecca Powell , and Ethan Goh . Android Android App Development Scooters Engineering Mobile 979 claps 979 1 Written by Android product engineer at Lyft Stories from Lyft Engineering. Written by Android product engineer at Lyft Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-02-11"},
{"website": "Lyft-Engineering", "title": "android happy hour lyft hq", "author": ["Alexey Zakharov"], "link": "https://eng.lyft.com/android-happy-hour-lyft-hq-4dbff58c2f7c", "abstract": "Data Data Science Engineering Mobile Product Security Are you an Android developer? Will you be in San Francisco at 6:30pm on Tuesday, October 24th ? Join us for a night of food, drinks, networking, and tech talks with Lyft’s Android engineers! RSVP here: https://lyftandroidhappyhour.splashthat.com/ Lyft works at the forefront of Android development, with applications that span millions of Android devices. And we love sharing that work with the broader tech community, which is why we’re hosting an Android Tech Talk at 6:30pm on October 24th at our SF offices. Come hear three of Lyft’s expert Android engineers dive into topics central to our development efforts. Check out previews of the talks below: Kathy Ma will introduce Lyft Shuttle, which recently launched as a beta program in San Francisco and Chicago. Lyft Shuttle aims to be a sustainable and affordable commuting solution — through building density along popular routes and filling higher occupancy vehicles, this feature can unlock lower per-ride price points for passengers and increase vehicle efficiency for drivers. Kathy will present the technical and product challenges of building this new product on Android while working with a cross-functional team (whose responsibilities include passenger and driver experiences, route generation, matching, dispatch, engagement, growth, and overall strategy). Pierce Johnson will cover an overview of integrating with the Lyft Amp, a Bluetooth LE embedded systems hardware device developed by Lyft and used by Lyft drivers. The device connects to the Lyft Driver app to display unique animations and information to drivers and passengers on two LED screens. The Amp can greet a passenger when the driver arrives at the pickup location, display the ETA for the ride’s destination, display the Seahawks colors on game days, and much more! While the product itself is unique to Lyft, the core problems and challenges of building embedded systems into Android applications are not. Very low level programming is unfamiliar to most mobile developers, but may become a larger part of our lives in the ever expanding network of connected devices. This presentation will dive into development processes and architectural components and decisions required for integrating bluetooth-enabled hardware into an Android app by using Lyft’s own embedded systems device — Lyft Amp — as an example. Originally Lyft was a single app where users could switch between driver and passenger modes to give and receive rides. There were initial advantages to having one app with multiple modes, but after time this bloated the app size and significantly slowed iteration on creating and releasing new features. Splitting these two modes (one into a Passenger app and one into a Driver app) could be done many ways, but we settled upon using the modularization of various aspects of the code for the task. Ryan Tempas’ talk will discuss the architecture and execution of this modularization choices made by Lyft in order to release the standalone Driver app. Interested and free at 6:30pm on Tuesday, October 24th? RSVP here: https://lyftandroidhappyhour.splashthat.com/ After the talks there will be an opportunity to ask questions and mingle with other Android enthusiasts. So if you’re around SF, swing by! Stories from Lyft Engineering. 86 Thanks to Ryan Lane . Lyft Android Android App Development AndroidDev Mobile 86 claps 86 Written by Software Engineer at Square Stories from Lyft Engineering. Written by Software Engineer at Square Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-10-13"},
{"website": "Lyft-Engineering", "title": "interactions in fraud experiments a case study in multivariable testing", "author": ["Hao Yi Ong"], "link": "https://eng.lyft.com/interactions-in-fraud-experiments-a-case-study-in-multivariable-testing-e0525b11751", "abstract": "Data Data Science Engineering Mobile Product Security A while ago we observed something curious when we ran a set of simultaneous A/B tests around multiple antifraud features. These tests were to improve our passengers’ ride payment experience and our ability to collect fares to pay our drivers. The features centered around the temporary authorization hold we use to determine if a passenger has enough money for a Lyft ride. In general, we apply an authorization hold (or auth) to ride requests that get flagged by our system of business rules and machine learning models. While the auth decreases our fraud exposure, it increases passenger churn due to side effects such as banks not promptly releasing the credit held on the payment method. In rare cases, auth releases can take up to a week, which understandably frustrates our users. The goal of these tests was to minimize fraud loss without sacrificing good user experience. Suppose that prior to these tests Lyft would auth users at most once per week. In one of these tests, we auth all system-flagged users if they have not been auth’ed within the past day. This was a user-split A/B test that we’ll henceforth refer to as the 24HoursTrust test. (Note: The information in this paragraph is not entirely true — the actual product and experiment variables were different and more nuanced. Did you think we’d simply show our hand? 😎) Intuitively, because of higher user friction from auths, we expect that there would be a relative increase in user churn in 24HoursTrust’s treatment group compared to its control group, where we auth users at most once every week. Surprisingly, we saw a statistically significant decrease in user churn rate in the test. This got us scratching our heads for a while. What happened? (Spoiler: After figuring out what the issue was, we revamped the experimental design and learned that what we intuited was right — user churn did increase with increased auth frequency.) Introducing payment fraud at Lyft Fine-tuning the auth experience Designing bad auth experiments Designing good auth experiments The cause of the surprising result above can be somewhat obvious to the careful reader. But let’s take a step back and get some context around this problem. Ultimately, we’d like to understand the cause of this issue and how to better design experiments. Like many consumer-facing online services, Lyft faces the risk of fraudsters who use stolen credit cards to pay for rides. Often, these bad credit cards don’t have enough money and result in failed transactions for rides. Lyft protects our drivers from passengers who took rides but can’t pay. But this protection also means that we can incur significant losses if we don’t adequately defend ourselves. As defense against fraudsters who don’t have money to pay for rides requested, Lyft may contact the user’s bank (e.g., credit card issuer) to put a temporary authorization hold and confirm the payment method. Unless the ride is completed, the authorization hold (or auth) never processes and may show as a pending transaction on the bank statement. The side effect of auth’ing the user is, unfortunately, the inconvenience of not being able to use the credit held out by the bank while the transaction is pending. Additionally, passengers can misinterpret auths as actual charges because of their banks’ confusing interface and become frustrated with what seem to be Lyft charges. There are many aspects of the auth that Lyft actively tweaks to fine-tune the user experience. One is simply how often we auth a user: If we had recently determined that a passenger has sufficient funds for rides, we shouldn’t need to challenge their credit-worthiness so soon, again. The above set of tests challenge the assumption that one week is how long we can trust an auth’ed user after a successful check. On the one hand, not challenging our passengers as frequently can reduce overall user friction. On the other hand, clever fraudsters that notice this business logic can take a cheap ride followed by an expensive one that results in fraud loss. The trick is finding the sweet spot to operate at. So why did we observe the counterintuitive result of decreased user churn despite increased user friction? To see why, we’ll have to consider the combined impact of the different tests we ran concurrently. As before, we have the 24HoursTrust test, which auths all flagged users if they have not been auth’ed within the past day. On top of that, we have the 1HourSubsetTrust user-split test, which auths a subset of the same users flagged in the 24HoursTrust test, except that it auths them if they have not been auth’ed within the past hour at ride request. (Not too important, but this subset considered in 1HourSubsetTest is just the “most suspicious users” within the 24HoursTrust test’s subjects.) Notice the two key differences here: 1HourSubsetTrust’s one hour versus 24HoursTrust’s one day post-auth “trust period.” 1HourSubsetTrust considers a subset of system-flagged users but 24HoursTrust considers all system-flagged users. Crucially, this setup means that the flagged users in the 1HourSubsetTrust test formed a sizable, strict subset of the flagged users in the 24HoursTrust test. In fact, when we checked, it was more than half of all system-flagged users. At this point, the key insight reveals itself: Many users in the control group of the 24HoursTrust test were simultaneously users in the treatment group of the 1HourSubsetTrust test. Because of this peculiarity, the overlapping 24HoursTrust control users can be auth’ed up to once every one hour. As a result, they ended up experiencing more instances of pending auth charges and greater user friction than those in the 24HoursTrust treatment group, where users cannot experience more than a single auth in a contiguous 24 hours period. That there is a significant overlap between user assignments between two product factors that interacted escaped us in planning the experiments. We simply rarely encounter tests that strongly interact! And while this is no excuse, contemporary literature would also have us believe A/B tests rarely coincide in practice [ 1 , 2 ]. Normally, our oversight wouldn’t pose a big problem — we can simply consider all combinations of test treatment/control groups separately and evaluate how each performs relative to one another. In our case, however, we had carefully sized the experiment to guarantee a level of statistical power and confidence in our experiment. Sizing for statistical power and confidence translate to predetermining the experiment length and user bucket proportions to ensure some minimum probabilities of correctly rejecting and retaining the null hypothesis, respectively. That is, correctly rejecting or retaining the hypothesis that a treatment or a combination of treatments will not improve our metrics. For the hypothetical pair of A/B tests above, we could have had enough samples for a somewhat lower but reasonable power and confidence. In our actual, full set of tests, however, there were a lot more interacting variants than the four described above. In fact, there were so many interacting features in our tests that the total number of treatment variant combinations was an order of magnitude higher than just the number of treatment variants. The sample sizes in each group were therefore far from sufficient to make any causal inferences. To overuse an over-quoted quote, To consult the [data scientist] after an experiment is finished is often merely to ask [her] to conduct a post mortem examination. [She] can perhaps say what the experiment died of. — paraphrasing R. A. Fisher, 1938. Unfortunately, our flawed experiment design resulted in data that was useless. A good set of experiments must capture the strong interactions between experimental factors and the influence of pairing various factors together. It is thus infeasible to simply run concurrent tests (as we have) or sequential tests. It’s clear from our semi-fictitious example above that the former didn’t work for us. In the latter case, there’s no guarantee that sequentially running tests and greedily choosing the best treatment variant will lead to the best combination of experimental factors. In our revamped auth experiments, we finally settled on a sequential set of multivariable tests (MVTs). MVTs allow us to include more than a single factor and estimate interaction between them. In our MVTs, we tested different combinations of post-auth trust periods (as above), auth amounts, and so forth to improve the passenger’s experience. And while we could simply run a giant MVT, we had to split them into a non-overlapping sequence of MVTs because of experiment sizing requirements. Recall that since there is only a fixed rate of samples coming in, the numbers of samples in each variant decrease due to the increased number of variants. This implies that we must run an MVT longer than if we just had a simple A/B test. A more nuanced problem that arise in analyzing MVTs is that of multiple comparisons. In a traditional A/B test, we need only compare the treatment with the control variants for effect. In MVTs, however, we are not only comparing each treatment with the control, but also each treatment against one another to find the best combination of factors. To see this, consider 10 treatment/control variants’ 95% confidence intervals (bounds that include the true population parameter of interest with 95% probability). If we wanted to pick the top treatment variant, there are 45 pairwise comparisons being made and the expected number of intervals that don’t contain the null result is 2.25. That is, we expect more than two of the relative performance metrics between variants to be wrong. Here’s another way to think about it: If the intervals are statistically independent from each other, the probability that at least one interval does not contain the population parameter is 90.1%. Note that the intervals here refer to the 95% confidence intervals for the differences in the treatment/control variants’ impact. In other words, we’re more than 90% sure that at least one of the pairwise comparisons wouldn’t yield a correct result, which can lead to an incorrect decision about shipping the best treatment variant. It’s thus crucial to factor in something like the Bonferroni correction to counteract this problem. Specifically, the Bonferroni correction compensates for an increased likelihood of making the wrong decision by testing each individual hypothesis at a significance level of alpha divided by the number of hypotheses, where alpha is the desired overall significance level. Here, each individual hypothesis’s 95% confidence intervals would need to have a 100–5/45 = 99.9% coverage, which means we need even more samples to shrink the confidence intervals enough to make reasonable decisions. The sequence of MVTs we designed balanced the ability to test many factors simultaneously against maintaining a minimum sample size and hence statistical power and confidence in our results. Factors that we had strong reason to believe would interact were grouped together and tested in multiple tests. Specifically, we ran full factorial MVTs (all combinations of factor levels) because the interactions were important. In the end, we were able to reconcile our revamped experimental results with intuition about how the auth experience improves with different features. If you enjoyed this post, follow and recommend! Experimental design is a rich field and there is good literature on planning efficient tests. An example is Taguchi’s fractional factorial design , which uses subsets of full factorial designs that allow experimenters to maintain some sufficient resolution to estimate treatment effects and interactions that are of interest. Also check out our other data science blog posts: Experimentation in a Ridesharing Marketplace: Interference Across a Network Experimentation in a Ridesharing Marketplace: Simulating a Ridesharing Marketplace Experimentation in a Ridesharing Marketplace: Bias and Variance Build Your Data Science Team Like a Swiss Army Knife In SF and interested in hearing more about data science at Lyft? We’re hosting the SF Big Analytics meetup on Wednesday, October 25th at Lyft HQ — please join us if you can! And, of course, Lyft data science is hiring! If you’re interested in developing experiments or building the artificial intelligence that powers them, read more about our role and reach out to me ! Stories from Lyft Engineering. 200 1 Data Science Lyft Ab Testing Fraud 200 claps 200 1 Written by Research Scientist at Lyft Stories from Lyft Engineering. Written by Research Scientist at Lyft Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-07-03"},
{"website": "Lyft-Engineering", "title": "awesome tech specs", "author": ["Black Queen of Tech"], "link": "https://eng.lyft.com/awesome-tech-specs-86eea8e45bb9", "abstract": "Data Data Science Engineering Mobile Product Security Consider the following nightmares. You’re about to launch a new feature your team has spent weeks on. When you try to verify in staging, nothing works as expected. After digging into the problem, you discover that your feature relies on a service that another team recently deprecated. While trying to integrate client and server work for a new feature in your app, you discover that the client engineer has been working off of an outdated tech spec, nullifying weeks of their work. After announcing that your team is working on a new feature, you’re flooded with requests to widen the scope of the project. Bogged down by new requirements, you begin to wonder whether the feature will ever make it into production. What do these nightmare scenarios have in common? Each could have been prevented by an awesome tech spec: a document, usually written by an engineer, that describes how a feature, project, or service will work from a technical perspective. The very idea of a tech spec can seem contrary to the Silicon Valley ethos. Move fast — break things — rapidly iterate — be a doer. Why spend time writing, disseminating, and updating a tech spec when that time could be spent actually building the damn thing? But tech specs have more utility than most engineers realize. A thoughtful, well-written tech spec unlocks many benefits, such as: A thorough tech spec exposes broad ideas (and often low-level implementation details like endpoint names and error codes) to a wide audience, maximizing the chance that a bug or regression will be caught sooner rather than later. There are always unexpected bugs, but a good tech spec can eliminate the vast majority by getting many sets of eyes on your proposal. Your tech spec serves as documentation both during feature implementation and after feature launch. During feature implementation, it specifies exactly what work needs to be done. After launch, it helps uninformed engineers get up to speed quickly on the inner workings of a feature and the tradeoffs involved. Of course, documentation is only as useful as it is accessible. At Lyft we have a collection of tech specs — that any engineer can view and contribute to — organized under a mailing list. Many engineers use it to solicit feedback, raise awareness of new projects, and collaborate across teams. Others use the collection to better understand existing features and services. Its high visibility and transparency allows even the most junior engineers to build with confidence. Reaching consensus on feature design and implementation in the tech spec stage means less contention down the road — invaluable when you’re under a crunch, trying to get a feature launched. After launch, the tech spec serves as a valuable reference guide where stakeholders can quickly locate accurate information — like why certain implementation decisions were made, what the project scope is, and how it integrates with other platforms and services. After reading this guide, you’ll be equipped to author fully-realized tech specs that take your features and team to the next level. You’ll thank us later! The Pareto Principle — that only 20% of input generally results in 80% of output — quantifies what most people intuitively understand: some uses of time are more efficient than others. The same rule applies to writing tech specs. Spending your time and effort wisely will pay outsized dividends later. A well thought out tech spec is a tool that works on your behalf , making your job easier and your feature better. It has a purpose — improving intra-team communication, for example, or anticipating and addressing stakeholder concerns. A tech spec without a purpose? It’s a waste of time. To maximize the utility of your tech spec, define its purpose before you start writing. Ask yourself, “What do I hope to achieve through this tech spec?” Making this decision beforehand streamlines the writing process and ensures the spec will have value to its readers (and, therefore, to you). Your answer will be the foundation of your tech spec, determining attributes like technical detail. This grid lays out several common purposes for tech specs, and how those purposes are reflected in the final tech spec: As your project and purpose changes, your spec will too. The early stages of a project may call for a high-level spec designed to get buy-in from stakeholders. After buy-in is achieved, you might transition your spec to a lower-level document that outlines all the engineering work needed to complete the project, including specific APIs, errors, and analytics. Although every tech spec looks different, starting from a template allows you to take advantage of known best practices. Here we’ll introduce a loose template for tech specs by walking through the spec for a hypothetical project called Spot the Bot — a Twitter bot that will tweet cute pictures of puppies. Here’s what the header section for a Spot the Bot tech spec would look like at Lyft: If applicable — e.g., for frontend or client work — add a primary mock or screenshot at the top of document so readers can get an intuitive, visual sense of the project. This is your tech spec’s abstract: the who/what/when/where/why of your entire proposal, made succinct. Spot the bot is a twitter bot that tweets pictures of dogs at predefined chronological intervals. The dog images are retrieved via a GET call to Dog API. Contextualize your project: why build it? What is the motivation? What user problems are you attempting to solve? What previous efforts, if any, have been made to solve this problem? We are looking to expand our brand imprint within the millennial segment. Spot the Bot will target the millennial audience by providing instant access to high-quality, curated dog pictures. We will differentiate ourselves from the competition by delivering higher-quality pictures. Highlight all the outcomes that you predict will result from your work, both purposeful and inadvertent. This is especially important when contributing to a large ecosystem of services and code. This section, along with the Measurable Impact section, are the yardstick by which you’ll evaluate the success of your project: “Did we achieve our intended goals and impact?” - Disrupt monotonous Twitter feeds with fun, unexpected, and “on-brand” images. (subjective) - Introduce millennial users to our brand with relevant and engaging content. (subjective) - Integrate with Twitter (to automate tweeting) and Dog API (to pull puppy photo content) (objective) The concept of a “non-goal” can be unintuitive and confusing. Here’s how it’s defined by Lyft’s internal tech spec guide: “A non-goal is something you are intentionally not doing or solving with your project, even if it could be related. Defining non-goals helps limit the scope of your project and prevents feature creep.” Just like goals, non-goals should be eminently readable (we suggest bullet points) so they immediately jump out to reviewers. Burying your non-goals deep in your tech spec virtually ensures that a casual reader will try to widen your project’s scope. - Disseminating dog pictures via another platform (i.e. facebook, instagram) - Creating or hosting an in-house dog photo database (we will instead take a dependency on Dog API) - Configurable “post times” — in v1, dog photos will be posted at a hardcoded chronological interval. This will be the longest part of your tech spec; it is the product of most of your research and preparation. In this section, you’ll describe your engineering approach. If you haven’t decided on a single course of action to accomplish your project, list the approaches you’re considering; this will enable your reviewers to help you choose. Your level of detail will depend on your tech spec’s purpose and your intended audience. Make it descriptive enough to encourage productive suggestions! This is also a good place to include illustrations that show how your project interacts with existing systems. This may mean a flow chart representing the user experience…or a diagram showing data flow through different services and databases. If your tech spec covers low-level topics, consider including HTTP response codes, JSON request/response snippets, and error names. This section defines specific metrics that you expect your project will impact; they should map directly to your goals. At Lyft, engineers collaborate with data scientists and product managers to define these metrics; we are fortunate to have robust data pipelines and analytical tools to facilitate this process. In the early stages of a project, these metrics can help answer difficult questions around prioritization (“Given our finite engineering resources, which feature is more important to build?”). After launch, they become means of gauging the project’s success and identifying areas for improvement. - Reach 2K twitter followers within 2 months of launch via cross-account and cross-platform promotion. - At least 50% followers are non-bots; at least 20% are age 18–35 If the project is external-facing, list the ways in which malicious users could exploit your change. Highlighting the risks of your project can be anxiety-inducing; what if reviewers criticize you for introducing all this risk to the system/product? Still, it’s important to solicit these critiques so that reviewers can pose questions and solutions that will ultimately make your feature more robust. Discuss approaches you considered (but ultimately decided against). This serves as a form of documentation and can also preempt suggestions from reviewers to investigate approaches you’ve already discarded. Spot the Bot Email Bot: emails instead of tweets. Decided not to implement because it doesn’t scale well, and user demand is low. To keep your project on track, divide all work into key accomplishments and assign estimated dates. - Dog API integration complete: October 14th - Post interval configurable: October 17th - QA complete: October 21st Make this section a source of truth — if a milestone is pushed back, note it here. Keeping milestones up-to-date can prevent misunderstandings about the project’s status and help ensure that the project ships in a reasonable time frame. This is especially useful for larger teams and organizations. List unresolved design or implementation questions, along with an invitation for reviewers to give feedback. Feel free to call out specific people who may have valuable input. We’ll start by tweeting once ever hour, on the hour. Can someone on Data Analytics confirm that this rate will maximize our audience? Polished tech specs are more likely to get timely, thorough, and useful feedback from reviewers. To that end, it’s a good idea to ask a teammate for an initial review before distributing your tech spec widely — they can catch any glaring errors or omissions. Then send your tech spec to wider audiences: first your team, then a stakeholder, and finally all interested parties. With each round, your spec becomes more refined (and more appealing to reviewers). Most comments warrant a response, so be sure they have one, whether from you or another reader. When an issue is resolved, note its resolution explicitly in the comments or in the tech spec itself. Engage with your readers and reviewers; they want to help you build the best possible product! If you find yourself at a point where comments are no longer helpful, it may be time to close your tech spec to feedback in order to reach a version of the spec that you can start to build on or ticket up. Any non-critical feedback after this point can be incorporated into a v2. A tech spec is a living document, and it’s important for the reader’s sake — and for yours — that you date every major change you make. This allows readers to quickly assess your tech spec’s relevance and identify changes. We hope this has convinced you to join the minority (but soon to be majority) of folks who invest time into crafting good tech specs. Why? Because tech specs are an upfront investment of time and effort that in the long run can create exponential payoffs for you and your team. Use them well. This post was written by two software engineers at Lyft, @blackqueentech and @chloerevery . We encourage you to contact us with questions or feedback in the comments. Like what you’ve heard? Join our team and help build the future of transportation! Stories from Lyft Engineering. 3.6K 6 Thanks to Chloe Revery . Agile Software Development Process Engineering 3.6K claps 3.6K 6 Written by Or Della Anjeh. Stories from Lyft Engineering. Written by Or Della Anjeh. Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-10-29"},
{"website": "Lyft-Engineering", "title": "envoy joins the cncf", "author": ["Matt Klein"], "link": "https://eng.lyft.com/envoy-joins-the-cncf-dc18baefbc22", "abstract": "Data Data Science Engineering Mobile Product Security Today I am thrilled to share that Envoy is joining the Cloud Native Computing Foundation ( CNCF ), home of Kubernetes , as its 11th hosted project. Lyft originally announced Envoy almost exactly one year ago on September 14, 2016. What an incredible year it has been! In this post I will briefly recount the history of the project and how we arrived at this momentous day. I joined Lyft in May 2015. At that time Lyft had already begun its journey from a monolith to microservice-based architecture, having deployed more than 30 services. Although Lyft was already experiencing many of the organizational benefits of parallel and decoupled development, the new architecture also brought challenges. Primarily, Lyft developers were facing the reality that the network is inherently unreliable; when problems occurred it was nearly impossible to determine the source of the issue. Physical network? Virtual network? Hardware? App? Who knew? During this era, Lyft developers were unwilling to move mission critical functionality out of the monolith because of the perceived instability of our burgeoning microservices-based infrastructure. If Lyft was to realize the complete benefits of a fully distributed architecture it was imperative that we directly tackle microservice networking and observability. Before Lyft, I had the opportunity to work on large scale distributed networking for many years both in Amazon’s EC2 as well as Twitter. I had observed how different organizations attempted to solve the general problem of distributed networking. At Twitter specifically, I saw the pros and cons of using Finagle for all service to service communication. At the same time, I led development of a new C++ edge proxy that continues to serve all of Twitter’s traffic today. The power of libraries like Finagle and the Netflix OSS suite cannot be understated. They provide a rich set of distributed systems networking functionality such as service discovery, load balancing, retries, timeouts, circuit breakers, protocol translation, stats, logging, tracing, etc. All of which aim to make the network transparent to application developers. However, the library based deployment approach was problematic at Twitter even though nearly 100% of services ran on the JVM; an update to Finagle might take months to fully rollout as service owners slowly upgraded and deployed. At Lyft we wanted to replicate and expand on the feature set of Finagle, but move the functionality into a self-contained process that was easier to iterate and deploy. Lyft — like many modern companies — also has services written in many languages, making the out of process proxy approach even more critical so as to avoid replicating every feature in many different libraries. Additionally, from my work on edge serving systems, it was clear that performance, and in particular minimizing tail latency variance, is critical for distributed networking components. And thus, after investigating existing open source options and determining there was no good fit, Envoy was born. For performance reasons, modern C++ was chosen as the implementation language and development began in May 2015. The MVP was completed and deployed in early September 2015. Initially, Envoy was used as Lyft’s edge proxy. Iteratively, our small but growing team added features and also began to deploy Envoy as our sidecar service-to-service proxy. By the early summer of 2016, Envoy was fully deployed at Lyft and being used for all edge and service to service networking, forming a mesh between over a hundred services and transiting millions of requests per second. Perhaps most importantly, Lyft engineering had moved into a phase where developers trusted the networking abstraction that Envoy provides . Critical functionality was being moved out of the monolith at an increasing pace with little concern given to overall network stability and observability. Lyft’s business is almost entirely based on open source technology. Without it, it’s unlikely that the ridesharing service we know and love would exist today. Given the large development effort that had gone into Envoy, and understanding that many other organizations face identical challenges when moving from a monolithic to microservice architecture, we wanted to give back to the larger community that had nurtured our own company growth. Therefore, we decided to proceed with open sourcing Envoy and working to build a community around it. I won’t fully recap what happened in the months after open sourcing as I already wrote about it . I also won’t spend a lot of time discussing in detail why I think Envoy has seen such tremendous growth in the past year (the previously linked post discusses that as does my recent post on the universal data plane API ). I will, however, mention that all of us at Lyft have been both amazed and humbled at the industry-wide reaction over the past year. The number of large organizations that are Envoy users continues to grow, along with the number of products being built on top of Envoy as a fundamental technology. It would have seemed incomprehensible a year ago to think that Envoy might become a foundational networking technology of the modern Internet. Yet, a year later, it seems conceivable that the project is on that trajectory. The incredible growth of the Envoy community has not been without challenges. The trials and tribulations of maintaining a successful OSS project have been getting more attention lately as it is a tough and often thankless job. Although Google has been an incredible partner in the past 9 months (we now have several Google maintainers!), I still have at times felt that the project was being held back by my lack of time to fully focus on things like marketing, community organization, documentation, developer outreach, etc. Because of this, we started investigating contributing Envoy to a foundation that might help share the maintenance load across the entire breadth of things that need doing along with helping us grow. Additionally, the right foundation would also help advance Envoy’s technical goals with close synergies to existing hosted projects. After researching the options, many discussions later, and a formal application process and vote, we have found our home in the CNCF. Lyft is extremely excited to contribute a technology that has been critical to the stability of our infrastructure during our massive growth. We hope that Envoy will help many other organizations as much as it helped us. The CNCF offers a staff of experts on all things open source who can help with project best practices, as well as an existing stable of technologies such as Kubernetes, Prometheus, OpenTracing, gRPC, and Fluentd that are complementary to Envoy and the overall cloud native development community. As of this writing, Envoy has 78 contributors from at least 10 different organizations with primary maintainers working at Lyft and Google. Overall the project has been going incredibly well with massive growth and uptake. Lyft’s contribution of Envoy to the CNCF is both a way to give back to the larger open source community as well as a nod to the fact that the targeted resources of the foundation will help unlock the next stage of project growth. As a technology, Envoy has the opportunity to become a primary building block of modern service architectures. All of us at Lyft — and all of us who work on Envoy across many different organizations — are extremely excited for what the future holds in partnership with the CNCF. Stories from Lyft Engineering. 417 Microservices Lyft Cloud Computing Engineering 417 claps 417 Written by Engineer @lyft (https://mattklein123.dev/) Stories from Lyft Engineering. Written by Engineer @lyft (https://mattklein123.dev/) Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-09-13"},
{"website": "Lyft-Engineering", "title": "overcoming aws complexity with saltstack patterns", "author": ["Aneesh Agrawal"], "link": "https://eng.lyft.com/overcoming-aws-complexity-with-saltstack-patterns-1472981f43c6", "abstract": "Data Data Science Engineering Mobile Product Security I’m an intern at Lyft this summer, and I’ve had a great experience in our Infrastructure org, working on improving Salt and our use of it for orchestration and config management. In this post, I’m going to share some Salt patterns that we employ in the Salt AWS-related boto modules and that I’ve implemented this summer as part of my project. I’ve been working with Salt for about two and a half years now, so I’m familiar with the project, but I hadn’t used any of the AWS-related parts of the project before. As the originators and some of the primary maintainers of the AWS-related boto modules in Salt, my Lyft team was able to teach me a lot of new things. This post is going to build on the some of those concepts which are introduced in my mentor Ryan Lane’s blog post , so make sure to read it for context! My mentor and I decided working on a totally new module was the best path to learn best practices; after building experience, I would change my focus to update older, larger modules. I started with AWS CloudFront — Amazon’s CDN service — which we use to power a number of our public facing sites, including the main Lyft website . CloudFront configurations are large and intricate— for reference, one config is about 400 lines as YAML — and updates take between 15 minutes and an hour to propagate, so it’s important that we have our configurations in source control. This places CloudFront as one of many inherently complicated AWS services. Until recently CloudFront distributions could not be named; as mentioned in the previous post , we rely on being able to reference AWS resources by a user-provided name. This meant we could not orchestrate them via Salt, which has been a long-standing pain point for us (2 year old internal bug!). Amazon recently added support for tagging , leading me to implement Salt execution and state modules for CloudFront that use a `Name` tag. However, one caveat of the new tagging support is that it isn’t possible to look up a distribution by a given tag, so we have to iterate through all distributions to find each specific distribution by name, and make separate API calls to fetch each configuration. This is expensive, so I added caching to alleviate the cost of the lookups using the Salt __context__ feature and a boto specific utility wrapper. After implementing the modules, I shifted my focus to the primary goal: migrating our existing distributions into source control. We have 17 distributions — each with large configurations — so manual copying was not a practical option. I leaned on another Salt idiom and implemented a boto_cloudfront.export_distributions function. With one command, I was able to export all of our existing distributions into an SLS file, ready to be checked into version control and run via Salt: Complex services like CloudFront can be hard to manage due to the sheer number of options, criticality to our infrastructure and changing feature set. Patterns like caching and exporting make them easier to manage by reducing the machine and human time investment required. One of the next modules I worked on was originally not part of my project, but came in as a request from another team and dovetailed with it. Our datastores team has been working on ensuring all of our and our users’ data is encrypted at rest (EaR). Part of this is ensuring compliance with our encryption at rest policy for objects in our S3 buckets; unfortunately, there is no bucket-wide option for this, and it must be specified on each upload. This follows a larger trend of ensuring compliance with best practices like default alarms and backups. The complexity here comes from being able to enforce defaults transparently while still allowing for individual variation for special cases. S3 supports about 15 different options on upload. We already had an internal Salt S3 module which did not support specifying any of them, so I added support for arbitrary options to smooth future compliance efforts. I ended up doing a total rewrite of the module due to some unrelated roadblocks, resulting in a much better module that’s ready for upstreaming . The boto3 library has an upload_file function which conveniently allows specifying the S3 upload arguments in a grab-bag extra arguments dictionary. I added support for specifying these extra arguments directly in the Salt S3 object state, as well as an additional argument which loads defaults from a Salt pillar value. This made it possible for me to enforce EaR for S3 uploads one time in our shared pillar configuration: The module automatically merges any options included on a per-state basis on top of these pillar defaults, meaning this change is totally transparent to our users. For example, users can specify custom ACLs, cache control or storage class options without overriding the EaR default. Additionally, the module is equipped to handle any other default options we want to enact in the future, as well as new options AWS may add. While AWS provides a simple way to enable encryption at rest, other defaults such as our default alarms and default backups have much more complicated implementations. An alternate approach here might be to allow users to specify any arguments they want without defaults, and add a check that they did include EaR. However, using defaults stored in the pillar removes the burden from the human from having to include and manage these compliance-related arguments. Being able to enforce these defaults while enabling specific overrides and without bothering our many service teams allows us to keep our forward velocity. Another kind of complexity is inter-service dependency webs. AWS resources often depend on other resources as part of the AWS ecosystem. For example, auto-scaling groups (ASGs), which control a pool of EC2 instances, have associated launch configurations (LCs) which specify a template for new instances, as well as CloudWatch alarms which can be attached to most AWS resources. ELBs can also be registered with an ASG. These dependencies are complicated to manage since they may or may not be present, often have somewhat-hidden ordering constraints on creation and deletion, and can be tightly or loosely coupled with their parents. For example, AWS has a whole page of their docs dedicated to this intricate dance for deleting ASGs. I added support to Salt for automatically deleting these ASG subresources to make the process less tedious. (We currently have a manual process for ASG deletion for safety, but it uses a script that just runs Salt, with some safety checks.) Usually, Salt state modules call into their associated execution module to perform their work, and occasionally some utility functions. However, one of the great benefits of Salt is that it’s built out of regular Python functions and modules, so it is possible to cross-call other state module functions from an supervising state function. This uses one of Salt’s auto-injected global dunder variables, __states__ , which provides access to all of the state functions. To prepare for updating the boto_asg module, I did some work around making cross-calling states easier in Salt, by streamlining merging returns from a substate into the main return with the new salt.utils.state.merge_subreturn function, which makes it this simple: One non-obvious benefit to cross-calling states instead of calling into execution modules is that they already handle things like test=True mode correctly, making cross calling code very linear as opposed to the branchy nature of regular state code. An important difference between LCs and alarms is how they are found when recursively deleting an ASG. ASGs are tied to their current LC, so the state is always able to easily find and delete the right LC. However, alarms are more free-form because you can have any number of them, and they are not easy to find from the ASG they are monitoring. As mentioned in the previous section, we configure default alarms via the Salt pillar and allow specifying additional alarms on a per-state basis, and users can manually add alarms via the console. The state is best-effort in this case, using the configuration is passed to it. Note that the module only deletes subresources if explicitly given a recurse=True parameter, to avoid cascading deletion the user is not expecting. AWS offers a number of services which often rely on each other. We could manage each dependency individually as first-class resource, but Salt allows us to work at a higher-level by cross-calling subordinate Salt states for subresources. This cross-calling pattern enables previewing and making simple changes (e.g. removing an ASG) with cascading effects easily, taming complexity by handling the details internally. I’ve had a ton of fun this summer, thanks to my fellow interns, our university recruiting team, my team, and most of all my mentor Ryan Lane . I’ve learned a great deal, and in the process, made improvements to Salt and pushed them upstream for others to use as well. Stories from Lyft Engineering. 237 Thanks to Ryan Lane , Ethan Goh , and Zack Hsi . AWS Cloud Computing Lyft Engineering 237 claps 237 Written by Stories from Lyft Engineering. Written by Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-09-14"},
{"website": "Lyft-Engineering", "title": "build your data science team like a swiss army knife", "author": ["Nicholas Chamandy"], "link": "https://eng.lyft.com/build-your-data-science-team-like-a-swiss-army-knife-2331551c3590", "abstract": "Data Data Science Engineering Mobile Product Security Though far from a reality in many fields, it is now widely accepted that building a diverse team will make your organization more successful. Many insightful articles and research studies have driven this point home, focusing on diversity axes like gender, ethnicity, culture, age, orientation, and physical ability. Those dimensions are all important. If you participate in the hiring process for your team, and haven’t taken the time to read about why diversity is critical, you absolutely must. Like, now . This post can wait. Data Science is no exception to the rule that diversity breeds success. In fact, I’d argue that diversity of viewpoints has even more leverage in a data science team compared to other organizations. Why? Data Science is all about problem-solving: distilling an unstructured business need, translating it to a tractable mathematical framing, converging on the methodological details, and implementing the resulting model or algorithm. The problem-solving process has been summarized by Helge Seetzen of TandemLaunch as “mapping a fairly small set of ready-made solution archetypes onto the problem and going with the best fit.” It stands to reason that the larger the set of archetypes at your disposal, the better solution you will find. By adding new team members with different points of view, you are adding unique solution archetypes. A great way to diversify your set of solution archetypes is to hire scientists with many different academic backgrounds . For many of us, our grad school years are formative. It’s when we learn how to think like a scientist — when we hone the tools that will be brought to bear on real-world problems. For other data scientists, grad school is replaced by years of work experience in a domain, or by extensive self-study. These experiences train us to specialize — that is, to emerge with one or two really dependable, finely-tuned tools. As a data science team builder, your first goal should be to amass the biggest collection of high-quality tools that you can. From there, the obvious approach seems promising: given a new data science problem, choose the most appropriate tool from that large, diverse collection. In other words, build your team like a Swiss army knife. But the notion of discretely mapping a problem to a tool is actually too rigid in practice — there is considerable variation within any discipline, and methodologies are often combined for greater effect. Extending the idea, one can think of problem-solving as a general optimization exercise, with different academic orientations representing axes of the parameter space over which you are optimizing (see Figure 1 for a cartoon illustration). In this example, the best solution can be achieved with the right combination of ideas from statistics and operations research (OR). Or can it? The maximum in Figure 1 corresponds to the best solution we can achieve after projecting down to a 2-D subspace, namely the (statistics, OR) plane. With a third dimension, say machine learning, we may do even better! We’ll revisit this analogy later when discussing the tradeoffs between generalization and specialization. The more diverse your problem space, the more academically diverse your team should be. For example, Lyft has data science problems across far-flung problem areas like Line passenger matching, fraud detection, user engagement, location estimation, and fare prediction. It’s tempting to bucket these problems into broad categories and staff accordingly: hire statisticians for “inference problems,” operations researchers for “optimization problems,” computer scientists for “machine learning problems,” etc. This approach fails for two reasons: 1. A problem cannot fit squarely in one bucket Lyft Line matching would seem to map to a well-studied optimization problem: construct a graph of passengers, draw edges based on some constraints, and solve for a matching that globally maximizes a pre-specified objective function over some batch of ride requests. Unfortunately, both the constraints and objective function depend critically on variables which are either not yet observed or are unobservable: what is the fastest route for a given match? (optimization) will the passenger cancel given a particular detour length? (prediction) what is the local state of demand in the network? (inference) And it doesn’t stop there. To find the optimal route we must predict travel times and distances accurately; to build such prediction systems we must make inferences from historical data to construct detailed models of road network flow. In this sense, data science problems have a fractal nature: problems beget subproblems, which beget sub-subproblems, and so on. At each level, the paradigm we see most commonly at Lyft is illustrated in Figure 2. Models and inference feed into prediction systems, which feed into optimization modules, which make the decisions which generate more data, which feeds back into models. 2. Data science problems are not cookie-cutter Real data is messy — rarely will one find an open-and-shut case that yields to a ready-made textbook solution. In a diverse team, it will be equally rare to see two data scientists approach a problem from exactly the same angle. Take dynamic pricing: the problem of using price as a lever to balance our ridesharing marketplace at a micro scale and ensure reliable service for all users. An economist looks at this problem and sees supply and demand curves, and elasticity estimation. The electrical engineer sees a classical control system. The statistician sees a forecasting problem, the mathematician a spatio-temporal stochastic process, etc. They are all right, and each of these viewpoints can add value to the solution. Given the importance of diversity of academic orientation, it may be tempting to hire only generalists: jack-of-all trades types who have exposure to a broad range of data science concepts but limited depth in any one area. Generalists are versatile and can move from project to project with ease. In the early stages of a startup, scientists in this mold can be extremely valuable — especially if they have the scrappiness and engineering skills to go along with their methodological breadth. Suppose that a major problem area in your company spans two academic disciplines, statistics and OR. It’s tempting to think that hiring a generalist with some expertise in both fields yields the same scientific power as hiring two specialists — at half the cost! Or at worst that the single generalist will be half as effective as the pair of specialists. This intuition is incorrect for reasons rooted in the so-called Curse of Dimensionality. (The notion that as the number of dimensions increases, the amount of space covered by a product of 1-dimensional sets becomes, relatively speaking, smaller and smaller.) In general, suppose a problem’s optimal solution lives in n linear academic dimensions. Assume for simplicity that a scientist’s expertise occupies a contiguous interval of some length (possibly 0) in each domain. Her total expertise can then be defined as the sum of the lengths of these intervals. Let’s further assume that all scientists have the same total expertise E . Along each dimension of the solution space, a generalist’s expertise therefore only occupies a relatively small range: assume for simplicity that it’s E / n . By contrast, each specialist covers E units of expertise in a single dimension. If we assume a uniform prior on solutions to our problem, then the probability of finding an optimal solution is proportional to the volume, in n -D space, of the product of the 1-D expertise sets. The total volume that the generalist covers is ( E / n )^ n — this is vanishingly small for large n . By contrast, n specialists together account for E^n units of n -D expertise. This is n^n times the knowledge for n times the price — a bargain ! Hold on a second. Can’t we just add n - 1 more generalists, each with expertise E divided evenly among the axes, to achieve the same n -D volume, i.e. ( n E / n )^ n = E^n ? This would imply that a team of n generalists is just as effective as a team of n specialists. Unfortunately, it is likely that any two generalists will have highly overlapping expertise along a particular dimension. This is because in any subject area, it is necessary to learn the basics before going deep. As a result, the total expertise of n generalists along one axis may be substantially smaller than E . These phenomena are illustrated for n = 2 in Figure 3. Building a data science team is as much an art as a science, and there’s no simple formula. We’ve argued here that in a medium- to large-sized data science team, there’s value in hiring an academically diverse set of scientists with deep specialization . Granted, the analogies employed above are an oversimplification of the data science process — subject matter expertise cannot easily be reduced to singular dimensions. The reality is that you want your own unique blend of specialists and generalists, theoreticians and hackers, industry vets and fresh grads. The Lyft Data Science team faces a more diverse set of problems than perhaps any other technology company of a similar size. It’s therefore no surprise that our team has expertise across an equally diverse spectrum of academic disciplines, like computer science, mathematics and statistics, operations research, economics, physics, electrical engineering and transportation engineering. And the list is growing! Interested in geospatial problems, marketplace optimization, machine learning, causal inference, or Data Science in general? Lyft is hiring ! Drop me an email at chamandy@lyft.com. Stories from Lyft Engineering. 204 1 Data Science Engineering 204 claps 204 1 Written by Scientific Director at Lyft Stories from Lyft Engineering. Written by Scientific Director at Lyft Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-08-24"},
{"website": "Lyft-Engineering", "title": "interviewing with lyft engineering", "author": ["Anthony Velázquez"], "link": "https://eng.lyft.com/interviewing-with-lyft-engineering-7d3dd65b2001", "abstract": "Data Data Science Engineering Mobile Product Security (image modified from Carsten Kessler on Flickr , CC-BY-2.0) Every day engineers at Lyft are hard at work building products that are affecting the way millions of people are getting around their cities. Constantly working through challenges of scale, optimizing our network of passengers and drivers, and making complicated products intuitive and easy to understand for our users, at Lyft we’re always looking for great additions to our growing engineering team. If you’re interested in helping us build the future of transportation, we’d love to talk more. Through our interview process we aim to share a little bit more about Lyft, how our teams work, and give you a chance to show off some of your skills. Engineering interviews generally take place over the course of two phases, starting with an initial phone screen before transitioning into a day of on-site interviews. To ensure that you’re comfortable with this process and know what to expect, we’ve outlined our interview process in more depth here. Initial Conversations After reviewing your resume, you’ll be paired with a recruiter who will be your guide through the interview process at Lyft. To get you two acquainted, we’ll start with a short 30 minute conversation that will help your recruiter get to know you and make sure we’re setting you up to talk to people that can speak to the things you’re most interested. In order to keep you fully informed on what’s happening throughout the process, your recruiter will be your primary contact for questions about what’s coming next, and will ensure that the pace works well for you. Additionally, if there’s anything that’ll make you feel at ease — or if you need any accommodations — your recruiter will be available to ensure that you have everything you need from start to finish. Your Technical Phone Screens Our interviewers will begin with some “getting to know you” questions. For example, we’ll ask about previous positions, what gets you excited, and what you are looking for in your next position. We hope you’ll be comfortable enough with your interviewer to highlight your proudest moments in projects or work you’ve done. Our interviewers will then transition the conversation to a shared Coderpad (a shared workspace that supports over 20 different languages; you should work with your most fluent choice). You’re even able to run and debug your code right in the browser. After getting the prompt for the exercise, we encourage you to talk through how you’re thinking about approaching the problem. Our interviewers will be able to give you some feedback, and then you’re free to go to work like you normally would. Are you a “print debugger?” Do you prefer to write tests and work backwards? Sketch out some psuedo-code? Show us how you work. If anything is unclear, don’t be afraid to ask questions. Your interviewer will be there to guide you, work through any issues with you, and can handle any queries you might have. When you’re getting close to finishing, walk through your code with the interviewer one last time. Your interviewer might have some clarifying questions, or might help you find a couple bugs; work until you’re both satisfied with the solution. Finally, the interviewer will save some time at the end for you to get any of your questions about Lyft answered. Definitely zero in on anything you’d find interesting, especially since we will use that information to potentially align you with future interviewers who can go deeper on areas you find fascinating. Your On-Site Interview If you make it through the phone screen, congratulations! We’re very proud of our office and can’t wait to share it with you. We’ll even treat you to lunch! During this part of the process, we’ll get a better understanding of your skills through a series of in-person interviews. These are not meant to be grueling interrogations, regurgitation of memorized concepts, or puzzles with a trick solution. We want to see how you think and how you solve problems. We’re more interested in how you overcome mistakes rather than if you make them. We want to give you the opportunity to be yourself and demonstrate your best skills. Within that framework, you’re likely to encounter three different types of interviews. Problem Solving/Data Structures/Algorithms: In these interviews, we pair you with an interviewer who will present a problem, and you’ll come up with an algorithm or function to solve it. We’re looking for you to be able to bounce ideas around about different approaches, discuss trade-offs, and work through edge cases. Your interviewer will be there to help out and give some feedback on ideas you might have. Traditionally we’ve performed these with a whiteboard, but you’re also welcome to break out your laptop — or ask that one be provided — if you tend to think better in an IDE with the ability to see real input and output. Architecture and Design: We spend a good amount of time talking to each other about how to build our systems and model our data. There are many ways to solve these problems and they all have different strengths and weaknesses. We’re hoping to get a snapshot of the way you think about these problems and communicate decisions. You’ll work with your interviewer to potentially: design a way for your systems to communicate propose an API model out some database tables or perform other tasks where you’ll be sketching out a solution involving many interconnected parts This is an opportunity to go deeper into your experience. How have you seen things fail? What scaling issues are you anticipating? Where are you making sacrifices in order to make gains elsewhere? Being able to articulate trade-offs is incredibly valuable when working in Lyft’s ever-changing market. Laptop Assignment: As much as possible, we want our interview process to get you into environments approximating a normal work day. In that vein, one of our newer additions is the laptop programming exercise. For these challenges, our interviewers will present you with a problem statement, and we’re looking for you to put together a small module that takes in an input file and produces output to a different file. At 90 minutes, it’s slightly longer than our other interviews. You’ll start off by talking through your ideas with your interviewer. Once you settle on your design, you’re free to hit the keyboard and work like you would normally, including access to Google and any third-party libraries you think are helpful. Do you prefer listening to your favorite music? Maybe you prefer small talk and jokes while you work? Or do you prefer to work in a quiet room? We’ll cater to an environment that enables you to perform your best. After you’re finished, submit it via a provided private link and we’ll have it graded by two of our engineers. For the graders, correctness is most important but they’ll also be looking at code quality, performance, and test coverage. Leave Excited: We’re hoping that at the end of the day you’ll be as excited about Lyft as we are. Over the course of the day you’re going to meet a variety of people from multiple teams with many different journeys to and through Lyft. We want to share our stories with you and ensure that you’re getting all your questions answered. So much so that we leave a 30-minute closing conversation with one of our engineering managers dedicated to any questions or feedback you have. Some General Tips A couple of notes about our process: Choose Your Tools: We’re not attached to whiteboards. At the same time, we understand many candidates do their interview preparation with them in mind. Let your interviewer know the toolset for which you’re most comfortable. If you’ve got your presentation skills down and feel best with a whiteboard marker in your hand, then we’ll have fresh ones ready. On the other hand, if you operate best in your favorite IDE with syntax highlighting and shortcuts then go ahead and bring your laptop — or ask your recruiter about having one provided for your interviews. Take Time to Think: One morning, I tried to tie my shoes while brushing my teeth; it didn’t work. One of the most common pieces of advice we see in technical interviews is to “let the interviewer know what you’re thinking.” It’s a great piece of advice but at times can scramble your brain, similar to brushing your teeth while tying your shoes. Remember that talking and thinking are two activities that may not be best done simultaneously, so don’t be afraid to step back and say “hold on I want to think about X for a second.” When you’ve had a second to catch up, share your thoughts with the interviewer and then keep moving. Build on Previous Ideas: It’s entirely possible you’re going to get stuck at various points during the interview. This is common in our day-to-day work as well. Bounce ideas off your interviewer, try an alternative, or go back and see if there’s something simpler you can solve. Many candidates work through a couple different solutions before finding one that makes them happy, so don’t get too worried if it’s not perfect on your first try. We hit roadblocks pretty often — navigating them is the exciting part. Be Yourself: Throughout your series of interviews there will be opportunities where we’re not looking for a single specific answer but for you to share how you arrive at your destination. By sharing your experiences, how you’re coming to your decisions, and what you’ve learned so far, we learn a lot about what you can add to the team. The best products are built by diverse teams, but that only works when people can be their authentic selves. After the Interview We should be able to get back to you after a couple of days. Regardless of how the day went, we’d love to hear more from you, and definitely reach out to your recruiter if you have additional questions. We’re happy to put you in touch with any of your interviewers or other people who’ll help make the next steps easier for you. Feedback is always appreciated regardless of the outcome; we’re always looking to make our interview process better. Are You as Excited as We Are? Head over to our careers page and check out some of the job listings. If you’d like more preparation, consider attending our free technical interview prep event at Lyft HQ in San Francisco on Wednesday, November 8th, led by Cracking the Code Interview’s Gayle Laakmann McDowell . Hope to see you around soon! Note: Due to the current situation regarding COVID-19, at Lyft, we have taken some measures and until further notice, on-site interviews will be carried out remotely using Google Docs and Google Drawings . (Update as of 7/28/2020) Stories from Lyft Engineering. 814 2 Interview Lyft Programming Engineering 814 claps 814 2 Written by Corgi-centric Facebook ads were made to take all my money. Dog dad: @bleakdoghouse Stories from Lyft Engineering. Written by Corgi-centric Facebook ads were made to take all my money. Dog dad: @bleakdoghouse Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-07-28"},
{"website": "Lyft-Engineering", "title": "ramblings on experimentation pitfalls", "author": ["timothybrownsf"], "link": "https://eng.lyft.com/ramblings-on-experimentation-pitfalls-dd554ff87c0e", "abstract": "Data Data Science Engineering Mobile Product Security A common pitfall we see when people try to rollout an experiment, is to rollout by changing the distribution of the variants. I’ll give an example below to demonstrate how this leads to biased results. Let’s take the story of Bob, the online mango salesman who is interested in testing to see if he can increase his prices to make more money. Currently he sells mangos for $5.00 each (after all, they are Alphonso mangos), and he wants to see how it would effect business if he increased prices by 50 cents. He sets up an experiment with two variants, control and treatment, and sets the price for mangos to be $5 for control and $5.50 for treatment. He’s worried about rolling this out to 100% of users, as his livelihood depends on mango sales, so he starts with a 90/10 split (control/treatment). After a week he decides it seems safe and rolls it out to 50/50. He waits another week* and then starts to analyze the experiment. Looking at analytics events he finds that there were 100 users the first week (split 90/10) and 100 users the second week (split 50/50). He happened to have no repeat visitors (how convenient for the math)! This means there were a total of 140 users in control and 60 in treatment. He then looks at total mangos purchased and total revenue. 90 mangos were purchased the first week for the control group, and 10 for the treatment. In the second week there were 25 mangos purchased for the control group and 25 for the treatment group. He adds this up and sees that there were a total of 115 mangos for control and 35 for the treatment group. Now let’s do some math. 115 mangos / 140 users = .82 mangos per user for control 35 mangos / 60 users = .58 mangos per user for treatment Clearly this 50 cent increase doesn’t pay off, even if we look at it in terms of total revenue: .82 * $5.00 = $4.10 per user for the control .58 * $5.50 = $3.19 per user for the treatment Bob turns off his experiment and decides that he was doing just fine selling mangos at $5 a pop. This is an example of a common mistake that we see people making in experimentation. The reality is that the treatment ($5.50 per mango) actually performed better, and here’s why. Unbeknownst to the mango salesman, there was some bad news on CNN (who knows if it’s real…) about an international mango virus, which was announced at the start of the second week of the experiment. Notice that mango purchases changed from 1 per person in the first week (90/10 mangos for 90/10 users) to .5 per person in the second week (25/25 mangos for 50/50 users). However, in both week 1 and week two, the number of mangos per person was equal between control and treatment . This can be seen when plotted out on a weekly graph of mangos per user for control vs treatment, but when looking at cumulative numbers, it is biased due to the distribution change between variants. This actually means in both weeks of the experiment, the treatment performed better on a revenue per customer basis. We introduce a new concept of a rollout, which is separate from variant distribution. It means you keep your experiment at 50/50 the entire time, but start by only rolling it out to 20% of users, then ramp that up to 80% of users. What this actually means under the hood is that we have a third variant, called “Holdout”, which is not included in analysis, and we start with 10/10/80 (control / treatment / holdout). Then as we increase the rollout, we do so by adding users from the holdout group so it becomes 50/50/0. If Bob had used a holdout he’d actually be making $.50 more per mango (since mango sales per user did not change between control and treatment). That means Bob would be bringing home an extra 10%! This pitfall is actually an example of the Simpson’s Paradox. Check out the Wikipedia entry for more examples. * A week is an arbitrary amount of time and should not be taken as the correct time-frame to analyze an experiment. An entire post could be dedicated to the challenges of identifying the correct length of time to wait to analyze an experiment. Interested in experimentation and having a big impact? Lyft is hiring! Drop me a note on Twitter or at tim@lyft.com . Stories from Lyft Engineering. 108 1 Data Science Experimentation Lyft 108 claps 108 1 Written by Director of Engineering, Lyft Stories from Lyft Engineering. Written by Director of Engineering, Lyft Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-05-30"},
{"website": "Lyft-Engineering", "title": "typescript at lyft", "author": ["Mohsen Azimi"], "link": "https://eng.lyft.com/typescript-at-lyft-64f0702346ea", "abstract": "Data Data Science Engineering Mobile Product Security In my early days as a JavaScript developer, when I learned about efforts to add types to JavaScript projects, the first question I asked myself was: why? Now, as a seasoned JavaScript veteran, I cannot imagine writing JavaScript without support of a type system. Large JavaScript applications need type information for scalability and maintainability. Our numerous JavaScript projects at Lyft are no exception, from our Lyft.com website to our many internal tools. While I am still an ardent JavaScript fan, watching Lyft’s teams and codebase grow has convinced me that vanilla JavaScript was never intended for use in large-scale applications. But that’s no reason to throw out JavaScript altogether. Type systems can make all the difference. It’s no secret that type systems can avoid bugs and help engineers more easily navigate codebases. Let’s consider how — and why — Lyft chose TypeScript for the job. Uncaught TypeError: Cannot read property \"foo\" of undefined is one of the most common JavaScript errors. It occurs when accessing properties of a reference that could possibly be undefined. For our vanilla JavaScript codebases, some form of this error frequently cropped up in Lyft’s production environment. Other category of bugs of this sort include simple typos; something like document.getElementbyId can cause big problems in production. Less frequent — but more important — bugs are REST API type mismatches between the client and the server. For instance, when a field is changed from number type to string in the API response, the JavaScript code can result in very unexpected behaviors . Navigating large JavaScript codebases can be time consuming and confusing. It’s often hard to find where functions are defined and what parameters they expect. Take for instance the following JavaScript code: This function makes an HTMLInputElement , and depending on the type , it sets the checkboxes checked attribute to the (supposedly) boolean value argument. Otherwise it will assign the value argument to the value attribute of the input. There’s a possible, subtle bug here. Consider what happens if you assign the wrong type of value when making a checkbox input: Even with an accurate JSDoc comment, this bug is not prevented. Setting the checked value argument to the string \"false\" will actually make createInput return an input in checked state — because the string \"false\" is evaluated as a boolean true ! With a type system we can declare function overloads that help developers write correct code from the get-go: Type systems make discovering code capabilities much easier. They also prevent bugs very early on by hinting at API usages semantics. Type systems make refactoring easier by giving developers confidence about their changes. For example, when a utility function signature changes, the TypeScript compiler will not allow compilation until all call-sites are corrected with the new signature. Moreover, strongly typed programs are easier to refactor because the type-checker ensures your changes are compatible with other parts of the project. IDEs and code editors can provide refactoring features backed by the type system. Type systems also help with the maintainability by distributing modules with type information. Modules written in TypeScript can enable some editors to provide very helpful API usage hints. Once we were convinced that a type system would help scale our frontend applications, we considered our options. There are a few type systems available for JavaScript: Google Closure Compiler with JSDoc type annotations FlowType TypeScript We ultimately decided to use TypeScript, but it wasn’t an easy decision. Our team was evenly divided between those who preferred FlowType versus those who preferred TypeScript. There were many arguments on both sides, and it’s helpful to consider them in detail. FlowType is sometimes advertised as “just JavaScript” or “JavaScript with type annotations.” This statement is very misleading. FlowType is an independent language, with syntax that’s a superset of JavaScript. The convention of using .js extensions for FlowType files contributes to this confusion. But in reality, neither JSX or FlowType are JavaScript. The same goes for TypeScript and even EcmaScript proposals that are not yet in the spec. Neither FlowType nor TypeScript are JavaScript; they are different languages. One of much-hailed features of FlowType is call-site based type checking for function calls. Consider this erroneous code: FlowType will — correctly — warn about passing a string to a function that clearly requires a number as its sole argument. TypeScript will consider a to be any type and compile the above code without errors. This feature looks impressive but it falls short as soon as we complicate our function even slightly: This code will get compiled in FlowType without any errors. Call-site type checking is impressive but casts a false sense of security when burdened with the codebase of real-world applications. Since React and FlowType are both open source projects from Facebook, it’s often assumed that FlowType works better than TypeScript with React. But with projects at Lyft, we didn’t find any meaningful difference when using either with React. React type definitions in DefinitelyTyped are very accurate and helpful. Even if you consider both options to be equally suitable, for future ecosystem growth it’s critical to consider a project’s popularity. Using a popular option will help Lyft attract more talent while ensuring broader access to other open source projects written in that language. Though measuring the popularity of an open source project can be a subjective art, I tried my best to find reasonable metrics as a basis for comparison: StackOverflow questions : FlowType: ~ 900 ; TypeScript~ 38,000 GitHub Issues: FlowType: ~1,500 Open, 2,200 Closed; TypeScript: ~2,400 Open, 11,200 Closed GitHub pull requests FlowType : ~60 Open, 1,200 Closed; TypeScript: ~100 Open, ~ 5,000 Closed npm download per month FlowType : ~2.9 million/month , TypeScript: ~7.2 million/month Number of external type definitions FlowType: ~340 external, 43k for “flow-typed” directory in GitHub; Some libraries provide .flow type definition TypeScript: ~3,700 external, ~250k results for “typings” in package.json in GitHub, Even FB repos like Redux and ImmutableJS provide TS type definitions We even sent out a survey to our own engineers to investigate internal popularity of each project.. Mirroring the external numbers I collected, we discovered that TypeScript was also more popular internally. We have a daunting amount of preexisting vanilla JavaScript code. Converting all of our JavaScript codebase to TypeScript was not an option. So instead, we took the incremental approach. We are using Webpack to compile our frontend applications. It’s painless to introduce TypeScript to Webpack via TypeScript-loader . Once TypeScript-loader is added to the Webpack configuration of a project, we were able to write new files in TypeScript and update our codebase piecemeal. The TypeScript compiler can type-check JavaScript files as well. We took advantage of this feature to check our existing JavaScript code for type errors. This only works if a JavaScript file is directly imported into TypeScript. In the most recent version (2.5) it’s also possible to type-check standalone JavaScript files. There are many resources online for learning TypeScript. The TypeScript website itself includes great learning material, making onboarding engineers a snap. Moreover, since TypeScript syntax is just a superset of JavaScript, it’s very intuitive for frontend engineers. Linting is another useful tool to educate new developers, and as a bonus, it helps maintain consistent and idiomatic code. JavaScript Linters prevent common pitfalls even without type systems, but are stronger at protecting code with a type system, which is why we use TSLint with all of our TypeScript projects. TSLint can do more than standard linters; for example, TSLint can catch interesting “gotchas!” like awaiting-non-promises , something not possible with vanilla JavaScript linters. In the process of training engineers to use TypeScript and iteratively adding TypeScript to our codebase, we discovered that much of our JavaScript code did not easily lend itself to typing. While frustrating, discovering these scenarios is a good indicator that code is not well architected, which helped us prioritize refactoring. Since we adopted TypeScript, many new Lyft projects are TypeScript-only. These are just a few samples of projects where we’ve adopted TypeScript. React has a runtime-based type system for its components: PropTypes, a powerful tool for enforcing interface of shared components. We use PropTypes extensively at Lyft for our non-TypeScript repositories. But in TypeScript, runtime type checking is unnecessary and all of type-checking happens via the TypeScript compiler. That’s why we invested time in developing a React JavaScript-to-TypeScript converter. It takes advantage of TypeScript compiler transformers and converts all PropTypes in a React component into TypeScript interfaces. This project rapidly accelerated our adoption of TypeScript, saving us hundreds of engineer-hours. In this project we tried to solve the problem of rendering dynamically loaded components on the server-side. It’s written entirely in TypeScript, and represents a good example of how to write shared code with TypeScript by leveraging shared type definitions. At Lyft, we use an internal atomic CSS library called Tetris. The methodology behind atomic CSS is couched in the idea that repeating class names is usually cheaper than repeating CSS code. Atomic CSS frameworks introduce many small CSS classes that can be combined to style an element. When using Tetris, developers must remember countless CSS class names (e.g., like p-a-m, which adds padding around a box). To enhance our developers’ productivity — and reduce the need for perfect memory — we defined a single TypeScript file that exports every Tetris class name. Now Lyft developers can import this file and use their favorite editor’s autocomplete feature to get hints for class names: One of the most intractable categories of bugs are those that frequently appear when working with backend APIs. A backend change can break frontend code — or sometimes frontend code changes break compatibility with backend API schemas. Our backend APIs are described with OpenAPI (Swagger 2.0). Creating a formal specification in this manner gives us the perfect opportunity for strongly typing our API usage in frontend applications. We use Swagger JSON Schema models to automatically generate TypeScript interfaces for our API clients. We hope this gave you some perspective about why Lyft chose TypeScript over FlowType, and how we’ve begun our transition to strongly typed JavaScript. If you’re still intrigued, and want to work with TypeScript at Lyft, we’re hiring ! Stories from Lyft Engineering. 2K 18 JavaScript Lyft Typescript Engineering 2K claps 2K 18 Written by Frontend @ Lyft Stories from Lyft Engineering. Written by Frontend @ Lyft Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-09-27"},
{"website": "Lyft-Engineering", "title": "envoy 7 months later", "author": ["Matt Klein"], "link": "https://eng.lyft.com/envoy-7-months-later-41986c2fd443", "abstract": "Data Data Science Engineering Mobile Product Security On September 14, 2016 we announced Envoy , our L7 proxy and communication bus. In a nutshell, Envoy is a “service mesh” substrate that provides common utilities such as service discovery, load balancing, rate limiting, circuit breaking, stats, logging, tracing, etc. to polyglot (heterogeneous) application architectures. We knew that we had built a compelling product that was central to Lyft’s ability to scale its service-oriented architecture; however, we were surprised by the industry wide interest in Envoy following its launch. It’s been an exciting (and overwhelming) 7 months! We are thrilled by the positive reception and wide uptake Envoy has since received. As it turns out, almost every company with a moderately-sized service oriented architecture is having the same problems that Lyft did prior to the development and deployment of Envoy: An architecture composed of a variety of languages, each containing a half-baked RPC library, including partial (or zero) implementations of rate limiting, circuit breaking, timeouts, retries, etc. Differing or partial implementations of stats, logging, and tracing across both owned services as well as infrastructure components such as ELBs. A desire to move to SoA for the decompositional scaling benefits, but an on-the-ground reality of chaos as application developers struggle to make sense of an inherently unreliable network substrate. In summary: an operational and reliability headache. Though Envoy contains an abundance of features, the industry appears to view the following design points as the most compelling: High performance native code implementation : Like it or not, most large organizations still have a “performance checkmark” for system components like sidecar proxies, which can only be satisfied by native code, especially regarding CPU usage, memory usage, and tail latency properties. Historically, HAProxy and NGINX (including the paid Plus version) have dominated this category. HAProxy has not sustained the feature velocity required for a modern service mesh, and so is starting to fall by the wayside. NGINX has focused most of their development efforts in this space on their paid Plus product. Furthermore, NGINX is known to have a somewhat opaque development process. These points have culminated in a desire within the industry for a community-first, high performance, well-designed and extensible modern native code proxy. This desire was much larger than we realized when we first open sourced Envoy, and Envoy fills the gap. Eventually consistent service discovery: Historically, most SoAs have used fully consistent service discovery systems that are hard to run at scale. Envoy treats service discovery as eventually consistent and lossy. At Lyft, this has lead to extremely high reliability without the maintenance headache of systems typically used for this purpose such as etcd, Zookeeper, etc. API driven configuration: Fundamentally, we view Envoy as a universal dataplane for SoAs. However, every deployment is different and it makes little sense to be opinionated about all of the ancillary components that are required for Envoy to function. To this end, we are clearly documenting all of the APIs that Envoy uses to interact with control plane components and other services. For example, Envoy documents and implements the Service Discovery Service (SDS), Cluster Discovery Service (CDS), and Route Discovery Service (RDS) REST APIs that can be implemented by management systems to dynamically configure Envoy. Other defined APIs include a global rate limiting service as well as client TLS authentication. More are on the way, including gRPC variants of the REST APIs. Using the published APIs, integrators can build systems that are simultaneously extremely complex and user friendly, tailored to a particular deployment. We have open sourced the discovery and ratelimit services that we use in production as reference implementations. Filter based L4 core: Envoy is an L4 (TCP) proxy with an extensible filter chain mechanism. This allows it to be used for a variety of use cases, including transparent TLS proxying (stunnel replacement), MongoDB sniffing, Redis proxying, as well as complex HTTP-based filtering and routing. We look forward to community contributions that add support for different protocols. HTTP/2 first: Envoy was designed from the start to be a transparent HTTP/1 to HTTP/2 proxy in both directions. Most production proxies still do not have this capability, which means that they cannot be used for gRPC , the increasingly popular RPC protocol from Google. It’s all about observability: From an operational and reliability standpoint, having consistent observability within an SoA is by far the most important objective to obtain. A deployed Envoy mesh immediately provides consistent stats, logs, and traces that transform SoA networking from an intractable problem to something that can be reasoned about and acted upon. When we made the decision to open source Envoy, we were committed to developing a community and an ecosystem; however, we had no idea if anyone would show up! As it turns out, companies both large and small have arrived in substantial numbers, drawn by all of the reasons laid out above. Ultimately, it’s our hope that Envoy is a powerful tool for meeting the SoA needs of many organizations — not just Lyft. We are excited to announce that we are working in partnership with both Google and IBM to bring Envoy to Kubernetes. Fun fact: there are now more people working on Envoy at Google than there are at Lyft! We have a lot of other things planned with Google that we will be able to share more about in the coming months. We are eager to move Envoy forward within the open source community. In only 7 months Envoy has amassed over 40 contributors, with substantial contributions from both Google and IBM. Next week, we will be providing Google with commit access, making Envoy a true multi-organizational project. Over the next 6 months we expect to see many more public announcements about large companies using Envoy, startups beginning to coalesce around offering commercial Envoy integrations, as well as other large companies joining the project as full committers. It’s very rewarding for us to see the momentum around this project and we are excited to discover what the future holds (potentially including donation to the CNCF ). We have an ambitious roadmap planned over the coming months between Lyft, Google, and IBM. Some of the major features include: Sharded Redis support with cluster discovery, consistent hashing, health checking, and self healing. Full end-to-end flow control across both HTTP/1 and HTTP/2. IPv6 support. Further investment in centralized control plane APIs include a Listener Discovery Service (LDS) API which will allow fully dynamic filter instantiation, a Health Discovery Service (HDS) API which will allow Envoy to be used as a distributed health checker, and load/failure reporting APIs which will allow Envoy runtime data to be fed back into global control plane and load balancing systems. Bidirectional streaming gRPC variants of all of the control plane APIs, allowing for higher performance, a strongly typed IDL, and faster reaction times to updates. Zipkin tracing support. Further dynamic outlier detection including latency variance analysis. Tighter integration with gRPC including gRPC to JSON transcoding . More expressive global rate limiting via IP tagging. More rigorous stress, performance, and fuzz testing. Please reach out to us and let us know if there are other items that you would like to see! Before we open sourced Envoy 7 months ago, never in our wildest dreams would we have imagined that we might have the chance to start a project that has the potential to become a building block of the modern internet. We were simply trying to meet the needs at Lyft as best as we possibly could. The trajectory that Envoy is now on is thrilling and daunting at the same time. There is still a ton of work to do to make service mesh networking transparently available, and it will take the efforts of many talented developers and organizations to bring Envoy’s full potential to fruition. The next 6–12 months are likely to see a further increase in community interest, commercial availability, and adoption. If we haven’t heard from you yet and you are interested in learning more about Envoy or participating in its development, please reach out via GitHub, email, or Gitter . Onward! Interested in open source work and having a big impact? Lyft is hiring ! Drop me a note on Twitter or at mklein@lyft.com . Stories from Lyft Engineering. 461 1 Microservices Grpc Cplusplus Kubernetes Engineering 461 claps 461 1 Written by Engineer @lyft (https://mattklein123.dev/) Stories from Lyft Engineering. Written by Engineer @lyft (https://mattklein123.dev/) Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-04-25"},
{"website": "Lyft-Engineering", "title": "toasted marshmallow marshmallow but 15x faster", "author": ["Roy Williams"], "link": "https://eng.lyft.com/toasted-marshmallow-marshmallow-but-15x-faster-17bdcf34c760", "abstract": "Data Data Science Engineering Mobile Product Security Marshmallow is a fantastic ODM that we leverage extensively at Lyft. It has an extensible, easy-to-use API and is framework agnostic. We use it to validate incoming requests, to deserialize data from Mongo or Dynamo, and to serialize outgoing requests. Unfortunately, we ran into a sticky problem with Marshmallow. Benchmarks suggest it’s one of the slowest serialization frameworks for Python. This put us in a tough spot…we love the powerful, easy-to-use API, but we wanted better performance than Marshmallow could offer. We huddled up and consulted with our trusted confidant, who had an incredible suggestion: To understand how both are possible, one must first understand how Marshmallow works. Marshmallow is slow because it’s effectively implementing an interpreter in an interpreter. When you serialize an object through Marshmallow it iterates through all the fields it knows about, then does a ton of reflection to extract the field from the input object. In practice, this is hugely wasteful. We are serializing the same object over and over again and it generally complies with the provided schema. With Toasted Marshmallow we decided to take an approach where we generate a serialization method at runtime that assumes the incoming object complies with the schema, falling back to the original Marshmallow code if it doesn’t. As a concrete example, say we had the following Schema: The generated serialization method would be: Toasted Marshmallow will first use this method, and — should any exceptions get thrown or the object fail to parse — fall back to the original Marshmallow code. This approach is 15–20x faster than reflection alone. In addition to reducing CPU consumption of our hosts using Marshmallow, rolling out Toasted Marshmallow resulted in nearly a 50% drop in our p95 response times! Toasted Marshmallow is a drop in replacement for Marshmallow, there should be no difference in your code. Simply replace your dependency on marshmallow==3.0.0b2 with toastedmarshmallow==0.1.0 and set the environment variable MARSHMALLOW_SCHEMA_DEFAULT_JIT to toastedmarshmallow.Jit (you can also optimize individual schemas ). Toasted Marshmallow includes a slightly modified version of Marshmallow to add some necessary hooks for calling into the generated code first and regenerating the code when needed. We structured the code to minimize the modifications needed in Marshmallow making it easier to track upstream in the future. We treat any differences as a bug, so if you have any code that worked with Marshmallow but fails with Toasted Marshmallow please file an issue . Check out Toasted Marshmallow today, and add s’more speed to your code! Interested in open source work while having a big impact on the future of transportation? Lyft is hiring! Apply through our application system , or drop me a note at rwilliams@lyft.com . Stories from Lyft Engineering. 225 Python Engineering Open Source 225 claps 225 Written by Stories from Lyft Engineering. Written by Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-08-09"},
{"website": "Lyft-Engineering", "title": "announcing ratelimit", "author": ["Jose Nino"], "link": "https://eng.lyft.com/announcing-ratelimit-c2e8f3182555", "abstract": "Data Data Science Engineering Mobile Product Security Today we are excited to open source Ratelimit, a Go/gRPC service designed to enable generic rate limit scenarios from different types of applications. For example, per IP rate limiting, or rate limiting the number of connections per second made to a database. Applications request rate limit decisions based on a domain and a set of descriptors . The Ratelimit service takes the request, matches it against the loaded configuration, checks against a Redis cache (although the in memory store can be easily swapped), and returns a decision to the caller. Ratelimit is in production use at Lyft, and handles tens of thousands of rate limit requests per second. It is a reference implementation of the rate limit API that Envoy , our proxy and communications bus, uses. We use Ratelimit in both our edge proxies, and our internal service mesh to rate limit requests. The Ratelimit service conforms to the Ratelimit protobuf defined here . It receives RateLimitRequests which are composed of a domain and a set of descriptors . A domain is a unique string that enables configuration to be application specific, without overlap. descriptors are a list of hierarchical entries that are used to determine the final key to lookup in the Redis cache. Combining these two concepts, Ratelimit provides a flexible framework for rate limiting a wide variety of scenarios. For example, to set a rate limit on connections per second to your datastore, you could define a configuration like this: Then send requests to the Ratelimit service like: To check if your service should be allowed to create a new connection to the database. You can find more requests examples and explanations on how they work here and here . Ratelimit loads YAML configuration files from disk. Ratelimit uses a library we recently open sourced called runtime to load these files. The configuration file reflects the Ratelimit API described above. It contains a domain , descriptors for that domain, and rate_limit . Currently the service supports per second, minute, hour, and day limits. For more information on how to setup the service, and configure it please read the docs . Envoy integrates with the Ratelimit service via two filters : 1. Network Level Filter : Envoy calls the Ratelimit service for every new connection on the listener where the filter is installed. This way you can rate limit the connections per second that transit the listener. 2. HTTP Level Filter : Envoy calls the Ratelimit service for every new request on the listener where the filter is installed and the route table specifies that the Ratelimit service should be called. A lot of work is going into expanding the capabilities of the HTTP filter. Like discovery , Ratelimit expands the Envoy ecosystem and enhances the functionality that Envoy can accomplish in your infrastructure. We are excited to be releasing this additional piece of the Envoy ecosystem and can’t wait to hear what you think about it and do with it. Interested in open source work and having a big impact? Lyft is hiring! Drop me a note on Twitter or at jnino@lyft.com Stories from Lyft Engineering. 79 1 Security Golang DevOps Microservices Engineering 79 claps 79 1 Written by Engineer @lyft Stories from Lyft Engineering. Written by Engineer @lyft Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-02-09"},
{"website": "Lyft-Engineering", "title": "saltstack as an alternative to terraform for aws orchestration", "author": ["Ryan Lane"], "link": "https://eng.lyft.com/saltstack-as-an-alternative-to-terraform-for-aws-orchestration-cd2ceb06bf8c", "abstract": "Data Data Science Engineering Mobile Product Security A few years ago Lyft made a decision to move away from using Puppet for configuration management, to using SaltStack (salt). We had already been using salt for AWS orchestration, and that helped in our decision to use salt for configuration management. Our AWS orchestration code was a bunch of custom state and execution modules that we maintained in-house. We felt the modules were really useful, and the idea of fully maintaining these ourselves was a bit overwhelming, so we’ve open sourced all of our changes over time. Since we wrote these, Terraform has been released and has become extremely popular. Terraform is pretty great, so why do we continue to use and maintain the modules in salt? In this post, I’ll walk through the pros and cons our design decisions in the salt modules, how our engineering culture influenced these decisions, and how these decisions affect our engineering culture. A tenet of our design is that AWS already has our infrastructure’s state, and that we shouldn’t track state separately, as it will diverge. Additionally, we want to be able to run our orchestration in parallel across service deploys, and potentially across an arbitrary number of services at a time. Before moving on too far, here’s some basic salt terminology: states: idempotent salt (sls) code that describes a resource state (python) modules: python code that implements the functionality of the idempotent states execution (python) modules: non-idempotent python code that does the actions behind the scenes pillars: global configuration data that can be referenced in states grains: environment data that can be referenced in states and pillars When we write states, we assume that we can reference other states that have previously run, by their names or tags. The python module code will do the heavy lifting for us, converting the names into the IDs required for the various APIs. Let’s take a look at an example of this: Notice that we’re creating two security groups. In the service_name group we reference the elb-external group in a rule for a source group. Additionally, we self-reference the service_name group within a rule for itself. This works because under the hood we translate these names into security group IDs by looking up the name within the referenced virtual private cloud (VPC). By referencing resources by name or tag, we can avoid chaining outputs into inputs, which makes the code much easier to read and write. It also means we don’t need to track the state of the resources (such as the resource ID), since we’ll always be able to re-reference the resource by name, even in disparate code bases. There are, of course, some downsides to this design. Since the system references other resources loosely, we can’t automatically determine the ordering in which the states need to be applied, which places the burden of ensuring correct ordering on the end-user. Assuming general knowledge of AWS from the end-user’s perspective, this hasn’t been a hurdle in practice for us, but it’s a complexity avoided in Terraform. Additionally, not every AWS API supports name attributes or tagging. In my opinion, an API is broken if it is written without a mechanism to reference the resources by a consistent name or tag, unless there was never the intent to create the resource through orchestration (secret management systems are a good example). Most APIs in AWS, at this point, support tagging or name attributes, but occasionally new services will be released without this support initially. Another tenet of our design is that we want our service teams to be able to manage their orchestration code inside of their own service repositories. Based on this, we designed the modules to manage resources in isolation. For instance, it should be possible to manage an autoscale group (ASG), a security group, or other resources inside of a VPC, with the ability to reference the VPC by name, without needing to define the VPC within the same code base, and without needing to run the VPC orchestration code along with the service’s orchestration code. This goal is much easier to achieve thanks to the stateless design. Since we reference resources by name tag, it’s possible to run the orchestration for a particular service, and only include the service’s resources in the run. Referencing resources by name or tag means we don’t need to chain our resources together via requisites into a digraph, don’t need to chain outputs to inputs, and can easily reference resources across disparate codebases. For example, in the above gist for security groups, the VPC isn’t defined in the code, but this code in isolation will still run correctly. When we built the orchestration modules, we were originally making a decision between writing our own orchestration and using Cloud Formation (CF). Our primary concern with CF was that it may automatically perform a destructive action upon detection of a change in a resource definition. It’s easy to have an unintended outage through CF, even though it isn’t at all obvious from a change. This situation has improved over time, but without a relatively strong knowledge of CF it’s still easy to do the wrong thing. Terraform improves this situation via its plan command, which will show you the actions that it’s going to take; however, Terraform, like CF, may still automatically take destructive actions in cases you didn’t intend, and needing to always run plan means it can be difficult to include a Terraform run in a continuous delivery pipeline. Our state python modules are designed to not take automated destructive actions unless the actions are explicit. For instance, if you modify a launch configuration (LC) for an autoscale group (ASG), a new LC will be attached to the ASG with the updated information; however, for this action to take effect, it’s necessary to relaunch the instances inside of the ASG, or to delete and recreate the ASG. The modules will make the modifications necessary for new instances to launch with the new LC, but will not terminate running instances, and will not recreate the ASG. Destruction of an instance or ASG requires the use of an explicit absent state. This design, of course, isn’t perfect. If you modify a resource that requires a destructive action, you still need to take some destructive action to make that change properly apply, whether automated or manual. We approach this problem through well documented processes. For example, when rotating an ASG, we manually terminate a single instance to ensure the new LC is successful, we then let the nodes cycle in naturally through scaling policy. Alternatively, we may detach all running nodes from an ASG, let a new set of nodes come in, then terminate all the detached instances. We’ve written tooling for this and we keep the processes in runbooks for teams to reference, with tradeoffs and best practices related to the destructive actions. Although it may seem like a pain to handle these actions manually, this design gives more fine-grained control to the service team initiating the action, which may depend on the change being made or the profile of the service. It’s possible a service can only have a few nodes safely cycled at a time, or an ASG is so large that we may run into capacity issues when cycling it. Leaving these decisions up to the team initiating the action is empowering. Handing this control to the service team can be potentially dangerous if the team hasn’t read the documentation regarding their change; for example, they may replace a launch configuration and fail to test the replacement of an instance. For cases like this, automated infrastructure auditing comes in handy, as you can alert a team on this condition. If you’ve ever used autoscale groups (ASG), you know that you need to link ASGs with launch configurations (LC). LCs are immutable, so any time you need to modify anything in an LC you need to create a new one, link it with the ASG, then delete the old one. This is rage-inducing if you’re frequently making LC changes because you need to manage these resources separately. There are a lot of other examples here. For instance, when managing ELBs, we frequently want to associate route53 records with the ELBs. Also, when creating resources, we normally want to add CloudWatch alarms on those resources. The state python modules are designed to manage combined resources as single states. When managing an ELB resource, there’s an argument for route53 records. When managing ASGs, there’s a launch configuration argument. When managing most resources, there are CloudWatch alarm arguments, which are tailored to the individual resource. Here’s an example of this: Notice in the above example that we’re having the ELB state auto-link a few cnames. Over time, as we added features, there were some actions that were so common that we wanted them applied by default for every instance of that resource. A good example is management of DynamoDB tables. We want every DynamoDB table to be backed up through a DataPipeline job on a schedule. It’s non-trivial and incredibly verbose to write valid orchestration code for a DataPipeline job (no matter the orchestration engine used). Every table needs to have the DataPipeline, and it is critical that it works correctly; so, we added DataPipeline management to the DynamoDB module, to make it possible to specify a schedule and a location for the backup to land. However, we want it to be dead simple to do this. For features like these, we take advantage of salt’s pillar system. Many arguments in our AWS orchestration state modules also have an associated <argument>_from_pillars argument, which has a default pillar key. for the boto_dynamodb module, there’s a backup_configs_from_pillars argument, with a default key of boto_dynamodb_backup_configs . In our pillars, we add backup configuration settings in the boto_dynamodb_backup_configs key so that by default every table created will automatically have a backup created. From the end-user’s point of view, it looks like they’re just defining a table; everything is handled automatically for them, keeping things simple. There are plenty of other examples. For instance, boto_asg supports scaling policies and alarms from pillars, so by default every ASG in our fleet has a default scaling policy. Most resources (ELBs, DynamoDB tables, SQS queues, etc.) have default CloudWatch alarms. Defaults are great, but teams also need the ability to tune these fields for their services. Many of the _from_pillars arguments have values that are dictionaries or lists, and for those types of resources we make it possible to override specific settings inside of the defaults. For instance, here’s how a service team member would override the scaling policy for an ASG: The above asg.sls state overrides just the alarm threshold setting in the ASG’s default alarms; the rest of the global default alarm settings still apply. Scaling policies in the module don’t currently support overrides, so it’s overridden completely here (if you’re interested in helping, this is a great place to start). When balancing service team empowerment and global consistency, we generally favor the empowerment of the service teams. Rather than requiring centralized salt code for orchestration, each service team manages their own salt code. This allows a team a lot of flexibility and reduces blockers on other teams, but it also results in a lot of yak shaving, especially when a particular change needs to happen globally. A number of python module improvements were specifically created to avoid yak shaving. Rather than modifying all salt states for a particular module, we often introduce a change that’s applied globally through pillars, but can be overridden locally in each state. The examples above for things like DynamoDB and ASGs are good examples of this pattern. Additionally, over time we look at the patterns service teams are using in their orchestration code and we decide which states are used consistently enough to abstract into something that can be included and modified with pillars, jinja macros, or includable states to make things simpler and more globally consistent. If you’ve noticed throughout the examples above, we use minimal jinja templating pretty frequently. In some places we’re referencing pillars, and other places we’re using grains. Grains are defined at runtime and are specifically related to the orchestration run. For orchestration, we turn environment variables into grains ; for instance: This produces grains like: service_name, service_instance, region, and some combined grains like workers.web.cluster_name (confidant-web-production-iad). Since we’re using salt for both orchestration and config management, it allows us to use the same language, the same templating, and the same grains in both spots. It also makes it possible to share pillar data between the two systems. Interested in open source work and having a big impact? Lyft is hiring ! Drop me a note on Twitter or at ryan.lane@lyft.com . Stories from Lyft Engineering. 223 2 Thanks to Ethan Goh , Megan Kercher , Zack Hsi , and Aneesh Agrawal . AWS Orchestration Saltstack Terraform Engineering 223 claps 223 2 Written by Security Engineer and Open Source Team Lead at Lyft Stories from Lyft Engineering. Written by Security Engineer and Open Source Team Lead at Lyft Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-08-30"},
{"website": "Lyft-Engineering", "title": "blessing your ssh at lyft", "author": ["Chris Steipp"], "link": "https://eng.lyft.com/blessing-your-ssh-at-lyft-a1b38f81629d", "abstract": "Data Data Science Engineering Mobile Product Security Like many organizations, Lyft continually looks for ways to address critical risks identified in our organization. Last summer, Lyft’s Security Team identified a lack of two-factor authentication on our ssh logins as an area of concern. Today, we’re sharing our experience implementing two-factor for SSH at Lyft, and announcing our open-source client for interacting with Netflix’s BLESS . Over the past few years, Lyft has developed a system for quickly getting newly hired engineers access into our infrastructure. Engineers are often able to push changes to Lyft services on their first day of employment. This involves provisioning or linking accounts in AWS, Github, pagerduty, etc on their first day. For SSH, this has previously required provisioning a private SSH key for the user and pushing out the corresponding public key to all servers in our fleet. Lyft’s has a strong DevOps philosophy. We practice this by giving teams full control, and responsibility, to manage any service they write. While everyone in the organization agrees that protecting our data and infrastructure is extremely important, teams have to balance security with operational considerations. They need to quickly bring up and manage new services, debug those services in production, and respond efficiently during outages. This meant that any 2FA control that the Security Team put in place needed to be un-oppressive and work without failure, potentially while other systems in our environment were down. It also had to “Just Work” with the development and maintenance work of a fairly small security team (we have 5 members currently, and are hiring of course). The team considered a few solutions: Yubikeys are often used for two-factor authentication. Yubikeys have many benefits, but are costly and require a compatible authentication service to verify codes. This means that we would either need to rely on an external company (Yubico’s authentication service) for authentication, or run an authentication service internally with near 100% uptime. Using Yubikeys for SSH would also require installing a PAM module on every server, maintained by the security team. The team also considered DUO, since the login flow is a great two-factor experience. But this again put us in the position of needing to install a PAM module on every server, and needing to have every server contact an external service. Both of these solutions had some undesirable operational tradeoffs. We also had a use case where some engineers need to SSH into every server in an ASG, or our entire fleet, in response to an emergency. This is often thousands of servers at a time. We needed a solution where this was feasible, and performance was no worse than SSH-ing with a fixed private key. When looking for other options, the team came across Netflix’s “ Bastion’s Lambda Ephemeral SSH Service ” (BLESS), which had recently been open sourced. BLESS uses SSH certificates, added to OpenSSH in 5.4. SSH certificates allow a certificate authority to sign a user’s public key, along with a list of constraints; the user presents this certificate to the server during authentication. The server only needs to trust the CA, and does not need previous knowledge of the user’s public key. BLESS is designed to run as an AWS Lambda , Amazon’s infrastructure to run code without servers, and uses Amazon’s Key Management Service (KMS) to encrypt all secrets. This approach appealed to the team because: It was a simple configuration change to servers, instead of installing new code BLESS leveraged existing AWS infrastructure (KMS and Lambda), which minimized responsibility for the Security Team The cryptographic work to authenticate a user was the same for the user’s laptop, ensuring that performance impact would be negligible Certificates can expire quickly, so compromised SSH keys are useless after a short time Additionally, having our servers trust a CA instead of individual user keys improved our key-management process by allowing users to manage their own keys. Users could regenerate their key as often as they liked, with no coordination from Security or other infrastructure teams. One drawback of Netflix’s BLESS is that Netflix relies on users first SSH-ing to a bastion server where the user is strongly authenticated. Lyft’s goal was to use SSH certificates for all SSH connections including the connection to our bastion servers, so we explored moving the SSH certificate to our engineer’s laptops and using strong, cryptographic assertions to prove to the BLESS Lambda that the user was previously authenticated with an approved second authentication factor. To this end, Lyft developed blessclient , a simple python client that would run on an engineer’s laptop, allowing the user to authenticate to AWS, prove their identity to the BLESS Lambda, receive an SSH certificate, and setup the user’s normal OpenSSH client to use the certificate for authentication. Since the user would be telling the Lambda that they had been appropriately authenticated, we knew we needed a cryptographic token to prove the user’s identity and make an assertion about how the user was authenticated. Lyft already uses a similar process to prove a user or service’s identity to our secret management system, Confidant . Confidant uses Amazon’s KMS and IAM to generate and validate kmsauth tokens , which are short tokens encrypted by KMS where the encryption context is set to include the user’s identity. The KMS key has policy set on it to only allow an IAM user to encrypt using the key if their username is in the encryption context’s “from” field, and if they have authenticated to AWS using multi-factor authentication. If KMS is able to decrypt the token using the appropriate key, then the validating service knows that only the correct user could have generated the token, and only if they used MFA to authenticate to AWS recently. When establishing an SSH connection, blessclient will prompt the user for an MFA code and then use KMS to generate the kmsauth token. Blessclient assumes a role in a separate AWS account (we use a separate account to prevent admins in our main account from compromising our CA) and invokes the BLESS Lambda, passing the kmsauth token to prove the user’s identity along with the public key the user wants signed. The Lambda returns a certificate. Blessclient saves alongside the user’s private key and loads the identity into the user’s ssh agent. The user’s SSH client can then establish connections until the certificate expires. In most cases this all happens in less than a second, however the latency of three calls to AWS from the user’s laptop is perceptible. We mitigate this by caching enough information that blessclient makes about 1 call to AWS every 30 minutes, and only prompts for the MFA code once every 18 hours. These parameters are configurable in blessclient, so users can adjust for their risk. To install blessclient onto our users’ laptops, we again wanted to reuse existing infrastructure as much as possible. Users clone a git repo, then run a set of bash scripts with make. After seven days, blessclient will pull the git repo and reinstall the client, so users are always using an updated version of the client. Building on our experience with BLESS and blessclient, we thought it would be a good time to address server authentication at Lyft. Typically, when users first SSH to a new server, the SSH client displays the server’s public key fingerprint to the user, then asks the user to accept it. If an attacker later tries to impersonate the server, the public key will not match the known fingerprint and the client will warn the user that something may be wrong. In an AWS environment where we use autoscale groups that provision and terminate hundreds of servers each day, this can lead to security warning fatigue for users. Even more likely, users will simply tell their SSH client to accept all server fingerprints without question. To improve this, we implemented a Lambda and server-side agent to issue a certificate to each server when it starts. We then configure our users’ SSH clients to trust this CA on their laptops. This eliminates nearly all prompts about the server’s key fingerprint for our users, leaving only those where a legitimate issue may exist. Lyft has been using blessclient in production since September 2016. See the talk Lyft engineers gave at BSides SF for more details . Blessclient is open source and ready for other organizations to use! Interested in open source work and having a big impact? Lyft is hiring! Apply through our application system , or drop me a note at csteipp@lyft.com . Stories from Lyft Engineering. 186 Thanks to Evan Davis and Liz Neu . Security Ssh Encryption AWS Lambda Engineering 186 claps 186 Written by Stories from Lyft Engineering. Written by Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-04-11"},
{"website": "Lyft-Engineering", "title": "keeping lyft accessible", "author": ["Scott Berrevoets"], "link": "https://eng.lyft.com/keeping-lyft-accessible-53155f0098b9", "abstract": "Data Data Science Engineering Mobile Product Security At Lyft we’re in a unique position: every day we work on an app that affects people in the real, physical world — not just the digital one. This has an especially large impact on visually impaired users, because our app is easier to use than the real world alternatives — navigating a train station or queueing up at a taxi stand. Lyft aims to make a product that is accessible and easy to use for all of our users. Accessibility at Lyft If an app uses Apple’s standard controls and UI elements, it will likely already work well with VoiceOver ; however, Lyft’s UI deviates enough from standard controls such that we don’t get that benefit out of the box. Another complication is that many portions of the app are controlled by the server, including a lot of the text for buttons and labels. Making these UI elements compatible with VoiceOver often requires tweaking them manually, even if we use native controls. Since we try to keep all of our UI-related code in Interface Builder, and VoiceOver is just another form of UI, we have written a simple `UIView` extension that allows us to enable accessibility on most of our UI directly from Interface Builder. In this extension, we add 2 properties to `UIView`: accessibilitySources : An IBOutletCollection of UIView s accessibilityFormat : a simple string that represents the format of the accessibility label, which is subsequently passed to String(format:) . Every occurrence of %@ in this string will be replaced with the next element of the accessibilitySources , and [self] will automatically be replaced with the current view’s accessibilityLabel . We can ctrl + drag UI elements into the accessibilitySources , and by marking accessibilityFormat with @IBInspectable we can specify the string and the sources all from Interface Builder, keeping the code clean and to the point. For example, we have an accessibility format on the UILabel that displays the selected ride mode: [self] is replaced by the accessibility label, which for UILabel s is the text of the label itself. %@ is replaced by an accessibility source, which we can set up like this: By disabling the accessibility label for the subtitle, the top label’s accessibility label will read “Selected ride mode: Line. Carpool, 2 people maximum.” without touching any code at all. Changing the process As we ramped up our VoiceOver efforts, we wanted to assess whether the changes we made had a meaningful impact to end-users. As developers, we’re too familiar with our own apps to make an honest call about their usability, and when it comes to visual impairment, even a standard usability review could brush over things that would be challenging to a blind or a visually impaired user. This is why we’re working with a dedicated accessibility expert, himself a native VoiceOver user, to constantly validate our work. All the feedback we’ve been getting from VoiceOver users have further motivated our investment in accessibility. We have optimized VoiceOver in the main flows of our app, and we run weekly regression tests to ensure consistency and stability. Our VoiceOver user also works directly with QA engineers, to let them know what he is looking for and what is missing. Over the last few months, we have made many improvements in various parts of our app, but also in how seriously we take VoiceOver-related bugs: we block new releases if VoiceOver users experience bugs when going through a ride. All new features are expected to be optimized for VoiceOver from the start, and we are working hard to optimize existing features as well. We think of accessibility as part of the user interface, just like labels, buttons, and text fields. By having our developers implement support for VoiceOver as part of the initial feature, our visually impaired users will be able to use these features as easily as our regular users. Interested in building the future of ride sharing? Lyft is hiring ! Hit me up on Twitter or at scott@lyft.com . Stories from Lyft Engineering. 67 1 Thanks to Martin Conte Mac Donell . Accessibility iOS Swift Mobile 67 claps 67 1 Written by iOS Developer @ Lyft Stories from Lyft Engineering. Written by iOS Developer @ Lyft Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-09-08"},
{"website": "Lyft-Engineering", "title": "building for the end user", "author": ["Sean Bolton"], "link": "https://eng.lyft.com/building-for-the-end-user-93c9f3a4c7df", "abstract": "Data Data Science Engineering Mobile Product Security In every line of code, engineers make code architecture and implementation design decisions about the products they build. In summation, these micro-decisions set the functionality and ease of future development of the product. Every engineer should play a role in product decisions. When the engineer is a part of the product dialogue and decision making process, the engineer gets to be part of asking the bigger questions of why are they building what they are building and who are they building it for, which it saves time down the road and often makes the work more interesting. Furthermore, the engineer is likely to build toward the need of the customer in every line of code, creating an overall better experience for the end user and a more successful product. Building done in this way creates a better, more human end result. Take a task of making a very simple request for data via a Javascript promise and logging the response. That would look something like this: While this may work, the question is: who is performing this action? Is this something a user is hitting a button to initiate? In the example above, the user would have no idea what is happening. But with the simple addition of a requestInFlight variable which would change state according the the status of the request, we can show the user that a request is being made. At Lyft we built an internal tool to allow our operations team to more efficiently manage driver application documents. The ask was to display the documents and allow our operations and support teams to review them. What we experienced when developing this tool was a maximum latency of 1 second to make the request. But when we looked at who was actually going to be the user of this tool, we learned they could be on any type of internet connection speed, anywhere in the world. In a portion of cases we saw network requests take as long as 7 seconds. The users with the longer network response time would make many unnecessary clicks across the page which was a waste of time and in some cases made the initial request even slower, filling it with additional requests. By implementing the tiny Javascript example above, we were able to show the user what was happening across the network and therefore reduce unnecessary clicks on the page. Because we had a better end-to-end picture in mind, we could build for the people actually using the tool and save work down the road needing to add or change features we didn’t know about. Beyond that, there is something inherently gratifying in knowing you did something to make life a little easier for the people who will be using what you made. This is a very simple example to show that the concept of building for the end user exists in even the most basic project. But, imagine the implications of this concept in building much broader systems, like that for autonomous cars. If we, as engineers, never thought about who would be using these cars, those could be some very unfriendly cars to interact with. The most important skill in being able to implement this in your own project is curiosity. Identify stakeholders and work with them to identify the end users. Write down any questions you may have and set up a basic run through of product usage with these end users where they can use an MVP or beta version of what you are making. Be curious. Ask questions to understand using phrases like: “I noticed that…”, “Tell me more about when…”, or “I’m curious, what were you thinking about during…” These types of phrases are open ended and do not blame the end user of misuse. You may get a lot of information from these interactions, work with other members of your team to consolidate what to work on. Interested in having a big impact by building tools with a focus on the end user? Lyft is hiring ! Or want to chat? Drop me a note at sbolton@lyft.com . Stories from Lyft Engineering. 25 Thanks to Anthony Velázquez and Frank Yoo . UX Web Development Design Product Management Engineering 25 claps 25 Written by I’m a design and growth focused product maker on a mission to make the world feel more human. Learn more: http://seanbolton.me/ Stories from Lyft Engineering. Written by I’m a design and growth focused product maker on a mission to make the world feel more human. Learn more: http://seanbolton.me/ Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-12-07"},
{"website": "Lyft-Engineering", "title": "https medium com adamgreenhall simulating a ridesharing marketplace", "author": ["Adam Greenhall"], "link": "https://eng.lyft.com/https-medium-com-adamgreenhall-simulating-a-ridesharing-marketplace-36007a8a31f2", "abstract": "Data Data Science Engineering Mobile Product Security In this post, we’ll talk about how Lyft’s simulation framework works, and how simulation in general can be an indispensable tool for a data science team. On Lyft’s Marketplace teams, data scientists and engineers work on algorithms that are at the core of Lyft’s business, including dispatch, Lyft Line matching , and dynamic pricing. We’re constantly working on improving these algorithms and simulation and A/B testing are at the core of how we evaluate them. In our last data science blog post , we gave an example of how interference from a simple A/B user test can bias the results of experiments and discussed some alternative randomization strategies. In order to understand how we might evaluate different experimental designs, we need to talk about how we simulate the Lyft system. Marketplace algorithms are difficult to model because they involve the interaction of thousands of humans, all making decisions individually, but also depending on each other in complex ways. The Lyft Data Science team began working on a simulator in 2014 because we needed a tool to evaluate new algorithm ideas. Back then we were primarily interested in improving dispatch algorithms in cases of undersupply — when there were more passengers requesting rides than available drivers. The simulator turned out to be so useful that we expanded it to include many of the products areas and complex algorithms that we work on. Simulation is now an important tool for data scientists at Lyft to improve our algorithms and to evaluate their performance and theoretical efficiency as we grow. As simulation at Lyft has matured, we’ve come up with three guiding principles for running simulations: The simulator should produce metrics similar to what we see in production, for both the “baseline” system and new algorithms. When we launch an A/B test, we expect to get results similar to what we saw in the simulator. This means that the simulator has to stay up-to-date with the latest major product features. At minimum, a simulator should be “directionally accurate” — i.e. the algorithm that performs better in simulation also performs better in A/B tests. We expect results back quickly, and often evaluate algorithm performance across dozens of markets. To do this the simulator should be able to scale in parallel by market, time, and algorithm variant. The simulator should use the same codebases as production when possible. Simulation code should be modular; algorithms and user decision models should be clearly separated. Simulations should be easy to launch, track progress, and get results from. Re-using production code helps ensure accuracy and means that adding a new model to the simulator will streamline the work needed to productionize that model. While this workflow doesn’t necessarily mean that developing new algorithms is easier at the start of the process, the end result is higher quality algorithms, with fewer debugging iterations along the way. Simulations start with real historical passenger app sessions — not just rides, but also people who had the app open but decided not to request. For drivers, we use historically-based geographical distributions to determine starting locations. Then, we make an approximation: the simulator breaks time into discrete chunks, so that everything that happened in the real world in a 10 second period happens in the simulator all at once (10sec is an example, in general the period length is configurable and depends on what’s being simulated). At each time step, the simulator runs two core components: first the marketplace algorithms and then the user behavior models. The algorithms run and update information that the simulated users see. For passengers that are considering requesting, this information includes the Prime Time price and the estimated time to arrival (ETA). Then we use machine learning models to determine passenger and driver behavior in response to the simulated conditions. For example, passengers are more likely to request a ride after seeing a low price and a short ETA. These user behavior models can be very important when testing algorithms that significantly change the information a user sees, especially because we generally don’t have counterfactual data; we know how a user responded to the price they saw in reality, but not how they would have responded to a price that was 25% higher. As the simulation moves through time in its 10 second chunks, ETAs are estimated, ride states and driver locations are updated, Lyft Line matches are made, users “decide” to cancel rides and matches, passengers are dropped off, and key metrics are recorded. As the results come in from many different time slices and markets, we aggregate the results. For instance, if we are working on a new dispatch algorithm, one metric we would look at is the median ride pickup time. Building a simulator from scratch might seem like a daunting task for your team. But it doesn’t have to be. Here are some of the lessons we learned: Start small. Pick one aspect of your product and then build gradually. Build in accuracy evaluation at the start. Make comparing metrics from the production system to simulator metrics a core component of the simulator. When you add a new feature/model, also add its corresponding key metric. Reuse production codebases. You will write far less code and the code you do write will be a lot easier to maintain. Use the simplest models to start. This might seem counterintuitive — we just said we want accuracy and this kind of simplification isn’t good for comparing against baseline production metrics. But you can often get “directionally accurate” results in simulations with very simple models. For example, say that we are interested in whether Model A is better than Model B for Prime Time pricing. We could use a very basic ETA model (e.g. haversine distances with a fixed speed) and still correctly conclude that A is better than B — with at least enough confidence to test A in real life. While our simulation infrastructure was in its early stages, making these sorts of tradeoffs allowed us to keep moving forward with product testing. Automate. Run baseline simulations and track accuracy automatically. This helps developers avoid regressions and saves time when comparing simulations to baseline. The Lyft Data Science team often runs on a simulation-driven development cycle: Come up with a new algorithm idea Code it up Simulate Debug, refine, and/or discard. Not all ideas make it past this step, but if they do… A/B test in the real world. If things go well here, …. Ship improvement to all users This cycle has been very productive for us. We ship improvements regularly. And every idea that gets discarded in step 4 is a bad experience that real users never had to have. This allows Lyft data scientists to debug, refine, and tune complex algorithms, without real-world consequences, and in a much shorter timeframe than A/B testing. This doesn’t mean that A/B tests are obsolete, because the simulator isn’t always a perfect tool. Having a fast simulator leaves us exposed to a multiple testing problem — i.e. we can now simulate thousands of different configurations for an algorithm, one of which is bound to look great but may just be fitting to the inaccuracies in the simulator or in the user behavior models. Once we’ve found an algorithm worth shipping to users, we still use A/B testing to get a precise measurement of how much an algorithm has moved metrics in the real world. The simulator also allows us to test some things that are impossible to know if we could only run tests in the real world. Many of our algorithms implicitly or explicitly make forecasts and it’s often useful to know the “oracle” performance, or how well an algorithm could do if it knew the future perfectly. Some of our algorithms, like matching for Lyft Line, perform differently at different market scales, and it’s useful to know what to expect as Lyft grows in new markets. To answer these questions, we can simulate a market with scaled up supply and demand. With a simulator that is accurate, fast, and easy to use, data scientists gain a powerful tool. Simulation-driven development allows data scientists to test wild new ideas, without creating bad user experiences. “Oracle” and scaled simulations allow data scientists to uncover insights that improve business decisions. If your company wants to start running simulations, our advice is to pick a core use case first, and expand to new use cases as teams develop complex algorithms or business questions. Coming up in the next data science blog post: We’ll use the Lyft simulator to test different randomization schemes so that we can improve our A/B testing methodology and confront the interference problem head-on. If you’re excited about simulation or marketplace optimization, Lyft is hiring Data Scientists in San Francisco and Seattle. Drop me an email at agreenhall@lyft.com . Stories from Lyft Engineering. 220 Thanks to Nicholas Chamandy . Data Science Algorithms 220 claps 220 Written by Stories from Lyft Engineering. Written by Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-10-06"},
{"website": "Lyft-Engineering", "title": "announcing envoy c l7 proxy and communication bus", "author": ["Matt Klein"], "link": "https://eng.lyft.com/announcing-envoy-c-l7-proxy-and-communication-bus-92520b6c8191", "abstract": "Data Data Science Engineering Mobile Product Security Today we are incredibly excited to open source Envoy , our high performance C++ distributed proxy and communication bus designed for large service oriented architectures. The project was born out of the belief that: The network should be transparent to applications. When network and application problems do occur it should be easy to determine the source of the problem. Envoy runs on every host and abstracts the network by providing common features (load balancing, circuit breaking, service discovery, etc.) in a platform-agnostic manner. When all service traffic in an infrastructure flows via an Envoy mesh, it becomes easy to visualize problem areas, tune overall performance, and add substrate features in a single place. Envoy has been in development at Lyft for around 1.5 years. Before Envoy existed, Lyft’s networking setup was fairly standard for a company of our size. We used Amazon’s ELBs for service discovery and load balancing, and a mishmash of different libraries across both PHP and Python. In a few places we deployed HAProxy for increased performance. At the time, we had about 30 services and even at that level of scale we faced continuous issues with sporadic networking and service call failures to the extent that most developers were afraid to have high volume service calls in critical paths. It was incredibly difficult to understand where the problems were occurring. In the service code? In EC2 networking? In the ELB? Who knew? We relied on whatever statistics each application and HAProxy provided as well the extremely primitive CloudWatch ELB statistics and logging. Envoy is influenced by years of experience observing how different companies attempt to make sense of a confusing situation. Initially we used it as our front proxy, and gradually replaced our usage of ELBs across the infrastructure with direct mesh connections and local Envoys running on every service node. In practice, achieving complete network transparency is difficult. Envoy attempts to do so by providing the following high level features: Out of process architecture : Envoy is a self contained process that is designed to run alongside every application server. All of the Envoys form a transparent communication mesh in which each application sends and receives messages to and from localhost and is unaware of the network topology. The out of process architecture has two substantial benefits over the traditional library approach to service to service communication: Envoy works with any application language. A single Envoy deployment can form a mesh between Java, C++, Go, PHP, Python, etc. It is becoming increasingly common for service oriented architectures to use multiple application frameworks and languages. Envoy transparently bridges the gap. As anyone that has worked with a large service oriented architecture knows, deploying library upgrades can be painful. Envoy can be deployed and upgraded quickly across an entire infrastructure transparently. Modern C++11 code base : Envoy is written in C++11. Native code was chosen because we believe that an architectural component such as Envoy should get out of the way as much as possible. Modern application developers already deal with tail latencies that are difficult to understand due to deployments in shared cloud environments and the use of very productive but not particularly well performing languages such as PHP, Python, Ruby, Scala, etc. Native code provides generally excellent latency properties that don’t add additional confusion to an already confusing situation. Unlike other native code proxy solutions written in C, C++11 provides both excellent developer productivity and performance. L3/L4 filter architecture : At its core, Envoy is an L3/L4 network proxy. A pluggable filter chain mechanism allows filters to be written to perform different L3/L4 proxy tasks and inserted into the main server. Filters have already been written to support various tasks such as raw TCP proxy, HTTP proxy, TLS client certificate authentication, etc. HTTP L7 filter architecture : HTTP is such a critical component of modern application architectures that Envoy supports an additional HTTP L7 filter layer. HTTP filters can be plugged into the HTTP connection management subsystem that perform different tasks such as buffering, rate limiting, routing/forwarding, sniffing Amazon’s DynamoDB, etc. First class HTTP/2 support : When operating in HTTP mode, Envoy supports both HTTP/1.1 and HTTP/2. Envoy can operate as a transparent HTTP/1.1 to HTTP/2 proxy in both directions. This means that any combination of HTTP/1.1 and HTTP/2 clients and target servers can be bridged. Our recommended service to service configuration uses HTTP/2 between all Envoys to create a mesh of persistent connections that requests and responses can be multiplexed over. HTTP L7 routing : When operating in HTTP mode, Envoy supports a routing subsystem that is capable of routing and redirecting requests based on path, authority, content type, runtime values, etc. This functionality is most useful when using Envoy as a front/edge proxy but is also leveraged when building a service to service mesh. GRPC support : GRPC is a new RPC framework from Google that uses HTTP/2 as the underlying multiplexed transport. Envoy supports all of the HTTP/2 features required to be used as the routing and load balancing substrate for GRPC requests and responses. The two systems are very complementary. MongoDB L7 support : MongoDB is a popular database used in modern web applications. Envoy supports L7 sniffing, statistics production, and logging for MongoDB connections. MongoDB lacks decent hooks for observability, and at Lyft we have found the statistics Envoy produces are invaluable when running sharded MongoDB clusters in production. In summary, Envoy makes MongoDB far more web scale . DynamoDB L7 support : DynamoDB is Amazon’s hosted key/value NoSQL datastore. Envoy supports L7 sniffing and statistics production for DynamoDB connections. Similar to Envoy’s MongoDB support, having a single source of statistics for all DynamoDB connections from any application platform has been invaluable at Lyft. Service discovery : Service discovery is a critical component of service oriented architectures. Envoy supports multiple service discovery methods including asynchronous DNS resolution and REST based lookup via a service discovery service. Health checking : The recommended way of building an Envoy mesh is to treat service discovery as an eventually consistent process. Envoy includes a health checking subsystem which can optionally perform active health checking of upstream service clusters. Envoy then uses the union of service discovery and health checking information to determine healthy load balancing targets. Advanced load balancing : Load balancing among different components in a distributed system is a complex problem. Because Envoy is a self contained proxy instead of a library, it is able to implement advanced load balancing techniques in a single place and have them be accessible to any application. Currently Envoy includes support for automatic retries, circuit breaking, global rate limiting via our standalone Go/GRPC rate limiting service (to be open sourced shortly), and request shadowing. Future support is planned for automatic bad host outlier ejection and request racing. Front/edge proxy support : Although Envoy is primarily designed as a service to service communication system, there is benefit in using the same software at the edge (observability, management, identical service discovery and load balancing algorithms, etc.). Envoy includes enough features to make it usable as an edge proxy for most modern web application use cases. This includes TLS termination (including client certificate support with pinning), HTTP/1.1 and HTTP/2 support, HTTP L7 routing, as well as raw TCP/SSL proxying. Envoy’s TLS support earns Lyft an “A” on the SSL labs report . Best in class observability : As stated above, the primary goal of Envoy is to make the network transparent. However, problems occur both at the network level and at the application level. Envoy includes robust statistics support for all subsystems. Envoy also supports distributed tracing via third party providers. See the full Envoy documentation for a lot more information. Today we are also open sourcing our discovery service. This is a reference implementation of the service discovery API that Envoy calls, written using Python and using DynamoDB as the backing store. Today, we run Envoy on thousands of nodes and over one hundred services, which in aggregate process over 2 million requests per second, powering every system at Lyft, either real time or otherwise. At Lyft we use Envoy to proxy for Python, Go, C++, and PHP. The fact that we use the same software to proxy for every service means that we have consistent and reliable statistics of our entire distributed system. Developers can quickly see global network health, as well as the health of any individual service to service hop. Developers write their code without knowledge of network topology or whether it is running in development, staging, or production. And most importantly, developers are no longer scared to have service to service dependencies in high volume paths. We believe that the problems that Envoy attempts to solve are faced by many different companies and organizations. We are extremely excited to share Envoy and build a community around its usage. In the near term we have a roadmap planned which includes more rate limiting features including open sourcing our global rate limiting service, advanced load balancing features such as request racing and outlier ejection, and consistent hash load balancing, redis support, among others. Beyond that we are eager to see what feature requests we get once the community starts to take a look. Please reach out to us if you have questions about Envoy or are considering giving it a shot. We are happy to help and would love to hear from you. Envoy is the product of too many people to name here individually. From direct code contributions, design feedback, and ultimately usage by a large number of developers, many people have contributed to the product that we are releasing today. A huge thanks to everyone. Interested in open source work and having a big impact? Lyft is hiring ! Drop me a note on Twitter or at mklein@lyft.com . Stories from Lyft Engineering. 392 5 Microservices Http2 Grpc Cplusplus Engineering 392 claps 392 5 Written by Engineer @lyft (https://mattklein123.dev/) Stories from Lyft Engineering. Written by Engineer @lyft (https://mattklein123.dev/) Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-09-14"},
{"website": "Lyft-Engineering", "title": "extending iam policy and aws apis using kms and lambda", "author": ["Ryan Lane"], "link": "https://eng.lyft.com/extending-iam-policy-and-aws-apis-using-kms-and-lambda-13386dfb36af", "abstract": "Data Data Science Engineering Mobile Product Security I believe AWS Lambda is an exciting service, though maybe not for the reasons you’d expect. Rather than breathlessly talking about “serverless” services, I think Lambda has potential as a means of providing simple functions that can act as a sort of infrastructural standard library, or in the case of this post, as a means of providing functionality missing from AWS’s own APIs. Let’s take elastic IPs (EIPs) as an example. If you have an autoscale group that scales up and down based on CPU, but have a need to assign EIPs to each instance, you either have to run an out of band service that manages all of your EIPs, or you need to provide access to the autoscale group’s IAM role to manage every EIP in your infrastructure. It’s not possible to scope individual EIPs to IAM roles. A lambda that acts as an out of band service, triggered by SNS autoscale events, is a great option here, of course; however, there’s many other examples of AWS APIs that are also un-scopeable and don’t work with this model (most EC2 apis, most route53 actions, etc.). So, I had an idea I wanted to explore for a hackathon this past week, and the API for elastic IPs was a relatively simple API to target. Last year, during another hackathon, I explored re-using AWS’s IAM credentials for service to service authentication by using the KMS service. We call this KMS authentication. Since then, we’ve integrated it into Confidant and have split it into a general purpose library . The interesting thing about KMS authentication is that it’s an effective way of extending IAM policy, in a way that can be used within your own service. Combining the extension of IAM policy and lambdas allows us to “scope the un-scopeable”. Let’s take a look at an IAM policy that’s extended to represent actions and resources in a lambda: The IAM policy for the lambda itself is much simpler: Before invoking the lambda, we generate the token and username: Next, we invoke the lambda with the action, resource, instance-id and the username and token: The lambda will verify the token, then check to ensure the IAM role specified in the ‘from’ context is attached to the instance-id (as an instance profile). It then checks to ensure the action is valid for the specified resource. For instance, for an associate action, it’ll check to ensure an EIP isn’t already associated with another instance. Last, it’ll do the EIP action. This is a proof of concept, built as a hackathon project, so it could use some improvements, and some of the implementation and design is bound to change; but, take a look at the code , and if you’re interested help make it better! Interested in open source work and having a big impact? Lyft is hiring ! Drop me a note on Twitter or at ryan.lane@lyft.com . N E X T → Scoping AWS IAM roles to Docker containers Stories from Lyft Engineering. 13 Thanks to Steve Woodrow . AWS AWS Lambda Security Engineering 13 claps 13 Written by Security Engineer and Open Source Team Lead at Lyft Stories from Lyft Engineering. Written by Security Engineer and Open Source Team Lead at Lyft Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-08-02"},
{"website": "Lyft-Engineering", "title": "experimentation in a ridesharing marketplace", "author": ["Nicholas Chamandy"], "link": "https://eng.lyft.com/experimentation-in-a-ridesharing-marketplace-f75a9c4fcf01", "abstract": "Data Data Science Engineering Mobile Product Security This is the final installment in a three-part story on strategies for A/B testing in Lyft’s unique two-sided marketplace. In Part 1 we introduced interference bias , explained why experimentation is non-trivial when so-called network effects are present, and qualitatively explored bias-variance tradeoffs. Part 2 described the Lyft Data Science team’s simulation infrastructure, an indispensable tool in understanding market dynamics and sanity-testing new algorithms. If you have already read those posts — welcome back! If not, and you find yourself wondering What is the point of all this? , we suggest going back to give them a quick read. We have two main goals with this post. The first is to use the simulation methodology from Part 2 to quantify the phenomena outlined in Part 1 . Second, we will borrow arguments from causal inference to motivate a two-stage experiment design and some ideas for how to construct interference-free treatment effect estimators. We simulated a 4-week experiment using historical demand and supply in one of Lyft’s medium-sized markets. Specifically, we applied the simple experimental manipulation described in Part 1 : we subsidized Prime Time pricing (PT) for all passengers in the treatment group. Such a treatment is known to have strong network effects, as subsidizing PT for user A can mean that user B is less likely to have a driver nearby when she opens the Lyft app. This happens because during times of undersupply, users who see a cheaper price will tend to snap up the available drivers. First we ran our simulator over the entire period both with and without the PT subsidy, in order to take measurements in the global treatment and global control configurations. This gives us a ground truth comparison for each metric. Next, we simulated experiments under the following designs: (a) alternating time intervals (one hour), (b) randomized coarse and fine spatial units ( geohashes at granularities 5 and 6, respectively), (c) randomized user sessions. See Figure 1 for an illustration of the two different spatial randomizations in an arbitrarily chosen Lyft market. We focus on three key metrics for this simulation study. The first is availability , defined as the proportion of user app opens for which there is an available Lyft driver within some context-dependent radius. The second metric is the average (across user sessions) estimated time of arrival (ETA) of the nearest available driver. ETA is one measure of the quality of Lyft’s service levels. Third, we consider the number of completed Lyft rides , normalized by group size. Rides is our most important top-line business metric, but in a simulation setting is somewhat dependent on our models of passenger and driver behavior. In all cases we measure percent changes in the metric in the treatment group relative to the control group. When we compare the global Prime Time subsidy simulation to the global control simulation, we measure a decrease in availability, an increase in ETAs, and an increase in total rides. These changes are in line with the example in Part 1 , and reflect the fact that passengers are more likely to request a ride when PT is subsidized, potentially worsening undersupply situations. Figure 2 illustrates the claim made in Part 1 that different randomization schemes inhabit a spectrum of bias-variance tradeoffs. The hourly scheme is almost unbiased for all metrics, but can have large variance, e.g. for the ETA metric. This is because hour-to-hour variability in Lyft supply and demand is substantial, having components which are non-cyclical at the week level — including weather, traffic, special events and Lyft promotions. Broadly speaking, bias increases and variance decreases as the experimental units become finer. Interestingly, a different randomization scheme minimizes root-mean squared error (RMSE) for each metric. However, even having the smallest RMSE is not sufficient. For example, if we blindly applied a random-session design, we would wrongly conclude that subsidizing Prime Time has no effect on driver availability — with a tight confidence interval! The session design is most susceptible to network interference bias, since two users standing right next to each other, and thus sharing an identical pool of candidate drivers and Lyft Line matches, may well end up in different treatment groups. The hourly experiment did well at estimating the change in rides resulting from a Prime Time subsidy. However, because the alternating hour design has only two possible random treatment allocations (corresponding to the assignment of the first hour), a generic and accurate variance estimator does not exist to our knowledge. This is a major flaw in the alternating interval methodology. Moreover, that design failed badly on the task of estimating the change in ETA, a metric that is more sensitive to hourly fluctuations. In a sense, alternating time interval experiments perform well if the effect size is very large or we just happen to get lucky with temporal fluctuations — something that’s difficult if not impossible to know in practice. Spatial designs represent a nice compromise. But in this case they may have led to a bad product decision, by systematically underestimating the negative impact of the PT subsidy on Lyft service levels. Interference bias is clearly a problem when it comes to estimating the network-wide changes in our metrics of interest. And naive attempts to design our way out of this problem can indeed eliminate the bias, but bring in unwanted variance. The remainder of this post presents a useful way of decomposing and understanding experimental effects contaminated by interference. First, let’s take a step back and ask a pretty fundamental question: What treatment effect do we wish to estimate? There are three important types of treatment effects in the context of interference: direct, indirect, and total effects (see this excellent article by Hudgens and Halloran). In a nutshell, the total effect is what we care about. It’s a sum of direct effects: how does a unit’s treatment assignment affect its outcome? indirect effects: how do the treatment assignments of other units affect a unit’s outcome? An experiment assignment mechanism is a rule for randomizing the population of units to either treatment or control. A direct causal effect is a function of one such rule, while an indirect effect is a function of a pair of rules. For simplicity, we assume simple random sampling and identify the assignment mechanism with the proportion of units that will be randomly selected for the treatment. (In general, assignment can be a much more complicated random function.) A unit here could mean different things: user, user session, geohash, etc. — more on that later. To be more precise, we need some mathematical notation. Feel free to skip ahead if that’s not your thing. Let Y_i ( Z ; p ) denote the response of unit i exposed to Z (= 0 for control or 1 for treatment) assuming that a proportion p of the N units are assigned to the treatment. Recall that these responses are potential outcomes , as discussed in Part 1 of this post. They are not all observable in the same experiment — a unit i only gets one condition assigned in reality. But they are nonetheless well-defined. We have: Direct causal effect of the assignment mechanism p : Indirect causal effects of the assignment mechanism p vs. mechanism q (there are two such effects, corresponding to units in the control or treatment): Total causal effect of the assignment mechanism p vs. mechanism q : The total effect can be decomposed as a sum of direct and indirect effects in two different ways: Note that when the assignment mechanism is held fixed ( p = q ), the total effect is the same as the direct effect, because both indirect effects are zero. Although we work with absolute-scale causal effects for simplicity, relative-scale effects can be defined analogously. Now let’s go back to our subsidized PT example. The direct and indirect effect functions may be interesting in their own right. But as discussed in Part 1 , the treatment effect that’s most critical is the difference between our usual PT algorithm applied throughout the network, and subsidizing every user’s PT. In the above notation, this is just T (1, 0). But the decomposition of T (1, 0) into direct and indirect effects doesn’t quite make sense, because neither Y (1; 0) nor Y (0; 1) is well-defined! Nevertheless, we can make a continuity assumption and assert the existence of a limiting total treatment effect This decomposition provides some pretty intriguing estimands — functions and scalars that we’d like to estimate. But how do we do it? One idea is to use a 2-stage experiment design motivated by the above formulas. In the first stage of randomization, we can divide the units into J independent, non-interfering groups. Say group j contains N_j units. Next we can draw a random sequence of group treatment proportions p_j for j = 1, …, J from some distribution on the unit interval. At the second stage, we can randomly assign p_j N_j units in group j to the treatment (and the rest to the control) for each j . In the Prime Time example, we might choose to use disjoint time intervals as groups, since they are approximately non-interfering. One could also make an argument for very coarse spatial groups. In defining within-group units we have a number of options, including fine spatial cells, users or user sessions. We simulated a two-stage design in the same region and four-week period as above, using hourly groups, user sessions as units, and a uniform distribution over treatment proportions. This design allows us to ‘observe’ a large number of different treatment assignment mechanisms acting on the network. From that spectrum of assignments, we can build up some fairly simple effect estimates. The most obvious way to estimate our decomposed causal effects in this 2-stage setting is by first discretizing the p’ s into some number of bins. Let z and y denote, respectively, the realized binary treatment assignment and observed response for a given unit in a given group. We can estimate the direct treatment effect at treatment proportion p as and the indirect treatment effects of p vs. q as (It will often make sense to replace these sums with weighted sums, for example when the units are of different sizes.) This gives us a vector of direct effect estimates and two matrices of indirect effect estimates. In practice these may be quite noisy, but we can smooth them. Smoothing should be done with care, as we wish to extrapolate the estimates to p , q in {0, 1}. In our simulation study we’ve obtained smooth estimates of the direct effect curve and the indirect effect surfaces using R’s loess package . See Figures 3 and 4 for visualizations of our estimates of causal effects on the rides metric. The raw effects were indeed quite noisy for these data, as can be seen in the plots. Let a tilde (~) denote a smoothed functional estimate. We can estimate the global treatment effect of interest by Alternatively, we could directly smooth the 2-d total effects matrix (not shown here). In our simulation study, this procedure led to essentially unbiased estimates for the rides and ETA metrics (about as unbiased as the alternating hour design). It did not perform as well for the availability metric, however. This may be partly explained by the smaller baseline effect size for that metric — availability is very close to 100% in this market. Given its binary nature, availability may also vary less smoothly as a function of p . The idea outlined above is nice for three main reasons: It has the potential to reduce or eliminate interference bias . It allows us to estimate a continuum of direct and indirect effects , which is useful in problems where the global treatment effect is not the only thing of interest. For example, Lyft may want to provide coupons to a subset of passengers, and understand the effect on system-wide ETAs as a function of subset size. Because 2-stage randomization allocates fine units (within coarse units), there are natural resampling-based variance estimators for these statistics. Disaggregating the total treatment effect into direct and indirect components is a useful way to think about the network interference problem. However, it is not a silver bullet, and there are some caveats worth mentioning. The 2-stage estimator did not manage to decrease variance in the above example — in fact, the variance was higher than for the alternating hour design for all three metrics. This is probably not a surprise after looking at the raw estimates in Figures 3 and 4. And because fitting a smooth curve to noisy points is itself a noisy process — especially when extrapolating to the edges of the data. Nevertheless, there are some obvious improvements that can help reduce variance, including Sampling treatment proportions from a more U-shaped distribution Imposing negative correlation between treatment proportions in adjacent groups Stratification or pairing of groups and units within groups The Lyft Data Science team is excitedly exploring these and other directions in our quest for the perfect experiment! In this three-part journey we encountered a nonstandard data science challenge stemming from Lyft’s unique two-sided market dynamics. In order to shed light on it, we leveraged two very different technologies: scalable, high-fidelity simulation, and causal inference. In the process we illustrated two things that Lyft’s Data Science team members do on a regular basis: build cool machinery and think deeply about data! If you’re interested in developing experimentation methodology, or in building the underlying algorithms that we pit head-to-head in these A/B tests, then join us — Lyft Data Science is hiring ! Send me an email at chamandy@lyft.com. Stories from Lyft Engineering. 263 Data Science Software Development Ridesharing Experimentation 263 claps 263 Written by Scientific Director at Lyft Stories from Lyft Engineering. Written by Scientific Director at Lyft Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-12-15"},
{"website": "Lyft-Engineering", "title": "confidant version 1 1 released", "author": ["Ryan Lane"], "link": "https://eng.lyft.com/confidant-version-1-1-released-4ce8e7f39866", "abstract": "Data Data Science Engineering Mobile Product Security Nearly a year ago we announced the initial release of Confidant . Since then we’ve continued active development, and today are happy to announce the release of version 1.1, with some new major features and lots of bugfixes. Here’s some of the major changes we’ve made: We’ve split our KMS authentication code into a separate library, kmsauth . Along with this, we’ve also introduced a new KMS authentication token version (v2), which enables service or user authentication. This change also allows for the KMS authentication token specification to be modified over time in a versioned manner. To make KMS authentication easier to manage we’ve added documentation for using IAM policy, rather than needing to rely on Confidant-managed KMS key grants. It’s now possible to disable the grant management in Confidant and solely rely on IAM policy. We’ve added a new form of secret, called “server-blinded credentials”. Server-blinded credentials enable you to manage secrets that even the Confidant server can’t decrypt. This feature can be used for secrets that are considered to be extra-sensitive, like TLS certificate keys, ensuring that only the intended targets can decrypt the secrets, even if access is accidentally granted elsewhere, or if the Confidant service itself is compromised. We’ve added an opinionated client: confidant-client . confidant-client can be installed through pip, via pypi. We believe the primary use of this library will be to fetch service secrets, so we’ve also included a formatter that can reformat the service secret data into a few output formats. Thanks to v2 KMS auth tokens, the client can be used to update Confidant data from the CLI, including support for the new server-blinded credentials feature. This client can also be used as a library, for directly embedding Confidant into your Python applications. Thanks to contributions from Andy Brody at The U.S. Digital Service (USDS), the authentication code for frontend authentication has been refactored to be modular. As part of this work, Andy contributed an authentication module to support SAML authentication. Also, a huge thanks to Andrew Dunham and Jesse Luehrs, from Stripe, for contributing an authentication module for header authentication, for offloading frontend authentication to a proxy. We’ve added a proper login/logout flow for frontend authentication, and have switched the default session management to use secure cookies, rather than redis-backed sessions, removing the redis requirement for installation. We’ve made it possible for Confidant to bootstrap its own secrets, using KMS, to make deployment more secure. There’s more features and fixes that come along with this release; see the changelog for more information. We’re also changing the development model for Confidant starting with this release. Though Lyft has a continuous release model for Confidant internally, our branching model thus far, for the public version, hasn’t properly reflected our efforts. We’ve been working in the 1.1 branch, and have been keeping master stable, to keep the initial release installation instructions accurate for the stable version. When viewing the project, this makes it seem like no work is occurring, since it’s in a non-visible branch. Going forward we’ll be using a standard open source model of having master be the development branch, while having branches for stable and unreleased versions. The docker images for Confidant will reflect this, with latest being bleeding edge, and tags available for branches and releases. As before, if you have any questions or need any help with Confidant, you can reach us in the #confidant channel in freenode , or the confidant-users in google group . You should also check out the docs and give us feedback in our github issues . Interested in open source work and having a big impact? Lyft is hiring ! Drop me a note on Twitter or at ryan.lane@lyft.com . N E X T → Extending IAM Policy and AWS APIs Using KMS and Lambda Stories from Lyft Engineering. 18 Thanks to Martin Conte Mac Donell and Steve Woodrow . Security AWS Engineering 18 claps 18 Written by Security Engineer and Open Source Team Lead at Lyft Stories from Lyft Engineering. Written by Security Engineer and Open Source Team Lead at Lyft Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-08-30"},
{"website": "Lyft-Engineering", "title": "experimentation in a ridesharing marketplace", "author": ["Nicholas Chamandy"], "link": "https://eng.lyft.com/experimentation-in-a-ridesharing-marketplace-b39db027a66e", "abstract": "Data Data Science Engineering Mobile Product Security Technology companies strive to make data-driven product decisions — and Lyft is no exception. Because of that, online experimentation, or A/B testing, has become ubiquitous. The way it’s bandied about, you’d be excused for thinking that online experimentation is a completely solved problem. In this post, we’ll illustrate why that’s far from the case for systems — like a ridesharing marketplace — that evolve according to network dynamics. As we’ll see, naively partitioning users into treatment and control groups can bias the effect estimates you care about. To consult the [data scientist] after an experiment is finished is often merely to ask [her] to conduct a post mortem examination. [She] can perhaps say what the experiment died of. — paraphrasing R. A. Fisher, 1938. What Fisher was getting at, simply put, is that experiment design matters. So much so that a careless experiment design can sometimes result in data that’s useless for answering the question of interest. Imagine that the entire Lyft network is encapsulated by the tiny square region illustrated below. When users A and B open the Lyft app (at approximately the same time), there is only one driver available nearby. We call such a scenario undersupply . In such cases, Lyft sometimes applies Prime Time pricing in order to maintain driver availability. Prime Time is a price surcharge expressed in percentage terms, and can take on different values depending on the extent of undersupply (+25%, +50%, etc). For simplicity, in this example we assume that Prime Time is binary — there either is Prime Time (at some fixed value) or there is not. We also assume that the supply effects of Prime Time happen at a slower timescale than the demand effects, and therefore we can ignore them. In other words, that passengers react more quickly to Prime Time than drivers do. Suppose that we want to estimate the effect of Lyft subsidizing Prime Time — i.e. paying Prime Time on behalf of the passenger, without ever even displaying it to the passenger. We’ll use a green marker in subsequent pictures to denote a passenger who got the subsidy. A fun metaphor here is that of two parallel universes. We are interested in the difference between the factual universe, where users get Prime Time when there is undersupply, and a counterfactual one, where Lyft subsidizes Prime Time. These two universes are illustrated in the picture below. Note that without any intervention, we would only observe the top universe, which we call the global control. The global treatment, on the other hand, corresponds to treating all passengers with the Prime Time subsidy. Suppose that the metric of interest is the total number of rides completed on average (or in expectation). We would like to know how this number changes between the two parallel universes. Let’s assume a simple probability model to make this example easy. Specifically: When there is no Prime Time, a passenger who opens the app and sees a driver available always requests a ride When there is Prime Time, the same passenger has a 50% chance of requesting a ride Neither drivers nor passengers ever cancel — every request leads to a completed ride Under the global control scenario, the average number of rides taken by passengers A and B is 0.75. To see why, assume without loss of generality that passenger A opens the app a few seconds before passenger B. Half the time, user A will take the ride despite seeing Prime Time, and the number of rides is 1. A quarter of the time, User A will choose not to request, and User B will take the single ride instead. Otherwise, both users decline the Prime Time, and n0 rides are taken. By symmetry, the same is true if B opens first. The expectation is therefore Under the global treatment, neither passenger sees Prime Time and the situation is much simpler. The first user to open the app automatically takes a ride, and the second is out of luck. Since a single ride is always taken, 1 is also the expectation. Comparing these two universes, we see that the global average treatment effect, i.e. the ground truth we would like to estimate, is given by Subsidizing Prime Time results in a 1/3 increase in rides in our simple model. This treatment effect is of course unobservable in real life. So we must find some way to estimate it. The standard way to A/B test an intervention like subsidized Prime Time is to pseudo-randomly assign users (in this case passengers) to either the treatment or control group, for instance by hashing their user IDs into buckets. In our example, the average result of such a randomization is that one user sees Prime Time while the other doesn’t — as illustrated by the following picture. This scenario corresponds to yet a third (mutually exclusive) parallel universe! In order to estimate the effect of the treatment on a metric of interest for a random-user experiment like this one, one typically does the following: Estimate the global control metric value by restricting to users in the control group Estimate the global treatment metric value by restricting to users in the treatment group Compute the relative difference between the estimates from 1 and 2 Let’s see what happens when we apply this logic to our simple example. Remember that each user has a 50% chance of opening the app first. Let’s first consider user B, who happens to be in the treatment group (subsidized Prime Time). In our simple model, she is guaranteed to request and complete exactly one ride if she opens the app first. On the other hand, if she opens the app second, she will complete one ride if and only if user A decided not to request. That event also happens with a 50% probability, so given that user A opens the app first, user B expects to take half a ride. Combining all this, the expected number of rides for user B is The situation for user A is even easier. User A cannot take a ride if user B opens the app first — so the expected value is 0 in that case. And we know that user A, who sees Prime Time, will request a ride half of the time given driver availability. So the expected number of rides completed by A is Now let’s compute an estimate of the percent change in our metric due to the Prime Time subsidy. Obviously, this is much bigger than the ground truth effect size of 33% that we calculated above — we overestimated the effect of the Prime Time subsidy by a factor of 6! Admittedly, two users is not very many, so you might think that this fictional A/B test is suffering from small sample size problems. Surely, a user cannot actually take 0.25 rides. But imagine that the real Lyft network is composed of copies of this 2-passenger sandbox, all evolving independently over time, replenishing drivers and passengers at a constant rate. We can construct a much larger scale example, with many such boxes, where all of the above calculations still hold. What happened in the above example is due to a statistical phenomenon known as interference (not to be confused with inference). To properly define it, we first have to introduce the notion of a potential outcome . The idea behind potential outcomes is simple: every experimental unit (e.g. user) walks around with two pieces of paper, one in each back pocket. On one of these papers is written that subject’s inevitable outcome should she happen to be assigned to the control group. On the other, her outcome given assignment to the treatment. Together, the two pieces of paper are a unit’s potential outcomes — the set of things that could potentially happen to her if she participates in the experiment. Typically, these outcomes are considered fixed and deterministic — the only thing that is random is the unit’s assignment to an experiment group. A key assumption of causal inference is that what’s written on those two pieces of paper is unaffected by the experimental assignment that the unit happens to get, and by the assignments of every other unit in the experiment . Interference occurs when the group assignment of unit A changes any of the potential outcomes of unit B. This is precisely what we saw in the toy example above, with the outcome of interest being whether or not a ride is completed. When user A’s Prime Time is subsidized, user B is less likely to be able to complete a ride (regardless of whether or not user B’s Prime Time is also subsidized). In medical statistics, the notion of interference arose in the study of vaccines for infectious diseases. The effectiveness of a vaccine on one subject’s outcomes depends on how many others in his social circle also received the immunization. In other words, one subject’s treatment can offer protective benefit to other, possibly untreated subjects. The result is that the measured difference between treated and untreated subjects (the benefit attributed to the vaccine) will shrink. Above, user A’s Prime Time was “protective” for user B’s propensity to successfully complete a Lyft ride — which in this case led to an exaggeration of the true effect size. In general, interference bias can occur in either direction. Lyft is not the only technology company trying to mitigate statistical interference in A/B testing. Researchers from Google and eBay have observed the same phenomenon in applications where advertisers or users interact within online auctions. Coarser randomization, say at the auction level, can help (but not completely) mitigate the bias. The eBay example is particularly germane to our toy example as the authors characterize interference bias in relation to supply and demand elasticity. The interference problem also occurs in experiments for social networks, where a user’s response to treatment may contaminate adjacent nodes in the graph. Some progress has been made on this problem for relatively static networks, with graph clustering playing a central role. Complicating things in our world is the fact that the Lyft network is both two-sided (passengers and drivers) and has a graph structure which is incredibly dynamic. Thus interference is difficult to model explicitly. Randomizing users is certainly not the only way to construct online experiments in a ridesharing marketplace. One can alternatively randomize app sessions, spatial units ranging from square blocks to entire cities, or even time intervals. The coarser these experimental units, the stronger the protection against interference bias in your effect estimates. However, the cost is increased variance in your estimators, because coarse units are naturally less numerous than fine units (variance scales as one over the sample size) — and sometimes just as heterogeneous. This cost can be substantial. Nevertheless, alternating time intervals between global control and global treatment configurations was a successful strategy for the Lyft Marketplace team in the early days of experimentation. The table below positions these various randomization schemes on the continuum of bias-variance tradeoffs. To rigorously quantify these tradeoffs, however, requires a careful simulation study. Before we can embark on that adventure, we need to describe the elaborate simulation framework designed and built by the Lyft Data Science team. Luckily, that is precisely the subject of our next installment of this blog post, Part 2: Simulating a ridesharing marketplace . Stay tuned! Interested in experiment design, marketplace optimization, or Data Science in general? Lyft is hiring ! Drop me an email at chamandy@lyft.com . Stories from Lyft Engineering. 569 7 Thanks to Carlos Whitt . Lyft Data Science 569 claps 569 7 Written by Scientific Director at Lyft Stories from Lyft Engineering. Written by Scientific Director at Lyft Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-09-02"},
{"website": "Lyft-Engineering", "title": "freezing time", "author": ["Martin Conte Mac Donell"], "link": "https://eng.lyft.com/freezing-time-6ebe8ffe3321", "abstract": "Data Data Science Engineering Mobile Product Security - “Time doesn’t exist, clocks exist” There are many features at Lyft where it’s critical to have an accurate (and trustworthy) time source. This apparently trivial requirement presents a few challenges when the timestamp comes from a device we can’t control: Can we trust the clock? Is the clock synchronized with an accurate time source? Can we ensure that the request to our APIs containing the timestamp was not tampered with? This post will focus on the first challenge, which is having an accurate time source but won’t get into details about the second part. The safest assumption is that any request can be tampered with, and safeguards on the server need to be implemented. Although most devices are synchronized to some external time source, many still have skewed clocks when time syncing is not enabled (which can happen either because it’s not configured correctly or because the user manually set the clock to an arbitrary time). As it turns out; based on more than 5M+ Lyft sessions on iOS, 1% of our users have skewed clocks. But how can two clocks ever be synchronized in the first place? … or we might even be more inquisitive about this issue by asking: how can anyone (or anything) observing a clock, communicate what the current time is with good enough precision? By taking a peek at the clock you can see the “current” time but, how long does it take you to read it, to think about it and to put together the phonetic combination of your speech? Is it the same time by the moment you say it out loud? What about when the recipient finally hears it? Was the wall clock even accurate in the first place? The keen reader may have noticed at this point that making this (arguably trivial) synchronization both accurate and precise is surprisingly difficult. Luckily computers are fast reading the local clock. This operation is heavily optimized on the kernel to the point that asking for the time would not even access the actual clock (the kernel synchronizes an independent system clock every once in a while with the Real Time Clock and keeps track of time by counting timer interrupts). But even if reading the local time is not an issue; trusting the clock and sending the information to another machine… well, it’s complicated . As the need for precise time synchronization has increased, several protocols have been developed to control system time. Three of the most common protocols are Network Time Protocol (NTP) , Inter-Range Instrumentation Group (IRIG) time code and Precision Time Protocol (PTP) . Given the simplicity of its implementation, its suitability for mobile networks and the fact that it doesn’t need extra hardware; we’ll focus on NTP. NTP relies on very accurate clocks and a protocol for transmitting the time between devices across shaky networks. It defines a layered hierarchy (strata) of time quality: on the top of the chain (a.k.a. stratum 0) we find extremely accurate reference clocks that can either be a very expensive atomic (cesium, rubidium) clock, receivers for time signals like GPS (which are just big cesium clocks orbiting the Earth) or other radio clocks. Stratum 1 devices are connected directly to these clocks. Each synchronization between devices increments the stratum number so, for example, devices on Stratum 3 are 2 “hops” to a reference clock. The synchronization between layers of the Strata is done using UDP/IP by sending several packets, the greatly simplified explanation is: when sending out a request the client stores the local time and for the response the server includes the server time when it got the request and when it sent the response. When the response is received by the client, a new timestamp is stored on the client. With these 4 times the client can calculate: The time difference between the two devices. The traveling time ( delay ) between the client and the server, which will be estimated to be half of the total delay minus the remote processing time. The maximum offset error ( dispersion ) which is an estimate of the total amount of error/variance between that server and the correct time. Great, we understand how NTP works but how can you use it from your iOS app?… We’re open sourcing Kronos which is a Swift library that calculates a sane time from a pool of NTP servers using the Network Time Protocol. Kronos’ interface is extremely simple with just two public interfaces ( Clock.now, Clock.sync) and its design is optimized for: Supporting a monotonic clock. Getting a sane time as fast as possible. Continuously improving the time accuracy by sending more NTP packets to many servers on a pool. Unlike the system’s clock, or a wall clock the time reported by this monotonic clock is not based on the device’s clock, and therefore is unaffected when the local time is changed while your app is running. Instead, the first time the clock is synchronized, Kronos will store the accurate time along with a delta since some arbitrary instant (e.g. uptime). After that, accessing: Will return the local time based on the last known accurate time + delta since last sync . Clock.now will get more and more accurate as the synchronization receive more responses from NTP servers but you can use it right away after the first packet. You can also access the time as soon as it’s available by: Kronos supports IPv4, IPv6 and up to NTP v4. If you need an accurate NTP clock try Kronos out, we believe it will show you a good time! Get started with Kronos on Github Interested in open source work and having a big impact? Lyft is hiring ! Hit me up on Twitter or at martin@lyft.com . N E X T → Extending IAM Policy and AWS APIs Using KMS and Lambda Stories from Lyft Engineering. 176 5 Thanks to Keith Smiley , Ryan Lane , and Carlos Whitt . Networking iOS Swift Engineering Mobile 176 claps 176 5 Written by @fz Stories from Lyft Engineering. Written by @fz Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-09-08"},
{"website": "Lyft-Engineering", "title": "building single activity apps using scoop", "author": ["Pierce Johnson"], "link": "https://eng.lyft.com/building-single-activity-apps-using-scoop-763d4271b41", "abstract": "Data Data Science Engineering Mobile Product Security More than a year ago, the Android team at Lyft transitioned from using Fragments to plain Android views. A few months ago, we open sourced Scoop , a framework that lays the foundation for this transition. In this post I’ll elaborate on the decision to transition to a view only framework, and then I’ll provide a brief introduction to Scoop so you can build your own view only apps. Since the beginning, the Lyft Android team has faced challenges instantiating more than one Activity because two core components of our app’s design, the map and the navigation drawer, posed technical limitations that precluded using more than one Activity. Let’s examine each individually. The Map The ride request screen is something that all of our customers are quite familiar with. This screen hosts a map, and some content layered on top of it exhibiting different states of our ride flow. If we instantiate the map more than once, our animations would drop frames because we would be consuming too much memory for a smooth transition, and therefore inevitably degrade the user experience. As a result, we only have one instance of the map, with various views layered on top of it. The Navigation Drawer Our navigation drawer is not much different than similar navigation menus on other apps. When you click an item in the drawer, the screen swap is reflected in the main view. We couldn’t use activities to implement this behavior because instantiating multiple activities to display views would break the drawer animation. We narrowed our choices to either views or Fragments, and moved towards a decision. Back in 2012, Android Fragments were popular within the Android community. They were designed to work with tablets, phones, and screens of all sizes, and to allow developers to decompose an app’s UI into small reusable components. So we evaluated Android Fragments and decided to build our application as a single activity with screens represented as fragments (and child fragments in more complicated scenarios). As the complexity of that application grew, we began to realize that the extensive usage of Fragments came at a high cost. The Fragment lifecycle was becoming more and more difficult to manage and created bugs that were difficult to debug. Other problems occurred due to what we call “Schrödinger’s Fragment” where you cannot immediately retrieve a reference to your Fragment back from the Fragment manager, which led to a race condition when you’d want to use the returned fragment but it wasn’t there yet. Square’s Pierre-Yves Ricau has a great article illustrating further pitfalls of Fragments here . Because of these issues, we decided to research other alternative solutions. We landed on a solution that has been there since the beginning — plain Android views. Views provided us with numerous benefits. They have a simple lifecycle with full control over instantiation, and we could remove the issues around race conditions occurring with Fragment transactions. However, views do sacrifice backstack and navigation support. In order to completely remove Fragments from our app, and focus solely on views, we needed to fill these technical gaps. We developed an open source view-only framework named Scoop to embrace the benefits of views and address the technical drawbacks of Fragments. Scoop is composed of five key components: Screen, ViewController, Router, UIContainer, and Scopes. Screen is a metadata object that specifies which view controller to display, optional data to pass to a view controller, and optional transitions between screens. Screens will lay the foundation for your application’s UI. ViewControllers manage a portion of the user interface as well as the interactions between that interface and the underlying data. Similar to an activity or a Fragment, ViewControllers require a layout id to render a view that they control. However, ViewControllers do not have a complex lifecycle. The ViewController lifecycle only has two states: “onAttach” and “onDetach”. Don’t think of ViewControllers as presenters, rather imagine them as less complex ViewGroups with some unique benefits. The goal of introducing ViewControllers was to solve 2 primary issues related to views: Inheritance View are usually inherited from different ViewGroups (LinearLayout, RelativeLayout, etc), so if you want a base class shared amongst ViewGroups you’re pretty much out of luck. While we normally recommend composition over inheritance , sometimes it is more effective to use inheritance when it adheres to Liskov’s substitution principle . 2. Android design preview If you create a custom view and have some nontrivial logic inside it, Android Studio’s design preview will throw exceptions and fail to render the view. You can add a flag, isInEditMode, to solve this issue, but we found that most developers choose to ignore the flag, and eventually most previews display exceptions similar to the picture above. The Router and UIContainer classes are closely related, and are responsible for navigation between Screens. The Router class maintains the Screen backstack, and provides 5 methods for navigation. goTo — Go to Screen C and add it to the backstack. replaceWith — Replace the top of the stack with Screen C. replaceAllWith — Replace the entire backstack with Screen C and Screen D. resetTo — Go to Screen C and remove all Screens after it from the backstack. If the Screen C is not in the backstack, remove all and make it the top of the backstack. goBack — Navigate to previous Screen. When one of these five methods is executed, the Router relays a screen change event to the MainActivity, which is passed to the UIContainer. The UIContainer is responsible for rendering the new (next) view by replacing the active (previous) view, and optionally animating a transition between the two views. Scoop’s namesake is the word “scope”. App scopes are similar to ice cream scoops: going deeper in the navigation stack is similar to adding another ice cream scoop to a cone. Scoops exist to provide access to named services, which are simply Java objects, and have limited scopes. For example, the Profile Screen may need to know about your name, password, and favorite music but the Home Screen doesn’t need to know that information. We can say that the Profile Screen’s scope has been limited to only the information it needs to render that particular view, this information is kept in Scoops. Instead of adding individual services to your Scoops, we strongly recommend integration with Dagger . Replacing the vanilla ScreenScooper with DaggerScreenScooper will set up your application for dependency injection. DaggerScoop builds on top of Scoop, and is readily available for public use within the Scoop repository as an additional open source project. In November of 2014, we first started development of Scoop. In December of 2015, we open sourced and released the framework for other developers to use. Our team is still heavily focused on building improvements and new features for Scoop, and we are always open to new feature requests or contributions. github.com Interested in open source work and having a big impact? Lyft is hiring ! Drop me a note on LinkedIn or at pjohnson@lyft.com . N E X T → Matchmaking in Lyft Line — Part 3 Stories from Lyft Engineering. 136 1 Thanks to Alexey Zakharov , Brennan Sherry , Carlos Whitt , and Ryan Lane . Android App Development Android Mobile 136 claps 136 1 Written by https://twitter.com/itspbj Stories from Lyft Engineering. Written by https://twitter.com/itspbj Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-09-08"},
{"website": "Lyft-Engineering", "title": "matchmaking in lyft line part 3", "author": ["timothybrownsf"], "link": "https://eng.lyft.com/matchmaking-in-lyft-line-part-3-d8f9497c0e51", "abstract": "Data Data Science Engineering Mobile Product Security This is the final post of a three part series around the history of Lyft Line. In the first post we talked about our motivation and some design decisions we considered when building the product, including an alternate version of Line that we decided not to build. We also talked a bit about the initial implementation of our matchmaking algorithm and the main constraints we put in place — pickup and detour time. This system was built fast and we quickly started to see its limitations. In the second post , we talked about how we improved the Line experience by addressing issues such as backtracking and predicting rush hour traffic. We also talked about Triple Matching — the ability to match up to four passengers — and the changes we made, like Longitudinal Sorting, to build an efficient system that could handle the exponential increase in comparisons needed for this feature. Lastly, we talked about Rerouting, an efficiency feature that let us change a driver’s route mid-ride to pick up a passenger along the way. This post will focus on some of the more recent changes we’ve made as we’ve continued to work on efficiency gains. Many of these features have unlocked massive cost savings for our passengers, as ultimately the more efficient our system is, the greater the discount we can provide. There were many efficiency gains along the way, but the next major development in our algorithm was to adapt to a well established product with high demand. Our greedy algorithm was built to find a match, and we made the first match that came along. As Line grew to become the majority of Lyft rides in San Francisco , we could no longer just pick the first match we found as that was leaving significant efficiency on the table. We instead had to find the best possible match. This included considering all riders in our system in addition to predicting future demand. For those interested in Algorithms, this became something of a maximum matching problem for a weighted graph combined with elements of a Secretary Problem . The optimal solution would combine an accurate prediction of future demand with an algorithm that optimized for all possible matches before making any one. Overhauling our architecture to solve the problem in this way was an ambitious change, and given our agile culture, we realized there was an intermediary step we could take along the way — Route Swapping. Route Swapping was a feature that would take a rider from one route, and swap them into another route. The passenger could be swapped before pickup and would be notified via SMS that we had a found a more efficient route for them. This gave us the ability to re-match passengers after they had already been matched if we found a more optimal pairing — essentially increasing our matching window from 60 seconds to a few minutes as we could keep looking for a better route even if we had already chosen a driver. An example of this is shown in the animation below, where passenger A is swapped from Route 2 to Route 1 after our system identified that the combined efficiency was more optimal with this pairing. Route Swapping reduced the burden of having to predict future demand, but also came with its own challenges. For example, if during our initial match we told a passenger that they would be picked up in six minutes, they might take that time to make breakfast or brush their teeth. If we then swapped them to a new route with only a 30 second pickup time, they might be unavailable and miss their ride. Similarly, if we had found an initial match with a low pickup time (e.g. one minute), but then found a better match that had a four minute pickup time (and perhaps a much smaller detour), the passenger might now have to wait on the curb for three more minutes — not the best user experience. In all scenarios, we would only swap the passenger if the new route reduced their pickup or detour time, but we had to be cautious about making drastic changes[1]. Needless to say, Route Swapping came with its own constraints but led to significant efficiency gains, and more importantly, faster rides for our passengers. After we had Route Swapping in place we began to whittle away on a more optimal solution. As mentioned before, this is a weighted maximal matching problem and solving this required many changes to our algorithm. Previously, our matching algorithm would constantly try to compare riders to see if we could put an unmatched rider into a route. When we added Route Swapping we also started comparing matched riders with other existing routes in our system. We would do this in a very tight loop, doing millions of comparisons a second, in hopes that a driver’s positional update might open up a match that wasn’t valid before. When switching to a more optimal solution, we no longer wanted to compare riders as frequently, instead we wanted to “accumulate” rides for a period of time before attempting to solve in a more complete way. Once we’d accumulated a large number of rides, we would look at all possible matches and determine the optimal set of matches to reduce the total detour, pickup time, and cost of the entire system. This is somewhat similar to the bucketed matching design we discussed in part one of the series, except with a very small bucket window (e.g. 30 seconds). This approach, though more efficient, still provides significant challenges on both the algorithmic and technical fronts. Building an optimal solution in a distributed way is challenging — it requires being able to partition your graph into smaller components in real-time without hurting your system’s efficiency. Attempting to do a static partitioning in a city like San Francisco by neighborhood would mean that matching passengers in the Marina and Cow Hollow on their way to work downtown wouldn’t be possible, yet we know those are some of our best matched routes. Identifying the right partitioning pattern and solving for optimal system efficiency is the secret sauce behind our Matching Exchange. Many of the improvements we’ve talked about in this series have been attempts to eke out efficiency in our matchmaking system while maintaining a quality user experience. Optimizing our matchmaking code, however, wasn’t the only way we could achieve a higher level of efficiency and we’ve tried other product changes along the way. Hotspots was a feature we launched that suggested pickup locations to our passengers in exchange for a cheaper ride. These Hotspots were designed to aggregate demand and reduce pickup and detour times. Standby was a feature that allowed passengers to wait up to 10 minutes for a match in exchange for an additional discount. This would increase our matching window and result in a higher match rate, quality, and system efficiency. A combination of these types of changes — algorithmic optimizations and product changes — allowed us to continue to improve our system efficiency without settling on a local maxima . As we have along the way, we A/B test most changes to our system, and build on knowledge we’ve learned from previous tests. In parallel we’re adapting our algorithm to work for cities of different composition — sparse cities like LA, North-South neighborhoods like Manhattan, and centralized downtown commuter cities like Chicago. We’re also refining what makes a good match for our passengers and learning from the feedback they give us — changing the weights of our constraints to meet their requirements. We’ve come a long way since launching Lyft Line in August 2014, but we never consider our work done. We’ll always continue to improve how we help you share rides as we strive for the perfect match. [1] Notice that this Route Swap animation is actually an example of Rerouting plus Route Swapping — passenger A was the next pickup for the driver in Route 2 and by route swapping we changed the driver’s next stop, making it a reroute. A swap like this would require significant cost savings to be worth all of the potential risks. If you’re interested in building the future of ridesharing, Lyft is hiring ! Shoot me a message on Twitter or email me at tim@lyft.com Stories from Lyft Engineering. 199 1 Lyft Algorithms Ridesharing Engineering 199 claps 199 1 Written by Director of Engineering, Lyft Stories from Lyft Engineering. Written by Director of Engineering, Lyft Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-04-20"},
{"website": "Lyft-Engineering", "title": "scoping aws iam roles to docker containers", "author": ["Ryan Lane"], "link": "https://eng.lyft.com/scoping-aws-iam-roles-to-docker-containers-c9c5f8f2f75", "abstract": "Data Data Science Engineering Mobile Product Security Amazon Web Services (AWS) has a really great security feature, called IAM roles, that can be used with EC2 as instance profiles. When you launch an EC2 instance with an instance profile, the IAM role credentials are available to the instance through the metadata service at http://169.254.169.254 . Unlike IAM user credentials, IAM role credentials automatically rotate on a schedule (generally every 15 minutes), so even if the credentials are stolen, they’re only good for a short time period. A downside of IAM roles is that every single process on the system has access to them. If you subscribe to the school of thought that every single instance (or ideally autoscale group) runs exactly one service, this isn’t such a big deal, but if you’re running services in a multi-tenant way, this becomes a problem. The problem is that your IAM role needs to have the sum of all IAM permissions necessary for all of your services that run on an instance. A common solution for people using Kubernetes or Mesos, or other schedulers is to simply give the IAM roles all AWS permissions. Another common solution is to switch back to IAM users. Neither solution is ideal. We do subscribe to the school of thought that each autoscale group should run one service. Of course one service doesn’t mean one process, and not every process on an instance needs the full level of privileges that come with the IAM role. Though we don’t do multi-tenancy for real workloads (staging and production) we do multi-tenancy in development, to better utilize resources. For development (and multi-tenancy) we’re using Docker. We’re also in the process of switching to Docker in staging and production, so we wanted to tackle two problems: We wanted development to look as much like AWS as possible. For this to work, we want to use IAM roles everywhere. We wanted to strictly scope IAM roles in staging and production to containers that require those privileges. We had an idea to build a web service that proxies calls to the metadata service on http://169.254.169.254 and pass through most of the calls to the real metadata service, but capture calls to the IAM endpoints. By capturing the IAM endpoints we can decide which IAM credentials we’ll hand back. Of course, if we had this idea, we were sure others did too, and they did. First we looked at ec2metaproxy , but we also needed to mock most of the routes outside of AWS. The same author has another project which does exactly that, called aws-mock-metadata . These are both great projects, but they’re written in different languages and it was taking a lot of effort to modify them to make them work together properly. The general problem wasn’t very involved and the metadata service has a relatively small number of routes, so we decided to build something from scratch, called metadataproxy . metadataproxy’s primary function is to capture IAM endpoints, figure out which IAM role should be assumed, then return those credentials. It can run in one of two modes. When the MOCK_API setting is set to true, it’ll mock most metadata endpoints. If it’s set to false, it’ll proxy those endpoints through to the metadata service. To know which IAM roles should be assumed, the metadataproxy has access to the docker socket. When it gets a request, it looks up the container, based on its request IP, finds that container’s environment variables and uses the value of the IAM_ROLE environment variable as the role to assume. It then uses STS to assume the role, caches the credentials in memory (for further requests) and returns them back to the caller. If the credentials cached in memory are set to expire, the proxy will re-assume the credentials. The metadataproxy can either be run directly on the host, or inside of a Docker container that’s set to the host-only networking. To capture and forward the traffic to the real metadata service, you can use iptables to forward this traffic from the docker0 interface to wherever your service is running. metadataproxy’s documentation has an example of the needed iptables rule. To contribute to the project, please visit the Github repository and file issues or make pull requests. We’re excited about the release and gladly welcome contribution. Our Github repository has a set of issues being tracked already which are marked easy/medium/hard, to make it easier to pick up any issues that may be relevant to your skillset. The documentation has installation and configuration information that should make it quick and easy to get a version of metadataproxy working in your infrastructure quickly. Interested in open source work and having a big impact? Lyft is hiring ! Drop me a note on Twitter or at ryan.lane@lyft.com . N E X T → Finding a needle in a haystack Stories from Lyft Engineering. 58 2 AWS Docker Security Engineering 58 claps 58 2 Written by Security Engineer and Open Source Team Lead at Lyft Stories from Lyft Engineering. Written by Security Engineer and Open Source Team Lead at Lyft Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-06-02"},
{"website": "Lyft-Engineering", "title": "automated style review with linty fresh", "author": ["Roy Williams"], "link": "https://eng.lyft.com/automated-style-review-with-linty-fresh-17c6cd9d2c3b", "abstract": "Data Data Science Engineering Mobile Product Security Lyft is happy to announce the open sourcing of Linty Fresh , a tool for integrating in Linters/Static Analysis tools into your code review process. At Lyft we have a very fast moving, rapidly growing engineering team. We ship our mobile apps once a week and we use continuous deployment for our backend services. We want to ship high quality code as fast as possible to improve our service for our users. We also want to ensure that new teams members can quickly feel productive and ramped up on our code. Code review has been a fantastic tool for both needs. Code review has been shown to be one of the most effective ways to reduce the number of bugs that get to production, like dereferencing a null pointer. Perhaps more importantly, it’s also an incredibly effective way to spread knowledge around an organization and to ensure multiple people in an organization understand the code that’s being shipped to production. Even though code review is immensely valuable, it’s also not free. Engineers have to take time out of their day from writing features to review their peers code, so we wanted to focus on making this as efficient as possible. In the Microsoft study cited above, the most common kind of comment was “Code Improvements”, generally around code style or formatting. Engineers didn’t find those comments to be highly useful and found them to be nit-picky at times. This can lead to inter-personal conflict between engineers over bike shedding . Coding style is still important! Engineers really care about their style. The tabs vs. spaces holy war has raged as long as there have been text editors. Having a uniform style increases comprehension of code, and engineers spend significantly more time reading code or modifying existing code than writing new code. Clean codebases are less likely to rot due to broken windows. In order to maximize the effectiveness of code review however, we want engineers focused on higher level design and architecture and not being human linters. Leave the linting to the robots and let humans focus on high level design and architecture questions without getting burnt out on style problems. Linty Fresh is a tool that allows us to offload the task of checking things that machines can check to CI, but brings that information back into the context of code review. Linty Fresh is a tool that allows us to offload the task of checking things that machines can check to CI, but brings that information back into the context of code review. Running lint in CI ensures that our code base remains clean. Bringing those comments into the context of a code review reduces the amount of time engineers have to peck through hard to read log files and gives other engineers the assurance that style will be checked by a machine. We built Linty Fresh to be extremely easy to integrate. Lyft uses a combination of in house Jenkins with some third party CI providers for mobile builds and tests. We wanted integration to be as easy as `pipe` without having to set up any backend services. Solutions like SonarQube or Hound perform similar tasks, but require standing up separate services to handle reporting and tracking. To give Linty Fresh a try, check out https://github.com/lyft/linty_fresh and keep your codebase clean with the power of LINT. Linty Fresh uses the Apache 2 License . If you’re interested in building open source developer tools, Lyft is hiring! Shoot me a message on Twitter or email me at rwilliams@lyft.com Stories from Lyft Engineering. 60 2 Thanks to Carlos Whitt . Software Development Code Review Lint Engineering 60 claps 60 2 Written by Stories from Lyft Engineering. Written by Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2015-12-22"},
{"website": "Lyft-Engineering", "title": "using mapper to parse json in swift", "author": ["Keith Smiley"], "link": "https://eng.lyft.com/using-mapper-to-parse-json-in-swift-7788d5c57d74", "abstract": "Data Data Science Engineering Mobile Product Security Today we’re excited to release Mapper , Lyft’s first of many open source libraries for iOS written in Swift. Ever since Swift was released, parsing JSON has been a hot topic . At Lyft, when we completely rewrote our app in Swift , we wanted to find a simple way to solve this problem. Unfortunately, at the time there weren’t any open source libraries that fit our needs. So we ended up writing our own library to do this. The first version of this internal library looked like this: Overall this worked well for us, but we had 3 major issues with this approach. All our model properties had to be mutable. We want to enforce immutability wherever possible, especially for model objects, and we weren’t able to with this library. You could partially fix this by using private(set) on properties, but putting that everywhere made the code pretty ugly. We couldn’t conditionally return a User. Since we were first initializing an empty user, then filling it with JSON, we’d always return a non-optional User, even if the data we received from the server was invalid. Because we were always returning a non-optional User, we were forced to add default values for any properties we wanted to be non-optional. As you can see in the example above we defaulted the User’s id to an empty string. Meaning we were pushing this data problem to the rest of our application if we got bad data from the server. Because of these limitations, we put it on our to-do list to see if we could come up with a new Mapper that allowed us to use immutable properties. Then, when Swift 2.0 was released with the new error handling we came up with a new way to handle JSON. Using the same example from above, it now looks like this: Here we’re able to lean on the new error handling features of Swift 2 in order to fix all of the previous issues. Now all of our properties can be immutable since we’re initializing our model objects with the JSON. We can now fail to create a User if any required keys aren’t in the response, or aren’t the expected type. We no longer need default values for these required keys; instead we fail to initialize the User. We believe this new Mapper is a simple, elegant way to handle parsing JSON in Swift and we hope you do too! Get started with Mapper on Github Interested in open source work and having a big impact? Lyft is hiring ! Hit me up on Twitter or at keith@lyft.com . N E X T → Announcing Confidant, an open source secret management service from Lyft Stories from Lyft Engineering. 67 7 Thanks to Michelle . Swift iOS Mobile 67 claps 67 7 Written by iOS Engineer at Lyft Stories from Lyft Engineering. Written by iOS Engineer at Lyft Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-09-08"},
{"website": "Lyft-Engineering", "title": "communicating with non engineers", "author": ["Julia Grebenstein"], "link": "https://eng.lyft.com/communicating-with-non-engineers-d063db47525e", "abstract": "Data Data Science Engineering Mobile Product Security Communicating with Non-Engineers As a Release Manager, I see many of the conversations that happen between engineering and other departments. I can relate to both sides. When I first joined Lyft, I thought that request and response were verbs you used when you had written the word “said” too many times. Coming from a non-CS background, I‘ve experienced how intimidating it can be to be working with a developer and comprehending only every fifth word they are saying. Since becoming part of the engineering team a year and half ago, I also know how hard it can be to communicate what you’re working on. Let’s start with some examples of a variation of what every engineer has written as some point: My PR hasn’t merged in GitHub, the changes won’t be reflected in master. Questions a non-engineer has: Why are we talking to PR about this already? I think you spelled “get” wrong. Or what about: This is being caused by a 500 response from a Stripe HTTP call. This can lead to many questions: 500 is a big number. Should I be concerned? Stripe? Payments related. Probably Http, like a website? And, a confused takeaway: Something is being caused by a big number on a website that could be related to payments. So how can you make it so the other person isn’t desperately googling sentences from your email? I have three tips to keep in mind that will help you navigate conversations within your company. If your job has no cross-team elements, these tips should come in handy when talking to family, first dates or that one person left in your life that still has their AOL email address. Slow Your Roll. Regardless of your job, this is an important thing to keep in mind. There is a fine balance between dumbing things down and needing to stay accurate and precise, but just choosing different words can help. Trimming down the acronyms, slang, and technical words you’re using is the easiest way to start doing this. At the end of the day, it’s about translating your thoughts into a common language that will make sense. Benoit Hediard put out one of my favorite graphics that clearly explains an engineering topic . It uses visuals to express relationships, and if you look at the graphic again, you’ll notice there isn’t any slang or complex vocabulary used to get the point across. (As a side note, I’m personally excited for when we achieve pizza architecture.) Everyone you encounter in your company is an expert at something; they are just as able to spit out some obscure sounding terminology as you are. Imagine if you were in their shoes and they had to explain a nuanced part of their job. Think About Access. The second tip is to consider the access level that other teams have. Asking a member of the support team to look something up in MongoDB might not get you very far. Also keep in mind that different user support teams have different access to data. For example, risk and fraud teams will have more information at their fingertips than standard support. A great way to learn this is to shadow someone on a different team in your company; you’ll get a chance to see how they use tools and ways to improve on them. If the project you are working on together has multiple components be the bridge to help connect other people. For example, should the designer and the copywriter be talking to each other and not through someone? This is another type of access: social access. Is there an introduction you can offer to make? Is there someone more knowledgeable about the project that they could be talking to? Listen. I asked a friend on our driver operations team which engineers were good communicators and why. She said: “It’s less that they’re good at explaining technical things to me, although they’re good at that, they’re good at listening to me be not technical, and translating that in an approachable way.” I reached out to one of the engineers she highlighted during our conversation to find out what he does. He told me that he always likes to first ask if the other person is familiar with the technology. This is a great tip because you’ll know the level that you need to talk at, and also clues you into what vocabulary you can use during the conversation. This, of course, loops us back to the first tip. A trick I found useful in a past life as a teacher was having the other person repeat back what they have learned from you. This will help you to make sure they understand what was being communicated and also helps you to understand what you can explain better next time. Let’s revisit one of the examples we started with: My PR hasn’t merged in GitHub, the changes won’t be reflected in master. And tweak it just a little to be: The changes that will fix the crash aren’t live yet. Post here if you’re still seeing it in half an hour. So, remember: slow your roll, think about access, and listen to what the other person is saying. Have some other tips for inter-team communication? Let me know on Twitter Este artículo también está en español . N E X T → Building single activity apps using Scoop Stories from Lyft Engineering. 46 Thanks to Carlos Whitt and Ryan Lane . Startup Engineering Communication 46 claps 46 Written by Release Manager for @Lyft, Returned Peace Corps Volunteer, urban planning geek. Stories from Lyft Engineering. Written by Release Manager for @Lyft, Returned Peace Corps Volunteer, urban planning geek. Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-05-12"},
{"website": "Lyft-Engineering", "title": "matchmaking in lyft line", "author": ["timothybrownsf"], "link": "https://eng.lyft.com/matchmaking-in-lyft-line-9c2635fe62c4", "abstract": "Data Data Science Engineering Mobile Product Security Lyft Line is a ridesharing product that automatically pairs riders together with overlapping routes, allowing us to provide a cheaper ride for all passengers. The more overlap two routes have, the larger the discount we can provide. But what makes a good Lyft Line route? We think it’s a cheap, fast, efficient, and friendly ride. And how do we know if we’ve matched a passenger in a route that’s fast and efficient? How can we accurately predict whether two, three, or four passengers together would enjoy the route we’ve selected? That’s the challenge of our matching system, something we’ve been working on for over a year and a half. This is the story of how it’s evolved, and where it’s going, and will be laid out as a 3 part series. We launched Line in August of 2014, though it’s a project that Lyft has long been interested in building given its roots in ridesharing service Zimride. For several years before launch we’ve kept a pulse on the market, but wanted to wait until we had sufficient density before building a true ridesharing product. In May 2014, we ran a simulation on our existing Lyft Classic rides (rides that aren’t shared) and realized that in San Francisco, over 80% of rides shared a significant portion of their route with another ride[1]. This was a staggering number — only a portion of the cars in the city were Lyft rides, and yet potentially the majority of those could be paired together with other passengers. This was a huge opportunity to change transportation and provide more affordable rides to our users. In May 2014, we ran a simulation on our existing Lyft Classic rides and realized that in San Francisco, over 80% of rides shared a significant portion of their route with another ride. In the initial design of Line, passengers would enter in their origin and destination, receive a fare for the ride, and then be put in the matching pool for 1 minute before being assigned a driver. If we didn’t find a good match at the end of that minute, we’d still dispatch a driver to the passenger, and they’d still receive the quoted fare. However, this wasn’t the only design we had considered. We also contemplated building a bucketed matching system. Instead of being in the matching pool for a fixed period of time (1 minute), we’d have all requests stay in the matching pool until the next clock interval. A ten minute bucket interval would mean a ride requested at 8:03 or 8:09 would remain in the matching pool until 8:10, and a ride requested at 8:11 would be in the pool until 8:20. A longer time in the matching pool meant there would be more requests we could match together at one time and thus a higher likelihood of finding a match. Additionally, a variable bucket interval would allow us to adapt to lower density cities at launch, like Honolulu, as we could increase the bucket interval to be 15 or 20 minutes. However, this approach had some downsides. Finding drivers for all of our passengers at the same time would add a supply shock to our system as we’d need to have a pool of drivers available on the ten minute mark. And a ten minute interval meant a longer wait for passengers before showing them their route. Given the initial implementation was going to have some rough edges, a ten minute wait might be too long if they were then put into a bad match. Ultimately we decided that the one minute matching pool could provide the best experience for our passengers and was what users had come to expect from Lyft as an on-demand service. Because of the opportunity we had to revolutionize transportation, and our engineering philosophy of being agile, we started with a simple greedy haversine matchmaking system. A greedy system is one in which we take the first match we find that satisfies our constraints, as opposed to the best one for the entire system. Choosing a greedy system made sense — optimizing for multiple potential matches wouldn’t be possible until we’d grown the product to a sufficient size. Haversine distances are straight-line distances between two points and are multiplied by the region’s average speed to get a time estimate. A haversine implementation is quick to implement and with tight enough constraints (e.g. detours, time until pickup, etc.) would make sure users had a good match. The initial implementation compared every passenger with every other passenger in the system ( O(n^2) ), in all possible orderings. We denoted passengers as letters of the alphabet and every passenger has two stops — a pickup and drop-off — A and A’, B and B’, etc. So when comparing passenger A and B, we looked at 24 potential orderings: ABB’A’, ABA’B’, B’A’BA, B’AA’B, B’ABA’, BAA’B’, AA’B’B, B’BAA’, etc. We were able to reduce the number of permutations down to only 4 given that there would never be a drop-off before a pickup, and an ordering such as AA’BB’ had no overlap and thus wasn’t worth considering. We would look at all four permutations and eliminate ones that didn’t satisfy all of our constraints. We would then choose the most optimal ordering[2], make the match, and notify the passenger. For reference, we will now refer to an ABA’B’ route as ABAB since the second A is inferred to be A’ (drop-off). Our constraints started off with some of the more obvious considerations: detour and pickup time. For a match to be made, the total detour that match added for each passenger would have to be below an absolute threshold, but would also have to be below a proportional threshold. This makes sense as one can imagine a 5 minute detour is much more tolerable on a 30 minute ride than on a 5 minute ride. Detours were simple and efficient to calculate — we would take the estimated time for each passenger to get to their destination without a match and compare that with the total time it would take them to get to their destination when matched. For an ABAB ordering this meant passenger A’s detour was ABA minus AA, and passenger B’s detour was BAB minus BB. We had similar constraints for additional time until passengers were picked up, and would be calculated as follows for an ABAB route (assuming | represents the driver): |A is A’s pickup time, |AB is B’s pickup time. Clearly, being the second passenger in a Line ride reduced that rider’s average detour, as they might not have a another stop between pickup and drop-off (ABBA), but came with an increased pickup time. After launch, we quickly started to realize the limitations of our system. Passengers would post examples of their routes that matched them with a stop on the other side of a mountain or in a way that required an additional loop through crowded downtown one-way streets in rush hour. We started learning that passengers didn’t want to go backwards, and in fact the angle of the lines they saw on the map would affect their satisfaction with the match. In ways both obvious to our initial implementation and in other not so rational ways, our system as it was currently built wasn’t going to give the ride quality that we wanted. We had to make some improvements, and we had to do it fast or people were going to get frustrated with their experience. Coming in part 2 : improvements to our passenger experience and algorithmic efficiency. Read part 2 of the series here . [1] Our initial modeling required rides to share over 50% of their ride and allowed for a maximum detour of 10 minutes when paired. [2] The most optimal ordering is the one in which the total matched distance is minimized. For example if an ABBA route had a total distance of 3.5 miles, but an ABAB route had a total distance of 3.2 miles, we would select the ABAB route. If you’re interested in building the future of ridesharing, Lyft is hiring ! Shoot me a message on Twitter or email me at tim@lyft.com Stories from Lyft Engineering. 330 5 Thanks to Carlos Whitt . Lyft Algorithms Ridesharing Engineering 330 claps 330 5 Written by Director of Engineering, Lyft Stories from Lyft Engineering. Written by Director of Engineering, Lyft Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-08-28"},
{"website": "Lyft-Engineering", "title": "announcing confidant an open source secret management service from lyft", "author": ["Ryan Lane"], "link": "https://eng.lyft.com/announcing-confidant-an-open-source-secret-management-service-from-lyft-1e256fe628a3", "abstract": "Data Data Science Engineering Mobile Product Security I’m pleased to announce the initial open source release of Confidant , a secret management service for AWS users, built by engineers at Lyft. As Lyft has grown, we’ve added numerous services to our infrastructure. These services have credentials to internal services and external services, SSL keys, and other types of secrets. Each of these services has multiple environments, and to insulate these environments from each other, they have a version of each of these secrets for each environment. In many cases some of these secrets may be shared across a few services. Given a large number of services, this leads to a very large number of credentials. The rotation of these secrets can be a laborious process, especially credentials for external services, since a large number of external services don’t have rotation methods that can be done without some amount of downtime and coordination. Coordination of the rotation of secrets became a difficult and time-consuming process for us pretty early on, and we knew the problem would only get worse as we added more internal services and more external dependencies. To reduce time spent rotating secrets, we decided to build a secret management system with a basic design: a database of secrets and mappings of secrets to services. To reduce time spent rotating secrets, we decided to build a secret management system with a basic design: a database of secrets and mappings of secrets to services. We started in late January 2015, and one month later we had an MVP that stored the first secret and mapping to a service. Since then we’ve made Confidant production ready and it’s used across our infrastructure as the means for secret management. We had some specific design goals in mind as we created Confidant: Secrets should be mappable to multiple services and environments. Secrets and services should have history, since they’re essentially configuration data. Secret rotations are critical, but can be eventually consistent. Access to secrets is critical and must be highly available. Secrets and their backups must be stored in a secure manner. Access to secrets must be controllable by service and environment. All forms of access to secrets must be auditable. Authentication from services to the secret management system must be secure and we must handle the problem of securely bootstrapping authentication credentials onto EC2 instances in autoscale groups. We don’t care about being cloud agnostic and assume at least for now that a solution that only works for AWS is acceptable. The AWS-only design goal made the majority of our other design goals much easier, particularly the authentication goal, which is one of the more difficult problems in secret management. The AWS-only design goal made the majority of our other design goals much easier, particularly the authentication goal, which is one of the more difficult problems in secret management. Since Confidant is intended to be run in AWS, we designed it to act and feel like an extension of AWS. We extended the concept of IAM roles to represent services in Confidant so that they can be used to provide access to secrets. We chose IAM roles to represent our services because we were already providing a unique IAM role for every service and every environment for each service, so we could easily use roles to map secrets to a specific environment of a particular service. Securely storing secrets in the cloud is a challenge, due to the turtles-all-the-way-down nature of securing secrets. At some point, there must be a plaintext encryption key that’s used to encrypt the secrets, and the safe-keeping of that encryption key is essential. The same problem exists for authentication credentials from services to the secret management service. A secret that can be used for authentication must originate somewhere, and the plaintext version of that secret needs to be available for EC2 instances at boot time, if autoscaling is to be used. Confidant uses AWS’ KMS service to provide a solution for both of these problems. KMS provides access to master encryption keys , which can be used for encryption and decryption actions, but doesn’t provide direct access to the master key itself, so it can’t be stolen. Confidant uses two master keys: one for encryption, which only it can access, and another for authentication, which can be used by both Confidant and all services that authenticate to Confidant. Access to the authentication key is provided by KMS key grants, which are controlled by Confidant. The way KMS is used for authentication is a bit involved, so I won’t describe it in detail in this post, but the initial concept was implemented as a Lyft hackathon project that was mentioned in a blog post in June . For at-rest encryption, KMS is also being used. For every revision of every secret, Confidant generates a unique data key from KMS. The secrets are encrypted using cryptography.io’s implementation of Fernet , with the encrypted version of the data key stored along with the secret. Through a combination of Flask API access logs, action logs and CloudTrail logs, all actions in Confidant can be tracked to a specific user or service. Additionally, the service was designed to keep full history of secrets and mappings, so that it’s easy to revert to an older revision of a secret or mapping, in case of emergency. Graphite events can be sent for changes to secrets or mappings to make it easy to correlate Confidant events to time-series data in your services. Based on our design goals, our internal client implementation fetches secrets and caches them in-memory, encrypted at rest using KMS. If later calls to Confidant fail for any reason, we log the failure and use the cached credentials until a successful call is made. If fetches fail for a long enough period of time, we send alerts. Confidant provides metadata with the secrets so that it’s possible to know if anything has changed since last call. When we detect changes, we notify processes using the credentials, which causes them to update their secrets in-memory. On the server side, Confidant is primarily implemented in Python, using Flask for the API. AngularJS is used for the frontend. DynamoDB is used for primary storage, and Redis is used for end-user session storage. Google OAuth2 is used for end-user authentication and KMS is used for service authentication and encryption-at-rest. The Flask application is written in a stateless manner and can easily be run on an autoscale group behind a round-robin ELB. On the client-side (services fetching secrets), we’re using SaltStack and have written clients as SaltStack execution modules and external pillars. SaltStack isn’t a requirement, though. Writing a Confidant client is simple, and a basic implementation using just the AWS SDK and a single REST API call is all that’s necessary. We assume that client-side use-cases for Confidant will vary a lot, so we don’t provide an opinionated implementation. We do, however, provide a basic python implementation that will generate an authentication token, make a rest call, and return the secrets data. This is meant as a starting point and can be used directly from the CLI or as a library, for more sophisticated implementations. To contribute to the project, good places to start are the documentation and Github repository . We’re excited about the release and gladly welcome contribution. Our Github repository has a set of issues being tracked already which are marked easy/medium/hard, and backend/frontend/design, to make it easier to pick up any issues that may be relevant to your skillset. The documentation site has installation and configuration information that should make it quick and easy to get a version of Confidant working in your infrastructure quickly. Ostensibly Confidant, Vault , and Keywhiz provide the same function. The main difference between Confidant and the others is that Confidant is purposely not cloud agnostic, choosing to use AWS’s features to deliver a more integrated experience. By leveraging AWS’s KMS service, Confidant is able to ensure the master encryption key can’t be stolen, that authentication credentials for Confidant don’t need to be distributed to clients, and that authentication credentials for Confidant clients don’t need to generated and trusted through quasi-trustable metadata. We wrote Confidant months before either Vault or Keywhiz were released, otherwise we likely would have started with contributing to an upstream project. By the time either were released we had brought Confidant to a point where it wasn’t worth the effort for us to switch to another project, and the general model and simplicity of Confidant made it easier for us to continue maintaining a service than for us to switch. Yes. We have an open Github issue for this . The initial targets will be languages frequently used for systems development or scripting (like Go, bash, Ruby, etc). Of course, it’s not necessary to have your application read directly from Confidant. Instead you can have an out-of-band cron or daemon that calls Confidant and caches the returned value in a ramdisk (potentially encrypted at rest using KMS), to be used by your applications or configuration management systems. Yes, we have a users mailing list , a low-volume announcements mailing list for updates and security releases and a #confidant IRC channel on freenode. Confidant is Apache2 licensed . Get started with Confidant: Documentation and Github Interested in open source work and having a big impact? Lyft is hiring ! Drop me a note on Twitter or at ryan.lane@lyft.com. Stories from Lyft Engineering. 160 3 Thanks to Carlos Whitt and Steve Woodrow . AWS Cloud Computing Security Engineering 160 claps 160 3 Written by Security Engineer and Open Source Team Lead at Lyft Stories from Lyft Engineering. Written by Security Engineer and Open Source Team Lead at Lyft Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2015-11-04"},
{"website": "Lyft-Engineering", "title": "scissors an image cropping library for android", "author": ["Evelio Tarazona"], "link": "https://eng.lyft.com/scissors-an-image-cropping-library-for-android-a56369154a19", "abstract": "Data Data Science Engineering Mobile Product Security A fter we introduced profiles a few months ago, Helen — an Engineer in one of our features team — was tasked to implement the second iteration of profiles for our Android app. This update to profiles included numerous improvements including the ability to set a custom profile picture, either from the camera or picking it from external applications like Gallery or Photos. Before uploading the picture to our servers it had to be cropped following certain requirements, among which were: Pinch to zoom up to 200% Panning and snapping to the viewport Maintaining a certain ratio regardless of screen density Cropping based on current viewport dimensions At Lyft we love open source, so naturally we researched existing solutions. None of them fulfilled the requirements we needed so we decided to build our own. Fast forward a few months and we are now open sourcing the view that represents the core of this feature: Scissors . Scissors provides a view called CropView , which extends ImageView offering familiar ways to provide a Bitmap to crop, for example, using setImageBitmap . Once the user has panned and zoomed around (constrained by cropviewMaxScale and cropviewMinScale ) all you have to do is call The returned Bitmap matches the viewport dimensions, which can be controlled by using cropviewViewportHeightRatio. We also added some handy extensions to help with common tasks like: Loading a Bitmap to fit the viewport using Picasso or Glide into CropView You can also create a custom BitmapLoader to provide a Bitmap just the way you want it. Saving a cropped Bitmap into a File or stream without blocking the main thread You can also provide format and quality of resulting File /stream. We want to make Scissors as sharp as possible so in the future we aim to add double tap drag to zoom support, among other fixes and optimizations . We hope you find Scissors useful and it will become the library for all your image cropping needs. Get started with Scissors on Github Interested in open source, Android and cool features? Lyft is hiring ! Drop me a note on Google+ , Twitter or at evelio@lyft.com. N E X T → Announcing Confidant, an open source secret management service from Lyft Stories from Lyft Engineering. 146 2 Thanks to Carlos Whitt and Michelle . Android Android App Development Mobile 146 claps 146 2 Written by Android @ Lyft Stories from Lyft Engineering. Written by Android @ Lyft Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-09-08"},
{"website": "Lyft-Engineering", "title": "matchmaking in lyft line", "author": ["timothybrownsf"], "link": "https://eng.lyft.com/matchmaking-in-lyft-line-691a1a32a008", "abstract": "Data Data Science Engineering Mobile Product Security In part one of this series , we talked about the timing and design around our launch of Lyft Line. We described the basics of matchmaking and that we built a greedy Haversine system to get up and running quickly. Finally, we mentioned some of the limitations of our algorithm and issues that quickly arose. In this post, we’ll talk about some of the advancements we made that led to improvements to our passenger’s experience, to our system’s cost savings, and to the performance of our back-end servers. We launched Line on Wednesday August 6th, 2014, the week of Outside Lands — a large music festival in San Francisco. We thought this was the perfect weekend to launch a ride-sharing product: there would be copious amounts of cost-sensitive and friendly millennials heading to Golden Gate park to listen to famous artists like Kanye West, Macklemore and Tom Petty. This also meant that passengers would be sharing almost the exact same drop-off or pickup — perfect for our matching algorithm. What we didn’t realize until Day 1 of the festival, though, was that Golden Gate park would be impossible for drivers to traverse as traffic would be gridlocked, and our Haversine algorithm would probably be matching passengers on opposite sides of the park. Given the time constraints (3 hours until the first show), one of our engineers added some incredibly generic code to help reject matches across the park[1]. Obviously this code wasn’t ideal, but we had to move fast. Geohashing Outside Lands illuminated the fact that we needed to get away from using haversine estimates as they were just too inaccurate. We considered building a routing graph and using the A* algorithm , similar to OSRM and something we had done for our pricing estimates, but we knew it wouldn’t scale in our O(n^2) algorithm[2] without an investment in offline computational techniques like contraction hierarchies and a lot of work on building a scalable system. It was too early on in the product’s development to invest in that amount of work and so what we built instead was a geohash based model for estimates. Geohashing is a technique for bucketing latitude and longitude coordinates. Using historical data from past Lyft rides, we could record the average speed of our rides from one geohash to another and store that in a simple hash-table lookup. Then, when calculating estimates, we would multiply the haversine distance between those two points with this speed to figure out a time estimate. This approach proved to be much more accurate and helped us avoid matching through mountains and to differentiate between highway and street travel. It was also very efficient (nested hash table O(1) lookup), but still didn’t do a great job of resolving one-way streets or rush-hour traffic as our estimates were the same for all hours of the day. Fortunately, the geohash model was quick to implement and worked fairly well. We added another nested hash table for each hour of the week between origin and destination which reduced our inaccuracies around rush hour, and were able to continually iterate on this model to improve our estimates. This approach also became more accurate as we collected more data as we could break our model down into smaller geohash sizes. As we started to reduce the error inherent to some of our initial naïve estimation techniques, the feedback we started to receive around bad matches became more focused around experiential issues. Many of these were less than obvious, and ultimately led to a bad passenger experience. For example, we learned that passengers are very sensitive to going backwards, even more than the proportional additional detour it added. Users would often rather have a 10 min detour that had no backtracking then a 5 minute detour with backtracking. We also learned that people would rather have a short pick up time and long detour than vice versa, even if it meant a longer overall route. Many of these learnings are the types of things you can’t predict until building a product, but end up contributing to the polished experience your passengers expect. When we first built Line, we had 2 initial constraints — detour and pickup time. Now we have over 30. At this point, we had spent a lot of time focusing on passenger experience, but not a lot of time on product efficiency improvements that would let us provide larger discounts. So far, we had only considered putting two passengers together in a route. This left a lot of efficiency on the table — more passengers in the car at the same time meant more cost savings we could give back to our users. The obvious next step for us was to introduce Triple Matching — the ability to add a third, and fourth, passenger to the car. Triple Matching Triple Matching added significant efficiency to our system by increasing our average match rate[3], but also created a scaling nightmare. With two passengers in the car we had four permutations to consider, but with up to four passengers that meant we could now have routes such as ABCBCA or even ABACBDCD [4], adding up to a total of 1,776 permutations. This meant we had to quickly scale the efficiency of our system to handle this load, and also brought with it a new set of requirements for maintaining a good passenger experience. One route that quickly became evident as a bad experience was ABCDBCDA. That’s six stops between pickup and drop-off for passenger A. Even on a route with no detours, that would be a long ride given the extra time it takes for pickup and drop-offs. Additionally, though we asked users how many riders they had in their party, if any rider reported an incorrect number, drivers would have to awkwardly decide how to handle the fifth passenger. Lastly, although riding in a full car with four others is ultimately the most efficient route we can make, it doesn’t always create the best experience[5]. To adjust for these issues, more constraints were added to the system, and we focused on how to handle the level of scale that was required to support all permutations. Horizontally scaling a system with no clear partitioning pattern is difficult — tree structures (BSPs, quadtrees, etc) don’t work well when two pickups could be in different partitions yet still be a great match (e.g. riders could have origins 15 minutes apart), and we were building towards a future allowing continual matching at any point along the route. Solving this was difficult and is beyond the scope of this post, but one optimization we made was Longitudinal Sorting. Longitudinal Sorting One of our constraints for making a good match was the time it took for a passenger to be picked up. If it took 15 minutes to get picked up, it didn’t matter how fast the rest of the route was — passengers wouldn’t consider that an acceptable route. This meant that when considering pairing A and B together, there was some maximum distance they could be apart from each other. What we discovered, was that we could leverage this in our outer loop: if we sorted the outer and inner loops by longitude, we could short circuit out of that loop when we’ve passed this maximum distance. If we sorted all of the passengers in San Francisco from west to east, we didn’t need to compare someone in the Outer Sunset with someone in the Marina. This didn’t change the complexity of our system, it was still O(n^2), but it did give us a huge boost in efficiency. This is one example of a constraint built for passenger experience that actually allows us to reduce the problem space and help scale our system. Rerouting With improvements to our system performance, like the longitudinal sort, we were next able to tackle another milestone in Lyft Line efficiency gains: rerouting. Up until this point we had only been willing to add a passenger to a route if it didn’t change the driver’s next stop. That meant that in a |AA route (where the vertical bar is the driver), we couldn’t turn this into a |BABA route as we would have to change the driver’s first pick up from passenger A to passenger B. Similarly, in an AB|AB route, where both passengers had already been picked up, we couldn’t add passenger C to make it AB|CABC, though we could do AB|ACBC. There were multiple reasons to disallow rerouting, both on the technical and user-experience fronts. Imagine being 1 block away from your drop-off and the driver is told to turn right and pick up another passenger. Or even worse, the driver picked up the first passenger and merged onto a freeway onramp only to be told to get off the freeway, go backwards, and pick up a second passenger. It was a risky change, but for efficiency junkies like us, the gains were too tempting to ignore. Like most things at Lyft, we A/B tested it and made sure to add safe guards to prevent a bad user experience. Routes wouldn’t be rerouted if the driver was about to drop off a passenger, and we added some clever tricks to identify when a driver was about to get onto an onramp. All in all rerouting added quite a bit of efficiency gains and we were able to ship it with a minimal hit to the passenger experience. At this point, we had implemented some of the more obvious efficiency features (many of which we haven’t talked about) and were reaching the limitations of a greedy system. Triple Matching was giving us huge wins, as was Rerouting, but we weren’t able to take full advantage of the volume of rides we were starting to see. As Line ridership passed that of Lyft Classic in San Francisco, we needed to not just choose the first acceptable match for a rider, but to optimize the entire system. Coming next time : more efficiency improvements on the road to becoming less Greedy. Read the third and final part of the series here . [1] Internally referred to as the “Golden Gate Mason Dixon Line” [2] Our O(n^2) algorithm is described in Part 1 . [3] The Match Rate is calculated by dividing the number of passenger ride requests by the number of routes (driver rides). A match rate of 2.0 means that, on average, every rider is paired with another rider. [4] For a refresher on what these letters mean, read the first post of the series. [5] Lies! Who doesn’t love a full car? If you’re interested in building the future of ridesharing, Lyft is hiring ! Shoot me a message on Twitter or email me at tim@lyft.com Stories from Lyft Engineering. 256 7 Thanks to Carlos Whitt . Lyft Algorithms Ridesharing Engineering 256 claps 256 7 Written by Director of Engineering, Lyft Stories from Lyft Engineering. Written by Director of Engineering, Lyft Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-08-28"},
{"website": "Lyft-Engineering", "title": "finding a needle in a haystack", "author": ["Ryan Lane"], "link": "https://eng.lyft.com/finding-a-needle-in-a-haystack-b7e0627b01f6", "abstract": "Data Data Science Engineering Mobile Product Security Secrets don’t belong in source code. At Lyft we use a secret management system ( Confidant ) to ensure our repositories are free of secrets, so we also want to ensure no one accidentally adds secrets into repositories. Manually auditing for this is laborious, so we wanted to add automated tests that fail if secrets are introduced in pull requests. OpenStack recently released Bandit, a static analyzer that traverses abstract syntax trees ( ASTs ) of Python code. Bandit looked like a solid base for building an automated Python analysis tool, and its architecture supports plugins, so we created a Bandit plugin for identifying secrets in source code. Our plugin, bandit-high-entropy-string , captures strings from the AST and attempts to detect if the strings are secrets. The problem with most secrets is that they’re randomly generated (have high entropy), which means unlike other strings, for instance dangerous SQL statements, secrets are difficult to find in an automated way. The high entropy itself is something that we can use as a marker, though. As an example, let’s look at the following code: In the above, when the AST is parsed, the plugin captures assignments, function definitions, function calls, comparisons, dicts, lists, etc. When it captures these, it takes a look at the surrounding context to bump the confidence level up or down. If a string is being assigned to a variable that looks like it would store a secret, it has higher confidence. If it’s being assigned to a variable that looks safe or is inside of a function call that looks safe, it has lower confidence. We also lower the confidence on strings that look like common patterns in Python. For instance, strings referencing files on a filesystem, or URLs, or Flask routes. We first assess the confidence of a string based on its context and then we use Dropbox’s zxcvbn library to determine the entropy of each string. If the string’s total entropy is high we increase the confidence, if the entropy per character (total entropy / string length) is high we increase the confidence some more. Right now the plugin has relatively low noise and good signal when Bandit is run to filter all but high confidence issues (-iii). It’s not perfect by any means, though, so please give it a try, and open issues and send in pull requests for improvements! Thanks to the OpenStack security team for giving us help and feedback on the plugin and, of course, for writing Bandit. Thanks also to the Dropbox product security team for feedback on docs and signal/noise issues. Interested in open source work and having a big impact? Lyft is hiring ! Drop me a note on Twitter or at ryan.lane@lyft.com . N E X T → Announcing Confidant, an open source secret management service from Lyft Stories from Lyft Engineering. 50 Thanks to Steve Woodrow . Engineering Security Open Source 50 claps 50 Written by Security Engineer and Open Source Team Lead at Lyft Stories from Lyft Engineering. Written by Security Engineer and Open Source Team Lead at Lyft Stories from Lyft Engineering. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-02-09"}
]