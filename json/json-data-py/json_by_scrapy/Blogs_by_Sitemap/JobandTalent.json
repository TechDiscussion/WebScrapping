[
{"website": "JobandTalent", "title": "api authentication strategies in a microservices architecture", "author": ["Gonzalo Gómez"], "link": "https://jobandtalent.engineering/api-authentication-strategies-in-a-microservices-architecture-dc84cc61c5cc", "abstract": "Data Science Backend Product Mobile Platform Frontend We are hiring! A key point to consider when designing distributed systems is the way our services communicate with each other. To ensure robustness and data consistency between services, both synchronous and asynchronous communications are usually employed. When talking about synchronous communications, a common solution is to use HTTP endpoints to grant access to specific business resources. But, as you may know, there is no easy way to manage the access to these endpoints. Suppose that our platform has two services, A and B, and each one of them manages the resources of a specific business domain. We have to deal with concepts such as authorization (which resources can be accessed and by whom), user authentication and authentication between services . Since our platform here at Jobandtalent has multiple constantly evolving services , today we would like to review some strategies to improve API design and authentication between services by trying to answer questions like: Should service A be able to access service B without authentication? How do we translate domain-related permissions to APIs authentication? Who will be in charge of aggregating data from different services to be displayed on the client side? The first consideration relates to an architectural decision: having the authentication managed by each service or having it centralized. On one hand, we can add an authentication layer to each service , acting as a proxy that accepts several authentication types. With this solution, we are splitting the authentication responsibility across different services. This option might be valid if you only have a few services, or if the authentication differs greatly between services. But we will probably end up duplicating logic and adding complexity to each one of our services. Choosing this option depends basically on how our system is structured and the number of services it has. On the other hand, we can syndicate the authentication between the different services APIs, using an Authentication Provider , which can be internal or from a third party. When we follow this pattern, each service has permissions on a specific set of endpoints, provided by the services that own the logic for each domain. This means every domain-related service needs to be explicit about what services it allows access to. Let’s see with an example: suppose we have service A, which has a frontend interface (presentation layer) that retrieves and presents data from different backend services. Service A sends authentication data to the Authentication Provider, requesting authentication to access service B. The Authentication Provider validates the identify of service A. If the authentication is successful, the provider generates a token to identify the service against any other entity. Service A receives the token and stores it to validate the following requests. Service A calls an endpoint of a secured resource. Service B checks the validity of the token with the Authentication Provider. If the authentication is successful, the requested operation on that resource is allowed. A common pitfall of this approach is the fact there is a single point of failure . If the Authentication Provider is down, this will stop our entire architecture from working. However, this can be avoided by having multiple instances of the Authentication Provider distributed across different availability zones, which will mitigate the risk. Fault isolation zones and redundant components are key concepts in a high-availability application architecture. jobandtalent.engineering Now that we have reviewed the architecture approaches, and supposing that we choose the centralized authentication, let’s see different authentication types for communication between services. API keys to authenticate requests between services. Each HTTP request from one service to another will include the API key, that will be validated by the accessed service. We may end up having an internal service (like the Authentication Provider explained above), that handles all API keys, which can be periodically rotated. Depending on our needs, we could add more complexity to this centralized API key management service. One of the main issues with the API keys is that they usually give us full access to all the CRUD (create, read, update and delete) operations that the API can perform. And this is not the best approach in terms of security. Furthermore, we need to query the database to validate if a key is valid or not. Besides, there is not a standard that regulates API keys format. JSON Web Tokens (JWTs): a JWT, standardized in RFC 7519, defines both a token content and an encryption method. The origin service signs the JWT with a secret. The accessed service can verify the token with the same secret as the origin service. One advantage of this method is that it does not require database querying. And we can also include authorization info in the JWT (what resources are being accessed, etc). But these tokens cannot be easily revoked. Therefore, their expiration time is usually short, and revocation is handled via a refresh token. OAuth standard-based solution : this option also allows you to handle user authentication and authorization (apart from authentication between services). → A third party authentication + authorization service like Auth0. → A directory service. → A social network. ex: Google, Facebook, etc. → An identification service: Github, Microsoft, etc. → A custom implementation of the OAuth standard. Another step further would be defining infrastructure-level rules that fortify access between services. This can be achieved at infrastructure or API gateway levels. Apart from application-level security, we would add security at infrastructure-level (for instance, an AWS Security Group ). If attackers access service A, they would not have access to perform more than a couple of calls to service B, not having access to any other service at the network level except service B. This approach consists in isolating layers at the virtual network level, so that each service only accesses what it needs. There might be some issues due to the fact that the same cluster is used for different services. For instance, if our hosting relies on AWS, there are ways (such us using awsvpc network mode) to manage security and authentication between services running in the same cluster. This mode in ECS-EC2 has a limit of ENIs imposed, that was increased r ecently. There is a potential low-limit, mostly depending on the number of containers expected to run per server. That does not happen in ECS-Fargate though. We can also use task/instance profiles to limit the usage of other AWS services without using explicit credentials. Or we could even use IAM to generate permissions. Additionally, we could use an API gateway that would talk to both presentation layer and backend services, being the single entry point for every request and redirecting to the appropriate service or aggregating information from different services and consolidating it into a format that the client understands. In Jobandtalent we have implemented this pattern with a Backend for Frontend (BFF) service. The API gateway can also handle the authentication of the incoming requests. Using an API gateway is especially useful if we want to set timeout policies for our clients or a common client-understandable format for our services’ error responses. Furthermore, this pattern is very useful to enable us to reduce the risk of Denial of Service attacks (DDoS). Since the API gateway is the entryway to every request, our backend services will not be exposed to these kinds of attacks. In any case, both solutions (infrastructure security and API gateway) are combinable. jobandtalent.engineering There is no golden rule to designing a good authentication system for a Service Oriented Architecture. There are several circumstances that need to be taken into account: number of requests, centralized authentication vs service-specific, an internal tool vs a third party authentication service… Reasons to choose a strategy can be varied: from development productivity (having the authentication externalized simplifies things for the development team) to non-technical security reasons (our customers or stakeholders demand having all authentication systems within our network). Hence, identifying the requirements that suit our business best is key before adopting one of these approaches. It is better to invest the time at the design phase than having to go back once the service is deployed and being used. This is even more important in an area as sensitive as authentication. Acknowledgments: thanks to Sergio Espeja , Juanjo Martín , Luis Recuenco and John McLachlan for their feedback while writing this article. If you want to know more about how is work at Jobandtalent you can read the first impressions of some of our teammates on this blog post or visit our Twitter . Jobandtalent Engineering team blog. 8 Thanks to Luis Recuenco , John McLachlan , and Sergio Espeja . Backend API Authentication Software Development Api Gateway 8 claps 8 Written by Software Engineer https://www.linkedin.com/in/gonzalogomezheredia/ Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Written by Software Engineer https://www.linkedin.com/in/gonzalogomezheredia/ Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-02-24"},
{"website": "JobandTalent", "title": "solve the planning problem with integer programming 2 2", "author": ["Michele Trevisiol"], "link": "https://jobandtalent.engineering/solve-the-planning-problem-with-integer-programming-2-2-b629fa4402fb", "abstract": "Data Science Backend Product Mobile Platform Frontend We are hiring! T his is a technical introduction to solving the Planning problem of a Workforce Scheduler . For more details about what the Scheduler is and why Jobandtalent have built such a product, check out the previous article: jobandtalent.engineering What do we know? We have a given demand of workforce over a certain period of time, a set of constraints that we need to respect in order to have a valid solution, and a set of preferences and costs that we want to optimize. (a) Workforce demand : you can think of the demand as a list that states how many workers are needed in each hour. (b) Constraints : the conditions that need to be satisfied to have a valid solution (shift size, shift type: continuous hours or with breaks, etc. ). (c) Preference and costs : the “preferences” of the workers or the clients ( e.g. , ideal number of hours per shift), and the cost of oversupply and undersupply, number of active workers, etc. We will formulate both of these concepts into our Cost Function. As we mentioned in the previous article, we are dealing with a Discrete Optimization problem because we need to search for the optimal solution among a finite set of candidate solutions (which could be quite large). With a bit of mathematics, we can describe the problem using Linear Integer Programming and run the search with one of the Open Source Solvers. What is an “Integer Programming” Problem? It’s a specific kind of Linear Programming (LP) or Linear Optimization . In short, it is a method that tries to find the best solutions to a mathematical model whose constraints are represented by linear relationships . In our scenario, all of the variables are restricted to be integers , and this is a special case which is called (Linear) Integer Programming (LIP or IP) . Once we design the IP problem we can run a Solver to search for the solution. What is a “Solver”? It’s a very efficient piece of software that is able to interpret mathematical optimization models, navigating their search space applying a variety of mathematical tricks to find the solution in the minimum possible time. The best available Solvers number only a few , you can count them on two hands. The most critical part is how to design the model, and that is where your creativity and ability play a key role. You can define the variables you want, their interactions, how they can be mapped with the constraints, how they are combined to build the cost function, and so on. Mathematical formulation is just a way to define the problems with variables and operations . You need to define what the basic elements of the model you are building are, such as the hours, the demand, the shifts, and you can think of them as the players of the mathematical model . Once we know what these players are, we need to translate the relationship between them into mathematical formulas using classic operations, mapping them with the problem constraints, then creating the model. Let’s think about a simple Knapsack Problem . We want to pack our small 20L backpack with different books (which have different volumes) that we can sell at different prices . The idea is to select them to fit inside the backpack capacity while maximizing their values (better to sells 5 30€ books, than 10 5€ ones). The players of our story are simple: We have n books , that we can define as n variables {x1, x2, …, xn} which can be equal to 1 when the book is selected, or 0 when it is left at home. Each book has a volume (for simplicity we assume the shape is not relevant) and thus we have n other variables : {v1, v2, …, vn} And finally, we have the n prices : {p1, p2, …, pn} We want to maximize the total possible profit (1) while respecting the maximum backpack capacity (2). And this would be our model: If we select only books 1, 3, and 7, only the variables x1, x3, x7 will be non zero, and as a consequence, (1) will sum up only their prices (p1+p3+p7) and only their volumes will be considered in the inequality (2): v1+v3+v7 ≤ 10L and thus, just by playing with the x variables , we can mathematically compute and verify the cost function while checking that the constraint is respected. That’s it. We converted our real problem into a mathematical formulation that can be interpreted by a Solver. We are finally ready to talk about Jobandtalent’s case study: our Planning Algorithm. Let’s consider a simple demand of 24 hours like the one below, from which we need to create the working shifts to associate with different workers. If you have to manually create these shifts, one very intuitive and visual approach is to extend this list with a matrix in which each row represents a shift (or worker) and each column contains the working status of that hour, when equal to 1 the shift is active, otherwise is not active. Then, we can play with the 1s and 0s, limiting the possible length of the shifts we generate, and ensuring that the 1s are all contiguous. The image below should help you visualize it with a possible solution. What we just did was think about a possible model or logic that could be used to find the solution. Now let’s see if we can formulate it mathematically. First, let’s define the variables and constraints for our problem. We should start by defining the basic ones: And the matrix: After defining the first variables, we can already formulate some constraints. In (1) we are stating that the solution we are looking for must contain shifts that cover exactly the given demand (1a). This is a very strict constraint because when a perfect solution doesn’t exist, it means that our Solver won’t return anything. If we want to be more flexible we could replace the equality operator with a “≥” or “≤” to allow oversupply (1b) or undersupply (1c). In (4) and (5) we are stating that each shift must have a length that falls in the range of Smin and Smax (by summing up all the 1s each row contains). The variable f is used to force the inactive shifts to have 0s in all the rows of the matrix. The Solver, when looking for the solution, tries different permutations of fi always trying to optimize the cost function. For example, when it is checking f1=0 it is forcing all the integer column variables of the first shift to be 0 as well, in other words, it is disabling the first shift. Contiguous Shifts Constraint We have our matrix which tracks the activity hours of each possible shift. That’s great, but there is a big issue we have to solve now. How can we (mathematically) state that all the 1s of a row should be always contiguous? Basically, the problem we have defined so far is not discarding the “uncontiguous shifts”, and we need to think about a constraint that will invalidate them making the Solver search for other solutions. One possible solution is to identify “when the shift starts”, which happens when we have a 0 followed by a 1 (keep in mind the particular case of the first hour). To keep track of this we need almost a variable for each cell, basically, we define another matrix of variables. For the previous example, that specific row of the matrix C will look like this: What we can do is count the number of times the “shift starts” in each row, basically when, taking two cells, the latter is bigger than the former, as shown in (7). Finally, in (8) we limit the sum to be lower or equal at 1, which means the shift is empty (no starting hours) or that it starts just once. Finally, our objective function has to minimize the cost of adding extra shifts. For simplicity, we can just minimize the number of shifts, so the sum of the fi . If we have to find a solution that matches exactly the demand, basically using (1), this objective function is enough. Instead, if we used (1a) or (1b), we need to extend the objective function with additional costs: the number of oversupplied and/or undersupplied hours. In the final objective function (11) we can easily balance the importance of minimizing the shifts (9) and the over/undersupply (10). Note that to calculate the over and undersupply in (10), we cannot use the absolute and max function since they are not linear and thus not compatible with Linear Programming. Instead, we need to use a little trick. We have to create two non-negative variables, (12a) and (12b), and equalize their difference with the difference of hours allocated and demand for each column. For example, assume column 1 had a demand of 8 workers but our solution allocated 10, with this formula, because we are minimizing oversupply and undersupply, the Solver will automatically assign 0 to undersupply and 2 to oversupply, validating the equality. And this is it. We have built our objective function, the last piece of our model. Now we can run the Solver to search for the optimal solution to our problem. In this article, we have discussed a simple Integer Programming model that is able to solve a wide set of generic Planning problems. The model can become substantially more complex if extended with more complex preferences or requirements, such as: Oversupply/Undersupply weights . Having a worker that does more hours than the required has a certain cost. Not being able to cover the initial demand because we undersupply, often has a different cost. Dynamic weighting . A solution with 1 extra hour over 5 different time slots might be better than a solution with 5 extra hours in one single slot. One way to account for this difference is to have dynamic weightings that give a higher penalization to the “higher error” ( i.e. , check out the stepwise function since non-linear functions are always forbidden). Accepting shifts with breaks . What about if we can accept solutions with shifts that contain 1 or 2 hours of breaks? Well, we should find a way to add it to the C matrix, and if that’s not possible we have to think of a completely different model. Adding more preferences means adding more variables and constraints to the model while ensuring they can work together. Moreover, note that all these extensions will make the search problem substantially more complex for the Solver. If you want to know more about how it works at Jobandtalent, you can read the first impressions of some of our teammates on this blog post or visit our Twitter . Acknowledgments: thanks to Antoine Hachez and Cecil Fernandez for feedback and reviews. Thanks to the Data Science and Operations teams for making this project a success. Jobandtalent Engineering team blog. 17 Thanks to Antoine Hachez . Integer Programming Algorithms Optimization Linear Programming Data Science 17 claps 17 Written by VP of Data @Jobandtalent • AI • IR • Optimization #ORMS • RecSys • Previously @Stuart @Yahoo @Twitter @INRIA Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Written by VP of Data @Jobandtalent • AI • IR • Optimization #ORMS • RecSys • Previously @Stuart @Yahoo @Twitter @INRIA Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2021-02-07"},
{"website": "JobandTalent", "title": "save money and increase workers happiness by optimising your workforce schedule 1 2", "author": ["Michele Trevisiol"], "link": "https://jobandtalent.engineering/save-money-and-increase-workers-happiness-by-optimising-your-workforce-schedule-1-2-b272803718f9", "abstract": "Data Science Backend Product Mobile Platform Frontend We are hiring! B eing able to solve complex optimization problems is critical for many enterprises and startups, but don’t think it is useful for only a few selected markets. Understanding these problems and their solutions highlight new applications in very different contexts , maybe even close to yours. If you understand these problems and their solutions you may also find great applications for your company. Think about the problem of packing a suitcase , meeting the weight limit for the airline , but including all of your necessary belongings: clothes, shoes, personal hygiene products, electronic devices, and so on. From all these categories you need to pack enough items for the entire trip, without leaving the essentials ones , but being prepared for the majority of plans (work, relaxing evenings, fancy dinners for work events, sports activities, etc. ). We can summarize the problem defining (1) the constraints : You have a finite amount of items in each category. The sum weight of your clothes should be lower than the maximum weight allowed by the airline. All the selected clothes have to fit within the suitcase capacity. Certain clothes have to be selected together ( e.g. , running shoes with running clothes and maybe a phone armband). and the (2) objective function : Minimize the weight while maximizing the number of different activities you can do with the selected clothes. Ideally, we could try all the valid solutions and select the best one given these criteria (hello Brute Force!). This is possible with a dummy example, but when the number of solutions is very high, this strategy is unrealistic, since the search space (the “list” of all possible solutions) will grow exponentially. Here is where Combinatorial Optimization comes to the rescue, often referred to as Discrete Optimization since we can mathematically formulate this problem using discrete variables. Combinatorial optimization means searching for an optimal solution in a finite set of potential solutions. Optimality is defined with respect to some criterion function, which is to be minimized (cost) or maximized (objective). If you want to know more about these topics, you could start exploring the beautiful world of Operations Research . Traveling Salesman Problem : find the best route a dispatcher should follow to visit a set of locations and go back to the origin, at a minimum cost ( e.g. , time, distance). Probably the most famous NP-hard problem in combinatorial optimization . This is the basis for more complex problems like the (Capacitated) Vehicle Routing, daily bread for logistics companies such as Amazon, car-sharing, last-mile deliveries, etc. Portfolio Optimization : how you should balance your financial portfolio (of any asset) in order to minimize the risk of the overall portfolio while maintaining a level of monetary return. Assignment Problem : A group of workers has to perform a set of tasks. For each worker and task, there is a fixed cost for that worker to perform the task. Find the best assignation that minimizes the total cost. Crew scheduling : Assign crews to different airline flight segments to minimize total cost while ensuring that a crew “rotation” begins and ends in the same city. Similar to shifts scheduling of hospital personnel. There are plenty of use-cases in our everyday life in which Combinatorial Optimization plays an important role. That’s why it is a very interesting topic and an extremely useful one. More examples here and here . In Jobandtalent , we had worked quite a lot with the routing optimization problem , but in this article, we want to focus on one of our specialties: optimizing workers' and clients’ schedule in their day-to-day lives. The core business of many companies nowadays relies on a dynamic work demand that needs to be fulfilled by the best workers available at each moment (while trying to minimize the costs). For some clients, scheduling is directly related to costs. Think about delivery and logistics companies, supermarkets, warehouses, or any business that relies on a fleet or a crew of workers. One of the main problems they have in common is how to optimize the working time of their staff because it’s one of the highest costs they have to deal with, and it’s critical to increase their (often tiny) margins. For instance, imagine that your business needs 10 workers next Monday all-day, but you forecast a work peak in the morning (8:00–10:00) in which you should have 15 workers to maintain a high-quality service. Unfortunately, you cannot have 5 workers performing just 2h shift, because the minimum of their contract is 4h. Therefore, you have to decide whether to oversupply , assigning 15 workers and incur extra costs for the extra hours, or undersupply , putting the quality of your service at risk and, in some cases, even incurring penalizations. This dummy example shows that you definitely want to optimize the total number of shifts as well as their size (number of hours) to optimally meet the demand. Moreover, an additional and more straightforward cost is given by the hours invested in manually building these shifts. Some of our clients had at least 1 person per site that was working on the scheduler spending an average of three full days per week (just for one site!!). For some workers, scheduling is often related to better working conditions and, thus, happiness. Another huge benefit of this problem, which is rarely taken into account, is that we can include the workers’ preference in our cost function such as working in the morning, weekdays, or being assigned to a particular part of the city or workplace. Having workers that are listened to, and have better working conditions, it is one of our priorities. Our algorithms will try to find a solution that satisfies both the client and the workers. As you might guess, in order to solve these scheduling problems we need to design some Discrete Optimization algorithms . The solution that Jobandtalent offers has been developed internally from scratch and integrated with the rest of Jobandalent’s product suits. It serves some of our clients that benefit the most from optimizing their workforce scheduling. It has a very cool interface ( Scheduler UI ) in which is possible to upload and change the demand, manually or automatically create the shifts and allocate them to the workers, and finally, also interact with them via the Jobandtalent’s mobile app. The automatization is taken care by a set of optimization algorithms that are working on two separated steps: Planner : given the demand, they generate the shifts respecting its constraints and optimizing its objective function. Allocator : given the shifts and the clients’ workers, they allocate them respecting their constraints and optimizing their objective function. We can upload a demand of a few days, few weeks, or even entire months. Below you can see an example of a weekly demand, one for each different workplace ( e.g. , different warehouses or supermarkets) of a demo client. In order to generate the “optimal shifts” we need to understand when a shift is a valid shift , and what makes one solution better than another one . From our web interface, we can specify a set of requirements that can be constraints , i.e. mandatory characteristics that define a valid solution, or preferences , i.e. “nice-to-have” characteristics. For example: Shifts should last a maximum of 8 hours for standard contracts ( e.g. , but night shifts in hospitals might even last 12h, and we should also support lunch-breaks). Shifts should last at least 4 hours since your workers might not be happy to work less (in some cases they won’t accept a shift if it’s shorter). Minimize the hours of oversupply (how much does it cost to have a worker doing nothing?) and undersupply (how much does it cost not having enough workers to finish a task or cover the service?). Minimize the total number of shifts generated (the highest, the more workers are required and the more expensive will be to cover the demand). Once we have all this information, we can map them into our Combinatorial Optimization model (that we have previously designed and mathematically formulated), and we will get the best solution that our algorithm is able to find, again within our Scheduler UI. It’s important to understand that the more constraints and preferences we have to consider when searching for the solution, the more complex it will be to design an algorithm that works well and fast. It’s not a big deal with these dummy examples, but when you have 20, 50, 100, or higher peaks of demand over time, then the problem becomes exponentially more complex. Not surprisingly this type of problem is classified as NP-hard . We have seen how the planning phase works. Now that we created the shifts, we have to allocate our workers, and this is the second part: the allocation . We have a bunch of shifts previously generated and a list of workers for the current client that has to be assigned to these shifts in order to properly fulfill the demand. Even for the allocator’s algorithms, we have different constraints and preferences that we have to use to design our model and find the solution that minimizes our cost function . Some examples below. Minimum and maximum working hours ( e.g. , workers cannot work less/more than 40 hours per week). These constraints should also be formulated with the monthly limits. Compatibility between workers’ skills and workplace (in some cases only workers with special training can perform certain tasks). Balance the number of weekends and night shifts assigned across the workers (avoid the risk of always assigning the weekend to the same workers). Keep in mind the preferred working hours of the workers to keep them happy and engaged. As usual, the more flexible we are, the better the solution for our workers and clients will be , but also the more complex the search for optimality will be. As with the Planner, we define all of this information from the Scheduler UI and with the power of a click, we just wait for the best solution to appear. The interface would look like this: From the Scheduler UI, we can manage all the shifts and workers, modify any shift and allocation, as well as interact with the workers via push notifications to the Jobandtalent’s mobile app , email or SMS. With the Scheduler UI it is very easy to manage a fleet and quickly control the dynamic demand even in real-time for day-to-day operations. As we have discussed, in order to solve the whole scheduling problem, we had split it into two different sub-problems: Planner and Allocator. We have formulated both of them as an Integer Programming (IP) problem , which is a particular approach to solving Discrete Optimization problems. In the next article (below), we are going to explore in more detail how we can build a model that solves the Planner . jobandtalent.engineering It’s one of the first models we built, and it cannot handle all of the clients’ requests, but it’s a good starting point since it’s able to solve complex scenarios while remaining fairly easy to understand. If you want to know more about how it works at Jobandtalent, you can read the first impressions of some of our teammates on this blog post or visit our Twitter . Acknowledgments: thanks to Ana Freire , Antoine Hachez , and Daniel Lovazzano for feedback and reviews. Thanks to the Data Science and Operations teams for making this project a success. Jobandtalent Engineering team blog. 47 Thanks to Antoine Hachez . Operation Research Optimization Algorithms Scheduler Discrete Mathematics Data Science 47 claps 47 Written by VP of Data @Jobandtalent • AI • IR • Optimization #ORMS • RecSys • Previously @Stuart @Yahoo @Twitter @INRIA Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Written by VP of Data @Jobandtalent • AI • IR • Optimization #ORMS • RecSys • Previously @Stuart @Yahoo @Twitter @INRIA Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2021-02-07"},
{"website": "JobandTalent", "title": "our first impressions working at jobandtalent part 2", "author": ["jobandtalent Engineering"], "link": "https://jobandtalent.engineering/our-first-impressions-working-at-jobandtalent-part-2-a02b93af40dd", "abstract": "Data Science Backend Product Mobile Platform Frontend We are hiring! This is the second post where our colleagues at Jobandtalent talk about their experiences in their first months here. These are the impressions of Manuel, Nieves, and Javier. Manu is working with us as a Backend Developer I’ve worked in several companies prior to Jobandtalent, but to me, there is a huge gap between the former ones and Jobandtalent. I’ve started working here as a backend engineer, being part of several and transversal teams that build an awesome group of talented people. We build a great product, helping millions of people to find a nice job. The first thing I noticed here and impressed the most was the IT platform that we manage. Every deployment and development flow is perfectly automated, relying on some of the sysadmin tasks on our loved bots , besides the use of orchestrators and other great automation and management tools. I like it a lot because I am a fanboy of Unix systems. I’ve learned a bit of it on my developer career, but I’ve never seen and touched a system so well structured and complete. Kudos to the platform team! In terms of cultural approach, I feel like home since the first day. You’re quickly introduced to how the company works. Of course, there is a lot of complexity here, but in the first days, you can draw a big picture about the product’s architecture and how all the services interact between themselves , thanks to a lot of gently onboarding meetings and one to ones with key people across your teams. Remote work is well established here, and it works like a clockwork: everybody can attend meetings at any time and communicate seamless, as we have rooms with shiny screencast devices at the office. It’s great to have the freedom to if you needed to stay at home , and still feel like you’re like at the office. My closest partner in crime in the backend side here, GermanDZ , has been very patient and helpful to me the first days, and since then, I enjoy our instructive pair programming sessions a lot! I’ve been real coding since the first day and was a great feeling to me to fit so quickly into a new environment with new people. Owing to our continuous communications and meetings between the whole team, we keep in mind a clear big picture of the direction and state of the product development. You really know what you build and what you’ll build. Bonus: code reviews are a great opportunity to see other people’s mind and be informed on how they do on the other teams :-) Last but not least, other key feature I enjoy here: we are encouraged to learn new technologies, languages, approaches, patterns and a wide range of computer science fields that we, as engineers, apply to our everyday work. Of course, there is allowed as well to propose new things, improve processes or tools, and to be brave!. This gives me a great opportunity to work and improve at the same time. You can buy books and take courses if you want! Do I hear more? Twitter Medium Nieves is working with us as IT recruiter Making a change is never easy, I remember doing the interviews for Jobandtalent with a mixture of illusion for what I was seeing and anguish in case everything was good and I had to make THE decision. Today, almost a year later, I can say that I do not regret it at all. Joining in HR to work in an IT team often involves a hard struggle to obtain enough information about the profiles, actual product structure, positions to be filled, technologies with which they work … here, these things were resolved during the first Onboarding week , where I had the opportunity to meet each team, understand the product inside (with a technical vision) and, of course, see the people with whom I would collaborate day by day in the company. Jobandtalent is different in many ways, but the most striking from the point of view of a recruiter is the total transparency and closeness that is possible to have with the team you work with ; It is something that is not forced, that is not coerced because, thanks to the reception (onboardings, presentation meetings …), the information (access to the product, code guides, technical groups, documentation …) and clarity in the processes, we can all understand and value the work of others much more, focus our efforts in developing strategies to optimize everything. Knowledge is shared and the opinion of each person is taken into account, independent of their background or experience. In these months, I can say that the greatest learning that JT has given to me is that, nobody becomes essential in a company for the information that is saved, you become essential when you share. jobandtalent.engineering Javi is working with us as a Backend Developer I’m working at Jobandtalent since 6 months ago and at this time I have learned a lot and I have to say that Jobandtalent brings their workers an incredible opportunity to raise to the next level . When I decided to join Jobandtalent I was looking for a new challenge in my career, something that let me grow as a developer, because I was feeling stuck and want to improve my skills. And then I made the right decision, join Jobandtalent. The first thing that I found at Jobandtalent was a lot of people willing to help me with anything and trying to make me feel comfortable in my first days , something that’s always difficult when you are facing this type of change. This welcome lets you start every day full of energy to face any new challenge. Something that also helps workers to improve every day and give the best of ourselves is that Jobandtalent encourages us to improve and share our knowledge, they promote tech talks, read and comment tech books, write articles and try new technologies, and this is something that makes a difference in terms of technical growth. But maybe the most impressive thing that you find when joining Jobandtalent is how big the product is, you cannot imagine at all, and in the first months you will be everyday learning about the product, but don’t worry about that, here you don’t have to read tons of documentation, or stay some weeks configuring your development environment, you will be working some hours after you arrive the office, and that is something that I really appreciate, you can stay learning about the product, the company and the way of working while you are being productive and without those boring first weeks that you usually have to face up when you change jobs. Here at Jobandtalent everything is very well structured and thought to be developer-friendly, you don’t have to tinker with boring and repetitive stuff if something can be automated that will be automated, deployment, maintenance tasks, plug and play development environments, CI and everything that can be a pain for a developer … All these small things let you focus on cool stuff, and that’s why I’m happy with this change because is letting me grow incredibly fast, I don’t have to waste my time with repetitive tasks or simple things, I can work with new technologies, focus on writing clean and quality code, improve my testing skills, contribute with new ideas, in short, do my bit to improve together every day. Twitter Medium If you want to know more about how is work at Jobandtalent you can read the first impressions of some of our teammates on this blog post or visit our twitter . Jobandtalent Engineering team blog. 9 Thanks to Manuel González Merino . Software Development Programming Startup Product Life Balance 9 claps 9 Written by The magicians doing software, devops, front and backend, mobile apps, recommendation systems, machine learning, clouded infrastructure and all that jazz. Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Written by The magicians doing software, devops, front and backend, mobile apps, recommendation systems, machine learning, clouded infrastructure and all that jazz. Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-01-22"},
{"website": "JobandTalent", "title": "screenshot testing on android", "author": ["Eduardo Pascua"], "link": "https://jobandtalent.engineering/screenshot-testing-on-android-88da6c004cf0", "abstract": "Data Science Backend Product Mobile Platform Frontend We are hiring! I just want to sleep well the night after we release a new version of Jobandtalent’s android app. In the first place, here, at jobandtalent Engineering , we have around 1,3K unit tests . When talking about testing our user interface, we also have quite a large suite of UI tests (another set of more than 1K instrumentation tests ) covering the different scenarios that our users may come across. We use Espresso to verify that our app responds and behaves as expected. However, there are some cases in which Espresso does not fit properly into what we want to achieve. We need a more powerful tool for those concrete scenarios. And this is why we began our experiment adopting an additional approach to UI testing: screenshot testing . In the following article, I am just presenting a strategy that works fine for us . This is neither universal truth nor something that may work for every team. There are two basic things that we want to make sure that our code is doing: testing user inputs and visual outputs to the user . On the one hand, we may want to check that the app manages correctly a concrete user input . For instance, when our users tap the login button, we want our login presenter to be notified about this event, so it executes the underlying logic. This is where we make use of Espresso to perform those necessary assertions on the code. In the previous example, we are checking that method onRefreshClick() in our presenter is actually called when the user taps the button with the text R.string.common_retry.button . On the other hand, we may want to test that the UI updates correctly to provide the user with visual output . For example, when there is a network error, we want the user to see an error state explaining the problem. In this case you can, of course, do something like this: This works fine. This test will fail if either the title, message, picture or button of the error state is not shown. Also, it will fail in the case that the message text is incorrect. Now is when it comes to the decision. Is this enough for us? Actually, it may be enough for some scenarios and that’s OK. There is no point in testing every little thing if that doesn’t provide us with an income of some kind. Let’s have a closer look at what that test is not testing . In the previous example, we are just checking that the picture is being shown, but we are not making any assertion on what picture is being shown . Furthermore, the picture must be in the center of the screen, but it may actually be on the right because of a bug and that test will not fail either. Not to mention that paddings could be one pixel wider than they should, and I only would realize this when a designer tells me (sorry guys, my eyes are not that precise 😉). You may think that this kind of bugs is not so common and that finding an alternative is not worth the effort. But we all know that upgrading the support library (or migrating to AndroidX) sometimes has unexpected outcomes. Wouldn’t it be great to have some automatic check that reports any problem with these small details that traditional instrumented tests are not covering? So, to avoid these kinds of tricky bugs to be detected in production, at Jobandtalent we decided to implement screenshot testing . In case you are not familiar with screenshot testing, I strongly suggest you have a look at the Facebook screenshot testing library . Screenshot testing basically consists of taking screenshots of some state of your app and comparing them against some reference that is considered to be the source truth. If you have been curious about that link before, you may have noticed that it is a library Facebook has developed to make screenshot testing easier. We are making use of it to implement our screenshots, along with the shot Gradle plugin that our friends at Karumi have developed. This plugin makes a lot easier to read the output from the Facebook screenshot testing library. It generates an HTML file that shows the original screenshots recorded, the actual screenshots were taken and the differences for each one. This allows us to check the differences with a single glance. The process that we have adopted consists of the following steps: In the first place, we implement our new custom view (or fragment/activity) and its corresponding Espresso tests as normal (screenshot and Espresso tests do not need to be exclusive, of course). Afterward, we think about those regular cases that the user will see (i.e the happy paths). We have to slightly change the way we create tests and now think about those stranger scenarios and how the app should handle them. For example, what happens if a TextView contains a very long line of text (should it wrap onto several lines or should it just ellipsize after the first line?). Then we run the shot plugin task to record the references for the first time. Finally, we review the references, if possible along with a member of our design team, to check that the pictures reflect what we expect from the app. If everything is OK, we commit those references into our repository, converting them into the source of truth. Following executions of the shot plugin will compare the screenshot results, pixel by pixel, against those references in the repository. These executions run automatically on our Jenkins environment, so if I have changed something in the UI that I was not supposed to change (even a color), then the corresponding test will break (and I will probably have to buy ice-cream for the rest of team…). When there is a failing test, the plugin will warn us and show us the difference from the reference UI. As you can see, the difference specifies what’s actually the problem showing explicitly the difference: This mechanism is especially useful for us when testing custom views that represent components of our design system (you may want to check out our post Working with a design system by my colleague Marcos Trujillo ). Once we have recorded our references, we use them not only for testing automatically against them, but also to have a visual archive of the most common (and not so common) visual scenarios that a user may run into. However, we also have screenshot tests for whole activities and fragments. jobandtalent.engineering Screenshot testing is a powerful tool that allows us to test pixel by pixel our user interface state. It has some clear advantages: It can detect bugs that will remain undetected using standard Espresso tests. For example bad layout implementation, incorrect spaces, paddings or colours… The design team can come and join us during this testing process. This allows us to receive early feedback from them, instead of showing them the final implementation or releasing an internal beta version. Feedback during this phase is easier to apply than feedback received, for example, during a QA session. These tests can also run automatically on our CI environment, so failures will be reported in a short period of time. However, this does not come for free. It has some disadvantages that should be taken into account: Tests become more fragile. Any minimal desired change will make the references outdated, meaning that they need to be recorded again. Recording references is not a very fast process. These tests generate a lot of png files that need to be stored in the repository. This system can be improved in the near future in several ways: Opening as many testing branches as languages supported by our app to make sure that everything is working fine for every locale configuration. Open new testing branches that use different API versions and devices, so we can check that some concrete view is working properly in different Android versions. Automatically publish the recorded references so they can be checked out not only by the design and engineering team but also by the rest of the company. As a conclusion, we can say that this kind of tests has helped us a lot to detect bugs with much higher precision. However, if some views change very often, we have to rethink if all the processes that come with this testing mechanism are worthwhile. Like any other thing, this is the right tool for the right problem. I want to give some kudos to the Karumi team for that awesome formation on mobile testing received in one of their courses which I can’t recommend enough. If you want to know more about it’s like work at Jobandtalent you can read the first impressions of some of our teammates in this blog post or visit our twitter . jobandtalent.engineering Jobandtalent Engineering team blog. 144 Thanks to jobandtalent Engineering and Sergio Espeja . Android Mobile Testing Testing Development Mobile 144 claps 144 Written by Android Engineer @ Jobandtalent Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Written by Android Engineer @ Jobandtalent Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-09-25"},
{"website": "JobandTalent", "title": "how to get the most out of salesforce data from business to data science", "author": ["Fernando Villanueva"], "link": "https://jobandtalent.engineering/how-to-get-the-most-out-of-salesforce-data-from-business-to-data-science-41e539fe212e", "abstract": "Data Science Backend Product Mobile Platform Frontend We are hiring! Jobandtalent is a very ambitious data-driven company that aims to automate any data process. Finding the best way to accomplish this goal is one of the responsibilities of the Data Science team, but the first step is to keep calm and understand these processes in depth. This is why we need to have a very close relationship with the other departments; Product , Ops , Sales , and Marketing are indeed among our main partners and stakeholders. This creates a win-win relationship between the Business and Data teams , where limerence can happen: The Business teams keep us up to date with their most relevant projects, convey to us their needs, and allow us to learn from their strategies — and this is super exciting! The Data Science team supports them, either through Exploratory Analysis, Dashboard Systems, ETL Automation, Advanced Analysis, or Artificial Intelligence solutions — and this is super rewarding! It is important to point out that many times Business teams start proactively with an initial solution that quickly solves their initial problem. This is very positive for Jobandtalent as well as for the Data team, since this solution can meet their initial business needs and they can test, evaluate, mature and use their solution before the Data team steps in to build an automated and scalable one. One of the best examples is when it comes to integrating key 3rd party products such as Salesforce . For the Data Automation team, it’s not just a matter of integration, it’s much more than that. In order to get the most out of your CRM data, it is necessary to understand its meaning. Integrating several data sources, generating multiple tables whose information comes from 8 Salesforce objects and other Jobandtalent data, and building 6 dashboards with more than 300 charts, is only possible by working side by side with the Sales Operations team . That is why we believe it is important to talk about not only the final Business Intelligence solution but also each of the steps that every team had to take before reaching it. Salesforce helps us to manage all the information related to our clients and prospects and to record all of our Sales team’s actions in each of the 7 countries in which we operate. This means that Salesforce stores a large volume of data about accounts, opportunities, events (meetings), tasks (calls, emails, LinkedIn messages), contacts, users, etc. It is not only important, but also imperative to define and analyse the related KPIs in order to make the right business decisions. To this end, the Sales Operations team performs these main analyses: Compares real vs. target information of calls, meetings, open opportunities, sent proposals, won opportunities, and tracks the fulfilment and its temporal evolution. Calculates ratios regarding opportunities. Follows up the open opportunities according to the stage they are in and the type of opportunity. Calculates conversion rates between calls, meetings and opportunities. Makes predictions of results based on historical data. Tracks the individual fulfilment of targets for each member of the Sales team. Before defining the final requirements and involving the rest of the teams, the Operations team began to exploit this data through a first manual approach consisting of: Creating reports in Salesforce. Exportation of each report to a CSV file. Importation of the files into an Excel template containing all the calculations. Sending of the weekly report by email, through screenshots. This first manual solution allowed the business team to explore the data and start getting super useful insights. However, there were some drawbacks : Too much time spent in manual data export and import, and adaptation of the template. Tedious manual preparation of various reports and emails. Possible human errors in data manipulation. The Operations team, aware that they were spending too much time on manual actions, decided to make a semi-automation of their first solution , including: A G-Suite marketplace add-on that allows you to connect Salesforce and Google Sheets and automatically import reports. Automatic mailing of some reports using the script editor . As their project evolved, the following issues emerged: This add-on only allows us to refresh the information every 5 hours. There were more and more tabs on the spreadsheet, and the calculations were becoming too many and too complex. It required a lot of maintenance. Automation only saved time in exporting from Salesforce and importing into the spreadsheet. The team spent a lot of time reviewing the spreadsheet in search of details. A solution was needed that would allow them to easily drill-down into the data. The project was highly collaborative, so too many people accessed this spreadsheet and that affected its loading performance. At this point, the Data Science team stepped in to automate the entire process by an ETL solution, helping the Business team to forget about any manual tasks and focus exclusively on analysis and decision making . Before starting on any complex integration, it is necessary to make first contact with the data and start getting familiar with it. The first objective is to obtain an automated MVP , following the steps below: Extraction Use the Salesforce API to retrieve the Salesforce data we need. The Simple Salesforce library allows you to retrieve the data through SOQL queries or directly fetch reports already created. Transformation Analyse and understand all the calculations made on the spreadsheet. Work with Operations to resolve any concerns that may exist, before, during, and after the construction of the MVP. Using Python , build a local ETL to replicate Salesforce reports in a Jupyter notebook , performing all the calculations and transformations defined by the Operations team, resulting in several dataframes about Accounts, Opportunities, Meetings, and Calls. Load With these dataframes, create the test tables in our Data Warehouse to start making the first dashboarding tests in Metabase (read our blog post about Data Visualization tools at Jobandtalent). The time has come to implement the process in our ETL in production . The Data Engineering team adapts and refactors the code until a robust and optimized solution is achieved. The final solution is the implementation of the following ETL in Airflow : Extraction Through Stitch , every hour we dump new Salesforce raw data into our Data Lake . An Airflow task unloads the data from the Data Lake . Transformation Creates the dataframes that replicate Salesforce reports . Generates the in-depth reports , which will be the final dataframes containing all the calculated fields. Load Uploads the final tables to our Data Warehouse . Finishes building and fine-tuning the final dashboards in Metabase . Small improvements , as an adaptation of small changes Updating dashboards with new values of existing fields. Adding new fields to existing Salesforce tables. Creation of new charts. Great improvements , as a result of the evolution of the Sales Operations reports: Adaptation of ETL for the integration of Salesforce with other tools, such as SalesLoft . Creation of new tables and new metrics. Construction of new dashboards. By working together with the Operations and Data Science teams , we have achieved a robust and automated solution that retrieves and calculates relevant information from Salesforce, and currently feeds a total of 6 dashboards with over 300 charts . In short, the solution adopted has the following advantages : Complete elimination of manual actions . The Operations team focuses exclusively on tracking dashboards and defining strategy. Fast loading of dashboards . Very efficient for all Operations meetings. A robust solution that allows us to have all the critical parts of the code properly tested (something that is not possible in other solutions even with “drag & click” ETL). Clearer visualization of the results, with charts adapted to the nature of data. Higher frequency of execution of ETL’s DAG . The data is updated every N minutes, where we can configure N according to the needs. Drill-down that allows deep-diving into data. Detailed and accessible documentation . The ready-to-use tables available in the Data Warehouse allow Operations to continue experimenting with new reports, which will be automated in the near future. If you want to know more about how is work at Jobandtalent, you can read the first impressions of some of our teammates on this blog post or visit our Twitter . Acknowledgments: thanks to Michele Trevisiol , Cecil Fernandez , and Sergio Espeja for feedback and reviews. Thanks to the Data Science and Operations teams for making this project a success. Jobandtalent Engineering team blog. 41 Thanks to Michele Trevisiol . Development Data Science Salesforce Data 41 claps 41 Written by Data Analyst at Jobandtalent https://www.linkedin.com/in/fernando-villanueva-arce/ Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Written by Data Analyst at Jobandtalent https://www.linkedin.com/in/fernando-villanueva-arce/ Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-07-02"},
{"website": "JobandTalent", "title": "the navigator", "author": ["Rubén Méndez"], "link": "https://jobandtalent.engineering/the-navigator-420b24fc57da", "abstract": "Data Science Backend Product Mobile Platform Frontend We are hiring! Many iOS applications are pretty simple regarding navigation flows. In fact, having simple flows really helps to have a very easy-to-use app. Many apps are based on a tab bar controller with some sections. Each section tends to have a navigation controller that stacks other view controllers. And very often, a view controller A always presents a view controller B. To handle that navigation, we traditionally end up adding this kind of code in our view controllers: It could be discussed if having this kind of code in the view controller is right or not, but in my opinion, this could work perfectly fine if your app is simple and it’s not going to grow in complexity. But in more complex apps, like the Jobandtalent app, this kind of navigation doesn’t fit well and has several associated problems. The Jobandtalent app has experienced many changes since it was originally launched. In a start-up, the business model is likely to change very often until the company finds its product-market fit, and our app wasn’t an exception. The Jobandtalent iOS engineering team had to face this fast-changing-context problem some time ago. Changing the business model had a direct effect on the navigation flows of the app, having to change very often the navigation stack. If an app doesn’t have a navigation architecture able to change easily, this becomes a problem when the team needs to iterate quickly. So we decided to really think about the navigation architecture of the app to make changes in a smoother and faster way. These were the main requirements we wanted to fulfill: We should be able to change the navigation stack easily. Reuse controllers in different navigation flows. Reuse navigation flows. Having navigation logic in the controllers was getting in the way of reusing them. Reusing controllers in different flows, and creating new flows was impossible if the controllers themselves were responsible for the navigation logic. If controller A always navigates to controller B, it would be difficult to make, in other scenarios, that controller A navigates to controller C without making the controller’s logic even more complex. To fulfill these requirements, the iOS team has iterated over different navigation solutions. This article illustrates the different solutions we have applied, the trade-offs we have found and how we have reached the best possible solution for our application so far. At the time we started to think about our navigation problem, VIPER architecture started to sound loudly in the iOS community. That architecture was based on the Clean Architecture concept of splitting the app into different responsibility layers. One of them, called Router , was in charge of handling the navigation logic. Routers are also called Coordinators in other architectures, but they are pretty much the same. Using routers apparently fulfilled our initial requirements: We could change the navigation stack easily. We could decide if we present something modally or pushed into a navigation controller without changing any code in a view controller. We removed the navigation responsibility from the view controllers, so they could be reused in other flows. Having packed a navigation flow inside a router, we could reuse easily navigation flows. But after some time using this abstraction, we found that having the view controller layer separated from the router layer forced us to have code to sync them. Handling the lifecycle of routers became tricky. Routers should live as long as their underlying view controllers live, so dependencies get deallocated automatically when the navigation finishes. Routers make this quite difficult, with a lot of boilerplate and prone-to-error code to pull this off. Trying to fit routers with UIKit made us feel uncomfortable, so we started to search for a lesser constrained approach. After some research, we found a really interesting article where it was proposed to use something called “Manager View Controller” to coordinate navigations. We didn’t like the word Manager too much so we decided to give it a more semantic name to this abstraction. We called it FlowController . A FlowController has the same responsibility of coordinating the navigation like a Router , but the main difference is that the FlowController is a subclass of UIViewController . The use of UIViewControllers as coordinators comes with a lot of useful features for free. The most relevant ones are: View controllers can add other view controllers into their view hierarchy using the parent-child container relationship . UIViewController has useful capabilities by default like being able to present modally or push into a navigation controller other view controllers (therefore able to present other FlowControllers ) or even use some useful patterns like the responder chain. The main idea of using FlowController is to act both as a container and a coordinator. It can contain the controllers to be presented in a piece of navigation, and those contained controllers can delegate their navigation needs to the FlowController . Since FlowControllers are UIViewController subclasses, they can be integrated easily with the navigation stack of the application, thus, this reduces the two-layer-sync complexity we had with Routers . When the FlowControllers get out of the navigation stack, they are released, as well as all their child view controllers and all their dependencies, automatically. For a more detailed comparison between FlowControllers and Routers , please, read this nice post about the topic . After these iterations, we were pretty confident about what we had achieved in terms of improving navigation. We ended with all of our application flows embedded in different coordinators. Everything seemed pretty fine, our view controllers were not aware of navigation code anymore, we could change the navigations easily, it was very easy to create and present other coordinators using the FlowController abstraction, so everything was happiness. But after some time using flow controllers we faced up with the following problem: reusing them was not as simple as we thought. The problem was having to reuse a subset of the navigation embedded in a flow controller. If we needed to reuse that subset we were forced to split a flow controller into smaller ones to reuse the subset. That doesn’t seem to be a big issue, but each time we had to reuse a piece of navigation subset the development speed was affected and also the number of flow controllers raised more than we expected. This problem made us think that maybe we were adding an unnecessary layer of complexity to the project. We realized that having so many flow controllers was making our navigation logic very complex. In fact, some questions were flying around our heads. Do we really need flow controllers? Can we simplify our navigation logic so we can remove that abstraction from our architecture? Let’s review one of our main requirements, decoupling view controllers from the navigation logic. The main reason to decouple our view controllers from navigation was to be able to change the navigation easily or to have the same view controllers navigating to different flows depending on the business logic. But we realized that it didn’t happen very often. The view controller A always was presenting controller B and there were very few corner cases where controller A had to present a different controller. Thus, we were using an abstraction to only support very few corner cases, if any. This is a perfect example of early abstraction. We were making things more complicated without really needing it. What we actually needed was an easy way of presenting view controllers, abstracting the view controllers from the navigation stack. A collaborator to whom messages like “present this view controller with this navigation traits”, can be sent. This is what we have called The Navigator . The Navigator is a component that can perform a previously modeled set of navigations. This model defines which view controller to present and how we want to present it. The view controllers are only in charge of commanding that navigation to the Navigator, but they are not aware of how to instantiate the view controller to present, build any required dependency or knowing the navigation stack where the new view controllers will be pushed onto. Before going into detail about what the Navigator is for us and how it is implemented, we need a way to tell the navigator the controller to navigate to and how to perform that navigation. If we analyze carefully what the main navigations on an iOS app are we could find the following types: Present a section of the UITabbarController . Push a UIViewController on top of a UINavigationController . Present modally a UIViewController . Taking that analysis into account we can model our possible navigations into a Navigation model using value semantics. This is an example of our Navigation model: This simple Navigation model is defining all possible navigations of our application. The section case is describing a UITabbarController navigation with all sections defined in the Section model. Also, this Navigation model includes the possibility of presenting a UIViewController , modeled with the Screen struct, either modally or pushed into a UINavigationController . Modeling navigations with enums and default associated values has allowed us to pass optional parameters to configure navigations in cases where more customization is needed. Most of the times using .modal(.screen) is more than enough, but it would be possible to be more specific about the presentation style, for example using .modal(.screen, .popover) . Our first implementation of the Screen model was a simple enum, but we realized that scaling an enum adding cases was easy but the Screen enum became a massive file. The idea of using a struct and wrapping the view controller into a property allows us to extend easily the Screen struct adding static functions that return Screen instances. Those extensions can be in their own files, or even in their own modules so we avoid having a massive file with all possible screens. Another consideration is that the view controller property is not actually a UIViewController property, instead, it is a function. This is to load the view controller lazily just when it is needed, not when the navigation is created. Find some examples below: It is time to introduce the missing part of our navigation architecture. The Navigator is the logic piece in charge of performing the navigation defined by our Navigation model. The implementation of the navigator is pretty simple. We have some utility functions which the navigator uses to simplify the navigations: tabBarController() returns the main tab bar controller of the application. The navigator will tell it to present a concrete index associated with a section. topMostViewController() returns the currently presented view controller on the screen. This way, the navigator can present any view controller modally over this controller. currentNavigationController() returns the current navigation controller presented on the screen. The navigator just needs to push a new view controller into the current navigation controller. The key thing about those methods is that they are computed and aware of the view hierarchy in order to dynamically retrieve those. That means that the current view context is automatically figured . Navigation-wise, we usually always push on top of the current navigation stack. When we previously had different routers and coordinators maintaining and wrapping each of the current navigation stacks within each of the tabs, we now simply have a way to ask “hey, which is the current navigation controller visible?” and let that be retrieved at runtime, regardless of the tab we are at. Inside our architecture, the navigator is a single instance which is told to perform a piece of navigation, for example: As we can see, we have reduced a lot the complexity to handle navigations. We have a straightforward model defining navigations leveraging value semantics, a very simple piece of logic to perform them, and very important for us, an effortless way of commanding navigations from the effect handler of our architecture . The effect handler is totally abstracted from the navigation hierarchy of the app. It only knows which navigation to perform but not the way it is performed. This way of abstracting navigations is more consistent with the current Jobandtalent application architecture, nicely explained by Luis Recuenco in the following post series about state container architectures . Our Navigation model is the value describing the navigation and the Navigator is the interpreter of those values, converting those navigation values into navigation effects (view controller presentations). jobandtalent.engineering Besides, having the navigation modeled as value types has the advantage of leveraging snapshot-testing to test navigations in an easy way. For example, this is a real test in Jobandtalent application: This test creates a NavigationSpy to replace our real Navigator implementation. It then performs an action on the view controller which should eventually trigger a piece of navigation on the Navigator (NavigatorSpy) . Finally, the result of the snapshot navigation test is: If any change or refactor breaks this navigation, the snapshot test will warn us about that fact. We have replaced tests that would require quite a difficult mocking infrastructure to intercept calls and parameters in a simple, broad-coverage snapshot test that prevents us from nasty refactoring mistakes. And that’s it! We have completely removed the need to have many coordinators in favor of having just a simple component to handle that responsibility. We have leveraged the possibility of showing any view controller from any part of the application, reusing the navigation logic with a very simple implementation. We don’t know if the Navigator will be our last iteration about navigations because, at Jobandtalent, we like questioning our own implementations and architectural decisions constantly, but we are very happy with the result obtained so far. I have had many conversations about navigations with other colleagues and I have seen that the Coordinator pattern is the most popular approach in their projects. The Coordinator pattern is a great way of handling the navigation in an iOS app, but you might also not need that extra complexity. Do not adopt an architecture or pattern because of fashion trends, without questioning their trade-off. Do not over complicate our applications without questioning things, and remember that we have created great apps using a very simple approach . Take into account that this navigation architecture fits our project needs, but other projects or applications may need a different way. The point of this article is to provide some inspiration for other projects and to show our refinement process to get something more adapted to our application needs. One of the main goals of the iOS team at Jobandtalent is to seek happiness in our day-to-day development and being critical with our own decisions. This navigation analysis is just one example of that. Thanks to Luis Recuenco and Victor Baro for all the valuable feedback while developing The Navigator. If you want to know more about what it is like to work at Jobandtalent, you can read the first impressions of some of our teammates on this blog post or visit our Twitter . Jobandtalent Engineering team blog. 179 1 Thanks to Luis Recuenco , Victor Baro , and Miguel Ferrando . iOS Swift Mobile Development Navigation 179 claps 179 1 Written by Software Developer @jobandtalentEng Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Written by Software Developer @jobandtalentEng Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-03-09"},
{"website": "JobandTalent", "title": "how to create web components by a project", "author": ["Iris Carballo"], "link": "https://jobandtalent.engineering/how-to-create-web-components-by-a-project-7577e5cf2262", "abstract": "Data Science Backend Product Mobile Platform Frontend We are hiring! This is a how-to about creating native web components. I will skip the style part in the explanation because it’s not so relevant and the project would still work without it, albeit in a less aesthetically pleasing way. We’ll start by explaining a little bit about what web components are, then we’ll look at an step-by-step example that can be viewed with this repo or in this live demo To use web components it’s important to understand what they are, according to MDN , web components are based on three main technologies, which can be used together to create versatile custom elements with encapsulated functionality that can be reused wherever you like without fear of code collisions. Let’s introduce some web components definitions: Custom elements: A set of JavaScript APIs that allow you to define custom elements and their behaviour. Shadow DOM : A set of JavaScript APIs for attaching an encapsulated “shadow” DOM tree to an element — which is rendered separately from the main document DOM — and controlling associated functionality. Elements can be scripted and styled without the fear of collision with other parts of the document. HTML templates: The <template> and <slot> elements enable you to write markup templates that are not displayed in the rendered page. These can then be reused multiple times as the basis of a custom element’s structure. “W3C” includes: ES Module specification: defines the inclusion and reuse of JS documents in other JS documents. The idea of this project is to create a couple of components that are connected between them, not only to generate a component but also to see how you can pass data from one to another. In the end, we’ll have four buttons that, when you click on them, show you some movie quotes. So let’s start by creating an almost empty project with index.html, style.css, and main.js files. Our second step would be to create a container component into our main.js file to be able to pass props between its children. Here we are already using the three main technologies. First of all, we are creating a class that extends HTMLElement , an API that represents an element in the HTML document tree. HTMLElement is the base type for HTMLDivElement, HTMLSpanElement, HTMLImageElement and many others and allows us to create our own HTMLElements. Custom elements allow us to extend existing (native) HTML elements as well as other custom elements. Then we use the constructor to set some initial state, event listeners, and create the shadow DOM . An important thing here is { mode: ‘open’ }. What is this for? Well, it’s to avoid public access to the nodes within the shadow tree. So try and ‘Close the DOM!’ (this is funnier if you imagine it said by Hodor). However, both open and closed modes have the same benefits: an isolated DOM, scoped CSS, and a declarative, markup-based API. The problem is that it can be perceived as a ‘security’ feature. So maybe it is better if we keep the DOM open for now. If you’d like to find out more about it, this article is really interesting. Last, but not least important, we are using the <slot> element from HTML templates to include, in the near future, each of our inner components. Ok, now we already have our first web component, the hardest part… but let’s dig a little deeper. We are going to create our NavBar component. There are two new things here. First, we are generating the navbar options dynamically, so we’ll have as many tabs as objects in our data file. Second, we are handling the onclick event from our new option_button. This is important because it allows us to set the selected attribute in our display component. So finally we need to create the Display component: In the Display component, we need to recover our set attribute and we will do it with static get observedAttributes() , this method should return an array of strings where each string is the name of the attribute you wish to observe. It is also aware of the changes in these attributes with the attributeChangedCallback() method. In our case, selected , this will only work with the attributes that are being observed. Finally we call the setSelected function that will return one random item inside the array in the selected object from the data file. In conclusion, I would say that it is quite easy to get started with web components , especially if you have had some previous experience with frameworks such as React, Vue or any others. It would be a good launchpad to start using web components alongside the framework that you are already using as they have complementary purposes. While frameworks allow you to handle state, custom elements are stateless and provide your components with encapsulation, they also make your components agnostic which can be a important aspect of a good Design System but we will talk about this in another post. There are also a bunch of libraries that can help you to improve your web components as much as you want: - Stencil.js , open-source. - Polymer , build by Google. - And many others that you can check out here: https://www.webcomponents.org/libraries . Hope you enjoyed this article and feel free to clone or improve my project here: https://github.com/IrisCZ/web_components _the_beginning/ Or if you just want to see it work: https://iriscz.github.io/web_components_the_beginning/ Acknowledgments: many thanks to Sergio Espeja and Bethany Solberg, this article wouldn’t be the same without them. If you want to know more about it’s like work at Jobandtalent you can read the first impressions of some of our teammates in this blog post or visit our twitter . Jobandtalent Engineering team blog. 25 Thanks to Sergio Espeja . JavaScript Web Components How To Front End Development Development 25 claps 25 Written by Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Written by Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-09-26"},
{"website": "JobandTalent", "title": "data visualization tools at jobandtalent", "author": ["José Gabriel Martínez"], "link": "https://jobandtalent.engineering/data-visualization-tools-at-jobandtalent-fcd3bb93a2be", "abstract": "Data Science Backend Product Mobile Platform Frontend We are hiring! As a company like Jobandtalent grows, the data that it generates grows with it exponentially. You need to create processes that control huge and complex data flows, as well as finding resources to give it structure and meaning, as we mentioned in other posts . One important step in this process is choosing the right data visualization tool. In this post we will try to cover the most important aspects to consider, such as connections support, chart diversity, sharing features,… At Jobandtalent we have different data needs depending on the team, these needs have defined our dataflow . Regardless of the source of the data, our goal is that the Dashboard creation process is equivalent for all of them, and in order to achieve that we have designed a dataflow focused on visualizations. All the complex rules and calculations will be done by our ETL and loaded into our data warehouse in a format that is easy to read by the top layer. The dashboard layer is last layer and is independent and focused only on visualizations, without storing any complex logic, this allows us to switch between visualization frameworks more easily. Even if we need to access the raw data or create long-term visualizations on dashboards from processed data, this dataflow covers all our needs so far. This even allows us to incorporate external information and use it along with our data. In order to understand which visualization platform better fit our needs, we defined the following proxies as evaluation metrics: Connections support . It’s important to be able to connect to as many sources as possible, such as databases, upload CSVs, … Chart diversity . In order to better understand the data we are analyzing, a large variety of charts will allow us to get and share insights in a more efficient way Usability . If we want the framework to take root within the company, it needs to be user-friendly, personalizable, easy to use, easy to admin, … Sharing . This is one of the most important features, in the end, as we will need to share the dashboards both internally and externally. If the framework allows us to do it in a more transparent and easy way, the adoption within the company will grow faster Support community . As an open-source tool, it needs to have a large community behind it so it keeps evolving. We have been playing with different visualization platforms, in our case, we have been using Superset and recently Metabase . jobandtalent.engineering Superset is a web-based visualization tool originally designed by Airbnb and lately moved under the Apache Foundation . The tool has a visual and intuitive interface that allows the user to explore and visualize the data through a variety of different charts. With no coding and few clicks, we can build a dashboard and make it interactive using filters, directly from our data warehouse. Superset is based on Python, so as long as there is a Python driver with SQLAlchemy support , we are able to connect it to any data warehouse. What if the information we need to visualize is stored along with different databases? We can use the SQL Lab on the platform and create a new view from a query, from which we will be able to plot different visualizations and treat them as if they were a database table. Furthermore, you have the possibility to upload CSVs , which is a great feature that can come in handy at points during your data investigation. Once you make your data available you can create the chart. Superset offers a huge variety of charts, some of them requiring a specific table format (such as the Sankey visualization). Chart personalization is a bit limited unless you want to dive into the source code, but on the other hand, the huge number of visualization types compensate for that. Once the connection with the database is set, you need to manually add the table that you are planning to use, review the data type on each column and make them filterable/groupable. In order to plot the data, you have to previously create the metric you want to show, all in SQL and inside the table configuration menu . There is no way to test your metric before plotting it, so if you write a typo in your SQL you won’t notice until you go to the chart creation menu. Another little limitation we found is that you cannot organize the charts into folders , having only the possibility to filter them. This can become a bit tricky when the list of charts starts to increase. On the Users admin side, for each user’s role you need to assign a list of permissions, in my opinion, there are too many permissions and it can take a while until you get familiarized with them. In terms of sharing, Superset is very limited. You cannot share any dashboard with an external client using a public link, instead, you have to create a user for them, create a very limited role and assign it to your clients. This shouldn’t be a limitation if you are planning to use this tool internally. If sharing is not a big deal for your needs, but having a lot of visualization possibilities is, Superset will probably be what you are looking for. Another open-source project is Metabase. With a very friendly UI, you can create visualizations and dashboards very quickly. Furthermore, sharing dashboards is a very easy task with this tool, so if you don’t need a wide variety of charts, this could be a good option. Metabase supports a long list of data connection s such as BigQuery, PostgreSQL, Redshift, H2, … but you won’t be able to upload a CSV and work with it. For most cases, having these connections is enough. Another feature shared with Superset is that you can run a query, save it as a question and build a chart or a new query using it, which can come in handy if you don’t need the whole dataset. Metabase includes the basic charts to plot our data in the most common ways, but if you are planning to build complex charts with complex visualizations, such as Sankey, you won’t find this option within the tool. The personalization of each chart is slightly better than in Superset, you can choose the color of each plot, the order of columns in tables, the format of numbers, etc. A good thing about Metabase is that it allows you to create collections : a folder where you can store your charts and dashboards and keep them organized. With a very user-friendly interface, the process of creating and consuming a chart/dashboard is very simple even if you don’t know SQL. There are several ways to create a chart (or a question, as they call it). You can use a wizard or write your own SQL query . You can also play with the filters within the chart before using them in a dashboard, which can help you to tune the visualization. On the administration side, you will be able to add and manage the data connections very easily, as well as refresh the data whenever you need, the tool will automatically detect the tables inside the database and will display them ready to be managed. There’s no need to manually add every table you want to use. To manage the users you can send invitations by email, allow them to log in with Google and assign groups with different permissions (this part can be a bit limited if you want to grant complex permissions). There are still some bugs that you will find while using the platform. Some of them are more critical than others. The community behind the tool is solving them release after release, but depending on your needs it can take longer than expected, so it’s your choice if you decide stay with them or move to another tool. Sharing is one of the most powerful features of Metabase. Whether you need to get a public link to share a dashboard, embed it into your blog, or integrate it within your application, you will be able to show that dashboard to the world. It has also the option to create Pulses (email reports) but we think there is still some progress that needs to be made on this feature, as the format displayed in the email is a little bit poor. Superset: Easy to connect to almost any database . Possibility to use your own CSVs A huge amount of visualizations Perfect as an internal tool with a limited number of users using it Poor personalization features Larger support community behind the tool Metabase Supports most common databases Limited amount of visualizations , only the most basic ones. Very easy to share with an attractive UI More possibilities for personalization Smaller community behind the tool, solving a bug can take months In conclusion, both tools are open source and allow you to create charts and dashboards in an easy way. Depending on your visualization, sharing and personalization requirements you should choose one or another. At Jobandtalent we started with Superset but due to the increase in the number of users needing to have a BI visualization tool, the need of freedom to interact with data and the possibility to organize the charts and share the dashboards with others, meant we have settled on Metabase. If you want to know more about how is work at Jobandtalent you can read the first impressions of some of our teammates on this blog post or visit our twitter . Jobandtalent Engineering team blog. 94 Thanks to Joaquin and Michele Trevisiol . Analytics Data Data Visualization Data Science Visualization 94 claps 94 Written by Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Written by Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-07-18"},
{"website": "JobandTalent", "title": "predicting contract length with probabilistic programming", "author": ["Antoine Hachez"], "link": "https://jobandtalent.engineering/predicting-contract-length-with-probabilistic-programming-2015f7c7cccb", "abstract": "Data Science Backend Product Mobile Platform Frontend We are hiring! Jobandtalent manages the lifecycle of thousands of jobs. From the requirements, to the recruitment, to the management of the contract, to the end of the contract. We even try to accompany the worker to their next opportunity. One of the key metrics in this industry is obviously the length of the contracts as it is proportional to the revenue per contract, which is compared to the fixed and variable costs of managing it. However, the length of the contracts is not an easy KPI to measure because we do not know in advance when a contract will end (they are often renewed many times, and they can also be shortened). Therefore if we were interested in the average length of the contracts that started this month, we would have to wait until all contracts have finished to finally know the true value (it can take a few years!). Can we make a good prediction instead? The data team was tasked with tackling this challenge. In the following sections, we detail our approach that mixes elements of survival analysis and Bayesian inference . In the end, we implemented an algorithm that provides an accurate prediction, continuously updating it as more information becomes available. Survival analysis is the branch of statistics interested in modeling the time to an event. In this case, we are measuring the time between the creation (“birth”) and the end of a contract (“death”). As previously mentioned, the difficulty of the task is due to the fact that for all ongoing contracts, this data is missing. Those observations are defined as “censored” because at this point in time we don’t know yet when they will end. The most important tool in survival analysis is the survival function. It is a curve along a time axis that displays, for a given time, the proportion of the population that is expected to be “alive”. This curve tells us all we need to know about the length of the “lives” of the population. In fact, it can easily be shown that this curve is simply 1-CDF(T), where T is the random variable representing the lifetime, and CDF(T) is its cumulative distribution function. Our problem can thus be solved if we could just estimate the survival function of T. We choose to create a parametric model since we will need to predict the shape of the survival function well beyond the observed timeframe. However, this requires us to correctly specify a distribution. The usual tool for selecting a distribution is a QQ-plot , but this is generally not appropriate for censored data. Thankfully, the lifelines python library contains routines to take censoring into account and still create Q-Q plots. From the images above we can say that, among the distributions most used in survival analysis, the Weibull distribution fits the best. It is an approximation though, and we will have to take this into account when interpreting the results. Weibull distribution is characterized by two parameters, here called k and λ . λ is called the scale parameter because it controls how close to 0 the mass of the PDF (probability density function) is. k is called the shape parameter because it changes what the PDF looks like near 0. We would like to see the evolution over time of the length of the contract. It was decided that a month was the right granularity level, so we grouped the contracts by their month of creation. For each calendar month, we want to compute the average lifetime of the contracts that started during that month. The easiest approach is to consider all of those populations independent. For each month, we can provide the lifelines API with the censored and uncensored lengths and it will return the values for λ and k that are most likely to generate those observations. However, we found that this approach created very unstable historical predictions and a very unreliable prediction for recent months. We needed a model that would include our knowledge that one month cannot be that different from the next. We also needed a model that would tell us how much confidence we can have in its predictions. Probabilistic Programming met our needs very well. Probabilistic programming languages (or in our case, PyMC3 , a Python library) simplify the creation of probabilistic models and the inference of the parameters of the model. In simple terms, we can specify our model: Then, we need to tell our model the durations of the finished contracts we have observed: finished contract durations (in days): [50, 110, 33,…] In PyMC3, these two things can be done in a single line of code: It was very simple. Now, the idea is to let PyMC3 use its algorithm* to tell us what values λ and k can explain our observations. It will not return a single value, but a long list of samples that were taken from the joint posterior distribution of λ and k . We can just run: pm.sample() This was the core of the model. It is missing a few parts though. First, we should specify a prior for the parameters λ and k . Second, we should also include unfinished contracts! They contain essential information since we would vastly underestimate the average length of the contracts if we ignored them. For a contract that is 40 days old, we know that the time to finish will be higher than 40. The probability of this happening is given by the survival function with parameters k and λ : sf(40, k , λ ). For PyMC3 to take this into account we have to pass the log probability function to pm.Potential: Finally, we need to define all of the above within the context of a model (“with pm.Model():”). Here is a simple version of our model: So far we have defined a model that can infer the Weibull distribution for a given period, in the form of a trace of samples for the pair ( λ , k ). Since we have samples for λ and k , we can easily compute the theoretical average for each pair and get a credible interval on the actual average. Next, we will see how we can make predictions of neighboring months dependent on each other. Extending the model is relatively straightforward. We use a modeling trick (explained in more detail by Thomas Wiecki in his blog post ). Essentially, we take the series of log(λ ᵢ ) (representing the scale parameter for Tᵢ, the lifetime of contracts of month i), and tell the model our prior assumption that they will follow a random walk with a certain standard deviation sigma. With this prior, the log( λ ᵢ ) can be positive or negative, but λ =exp(log( λ ᵢ)) will always be positive, which is required for Weibull distribution. In effect, it penalizes the posterior where λ ᵢ is very different from λ ᵢ₊₁. “Very different” depends on sigma, which is itself a parameter learned from the data (with its own prior). From the previous section, we get estimates for λ ᵢ and k . We can use them directly to compute the “underlying” mean lifetime of the contracts of each month. Alternatively, we can use them to simulate only the lifetimes of the contracts that are still ongoing (sampling from the conditional distribution P(T|T>time_until_today)). By averaging over many of those simulations we can get an estimate of the average length we will actually observe. Since our approach is based on creating realistic simulations, we can generalize it to compute not only the expected average length but any KPI based on the length of the contracts (such as the average length without outliers). We have worked in close collaboration with the Strategy team, one of the main stakeholders for this metric. We had to make sure we covered their requirements and that they understood our approach. The first step was to understand how they had been computing this metric previously, and how they use it. Secondly, we set out to create the most naive predictor possible. It allowed us to quickly get acquainted with the data and understand the challenges. This provided us with a first baseline. Finally, we started iterating over more and more complex models, taking care to only add complexity where needed and to maintain the feedback loop with the Strategy team through weekly meetings. These iterations resulted in the model described above. The flexibility of our approach continued to pay off while we quickly iterated to create the KPIs that would fit their needs (actionable and unbiased by outliers). The Bayesian approach offers many advantages in this regard: Hypotheses are clearly stated in the code that mimics the generation process of the data. It generates realistic scenarios, based on which any KPI can easily be computed.. It provides uncertainty estimates for free. It is a clear and principled approach, no black box required. Parts of the model can easily be reused. For example, we created a variation of the model that differentiates contracts by the reason they are ending. It allowed us to predict changes to the contract length under different churn scenarios. *to read more about MCMC, a technique to get samples from the posterior distribution, a good starting point is: http://elevanth.org/blog/2017/11/28/build-a-better-markov-chain/ If you want to know more about it’s like work at Jobandtalent you can read the first impressions of some of our teammates in this blog post or visit our twitter . Jobandtalent Engineering team blog. 32 Thanks to Michele Trevisiol , Sergio Espeja , and John McLachlan . Machine Learning Pymc3 Survival Analysis Probabilistic Programming Data Science 32 claps 32 Written by Data Scientist @Jobandtalent Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Written by Data Scientist @Jobandtalent Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-01-09"},
{"website": "JobandTalent", "title": "statically typed json payload in swift", "author": ["Luis Recuenco"], "link": "https://jobandtalent.engineering/statically-typed-json-payload-in-swift-bd193a9e8cf2", "abstract": "Data Science Backend Product Mobile Platform Frontend We are hiring! Increasing signal-to-noise ratio via types Quite a lot has already been said about creating properly typed API clients on top of URLSession . In fact, two of my favorite blog posts about the topic are: Writing a Scalable API Client in Swift 5 by my good friend Víctor Pimentel . Modern Networking in Swift 5 with URLSession, Combine and Codable by Vadim Bulavin , whose blog has really high-quality content about Swift and iOS. Unlike Android, where Retrofit seems like the standard way to define API clients (establishing very specific ways to handle the structure of them), most iOS developers end up creating our own abstraction on top of URLSession . Even if Combine provides a better API on top of URLSession (fixing the wrong (Data?, URLSession?, Error?) tuple…), we still have to figure out how to model requests, headers, request bodies, query params, etc… Every Friday, the iOS team at Jobandtalent gathers together in a weekly meeting, where we usually discuss different topics about the project or iOS/Swift related in general. This time, one of the topics was analyzing the current implementation of our APIClient abstraction (which was created almost five years ago and has aged quite well, to be honest) and how we could improve it. Then, we opened a playground. This blog post (which doesn’t intend by any means to be a full-featured API client implementation in Swift) describes the next two hours after we opened that playground… It all started by taking a look at our current model for HTTPRequest . Some of the problems were: The parameters were reused both for GET and POST requests. For GET requests, parameters were encoded as query params, whereas for POST requests, they were encoded as a JSON payload inside the HTTP request body. Parameters for GET and POST requests have slightly different requirements, so it doesn’t really make sense to share those. The parameters’ type [String: Any] allowed putting any type as value, even if it doesn’t make sense and cannot be transformed properly to a JSON payload (in case of a POST request), leading to runtime issues. Nitpick detail, but the ergonomics of using a protocol to model our requests is slightly worse than using a struct and have the request values created via static let methods or properties. That way, we could leverage the addicted dot syntax ( .|) , whenever a HTTPRequest is needed, to list all the available requests. Even if Swift allows creating computed static vars or functions by extending a protocol, you’ll bump into the following error when using them Static member ‘xxx’ cannot be used on protocol metatype ‘HTTPRequest.Protocol’ . We went step by step, trying to solve the first and third problems first. A request could be a simple value, encapsulating the path, method, and the way the data is parsed. Most of the times we want the Data to be parsed to a Decodable model… … but sometimes, it can be useful to skip altogether the information sent back from the server (via the no-op closure). As stated before, both GET and POST request parameters have different invariants, and the way to maintain those invariants was to properly define different type definitions that made sense for each case. For GET requests, [String: CustomStringConvertible] seemed like a proper, simple type for abstracting query parameters that will later be transformed to a URLQueryItem type. We could easily declare a GET request by extending the Request type and have static methods or properties inside. When trying to use the APIClient to send a request, we could just type a dot whenever a Request type is needed ( apiClient.send(request: .|) ), having Xcode show the proper completions, which makes it a joy to use and improves the ergonomics of the API over the previously HTTPRequest protocol-based solution. But as you can see, we still had the second problem we mentioned before. The payload type for POST requests was [String: Any] , which could make the JSONSerialization.data(withJSONObject: payload) code crash if we were not careful enough about the types we put there. We thought we could leverage the Swift type system to do better, and have the compiler help us provide correct data that would avoid runtime issues. We won’t go into much detail about the actual APIClient class. It just wrapped a base URL and had a method that accepts Request<Output> types, transform them into URLRequest , and notified via a callback (Request<Output>, (Result<Output, Error>) -> Void) -> Void . Using the Encodable and JSONEncoder types seemed like the most idiomatic way to transform types into JSON Data . Unfortunately, this code didn’t compile, and for very good reasons. The line JSONEncoder().encode(payload) resulted in the following error. Value of protocol type ‘Encodable’ cannot conform to ‘Encodable’; only struct/enum/class types can conform to protocols. The problem was that, in order for the compiler to know how to properly convert a type to Data , it needs to know the exact nominal type, as the implementation of func encode(to encoder: Encoder) throws varies depending on the specific type. And how could we let the compiler know about the specific type? Of course, generics… The introduction of the generic spread all around, making us modify all the API client, from the send(request:) method, up to the Request type. An interesting option to avoid the impact of adding this new generic all around could have been to use type erasure and create a AnyEncodable type. Once we had all the infrastructure in place, we could create requests like this: As you can see, the first problem that we came across was the Payload type for GET requests… The first thing that might come to mind is to put Void there, but… type ‘Void’ does not conform to protocol ‘Encodable’ Unfortunately, we couldn’t extend Void to conform to Encodable , as Void is a non-nominal type (it’s an empty tuple in fact). So, if Void is not an option… was there any other type that could make sense and satisfy the type system for GET requests, where the Payload type doesn’t really make much sense? Luckily for us, yes. Never is what’s called an uninhabited type, which is a fancy word for a very simple thing, a type that cannot be instantiated and cannot hold any values. Different languages implement it differently. In Swift, it’s a simple enum with no cases. In Kotlin, it’s just a class with a private constructor. The main goal is the same, it’s a type that can satisfy the compiler and convey proper semantics for things that can never happen. For instance, AnyPublisher<Output, Never> means that the publisher will never fail. Whereas Never automatically implements the Error protocol, it doesn’t implement Encodable . But it’s as easy as this… With that very simple conformance, we could declare our GET request without problems. In fact, if you think about it, Never makes a lot of sense, as the code path that has to do with payloads ( case post(_ payload: Payload) ) is something that never gets executed for a GET request. We were pretty happy with the design so far. In fact, we could say that this was the most idiomatic and Swifty way we found. Two minor issues made us look for an alternative solution though. We needed to create intermediate structures for most of the payloads, whereas we could simply inline simple dictionaries before. In order to know what the actual JSON keys are sent to the server, you need to know how Encodable works. The keys might be the very same property names, or maybe they are the ones in the CodingKey enum… Or maybe they are different, depending on the keyEncodingStrategy property of JSONEncoder … Who knows ¯\\_(ツ)_/¯ This made our journey a little bit longer, but it was worth it in the end. A JSON, as defined in the ECMA-404 standard , can be: A number A boolean A string Null An array of those things A dictionary of those things Only by taking a look at that definition, our intuition made us know that we were dealing with a recursive data structure. Taking into account that the possibilities were well-known and there was no point in allowing extension, a recursive sum type seemed like the most convenient solution. To make it work for us, we needed to have it conform to Encodable . We could then change our HTTP method, removing the Payload generic type, and have the aforementioned JSON structure as the payload of our POST request. We were able to inline dictionaries in our payloads again, making it very clear the keys that were sent to the server. Unfortunately, the ergonomics did worsen quite a bit, having to wrap everything in the JSON case constructors. In fact, the example above doesn’t reflect how bad the ergonomics were, especially when dealing with nested arrays and dictionaries. Fortunately, Swift provides two really great features that we could use to fix most of those problems. ExpressibleBy protocols Custom operators Having the JSON type conform to the ExpressibleBy protocols was quite simple. With those conformances in place, we could then have any kind of literals in the POST payload. It worked similar to the initial [String: Any] payload type, but with the additional type-safety. Most of the time though, we won’t be dealing with literal values. So, even if we could have array and dictionary literals, which removed a lot of the noise already, we still had to wrap strings, numbers, and boolean values into those case constructors. This was where the custom operator came into play. Custom operators are a widely debated topic, not only in the Swift community but in programming in general. Some people love them, others hate them. I think that, given the right scoped context for them, they can make a great addition to your codebase, simplifying some tedious and chore tasks, providing useful syntax sugar that’s a joy to use. So finally, having the ExpressibleBy conformances and the custom operator in place, we could have something like this. As you can see, we dramatically reduced the noise that we had in the first version, taking advantage of the type system to provide the semantics and the safety that we were aiming for. I still remember, quite a long time ago already, when I studied Telecommunications Engineering at university. One of the recurring topics was the Signal-to-noise ratio (SNR) for linear time-invariant systems. I also like to apply the same SNR concept for programming, especially when dealing with types. We can think of the Signal as the type-safety, the conciseness, the correct semantics , whereas the noise would be the price to pay to have that, in terms of cumbersome syntax , workarounds, superfluous types, or boilerplate you have to create. API design is all about trade-offs. It’s all about maximizing SNR via proper types and abstractions. SNR is sometimes, unfortunately, quite subjective as well. For us at Jobandtalent, this playground, and the two hours we spent on it, gave us the best SNR we could have in an API client abstraction for Swift. But that’s, obviously, only our opinion. If you want to know more about how it works at Jobandtalent, you can read the first impressions of some of our teammates on this blog post or visit our Twitter . Jobandtalent Engineering team blog. 320 1 Thanks to jobandtalent Engineering . iOS Networking Swift Mobile App Development Mobile 320 claps 320 1 Written by iOS Team Lead at @jobandtalent_es. Former iOS at @plex and @tuenti. Creator of @ishowsapp. Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Written by iOS Team Lead at @jobandtalent_es. Former iOS at @plex and @tuenti. Creator of @ishowsapp. Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2021-03-04"},
{"website": "JobandTalent", "title": "10 years of the jobandtalent platform", "author": ["Sergio Espeja"], "link": "https://jobandtalent.engineering/10-years-of-the-jobandtalent-platform-6e6ae111735", "abstract": "Data Science Backend Product Mobile Platform Frontend We are hiring! Jobandtalent’s platform has changed a lot since its inception back in 2009. The search for the perfect product-market fit has triggered a lot of changes in the architecture of the platform, from various kinds of Monolithic applications to a Service Oriented Architecture (SOA) approach handling a huge amount of traffic and finally an SOA strategy that currently helps us to model complex business models. In this blog post, we will go through our current platform and technologies to explain its product components, its architecture, and how we operate it. Jobandtalent digitalizes the whole recruiting process, from a job vacancy to paying employees salaries and charging employer invoices. All this with a strong focus on optimizing companies’ workforces and workers’ work experiences. As a consequence, Jobandtalent has three different kinds of clients, employees, employers and internal staff; each one has access to different tools: Employees : Landings and Mobile App. Employers : Landings and Workforce Web app and API. Internal Staff : Admins, Farming Tool and Scheduler Web apps. The employee's mobile app is available for both Android and iOS platforms. The iOS app, implemented in Swift, abandoned the standard Model View Controller approach looking for a more novel, robust, yet easy to use unidirectional data flow architecture based on State Containers. A State Container is just a stateful piece of domain or view model that can be mutated via commands and observed from views that are re-rendered when the state changes and a new snapshot is received. The overall idea comes from other well-known architectures, such as Flux or Redux, but cleverly applied to the iOS world, without it feeling alienated. You can read more about it here: iOS Architecture: A State Container based approach . The Android app adopted Kotlin as the main programming language over two years ago, migrating many of the features to take advantage of its power. The structure of the app is based on Clean Architecture, which has allowed us to decouple the components (app, data, domain or UI) allowing much more robust unit and integration tests, using Espresso and Screenshot testing. It also allows us to work with a larger team of developers in parallel on the same functionality, using Jenkins, Codeship, and AWS to release more and more frequently. One of the fundamental pieces needed to build UI, of course, is still our Design System developed thanks to the close collaboration between developers and designers. The mobile apps consume a private API served by an Elixir service that acts as a Backend for Frontend (BFF) . This BFF handles all mobile apps requests and queries internal services APIs to compose a JSON result consumed by those mobile apps. The mobile private API and all internal APIs are documented using JSON schemas to ensure compatibility and speed up development. jobandtalent.engineering The Landings web application is developed using the Ruby on Rails framework and serves the Jobandtalent home page, information pages and forms for gathering information about potential employers or employees. Workforce is a web application that also exposes an API consumed by several employer clients. The main goal of Workforce is to act as a dashboard for the companies to browse current employee information, hiring documentation, lots of analytics and submit some requests to the Jobandtalent staff. This application is developed with Ruby on Rails framework and React to handle part of the interactivity. The Workforce API is specified using OpenAPI 3.0. The first phase of employee recruitment is done by our internal staff with the Farming Tool. This tool optimizes a lot of processes: lead management, vacancy requests, documentation handling, offer proposals, etc. The Farming Tool is developed with the Ruby on Rails framework and React to handle part of the interactivity. In addition to the Farming tool, our internal staff have access to legacy admins (Candidates and Companies) to manage employees, employers, and hirings. These legacy services are being migrated to new tools and being replaced using the Strangler Pattern . Finally, the last service used by Jobandtalent internal staff is the Scheduler, a web application that manages shifts, creating and optimizing them from submitted demand. This application is developed with the Ruby on Rails framework and React to handle part of the interactivity. Jobandtalent backend is composed of small interoperable services. Our Service Oriented Architecture leads us to encapsulate features in specialized services. Right now we have more than 25 different services in production. Internally there are plenty more services mainly written in Ruby on Rails. With the exception of a geocoding service that is developed in Go and several services developed in Scala by the Data Science team who bring Machine Learning and AI to the platform. Communication between services is done with a service bus for asynchronous communication aside from HTTP API based synchronous communication. Regarding data manipulation and processing we use ETLs developed in Python. Those ETLs gather data from our main Redshift Data Warehouse and enrich all our dashboards with useful data. The main programming languages and frameworks used are: Javascript & React HTML5 & Less CSS Kotlin Swift Ruby on Rails and Ruby Phoenix and Elixir Golang Python Scala Ansible Terraform Docker In Jobandtalent we try to use the best tool to tackle each problem, as we deal with very different problems we use very different tools. We don’t add every new hyped language/framework/tool into our platform but we’re continuously researching, prototyping and trying new and interesting tools to add to our toolbelt when they are production-ready and can really improve our delivered solutions. jobandtalent.engineering We operate in a flexible cloud solution with Amazon Web Services (AWS), with multiple availability zones, data replicas across several cloud providers, executing containerized and secured application services. There are 3 environments in the infrastructure. Production, staging (pre-production) and development. Production has all the necessary resources to run all services and their dependencies for their users. It is protected using high-security industry standards that will be mentioned later. Staging is an exact replication of production, downgrading the capacity of the resources and lowering the number of replication items, as it does not need to run at as large a volume in production. The development environment is composed of all software services defined within a tool we created on top of Docker Compose. This last environment can be executed locally or instantiated in a cloud server. We can run as many of them as needed, and it is common that every developer has their own. Services are run inside the Docker container, generated by a Continuous Integration (CI) service, Codeship. Also, the CI in every new image runs all available code tests to make sure the quality is maintained or increased in every modification to the code. We use Amazon Elastic Container Service (ECS) as a container orchestrator. It allows us to roll out new versions of the code without downtime, coordinating traffic in Load Balancers to start using new instances of the new Docker images of the code and removing old ones in a smooth and controlled way. Each service has its set of dependencies, database, cache, search, message bus, etc. we rely on the Infrastructure as Code (IaC) methodology to match every change to the platform as a change in a code repository. Therefore every service is defined, along with its dependencies, as a playbook that is applied when either it is built or modified. Jobandtalent is based on the Postgresql as the database engine, using the Amazon Relational Database Service (RDS) as a cloud-based database service. Other service dependencies based on Amazon are Memcache, a simple key-value in-memory mapping service, and Redis an in-memory data structure store, which are both provided by Amazon Elasticache. Service dependencies also include: Elasticsearch, as a document search engine, to improve the efficiency of text-based searches for services. RabbitMQ as message broker provider. Cassandra as a column oriented NoSQL database. Those dependencies are set up in a cluster of Amazon Elastic Compute Cloud (EC2) services. For security mechanisms, Jobandtalent uses Hashicorp Vault as the main secret storage system, as well as Amazon KMS and Amazon SSM Parameter Store for some service dependencies. Those secrets are injected into the running environment on deployment time from an administrator in case of Vault, or in runtime by the service itself in case of Amazon KMS/SSM. In the frontend, Jobandtalent uses Amazon CloudFront as a CDN, to improve user experience but also to mitigate risks of DoS attacks. There is a web application firewall as a security mechanism, provided by Amazon WAF, that filters potentially risky requests that are suspicious and could contain common attacks. In order to have all related data be consumed by the DataScience or Operations teams, Jobandtalent uses Amazon Redshift, a service ready to handle huge analytics needs, as data-warehouse, and Amazon Database Migration Service (DMS) in continuous replication mode, to connect Redshift with related data sources in almost real-time. Also for data analytics for several services, Amazon Kinesis Firehose is used to inject data from different sources into Redshift tables. In order to notify the candidate’s mobile application, Jobandtalent is using Amazon SNS as a push notification broker for both, Google Firebase and Apple Notification Service. As a scheduled task service we use Dkron , a distributable cron-like service built and open-sourced by @victorcoder who works as a Senior Engineer on the Jobandtalent Platform team. There is no magic platform architecture that meets all business needs. Startup business models evolve fast and pivot, so their systems architecture should change according to those changes in the company. The typical evolution of a platform is from a Monolithic approach that reduces the complexity of the platform, to an SOA architecture that facilitates scaling both team and traffic. A key strategy to be able to change and pivot your system architecture is to script and automate as much as you can. Invest in Infrastructure as Code as early as you can and be ready to change it when it better fits the stage the company is at. For Jobandtalent this strategy has been key to tackling its technical challenges. Do not add every new language/framework/tool that becomes available to your platform, but don’t be closed to research and innovation. Invest time in trying everything that could improve your platform and apply it when it becomes worthwhile. In Jobandtalent we are very curious and try out a lot of technologies in order to understand them and incorporate them when they clearly beat our current ones. If you want to know more about it’s like work at Jobandtalent you can read the first impressions of some of our teammates in this blog post or visit our twitter . jobandtalent.engineering Jobandtalent Engineering team blog. 100 Thanks to David González . Engineering Development Platform Startup Backend 100 claps 100 Written by CTO at jobandtalent Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Written by CTO at jobandtalent Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2021-05-12"},
{"website": "JobandTalent", "title": "command pattern how and why we use it", "author": ["Manuel González Merino"], "link": "https://jobandtalent.engineering/command-pattern-how-and-why-we-use-it-fa8af952bca1", "abstract": "Data Science Backend Product Mobile Platform Frontend We are hiring! Here at Jobandtalent, we face numerous challenges every day. One of them is to make it easier for people to use our software, and covering us on worst case scenarios. Our internal ops people work with large amounts of data, scheduling rosters for hundreds of employees, distributed along with a handful of locations. Some of these operations come along with some other automated actions, like sending emails and changing the status of other actors. The UI that allows us to prepare and trigger these events has to be good enough to avoid unintentional mistakes, and we have put a lot of effort into achieving this. When we are building features that involve direct interaction with the user interface, we design the feature keeping things like this in mind: Do we need to allow undo/redo operations? Is it a batch operation? Do we need to keep track of who triggered the operation? Do we need to keep track of the current and past status of the operation? Do we need to track internal data of the operation? Does the operation need to be transactional ? These kinds of problems can be approached with the command pattern. In short, the command pattern enforces the design encapsulation of a request or an action as an object, so you can parameterize all the data for every request, queue it, log it, and support un/redoable operations. The most simple analogy is to think of it as a text note application, where you are allowed to undo and redo using the history of the document. Actually, almost every modern user interface that involves some kind of editing operation, supports this. Let’s think about a more specific use case. Imagine that you want to schedule a collection of shifts, distributed in a week, and match a collection of workers to those shifts. Plus, you want to optimize this match adding constraints and worker preferences (this example will support the following definitions). To do that, we could use an external service to do the calculations. Sending it a payload of workers, shifts, locations, and constraints. The service would respond to us with a payload of matches between workers and shifts. Ideally, we want to keep track of: Who triggered the event Whether it was a single or batch operation Whether it was scheduled or executed at that moment The data payload we sent to the service The data payload we received from the service Other internal props that allow us to redo/undo the operation If the operation should be idempotent Given that we work with considerable amounts of data, we prefer to schedule the action, tear up the process and log it more thoroughly. We also need to keep the operation transactional so, in case of some failure arises, we are able to rollback database commits. What we design when creating a new feature is based on two concepts: an action to implement behaviour and a runner to decide the strategy when running the action. jobandtalent.engineering Take a look at this class implementation: The example above is built on top of ActiveRecord::Base superclass to handle the transaction work. In turn, the next example will implement the former class as base to model a customized action, in which we can add, in the form of a JSONB field, the data we want to log: With this example, we have designed an action that could prepare a payload, then send it to an external service, and finally save the results. Plus, we could save some interesting props about the status and data of the action while it is being performed. The method process_data called in the example above, could, for example, hold three private method calls, one would handle the logic to build a payload, another one would send it to the external service, and the last one would persist the results. Some issues could come up here. Given that we depend on an external service and the action could be scheduled to perform later on —some data could change while the action is being completed. To keep track of what could go wrong, we define the following states to properly mark the action in specific scenarios: Waiting to run , init state. Running , the action is now performing. The runner set this state. Succeeded : everything went well, i.e., the data payload could be formed, the service had no problems with the data we sent and then responded with a result that we saved successfully in the database. Failed : some business rule or constraint didn’t allow us to perform the action. For example, there could be changes to the availability of the workers, deleted shifts or some other scenarios where the data we need to send to the other service is malformed or not available in that specific moment. Crashed : a tech failure. Connectivity issues, response times longer than expected, etc. Those states are managed in part by the action and in part by the runner. Specifically, in case the action has crashed, it is desirable to re-schedule it to try again later, so the runner will manage the status and will ask for more retries. On the other hand, succeeded and failed states are managed by the action. Now it’s time to think about performing our action. Do we want to execute it at the very moment? Or do we want to wait to some point in the future? Both approaches are supported thanks to an implementation that allows us to: Launch a Runner that executes the action inline (same thread) Run the given action in the background using Sidekiq to manage the queues. The base class to achieve this could be wrapped like this: Now, following this next implementation using the former as a base class, the inline runner simply runs the action as an inline method: Actions::Runner.run self In some slightly different fashion, the async runner works with Sidekiq retries and only crashes after all the retries have expired. We can pass as well the action method we want to run. The Worker implementation that handles the async work under the hood is the following (some details have been omitted for the sake of brevity): On top of the previous features, we finally would want a good audit trail about what happened when performing the action. To be more precise: Account , as a scope Initiator : the user/component that started the action Target : the thing that we are acting upon Timestamps Action-specific configuration/data payload As all of the properties, except the last one, are general enough to be applied in business rules, we can take advantage of a JSONB field. This helps us to create a specific data structure to hold other kinds of information we want to track when designing new actions. All this data could be nicely formatted and available in a backoffice for devs as well as for ops to check their work, and do some troubleshooting in case of errors. We can also undo a series of actions and redo them exactly to where they were before with relative ease, thanks to the detailed logging data we have. The point behind this approach is to be able to treat operations that come along with big amounts of data thoroughly, especially for those non-friendly scenarios where things went wrong. The states will show us where the action failed and hopefully, why. Logging all the data involved in the operation brings us the possibility of undoing mistakes, and redoing them just feeding input into the action. Also, avoiding wrongly re-doing several batches of hundreds of actions gives peace of mind to our platform engineers. Sometimes, there is a set of obscure business rules that only operations people know, or at least, know the why more deeply than developers. Sharing error responses to them through the UI, conveniently translated into their jargon, usually results in spotting the error quicker. Finally, being in possession of the payload which the action sent to external/third party services, is tremendously helpful when debugging issues between both ends. Obviously, this approach is not always valid or the silver bullet for all of our problems. As always, we must first think in the internals of our intention to choose the best pattern, and if you read the article, now you will have a basic idea of what kind of problems could be solved with this implementation. Looking forward, we want to include the following ideas in future iterations of development: An external workflow/actions engine Reusability between services Orchestration of complex flows (multiple services, human interaction, per client workflows…) Acknowledgments: special thanks to Sergio Espeja , German Del Zotto , Luis Recuenco and John McLachlan for his valuable feedback while writing this article. If you want to know more about it’s like work at Jobandtalent you can read the first impressions of some of our teammates in this blog post or visit our twitter . jobandtalent.engineering Jobandtalent Engineering team blog. 162 Thanks to Luis Recuenco and Sergio Espeja . Software Development Backend Development Ruby Ruby on Rails 162 claps 162 Written by Backend Engineer at Jobandtalent Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Written by Backend Engineer at Jobandtalent Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-09-25"},
{"website": "JobandTalent", "title": "hack talent 2019 jobandtalent engineering hackathon", "author": ["Sergio Espeja"], "link": "https://jobandtalent.engineering/hack-talent-2019-jobandtalent-engineering-hackathon-2489e978e011", "abstract": "Data Science Backend Product Mobile Platform Frontend We are hiring! Following the success of last year’s Hackathon , the Jobandtalent Product Team gathers together to try to create even better projects than last year and above all have fun! The experience was amazing; high-quality projects, cool bleeding edge technologies, imaginative solutions to difficult problems, hardware & software development and a lot of learning! A seamless way to clock-in workers using beacons to detect when they arrive at the office. Clocking in has historically been a manual process and usually, there was no real-time way of knowing who is working or not at a specific time. This information could be crucial for a company in order to take measures if there is a peak in their business. In these situations there can be not enough people to cover the workload and it is very difficult to track when temporary employees are working and pay them accordingly. We wanted to try to come up with a seamless way of clocking in by completely removing the need to clock in and giving employers a real-time way of knowing who is working or not. We used Estimote devices as their development kit was pretty good for our needs, we also added a feature to our iOS application to give users a way to follow the clock in / clock out events. We created an iOS iPad app where the employers can see which of their workers were at the office, when they arrived and which of them weren’t at the office yet. The backend was developed in Elixir. jobandtalent.engineering A set of tools to make the reception of candidates to our office as painless as possible. Our office is visited by hundreds of candidates each day for several reasons. The main one is attending group interviews for one of our job processes. Dealing with this flow of people is something that takes up a lot of time and makes the experience for our candidates painful. Also, candidates who come to the office have different levels of “ready”. There are people without a Jobandtalent account, people without the application installed, or people that have everything ready. The Totemrracho project tried to make the experience fast and “magic” and get all our candidates in the “all set” state. The project implemented two solutions: A totem and a new login system. The first solution was software for a “Totem”, like the ones you can find in a McDonalds, which prepare and inform the users about what they should do and where they should go. It is connected to our systems to create an account if needed, send an SMS to recommend downloading the app or indicate where their group interview is. The second one was motivated by the fact that we found the App login process very annoying, user and password are not as fast as we need so we changed the login system to accept Login by SMS. Enter the app, introduce your phone, wait until receiving the code confirmation and automatically introduce it and log in. Also, it checks if the user has a group interview and open the screen with all the information and some recommendations to be ready for it. Interaction tracking system able to handle all interface actions across Jobandtalent platforms (mobile apps, web applications, system events). The system followed the Snowplow Analytics approach to collect, enrich and store the events data in a canonical format that was easy to visualize and extract metrics. Elixir was the language chosen to implement the proxy service. This proxy captures users actions and exposes them to the Collector via an internal endpoint, only accessible from inside Jobandtalent’s platform. The Collector reads the data from the Tracker and forwards it to the Enricher, which formats the data in a legible format. Then, the events are stored in a database, ready to be analyzed by a Metrics / Monitorization system. In this way, we can cross and aggregate events data from different inputs, helping us to understand our users’ behavior and provide them with a better experience. The Parrot project was based on two pillars: investigating the possibilities of Dialog Flow to convert the natural language by which our users request information and exploring the different media formats which we can use to offer help, beyond the app or the website. First of all, thanks to Dialog Flow we were able to build an interaction of common flows such as: asking for an interview, answering specific real questions about our hiring process or requesting information about a particular position. One of the things that helped us integrate with Jobandtalent’s platform was the versatility of webhooks and the ability to implement our own fulfilment to create the actions. We used all of the natural language recognition through the ML of Dialog Flow, which allowed us to create models of answers to common questions that our users usually have. A key element of integration, which allowed us to carry out tests during the hackathon was Google Assistant, available on Android and iOS. Despite not having installed the Jobandtalent app we could interact with users and resolve certain issues to facilitate them joining Jobandtalent. We also explored the integration through Twilio of Telegram and WhatsApp, which opened up our platform to other messaging tools to facilitate communication with our users. Data-driven virtual assistants that provide a personalized experience to our users The Mantastic project was based on implementing processes driven by virtual hosts with a fully conversational interface in rich 3D environments. As it is powered by data, the host can guide you thoroughly through the interview processes based on your preferences, skills or market demands, answering questions about any specific job positions. This project relied on Amazon Sumerian and Amazon Lex (the technology that powers Amazon Alexa) to implement hosts that the users can interact within a 3D environment and conversational interfaces through speech recognition and natural language understanding respectively. A substantial part of the Jobandtalent’s infrastructure is based on AWS, so relying on products within this same ecosystem helped us to integrate the Mantastic processes with our other systems. This first iteration felt like we barely scratched the surface of these technologies as there are many other features that would be very interesting to explore, such as AI, face and voice recognition, and localization options based on user preferences. A centralized authorization system for our users on the Jobandtalent platform. This new system, known as Authtonio, is based on Auth0’s authentication and authorization features. Previously, each of our services had to develop its own user roles /permissions mechanism, something that could be very time-consuming. Authtonio aimed to avoid wasting time developing the same logic several times. With Authtonio our systems only have to query this system to retrieve the user role and permissions. Our platform also benefited from this approach as it had all the users permissions in one place, allowing this critical information to be managed very efficiently. Authtonio’s management interface was developed using React , while in the Backend side we developed a ruby library to be used within the RubyOnRails framework. Real-time location tracking and driving performance analysis for vehicle fleets. There are lots of fleet management solutions on the market, but generally speaking, they store your data on their system and don’t always offer real-time analysis or a proper API to retrieve data. If you are going to manage fleets of vehicles, it becomes crucial to have a platform to track vehicle location and driving performance. This allows you to optimize routes and vehicle maintenance, properly analyzing this data could make major a improvement to revenue. The system was implemented using a device based on Raspberry Pi and an optional OBDII interface that is installed in vehicles, providing a small and efficient solution. This device sends data directly to our systems where we crunch the numbers and prepare the data for visualization. For GPS tracking we used a custom-built python agent in the device and the Traccar server. For driving data ingestion we used a combination of AWS Firehose and IoT data pipelines. For the real-time dashboards we used Superset. Thanks to Marcos Novo , Marcos Trujillo , Gonzalo Gómez , Txema , David Anguita , Alex Martin and Victor Castell for each section of this post. If you want to know more about it’s like work at Jobandtalent you can read the first impressions of some of our teammates in this blog post or visit our twitter . Jobandtalent Engineering team blog. 90 Thanks to jobandtalent Engineering . Hackathons Engineering Product Programming Software Development 90 claps 90 Written by CTO at jobandtalent Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Written by CTO at jobandtalent Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-09-25"},
{"website": "JobandTalent", "title": "the power of mixins in swift", "author": ["Luis Recuenco"], "link": "https://jobandtalent.engineering/the-power-of-mixins-in-swift-f9013254c503", "abstract": "Data Science Backend Product Mobile Platform Frontend We are hiring! As expected from any new modern language, Swift 1.0 launched with the concept of protocols . Protocols serve as a way to define a blueprint of methods, properties, and contracts that other types must fulfill. Some key benefits we get from using protocols are: The usage of subtype/inclusion polymorphism. Code change tolerance. Decoupling from I/O or other volatile dependencies like network or persistence. Nothing new though. We were already used to all of this from our old Objective-C days. Swift 2.0 changed everything. The fact that protocols might now have default implementations opened a whole new world of possibilities. We started bundling pieces of functionality in standalone protocols whose default implementations defined the appropriate behavior. Then, objects only needed to adopt the protocol to acquire all the appropriate behavior with no extra work from their side. This is what we usually call a trait . One of the main benefits of traits is that they avoid the need for inheritance that it’s usually abused in order to acquire some piece of functionality. Also, they allow to mimic multiple inheritance capabilities, but as always, we got into problems when we need to deal with… state. Mixins are a combination of traits and state . We could say that a mixin is a trait with superpowers. And… with great power comes great responsibility. In Swift, mixins are not allowed by design, as protocol extensions cannot contain stored properties, but only computed ones. This is quite a big limitation for some specific behaviors. Forcing us to handle some piece of state in the different classes conforming the protocol makes the solution quite cumbersome and more verbose that it could be by using pure mixins in the first place. Not all is lost, however. We are going to bring back to life kind of an obscure old friend from our Objective-C days: Associated Objects. Associated objects are a feature in the Objective-C runtime that allows us to add custom properties to existing Swift classes. In the right hands, this is a powerful handy way of adding state, not only to our own classes, but more interestingly, to external ones. As always, in the wrong hands, we could make everything explode. Associated Objects date back to the Objective-C 2.0 runtime, introduced alongside OSX Snow Leopard in 2009. We can leverage all its power by using three simple C methods: Even if simple, the API is far from being Swift-friendly. We want mixins to be as easy to use and create as possible, so it’s a good idea to wrap this old C API in a more modern and safer Swift one. We can even leverage extensions to automatically make all NSObject subclasses have the mixin capabilities automatically: Now that we have a solid foundation in place, it’s time to show some specific use cases where we have benefited from the power of mixins here at Jobandtalent. jobandtalent.engineering Imagine that we have several views that represent a Job in Jobandtalent’s domain. Now imagine that we want to allow power users to apply to any of those jobs via a long press gesture. One way to do it is by having those views create and save the gesture recognizer needed to handle long press capabilities. The other… you guessed it… mixins! Now, we can add the long press behavior to apply as follows: If you read any of the previous articles we wrote about iOS architecture, you know that we use a reactive architecture where views subscribe to a state-holder object called Store , waiting for state snapshots to be sent to them. In this context, we use a RAII pattern where views are given a token that they must keep alive as long as they want to keep receiving state updates. It looks like this: Forcing all views hold that token was something we wanted to avoid. How about having a simple mixin that saves a piece of state with the different tokens from the different stores we might be subscribed to? That’s exactly what the following code does: Finally, making our views conform to the mixin, we can subscribe to the store knowing that we are going to be notified about state changes in the store as long as we are alive, without having to save any extra piece of state whatsoever. Most of our views show asynchronous data coming from our API. This means: We should show feedback while that info is being retrieved. We should show feedback about any problems that may arise. The Algebraic Data Type (ADT) used to feed most of these kinds of views is the sum type LoadingState . The logic behind showing or hiding the loading and error views is as follows: loading(data == nil) : show loading view and hide error view. loading(data != nil) : show current data alongside loading indicator. loaded : hide both loading and error views. error(_, data == nil) : show error view and hide loading view. error(_, data != nil) : hide both loading and error views. Show error alert if needed. This is quite an important logic that we were spreading all around different views, even if that logic is quite trivial from a view’s perspective via some extensions on LoadingState : This kind of method extensions on sum types are quite handy, encapsulating the destructuring logic over our data, minimizing the impact that future refactors might have in our code. The usage was quite simple, yet cumbersome to handle in every single view. Once more, we can make use of mixins to handle both the logic and the loading and error child controllers hierarchy automatically for us. Let’s first define what our reactive views look like. We call them Stateful Views : As you can see, they are basically views that can be rendered with an Equatable piece of state. With that in place, we can define LoadingStatefulView, a StatefulView that knows how to render a LoadingState ADT. It will take care of showing and hiding the error and loading views on our behalf, letting us know when we have to render the actual data. Thus, the tricky part, as you already know by now, is handled in the protocol extension. As you can see, we attach a loading and error child view controller to our stateful view controller via associated objects. Then, we simply show and hide them by pattern matching on the state. Simple and powerful. The final step is to make our view controller conform to LoadingStatefulView and let the mixin do the magic under the hood for us. Thanks a lot for reading. We have seen three examples where we have managed to leverage the power of mixins to greatly simplify our code. Mixins are another powerful technique that we have at our disposal. As it always happens when we learn any new tool or design pattern, we tend to overuse it. We first need to learn when it’s the right solution to our problem because… when you’re a hammer, everything looks like a nail . As everything that deals with state, mixins are quite risky and can make our designs fragile if used the wrong way. Use them wisely. They work like magic, but magic is not free. Because, as I previously said… With great power comes great responsibility . If you want to know more about it’s like work at Jobandtalent you can read the first impressions of some of our teammates in this blog post or visit our twitter . Jobandtalent Engineering team blog. 116 1 Thanks to Victor Baro and Rubén Méndez . Swift iOS Mobile Programming Development 116 claps 116 1 Written by iOS Team Lead at @jobandtalent_es. Former iOS at @plex and @tuenti. Creator of @ishowsapp. Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Written by iOS Team Lead at @jobandtalent_es. Former iOS at @plex and @tuenti. Creator of @ishowsapp. Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-09-25"},
{"website": "JobandTalent", "title": "speeding up superset by choosing the right database", "author": ["Javier Fortea"], "link": "https://jobandtalent.engineering/speeding-up-superset-by-choosing-the-right-database-d85283d39f75", "abstract": "Data Science Backend Product Mobile Platform Frontend We are hiring! Data is one of the key ingredients in the success and development of any organization, and here at Jobandtalent, we take this really seriously. For this reason, we gather and analyze lots of data in order to extract insights and trends, and ultimately to help in the internal decision making. In order to be able to do that, raw data needs to be transformed into information and be contextualized . For that, we have different ETL pipelines (ETL stands for “Extract, transform, load” ), that are in charge of selecting the required data from different data sources (in our case, most of the data comes from our Redshift cluster), processing and aggregating it and finally loading the resulting data to a database so it can be consumed for further analysis. Besides data processing, visualizations are an essential tool to present and make sense of processed data. They are usually presented in the form of dashboards that group different kinds of charts, putting data in context, identifying trends and correlations, etc. In this post, we’ll talk about the tool that we use for our visualizations, Superset , focusing on some performance challenges that we found, and how we solved them. Apache Superset is a data exploration and visualization web application originally developed by Airbnb. It’s currently “an effort undergoing incubation at The Apache Software Foundation (ASF)”, meaning that the project has yet to be fully endorsed by the ASF, but the project is mature in terms of stability and feature richness. It’s a modern business intelligence tool, using some well-known Python libraries and frameworks (Flask, SQLAlchemy, Gunicorn…). We use it as our main visualization and dashboards tool at Jobandtalent because of its ease of use and its support for the databases and the data flow that we need. Creating your first dashboard in Superset is just a matter of setting up the database connection to your data sources and it will automatically identify the tables there, together with the table columns, creating even some predefined metrics (e.g. average, sum… metrics will be automatically created for the numeric columns). You can later create your own table metrics that can be re-used in your charts. Superset is really flexible allowing you to create independent charts that can be later added to one or more dashboards. You can choose from a large variety of types of charts (time series, heat-maps, Sankey diagrams, box-plots, etc. — you can get an idea of the diversity checking the partial list in fig. 3) in order to achieve the best visualization of your data, or interactively explore your data using the “SQL Lab” that Superset includes, saving queries for later use. Our data warehouse sits on Redshift. Our operational databases are replicated in Redshift using “ AWS Database Migration Service ” (DMS), so our ETL processes (that are executed in Airflow ) can easily access to all the data that they need. Our ETL pipelines processed the data and used to store the resulting data in separate tables in Redshift, that were visualized on our Superset dashboards. Redshift was then the main data source from all of our dashboards in Superset. Our data pipelines used Redshift both as data source and data storage, as described graphically in figure 4: At first, our setup showed serious performance issues. The heaviest dashboards were taking too long to load the first time (when they were not cached) and some charts in them were not shown at all when many filters were applied, just an error message instead. When this happened, often, the whole Superset website became unresponsive , and any other request that came in at that moment took also a long time to get a response. After checking our Superset configurations, we confirmed that the cache (we are using Redis in the cache layer) was correctly configured, as the behaviour of the website was already revealing. The problem was clearly only with the dashboards with data that had not been cached yet, and that’s something that we could reproduce easily using the “Force refresh dashboard” option in Superset. The AWS instance in charge of serving the Superset website had enough resources, so that wasn’t the limiting factor. Also, another interesting point is that Superset tries to execute as many queries as possible in parallel, but even with complex dashboards with a large number of charts, “Redshift dashboard” in AWS was showing quite a low number of concurrent queries. We found out that the Gunicorn configuration that we were using (that was the configuration used in the Docker image referenced in the Superset documentation and repository ) was limiting too much the number of processes that Superset could create. Gunicorn uses the “ pre-fork worker model ”, which means that a bunch of processes is created when the server is started. You can choose which type of worker class you want to use : from the simplest synchronous worker class (“sync” worker class) to different asynchronous worker classes (\"eventlet”, “gevent”, etc) and a threaded worker (“gthread”), and how many processes and/or threads you want to start , among other useful options . After increasing the number of pre-forked processes and changing the default worker class (at the beginning, we were using only 2 processes and the “sync” worker class), we were able to increase substantially the number of requests and queries we were able to handle at the same time (see below the Redshift concurrent queries graph — figure 5). In addition to changing the server configuration, we also started to use a multi-server architecture for high-availability and performance reasons. After these changes, our business intelligence web application was able to deal with a much larger number of concurrent requests, but it still felt slow when it had to deal with non-cached complex dashboards (some of our dashboards have a large number of filters, so it’s quite common for the users of our dashboards to visualize non-cached data). To be able to improve the performance of Redshift is necessary to understand quite well its architecture (reading Data Warehouse System Architecture and checking Internal Architecture and System Operation is a good start). The most important ideas are that Redshift is a distributed database with parallel processing (it has a “leader” node and a number of “compute” nodes, that can execute queries in a cooperative way) and is column-oriented (being the main advantage that some aggregations can be done easily without reading unnecessary data). In order to achieve a reasonable performance from Redshift, you need to configure correctly different elements (more about that below) and, even with that configuration, it is a database that is designed with a very specific workload in mind. The Redshift Workload Management allows you to create query queues associated to certain users or groups with some resources guaranteed, so as Redshift is also used by other users, we created one to guarantee resources to Superset. We also tested different DistKeys (distribution keys) for our tables. In Redshift, you can define how the data is going to be distributed among the nodes (‘All’ to have a copy of the table in each of the nodes, ‘Even’ to spread the data, or ‘Key’ to define a column you want to distribute the data on), so you can avoid some network traffic and aggregations at the “leader” node if your tables are small and you distribute them to all the nodes (you have to choose the most efficient strategy for your tables depending on your data access pattern). One of the Redshift limitations is that you cannot define indexes as in a traditional RDBMS. You can define a single SortKey per table , composed by one or more columns, and Redshift will store your data on disk according to it. We reviewed the SortKeys that we already had on our tables and added them to a few tables that didn’t have them yet. After all these changes, we improved the performance of Redshift, but still, the executions times were not fast enough to be used from an interactive web environment, so we decided to look for an alternative database for our dashboards. As the tables that Superset was consuming were quite small, and after understanding better the drawbacks of using a database as Redshift, we decided to check the performance using a traditional RDBMS . At that point, we were pretty confident that the behaviour improvement could be quite significant. We chose PostgreSQL because it is a database that we know pretty well (it’s the database of choice at the company), we knew what to expect from it, and how to bring out the best in it. After some successful and promising tests using the anonymized production data in our local environment, we decided to go ahead and adapt our ETL processes so we could have the same data that we were processing and storing in Redshift, also in Postgres. Once we had decided to start using Postgres in our dashboards, we were interested in moving forward as fast as possible, because the user experience of the dashboards was quite worrying. We found out that the fastest way of confirming that our decision was correct was to use “dblink”. Dblink is a Postgres extension that allows you to execute queries in an external database from PostgreSQL. We just need to configure the connection from Postgres to Redshift using a “ CREATE SERVER ” clause (defining the host, port and dbname) and then map the Postgres user with the Redshift user (using a “ CREATE USER MAPPING ” clause where we specify the foreign database user credentials). After that, we can execute queries in Postgres that will fetch data from Redshift. Of course, we would not get any performance advantage if Superset were connecting to Postgres and Postgres was hitting Redshift for each query; but we can use this connection to create a “ materialized view ” or just a normal table with the data retrieved from Redshift (e.g. we can execute the usual “INSERT INTO…SELECT…” but using “dblink” in the query part to get the data from an external database). You can find more information about this technique on the article JOIN Amazon Redshift AND Amazon RDS PostgreSQL WITH dblink in the “AWS Big Data Blog”, or read how Intermix uses this technique to use Postgres as a caching layer for Redshift data. As we are generating the data consumed in our dashboards only from our ETL processes, we only need to add a step in our processes to copy the data from Redshift to Postgres, as the last step after the data has already been processed and stored in Redshift. In figure 8, you can see the visual representation of one of our ETL pipelines in Airflow after adding this last task. The pipeline flow goes from left to right: tasks in purple read or write data from Redshift (data is retrieved in the first step, processed in the second one, and then loaded to another table in Redshift in the third step); the rightmost orange task is the new task that we added, being in charge of copying the data from Redshift to PostgreSQL using the technique that we just explained (just using “dblink” and the correct configuration in PostgreSQL to connect to Redshift). As the tables that we are creating in Redshift and Postgres have the same structure but with some small subtleties (e.g. in Postgres we create indexes, that doesn’t exist in Redshift; or in Redshift, our tables have SortKeys, that are not valid for Postgres), we also re-implemented the creation of tables using SQLAlchemy and sqlalchemy-redshift , getting a simpler and more maintainable code. Superset allowed us to easily change the charts of our dashboards to get the data from Postgres instead of Redshift. It was just a matter of creating the new database connection and editing the tables, changing the source “database”, automatically linking our charts to the tables in Postgres. We needed to fix a few metrics because of small differences between Redshift and Postgres, but configuring Superset to use the Postgres database was really straightforward. The results using Postgres have been really positive: the dashboards load really fast even when applying many filters, the website is not getting unresponsive anymore, and this was possible without using lots of development time or computational resources. We don’t need to store the data originated in our ETL processes in Redshift anymore to be used in our dashboards, so we are adapting our pipelines so they only write the results to our Postgres database. At the moment, we are in an intermediary step where our pipelines are still storing the results both in Redshift and PostgreSQL, but we are already doing this in parallel, loading the data in PostgreSQL directly from the processed data, without getting the data through Redshift anymore. As having all data aggregated into one data source simplify a lot some operations, we plan to use “AWS Database Migration Service” (DMS) to sync the resulting data from Postgres to Redshift, so our ETL pipelines can stop writing to Redshift. This way, they will have fewer responsibilities, but the data will still be available and updated in our warehouse, just relying on a trustworthy tool like “AWS DMS”, that is already used by us to make the data in all our databases available in Redshift. Redshift is an amazing database, but it’s not a general purpose database. It performs well with big data analytical workloads where you need to process lots of data in a relatively small amount of time, but it’s not a good fit when it comes to processing small tables and/or you need low-latency. If that’s your case, you will probably obtain a much better result using a traditional RDBMS. If instead, you need to execute analytical queries across a large number of rows, Redshift will be able to do a good job, as it can use the computing resources of the cluster and get you where traditional databases simply can’t. We need to look for the right tool for our needs. Even if Superset (and other dashboard solutions) supports lots of databases, not all of them are a good fit for the job, and this will probably depend on your current circumstances (data size, queries complexity…). Special thanks to David for his time and efforts investigating the performance issues. Thanks Michele , Joaquín , Jose , John , Luis , Sergio for reviewing this post and for all the valuable feedback! If you want to know more about it’s like work at Jobandtalent you can read the first impressions of some of our teammates in this blog post or visit our twitter . Jobandtalent Engineering team blog. 134 Thanks to José Gabriel Martínez , Michele Trevisiol , Joaquin , Luis Recuenco , John McLachlan , and Sergio Espeja . Superset Redshift Airflow Data Visualization Data Science 134 claps 134 Written by Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Written by Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-05-27"},
{"website": "JobandTalent", "title": "our first impressions working at jobandtalent part 1", "author": ["jobandtalent Engineering"], "link": "https://jobandtalent.engineering/our-first-impressions-working-at-jobandtalent-part-1-991a48eac2a4", "abstract": "Data Science Backend Product Mobile Platform Frontend We are hiring! Imagine that you are working in a team that tries to raise the bar every day as a collective way of thinking. After working for a long time in that team chances are that mind-blowing achievements in the past now feel normal. They become the foundation to building new things and are used on a daily basis. As we are in a process of hiring a lot of new people , when we talked with our new co-workers they are very surprised by those “ normal ” things. So we decided to give them a space to talk about their experiences and impressions when they joined Jobandtalent. This is the first post of a “ first impressions ” series written by our new friends with less than 6 months at Jobandtalent. Free format and opinions! Iris is working with us as a Junior Frontend Developer At Jobandtalent they encourage us to write blogposts, to do our bit to collaborate with the community. Thinking about that I remembered when I started the selection process for a junior frontend developer position, I was on a four month long trip around Central America, Costa Rica at that moment. I guess I was missing the fellowship I found in my bootcamp, called “run”, at Devscola. And what I’ve found here was even more than that. Something really good that you get when you arrive at Jobandtalent is onboardings where you have a meeting with every team in the product department. That allows you to get to know almost every person in the company and gives you a deep understanding of what we do in Jobandtalent, not only individually but globally. Another cool thing is that we work with agile methodologies . I’m in the Fleet Management department and we use Kanban but other teams use Scrum, every squad is free for using whatever works best for them. All of us are able and encouraged to suggest new technologies or tools. This means that there are always new things to learn. Improvement of everyone’s skills is very important at Jobandtalent. During my first five months, I have been in meetings to organise going to conferences together, weekly workshops like the Elixir Learning Club to learn a new technologies or languages, or an advanced course in React for the newbies like me. There are also mentoring programs that everyone arranges with their mentor, I could never be grateful enough to Abel for all his dedication and patience _:) We also have internal talks in the Remotes Week , when about once every two months, all our colleagues that work remotely come to Madrid and we take advantage to go and eat one of best ‘pulpos a la brasa’ in city. Aside from that, we also can work remotely and with flexible working hours! So we could say that I ended a journey to start another even more exciting and enriching one. Twitter Medium Gonzalo is helping us in Backend I have been working at Jobandtalent for six months, and I can say for sure that the onboarding process has been very productive. As I was joining as a Backend engineer, I needed to learn about all the internal tools and services that shape a very complex infrastructure . It seemed like a huge challenge, but my mentors and colleagues have taught me a lot and helped me to find my place in the company in the best possible way. Knowledge sharing is constant at Jobandtalent. In the Backend team, we have weekly meetings to review interesting topics and discuss them in an open-minded and respectful way, always learning from each other’s experiences. My colleagues have always taken my opinions into account, even if their experience was double or triple mine. Other powerful initiatives have been launched in recent months. For instance, weekly sessions to learn the Elixir programming language , as it is going to be used in some of our new services. Furthermore, Jobandtalent has proved to be a truly remote-friendly company . In my team, we have engineers working from 3 different locations around the globe, and the communication and development flows are perfectly adapted to this. For me, the main lesson I learned in these recent months at Jobandtalent is that the more we engineers know about our business domain, the more value we can add to the company and to our customers . This makes the difference between a good engineer and a brilliant engineer. That is the path I have in front of me and I firmly believe the best place to achieve it is Jobandtalent. Medium José Gabriel is working with us as a Data Analyst in the Data Science team It’s not easy when you start to have that feeling: you need a change, to grow personally and professionally, to leave your comfort zone without knowing if you are making the right decision, but you finally decide to jump. The first days are never good, or that’s what I thought before starting at Jobandtalent. Coffee and good talks were the perfect recipes to breaking the ice, everybody lets you know that they are here to help you with whatever you need, that you are together within the same company, moving in the same direction. After the warm welcome by your team, you are introduced to the rest of the teams (even if they are working remotely), which was a very enriching experience : to see everyone’s faces, get to know what they are doing and start to understand your position within the company, furthermore, you soon realize how easy it is to work with them. There is a lot going on inside Jobandtalent. When you start working you can get lost with all the information coming from everywhere, but the weekly product update allows you to have a global vision of all the teams , with the most important changes made in the previous week and some insights about how everything is going and the following goals. This way, even if you are new, you will start to put all the pieces together very soon. When you start to work with your team, you realize that is very dynamic, they take you into account and the decisions are made together , which is the best way to make you feel part of the growth of the company. You also hear about a thing called the “remotes weeks” : a week every two months when all the remote engineers and teams gather together at the headquarters, it’s the perfect time to get to know more about your colleagues, their work, and enjoy the after-work events. At this point you realize that you are working with brilliant engineers and amazing people. You feel part of the team and the company, learning and improving every day, and then you can only be grateful about that feeling that made the right decision choosing Jobandtalent. Twitter Medium jobandtalent.engineering If you want to know more about how is work at Jobandtalent you can read the first impressions of some of our teammates on this blog post or visit our twitter . Jobandtalent Engineering team blog. 51 Thanks to Gonzalo Gómez , Iris Carballo , David González , Luis Recuenco , and Sergio Espeja . Startup Work Life Balance Work Product Programming 51 claps 51 Written by The magicians doing software, devops, front and backend, mobile apps, recommendation systems, machine learning, clouded infrastructure and all that jazz. Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Written by The magicians doing software, devops, front and backend, mobile apps, recommendation systems, machine learning, clouded infrastructure and all that jazz. Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-05-07"},
{"website": "JobandTalent", "title": "adopting elixir the bff case study", "author": ["Sergio Espeja"], "link": "https://jobandtalent.engineering/adopting-elixir-the-bff-case-study-b575a81ec794", "abstract": "Data Science Backend Product Mobile Platform Frontend We are hiring! Here at Jobandtalent, we have started to implement new services using Elixir. We are feeling quite comfortable and pleased with the decision and wanted to share the experience. In this post, we’ll talk about a case study of Elixir applied to: A long-term running business. Jobandtalent’s last “ rebuild-everything-from-scratch ” dates back to 2012. Architecture based on a mix of Microservices and Self-Contained Systems . Our system is composed of ~30 services. 80% Ruby on Rails codebase, with some services in Scala, Go and Python. Running on Amazon Web Services within Docker containers. What is a BFF A Backend For Frontend (BFF) is a service that looks after encapsulating, aggregating and abstracting internal services for a particular client. Some authors refer to it as API Gateway too. The BFF abstracts the internal architecture of a system providing a tailor-made API contract for a given client. Why do we need a BFF Four main reasons pushed us towards the implementation of a BFF: Aggregating results from different internal services to provide a single response to a client . We want a layer that abstracts internal services to allow changes to them without affecting clients directly. Optimizing views for a given client without coupling that client to the different services . Our main concern regarding communication between the Ruby on Rails services and the mobile apps had always been that RoR pushes developers to implement RESTful-ish endpoints. Mobile apps can usually benefit from single requests to endpoints that aggregate data instead of multiple requests to different resource focused endpoints. Involving the mobile team in backend development to improve their communication . Learning Elixir in Jobandtalent has been a cross-guild effort, where members of all Jobandtalent’s guilds ( Mobile, Backend, Frontend, …) are encouraged to learn Elixir, review and implement BFF Pull Requests. Embracing the Strangler pattern to remove some legacy services . The Strangler Application Pattern has been proved to be a good approach to dealing with legacy applications. The goal of this pattern is to move requests from a legacy service to a non-legacy one little by little. It starts with the addition of a layer that catches all requests and only acts as a proxy to the old service. Then as functionality is being implemented in the non-legacy service proxies it to the new implementation. Why Elixir Elixir is a dynamic, functional language designed for building scalable and maintainable applications. Elixir leverages the Erlang VM, known for running low-latency, distributed and fault-tolerant systems, while also being successfully used in web development and the embedded software domain. A request to a BFF service usually executes lots of requests to internal services, composes them and returns the result. That means concurrency and speed is a must. Elixir helps to handle concurrency effortlessy and it’s fast . The Jobandtalent team is eager to experiment with and learn new technologies, but it’s important to get a true understanding of a new technology before launching it to production, you need in-team expertise . At Jobandtalent we have senior engineers with in-depth Elixir knowledge. Team motivation is another key factor in deciding to add a new stack in a team. Elixir is one of the favorite new languages for Ruby on Rails engineers to learn. It is functional as well and attracts Jobandtalent’s non-RoR engineers. Elixir ships with a great set of tools to ease development, has a growing ecosystem and interest in it has been rising since its creation in 2011. The community is well known for writing good documentation and helping newcomers. Case Risks In our case, the main risks were the failure in the adoption of the new language, having an ecosystem that was not mature enough and the inability to operate it in production. The adoption risk was a small as the BFF service should just aggregate information from other services and wouldn’t handle business logic. In case of failure to adopt Elixir, the technology could be easily replaced. We have found that the ecosystem was mature enough for almost all of our current use cases, with just some exceptions in specific client libraries. Fortunately, we managed to get past them while contributing to improving those libraries . Regarding production operation, we are using a Docker-based platform and more success stories had been documented reducing a lot the probability of failure significantly. Conclusions After some months of running our BFF in Elixir, we can confirm it fits the BFF case perfectly because of its ease of development and good performance. It also gave us enough experience to start developing more services in Elixir. We have to improve our efforts in pushing all guilds to contribute to the BFF code, but we are pretty confident with our weekly Elixir Bootcamp sessions. Week by week we are realizing that the whole team is improving their Elixir fluency. I hope this use case can help to increase the number of companies that evaluate the risks and start using Elixir in production, helping the ecosystem to grow. If you want to know more about how is work at Jobandtalent you can read the first impressions of some of our teammates on this blog post or visit our twitter . Jobandtalent Engineering team blog. 110 Thanks to Abel Muiño , Luis Recuenco , and David González . Elixir Backend Development Change Management Programming Backend 110 claps 110 Written by CTO at jobandtalent Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Written by CTO at jobandtalent Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-05-07"},
{"website": "JobandTalent", "title": "https medium com aracem working with a design system", "author": ["Marcos Trujillo"], "link": "https://jobandtalent.engineering/https-medium-com-aracem-working-with-a-design-system-f426be09c470", "abstract": "Data Science Backend Product Mobile Platform Frontend We are hiring! In 2016 Jobandtalent was a completely different company. We were a Job Board , a site where companies post job openings and people apply to them waiting for an answer that could come from inside the platform, i.e. in a chat, or outside with the method that the companies prefer. Back then, the product development cycle was the typical one that we have been familiar with in our careers. After a phase of ideation and research, some wireframing with feedback from the technical team, the designers would open their preferred design tool and give to the developers a fancy and cool design. Developers tried to figure out how to decompose the design into views that could be reused and develop them. But, is this decomposition easy for developers? In the summer of 2016, Jobandtalent decided to pivot the product. Why stop at the posting job phase? Why not handle the entire work cycle, from the job search to paying salaries? Imagine the challenge. All the teams were involved, a totally new product scope, a lot of new features, and a new look and feel for the app to represent the new concept correctly. We needed to improve these key points: Coordination Parallelization Consistency Anticipation Speed After thinking about how to improve them we come up with… “A design system is a collection of reusable components, guided by clear standards, that can be assembled together to build any number of applications” Will Fanguy The design team proposed a solution to improve how we work at Jobandtalent, to start using a Design system: Atomic design . The best way to understand what Atomic Designis, is by checking out this gif: Reducing the design to small components, or atoms , that could be composed to form bigger ones, or molecules , and finally combined all together to form structures or organisms . Design as a developer (I apologize to my fellow designers). Composition, abstraction, encapsulation, polymorphism… For more information about atomic design check out this blog post and Jobandtalent’s design team vision . With atomic design developers and designers have a common point of view. New screens could be built with components already developed and the UI could be built in a modular fashion, like Lego , and these screens are designed with the same philosophy: Coordination, consistency, speed. The Components library is a project in wich we built the Jobandtalent atomic design. It is a separated artifact from the Jobandtalent’s app, published in a maven repository that can be deployed locally or remotely . We can add a new component or make a modification that produces changes that break the components library and/or the app and work with a local deploy of the library without interrupting our teammates. It is versioned , to ensure controlled changes with low friction in the app and fewer merge problems. A new version requires a PR in the app only including the changes required for that version. This leads to a low level problems due to changes. Each atom, molecule or organism is built as a Custom View . Using custom views instead of plain layout XMLs helps us to easily compose views and makes developing the UI for the new features much easier. It also forces us to standardize how the custom views are developed with strong rules that all the team should follow. First, the structure is similar for all the custom views, all sharing a similar API. Naming conventions for setting up, parameters, listeners… All the views have an inner data class ViewState that encapsulates the render information independent from the business logic . It’s very important define it as a library not as an app’s custom view . We use generic names for the data . A view has a title and a description, never a userName or jobDescription even though the view will only be used with users or jobs in the app. Thinking agnostically help to make the library more composable effortlessly. It is also more predictable even for a new team member has no experience with it. In summary, 1 component 1 custom view, standardization, more standardization, render logic 👍, business logic 🙅‍. jobandtalent.engineering One of the best things about of having the components library is the Sample App . We have a sample of each component in separated activities where we can check how they look and see an example of how to integrate them. Designers and managers can check the components on their phones. Also, developing the components in a separated small app with really fast build times (less than 6 seconds without instant run) is really a pro feature. The app isn’t only a group of custom views, there is an IconKit representation, color kit representation, all the text styles, and the playground . The playground is a place where we can develop with all the components and tools that we have in the library however complex the view or screen that we have to develop for the app. This has a big benefit in compilation time and development speed. Of course, we don’t forget the testing side of thing. Our library is composed of two modules, one with the design system and another with a set of matchers, actions, and assertions to test all the components. All the custom views examples have UI tests to check the integrity of the components. As I commented at the beginning of this post the objective of using the design system and the components library was to improve how we work in Jobandtalent but this wasn’t the only success. During this past two years, we changed the colors of the app a number of times and the font family once. These changes would have been extremely hard to implement without the components library, the design system and the sample app. Before using them we postponed the decision to change the font family several times due to the difficulty of changing it in an app with more than 5 years of code. Checking the accuracy of the change in tens of screens and use cases requires a lot of time investment. But with the design system, the change is localized and much less difficult. Design team members could check the visual result in real devices with the sample app and fix possible errors or make improvements before changing the app. And this is only one example! Are you interested in knowing what steps we took to build the components library and the reasons for those decisions? Check out the slides of my talk ‘ Working with a design system ’ . My friends Victor Baro and Jorge Rodriguez wrote about the components libraries from a technical point of view in iOS and Android . If you want to know more about how is work at Jobandtalent you can read the first impressions of some of our teammates on this blog post or visit our twitter . The components library is the work of a lot of people that worked at Jobandtalent. Thank you all ❤️! Thanks Saúl , John , Eduardo , Chema , Luís , and Sergio for reviewing this post! Jobandtalent Engineering team blog. 299 Thanks to Saul Diaz , Eduardo Pascua , and Sergio Espeja . Design Android iOS Product Mobile 299 claps 299 Written by I need music to live. Progressive rock! Love technology. Senior Android Developer at @jobandtalentEng Co-Founder & Android developer of http://storybeatapp.com Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Written by I need music to live. Progressive rock! Love technology. Senior Android Developer at @jobandtalentEng Co-Founder & Android developer of http://storybeatapp.com Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-05-07"},
{"website": "JobandTalent", "title": "when the best practice is avoiding best practices", "author": ["jobandtalent Engineering"], "link": "https://jobandtalent.engineering/when-the-best-practice-is-avoiding-best-practices-d7da6f10407e", "abstract": "Data Science Backend Product Mobile Platform Frontend We are hiring! T here’s a common pattern in programming to convert an array into a dictionary. You might want to count elements, deduplicate them or do fast lookups. It’s a useful thing to be able to do. Here I want to talk about a specific Javascript incarnation of this pattern. This version uses reduce to turn the elements of an array into the keys of a newly created object. It usually comes with the handy spread operator like this: Succinct and elegant, isn’t it? Yes, it’s succinct, elegant and also a very bad idea. I’ll try to persuade you avoid this idiom like the plague. The piece of code above gets the job done. Furthermore, it does it by using only pure functions and, in fact, it’s the way that several style guides and blog posts recommend it be done. Given an array, it creates an object in which the keys are the elements of the array. But look what happens when the array has a non-trivial length: That’s right. This operation takes more than 10 seconds to run with a modern CPU and browser. You can try this out by copying and pasting this piece of code into your browser’s console or running this JSPerf test case . As an object with 10,000 keys, is it so big that it takes the Javascript engine a long time to instantiate it? No, the problem is not the size of the object, but the way we’re building it. The truth is this should be several thousand times faster. The problem lies inside the combining function. At every iteration, instead of adding a new key to the accumulator, we’re using the spread operator (or Object.assign ) to build a new object that contains the accumulator and a new key. Needless to say, creating a copy of the accumulator at every iteration gets nasty very quickly. There are two reasons for this: Time complexity is O(n²) . Creating and immediately destroying thousands of increasingly bigger objects puts a lot of pressure on the garbage collector. This informal test shows how the performance suffers very clearly: Simply, avoid copying the accumulator. When you modify it directly, it’s a different story: This is the time profile of this version: Yeah, well, if you choose the fast version, there’s a chance that someone who reads your code is going to complain, be it a teammate or the linter. Why? Because there’s the general rule that mutating a function’s argument is bad practice. The reason for that is that you’re turning it into an impure function, introducing an accidental state and breaking referential transparency. This comes at the expense of legibility, robustness, testability, parallelization, composability and a series of benefits that come with functional programming. But, in my opinion, a case like this more than warrants the breaking of this rule. The quest for complexity reduction should never advance at the expense of the practicality of the software for the user. In other words, we shouldn’t place the developer’s interests before the user’s, unless there’s a huge maintainability gain. In our particular case, the performance penalty from the pure version is so big that choosing this option just to fulfill some functional programming ideal (that’s not that relevant in the Javascript world) is, in the words of someone who speaks better English , the tail wagging the dog. Besides, the sacrifice is minor. Although we’re introducing impure definitions, these are kept completely encapsulated inside the combining function. External purity of the reduce function is preserved so it keeps its referential transparency. It’s tempting to argue that, in spite of the benefit, we don’t want to abandon good practices in favour of “premature optimization” and delay the optimization until there’s really a problem with performance. However, it’s not the blatant, seconds-long case that concerns me. This case will scream to be profiled and fixed. It’s the small, inconspicuous cases that concern me. If you’re following best practices, these can riddle your codebase with little performance hits. These individually may be harmless, but collectively can have an accumulative effect that makes the whole application sluggish. When there’s no one single cause but many difficult to detect causes, the best-case scenario is extra developer time to profile the app, and the worst-case is they’ll go unnoticed and harm the user experience. So please, don’t get OCD about making the linter happy. Just use the version that’s 12000x faster. If you want to know more about how is work at Jobandtalent you can read the first impressions of some of our teammates on this blog post or visit our twitter . This post was written by Gabriel Rodríguez Alberich and proofread by John McLachlan . Jobandtalent Engineering team blog. 39 Thanks to Sergio Espeja . JavaScript Performance Best Practices Backend Programming 39 claps 39 Written by The magicians doing software, devops, front and backend, mobile apps, recommendation systems, machine learning, clouded infrastructure and all that jazz. Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Written by The magicians doing software, devops, front and backend, mobile apps, recommendation systems, machine learning, clouded infrastructure and all that jazz. Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-05-07"},
{"website": "JobandTalent", "title": "ios architecture separating logic from effects", "author": ["Luis Recuenco"], "link": "https://jobandtalent.engineering/ios-architecture-separating-logic-from-effects-7629cb763352", "abstract": "Data Science Backend Product Mobile Platform Frontend We are hiring! This article is the second in a two-part series. You can find the first one here . It’s been almost a year since I last wrote about the new architecture we implemented in the iOS team at Jobandtalent. It was an architecture that tried to take advantage of some of the best features of Swift (value types, generics and sum types to name a few), while embracing some unidirectional data flow fundamentals. The results have been incredibly positive so far. Our code ended up being much simpler and easy to reason about than it used to be. We think about state modelling in a totally different way, avoiding code with the two main problems I explained in detail in my previous article: The problem about multiple, scattered, out-of-sync outputs. The problem about exclusive states. This was possible due to a single, unified structure modelling the state of a specific domain and due to the fact that Swift has a rich type system with Algebraic Data Types (ADTs from now on) that made us model our domain in a better way. Still, we had a big underlying problem. A problem that was hard to foresee at first. And that’s the problem of coupled logic and effects . This article will describe the evolution of our previous architecture into a better approach by moving the logic to the value layer (pure part) and making the reference types layer (impure part) handle effects and coordination between objects. But before I get into that, I’d like to establish some fundamentals and concerns about the State Container Architecture. Many iOS and Swift developers aren’t really familiar with the concept of sum types (also called tagged unions, variant types or more formally, coproduct types ). Swift exposes us to the concept of enum with associated values , and that’s perfectly fine. The real problem comes when we cannot explain what the real advantage of that enum with associated values is. And when we don’t understand what’s the real purpose of the type, it’s very difficult for us to use it appropriately. So, here it is: The Sum Type makes illegal states impossible It’s that simple. I guess the next question is w hat’s an illegal state ? For that, let’s imagine a simple callback function like this (Data?, Error?)-> Void . These types of callbacks were extremely common back in the old Objective-C days. Which are the possible values of the tuple (product type) (Data?, Error?) ? As you can see, there’s only two legal states , but, as (Data?, Error?) is a product type, we have exactly 2 * 2 possible combinations for the values inside the type. We usually have to ways of solving this: Convention : We check whether we have a valid error first. If that’s the case, we don’t care about the data. Testing : Making sure none of the code paths produce the invalid states. I don’t want convention for this. I don’t even want tests for this. I want a rich algebraic type system that prevents me from doing this in the first place. There’s no better way to protect you from illegal code that the fact that you cannot write it. So, what’s the proper way to model this behaviour? You guessed it, a sum type: There is simply no way of writing the previous valid data and valid error case. Swift is preventing us from making illegal states possible. And that’s the real magic of sum types. Once you start modelling your state using sum types, it’s really hard to go back to languages without them. It’s the first thing I miss when I have to touch old Objective-C code. And in case you want to make the error case an impossible state , that’s what bottom types are for. In Swift we have Never , but you can easily create your own, more semantic version by simply having an empty enum. I’ve always loved Unix philosophy . Small and simple programs that make one thing well and that are able to compose and cooperate nicely to solve bigger problems (that’s what FP is all about with functions or OOP with objects…). But there’s a rule that really made me think the first time, and that’s the Rule of Representation . Fold knowledge into data so program logic can be stupid and robust That’s exactly what proper, rich ADTs make me feel. They hide all the complexity so I don’t need to have all that nasty logic that prevents the illegal states and makes my code complex and hard to reason about. By the way, chapter 9 of The Mythical Man-Month stated this already in 1975. Show me your flowchart and conceal your tables, and I shall continue to be mystified. Show me your tables, and I won’t usually need your flowchart; it’ll be obvious Let’s consider a simple app where we have to draw geometric figures like squares or circles. In a more OOP-ish approach, we would create an interface or base class Figure with a draw method. Then, the implementations Square and Circle would implement that draw method providing the appropriate behaviour. In a more FP-ish approach, we would create a sum type Square | Circle with a method draw . What are the advantages and disadvantages of each approach? OOP approach: we can easily add new figures by creating new classes that implement the abstraction. Unfortunately, adding new specific operations (behaviours) is difficult, as you’d have to implement the new methods in all the different classes. FP approach: adding figures is more difficult, as you would need to change all the different methods that accept a figure to handle the new case. Fortunately, adding new methods and behaviours is a simple as extending the type creating a new method. The OOP approach complies with OCP. The second doesn’t. Does this mean it’s a bad choice? Well, not all code that complies with OCP is necessarily better code. You might end up in a situation where it’s not so common to add new figures, but more behaviours… But, can we have the best of both worlds? Can we have most of your code comply with OCP while using sum types. The answer is yes. Let’s take a look at a simple example. Now, imagine we add a new error case to the enum. The good news is that the code won’t compile until we handle the error case. The bad news is that there might be a lot of places that need fixing. In order to comply with OCP and avoid the impact that a new case can have in our code, the simplest solution is to abstract the functions that destructure the State type (what we called queries in the previous article). Now, in case we need to add the error case, we only have to update a few local functions that are the ones used all across the code base where state information is needed. That way, we guarantee that all clients of our State object comply with OCP. This way of having client code protected from our state shape not only makes our code more robust for future changes, but it also helps us avoid problems related to view reuse. By modelling your view state as a sum type, your view will be reused for the different states ( loading state, loaded state, etc). It’s quite easy to forget setting a subview in any of the cases (you hide a view in the loading state but forget to show it back in the loaded state). Using queries guarantees that we always set the appropriate piece of state for each one of the views that we need to handle. All this is very related to the Expression Problem and the extensibility in two dimensions. In case you want to know more, I recommend Brandon Kase’s talk, Finally Solving the Expression Problem . Value types are all about inert data, immutability, and about equivalence. They don’t base the concept of equality on identity, rather, on their underlying value . Swift is the first language that really exposed me to value types. There are a lot of things that make value types great. Their memory is handled in the stack and they lack some of the overhead needed by reference types to handle identity or heap allocation (reference count, etc). But the real big deal is that they are copied on assignment (with optional Copy-On-Write (COW) semantics). What does this mean? This simply means that, by using value types, we can avoid the implicit coupling we have when using reference types (aliasing bugs). Being able to reason about code in a way that we can be sure it cannot be modified by other parts of the code base makes a huge difference. This also makes value types perfect for multithreading environment as there’s no need for synchronization primitives. Swift makes it extremely convenient to handle the immutability of value types via the mutating word. Immutability has great advantages, but it can make our code, and ultimately our architecture, a little bit cumbersome to use. By using mutating , we maintain the immutability semantics and have the convenience of being able to mutate the value type in place by letting Swift create the new value and assign it back to the very same variable. It’s the best of both worlds. And finally, value types have enormous advantages in testing. They lead to easy data in, data out testing without further ceremony (no mocks whatsoever). Not everything is great about value types though. Mutating big value types is not as performant as it should be due to the lack of persistent data structures . But I’m confident we’ll have them in future Swift versions, hopefully. I’m a firm believer that testing gives you quite an accurate idea about the quality of your code. But I’m not talking about the number of tests or your test coverage, I’m talking about how easily you can test your code . If your code is simple and easy to test, that’s definitely a symptom of good design. Unfortunately, Swift, and the lack of some reflection capabilities (which most of the mocking libraries depend upon), hinder this task. Let’s imagine a simple app where we have a view model, which depends on a service object, which depends on an API client. We want to test the view model in isolation. Let’s also suppose we use dependency injection so we can pass any test doubles we want. Ideally, we would do something like this: The mock made it very easy to cut the dependency graph and let us test the outgoing command ( service.downloadData ) without asserting against the real side effect outcome (the actual state change due to the data download). With Swift, the environment for the test gets much more complicated than it used to be. As you can see, we had to subclass Service to create the spy. And the real issue is that, in order to create the mock, we need to supply all the appropriate dependencies. In this case, it’s only APIClient , but it could be much worse. And of course you need to create that false APIClient providing all its dependencies… and so on and so forth. Another problem with the subclass and override approach is that you have to be sure to maintain the invariants of the base class. Forget to call super in any of the overridden methods and prepare yourself to have green tests even if your real code is failing miserably. This makes it incredibly cumbersome and uncomfortable to test some code. There are some options like creating interfaces for the sole purpose of mocking, but we don’t think this explosion of polymorphic interfaces makes the design any better. There are of course places where it makes a lot of sense to abstract ourselves from the outside world (database implementations for instance) via an interface, which let us design by contract and use techniques like Outside-in TDD . You can start designing with your high level contracts without actually deciding which lower level pieces will fulfil those contracts (you can put off your actual persistence implementation choice for instance). If you want to automate the creation of mocks for this very cases, Sourcery is a great tool for that. There’s also the option to subclass NSObject if you prefer, but we’ll stick to pure swift reference types for the rest of the article. Our current solution is to have a simple composition root approach where we can inject some test doubles for specific parts of the graph without supplying the rest of the dependencies. This eases some of the pain and we might talk about this in a future post. Just to be clear, I’m not advocating the use of mocks in testing, but I do think they are very valuable for some cases. They let us test in isolation and more importantly, they make obvious the bad code when you need to mock six dependencies and override quite a few methods to be able to exercise you subject under test. They tend to be coupled quite often with implementation details, thus, making the tests break when those details are changed, without actually helping catch real issues. Also, there are some people that think that not testing against the real thing is not valuable at all. Some of us have had bad experiences with false positives in the past, where the problem happened even if one of the tests should have caught the issue in the first place. Integration tests are not really the answer to our problems though. Even if they might seem like a good solution to our mocking issues, the number of integration tests you need to cover all the important code paths grows exponentially instead of linearly with isolated tests. To know more, I highly recommend J. B. Rainsberger’s talk: Integrated Tests are a scam . Not everything in Swift made testing code harder. In fact, the aforementioned Rule of Representation and the rich ADTs make it possible to skip some tests that are mandatory in some other languages. Testing that some input is in a valid range is something that can be easily modelled by a proper type for instance. I’d like to quote a sentence from Uncle Bob’s article The Dark Path . Now, ask yourself why these defects happen too often. If your answer is that our languages don’t prevent them, then I strongly suggest that you quit your job and never think about being a programmer again; because defects are never the fault of our languages. Defects are the fault of programmers. It is programmers who create defects — not languages. And what is it that programmers are supposed to do to prevent defects? I’ll give you one guess. Here are some hints. It’s a verb. It starts with a “T”. Yeah. You got it. TEST! While there’s some truth in that sentence, it’s also true that some languages make it easier to commit more mistakes than others. Tests are definitely the first tool to prevent mistakes in software, but we shouldn’t forget about types and static analysis tools, which make a big number of the typical dynamically typed languages tests obsolete. So… Interfaces are not the solution. Integrations tests are not the solution either. Which is the solution then? In my experience, code that is not easily tested ends up not being tested at all, at least not all the proper code paths. So, we asked ourselves what we could do to make the most out of our testing suite and be able to test most of the important business rules without suffering on the way. That’s when we discovered the problem about coupled logic and effects . Let’s take a look at the following code. That code is really simple, on purpose, but perfectly shows the problem about entangled logic and effects. Let’s imagine we want to test that code. It seems sensible to write these two tests: Given state.isLoading = false , service receives the method downloadData when calling fetchData. Given state.isLoading = true , service does not receive the method downloadData after calling fetchData . In order to do this, we need to build the proper infrastructure for the test, that is, setting both the correct state and the service spy to double check that the messages are being sent correctly. This seems like an easy task when we just have two tests, but imagine a real life example, where the logic is much more complex and the effect can be triggered under different circumstances. So, what’s the impact of logic and effects in terms of testing? Logic is responsible for the different code paths. The more logic we have, the more code paths we need to test. Effects makes our code difficult to test, as they require testing infrastructure (test doubles). What happens when we have logic and effects together? We certainly have the worst of both worlds, a lot of difficult code to test . Let’s try a different approach. How about moving the logic that is responsible for an effect to a value layer that doesn’t trigger the effect but rather, models the forthcoming effect via a value type? That layer will only be responsible for performing the logic and deciding if the effect should be triggered. Then, someone else will be responsible for taking that effect value object and actually performing it (network request, data base access, IO, etc). This separation is much more profound that it may seem at first. We’ve separated the what from the how . The decision making from the actual decision . The algebra from the interpreter . We’ve made the value, the boundary. But let’s go back to our testing example. Now, no matter how complex the logic is, the tests are extremely simple. We only need to create the correct state model ( given ) and assert that the fetchData ( when ) result is the correct Effect ( then ). No need for cumbersome infrastructure with test doubles. No need for difficult tests that grow whenever our logic grows. When our logic grows and our effect has to be triggered under different circumstances, we only need to write new dumb data in, data out tests checking that the Effect is the one I expect. Sure, we still have to test that the effect handler actually handles the effects correctly, and that’s where we’d have to use test doubles, but we only have to do that once . Compare that with our previous example where the logic and the effects where together. In fact, as we’ve dramatically reduced the code paths in our imperative shell layer, we could easily use integration testing without worrying about the number of tests we’d need to cover the appropriate code paths. This would avoid the need to create cumbersome test doubles. We have also made our testing code more robust. Now, if our mock implementation breaks, we only break the only test that handles the mock interaction, the collaboration test we have for the class EffectHandler . In our previous implementation, every single test that dealt with the effect would have broken. Our new game has two simple rules: Move as much logic as possible to the value layer. This is where our business rules are. This is where isolated testing is easy. This is our pure part, our functional core . Model effects as inert value objects that don’t behave and have a thin layer of objects handling effects and coordination on top. This is where we download the data. This is where we save data to disk. This is where we handle I/O. This is where we can afford integration testing. This is our impure part, our imperative shell . The concept of functional core, imperative shell is not new. In fact, it dates back to 2012, when Gary Bernhardt’s talked about it in Boundaries . This is one of the best talks I’ve ever watched and a lot of the concepts in this article are heavily inspired by it. Architecture changes are always quite difficult for brownfield code bases. You have to be quite careful about the amount of new things you bring to the table. It’s important to analyze all the trade-offs and always try to have something incremental with fewer impact in developer productivity and noticeable improvements. This is easier said than done, of course. The State Container based approach we took a year ago was exactly the right trade-off. It contained just the few amount of new things so people didn’t feel overwhelmed by the change. It was an incremental update to the traditional MVVM approach we were already using. Now it’s exactly the right time to bring some new ideas, like modelling events and effects as sum types and have some other deeper constraints to force us have clear separation of logic and effects. Let’s now talk about the main components of the architecture. The state is arguably the most important part of the architecture, our functional core. The consumption of the state follows the same rules as the previous architecture. We use the concept of queries to make the state clients agnostic of their internals and comply with OCP, like we previously explained. As for mutation, we previously had several mutating methods, that we called commands . Now we have a public and unified way to change the state, the mutating handle(event:) function, which returns an optional Effect . This is one of the most important changes. The State will not only be responsible for deciding the next state based on the event, but also what happens next (effect). This was inspired by how the Elm language handles side effects. The state definition looks like this. To get a clear understanding, let’s consider a simple app that downloads all the recruitment processes where a candidate can apply. The state will look like this: As you can see, when we send the event fetchProcesses , the state changes to the loading state and returns the effect that needs to be triggered next, downloadProcesses . The responsible for handling this effect is the next piece of the puzzle, the effect handler . But before we get to that, let’s talk about events . Using a sum type to model messages to objects might not be the most idiomatic thing at first. We are used to modelling messages by using methods, but they have some nice advantages that make them a welcome addition. Common API. Where we previously had different mutating methods to change the state, we now have a single method for that. Thanks to the IDE autocompletion, we can easily see all the different events that will produce state change by writing state.handle(event: .|) , where | represents the caret. The event represents the future state change. It’s not actually performing any state change. It’s the same idea behind modelling effects as value objects. Events can be logged or serialized. Events, modelled via sum types, allows us to leverage optics and prisms to compose different states in a simple way . We are not opinionated about how you should structure your state. You might find interesting to have a single source of truth structure (like Redux) or multiple structures (like Flux). Having a single structure makes some things easier, like avoiding inconsistent states across different nodes of your state tree, but it might not be easy to adopt in your current application. Having several stateful pieces is much easier to adopt for brownfield projects, but you need to take care of synchronisation between dependencies around those stateful pieces. Choose wisely. An effect handler will be responsible for handling the different effects associated with a specific state shape. The definition looks like this. As you can see, the effect handler is not only responsible for the effect, but also for providing the optional event that will be sent back to the state. The real effect handler will look like this. State and effects handlers are not enough. We need something listening to our different stateful pieces that eventually need to update the state. That’s what an event source is for. The definition looks like this. Let’s imagine that we have a store handling our current session state and we might need to clear all our processes when the user logs out. As you can see, we purposely only provided the function S.Event -> Void in the configureEventSource function. This avoids any access to the underlying state shape, forcing us to send the event with the appropriate data and move the logic to the value layer. We are almost there… There’s only one missing piece… The piece that glues everything together, the Store. The store is the missing fundamental piece of the puzzle and it’s responsible for quite a few things: It wraps the state value object and allows others to know when it changes (subscription handling). It coordinates state and effect handler to produce the new state and execute the different associated effects. It provides the event source with the function to use to send events. It’s the façade for view controllers and other objects to send events to the state. It handles memory management of effect handler and event source. The implementation is the most complex one. It uses type erasure to handle communication with the effect handler and event source. For simplicity, I will omit all the type erasure and state subscription, which is exactly the same code as in the previous article. The most interesting part is the dispatch function, which is responsible for the state/effect loop . It processes an incoming event, producing a new state and an optional effect. Next, we handle that effect calling dispatch concurrently until we finish the chain and produce the final state. The use of futures is optional, but it’s handy to know when a specific event has finished. Finally, it is meant to be used like this. Let’s finally have a quick recap of all the concepts a some clarifying diagrams to see how everything fits together. State : Your logic lives here. Your data lives here. This is where you have your business rules and decision making, as well as deciding which effects should be triggered after the different state changes and events. Effect Handler : Responsible for deciding the actions to take for any of the effects. Event Source : Responsible for listening to different stateful pieces and send events to change the state accordingly. Store : The piece that ties everything together. The external façade to the system. The UI talks to it to send events and change state. I’m quite pragmatic about architectures. As I said in my previous article, there’s no such concept as best architecture . The same way some languages make you commit less mistakes than others, I do think that some architectures make you be able to change your code more easily over time. And remember, one architecture is better than other if it lets you change your code more easily over time. Unfortunately, there is no silver bullet for that. You can choose to have a lot of rules and obstacles about how you should structure your code. This will make developers ship features more slowly and will ultimately be less happy. You can choose to have a more flexible architecture, trusting on the good judgement of people to do the right thing, with some shallow guidelines about the structure of the code base. This will likely lead to inconsistencies and code that it’s not very easy to maintain in the long run. The ideal architecture is the one that makes our code evolve healthy over time without redundant overhead. Over the lasts years, the iOS community has taken a lot of ideas from other areas. React and Flux popularised the unidirectional approach. Also, functional programming is gaining a lot of traction and popularity lately. Unfortunately, we sometimes fail to consider the imperative context where iOS lives, and try to force ourselves to use some techniques than aren’t simply idiomatic enough for our platform. And that was our goal from the beginning, create an architecture that was inspired by a lot of different languages or paradigms and feels good to use in iOS without feeling alienated. This is only the beginning, but I think we’ve made a pretty good start. If you want to know more about how is work at Jobandtalent you can read the first impressions of some of our teammates on this blog post or visit our twitter . Thanks Ruben Mendez and Victor Baro for all the valuable feedback while developing the architecture. Jobandtalent Engineering team blog. 144 3 Thanks to Jorge Rodriguez , Marcos Trujillo , Victor Baro , Sergio PR , and Rubén Méndez . Swift iOS Mobile App Development Android Mobile 144 claps 144 3 Written by iOS Team Lead at @jobandtalent_es. Former iOS at @plex and @tuenti. Creator of @ishowsapp. Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Written by iOS Team Lead at @jobandtalent_es. Former iOS at @plex and @tuenti. Creator of @ishowsapp. Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-05-07"},
{"website": "JobandTalent", "title": "hack talent 2018 jobandtalent engineering hackathon", "author": ["Sergio Espeja"], "link": "https://jobandtalent.engineering/hack-talent-2018-jobandtalent-engineering-hackathon-7196bcfeeff3", "abstract": "Data Science Backend Product Mobile Platform Frontend We are hiring! A hackathon (also known as a hack day , hackfest or codefest ) is a design sprint-like event in which computer programmers and others involved in software development, including graphic designers, interface designers, project managers, and others, often including subject-matter-experts, collaborate intensively on software projects. https://en.wikipedia.org/wiki/Hackathon The main goal of Hack&Talent 2018 was team building, having fun building new valuable features/products for Jobandtalent and working with people from other teams than yours. The rules: 48 hours. No overnight work. ≥2 members teams. Multidisciplinary teams. Work with teammates different from the day-to-day work. After two days of thinking, hacking and building, we end up with 5 cool projects and we presented and voted them together. The winner will keep the Jobandtalent product cup till next Hack&Talent edition. NFC Clocking Build a clocking service that allows to track working hours of employees at client work places. Using NFC technologies and geo tracking. Extra: Clocking with QR Codes. Goals : Build a digital clocking mechanism. Store clocking data. Technologies : iOS - CoreNFC: https://developer.apple.com/documentation/corenfc Ruby on Rails: http://rubyonrails.org/ Front: MediaCapture API https://developer.mozilla.org/en-US/docs/Web/API/Media_Streams_API + https://github.com/schmich/instascan Front: Fetch API https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API Instant app welcome Instant App to be able to welcome Candidates to interviews without having to download the App. Goals : Make the interview process registration easier and faster. Improve user experience. Automatize interview all process registration. Technologies : Instant apps: https://developer.android.com/topic/instant-apps/ Kotlin: https://kotlinlang.org/ Firebase: https://firebase.google.com/ Lurch-ito Assistant bot to help employees access different actions and information across our many applications. Lurchito make me Super-Admin! Goals : Easier, faster and replicable way to operate with our frontend services. Technology : Ruby on Rails: http://rubyonrails.org/ React: https://reactjs.org/ Cucumber Expressions https://github.com/cucumber/cucumber/tree/master/cucumber-expressions Best Friends Forever Development of a BFF (Backend for frontend) prototype. Goals : Provide a API with specific endpoints and representation of resources for mobile. Improve interactions between Backend & Mobile Clients. Technology : Phoenix Framework: http://phoenixframework.org/ Elixir: https://elixir-lang.org/ Docker: https://www.docker.com/ Jean Claude Vand App Be able to generate iOS builds and uploads to store from Slack channels. Goals : Anyone can generate a working build from a repository branch. Anyone can upload the iOS app to the store. Technologies : Ruby, sinatra: http://sinatrarb.com/ ngrok: https://ngrok.com/ fastlane: https://github.com/fastlane/fastlane Papers please Automatically detect kind of documents: driver licenses, national identity cards, versions and front/back of them, and extract information whenever is possible. Goals : Improve the efficiency of our documentation proceses. Technology : React: https://reactjs.org/ Python: https://www.python.org/ Tesseract (OCR): https://github.com/tesseract-ocr/tesseract Tensorflow, Inception-v3 based model: https://github.com/tensorflow/models/tree/master/research/inception Kubernetes: https://kubernetes.io/ This Hackathon was not only a great team-building experience, but impressed everyone in the company with the quality and usefulness of the projects that came out. Solving long-lasting pains from some key departments in Jobandtalent. The plan is now to make sure most of those features make it to the Product Roadmap, some of them (like Jean Claud Vand App ) are even already in Production. Congrats to the Hack&Talent 2018 winner’s, Papers please ! Cannot wait for Hack&Talent 2019 edition! jobandtalent.engineering If you want to know more about how is work at Jobandtalent you can read the first impressions of some of our teammates on this blog post or visit our twitter . Jobandtalent Engineering team blog. 73 Thanks to David González and Jérémie Mairesse . Hackathons Product Programming Software Development Startup 73 claps 73 Written by CTO at jobandtalent Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Written by CTO at jobandtalent Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-05-07"},
{"website": "JobandTalent", "title": "fostering security with hashicorp vault", "author": ["Jorge Quintás"], "link": "https://jobandtalent.engineering/fostering-security-with-hashicorp-vault-4e45ec4eb399", "abstract": "Data Science Backend Product Mobile Platform Frontend We are hiring! A standard monolithic stack uses a couple of secrets such as database passwords, certificates, tokens, Cloud Service Provider credentials, etc. But when using microservices you might end up having a lot more secrets, increasing not only the amount of work needed to carefully manage them but also the risks of getting into trouble in case the work is not properly done. Ask yourself the following questions: Do you know which users have access to each secret? Do you have a policy regarding the secrets’ life cycle? Do you enforce that policy? Can you trace when a secret has been accessed (read, written, renewed, etc.) and by whom? Do you have plaintext secrets in your VCS? If your answer was no to any of the first three questions and/or yes to the last one, this post is for you. If not, congratulations on your good work! But you might find it interesting regardless. We’ve chosen Hashicorp Vault (Vault from now on) to help us follow the best practices for handling secrets, and to improve the overall security of our platform without having more workload every time the number of secrets we handle increases. Vault is a tool that provides safe access to secrets. It’s formed by multiple backends (with some glue between them) responsible for specific tasks. The Vault’s main components are: storage , secret , audit and auth backends. This backend is responsible for interacting with a large list of supported storage backends . Storage is not trusted, so all the data leaving the Vault for storage will be encrypted. If you want to have a High Availability cluster of Vault check which storage backends support HA operation. Secret backends either store or generate secrets. Some secret backend examples are: kv : stores key-value pairs database : (if correctly configured) can be used to dynamically generate users for multiple db engines. transient : can be used to easily encrypt/decrypt texts. Secrets are managed as if they were Virtual File Systems. You can: Mount : so you could read or write the specified mount point Unmount : make the path unavailable, revoking all secrets belonging to the unmounted path Remount : to change the path, this operation revokes secrets belonging to the old path Full list of secret backends Audit Backends are responsible for tracing every interaction with Vault. Sensitive information will appear hashed with a salt using HMAC-SHA256, so analysis can be performed without leaking any secrets. Multiple audit backends can be configured simultaneously, but bear in mind that at least one must be able to persist the data for the operation to be completed by Vault. If all audit backends are blocked (i.e. full disk when a local file is configured) Vault will remain unresponsive until the problem is fixed. This guarantees that no operation is done without leaving a trace. Vault supports several authentication methods that cover a broad set of use cases. You can find the full list here . Some of them are intended to authorize users while others are better suited being used by applications. For example, you might want to use your organization LDAP to authenticate users, but find it more convenient to use AWS’s instance profiles for your applications. Once the authentication is successful, a token is returned to the client. This token could be understood as a session cookie; this way Vault’s client will be able to use it on your requests. If you are using the API instead of the binary client, you should send said token in X-Vault-Token HTTP header. Furthermore, with a successful authentication, the applicable policies are added to the token. Depending on your current setup you’ll need to find the most suitable authentication backends among the available ones. For now, the information to take into account is: If authentication is successful, you will receive a token that will be used in further requests. A list of configured policies will be attached to the token defining the operations that can be done while using it. All users who are granted authentication will have a default policy with limited capabilities, like management of their own token and access to a private secret endpoint called cubbyhole. Policy files are described using HCL format. A simple read-only policy for ansible mountpoint: Policies are deny-by-default, so you’ll have to explicitly grant access to the paths. A token with the above policy will have read-only access to /ansible . Lists of policies can be attached to users or groups so that the effect of each policy is combined. Now let’s get down to the fun part, a real life example: our own case scenario. We asked ourselves the 4 questions presented at the beginning of the post and didn’t like the answers. It’s not that we had the worst answers possible (4 wrong answers!), but we had enough wrong answers to realize that it was time to get back on track to better and sustainable security policies. Our main concerns, other than what Vault provided straight out of the box, were: High Availability : our infrastructure is completely automated and it can scale without manual intervention. In case more capacity is needed, at least one Vault server must be up and running in order to send secrets to our provisioners (name we gave to the instances in charge of automatically deploy new servers from scratch to production ready) Disaster and Recovery : we store a copy of Vault secrets in S3. The files are encrypted using GnuPG, the recipients are the members of DevOps guild. In case of data loss in Vault, we have an internal tool that configures it back again (authorization methods, policies, and secrets). We started with a small concept test using Vault in development mode and executed in docker, in order to get a feeling of what we had learned that far. We had been using docker in production for more than 2 years, and not regarding what Vault’s production hardening guide said, we deployed Vault in one of our ECS clusters with the purpose of further learning, and to start testing in High Availability using DynamoDB as the storage backend. Remember that it’s the storage backend’s responsibility to provide HA capabilities (and so DynamoDB does) For the final setup we decided to use EC2 instances behind an internal ABL. With this setup the ALB sees the active node up and the inactive one down (remember the return HTTP status 429 when unsealed ); which fits perfectly, since requests will be routed only to the active Vault instance. The HA in Vault is intended only to better the availability, not to increase capacity. By being assembled this way, just one Vault instance delivers requests while the rest are hot replicas. If the active node fails, one of the standby nodes becomes automatically active. We have created a privileged IAM role assigned to Vault instances’ profile, which gives access to dynamoDB allowing Vault to configure itself. Once Vault is able to start, more restrictive permits may be assigned to that role. Bear in mind that Vault uses some values that may be acceptable by default, but that might not suit your needs for this particular case. Review DynamoDB read and write capacities before production use. We took all these steps using Terraform and Ansible . We wrote a custom Systemd service unit configuration for Vault to ensure that a configuration exists and that the cap_ipc_lock Linux capability is set in Vault’s executable, so that Vault’s process memory is not swapped to disk (if the swap partition is not encrypted a malicious user could read the dumped memory content of the Vault’s process!). So far so good. We have a highly available Vault server living inside our VPC. What now? Let’s start using it. We added another tool to our tool belt to help us establish ssh tunnels through a bastion server in order to access our Vault’s leaders (yes, plural. We have the same setup in each of our different environments). We handle all of our environments as if they were the production environment: they are deployed, configured and monitored in much the same way. The only difference between them is in the scale. This way they all deserve the same security standards. When we talk about the “Vault server” we are referring either to “this specific environment Vault server”, or to “the Vault servers in any environment”, indistinctively. Now, we have to set Vault according to our needs and begin storing secrets. In order to accomplish this, we need to: Configure the authentication backend Mount the secret backends that we need Configure the policies for these assembly points Write secrets into Vault We soon realized that the best option was to build our own tool to make the bootstrap and to open the Vault server in order to get it to a functional state. Besides helping out with these four tasks, this tool also stores an encrypted copy of the secrets with GnuPG in S3; which helps us not only to manage secrets more easily, but also we get a versioned, encrypted and updated copy of Vault data. Furthermore, all this allows the tool to easily carry out recovery in the face of a “disaster”; it’s as easy as bootstrapping a fresh and unsealed Vault server, and feeding it the desired revision of the S3 backed up secrets. For the integration with Ansible we used ansible-vault , with some modifications made to our inventories to guarantee that secrets from different environments don’t cross paths. We use the same bootstrap tool previously mentioned to interactively add, remove or modify existing secrets. We found out that Vault is a powerful tool, one that will “force” us to follow the path of best security practices. We can now answer correctly the four questions at the beginning of the post: Do you know which users have access to each secret? yes Do you have a policy regarding the secrets’ life cycle? Do you enforce that policy? yes Can you trace when a secret has been accessed (read, written, renewed, etc.) and by whom? yes Do you have plaintext secrets in your VCS? not anymore And even better, Vault allows us to easily apply the same policies across our platform. If you want to know more about how is work at Jobandtalent you can read the first impressions of some of our teammates on this blog post or visit our twitter . Thanks to Sergio Espeja , David Gonzalez and Victor Castell for reviews and comments, and Ximena Guzman for language corrections Jobandtalent Engineering team blog. 67 1 Thanks to jobandtalent Engineering and Sergio Espeja . AWS Vault Hashicorp Security Platform 67 claps 67 1 Written by Platform engineer at jobandtalent Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Written by Platform engineer at jobandtalent Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-05-07"},
{"website": "JobandTalent", "title": "optimizing docker images for a faster development workflow", "author": ["jobandtalent Engineering"], "link": "https://jobandtalent.engineering/optimizing-docker-images-for-a-faster-development-workflow-591dc3ac4de0", "abstract": "Data Science Backend Product Mobile Platform Frontend We are hiring! Docker is now a key part of the release lifecycle here at Jobandtalent. We’ve been using it in development for the last three years and actively using it in production for the last two years . Acquiring it back when we did (and when many others did as well) was key to increase efficiency in several aspects of the development delivery process. Container based architectures allow us to simplify the tooling used to manage services and sub-services seeing them as if they were sort of black-boxes. They also allow us to homogenize the way we manage complex service based platforms, whether they are run on a laptop, on a development server in the cloud using specific branches for each part, or on a more stable full fledged production environment. Being based on docker means that every change that improves the usage of a resource (time to execute, memory usage, disk space usage, network usage…) in any stage of the process, will have an impact in each one of the environments. We’ve recently improved the way we were creating images to make them lose a bunch of megabytes. This means waiting less time for the service to be up and running, less disk space to store the image and less base RAM needed when running it. Even while being content and hardcore users of Docker for so long, we were still standing by some decisions made at the beginning that were not optimized and therefore had room for improvement. Here’s what we found out: Most of Jobandtalent’s applications run on Ruby on Rails. Before Docker they were running on virtual ubuntu servers configured via Ansible and relying (by dependencies) on libraries provided by the OS. Back then it was reasonable to keep using same base image, ubuntu, which weights around 870MB. Nowadays there exists an actively maintained group of alpine based Docker images ready to be used. Each one of them includes the dependencies needed to run a specific version of ruby, for example. Therefore, we can have a small but fully functional base environment just by pointing directly to the alpine image with the version of ruby we need. Comparatively speaking, by switching to an alpine based image we got a starting point of just above 61MB for new developed services. Obviously a small base image is rendered futile if the first thing we do is fill it again with useless software. Being strict with the dependencies is easier said than done, but it is key for avoiding a considerable percentage of increment in image size. For example, we can switch from one Ruby gem to a different one and forget to delete the old one and its dependencies. This also happens with the explicitly set dependencies for libraries or software defined in the Dockerfile that are not needed anymore as the project evolves. Often reviewing what is being included will keep the image fit. Dockerignore file is an important point to bear in mind in order to avoid including unwanted files into the final image, which can sometimes greatly increase the image due to temporary files, vendor dependencies or files that are only needed in one of the steps. Fine tuning the dockerignore file will not only help decrease the image’s size but also the build time, since the context that needs to be sent can be, most of the times, reduced to what is only really needed at that point. Dockerfile steps matter when dealing with cache layering. It is easy to end up grouping all the RUN steps in either the same step or in as few steps as possible. This sometimes leads to humongous one-splitted-liners in Dockerfiles, using commands that install, build, configure and remove. Some of them are probably artistically indented to try to keep the file as readable as possible. This point has a side effect that must be taken into account: once a layer is cached and when no modifications cause a layer cache miss, that layer will keep being the same, even if it points towards external dependencies that can change. For example, when running an apk add, that run will be marked as cached for the next builds unless a cache invalidation happens in previous steps. Therefore the same versions will be used for all the builds until an invalidation happens. This can be good or bad. Good for having the same versions in all the builds until an invalidation is introduced. Bad if this is not taken into account and one keeps thinking that a generic add/update step will keep the latest versions of the packages installed. It’s worth mentioning a quick point here about using COPY or ADD directives. Both of them are very similar, including external content inside the container. Bear in mind that ADD can have side effects when dealing with URLs or compressed files, and that as with RUN steps, cache invalidation is not trivial. For example, Last-Modified HTTP Header does not count against the cache invalidation so it can, at the end, behave different than expected. It may be simpler to keep the content locally and use COPY and dockerignore. Alpine’s APK comes with a handy feature that allows you to install dependencies in a virtual setup . This means that the base image is populated with dependencies when a command is run only once in build time, and it’s easy to clean when the command finishes. The goal is to keep each layer as small as possible, and as mentioned in the previous point, that can only be achieved by deleting the software that is not needed anymore within the same step. The flag --virtual <name> allows the packages being installed to be defined as <name> . Afterwards, they can be easily deleted by running apk del <name> The flag --no-cache will make APK update the index without having to create a disk cache. Builder pattern was deprecated after the multi stage feature of Docker was released last year. However, we still find it useful for getting all the juice in continuous integration/continuous deployment services. A CI/CD service will build the final image, run tests on it, and if all is green push the image to a repository and, if it applies, trigger the deployment. On the other hand Docker cache layering shines when only the last layers vary. Therefore, we’ll reach the highest performance if the new group of commits only affect the last layers. It’s likely that some dependencies will be needed for running test, but including them in a layer means that they will be included in the final image. This is also the case for some other work items like deliverables, documentation, assets, etc. Add this to the fact that the same application is run as different profiles: a webapp, a webclient, as workers for job processing, as event consumers… having, the final image, the union of all the dependencies and the necessary resources for all of them. Moving the generation of assets and other dependencies to CI/CD instead of having one-fits-all images will reduce the final image size that will run in a given environment. For example, preparing an image that contains the requirements to run tests, another one with requirements to generate assets and of course a vainilla one for running the proper application will improve the layering usage of the docker running on the CI/CD service without impacting the resources of having one unique image. Let’s apply these techniques to a real project. In this case we’ve chosen an open source, self hosted and mobile optimized CI, powered by fastlane https://fastlane.tools (Source code: https://github.com/fastlane/ci ) Building the image As usual, build the Docker image from the already included Dockerfile. docker build . Optimize Change the Dockerfile: Image size comparison Our results in original and optimized image shows a ~70% size reduction. Time taken on MacBook Pro i7 8 cores, 16GB RAM By following these points we’ve been able to reduce RoR Docker images that weighted in the order of GB to below 100 MB. Also, we’ve been able to reduce continuous integration times from 17 minutes to 7 minutes. This was achieved by moving to Alpine based images, by only keeping the necessary dependencies, by having the building command dependencies deleted within the same step in the Dockerfile and by effectively using the Docker layer cache. Related links docs.docker.com If you want to know more about how is work at Jobandtalent you can read the first impressions of some of our teammates on this blog post or visit our twitter . This blogpost was developed and written by German Del Zotto and David González . Thanks to Sergio Espeja , Jorge Quintás and Victor Castell for reviews and comments, and Ximena Guzman for language corrections. Jobandtalent Engineering team blog. 133 2 Docker Alpine DevOps Programming Platform 133 claps 133 2 Written by The magicians doing software, devops, front and backend, mobile apps, recommendation systems, machine learning, clouded infrastructure and all that jazz. Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Written by The magicians doing software, devops, front and backend, mobile apps, recommendation systems, machine learning, clouded infrastructure and all that jazz. Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-05-07"},
{"website": "JobandTalent", "title": "ios architecture an state container based approach", "author": ["Luis Recuenco"], "link": "https://jobandtalent.engineering/ios-architecture-an-state-container-based-approach-4f1a9b00b82e", "abstract": "Data Science Backend Product Mobile Platform Frontend We are hiring! This article is the first in a two-part series. You can find the second one here . I find it incredibly difficult to talk about architecture. Unlike other topics on computer science, architecture has a major component of subjectivity, which makes it really difficult to justify why some architectures are better than others. There is no such concept as best architecture . There is an architecture that best fit your needs for a very specific context and problem space. A reactive architecture for instance, might be a great choice when you can describe your domain as streams of data than can be easily transformed and composed in a declarative manner to get a specific state output. A VIPER architecture might be good one for a long-term project in a mid-sized team. Otherwise, it might add unnecessary complexity and would not be a good fit. What did work for you in the past, under specific circumstances, might not work any longer in your new current condition. It is very common to look up to what some big corporations are doing and try to mimic their way of doing things. That is not necessarily a bad idea, but in case you are not in one of those big corporations, you might not have their needs. In that regard, we have tried to be pragmatic and find an architecture solution that best solves our engineering and business problems. The goal of this blog post is to show the architecture that we have been developing during the last months on the iOS team at Jobandtalent. It is an architecture that leverages Swift, value types, generics, sum types and some React Native experiences. It works for us and it might work for you as well. During the last eight years, I have had the chance to use most of the common architectures patterns to develop iOS apps. As most iOS engineers, I started my career working with MVC. Then, Model View Presenter (MVP), Model View View Model (MVVM), clean architecture approaches like VIPER, or the new unidirectional architectures like Flux, Redux or Model View Intent (MVI) came along. Here’s a secret: I have seen quite bad and good code in any of those. MVC has been attacked for the last years. As if you were condemned to failure only by the fact that you chose that architecture. The problem is not that MVC is a bad way to structure your code. The real problem is that MVC is a broad concept. You usually need more rules and constraints about how you should structure your code for it to scale nicely in the future. But even if you can write good code using MVC, it is very easy to end up having the so-called Massive View Controller. MVP or MVVM tried to solve some of the problems that MVC had. They solved the problem about Massive View Controllers, but they created others, like the massive presenters or view models. Once more, MVP or MVVM are broad concepts. They only tell you how your data should be separated from your view and how they should communicate. VIPER, the MVP clean architecture iOS implementation, did solve some of those problems. It added quite a few more actors like interactors or routers, and there is just less uncertainty about how to structure your code. The real problem about MVP or MVVM is in the view layer: you have multiple, scattered, easily out-of-sync-with-domain outputs. In MVP, we will define an output as a command that the presenter invokes on the view. In MVVM, an output will just be a property set in the implementation of the view model that will be reflected on the view using bindings. Let’s try to use a simple example that shows the problem. The following view model will download all the recruitment processes where a candidate can apply. As we can see, the view model has three outputs: loading , processes and errors . These outputs could also change as a result of calling other methods, or even as a response to business logic callbacks, which is specially dangerous in a multithreading environment. This will inevitably lead to conflicting states in the view. MVVM, unlike MVP, has the big problem of invisible outputs . In MVP, you perfectly know when your view renders, as you are calling methods on the view, or the abstraction of it. In MVVM you just have some properties and you cannot be sure of what is going to trigger view updates unless you check implementation details. Some of those properties might be observed by the view, some not. That is unneeded uncertainty about how your view layer behaves. Also, there is no way we can track the state changes of the view. We do not have traceability whatsoever about the different state changes that happened in the view model. We can’t reproduce a state history, and that is something that could be very interesting when some problem arises. There is another important problem that has to do with the consistency of the state. Imagine there has been an error and we try to fetch the processes again. We would have loading = true and error != nil for a little while. Does that make sense? It depends, but having consistency across the different outputs of your view model can be quite hard. Let’s introduce a new concept, the ViewState . A model snapshot that represents the current state of the view. Instead of setting different outputs in different places of the view model, let's just set a new view state snapshot each time. As you can see, we now have perfect traceability of the state. Every state change can now be easily logged, saved to disk, or even discarded if the new state is equal to the previous one. We can even have undo and redo capabilities if needed. It is also protected against race conditions by making changes via write transactions inside a serial queue. Also, by having a view state as a value object, your view will receive a state snapshot whose only owner will be the view. No race conditions whatsoever. We have deterministic render updates. And that is big news. Another advantage is that we have moved the outputs consistency problem to the value layer, having different methods that return the different possible exclusive view states. As a general rule, is good to move business logic to the value layer. Due to the inertness nature of value objects, business logic inside them tend to be inside pure function that can be easily tested. Also, testability is greatly simplified, specially when using MVP. We no longer have to check if the view has received a specific message after exercising the presenter. We can simply check the correct state. Testing state equality will lead us to more robust testing, unlike testing interaction via mocks, coupling to the implementation details. Once we have properly modelled our view state, it is time for the view to consume. Some questions come to my mind when I look at that render function: Should I always check state.error before state.loading ? Can state.processes be valid when state.loading == true ? Or when in state.error != nil ? Should I always access state.processes after having checked state.error and state.loading ? This is where the aforementioned outputs consistency problem makes special sense. If we can be sure about the consistency, then the order really doesn’t matter. I can even forget about an else sentence there, and everything will work as expected. In any case, from an API point of view, nothing is telling me all that. We should aim to make our APIs convey the proper usage. Sum types are one of the most overlooked features when starting developing in Swift. As product types, sum types are a type of Algebraic Data Type, also known as tagged or disjoint unions. An example of product types in Swift are tuples. Enums with associated values are the Swift version of a sum type. Sum types, alongside pattern matching and exhaustive switches can model exclusive states in a simple, yet powerful way. Let’s go back to ProcessListViewState and see how it might look with sum types. Not only have we achieved exclusive state semantics, forbidding access to properties that do not belong to the current state, but, thanks to the exhaustive switching, compile will fail when adding a new extra case to ProcessListViewState that is not properly handled on the view. That is a big deal. Most of the times, you can think of your UI as a state machine. In a lot of your screens, I am sure you always have states like loading , emptyCase , error and loaded . Each of those states have slight different concerns about the piece of state the should access. Sum types are a great way to avoid mistakes and convey, from an API point of view, which subset of the state should be consumed in each of the cases. You could even have checks about proper transitions. Imagine that we want to know now if the processes have finished loading from a totally different screen. At the moment, we have all the state deep down in our view layer. There is no way that other views can access that information. The solution is easy: let’s move view state down to domain state and let’s turn the former in a function of the latter. In the last year, I have enjoyed working in some React Native projects. That is what made me change my mind and think so thoroughly about states in the first place. If I had to choose one thing over the many cool features that React Native has, I would choose the fact that your view rendering is a pure function of the state: App = UI(state) We usually have two kind of states: Domain State and View State. View state will usually be a function of the domain state: App = UI(viewState(domainState)) Domain State is the most important of the two. That is the single source of truth of your app and where most of the business logic lives. It should hold the minimum amount of state possible and then derive all the rest that your views need. Value semantics tend to be a really good fit to model the domain layer. Not only will your mutation functions be pure and easily testable, but it will also avoid the implicit coupling that reference types usually involve. Different actors of the app willing to consume a piece of state will have their own snapshot of it and they will be the only owners of them. Here’s a typical domain model layer: As you can see, we have two important new concepts: queries and commands . They are responsible for computing derived state. A lot of times, state can be a complex thing to manage. It can be a huge tree with a lot of nodes and accessing a piece of it will not always be as easy as it sounds. Queries are functions that return some parts of the state for easy consumption. They are composable, as they can use other queries as well. One important feature of queries is that they decouple the domain state shape from our views, making it feasible to refactor domain state to other different shape in the future without huge impact. This is what is called Selectors in the React world. The mutating functions are our commands . They represent how our state can be changed. It is where most of our business logic lives and the only place where the state is mutated . Do not be misled by the mutating word. It is not really mutating anything in place, it will recreate the whole value layer tree. Wow!, what about performance? , you might think. Most of the times, this is cheap. But in case you have a huge data set, you might bump into performance issues due to the fact that we do not have persistent data structures in Swift yet. This is what is called Actions in the React world. Stores are the missing piece of the puzzle. They are domain state holders and coordinators. They communicate with other collaborators, like services to do network requests, or the persistence layer to save data. They are responsible for side effects in a way. They also make sure that the state is only mutated inside transactions in the proper serial queue. The goal is always the same: mutate the state appropriately once the specific job is done and let the world know, if interested. A typical store is like follows: With that store in place, now the view state can be a pure function of the state. What is a state container then? View Models are. Stores are. A state container is simply an object that is able to hold a single state property and knows how to tell the world when it changes. They are quite smart though about mutation and propagation of the state. They will only allow mutation via a transaction. Other mutations will wait for the current mutation to finish. This will avoid unpleasant race conditions. They will only propagate changes when it is needed. State mutations that result in the very same state will not be broadcasted. Stores can subscribe to other stores. View models can subscribe to other view models and stores. Views and view controllers can subscribe to a view model. Multiple views can subscribe to the very same view model if needed. View models that subscribe to other view models is not the most common thing, but it is a handy pattern when communicating superviews with subviews. Imagine that a superview needs to know about some specific detail in the subview (if a button was pressed for instance). Even if we could move this information to a separate object (like a store) so it can be listened to, it would be way more inconvenient and cumbersome than being able to communicate via view model subscription. Superview model would simply subscribe to subview model. That’s it. Before digging into the implementation details and how leveraging swift generics allowed us to automate most of the tasks, let’s try to sum up the main components of the architecture. View/ViewController : it only knows about a view model. It renders a View State when that view model commands. View Model : View State Container that subscribes to other stores and apply pure transformations DomainState -> ViewState when the domain model changes. Store : Domain State Container in charge of coordinating other actors (like network services) to perform business logic and side effects. It usually sets a new state as the final result of those side effects. As we mentioned before, a good architecture should be easy to test, and there is no better way to test than data in, data out testing. In fact, XCTAssert(actual == expected) is usually enough for most of our testing needs. One of the most common problems that people come across when trying to test their business logic is how difficult is to exercise it. And that is mainly because it tends to be hidden deep down in some hard-to-reach code path. In order to be able to exercise it, we need to mock some collaborators to create the proper environment for the test to run. Swift, unfortunately, is not very friendly about mocking. One of the great advantages of the architecture in this regard is that most of the important business logic is in the value layer. Value objects are easy to create and exercise. There is no need to create cumbersome environments or mocking a lot of dependencies. Just create the value object, exercise it and assert the output. And that’s as easy as it sounds. Actors like view models and stores are simply coordinators. We can test the interaction of those different modules, but we would be coupling somehow to the implementation details without really getting much value out of those tests. This does not mean that they should not be tested, this only means that those are not the tests with the greater return of investment (ROI). Let’s see how each of the architecture layers are usually tested: View/ViewController : UI Testing. View Model : We test DomainState -> ViewState transformation that lives in the ViewState value object. In case there is any important interaction with stores, we mock them and check that the proper methods are being called. Store : We test the DomainState value object where most of the business logic lives. Queries and Commands are easily tested via data in, data out testing. As with view models, we also test some important interaction. For instance, we may test that the data is being sent to the persistence layer appropriately. Finally, integration tests are really valuable. Asserting against the final view model state after having exercised both view model and store code paths, gives us a lot more confidence in our testing suit. We won’t cover the full implementation, you can check GitHub for that, but we will highlight some interesting parts. The first thing is defining our state. We use Equatable to avoid sending the very same state to subscribers. After some time using the architecture, we figured three things: Making every state conform to Equatable can be quite annoying. Conforming sum types states to Equatable is a lot of unnecessary noise. UIKit is quite optimized and rendering the very same state several times will not be a problem in most cases. When testing, there is no real need to compare the whole state tree: XCTAssert(expectedState == actualState) . We can just test that a specific part of the tree is correct: XCTAssert(expectedState.expectedSubState == actualState.actualSubState) Thus, we decided not to force the Equatable implementation, but you can provide a custom one if needed. Let’s see how the store is implemented. Some highlights: Stores are always created with a initial state. For safety reasons, there is no option that a store does not have a valid state. State changes are forced to be done via the write function in a transaction. Whenever a state change is detected via property observers, we check if the state is different from the previous one. If that is the case, we notify observers. Subscription is a core part of any store. It will use closures as the callback mechanism that will be attached to a StateSubscription object, a token that will be hooked to the lifecycle of the object interested in receiving notifications. When this object is released, so will be the token and the closure will be nullified ( RAII: Resource acquisition is initialization ). The store maintains a collection of the subscription tokens, weakly referenced to avoid retain cycles. It is important to point out that the real implementation of the store has helper methods to subscribe to other stores without forcing store subclasses to maintain those tokens. Really convenient. Let’s now focus on the view layer. As we talked before, we want our views to have a render(state:) method that will be invoked by the view model when the view state changes. The renderPolicy will be very helpful, as it will avoid that we try to render a view when it is not ready yet. For instance, a view controller can't be rendered until all their outlets are fully set. This usually mean that we should wait till viewDidLoad to subscribe to the view model and be rendered. Now, the first approach when implementing the View Model would be to maintain an array of weak references with the views interested in the view model state. Well, it is not that easy. Swift cannot know at compile time the proper types of the objects in that views array, due to the fact that they conform to a protocol with an associated type. Compiler will complain with the following error: Protocol ‘StatefulView’ can only be used as a generic constraint because it has Self or associated type requirements We have two solutions at this point: Ditch the associated type in the protocol and simply have render function. This has the drawback that the view is forced to maintain a reference to the view model around to access the state. Views maintaining view models will be the most common case for sure, but will not always be true. Type erasure. First option was not really an option for us, so we went with the second one and created our AnyStatefulView class, wrapping weakly our view object. View Model implementation looks like this. The handlePossibleRender method will take care of switching to the main thread if necessary, so View Model subclasses do not need to care about this. The handleNotPossibleRender method will assert promptly in case the view cannot be rendered properly. Crash early, crash often. Now, let’s rewrite some of the code examples we saw in the first part of the post. Remember the use case: we want to download a list of recruitment processes where the prospect can enter. Let’s define the recruitment model domain first. Now, we will define the store state. To simplify the example, we have skipped the specific error or the fact that the loading state could have associated data when reloading, for instance. The store would look like this: Let’s move now to the view layer. Let’s define the view state and the transformations that it applies to the domain state. The view model look like this: And finally, the view controller: As every architecture, it comes with some drawbacks. The most noticeable ones are: Not being able to subscribe to specific parts of the state tree. It is all or nothing. Queries and pure DomainState -> ViewState transformations ease the pain. Not persistent data structures or structural sharing can cause performance problems with huge data sets. No guarantee that only subclasses change the state: no protected access control in swift. Mutating data can be a little bit cumbersome sometimes. As we are recreating the state tree in every change, we usually need to leverage composition to send mutating messages to all the different tree nodes involved in the mutation. This is similar to reducer composition in React. Thanks a lot for reading. I would like to go back to what I first wrote at the beginning of the article: architectures really are a very complex topic to talk about. There are trade-offs just about everything. Ease of development, valuable testing, simple code to reason about or long term scalability is some of the core points that you should think about. Know your problem space, know your domain, know your business and try to come up with the solution that best fits your current context. There is no silver bullet. jobandtalent.engineering If you want to know more about how is work at Jobandtalent you can read the first impressions of some of our teammates on this blog post or visit our twitter . Thanks Ruben Mendez , Victor Baro and Daniel García for all the valuable feedback while developing the architecture. You can check the full implementation in our GitHub repository . Jobandtalent Engineering team blog. 2.5K 12 iOS Mobile Swift Kotlin Android 2.5K claps 2.5K 12 Written by iOS Team Lead at @jobandtalent_es. Former iOS at @plex and @tuenti. Creator of @ishowsapp. Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Written by iOS Team Lead at @jobandtalent_es. Former iOS at @plex and @tuenti. Creator of @ishowsapp. Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-05-07"},
{"website": "JobandTalent", "title": "learning to develop jobandtalents design system for android", "author": ["Jorge Rodriguez"], "link": "https://jobandtalent.engineering/learning-to-develop-jobandtalents-design-system-for-android-54160a571d7b", "abstract": "Data Science Backend Product Mobile Platform Frontend We are hiring! It has been a year already since the product team started working following atomic design ideas. This article explores the results of this decision and what we have learnt since then. You can also check out the wonderful article that our iOS colleagues wrote! One of the biggest challenges engineering teams have to face is balancing new developments while at the same time improving current codebase to avoid increasing project entropy. Jobandtalent’s android application was created more than four years ago and has never been rewritten from scratch. Despite the best efforts of the team, there are still modules in the application that look like growth rings, but instead of years, you could count redesigns, product pivots or team changes. Even when using tools such as Zeplin, it was not really intuitive for developers choosing the appropriate references when styling, nor were they clear on wether a layout should be reused.. So when a year ago the design team proposed working with an atomic design we immediately signed up for it! To simplify, the design system code implementation is achieved by composing simple XML resources and custom views into more complex ones, over and over. Typical Android XML resources become the atoms in our system as they are the simplest and most reusable elements. We work with a delimited colour palette, font-book built with styles, fixed dimens for spacings and sizes as well as a nimations , state selectors and drawables that are consistent across the application. Molecules and organisms are both implemented as composite views encapsulating the view attributes, logic and layout. Simple public methods that accept the state of the view being rendered are exposed as the public contract of the component. The former are meant to be composed within the latter. As you go up in atomic design hierarchy this process is repeated, diminishing the required internal styling and configuration and becoming simple organised containers of reused components. Creating screens and templates becomes easier and more intuitive. Check the design, identify the components, add them to the layout, easily set their style. It is the presenters’ responsibility to coordinate fetching the different information required for a screen, mapping from specific domain entities to state objects that have to be drawn. But there is always a tradeoff, and creating components now requires more effort. Just as any public API we need to be more aware of the contracts we are generating, avoid coupling them to business logic and have clear specifications that define their purpose and behaviour. Composite views was not the original approach we decided. High level components usually act as a proxy, forwarding actions and state to the correct child. To avoid an overhead of such composite views, we started having simple layout composition through <include> tags. However we soon realised that this approach forced us to know about implementation details as the internal views were exposed in our activities, fragments or tests in order to interact with them. The more complicated the hierarchy, the more cumbersome it would be to find the proper view due to repeated ids. Also the more a component was reused, the more code we would duplicate. The first issue we tried to solve when the design system implementation started was how to make easier recognising which pieces were already created and should be used when building new templates. With the idea that segregation would improve the visibility on this matter, we started by adding a new resources folder where we could place what conformed the new base resources and layouts that matched the design system. At the beginning it worked fine, but as we started building more components polishing them out was a real pain since the project would take too long to build for the small changes required. It was clear we had to extract the reusable parts that conform the core of our design system from the application module. Having the components as a dependency allowed a small sample application to be created, which could help in their development. The application is quite useful as it allows you to fidget with values and get fast feedback of the changes, and as it is isolated from the application we can work out edge cases without having to bother with restrictions such as navigating to an specific screen. Once a component is finished, the playgrounds that helped build them are left behind inside of the sample application. It has become an interactive reference for android and design teams, allowing one to easily review if the look and the feel is the expected one and to check how they behave across different android versions or configurations. Be aware of something, though; at the early stage of a design system expect a lot of changes to happen. First of all there are many things to learn, and making mistakes teaches a lot but requires time-consuming corrections. Also while features are developed, new necessities will appear and more changes will come. Lastly, moving code from your main application to a separated repository means a more tedious refactor. During this stage it might have been simpler for us to have the components as a separated module in the same project. As you can see, most of this does not differ much from regular android development. But this should not be shocking as the change affected how the teams think and collaborate among them. As developers we tried to reduce final designs into repeatable patterns that we could code and reuse… but our decisions often deviated from the ones of the designers. Nevertheless, last year’s work has turned this over, and by assembling rather than deconstructing both teams are staying aligned. As the system matures, we can rely more and more on the components library. From the product development perspective this reduces the burden of UI development, allowing us to shift our focus to feature requirements. It is not only related to the amount of code required to complete the task. For example, the consistency in the application not only benefited the users, but also ourselves. After a while it is easier to recognise the elements from design, even repeating patterns in styles, so there are fewer misinterpretations and mistakes. Also, as code is more concise code reviews become lighter and sharper. On the other hand we have the components library, and as mentioned before the development investment has increased as a consequence. However, we found out that the effort pays off exponentially. Not only because there is less code to be written, but also because other tasks benefit from the reusability. Exhaustive design reviews can be done per component, whereas before it would not be as worthy when what really mattered was shipping a new feature. Another example can be writing custom view assertions or matchers that aid when building UI tests. Features have become simpler to develop. Components library moves most of interface complexity away from application, providing an easy way to build screens and reducing the amount of details to worry about. Our relation with design team has tighten, as we collaborate more often than before. We coordinate on new features to anticipate component creation, as well as improving existing ones. The sample application gathers the implementation of the design system and allows the whole team to explore and experiment solutions in a separated sandbox. Overall, we have managed to improve our efficiency compared to a year ago. jobandtalent.engineering If you want to know more about how is work at Jobandtalent you can read the first impressions of some of our teammates on this blog post or visit our twitter . Jobandtalent Engineering team blog. 111 Thanks to Sergio Espeja , Victor Muñoz , and Luis Redondo Cañada . Design Android Mobile App Development Mobile Atomic Design 111 claps 111 Written by Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Written by Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-05-07"},
{"website": "JobandTalent", "title": "manage aws ecs services with docker compose", "author": ["Victor Castell"], "link": "https://jobandtalent.engineering/manage-aws-ecs-services-with-docker-compose-b609028f0ff6", "abstract": "Data Science Backend Product Mobile Platform Frontend We are hiring! In this post, we will show you the AWS pattern for service discovery using Load Balancers, and how to use a Docker Compose defined service to setup and run a service in ECS and associate an existing Application Load Balancer to it. When you start using EC2 Container Service (ECS), you could face lots of new concepts: clusters, task definitions, services definitions, etc. If you are used to working with Docker and Docker Compose , you could be overwhelmed by the concept differences between what you expect in ECS, and how it actually works. ECS CLI, is a command line tool that helps you manage an ECS cluster and services. Best of all its compatible with Docker Compose. From the docs: The Amazon EC2 Container Service (Amazon ECS) command line interface (CLI) provides high-level commands to simplify creating, updating, and monitoring clusters and tasks from a local development environment. ECS CLI provides an abstraction of ECS that allows to manage Task definitions, Service definitons, ECS clusters, in a transparent way for the user. At Jobandtalent we started using ECS to run our platform a year ago, and we’re quite happy with it. We use Ansible for our configuration automation as well as to automate ECS tasks and services. We are now starting to manage some of our ECS services using compose through ECS CLI. Using docker compose is a much more familiar way of managing docker services for the end user. It abstracts all the process of Task Definition creation and Service Definition creation. For example, you can have your services definition tied to your application code in the same repository, and in a format that is pretty familiar for the docker user, especially if you’re using docker and compose for development. Until now, one thing that ECS CLI didn’t allow you to do, was to associate an Elastic Load Balancer(classic or application) with your service upon creation. This was a real show stopper for us, as the use of the tool was limited to services that didn’t serve HTTP, for example message queues or background processes but not web services. This is why we decided to add some support to it, contributing the necessary code that allows us to assign an Elastic Load Balancer or an Application Load Balancer to a Service on service creation. The beauty of open source software. If you want to run your application in ECS you must first choose what method you want to use to allow these containers to be reachable. Fortunately, AWS provides you, with the necessary building blocks that you’ll need to discover your apps running inside ECS. There are two types of Load Balancers in AWS, the Classic Load Balancer and the Application Load Balancer . One component of an ALB is the Target Group , which is used to register EC2 instances to route requests to them. In AWS the recommended service discovery pattern , is implemented by registering containers, started in Tasks, against the Service’s Load Balancer or Target Group. You tell AWS to associate a Service with a Load Balancer if you are using a Classic Load Balancer (ELB) or a Target Group if you are using an Application Load Balancer (ALB), and when your task starts, it automatically registers the assigned port to the Load Balancer or Target Group . Example Service registered to a Target Group Example Service registered to a Classic Load Balancer This pattern is shown in the diagram bellow This allows your HTTP request to be distributed evenly between all the containers running your app. In this pattern, the responsibility of the service registration is delegated to ECS, making your application completely agnostic to the platform from where it’s running. You must associate an ALB to an ECS service definition at creation time, and it cannot be changed afterwards. ECS CLI provides some new params that allows you to assign a Load Balancer when creating the service. To assign an ALB to the service now you can set the following params: --target-group-arn : The full Amazon Resource Name (ARN) of the Elastic Load Balancing target group associated with a service. --load-balancer-name : The name of the classic load balancer. --container-name : The name of the container (as it appears in a container definition) to associate with the load balancer. --container-port : The port on the container to associate with the load balancer. This port must correspond to a containerPort in the service's task definition. Your container instances must allow ingress traffic on the hostPort of the port mapping. --role : The name or full Amazon Resource Name (ARN) of the IAM role that allows Amazon ECS to make calls to your load balancer on your behalf. This parameter is required if you are using a load balancer with your service. If you specify the role parameter, you must also specify a load balancer object with the loadBalancers parameter. These are the new params that we added to ECS CLI compose service to allow the association of a Load Balancer. In the following section we will show you the steps needed to create a service. The first step is to configure it to work with our AWS environment. If you have correctly completed the setup of your AWS credentials (recommended) as described in http://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/setup-credentials.html you’re ready to start using ECS CLI and the Amazon AWS command line interface. Starting with an example compose file that models an HTTP service with a memcached instance, job processing queues and your application running in port 3000. NOTE: All values are fake, you must indicate the correct values for the service and the AWS account. First you need to create an Application Load Balancer (ALB) that will be used to serve your application to the outside world: Create a target group Register your target group with the ALB We’re ready to create our service in ECS. The first step is to configure your ECS CLI environment to point to the desired region and ECS cluster. Then, create a service in ECS that runs your application defined in the compose file, passing the target group ARN, that you want your service to associate with. Follow the output of this command and after a while you should have your service running in your configured cluster. Log in to the AWS console, and observe if the generated task is already running in the service. You must be able to access your service via the Load Balancer URL. Since this URL is difficult to remember, you can use Route53 to create a handy URL for your service in the form of: myservice.mydomain.com To scale your service use: The number of tasks running should increase and be up in your service within a few moments, and the requests to the service URL should be balanced between all running instances. ECS is able to run docker containers in production and can be quite reliable, AWS offers you the building blocks needed to operate your services in production and have all the pieces required to build the complete container lifecycle. It implements a server-side service discovery pattern. And even though there are some tradeoffs (as with everything else), we do like that it actually is quite platform agnostic. We presented a method to setup and scale a web service in ECS that can be fully managed by using the docker compose and the ECS CLI tool and that covers any type of docker app, including HTTP services. We hope that this information will help you get a better understanding on how ECS works and make your life easier when working with the AWS ecosystem. Thanks to Sergio Espeja , David Gonzalez and Jorge Quintás for reviews and comments, Ximena Guzman for language corrections, and David de la Iglesia for the beautiful diagram. jobandtalent.engineering If you want to know more about how is work at Jobandtalent you can read the first impressions of some of our teammates on this blog post or visit our twitter . Jobandtalent Engineering team blog. 138 3 Thanks to Sergio Espeja . AWS Docker Elastic Load Balancer Platform DevOps 138 claps 138 3 Written by Ruby, Go, ops, distributed systems, proud father and husband, not necessarily in this order. Platform engineer @jobandtalent, http://dkron.io author Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Written by Ruby, Go, ops, distributed systems, proud father and husband, not necessarily in this order. Platform engineer @jobandtalent, http://dkron.io author Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-05-07"},
{"website": "JobandTalent", "title": "learning to retrieve and rank intuitive overview part iii", "author": ["Michele Trevisiol"], "link": "https://jobandtalent.engineering/learning-to-retrieve-and-rank-intuitive-overview-part-iii-1292f4259315", "abstract": "Data Science Backend Product Mobile Platform Frontend We are hiring! Real case candidate/job opening ranking framework at Jobandtalent (JT) In previous posts ( part I and II ) we have seen how to build a candidate matrix D , filled with a set of online and offline features, given a job position q i ∈ Q . This is a collection of a set of information regarding the matching between that job and its candidates, and the candidate and company’s historical behaviors . What we need now is an algorithm that can interpret this data and, based on the candidate relevance , optimize the order of the returned candidates. One thing that we didn’t mention is the need for collecting the relevance score for each <job, candidate> pair. The relevance very much depends on what we want to optimise, at Jobandtalent we are taking care of the entire funnel, from the search till the offer signed (directly within the app), thus we can define our relevance scores based on how far a candidate went within the funnel, e.g. , y 1 : shortlisted, y 2 : contacted, …, yk : interviewed, …, y n : hiring succeed. Where each score y j ∈ ℕ (e.g., {y 1 =1, y 2 =2, …, y n =n} ). This class of problems is known as Ranking Problem , and the most popular set of supervised Machine Learning methods that aim to solve them is called “ Learning to Rank ” (LTR). There are three main approaches when dealing with the Ranking Problem, called Pointwise , Pairwise and Listwise , that we briefly summarise below. For our use-case, we decided to use LambdaMART ( TechReport , Microsoft 2010), the last of three popular algorithms (RankNet ICML2005 , LambdaRank NIPS2006 ) main authored by Chris Burges. Briefly, RankNet introduces the use of the Gradient Descent (GD) to learn the learning function (update the weights or model parameters ) for a Learning to Rank problem. Since the GD requires the calculation of gradient, RankNet requires a model for which the output is a differentiable function — meaning that its derivative always exists at each point in its domain (they use neural networks but it can be any other model with this property). RankNet is a pairwise approach and uses the GD to update the model parameters in order to minimise the cost (RankNet was presented with the Cross-Entropy cost function). This is like defining the force and the direction to apply when updating the positions of the two candidates (the one ranked higher up in the list while the other one down but with the same force ). As an optimisation final decision, they speed up the whole process using the Mini-batch Stochastic Gradient Descent (computing all the weight updates for a given query, before actually applying them). Note that when using RankNet, two cost functions are usually applied: one for optimization ( e.g. , Cross-Entropy) and one for the final ranking quality (nDCG, MRR, MAP, etc. ). This is quite common in classification and regression problems since the former cost function needs to respect more strict constraints in order to be easily optimized (smooth, convex, etc. ), but it is the latter one the most interesting for the task in which the model is finally applied (this is defined by Burges as the target cost ). LambdaRank is based on the idea that we can use the same direction (gradient estimated from the candidates pair, defined as lambda ) for the swapping, but scaling it by the change of the final metric , such as nDCG, at each step ( e.g. , swapping the pair and immediately computing the nDCG delta). This is a very tractable approach since it supports any model (with differentiable output) with the ranking metric we want to optimize in our use case. LambdaMART is inspired by LambdaRank but it is based on a family of models called MART (Multiple Additive Regression Trees). These models exploit the Gradient Boosted Trees that is a cascade of trees, in which the gradients are computed after each new tree, to estimate the direction that minimises the loss function (that will be scaled by the contribution of the next tree). In other words, each tree contributes to a gradient step in the direction that minimizes the loss function. The ensemble of these trees is the final model ( i.e. , Gradient Boosting Trees). LambdaMART uses this ensemble but it replaces that gradient with the lambda (gradient computed given the candidate pairs) presented in LambdaRank. This algorithm is often considered a Pairwise approach since the lambda considers pairs of candidates, but it actually requires to know the entire ranked list ( i.e. , scaling the gradient by a factor of the nDCG metric, that keeps into account the whole list) – with a clear characteristic of a Listwise approach . For more details, refer to [Burges et al. 2010] for the description of the three approaches in details or Wellecks’ blog posts on Learning to Rank or on LambdaMART for a nice read with great visualizations. Now, going back to our use case the first step, as usual, it’s to prepare the data that we need for training LambdaMART. The idea is to reproduce the normal behavior of our Information Retrieval system for a historical time frame since we also need the relevance score (so we need to know what happened for each <job, candidate> pair). For example, let’s assume we want to build the dataset between last October 1st and November 30th , this means that we have to collect all the jobs 𝙌 ⊆ Q created and retrieve all the candidates 𝘿 ⊆ D that were returned in those two months. Then we have also to compute the relevance scores 𝙔 ⊆ Y for those candidates ( i.e. , how far they went through the funnel) considering a temporal offset of a few weeks. Jobs created the last week of November need more time to have a comparable set of relevance scores for all its candidates. In other words, we need to store all the historical data somewhere, in order to build this training set correctly. This is fairly simple when we deal with databases or logs, but it’s a bit more problematic when we deal with a search engine such as Elasticsearch. In another blog post, we’ll discuss a trick that can be used to maintain (almost effortless) a parallel index that keeps the historical changes of our candidates’ profiles. Once we have a historical dataset, we need to train the LambdaMART model using Cross-Validation (CV) to perform parameters tuning. We are using RankLib , a popular BSD licensed library written in Java that includes, among others, implementation of LambdaMART. For alternatives libraries see this Quora Q&A , but considering the maturity and stability level of RankLib, it seems probably the best choice out there. As evaluation metric, we are using nDCG a very popular ranking metric that is computed normalizing the Discounted Cumulative Gain . Very quickly, the Cumulative Gain (CG) is the sum of the relevance scores of the documents (candidates) retrieved, if we are considering the top k positions, it is calculated as the summation of the first k relevance scores (see Fig5/a ). Since we are dealing with a ranking list, a candidate with relevance y = 5 ranked fifth should not count the same as if it was ranked first . That’s why it was introduced the Discounted CG (DCG) , that basically penalizes the relevance score in the function of the document’s position (see Fig5/b ). However, these scores don’t have a fixed upper bound limit since it depends on the number of results returned by the query, and thus in order to compare DCG of different queries we need to normalize them . To do so, we need to compute the best possible ranking (remember that we know all the relevance scores because we are dealing with historical data) and divide the DCG value by this Ideal DCG (IDCG) , obtaining the nDCG , a value between 0 and 1. This is why the nDCG is a very good metric for ranking problems. We are using it to evaluate the ranks computed by LambdaMART with different parameter settings, splitting the data with a Cross-Validation fashion. Finally, we have a model trained to compute high-quality rankings given a set of candidates’ features. We are almost done, but we should spend some time analyzing the results we’ve got. A strong advantage of the MART model, on which is based LambdaMART , is that it allows a deep exploration of the use of the features. Since they are Trees , we can investigate how the features are used and, in particular, which is their impact in predicting the right score ( e.g. , feature importance). Unfortunately, RankLib doesn’t provide any function to estimate the importance of the features , and thus we implemented it from scratch using the Gini Index to estimate the impurity of the split. If you’re not familiar with Trees and these terms, see [Louppe et al. 2014] , [Loh 2011] , or Jason Brownlee’s blog post for further explanation. This and other types of analysis sheds many lights on the features relation with the business and the customer’s needs in general, and they can give us important insights on how to improve the features engineering step (how to select the features, how to compute them, etc.). A typical workflow includes multiple iterations of the whole process , especially when these analyses show strong evidence that adding, removing or changing certain features may lead to improvements in the final ranking. With this set of three blog posts we have described, on a very high level, the backbone of the Information Retrieval framework that we used at Jobandtalent . As we have seen there are many possible configurations and decisions that can significantly change the quality of the final ranking . However, with this overview, you should have enough insights to understand how similar problems can be solved. Soon we are planning to post related articles (more advanced), re-discuss topics with more technical details (e.g., Ensemble of NERs, Query Expansion), or talk about new challenges ( e.g. , Elasticsearch training index, ETL Framework, Features Builder at scale). jobandtalent.engineering If you want to know more about how is work at Jobandtalent you can read the first impressions of some of our teammates on this blog post or visit our twitter , or check out my personal one . Thanks to Sebastián Ortega , Sergio Espeja and Ana Freire for feedback and reviews. Jobandtalent Engineering team blog. 211 4 Thanks to Sebastián Ortega , Sergio Espeja . Machine Learning Data Science Rankings Lambdamart Learning To Rank 211 claps 211 4 Written by VP of Data @Jobandtalent • AI • IR • Optimization #ORMS • RecSys • Previously @Stuart @Yahoo @Twitter @INRIA Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Written by VP of Data @Jobandtalent • AI • IR • Optimization #ORMS • RecSys • Previously @Stuart @Yahoo @Twitter @INRIA Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-07-14"},
{"website": "JobandTalent", "title": "visualkit ui framework", "author": ["Victor Baro"], "link": "https://jobandtalent.engineering/visualkit-ui-framework-74ab8aae0d42", "abstract": "Data Science Backend Product Mobile Platform Frontend We are hiring! This post covers how the iOS team has created and dealt with a UI Framework, its structure and relation with the main project, as well as the benefits and downsides of this decision. Don’t miss the post on the same topic from the android team! About a year ago, the mobile team at Jobandtalent was presented with a huge project. This new project had many new features, screens, interactions… The design team took this opportunity to improve their workflow by introducing atomic design , which defines a blueprint , with buttons, controls, cells, etc. That is, to create new screens they only used existing components and combinations of them. Based on this concept, the design team at Jobadntalent created their own design system. We applied the atomic design philosophy in our project by creating new standard components and using them in multiple scenarios. This approach allowed us to write reusable UI code. VisualKit is Jobandtalent’s UI Framework. In a nutshell, VisualKit is a big modular package of views and controls where each one of them can be easily themed and modified to fit the design, just like playing with Lego. jobandtalent.engineering VisualKit contains views and controls, including blank slates, buttons, loaders, segmented controls, switches, cells, etc. All new visual components are created in VisualKit. Let’s have a look at an example on how we break down a screen into different components. Forget about the navigation and tab bars, and let’s focus on the main tableView. Each cell has three different sections : Header: contains the avatar, labels, a disclosure and a red dot (optional). Special label: we call it ChipView and indicates state. It can be red, green or yellow. Information breakdown: contains labels displayed in two columns and multiple rows. VisualKit declares any view needed from the main project as public . In the previous case, each individual section , as well as the cell (which is a combination of the 3 sections) will be declared public . This allow us to reuse each section throughout the main project. As shown below, the same cell is used multiple times. If we make the cell modular, we only need to code it once . We simply need to: 1. Create each individual component (avatar, label, disclosure..) 2. Make the container view capable of rendering itself in different configurations by combining its individual components. StackViews are a great tool for this modular approach. For example, to create the header, we use a horizontal StackView that contains the red indicator, the avatar, another vertical StackView and the right disclosure. The vertical StackView contains the two main labels. VisualKit makes the header publicly available and configurable from the main project. The API for components like the header is very simple and resembles UIKit standard components. This view is not only ready to be used from the main project, but also from within VisualKit itself to generate more complex views. There is only one particular property, though, it is worth to pay attention to: style . In VisualKit each view conforms to the following protocol: It is a simple yet powerful protocol that makes all components behave equally. Furthermore, each component must define its own styles. For example, our ChipView has three different styles. To make the ChipView conform to our Styleable protocol, we first need to define each style. We do so by creating a public struct, where we define each aspect to be customised by each style. This same struct will also have different class initializers for the different styles. The struct initialiser is not publicly available. This restricts anyone from the outside to create and set new styles. Last, we can extend our chipView to conform to our protocol ( Note how we specify the type, in this case, ChipStyle ): This is useful, particularly when all your components conform to the same protocol. Now, from the main project, we can simply do: And we are done. Create each component to adapt itself to change between different styles is hard work, but it pays off. Once is set up, it is incredibly easy to create new styles. The same applies for buttons, cells, and may many other components. It is also useful for UIKit classes like UILabel . Here is an example: Styles: Colors and Fonts We created two abstractions over UIColor and UIFont in Visualkit, ColorPalette and FontBook . They are simple but they get the job done. Let’s start with FontBook . This class manages the full set of fonts available in the Jobandtalent app. We only use the fonts designers have previously defined. Nevertheless, there is a public function that takes a name and a size and returns the correct Font. To use it, for example, we call FontBook.extralarge The ColorPalette is, again, a series of defined colors. The problem with colors is the alpha value. Even if your designer uses UIColor.blue there is a wide range of colors that they can choose just by playing with the opacity. This is why all our colors only have seven alpha possibilities + the base color (alpha = 1). In the project, we only use colors defined inside the ColorPalette like this: view.backgroundColor = ColorPalette.dark.alpha80 One of the nicest “side effects” of having a separate UI Framework is to reuse it in different projects. We created a new specific project to play with VisualKit. The example app serves many different purposes: To develop our controls outside the main project. VisualKit is all about UI: it is quicker to test controls and see them in action without having to navigate to a specific screen buried in the app. Playgrounds are very useful for that. To help the design team validate new components. The app includes Flex , which is awesome for tweaking values in realtime. To be used as a “library”. Once the framework started to grow, we needed a quick visual reference to each individual component. Our project already managed dependencies using Cocoapods . Once we decided to create this framework, it made sense to take advantage and develop it as a private pod (check this brilliant guide for more details on how to set it up). I would like to give you my 2 cents on how easy/hard quick/slow things can become when you move most of your UI classes to an external source: Cocoapods is a great tool. For instance, when using developments pods, you are able to modify them within your project. However, every time you create a new file, you need to run pod install and obviously let your team know about it. We do not have it versioned as it is not mature enough, but it is definitely something to consider. Public , internal , open and private become more powerful than ever. When working on a project, you hardly ever use public. By default ( internal ) all files can see each other. When you encapsulate your classes in a framework, only those (classes, functions, protocols) marked as public will be available. This simple fact makes you more aware when designing your classes’ APIs and gives you more power (now you have a three layer access). You need to import VisualKit in (mostly) every view controller. We could use the .pch but VisualKit should only be available from the UI Layer. Other internal layers do not need to know or deal with it. Opensourcing VisualKit as it is today would be useless: it is coupled to our app’s design. However, everytime we create a new component we make it as an isolated instance. From VisualKit, we have already released two components: AnimatedTextInput and CardStackController . We will release new components as soon as they are ready. Everyone on the iOS team agrees that having VisualKit as a framework was a great decision. It made us work faster and respond quicker to design changes. We could argue about the usefulness of having these abstractions over colors and fonts, or having styles for each component. Is this worth it? Can’t I just go ahead and do that on the storyboard/xib or in code every time I create a new view? Obviously you can. But we should be able to respond fast to any design change. Maybe designers want a brighter blue, a new font or a faster animation. It is difficult to foresee what will change in the future, but working alongside designers can help you and save you time. Our solution tries to mimic our designers approach. We create screens in a very similar way as they do. If they update the color palette, we can update it too and everything will “react” to this change. It shouldn’t be a hassle for developers to introduce a new font across the app. Working alongside designers helped us understand their design process. In turn, designers also learnt from us, finding new solutions to overcome technical restrictions. As a final note, it might not be a developer’s job to be part of early design decisions. It is usually managed by the project managers and designers. However, during this time I have experienced that being part of that process as early as possible, will really help your team. Thanks to Luis Recuenco , Daniel García , Rubén Mendez , Rafa Aguilar , Xavier Jurado , Pol Quintana , Daniel Martín , Alexis Santos , Isaac Roldán , Andrés Brun and Ramón Argüello for contributing and making this possible. — — — — — — — — — — — — — — — — — — — — — — — — — I love playgrounds, and live updates give us superpowers. I use them all the time when creating a new component for VisualKit. Here is a simple template I use to start a new Playground. I have speed-recorded myself creating a new component. You can find it here . I believe it is a good example of a VisualKit component (atom), that we have reused multiple times as part of other views (molecules). jobandtalent.engineering If you want to know more about how is work at Jobandtalent you can read the first impressions of some of our teammates on this blog post or visit our twitter . Jobandtalent Engineering team blog. 557 1 Thanks to Sergio Espeja and Jorge Rodriguez . iOS Swift Atomic Design Mobile Design 557 claps 557 1 Written by iOS Developer @onUniverse. Previously @Dexcom @jobandtalentEng. @produkt co-founder Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Written by iOS Developer @onUniverse. Previously @Dexcom @jobandtalentEng. @produkt co-founder Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-05-07"},
{"website": "JobandTalent", "title": "learning to retrieve and rank intuitive overview part ii", "author": ["Michele Trevisiol"], "link": "https://jobandtalent.engineering/learning-to-retrieve-and-rank-intuitive-overview-part-ii-79c3791c558f", "abstract": "Data Science Backend Product Mobile Platform Frontend We are hiring! Real case candidate/job opening ranking framework at Jobandtalent (JT) In the previous post we discussed how to extract meaningful information from the input query (in our case a job position ). At this point we have (some of) our terms associated to a set of categories thus, let’s see how we can exploit them to search for relevant candidates. The goal of this step is to build a matrix which rows represent candidates, and columns feature values. This is going to be the input for the ranking algorithm. As a search engine we use Elasticsearch , released as Open Source and based on Lucene. This is a distributed search engine that allow to fast retrieve documents ( i.e. , candidates in our domain) given a structured query ( i.e ., in a JSON format). Here we can basically index any information we want to retrieve later on (at the end this is a NoSQL database) after we decide in which JSON structure we want to store it. The fact of being distributed guarantees great scalability (playing with the clustering size) and strong fault-tolerance, since we can have multiple indexes divided in various shards that might have many replicas . We won’t explain deeply this topic right now, just keep in mind that we have a set of indexes that give us the ability to search on a widely distributed set of candidates . In order to retrieve the candidates, we should decide how to design our queries , e.g. , which fields to search for, what kind of matching function we wanna use, how to pre-process the data, and so on. For any additional information about Elasticsearch you can refer to this tutorial . Now that we have our terms classified we should decide how to combine them to retrieve all the relevant candidates, in other words, how to build our query (or queries) for Elasticsearch . Note that in this step we are ideally focusing on Recall — we want to maximise the number of relevant candidates returned, but in practice there is a trade-off between Recall and the volume of candidates: the more we return, the higher the computational costs. Therefore we need to carefully design “smart” queries to retrieve relevant documents, maintaining the overall volume in an acceptable size. This step is very customisable since it depends on the domain and type of data stored. We are going to discuss some of the choices we did, but as you probably guess there are many possible decisions and configurations. In the search domain of Jobandtalent we assign different importance to each category since a match by job_position has a different impact than a match by location . For example, a good query might ensure that we have a match with the job_position ‘s terms as a constraint , before matching anything else. Applying this logic in our omnipresent example (see below) means that we won’t return any matches if the candidate’s profile doesn’t contain the term “developer”. This is a conservative approach that avoids to return noisy results. A more interesting query may assign a different weight to each term ( e.g. , by its class) in order to influence the final score by keeping into account the most important category we care about. Note that some classes might be more impactful if used separately, such as terms belonging to location or prog_lang (programming language). The reason is because we store in different fields the programming skills of the candidates, as well as their locations or spoken languages. This helps minimising the noise ( e.g. , ambiguities) for certain fields, but it depends on the way the data is collected. In our case, in the candidate’s sign up form that profile information is filled in dedicated input boxes . Another decision point is how to pre-process the text , for example, a very common practice in Information Retrieval tasks is to apply a Stemmer , a process that reduce each word to its root ( e.g. , reducing “fishing”, “fished”, and “fisher” to the root word, “fish”). This is very useful when dealing with plural terms , genders , verbs , etc., since querying with these roots return more matches. However, in our domain this is not the case for multiple reasons: first, we have a mix of languages in the same query and, since the stemmer are language-based, they will create lot of mess when applied to the wrong language. Second , as our dictionary is very specific, words sharing the same root might lead to very different matches (see below). As we can see the “develop” root is quite ambiguous since it can match with “developing” , “developed” , etc. , and “ruby” become “rubi”, that doesn’t have anything to do with the original term. There are many other choices that need to be made also regarding the matching and the scoring functions integrated in Elasticsearch. The former ones are related to the type of match (“exact match” as we use for these queries, or “fuzzy match” as we use for the Query Expansion — see next chapter). The latter ones are related to how to compute the score between the query and each document ( TF-IDF and BM25 are the most famous). Note that the score is a value that estimate the similarity between the job position and the candidate title. These examples show us how this step is extremely customisable where the choices mainly depend on the domain and on our data. If our system is well designed, we should be able to run different setups and evaluate the quality of the results to finally chose the best setting. We have seen how to build a query from the terms previously classified by the NER model, but there are plenty of other things that we can do to extent our understanding of the initial query . One of them is Query Expansion , which consists of expanding the query with words related to the input terms . This means that given a query with the following terms: if we were able to extend it with these terms: we would rely on a wider information set in finding the correct candidates. To do this we are exploiting a very well known algorithm proposed by Mikolov et al. at Google in 2013, that is called word2vec . Basically, we use it with the goal of learning to predict the best word given a context (surrounding words). In our case the context is the query itself, and we want to train the algorithm to find the most relevant related terms. This powerful approach allows us to build a representative vector for each word (word embedding), that we can use to perform different operations such as distance between terms, clustering and so on. Note that there are many other algorithms to do words embeddings, we are also using GloVe (by Stanford), and FastText (by Facebook) for example. For more information and examples check out this blog post and take a look at “ Query Expansion with Locally-Trained Word Embeddings ” (Diaz, 2016). The tricky part now is the preparation of the dataset that we use for training. There are not available datasets out there related to our domain, thus we have to prepare it from our historical data –full of noise, mistakes and ambiguities. The quality of the final suggested terms depends mainly on which data we use, how did we prepare it (cleansing, pre-processing), and how we train the final model (parameters tuning, evaluation, etc. ). When the Query Expansion model is well trained, we move it to production and we call it every time a new job/query arrives. Then, with the expanded terms we build another query for Elasticsearch that can contribute in retrieving and scoring more (hopefully relevant) candidates. In our case the scores returned by word2vec are used to build the weights as shown in Fig4. We have discussed multiple ways of creating queries, and thus searching in our collection, with the goal of finding as many relevant candidates as we can. However, keep always in mind the overall computational time, since the result set needs to be processed later on, each query needs time to be executed, and we definitely don’t want to crash Elasticsearch when fed by a burst of requests. A first solution is to use a simple cascade approach that executes: first, the queries with higher specificity (that in our tests return the most accurate candidates but not always enough results ) and then, a series of fallback queries that are less accurate but always able to return many candidates. The fallback queries are executed only when the number of candidates returned by the former one is not sufficient. What we mean by “enough candidates” are values that depend on our historical experiments: based on certain classes (categories, country, etc.) and on the browsing behaviours of the users ( e.g. , how many candidates are visited in average). Finally we get a set of candidates that, for different reasons, have shown to be relevant for the given job. Now, the only features related to these matches are the scores (between the job and each candidate) computed by Elasticsearch. However, we need far more information in order to do a better job in optimising the ranking of this pool of candidates, and that’s why we rely on two types of features: online features : these are the information collected in real-time about the matching between the job and the candidate, such as the ElasticSearch score. offline features : these are all the information about the candidate (or the company) that can be pre-computed based on historical behaviour (preferences, actions done, availability, etc. ). The offline features play a key role here because they are very affordable to compute ( e.g. , they can be pre-computed on a daily/hourly bases) and thus they can bring tons of additional information needed by the algorithm(s) to optimise the ranking. Sample of candidates offline-features includes: previous job applications, average distance (km) that the candidate is willing to perform to accept a job, historical performance of the worker, and so on. These features are mainly computed by our ETL on a clean and very scalable design , but this will be described in a completely different post. Finally, we have our huge matrix containing a set of potential candidates, with a list of features freshly computed ( online ) or collected from our data warehouse ( offline ). And now, it’s time to move to our ranking step. — Part III is here . jobandtalent.engineering If you want to know more about how is work at Jobandtalent you can read the first impressions of some of our teammates on this blog post or visit our twitter , or check out my personal one . Thanks to Sebastián Ortega , Sergio Espeja and Ana Freire for feedback and reviews. Jobandtalent Engineering team blog. 170 3 Thanks to Sebastián Ortega , Sergio Espeja . Elasticsearch Machine Learning Query Understanding Data Science Programming 170 claps 170 3 Written by VP of Data @Jobandtalent • AI • IR • Optimization #ORMS • RecSys • Previously @Stuart @Yahoo @Twitter @INRIA Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Written by VP of Data @Jobandtalent • AI • IR • Optimization #ORMS • RecSys • Previously @Stuart @Yahoo @Twitter @INRIA Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-07-14"},
{"website": "JobandTalent", "title": "learning to retrieve and rank intuitive overview part i", "author": ["Michele Trevisiol"], "link": "https://jobandtalent.engineering/learning-to-retrieve-and-rank-intuitive-overview-part-i-5340fcf4a863", "abstract": "Data Science Backend Product Mobile Platform Frontend We are hiring! Real case candidate/job opening ranking framework at Jobandtalent (JT) This is a very high-level introduction about how candidates (documents) are retrieved and ranked given a new job (query). We are going to share and discuss part of the framework we have built at Jobandtalent , and some of the ongoing projects we are working on to improve it. With this post, we aim to help you in building your own systems to start solving similar challenges. For the records, Jobandtalent is a platform that aims to find the best employees for companies and the best employment for candidates, taking care of the whole funnel, from the search till the signature of the contract. This first article discusses the topic of Text Understanding , in particular on how to process the job position (or query if we are speaking about search) to have some insights about its content. Fig1 shows an overview of the framework that we’re building at Jobandtalent that takes care of retrieving and ranking the candidates. Some steps are already used in production while others are still under development and basically everything is always under constant improvement . In any case, this post will give you an idea about where we are heading. In between a job is created and a set of candidates is retrieved, many things happen, and we can summarise them in three main phases : Text Understanding . Among the information of the new job, the textual data is by far the most important and thus it’s critical to understand and process it in the correct way. Here Natural Language Processing ( NLP) techniques come to help. Candidate Retrieval . Once we know what we’re looking for, there are multiple ways to search for it, and we need to design the infrastructure in a way that can support our needs. Information Retrieval is required. Candidate Ranking . Now that we have the (possibly huge) set of candidates, we want to rank them in order to place higher the most relevant ones. Learning to Rank techniques are exactly what we need. Let’s discuss these steps with simple explanations and a few examples. Let’s assume we receive the following input query ( e.g. , job search): “Senior Ruby developer working remotely” In order to understand a sentence, we need to identify the meaning of its words. A common approach is to work at the term-level to automatically identify in which category a word will fit. The categories are pre-defined and should be designed in a way to help us structure the information they contain. For example, let’s assume we can extract the following classes: And then exploit them to better retrieve candidates, for example using the location to filter candidates by their geographic preferences, prog_lang ( i.e. , programming language) to emphasize developers that own such skills, seniority to select candidates with the correct experience, and finally job_position as the role that we are looking for. There is a set of techniques known as Named-Entity Recognition (NER) that handle this type of tasks. There are fairly good libraries available on the Web for a quick start, two well-known examples are Apache OpenNLP and Stanford NER . The workflow to train them is a classic one for supervised Machine Learning problems, that we can summarise as follow: First, prepare the dataset considering the algorithms and the settings that we are going to use. For example, in order to recognize a specific entity ( e.g., a job_position ) we need to be sure that it appears enough times in our dataset, otherwise, certain algorithms won’t be able to learn how to recognize it. The annotation step is very time consuming but it’s the most important one. Here we create the “ ground truth ” the algorithm will use to learn what and when to recognise. Unfortunately, there aren’t available datasets ready to use on our domain, meaning that we need to build the whole annotated set by ourselves. This is a long (and quite boring) manual task, that deals with text homonyms/heteronyms , typos , ambiguities , incomplete descriptions , etc . Moreover, our domain has its own terminology and often a mixture of languages, e.g., job_position in Spain contains also English terms. All of this makes the classification extremely tricky, and that’s the reason why a manual step will guarantee a better ground truth . These are some common examples: There are terms that can be classified as job_position (“pizza” in “pizza chef”) in some cases, while discarded in others (“Delivery Driver” without “pizza”). These are examples where the manual annotation makes the difference. For example, an algorithm should understand that the term chef when surrounded by certain nouns (e.g., pizza chef , sushi chef , sous chef ) or when followed by certain prepositions and nouns (e.g., chef de partie ) means different job_position , and it should also recognise when the nouns should not belong to the same class ( e.g. , head chef , full time chef , event chef ). The manual annotation allows us to reach this level of accuracy, it’s an investment of time and resources but it is definitely worth the effort. Finally, the model training step which, if well designed, could be pretty straightforward. It’s always recommended to tune the parameters with cross-validation ( i.e. , splitting the data in a way that reduces the chances of over-fitting our model) and use metrics that well suit our classification problem. For example, when dealing with job_position we want to minimise the missing classified terms (otherwise we will have a very hard time to find related candidates), thus we are interested in emphasizing Recall over Precision, F1-Beta score comes handy. The methods we are using will determine the complexity of the feature engineering task. You can think about a feature as a value that gives us information regarding some characteristics of the data. In our case, if we want to classify a term as a job_position the surrounding terms and the type of those terms (nouns, adjectives, etc.) might be useful features. These features strongly depend on the algorithm we are going to use. For example using a method based on Conditional Random Fields ( e.g. , Stanford NER) we can select the sliding windows of the terms to consider, design POS tagging successions ( adjective followed by a noun ), and many other characteristics (terms finishing in -ing, plurals, etc.). The contribution of the features needs to be studied and evaluated in order to find a good setup with respect to the final accuracy of the model. For more information, check out this overview of variable and feature selection . We have discussed a possible flow that can be implemented to extract insights ( classes ) from our initial text ( query ). In the next article, we are going to see how these insights can be used to retrieve a pool of candidates. A little parenthesis about a more advanced approach that we’re building at Jobandtalent, based on the votes of different NER models . We basically train (as we’ve seen in Fig3) multiple models and/or multiple times each model with different parameters. The idea is to build a series of models where each one can learn slightly differently from the data, with the assumption that at the end, the votes cast will lead to better final accuracy. Once each model classifies the input text with its own “experience”, a simple approach is to take the weighted average of the votes to infer the ultimate classification. Another approach consists of training a model that can define customized weights, which is called meta-modeling. See how Stacking works (a popular approach in the Kaggle challenges ). Below a simple representation of our ensemble of NERs. We basically exploit the diversity of the models, with the assumption that considering many different approaches leads to better overall accuracy than listening to just one of them. Apache OpenNLP, for instance, is based on Maximum Entropy Modelling , where Stanford NER is built upon a derived class of techniques called Conditional Random Fields (CRFs). Other models becoming popular are based on Word-Embedding and RNNs . They are all great models that tend to learn the same information but in different ways. After this little digression, let’s move to the next article to see how to use the information we have inferred from the text in order to retrieve the initial set of candidates. jobandtalent.engineering If you want to know more about how is work at Jobandtalent you can read the first impressions of some of our teammates on this blog post or visit our Twitter , or check out my personal one . Thanks to Sebastián Ortega , Sergio Espeja and Ana Freire for feedback and reviews. Jobandtalent Engineering team blog. 134 Thanks to Sebastián Ortega . Machine Learning Data Science NLP Modeling Programming 134 claps 134 Written by VP of Data @Jobandtalent • AI • IR • Optimization #ORMS • RecSys • Previously @Stuart @Yahoo @Twitter @INRIA Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Written by VP of Data @Jobandtalent • AI • IR • Optimization #ORMS • RecSys • Previously @Stuart @Yahoo @Twitter @INRIA Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-07-14"},
{"website": "JobandTalent", "title": "why i use story points to estimate tasks", "author": ["Santi Bel"], "link": "https://jobandtalent.engineering/why-i-use-story-points-to-estimate-tasks-d912fb15e821", "abstract": "Data Science Backend Product Mobile Platform Frontend We are hiring! One of the essential parts of SCRUM is cost estimation. Some authors even recommend estimating in 2 iterations. Very fuzzy and intuitive first estimation, which gives you a general idea of what can fit in the next Sprint. A second estimation, specific in real time amounts. It is usually made in the Planning meeting. In my case, I don’t do the second estimation with real time. I think it is an extra effort that doesn’t pay the bill. In the other hand, I think that the fuzzy-and-intuitive cost estimation with Story Points is great :). I like to think about Story points like perfect world points. In a perfect world, having that new cool feature would take: a small amount of time? — 1 Story Point half a day? — 2 Story Points 1 full day? — 4 Story Points 2 full days? — 8 Story Points With these estimations I assume that I won’t have any interruptions, I’ll be full of energy, I won’t get blocked by external factors and the first attempt will be the definitive one. This is not a perfect world and it will never take this amount of time. However, it does work great for relative cost estimation . Also, notice that there is nothing below 1 Story Point. Nothing takes less than a “Small amount of time” and there is nothing above 2 full days of work. If it takes more time than that it should be split. The important thing is not the conversion of Story Points -> Real Time. The really important thing is to be consistent and to keep the same scale Sprint after Sprint. If you always keep the same scale, after 6–8 Sprints you will be able to determine the Velocity. You will get to see if 2 Story Points are really half a day or 3 days. You will also see that it keeps the proportion. If 2 Story Points become 3 days of work, then 4 Story Points will be 6 days. Yes, there are some inaccuracies on this scale. However, they are significantly reduced. The error is distributed throughout all the estimations and by each team member. So? On average, the estimation tends to be in equilibrium and converges to an amount of real-time (Velocity). Imagine that Velocity is like a dart player that knows his statistics well. 7 out of 10 darts will hit the bullseye. He doesn’t know which shots are the good ones but he is confident anyway. You can get the Velocity by using this method. That’s what we actually need. E.g. When your team performs 100 Story Points on average in a 2-weeks Sprint, You know that at the end of the Sprint, ~100 Story Points will be done. Good. :) You don’t know how many Story Points will be done in the first half of the Sprint. Who cares? It’s ok. :) With this simple estimation scale, you can get all the benefits. There are many benefits to estimation, time calculation isn’t the only one. The concept of the budget appears. This budget will be used for better strategical decisions. New features will be evaluated looking at its impact and looking at its estimated cost as well. Visibility of refactoring or architectural costs. When the amount of Story Points converges into a number for 2–3 Sprints in a row you can save a fraction of this Velocity for refactors, version updates or other tech issues. Stakeholders learn about complexity and cost. When getting used to this simple notation of Story Points stakeholders are able to understand better the product. Knowledge sharing while discussing estimations. It is a good way to get the whole team aligned and focused. In case you are interested in knowing more, I recommend this book for understanding the methodology. Essential Scrum: A Practical Guide to the Most Popular Agile Process by Kenneth S. Rubin Jobandtalent Engineering team blog. 3 1 Thanks to Alfonso Jiménez . Agile Scrum Project Management Estimation Product 3 claps 3 1 Written by Senior Software Engineer @ Jobandtalent. Previously CTO @ Playfulbet. Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Written by Senior Software Engineer @ Jobandtalent. Previously CTO @ Playfulbet. Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-05-07"},
{"website": "JobandTalent", "title": "j t tech talk go 101", "author": ["Álex Go{,5z}"], "link": "https://jobandtalent.engineering/j-t-tech-talk-go-101-4045956c84d1", "abstract": "Data Science Backend Product Mobile Platform Frontend We are hiring! In this Tech Talk, I guide us through an introduction to Go — the programming language created by Google back in 2009. Using practical examples from our daily work here at Jobandtalent , this presentation introduces interesting features of Google’s programming language, such as channels or the treatment of errors. From the benefits and uses of Go to the drawbacks, everything you ever wanted to know about this programming language can be found right here! Check out my Tech Talk presentation above (FYI: it’s in Spanish), or read through the slides below for a quick overview… Now tell us what you think! Do you use Go in your daily work? What do you like (or not like) about it? Jobandtalent Engineering team blog. 7 Programming Golang Backend Software Development 7 claps 7 Written by Architect @jobandtalentEng. Break things and run fast (yep, correct order & correct words). Go, k8s, Elixir, Docker & basically whatever that is interesting! Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Written by Architect @jobandtalentEng. Break things and run fast (yep, correct order & correct words). Go, k8s, Elixir, Docker & basically whatever that is interesting! Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-11-21"},
{"website": "JobandTalent", "title": "test doubles in swift", "author": ["Daniel García"], "link": "https://jobandtalent.engineering/test-doubles-in-swift-fd9303f2591", "abstract": "Data Science Backend Product Mobile Platform Frontend We are hiring! One of the first things I realised when I started coding in Swift was that strict type check would force us to adapt the way we test our applications. Back in Objective-C, whenever we wanted to mock a class or create a stub, we simply had to use libraries like OCMock or OCMockito . These libraries can easily create test double for us by simply letting them know the class or protocol that we want to conform to. They take advantage of the Objective-C runtime, that ease the path to mocking classes. But the way type checker works under the hood plays a big roll as well. Type check in Objective-C occurs basically using Duck Typing. What is Duck Typing? Objective-C uses both dynamic and static typing. You can declare objects of type id and send them any message, and objects can decide at runtime if they will respond or not. This allows you tell the compiler that an object is of a different type, by casting it, as long as the new object responds to the same methods. What happens if we try to access a property or method? As Objective-C will use dynamic dispatch for all property and method access, if our object with the new type responds to the same methods and properties that the previous type, we won’t get any error and our code would simply work. This is called Duck Typing. “If it quacks like a duck, it walks like a duck,… to me, it’s a duck” If it responds to the same methods as type X, treat it as if were a type X Don’t check if it IS a Duck, check if it BEHAVES LIKE a Duck After this introduction to Duck Typing, why is this important when talking about mocks in Cocoa? Most Objective-C mocking libraries take advantage of this feature to create mocks. You ask for a mock of a given type and you get a Mock class, that is normally implemented using an NSProxy subclass casted to our desired class. This is a real problem if we want to mock using this approach in Swift, because now Swift compiler uses strict type check and doesn’t support Duck Typing. If a method receives an object of a type, we will only be able to pass an object of that specific type. Let’s imagine that we are building a job search application and we are coding the part where our users will be able to apply for a job. We could have something like this: We are using a Model View View-Model (MVVM) architecture, and we want to test whether our viewModel communicates with the injected JobsApplicant . JobsViewModel will be our \"System Under Test\" Our first approach will be mocking JobsApplicant manually. How? We will create a class JobsApplicantMock that will conform JobsApplicant interface. Nice! But we want to make sure that the view model talks to its jobsApplicant to apply to the proper job. Now we can check that our integration works as expected NOTE : In this example, we are defining JobsApplicant as a protocol. This is considered is a good practice when defining dependencies, because it eases the work of creating mocks, and also makes our code follow the Dependency Inversion Principle . Apple even gave a talk about Protocol-Oriented Programming in Swift. But the truth is that we don't always have our dependency defined as a protocol, but as a specific type. This makes it a little bit harder to work with. We can tackle this scenario following one of Michael Feathers' \"Working Effectively with Legacy Code\" Book techniques called \" Subclass and Override \" The previous approach works fine, but it may add a lot of boilerplate to our tests doubles, couples them to a specific test case, and makes them less reusable. Fortunately, there are some libraries that make this process easier for us. Some of the most populars are Dobby , SwiftMock or MockFive . We are going to use Dobby in the following example. These kind of libraries give us a simpler way to track function calls and create stubs. Now we can check that our code works as expected As you can see, changes are subtle but we have already gained the ability to reuse our JobsClientMock class in different tests, as we can define different expectations for each method mock. Swift strict static typing means a huge step forward in iOS development, but it has its drawbacks. One of them, is that we must satisfy all the dependencies of a class in order to instantiate an object. Imagine that JobsApplicant , our dependency previously defined by a protocol is now a specific type and we will be using the \"Subclass and override\" pattern to create mocks. We would have to instantiate all the dependencies of a class in order to mock it. This could be really painful because we will also have to instantiate all the dependencies of our dependencies. And also the dependencies of the dependencies of our dependencies… and so on and so forth Creating the whole dependency graph for each test may not be an option. Imagine the following piece of code: Our JobsClient has an apiClient dependency, and the latter may have some other dependencies. We need a way to break the dependency graph, at least for the dependencies that we are not going to use in this test . We can create objects with empty implementations just to satisfy these dependencies that we are not going to use. This kind of test doubles are normally called Test Dummies We can workaround this forcing the compiler to treat an object as if it were of a specific type using unsafeBitCast function. If we create a simple factory that receives a certain type and returns an instance of it, we will be able to satisfy these dependencies without having to actually instantiate them. Let’s see how this would work… As you can see, we are satisfying the dependency and breaking the dependency graph as we don’t have to satisfy APIClient dependencies. IMPORTANT NOTE : This will work as long as we don’t use the injected dummy dependency. If we try to make any call on this dummy object, we will get a runtime exception ( EXC_BAD_ACCESS ). Swift adds really important new features related to static type check. This makes our ability to create mocks and stubs a little bit harder, but there are always other ways to do it. Jobandtalent Engineering team blog. 22 Swift iOS Testing Unit Testing Mobile 22 claps 22 Written by iOS Developer @monzo @produkt Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Written by iOS Developer @monzo @produkt Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-11-16"},
{"website": "JobandTalent", "title": "https jobandtalent engineering hello world", "author": ["Sergio Espeja"], "link": "https://jobandtalent.engineering/https-jobandtalent-engineering-hello-world-bc38d2b97f33", "abstract": "Data Science Backend Product Mobile Platform Frontend We are hiring! Jobandtalent is the world’s leading job marketplace that matches SMEs with local talent, and is the first end-to-end, mobile solution to the whole process of recruitment, hiring and staff management. Building a global job marketplace that provides market liquidity and efficiency is not a small engineering feat. With more than 80 people in the product and engineering team, we’re going through many technical and organizational challenges that we’re figuring out along the way, whilst standing on the shoulders of giants that did it before us. In this blog, we’ll tell the behind-the-scenes stories about building our product from the point of view of software engineers, designers, product managers, data scientists, and QA. But the technical design of our 11 million users marketplace is only one part of the story: we’re facing many other challenges, from the recruiting of passionate people to team organization as we scale. We want to share our experiences and lessons learned on all those fronts to help other teams and startups, but also to keep learning and growing in the process. We hope you enjoy reading about all of it as much as we enjoy building our product! Jobandtalent Engineering team blog. 8 Startup Engineering Product 8 claps 8 Written by CTO at jobandtalent Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Written by CTO at jobandtalent Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-05-07"},
{"website": "JobandTalent", "title": "whats new in ruby 2 4", "author": ["Alfonso Jiménez"], "link": "https://jobandtalent.engineering/whats-new-in-ruby-2-4-f6e4fdd1a2b4", "abstract": "Data Science Backend Product Mobile Platform Frontend We are hiring! The first release candidate of Ruby 2.4.0 was recently published. And the final release is scheduled for this week. We would like to share some of the features coming in this new minor version. Now it is possible to sum all the items of an Enumerable object. Enumerable#sum supports an initial value, which by default is 0 . When summing a collection of non-integer objects, you need to explicitly specify an initial value. Fixnum and Bignum have been unified into Integer , getting a much more cleaner interface. Bignum objects hold values outside the range of Fixnum . At the moment, long numbers are automatically converted to Bignum when they are greater than 2**62-1 (or 2**30-1 in x86–32 archs). This is one of the compatibility issues of this release. At Ruby level, both Fixnum and Bignum constants are bound to Integer , but some inconsistencies could happen: Dir and File implement a new class method in order to check if a directory or a file is empty. Ruby 2.4 also supports natively Hash#compact and its bang version. It performs ~25% better than Hash#compact ActiveSupport method . Hash#transform_values ActiveSupport method and its destructive version have also been ported to Ruby 2.4. It performs a transformation to each hash value. String#upcase , String#downcase , String#capitalize and String#swapcase (and their bang variants) are no longer limited to ASCII characters. Regexp#match? returns true or false whether the regular expression is matched. This method is much faster than any other, since it avoids creating a MatchData object or saving backref: A place-value notation of an integer number can be extracted by using the new #digits method. The default radix is 10 . It accepts any other base as an argument. For example: Ruby 2.4 Array defines its own #min and #max instance methods instead of using the Enumerable implementations. This notably increases the performance. Net::HTTP standard lib introduces a new class method called post : … and much more. Check out the Ruby 2.4.0-rc1 changelog at GitHub . Jobandtalent Engineering team blog. 15 Ruby Programming Development Backend Software Development 15 claps 15 Written by Co-Founder of Furgo Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Written by Co-Founder of Furgo Jobandtalent Engineering team blog. The magicians that match people with the right jobs. How do they do it? We are Hiring! http://bit.ly/jt-is-hiring Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-12-22"}
]