[
{"website": "Moove-It", "title": "giving-developers-a-seat-on-the-table", "author": [" Guillermina Pose "], "link": "https://blog.moove-it.com/giving-developers-a-seat-on-the-table/", "abstract": "Giving developers a seat on the table The digital economy has propelled a new perspective towards software development and deployment. Jeff Bezos, Ex-CEO of Amazon, once said that his business wasn’t the content of their deliveries, but the software that made it possible to send those boxes to their customers. The final outcome is not necessarily the main occupation of the company, for without a platform, or software that would allow all transactions to take place, there wouldn’t be a tangible product. Companies like Uber, Lyft, or Spotify are focused on creating impact. Among many of the reasons that set them apart from other not-so-successful companies, there is one that stands out: quality software. Now more than ever, creating impact resides in originality. The moment is now to provide developers with the room to be creative rather than just coders. Quality software is not built in one day. A lot of companies embark in the adventure of adopting agile methodologies thinking this would solve all their problems. Not long after that, they realize nothing has changed. A profound change in the mindset of the company is needed. Listen to the ones with the ‘know-how’ Developers should be able to express opinions and make decisions about the technology they are developing. All in all, they are the ones with the deepest and more detailed knowledge on the subject. Juan Pablo Delprato , Developer and Team Leader on Moove It , thinks that one of the most frustrating issues for developers is that generally “they are not the ones who define WHAT needs to be done, but only HOW is supposed to happen”. He argues that being the ones with more expertise in the HOW, they are able to observe things that other team members in roles more related to management don’t see. “When you are the one seeing those things first hand, you generate questions or ideas about the WHAT more than the HOW, even more so than people who are strictly related with the WHAT (PM, PO, clients, stakeholders). Many times that ideas are of great value to the product”. If developers were to be introduced to the discussion early on, problems that may occur or objectives that don’t align with realistic possibilities, could be seen before the planning stage. Assign problems, not tasks. Developers can be described as being many things, one of them is problem-fixers. But assigning a task without giving context, reasons why it should be done, or contemplating other possibilities to solve it might result in bad quality software. Humans find motivation and passion on activities that involve problem solving. That gives purpose and demonstrates the confidence we have in other people’s creativity. Juan Pablo illustrates this problem with a clever metaphor: “If someone says: we have to lift this rock, how can we do it?, is not nearly as motivational as someone saying ‘This rock is on the way to discover the cure for cancer. We need to lift it to save millions. How can we do it?” An interesting thing here is that you would still be asking HOW while giving purpose, reason and context, three crucial aspects to achieve any goal. Keep developers close to customers A big problem that many developers have experienced, is being in the dark regarding the customer. They’d receive a task, finish it, handle it and the Team Leader or the one in contact with the client, would be the one explaining it and probably taking credit for it. In Moove It , things are done differently. Gabriel Fagundez, COO, explains that “our developers are always in contact with the client. They speak their minds in meetings with clients, and are always encouraged to share their expertise to upgrade the product”. Whether it has to do with core decisions, design processes, possible new features, or even discussing the size of a call to action button, the dev team is present. Developers working in Moove It are involved in the decision process from the beginning. They are the ones being consulted to know if a feature or solution can be achieved and how to do so. As experts on the subject matter, they are frequently challenged to find different ways to solve a problem, along with the PM, PO and other members of the team. Moreover, many of our current Team Leaders are in fact developers who previously worked as part of other projects and teams, and now are leading their own. The teams have daily meetings and weekly sprints to make sure that the project is always on the right track, the members of the team know why they are doing what they’re doing, and the user stories and personas are eventually being questioned and improved. Tolerate failure and keep up your continuous delivery pipeline Companies might benefit greatly from applying a culture of experimentation. Procuring an environment where failure is not punished, but conceived as a learning opportunity. This could mean giving developers the possibility to experiment and to fail in order to succeed. These techniques along with some of the ones previously mentioned, are key to maintaining your continuous delivery pipeline and actually discovering if your software is working or not and why, ahead of schedule. It’s mostly about delivering, creating robust MVP’s so that real waste is minimized. Although there are no magic recipes, maintaining an open line of communication and encouraging all parties to speak their minds and collaborate towards projects’ success with weekly or daily iterations will always result in good outcomes.", "date": "2021-02-03"},
{"website": "Moove-It", "title": "increasing-representation-women-technology-can", "author": [" Guillermina Pose "], "link": "https://blog.moove-it.com/increasing-representation-women-technology-can/", "abstract": "Increasing representation of women in technology: how can we do it? Where do we come from? In 2021, we commemorate the 46th anniversary of Women’s day, on March 8th. These last couple of years, the visibility towards gender disparity has grown. Although we get daily reminders from our colleagues, family, partners, friends, and the media, the inequity is still very real. We can all agree that talking about it is crucial to introduce it into our world’s agenda, but it is not enough to just talk, we need to take action and be part of the change we want to see in society. Where are we now? Most industries have different examples of how the gender gap reflects on their daily activities, but when it comes to the IT industry, the problem runs deeper. The fact that women grow up believing they are not capable of working as engineers, software developers, technicians, or any of the more technical roles associated with the industry, is due to the understanding that we have as to what roles, jobs, or activities should be for whom. Moreover, even when some smart, capable women adventure themselves entering this industry, they often face walls of disparity. Maybe in the form of a recurrent male figure as a boss, or rather in condescending attitudes, or even with more subtle approaches such as silencing women in meetings of not providing everyone with the same space to talk. Women often have to “prove” they deserve a certain job, for the preconceived notions say they are not capable, so they have to do more, be more, to prove themselves. What does gender have to do with intellectual capacity? Not a thing. Why do women have to put up with workplace harassment, above all the different types of abuse they already go through on a daily basis. What are we doing? In Moove It, we know and acknowledge this as a reality, that’s why we founded the Gender & Equality Commission, and we work daily to nourish an environment where gender is not a reason to diminish nor praise someone, but actually welcome every single difference and particularity an individual has. Moreover, we work to debug the gender gap and make Moove It an equal, safe place to work. The Gender Gap in the IT industry represents a worrying situation. Of course, we should be glad that more women join this field, but that can’t prevent us from seeing that world statistics still show an 80 – 20 % advantage men have over women by occupying IT roles. We are aware of this, and we keep trying to come up with new ways girls can be encouraged to study IT-related careers. So it is not a matter of whether we should or not up that percentage. It is a matter of how. Working together is how. Working to teach our children, our nephews, our cousins, our acquaintances. Teaching them that any child can choose what career they want and that they shouldn’t be biased to follow one path or the other because of specific attributes associated with gender. Letting them know they are capable, qualified, and skillful. Working to make sure women feel comfortable and represented. Everyone should be able to develop in a fair and just environment. We wanted our women in Moove It to speak their minds, so we asked them this question: ¿Could you name at least one aspect in the IT industry as a whole, that needs to change NOW in order to debug the gender gap? Some of the answers were: “The idea that women are not capable enough to work in this industry must disappear. It has caused female employees to work even harder to try to demonstrate their worth. It is an unnecessary barrier” Jeasmine Ñahui , Mobile developer at Moove It . “In order to bridge the gap, it is necessary to inform girls and teens of the benefits of the industry, share experiences, and break the myth that it is a men’s career. Talks can be organized in schools to achieve this and get their attention when they are deciding which career to choose” . Says Victoria Bernini , Operations Analyst at Moove It. “ We (women) need to stop being a minority in the IT sectors. This can only be achieved by dropping off masculinized stereotypes on the IT industry that’s teaching girls that they (we) “don’t have what it takes” for it. Girls need to be encouraged to try out STEAM stuff (read, watch, play, etc) and see that other women are succeeding in the area. In the workplace, “micromachism” (and other not so micro) needs to be identified and stopped. There are lots of daily situations that we need to put up with and most men don’t even realize (or worst: care). Finally, we should not be worried about how maternity is going to affect my work position and professional development. Maternity and Paternity leave needs to be redefined to give the same possibilities to both parents and companies should protect mothers jobs and support their professional development as well .” Maite Mañana , Senior Developer & Project Manager at Moove It. Maria Noel Burghi , CFO, has been working for over 13 years in Moove It, and she thinks that “ Many things should change so that everyone can access great personal and professional opportunities. We should work towards eliminating gender stereotypes. As adults, we need to be aware of what we communicate with our actions and words. It is highly important to achieve equality in the workplace. Choosing to be mothers or not should have nothing to do with growing in our careers or getting where we want to be. Every aspect of our lives should be measured equally so that there is a system that allows women to develop and grow outside their families. It is crucial as a way to eliminate gender violence. Having equal rights is the first step towards a more fair society. We have a long way to go” . Gender equality it’s a must, and we have to do anything in our power to guarantee it not only to fellow women but most importantly, to every girl who is wondering what would happen when she grows up. We hope more women feel comfortable enough to join us in this industry. We are working towards a healthier society, and the first step is taking daily action. Join us at Moove It and rest assured we will keep trying to make the world a more fair and just place.", "date": "2021-03-05"},
{"website": "Moove-It", "title": "how-to-successfully-present-an-idea-at-work", "author": [" Guillermina Pose "], "link": "https://blog.moove-it.com/how-to-successfully-present-an-idea-at-work/", "abstract": "How to Successfully Present an Idea at Work Creativity is everywhere. Whether you are an engineer, artist, lawyer, or chef, you have unique ideas. Across all origins and sizes, all ideas share a common trait: they will never become anything more unless voiced and acted upon. The process of making your idea a reality starts with the communication of that idea. The effective communication of this idea is the first step because it will determine whether you have supporters. When deciding how to present your idea, you should first focus on your audience. Always remember that you are playing the role of a speaker and presenter when you share your idea, so keep your audience in mind. Two main factors when choosing who to tell your idea: Is this person in the position to make decisions? If not, can she/he connect me with someone that is? Does this person have the time to actively listen to my proposition? Is my idea relevant to this person? This doesn’t mean you have to tell your idea to someone in a high-power position. Your audience may be an expert whose opinion is valuable. They can be someone who can link you with the right individual, or they can be a colleague that is simply willing to help you flesh out your idea. Knowing who to talk to and when to ask for a few minutes of someone’s time is highly important. It shows you know about strategy, that you are organized, and that you understand how your internal processes work. Who is this message for? Who in the company will this message resonate with most? Let’s say that you made your inquiries and know that the right person to pitch your idea to is your company’s CEO. As much as they want to listen to you, they are likely pressed for time. So in time-sensitive situations like this, you must learn to be precise. Avoid a storytelling format and go straight to what’s important. What problem or issue does your idea solve? And how does it do it? Linking your idea to a problem that needs to be solved will make its value more clear. If you cannot find a problem, keep looking. Ideas frequently come from the realization that something is not working as it should, and can be improved. Make that problem visible, then present your solution. People that can “sell” their ideas aren’t geniuses or magicians, they just believe in what they’re saying. You cannot expect someone to buy into your idea if you don’t believe it yourself. Make sure to convey that conviction with your body language, tone, and word choice. “So, how do you show that conviction?” Studies have shown that body language, vocal tone, and word choice both communicate your ideas and re-shape your own feelings and beliefs. One of the most cited studies on the importance of verbal and nonverbal messages in personal communication is one by Prof. Albert Mehrabian of the University of California in Los Angeles. His studies show that we overwhelmingly deduce our feelings, attitudes, and beliefs about what someone says not by the actual words spoken, but by the speaker’s body language and tone of voice. Prof. Mehrabian actually quantified this tendency: words, tone of voice, and body language respectively account for 7%, 38%, and 55% of personal communication. So, how much can an “empowering” posture help you build up your confidence? If you communicate with a drooping back and shoulders or simply have a posture that conflicts with your message , it weakens your message and raises doubts in your audience. Imagine you are not 100% convinced that your idea will work, but you feel it is worth trying. Practice projecting confidence through your posture, intonation, and general body language, and check the results with someone you trust. Would they buy that idea? That may give you some insights to improve your presentation. Trust that you are there because you are valuable to the company and the team, and thus have a perspective worth giving. Overall, some tips from the article: Find the right stakeholder to pitch your idea. You shouldn’t approach someone just because they are “important” in your company. Your main goal is to find someone whose inputs may be valuable for you. Link ideas to problems to be solved. Creativity is great, but effective creativity is better. Make sure your idea is solving or at least relates to the problem. Non-verbal communication can re-shape the way you feel and think about something. Use it in your favor. Project confidence. You are the first buyer of your idea. If you don’t believe in it, no one will. Trust that you are part of the company because you are valuable and that what you have to say is important. Whether you’re in a daily meeting, ideation session, or casual discussion, try to explain your idea clearly and remember that both you and your audience need to be engaged for effective communication.", "date": "2021-05-02"},
{"website": "Moove-It", "title": "women-stem-close-gender-gap", "author": [" Guillermina Pose "], "link": "https://blog.moove-it.com/women-stem-close-gender-gap/", "abstract": "Women & STEM: what are we doing to close the gender gap In 2015, the United Nations established the International Day of Women and Girls in Science (11 February) to raise awareness about the importance of diversifying STEM (science, technology, engineering and mathematics). It was not so long ago though, (1975), that women used to dominate these sectors. Grace Hopper, Ada Lovelace, Katherine Johnson, Dorothy Vaughn, and Joan Clarke, were all women pioneers in computing. Things have changed, and not exactly for the best. Nowadays, women hold only 25 percent of computing jobs in the United States. This is a global tendency. We are taking action Moove It embraces every chance to vindicate the importance of debugging the gender gap and actively encouraging more girls and women to join the industry. We support causes like Anima , a technological high school where teenagers can learn what is necessary to work in the industry. To join the World celebration of Girls in ICT, every April 23rd we join and support the #Techyxeldía activity and try to raise awareness about the industry situation. We also sponsor the Ricaldoni Scholarships, so that students living under precarious situations can finish their computing sciences degrees. More could and should be done That’s why Moove It founded an Equity and Gender Commission. Gladly, not only women but also men are part of this commission, working together to debug the gender gap, and to bring more equity to the company, and hopefully, the industry. The main goal of the Commission is to visualize how the gender gap, and gender inequity in general, manifests inside the company, and solve those issues. Along with local organizations like CUTI , the focus will be on raising awareness in the IT industry, and actively taking action into transforming the industry so that it is a fair, equal field to work, grow and develop on. “It is so important to make women’s role in the industry visible, so that more girls feel identified and represented. The so-called gender gap is a reality, but I’m sure every single one of us can do little things to close it bit by bit” said Cecilia Gutiérrez, developer in Moove It. Talks about gender will be organized, as well as workplace harassment workshops and a solid proposal to educate, correct and eradicate any type of not inclusive, misogynistic attitude. Change starts with education It is important to understand that young women’s choice to not pursue a career in technology despite wanting to do so, is mostly consequence of their upbringings. What do we make them believe they’re capable of doing? The IT Industry will only be able to hire smart, resourceful IT women, if young girls are given the opportunity to educate themselves in technology. That means talking to children, demonstrating girls are represented equally in the field, and above all, educating adults in power positions in the industry, so that they can be receptive to change. At the end of the day, what the IT industry should bet on, is compromise and attitude. These aspects know no gender, age, race or nationality. Most importantly, they are gained and discovered through equal opportunities. That’s why at Moove It, we keep asking ourselves this question and now encourage you to do the same: How do we bring more girls into this field?", "date": "2021-02-11"},
{"website": "Moove-It", "title": "soft-skills-in-the-it-industry-what-it-means-to-be-a-leader", "author": [" Guillermina Pose "], "link": "https://blog.moove-it.com/soft-skills-in-the-it-industry-what-it-means-to-be-a-leader/", "abstract": "Soft Skills in the IT Industry: what it means to be a Leader When I enrolled in UW- Madison University , I was thrilled to study the application of soft skills in the work environment, and the effectiveness of them constructing strong leaders. That degree was a milestone in my career as a Communicator. During that year, I learned how most soft skills could be grouped into 5 big areas: Critical Thinking, Creative Thinking, Self-regulated learning, Communication, and Teamwork. Each of these encompasses various skills and perspectives that now, more than ever, are key to hiring candidates, and most importantly, choosing the right Team leaders. In a Wall Street Journal survey of over 900 executives, 92% reported soft skills, including communication, curiosity, and critical thinking are as important as technical skills. However, 89% of those same executives reported they have a very difficult or somewhat difficult time finding hires with soft skills. Many people think learning and practicing soft skills is common sense. This is a problem, for it makes individuals believe that they already know all about those skills and that they are a secondary asset in their professional careers. The IT industry has shown huge growth in the past decade, and part of what makes that evolution possible is the emphasis on creating optimal work environments, with great, purposeful leaders. It is somehow a common misbelief that professionals with a more technical background, lack soft skills. As said before, we are not born with those, so It is just a matter of practice and dedication. Ideally, a Team Leader should be trained in soft skills, but what is crucial, is her/his openness to learn and practice daily. We can never reach a knowledge limit on soft skills. The critical thinker Leader Critical thinking is often defined as the ability to question our beliefs and assumptions, relieve our experiences, assess our take on different situations, and listen to what others have to say, before making any type of decision. Action must be taken according to the conclusions of this process. What does it mean daily? Analyze the whole picture. To do so, ask everyone who is involved in the day-to-day activities. Make them know their take on things is valuable, and that it is pivotal for you to make decisions. Be honest with yourself to evaluate if you’re biased towards something. If you are, ask yourself why, what are the assumptions that drove you to think that way. Surround yourself with people you trust. Make sure your team is diverse and that every single person on it, is better than you on their jobs. You are better at leading, they are best in other areas. Recognize that. The self-regulated learning Leader Any environment that stimulates learning and growth, needs a leader that thinks of her/himself as a lifelong learner. We don’t get to the Leader role because we know more than others, but because we have a purpose to help and lead, and a hunger to learn from others. The self-regulated learning Leader wants their team to learn, to grow, and to eventually become leaders themselves. She/he wants to be challenged by their team, for that is the only way they can evolve together. The communicative Leader Effective communicators are a rare species. The good news is that anyone can become a better leader and a better communicator. Good practices for the communicative leader: Practice Active Listening. This should be mandatory for everyone, but the role of a leader will not be as effective if this is not trained. Your team is not mind-readers. Explain what you need and most importantly, why do you need it. Always give problems to be solved, not just tasks. Practice your tone, improve your body language and carefully choose your words. Not above, not below, but part of the team Leaders are not above. In more practical terms, when you are above someone, you won’t know what happens inside the team, therefore you will make wrong decisions that show that you are not familiarized with the overall situation. Play in the field. It is challenging to be a Leader, mostly because you have to play the match and also be purposeful to motivate the team. Effective leaders ask for help. Leaders can’t survive without a team, for they wouldn’t have anyone to lead. So make sure everyone knows you are a team player, and that you consider that to be your number one role. Soft skills are not an if, but a must, and it is high time every organization makes it a priority to provide its members with the training they need to become better team players. At Moove It, we believe that purpose and soft skills are key assets, and equally important than technical skills. That’s the only way to be true to one of our core values: quality over quantity .", "date": "2021-03-17"},
{"website": "Moove-It", "title": "ui-a-new-life-for-well-known-editorial-design-concepts-and-tools", "author": [" Guido Kogan "], "link": "https://blog.moove-it.com/ui-a-new-life-for-well-known-editorial-design-concepts-and-tools/", "abstract": "UI: a new life for well-known editorial design concepts and tools. Have you ever thought about which branch of graphic design could be linked to user interface design? If we think about some concepts like patterns, readability, or accessibility, it sounds a bit like the fundamentals of editorial design. We rarely hear a relationship between these two worlds, but at the same time, we never analyzed that we could consider the user interface as part of the editorial design. Every day we see articles talking about the difference between UX and UI. There is a lot of focus on the need to differentiate the two roles. We always try to see where the UX comes from and what roles make it up, but do we ever wonder where the UI comes from? We are well aware that a UI designer has a similar profile to a graphic designer. However, by saying that we are making a generalization and ignoring the time and effort it took to identify all the aspects of the role of a graphic designer. Similar to how a packaging designer doesn’t have the same skills as a brand designer; the UI designer doesn’t have a cohesive relationship with these two. It’s the same type of differentiation we try to create within the UX world. Let’s take an example, suppose a packaging designer had to create an app. He would do a great job of course since all the roles within the graphic design role come with the same foundations, but is he the best designer to deal with interfaces? If we analyze the different roles of graphic design, we can think of associating the editorial designer with the UI designer. The editorial design As we already know, editorial design is a branch of graphic design whose main function is to optimally design and compose different types of publications. Classifying and structuring all the content is the key to this role, in fact, we could associate the Information Architecture (IA) with this concept, which guides the user throughout the project. The corresponding text can generate different feelings for each reader. I think this all sounds familiar to me. The editorial designer needs a grid that gives structure and coherence to the content, he must understand the typography, its combinations, variants, its meaning, and take into account that the text must be legible. He has to build a great harmony of colors and shapes throughout the project, giving strength to the content of the text. Aren’t these concepts similar to UI designer tasks? Is there a real difference between the two worlds? Let’s do a quick comparison between them. The editorial designer takes care of the print site. They talk to the printer about the specifications, needs, how it should look, discuss the end result, and then authorizes production. Its products are mostly physical (sometimes it provides a digital version with physical specifications) and these products are usually magazines, newspapers, books, or brochures. The UI designer communicates with engineers about tech limitations, takes design patterns into account, and reviews each screen in detail to deliver a high-quality product. He offers digital projects that can be turned into websites or apps. If you think about it, this difference is nothing more than an update of technologies and tools. They both start with low-fidelity wireframes, prioritizing layout and spaces, using grids as a law to maintain consistency throughout the project. Legibility, colors, fonts, structures, harmony, accessibility, etc. are concepts that create the profile and bases of these two roles. They share the methodology of work with design systems, both roles follow patterns to be repeated throughout their projects, defining styles of titles, colors, variables, and combinations of elements. Actually, the grid of a newspaper is absolutely related to the grid of a wireframe. They share the same idea of avoiding constantly designing, but design templates to reuse them. In other words We could say that the UI designer comes from editorial design with additions from other branches such as web design and interactive design. We may not be that far off with this formula if we add a lot of UX concepts. When the editorial design was thought to be dying with digitization, it was really reviving and becoming a broader concept– editorial user interface design, a world dominated by grids, structures, patterns, readability, usability, accessibility, and user experience. That is what design does, it mutates, adapts, and appears in new ways, always having its base in the past. Here’s the thought that doesn’t stop worrying us: what’s going to happen in the next 10 years?", "date": "2021-04-27"},
{"website": "Moove-It", "title": "prevent-bypassing-of-ssl-certificate-pinning-on-ios", "author": [" Patricio Aguirre "], "link": "https://blog.moove-it.com/prevent-bypassing-of-ssl-certificate-pinning-on-ios/", "abstract": "Prevent bypassing of SSL certificate pinning on iOS Nowadays, mobile apps have become the focal point in businesses such as booking, investing, food ordering, travel, shopping, banking, and more. They have become a must-have solution for small to large organizations. However, not everything is bright, and the amount of sensitive and private information that mobile apps handle and store makes them the primary targets for people with malicious intentions. That is why security should be a priority from the start. Hacking an Android or iOS application can be relatively easy if the right measures are not taken to make it more resilient against attacks. Here are some basic practices that should be implemented to improve mobile apps security: Do not put encryption keys, access codes, or tokens in the app code: Cyber attackers could find this data by reverse-engineering the app, all they have to do is download the app to do this, so, do not put this information in the app code. Test repeatedly: Securing a mobile app is a never-ending process. Design the code to be easy to update and patch. Test repeatedly and fix bugs as and when they are exposed. Principle of least privilege: What is not explicitly allowed should be prohibited by default. The principle of least privilege (PoLP), requires that in a particular abstraction layer of a computing environment, every module (such as a process, a user, etc. depending on the subject) must be able to access only the information and resources that are necessary for its legitimate purpose. Encrypt sensitive data: To protect the user data, use the mechanisms provided by the OS to store encrypted data. All the sensitive data collected through the mobile app should be encrypted. By encrypting data, it will impossible for cybercriminals to read them. Obfuscate the code: Bugs and vulnerabilities in code are the starting point most attackers use to break into a mobile app. Keep the security of the code in mind from day one and harden it, making it tough to break it through. Beware of libraries: Third-party libraries make our lives as developers easier, but in addition to taking into account their use licenses, we must be very careful and test the code thoroughly before using them. HTTPS: Mobile apps generally communicate with a backend server, there is some sort of back and forth of data over wired or wireless communication channels. It’s important to make data transportation as secure as possible. Https protocol should be the standard for any communication between the mobile app and the server. The topics mentioned above are just a set of basic measures to consider at the moment of developing a secure mobile application. What is SSL Pinning and how does it work? Most developers assume that using HTTPS is enough to be sure that user data transfer is fully secured and not compromised by a man-in-the-middle attack , but unfortunately is not. A MITM occurs when an attacker places himself between the server and the client, posing as one of them. In other words, when the client connects to the server, it is actually dealing with the hacker and vice versa. Thus, even though the client “thinks” that they have established an encrypted connection to the server, they are actually “talking” to the attacker, who can view and modify the data. One of the first things an attacker will do when reverse engineering a mobile application is to bypass the SSL/TLS protection to gain a better insight into the application’s functioning and the way it communicates with its server. Configuration and maintenance of SSL sessions are generally delegated to a system library. This means that the application that tries to establish a connection does not determine which certificates to trust and which not. The mobile app relies entirely on the certificates that are included in the OS trust store. A researcher who generates a self-signed certificate and includes it in the operating system’s trust store can set up a man-in-the-middle attack against any app that uses SSL. This would allow him to read and manipulate every single SSL session. The attacker could use this ability to reverse engineer the protocol the app uses or to extract API keys from the requests. Basically, SSL pinning is a technique used on the client-side to avoid MITM attacks. The process consists of associating a host with their expected [x509 certificate](https://en.wikipedia.org/wiki/X.509) or public key. If more than one certificate or public key is accepted, the advertised identity must match one of the elements in the certificate chainset. This allows the application to trust only valid or predefined certificates or public keys. SSL Pinning leverages knowledge of the pre-existing relationship between the client and the server to help make better security-related decisions. This technique should be used as an additional layer of security for mobile app traffic and to validate the identity of the remote host. If SSL Pinning is not implemented, the mobile application will trust the certificates installed by default in the operating system or custom certificates installed by the user allowing the proxy tools to intercept the traffic. SSL pinning can be achieved in 3 different ways – Certificate Pinning – Public Key Pinning – Hash Pinning. Certificate pinning We store the server certificate in our application, then at runtime, we retrieve the certificate from the server and compare them. If they match, we can trust the server, otherwise, we can not. However, there is a downside to pinning a certificate. Each time our server rotates it’s a certificate, we need to update our application. Public key pinning In this approach, we generate a key pair and put the private key on the server and the public key on the mobile app. We then verify the extracted public key against the embedded one. If it matches, we can trust the host. By using public key pinning, we can avoid frequent application updates as the public key can remain the same for long periods. However, there are two disadvantages to public key pinning. First, it is more difficult to work with keys as it involves the process of extracting the key from the certificate, and second, the key is static and may violate key rotation policies. Hash pinning In this approach, we pin the hash of the public key of the server’s certificate and compare it with the hash of the public key of the certificate received during a network request. This technique is more complex than others, but it is worth the effort. After we have the certificate, we can use the hashing algorithm that we prefer. This makes a certificate or public key anonymous. Bypassing SSL Pinning However, today adding SSL pinning is no longer enough to prevent MITM, since there are tools that allow bypass it. Bypassing SSL pinning can be achieved in different ways: – By avoiding the SSL pinning check or discarding the result of the check. – By replacing the pinned data in the application, for example, the certificate asset or the hashed key. Frida is a dynamic code instrumentation toolkit, which enables function hooking and allows to provide a definition to it during runtime. It works over Windows, macOS, GNU/Linux, iOS, Android, and QNX. Basically, it injects JavaScript code into a process. Suppose there is a function called “foo” with a specific implementation. With Frida, one can change the implementation of the “foo” function at runtime. Furthermore, It has a vast community of users sharing their custom scripts to accomplish different kinds of tasks, from enumerating processes and attached devices, trace function calls, check memory values, kill processes, bypass SSL pinning, bypass biometrics checks, and more. Frida isn’t the one tool someone can use to bypass SSL pinning, but the principal advantage over other tools like SSL Kill Switch 2 is that not require a rooted device to accomplish it. So, what can we do as developers to mitigate these kinds of threats? Mitigation As a first step, we could use a tool to obfuscate the code, making it much more difficult for the attacker to determine which method performs the certificate validation. Obfuscation will also throw off all automated tools looking for a known method name. An obfuscator can rename methods in a different way on each application build, forcing the attacker to look up the real name in each new version. Despite the code obfuscation, a reverse engineer with time and dedication could analyze the app control flow and at some point find the location where the certificate is verified and change it to achieve the goal he wants. So, we need to think in a different way to mitigate these types of threats. Attackers need to reverse engineer the mobile app to alter its behavior, this is the time when we can make their job harder. Implementing an anti-tamper mechanism and detecting if the app has been launched in a malicious environment it’s a better approach to deal with these kinds of attacks. iOS Security Suite is an advanced and easy-to-use platform security library written in Swift that simplifies the implementation of anti-tampering mechanisms in iOS applications. Swift code is considered to be harder to manipulate dynamically than Objective-C. Since this library was written in pure Swift, the IOSSecuritySuite methods shouldn’t be exposed to Objective-C runtime, which makes it more difficult to bypass. This library provides us an easy way to detect jailbreak, an attached debugger if the app was run in an emulator, and common reverse engineering tools running on the device. Furthermore, this library has a BSD 2 license. With the use of this library, we can abort the app execution if some reverse engineering technique is applied, preventing an attacker from altering the normal flow of the application. Where do we go from this? Since the mobile app market is growing, mobile security will continue to deliver tons of issues to deal with. Security is like a race between attackers and defenders, different techniques to break apps security are discovered every day and there is no silver bullet to be protected along with all threats. As developers, we should keep in mind that the data which the apps operate may be of some interest to third parties, we must be responsible and ensure that at least the basic security measures are implemented. To conclude, adding additional protection measures in an already productive mobile app requires a significant time investment and effort. Therefore, it is worth taking care of security in the very early stages of development.", "date": "2021-04-26"},
{"website": "Moove-It", "title": "coronavirus-how-to-work-from-home-and-stay-productive", "author": [" Mercedes Acevedo "], "link": "https://blog.moove-it.com/coronavirus-how-to-work-from-home-and-stay-productive/", "abstract": "Companies around the globe have switched to mandatory remote work due to the emergency health crisis that is taking place in many countries. At Moove It , home office and working remote dynamics are part of our culture from the very beginning. However, in the last few days, we had to take them as preventive actions against the spread of Coronavirus.  But how to get the right working mindset? If you are working from home due to this reason, or because this is your current working methodology, here we have some recommendations we already follow to work efficiently: Get started early For many of us, the most productive hours of the day are those in the morning. This is why you should try to execute the most complex tasks during this period and make sure you finish them on time. Create a working station and keep it clean Instead of lying in bed with your laptop, which is definitely not the right way to go,  choose a quiet and distraction-free working space, ideally away from the bedroom, where you can work comfortably with your computer and other tools you may need such as post-its, notepads, etc. Sit down in an upright chair, like you would at your office desk. Keep work time and personal time separate Working from home may give the wrong idea to the people who live with you or to your friends. This working methodology does not mean you are available to chat or to do some laundry. Try to create boundaries within your home that your family members understand, such as closing the door of your home office, meaning that you are occupied. Dress for success We get it, staying in your pajamas and fluffy slippers all day is very comfortable, but wearing your nightclothes during the day can have the effect of blurring the line between work and leisure. Furthermore, it may interfere with getting any work done. Dress just like if you were going out from home to work at the office. This can have a surprising subconscious effect on your productivity. Head into your workspace, sit upright, open the curtains and focus on the task at hand. Set yourself a daily goal with your team. Stop for a quick break every hour or so but stay on task, and when the goals are achieved at the end of the day, you can shut down the laptop and just chill safe at home. And of course, go back to your pajamas. Keep yourself to regular work hours and plan your workflow Define work schedules and share them with your team and dedicate your full attention to your job duties during working hours. Schedule and participate in a daily video call with your team, in which the most important goal should be discussing daily priorities.  This meeting should be brief and specific, and hopefully, at the same time every day. Set daily objectives, with defined metrics and deadlines. Communicate with your co-workers At Moove It, we use Slack to be in constant communication between teams and to make sure to notify anything relevant to the quality of the projects we are working on. It is always better to communicate via video calls, emails or phone calls in order to clarify questions. So there you go, whether you are planning to work from home while preventing the spread of this new virus, or doing it as your current working dynamic, stay connected and keep the spirit up. You are not alone, your team is there for you!", "date": "2020-03-18"},
{"website": "Moove-It", "title": "guidelines-managing-fixed-price-project-successfully", "author": [" Victoria Bernini "], "link": "https://blog.moove-it.com/guidelines-managing-fixed-price-project-successfully/", "abstract": "At Moove It , fixed cost projects have been very much discussed. There are many opinions about it, but the reality is that in order to achieve a long-term project, you must first gain the client’s trust. We can make our capacity known to the client giving them peace of mind that once the agreed scope is finalized, it is their decision to continue working with the company or not. How to manage a fixed – price project Managing a project with these characteristics is very different from what it takes to coordinate time and materials. To begin with, the customer usually performs a product discovery that may or may not have been done by us. If it was, we have a better start because we know that we will have a visual prototype that will help us understand the scope of each requirement. On the other hand, we know that the work was estimated by someone from our team and with our standards. If the new project did not have a product discovery made by us or didn’t have one at all, it means that the project is riskier because the requirements will be much less clear when estimating the time it will take to deliver an MVP (Minimum Viable Product). One of the first steps to take at the beginning of the project is to understand all the requirements of the new product, determine what will be included in the MVP, and write all the Epics, user stories, and tasks necessary to carry it out. The second step is to gather the entire development team for an initial Planning where the effort necessary to deliver the MVP will be estimated in story points, that is, all the tickets created in the previous step. The team must also determine how many tasks they think they will be able to complete in the first Sprint. Process Based on this assumption, the Team Leader will put together a Roadmap where they will repeat that amount of assumed story points over time, to determine how many Sprints it will take to deliver the first version of the product. With this first draft of Roadmap, the TL can realize if it is possible to reach the deadline that the client expected, or if otherwise, it is necessary to reduce the scope or add more developers. It is important to consider as part of the Roadmap, an initial period for Infrastructure/configurations, and a final period of at least two weeks for testing. After completing the first Sprint, it will be known whether or not the team reached the number of story points assumed in the initial planning, and the Roadmap must be adjusted accordingly. During the development stage, it is extremely important to let the client know it should not allow add or change functionalities that were not included in the MVP. It is very common for the client to start with an idea of what they THINK they want, but along the way, they begin to realize that it was better to do it differently or that they need more things to achieve their goal. It is the Team Leader’s job to guide the client, advise it, suggest, but above all, if it is decided to change or add something, the MVP’s delivery projection must be re-estimated and this must be communicated in the most transparent way possible. It should be clear that to add X, you may have to remove B in order to deliver the MVP on the promised date. The Roadmap must be adjusted each time the scope is modified and must be shared with the client for greater transparency. The demos are another important point in this type of project. It must be agreed with the client that every certain amount of time, the progress of the project will be presented by a developer of the team. After the Demos, it is important that the TL creates a Meeting Notes document, where the decisions, comments, or customer requests will be detailed. Following these guidelines, the limited-cost project should be able to be delivered as it was estimated, or otherwise, the client should agree and understand the modifications regarding the initial plan. This is the path to make a happy client decide to continue working with our company under a long-term contract. Do you want to learn more about how we work? Visit our website and give us a call.", "date": "2020-10-26"},
{"website": "Moove-It", "title": "meeting-room-no-shows", "author": [" JR González "], "link": "https://blog.moove-it.com/meeting-room-no-shows/", "abstract": "No-show meetings and room allocation problems A common problem that you’ve probably come across working in an office is when you decide on a time for a meeting but can’t find a meeting room. Sound familiar? The main problem with rooms that are booked and not used is that when a room is reserved, it remains booked, even when participants don’t show up. We will start by explaining how no-show meetings occur and why it is paramount for companies to take steps to effectively reduce them. The problem of no-show meetings No-show meetings occur all the time and for different reasons, such as rescheduling a meeting and forgetting to cancel it, thus the room remains unavailable. Another common issue is scheduling meetings as recurring events, especially when there are a lot of people in the office and rooms are hard to book, among others. When meetings are canceled, there is no way for the rest of the company to know, in real-time, that the room is now available. When the general perception is that the availability of the meeting rooms is low, more and more people end up creating recurring meetings to reserve space in case they need it later; making the problem worse. Unscheduled meetings also have a negative effect on employee satisfaction. Employees that are constantly frustrated by the lack of availability in meeting rooms are more likely to leave and work from another location. This inevitably affects the company’s culture. These types of meetings also take up a lot of resources and operating expenses. Modern meeting rooms employ technology such as WiFi and electricity; these costs are incurred even if the room is empty. The solution: Rumu It’s been a while since we, at Moove It, worked jointly with universities on technology and software development courses. In Uruguay, we joined a course program at the Universidad de la República, Engineering School, called PIS (Software Engineering Project). The goal of this course is to reaffirm and deepen the student’s knowledge of Software Engineering, to contrast it with its practical applications, and to integrate it with knowledge gained in other subjects like programming, architecture, operating systems, computer networks, and databases. In this course, Moove It becomes the client that the students have to develop software in order to complete the class. Moove It’s contribution to the course is to generate the requirements related to the project and to give the students continuous follow-up; introducing them to Agile development in the process. We decided to ask the students to look for a solution to the problem we had about the no-show meetings and how to take advantage of the rooms in a more effective way. The solution was called Rumu, an app for tablet devices whose main goal was to improve the process of scheduling a meeting and booking a room. Development phase To speed up the development we used Scrum methodology, in which we applied our best practices so that we could work collaboratively with the team and help to create the best solution possible with the students. Methodologies Together with the students, we developed a common process related to deploying the implemented functionalities. This served to demonstrate best practices for organizing and achieving good quality in the developed code. What does Rumu have to offer? These are the most valuable features: Notifications when you are arriving late to a meeting. Automatic meeting cancelations. Google Calendar synchronization. Creation of meetings on the fly without having the need to book in anticipation. Extend the duration of a meeting. Automatic check out at the time the meeting was scheduled to end. Visibility of the state of the room. Notification of the upcoming ending of a meeting. Reports with metrics for managers. Offline functionality. Conclusion Room allocation problems in a company can become complex if they are not effectively managed in order to take advantage of the resources that each room offers and to avoid no-show meetings. Rumu is our solution to reduce room search times, better organize our Google Calendar and improve daily work within the company. The students of the PIS program developed an excellent product that is still used at Moove It and has generated great feedback from those who use it, which is all of us! In the near future, we would like to add new features that we have envisioned to further enhance this excellent app.", "date": "2020-06-30"},
{"website": "Moove-It", "title": "what-did-we-do-during-quarantine", "author": [" Sofia Canoniero "], "link": "https://blog.moove-it.com/what-did-we-do-during-quarantine/", "abstract": "The pandemic has flipped our lives upside-down and the working world is being profoundly affected by the COVID-19 outbreak. Many businesses quickly shifted to remote work, and abandoned operations in a traditional office setting, encouraging people to stay at home. And although Moove It is a WFH friendly company, not all the team members had chosen this working dynamic on a daily basis, so we knew we needed to keep our team’s motivation alive throughout the isolation. As a company, we strongly believe that challenges can be seen as opportunities to grow and improve. For this reason, we developed different internal online activities designed to help overcome the quarantine together, regardless of the country where our team members were located. The Human Resources team, the Communications team, Operations, and Technical areas came together to develop and promote fun activities as well as opportunities to learn. These are some examples: We reinforced already acquired knowledge on how to work from home At the beginning of the quarantine, we focused on providing tools to the team that enables them to work remotely in a more effective way. For example, the Communications team produced a document regarding best practices for conducting a video conference. Moreover, Mercedes Acevedo, our Communications Specialist, met with all the teams to guide them and help them find a place in their houses that were as professional as possible to have a call with a client. You can see the document here . We coped with anxiety The People team gave tips on how to cope with anxiety as a result of confinement. Also, a gym teacher sent us video stretching routines, which were easy to implement at home. Check out the document here . We remain connected Team building activities are essential to achieve more motivated teams. Hence, we managed to organize online get-togethers and remote game nights. Also, Fridays are very special days at Moove It because we gather to have lunch. As we could no longer do so, we created a Zoom meeting instead for those who wanted to spend some time with the rest of the team while having lunch. We learned to dance salsa, or at least we tried We know how boring working from home can be. For this reason, Nelly Ramírez, our Cali Office Site Manager, gave a salsa master class! The main objective was to do some exercise, and without a doubt, to have fun! Now the entire Moove It team knows the basic salsa moves, LOL. We talked about the importance of a healthy diet Florencia Fernández is a developer at Moove It, but she is also a nutritionist. In this sense, Florencia prepared a presentation about the importance of a healthy diet. She gave tips on how to maintain a healthy lifestyle by ensuring we are eating a healthy diet. We continue to learn We also provided internal, online technical training so as to continue growing professionally. Therefore, we organized Python onboardings, for those people who were interested in learning the technology and also sponsored AI talks designed to demonstrate real-world applications for Python. We contribute to the community We decided to contribute to some of the initiatives that were taking place in one of the countries we have offices in: Uruguay. The goal was helping people in vulnerable situations during the pandemic. Some of the projects we were involved in were FeriasUY and CanastasUY, both are nonprofit organizations that try to connect people who needed help with those who could provide it. All in all… Although working from home has been always encouraged by Moove It, it was thanks to and through these activities that the team’s motivation was always kept high. Over time we are returning to the office, following a strict health protocol to prevent COVID-19 and keeping our employees happy, healthy, and safe.", "date": "2020-09-14"},
{"website": "Moove-It", "title": "our-renewed-office", "author": [" Mercedes Acevedo "], "link": "https://blog.moove-it.com/our-renewed-office/", "abstract": "Each and every one of our offices around the world is designed to inspire innovation. In the past few years, Moove It has experienced rapid growth. We have incorporated new talent all around the world, welcomed exciting new clients that challenge us day in and day out with state of the art technologies, and we are incorporating new business units that allow us to offer a wider portfolio of services to our clients. In order to best meet these challenges, we needed to update our infrastructure to accommodate the growth. And now, we are excited to announce we have completed an extensive office renovation in Montevideo. The new office was specially designed to encourage teamwork and to provide spaces that are conducive to reflection and concentration. Leisure time We have also added rooms that encourage employees to play and relax in the office after the work is over, such as the ping pong area, a living room with a video game console, and the beautiful backyard with a barbecue. Details All the details have been thought out, including an audio system exclusively designed for hosting workshops, online talks, and webinars. Spacious and modern, the space is perfectly suited to our work dynamics and culture, reinforcing our core values: teamwork, passion, commitment, quality over quantity,  employee happiness, and growth. After many months of work, we are extremely happy with the result, and wanted to thank María Noel Burghi – Mele – our CFO, for her great work and dedication in making sure everything went smoothly and on time, and to the architect in charge of the renovation, Iael Dorf . This is a new hub for Moove It and we would love to show you around! Want to see more of our office? Take a look at our YouTube channel. PS. If you are asking why and how we are going back to the office in the middle of COVID-19, you should take a look at the article our CEO, Ariel Ludueña, wrote in Forbes about how Uruguay managed the outbreak.", "date": "2020-09-11"},
{"website": "Moove-It", "title": "web-reactive-programming-spring-webflux", "author": [" Martin Baccino "], "link": "https://blog.moove-it.com/web-reactive-programming-spring-webflux/", "abstract": "At Moove It we love keeping up with the latest developments across a variety of technologies. In this article, we’ll introduce you to one of the exciting programming tools that have been released recently – Spring WebFlux – by first giving some useful background, then explaining the tool itself, and finishing with an example to help you get started. Try to follow along with the example while we explain it. Let’s dive in! Reactive Programming Reactive Programming is, as its name implies, a programming paradigm that promotes an asynchronous, non-blocking, event-driven approach to data processing. “Reactive” models in programming are built around the publisher-subscriber pattern (observer pattern), where the “publishers” are sent data from all parts of the application to then emit out to any “subscribers” that are listening, which in turn handle the data as needed. As such, publishers are the backbone of Reactive Programming, and the code is inherently asynchronous from the ground up. Blocking vs non-blocking (async) request processing Blocking request processing In traditional MVC applications, when a request comes to the server, a servlet thread is created. It delegates the request to worker threads for I/O operations such as database access. During the time worker threads are busy, the servlet thread (request thread) remains in a waiting status and thus is blocked. This can also be called “synchronous request processing.” Given that a server can have a finite number of request threads, this approach limits its capacity to process effectively at maximum load. Non-blocking request processing In non-blocking (or asynchronous) request processing, threads do not have a waiting state, and generally, there is only one primary request thread, with a separate thread pool (generally a small number of threads) for delegating work to. All incoming requests (events) arrive with a callback which will be called at some later time. The request thread delegates any incoming requests to the thread pool, which in turn delegates individual requests to individual threads within the pool. The delegated thread then uses the appropriate handler for the event it just received. When the handler function is complete, one of the threads from the pool collects the response and passes it to the callback function that the original event arrived with. The non-blocking nature of threads helps in scaling the performance of the application, and the small number of threads means less memory utilization as well as less context switching. Reactive Streams API The new Reactive Streams API was created by engineers from several organizations including Netflix, Pivotal, RedHat, Twitter, and Oracle, among others, and forms part of Java 9. It defines four interfaces: Publisher : emits events to Subscribers. A Publisher can serve multiple Subscribers. Subscriber : receives and processes events emitted by a Publisher. No events will be received until Subscription#request(long) is called to signal the request. Subscription : Defines a one-to-one relationship between a Publisher and a Subscriber. It can only be used once by a single Subscriber. It is used to both signal a desire for data and cancels any current requests, which aids resource cleanup. Processor : Represents a processing stage consisting of both a Subscriber and Publisher and obeys the contracts of both. Two popular implementations of Reactive Streams are RxJava and Project Reactor . Now that you’ve got some background, let’s talk about WebFlux. What is Spring WebFlux? Spring WebFlux is a parallel version of Spring MVC and supports fully non-blocking reactive streams. It supports the backpressure concept and uses Netty as a built-in server to run reactive applications. Spring WebFlux uses Project Reactor as a reactive library. Reactor is a Reactive Streams library, so its operators support non-blocking backpressure. Spring WebFlux mainly uses two Publishers: Mono : Returns 0 or 1 element. Flux : Returns 0…N elements. A Flux can be endless, meaning that it can keep emitting elements forever. Also it can return a sequence of elements and then emit a completion event when it has returned all of its elements. In Spring WebFlux, we call reactive APIs/functions that return Monos or Fluxes, which means your controllers will return the same Monos and Fluxes. When you invoke an API that returns a Mono or a Flux, it will come back immediately. The results of the function call will be delivered to you through the Mono or Flux when they become available. It’s important to mention that to build a truly non-blocking application, we must aim to design all of its components as non-blocking , including the client, controller, middleware services, and even the database. If any component blocks requests, we have not achieved our goal. Example Source code WebFlux supports two programming models: Annotation-based reactive components Functional routing and handling Today we will focus on the annotation-based approach. We will build a REST API for publishing and retrieving resources (in our case “Books”) using RestController to make it fully non-blocking. We will also use MongoDB as our database. 1. Maven dependencies We have to include spring-boot-starter-webflux and spring-boot-starter-data-mongodb-reactive dependencies. 2. Configurations Spring WebFlux supports annotation-based configurations just like the Spring Web MVC framework. Webflux Configuration MongoDB Configuration Spring boot application 3.  Model 4. REST Controller Let’s create endpoints in our controller that publish: A single Book resource. We will use a Mono of type Book because it will emit at most 1 element. The collection resource of all Books. We will use a Flux of type Book because it will emit 0..n elements. 5. DAO repository BookRepository can be any repository data that supports non-blocking reactive streams. We will use MongoDB . Demo Start the application by running WebfluxFunctionalApp and check requests and responses. HTTP POST http://localhost:8080/create Request 1 { “id”: 1, “name”: “book_1”, “pages”: 100 } Request 2 { “id”: 2, “name”: “book_2”, “pages”: 350 } HTTP GET http://localhost:8080/ Response data:{“id”:1,”name”:”book_1″,”pages”:100} data:{“id”:2,”name”:”book_2″,”pages”:350} Notice that we are testing the API with Postman which is a blocking client. It will display the result only when It has collected the response from both books. To verify the non-blocking response feature, hit the URL in the browser directly. The results will appear one at a time when they are available, in the form of events (text/event-stream). To view the results better, add a delay to the controller API. Conclusions Both Spring MVC and Spring WebFlux support a client-server architecture but differ in their concurrency models. In Spring MVC it is assumed that applications can block the current thread, while in WebFlux threads are non-blocking by default. Reactive and non-blocking applications do not inherently run faster than blocking applications, but rather provide scaling benefits with a small, fixed number of threads and smaller memory requirements. Sources https://projectreactor.io/ https://howtodoinjava.com/ https://www.baeldung.com/ https://dzone.com/ https://docs.mongodb.com/", "date": "2020-01-08"},
{"website": "Moove-It", "title": "mimochi-experience", "author": [" Florencia Sigalotti "], "link": "https://blog.moove-it.com/mimochi-experience/", "abstract": "Time to give For Moove It, it is important to contribute to society in many different ways. That’s why, at the beginning of the year, we decided to join the Mimochi initiative. And for us, it was a one-way ticket. Mimochi is a voluntary project that started in 2017, with the aim of giving backpacks filled with school supplies to underprivileged children at the beginning of every school year. Since then, it has been growing nonstop, with several organizations and volunteers working towards the same goal. People can collaborate by donating school equipment or complete backpacks. After contacting the organization and joining the project, we made an internal call to team up a group of volunteers. We began by organizing weekly meetings to coordinate the project, and worked with the Communication Team to bring to life the Mimochi campaign inside Moove It. In agreement with the company, Moove It would double the goal achieved by the collaborators, which made the purpose even more interesting. So, we defined that our aim was to collect 30 bags, in order to meet the needs of a whole classroom. As the materials and donations arrived, we took turns making inventories and assessing what was missing to fill the backpacks. Creative and unexpected initiatives emerged both within and outside the group of volunteers, which allowed the campaign to grow more successfully. After a month, once we had all the materials, we organized an after-office as an excuse to fill the backpacks with the gathered school supplies. We formed work teams, and in an organized way (with lots of laughs and games in between) we were able to complete all the backpacks in just an hour and a half! In coordination with the organizers of Mimochi, we delivered the backpacks to one of the beneficiary organizations, Gurises Unidos (a civil organization committed to the human rights of children and adolescents). It was a fulfilling experience, where everyone’s work was reflected in a very meaningful way. We knew it was going to be a heartwarming adventure for us, no matter what the outcome was. However, we were deeply surprised by the final result. Not only did we achieve the original goal, but we surpassed it, and ended up putting together 50 backpacks! Furthermore, this project brought us together as a team, to achieve a different and supportive goal, and also taught us that we can create special and fun experiences while helping others. Without a doubt, Mimochi made us much stronger as a team, and we hope to join more initiatives like this one in the near future.", "date": "2020-04-15"},
{"website": "Moove-It", "title": "how-sre-increases-team-efficiency", "author": [" Mercedes Acevedo "], "link": "https://blog.moove-it.com/how-sre-increases-team-efficiency/", "abstract": "Some time ago we started to identify differences regarding the implementation of processes in projects among the internal teams. With this in mind, we assembled a Site Reliability Engineering (SRE) team with the aim to standardize the way we run our production systems, and to provide DevOps, SRE and Cloud Consulting Services to our clients. What is SRE? SRE has the mission to protect, provide for, and progress the software and systems behind all development projects with an ever-watchful eye on their availability, latency, performance, and capacity. This team tries to tackle two main problems: the functionality and stability of the projects over time, providing tools to work on its productions while avoiding technical debt. Our SRE specialists are responsible for the implementation of cloud migration, log and secret management, the dockerizing of a project, and for the continuous integration and delivery of tools to catapult productivity and efficiency into our clients’ products. Moreover, relying on such a group offers benefits, not only for the company but also for the clients: Benefits for the company: Infrastructure and documentation organization. Standardized containers. Flexibility between projects. Guidelines and recommendations through an SRE. Compliance Framework. Improve the speed in setup and deployment processes. A suite of recommendation tools, like for instance, error tracking. This implies that the arrival of a new developer to a project that is already running may be as smooth as possible while working faster in a more productive and efficient way. Benefits for the clients: We can build more quality products. Create environments with high availability and scalability capabilities. Help them save time (and money!) due to fewer iterations. Economize budgets allocated for cloud services. Our team works as a plugin to organizations, bringing talented professionals, tools, and processes that are integrated smoothly into the company’s operations. Our clients’ success stories working with SRE Our SRE team has been working with several of our clients and projects such as Bucksapp , MapRight , Tabula Rasa, Finepanel, StarterHome, DriverFinder, Bancard, and Inari, among others. Bucksapp case study: SRE as a part of your team Bucksapp is an app that tackles a very relevant and urgent issue for college students: it advances cash in a fast and reliable way. We have been working with them since August 2019, and from that moment on the SRE team became part of the project. The integration between the Bucksapp and the SRE team was totally natural. Fabio Lima, Developer at Moove It, has been working with Bucksapp since the client started operating with us. “Counting on the SRE team is very helpful, especially when new people are added to the project. Using SRE techniques, especially the use of Docker, that problem isn’t an issue anymore, and as it works on any machine, regardless of its geographic location, it’s also very useful in deploys and for putting projects into production”, he adds. One of the tasks where SRE directly worked on was the secret management. Guillermo Chao, developer and member of the SRE team here at Moove It explained that the SRE team, together with the client’s team, started to handle the application’s secrets by using Vault. “SRE set up the Vault server in the client’s Google Cloud structure and accomplished to connect the application to that new server. By doing so we were able to negotiate the secrets and therefore, use them”. Do you want to read more about how our SRE team can help with your project? Check out our website. MapRight case study: SRE assessment MapRight is an application that helps users to explore large stretches of land without having to move from their homes. It creates impressive maps and templates in a fast and accessible way. MapRight’s tools and layers are designed to create maps that are beautiful, easy to read, and ready to be shared with anybody. This client has been working with us since June 2014. But it was not until last year that the SRE team has become a significant part of the improvement of the project. German Barrios, Principal at Moove It, works with MapRight directly, and explained what was the problem that led us to work with the SRE team: “As a result of a database failure, the client’s team asked SRE to make a situation analysis of the project in order to detect possible points of failure”. Finally, both teams jointly with the client came to terms that the best solution would be to migrate to AWS, for three main reasons: Greater confidence in this service. Long term, it would be financially advantageous for the client. AWS represented more and better features that the teams could take advantage of. Accordingly, SRE executed a cost plan to validate it with the client first, who was on board from the very beginning, and an action plan on how the migration would go. We started up an implementation roadmap to align expectations with our clients: So, along with our clients’ teams, we analyze their projects in terms of performance, good practices, and security standards, and accordingly provide the action plan that fits best to their needs. More benefits SRE is also beneficial when it comes to reducing conflicts between versions of the same application. For instance, when a developer is working on more than one project, and it has more than one environment installed on their computer, SRE allows us to separate it in a very efficient way, avoiding version conflicts. What is more, this is as well useful for several projects within the same client, or when working for more than one client at the same time. To conclude, thanks to SRE we now can accelerate the product development cycle, minimize costs, drive efficiency, and increase security. Do you have any questions for us? Get in touch!", "date": "2020-05-26"},
{"website": "Moove-It", "title": "6-tips-to-improve-your-communication-skills-in-a-meeting", "author": [" Mercedes Acevedo "], "link": "https://blog.moove-it.com/6-tips-to-improve-your-communication-skills-in-a-meeting/", "abstract": "Successful leaders know that communication is about more than just the words one uses. Below we have some tips to help you communicate in a meeting like a leader. 1) Go with the plan Whether you have a meeting or are about to take a business trip, you should always prepare ahead of time. Try to answer these questions: “What is the aim of this meeting?” and “Who is attending the meeting?”. These questions help frame the situation in particular ways that can then help direct your plan. Additionally, try getting in touch with co-workers that have conducted similar meetings, as they can give you valuable input about the topics or attendees. 2) Do your research Before a meeting, try to do a little research about the attendees: who are they, what are their positions at the company, do they have children, are they married, etc. Learning about the audience beforehand will help you avoid unnecessary discussions and hone your messaging. When meeting someone for the first make sure you listen carefully during introductions and try to remember their name throughout the whole conversation. 3) Small talk can help Small talk, also known as the art of knowing how to chat with a total stranger, can help make everybody feel more comfortable and is a useful way to avoid awkward silences. The FORD (family, occupation, recreation, dreams) method might help you come up with some topics to discuss. For instance: “Any plans for the weekend?” “How is everything going at the office?”, “Have you seen any good Netflix series lately?”. 4) Watch your body language You are constantly communicating, even when you’re completely silent. As such you should always be mindful of your body positioning. It doesn’t matter if you are in a video conference, you should always show interest. Try to make eye contact and smile, show confidence by sitting-up straight, when in a video chat look directly to the camera, and please, do not cross your arms, as it can make you appear defensive or withholding. 5) Show you are interested Be alert, mindful, and watchful of what is going on around you. Keep your phone out of sight, listen, and maintain eye contact. Asking questions and repeating the other person’s last words shows you’re interested in what they are saying and helps clarify points that could be misunderstood. Let the other person talk without interrupting, and avoid being quarrelsome before the person has finished talking. 6) Say goodbye to the distractions How do you feel when the person you’re talking to is distracted by their smartphone? Distractions in a meeting can seem harmless, but they can easily annoy participants. Using your phone, scribbling on a piece of paper, or staring out the window not only diverts attention from the meeting but also communicates that you are not interested in what others have to say. As difficult as it may seem, it is possible to eliminate most distractions. Next time you are in a meeting, remember to take your computer, a pad of paper, and a pen for notes. That is all you need. Remember that everything you do communicates messages to those around you, so to help ensure you have successful meetings, come prepared, be mindful of your body language, and stay engaged.", "date": "2020-03-04"},
{"website": "Moove-It", "title": "tech-debt", "author": [" Cecilia Gutierrez "], "link": "https://blog.moove-it.com/tech-debt/", "abstract": "– Do you think you’ll be able to finish this feature within this sprint? How many times have you ever been asked this question? And how many times have you developed things not as efficient and well designed as you would have preferred to? I bet many times. The problem Technical debt is a daily reality in software development. It arises directly from the agility required by constant requirement changes and incoming deadlines, typical characteristics of agile methodologies applied to the IT industry. More often than not, we have to deliver work for which we do not have all the details we need to do so. It’s very common to come across unexpected obstacles when developing software, and when the deadline arrives, in most cases, it’s preferred to ship code that’s working -despite how well it was done- rather than nothing. These above-mentioned issues are particularly important in large-scale projects, where there is a team of several developers working together, and not everybody is familiar with the code. In those cases, code understandability and extensibility are key. Quick and complex architectural decisions may be under your control -at least for a while- if you are working alone, but when your teammates are also going to work with it, it’s not so trivial, or at least it shouldn’t be. Small improvements are frequently delayed, and after a long time postponing them, the amount of pending architectural improvements keeps getting bigger, and it may end up becoming a huge unmanageable snowball of poorly written code. That’s how it turns into the popular and feared “legacy code” that nobody dares to change. At some point, we’ll come to a dead-end and the only viable solution will be migrating the code to a new repository, usually to new cutting-edge technology. So, how do we manage it? There’re plenty of approaches to identify and mitigate technical debt. Most of them are inspired by extreme programming (XP) best practices, such as following coding standards, having short cycles, and incremental design. All of them point to efficient, maintainable, and extensible code. Here at Moove It each project has different processes depending on its needs and what the client and team agree is more suitable. In some cases, technical debt is addressed and planned within each sprint, sometimes under a specific percentage of the planned hours. In a particular project, we’re using the following: a 3-week sprint, where the last week is exclusively dedicated to tech debt. Isn’t it fantastic? Which developer wouldn’t like to have a full week at the end of each sprint entirely dedicated to embracing product high-quality and maintainability? Quality over quantity is one of our key main values. We do our best to apply it across all the aspects of our services, and code is just one of them. When developing a feature, we’re constantly analyzing and looking for diverse alternatives, each with its pros and cons, so that when we come across a tech debt disjunctive, we proceed to create a new task to document this pending work. That way we ensure we don’t forget that something could have been done better, and it actually is done as soon as possible! We have a specific board to organize all these tasks separately from the ones assigned to the current sprint. Also, each member of the team is encouraged to suggest improvements whenever possible, and we consider every idea equally. Everyone has a valid voice that deserves to be heard on our team. Conclusion Technical debt is inevitable in all projects. The key is to figure out which is the best way for the team to handle it according to the project, and of course, the client. Teams should discuss and try different approaches if necessary until finding the most suitable solution, without trying to forcefully adapt themselves to a structured popular process. There’s no magic recipe, just different approaches for achieving the same objective: have the code as simple and elegant as our process and deadlines allow us. With all that being said, we invite you to re-think your process in case you’re not dedicating time for embracing code quality. Give it a try! You’ll notice the difference, and coding will be more enjoyable, for sure. How do you manage technical debt? We would love to hear!", "date": "2020-08-17"},
{"website": "Moove-It", "title": "aws-cdk-software-development-framework", "author": [" Jorge Montes "], "link": "https://blog.moove-it.com/aws-cdk-software-development-framework/", "abstract": "At Moove It, we work with a variety of exciting technologies every day. Today we’re going to explore the AWS CDK, a software development framework. The AWS Cloud Development Kit allows you to programmatically generate Cloud Formation templates to provision resources on AWS cloud infrastructure. It supports the following languages: Java TypeScript Python Javascript C# This enables you to write your CDK application in your preferred language, using the tools you’re already familiar with. In this article, we’ll focus on Java. Installation Prerequisites AWS CDK command-line tools Node.js (>= 10.3.0) AWS CLI tool Maven 3.5.4 and Java 8 You m ust specify both your credentials and an AWS Region to use the AWS CDK CLI: You must provide your credentials and an AWS Region to use the AWS CDK CLI. The CDK looks for identification and region in the following order: Using the –profile option to CDK commands. Using environment variables. AWS_ACCESS_KEY_ID – Specifies your access key. AWS_SECRET_ACCESS_KEY – Specifies your secret access key. AWS_DEFAULT_REGION – Specifies your default Region. Using the default profile as set by the AWS Command Line Interface (AWS CLI) ( AWS configure ). Installing AWS CDK npm install -g aws-cdk Concepts Construct : Represents a cloud component. It can be a low-level component (S3 bucket) or a higher-level one like an ECS application that includes other services like ELB, Escalation Rules, etc. Stack: Deployment unit. Each one relates to an AWS CloudFormation stack. All resources on the same stack are provisioned as a single deploy unit. Environment: An environment is the target AWS account and AWS Region into which the stack is intended to be deployed. Tagging: Allows you to tag all resources related to a construct (Tag.add(myConstruct, ‘key’, ‘value’);) Assets: Files or directories in your CDK application to be referenced by supported resources. Ej: ECS container images could use a docket file embedded in your app. A lambda handler could reference an asset to specify its code. Main commands Create a new folder for your CDK application. Inside that folder, open terminal and run: cdk init -- language java This will create a new maven project. When your application is ready, you will need to run: cdk synth That command will generate the Cloud Formation template that will be used on AWS to provision all resources. To deploy your application to AWS and effectively provision all resources, you need to run: cdk deploy Another useful command is cdk diff , which will show you the difference between the application currently deployed and your local version of it. Example ECS App app = new App() ; //Instantiates a new CDK app, which is a construct in itself Stack stack = new Stack(app , \"hello-cdk\" ) ; //Registers a new Stack to the construct Vpc vpc = new Vpc(stack , \"VPC\" ) ; //Creates a new VPN associated to a stack Cluster cluster = new Cluster(stack , \"My Cluster\" , ClusterProps. builder ().vpc(vpc).build()) ; //Creates a new cluster associated with the vpn FargateService service = new FargateService(stack , \"My Service\" , FargateServiceProps. builder ().cluster(cluster).desiredCount( 5 ) .taskDefinition( new TaskDefinition(stack , \"My Task\" , TaskDefinitionProps. builder ().cpu( \"512\" ).memoryMiB( \"1024\" ).build())) .build()) ; //Creates a Fargate Service and task associated with this cluster app.synth() ; After running cdk deploy for this application, an ECS cluster will be created associated with a VPC and a Fargate task using 512 CPU units and 1024 MB of memory. Specifying Region/Account Stack stackUs = new Stack(app , \"hello-cdk\" , StackProps. builder ().env(Environment. builder ().account( \"123\" ).region( \"us-west-2\" ).build()).build()) ; //Registers a new Stack to the construct Stack stackUe = new Stack(app , \"hello-cdk\" , StackProps. builder ().env(Environment. builder ().account( \"123\" ).region( \"us-west-2\" ).build()).build()) ; //Registers a new Stack to the construct Doing this from a single CDK app, you can provision resources on multiple AWS accounts and/or regions. More info API Reference: https://docs.aws.amazon.com/cdk/latest/guide/reference.html Construct Library: https://docs.aws.amazon.com/cdk/api/latest/docs/aws-construct-library.html Javadocs: https://docs.aws.amazon.com/cdk/api/latest/java/index.html", "date": "2020-02-03"},
{"website": "Moove-It", "title": "our-customer-centric-model-with-an-nps-of-82-certified-by-pwc", "author": [" Gabriel Fagundez "], "link": "https://blog.moove-it.com/our-customer-centric-model-with-an-nps-of-82-certified-by-pwc/", "abstract": "Since our foundation 15 years ago we have been proud of the feedback we have received from our clients. We have always received great comments regarding the quality of our work, how we approach problems as a team, and the way in which we grow our partnerships with our clients throughout their business lifecycle. We have also been aware of our errors. Every time we made mistakes in the past, we spent a reasonable amount of time figuring the real root of each problem, absorbing our inefficiencies with each affected client, and ultimately showing that we are a trustable partner. During all these years, we have accompanied many clients in their journey from their first days of life through the transformation to be recognized as a successful company in their own industry. Our customer-centric approach allowed us to transform ourselves and learn from each experience, growing to a 150 people company that is building products for a great variety of clients in a wide range of industries, including Healthcare, Fintech, Entertainment, Education, IoT, Real Estate, and even other smaller, as Computer Software, Chemicals, or Hiring & Recruiting. We are proud to be helping amazing startups, industry-recognized companies, and Fortune 500 companies disrupt their industries while improving people’s lives around the world. However… Were our impressions real? We were encouraged by the consistent positive feedback we received from our clients, but we wanted to look deeper so we started to investigate if there was a good metric we could use to accurately measure our client’s satisfaction. That’s why 2 years ago we decided to start measuring our client satisfaction using the Net Promoter Score (NPS) . The NPS is the percentage of clients rating their likelihood to recommend a company, a product, or a service to a friend or colleague. It has been widely adopted by Fortune 500 companies and other large organizations, and it is based on a simple question: How likely is it that you would recommend our company/product/service to a friend or colleague? Based on the answers to this simple question, you can calculate a number in the range of -100 to +100, which in turn gives you a measure of your client’s satisfaction with the provided service. The higher the Net Promoter Score is, the happier the different clients are with the quality of your service. Our latest score is amazing: 82.02 without any detractors Measuring the NPS consists of several stages, in which different roles are involved. At a glance, we first put together all the different clients that were exposed to our variety of services during the period we are measuring. That means all the companies that we have been working with, no matter if they were using our Product Discovery services, a staff augmentation development workforce, or a solid and deeper partnership involving several roles and profiles. All were included to give a complete picture. Secondly, we define the audience. Our policy in this stage is simple: we include in the survey all the different counterparts, no matter what role they are playing in their company, that were exposed to Moove It’s team. In this stage, we have several counterparts: C-Levels, project managers, product managers, other developers, designers, managers, and even company owners and board members. All of them share an important trait: they are interested in Moove It’s quality of service to some extent. After defining the audience, we proceeded to send the survey by email. This process was executed by our Operations Analyst, who is also in charge of the Weekly Reporting process. After getting the responses, we then proceed to the calculation: Scores of 9 and 10 are considered “Promoters”. Those are counterparts that are highly satisfied with our services and are willing to recommend us to friends, colleagues, and other companies. Scores of 7 and 8 are considered “Neutral”. Even though they are not fully satisfied with our services, they are not detractors. Scores of 6 and below are considered detractors. This group is not only satisfied but also not willing to recommend you at all. The final NPS is calculated by subtracting the % of survey respondents who are detractors from the % of survey respondents who are promoters . Your score will land somewhere between -100 to 100. Our final NPS, 82.02, is a simple demonstration of the high quality of our work and a validation of our engagement model. This number puts us solidly as a high-end company that truly delivers outstanding services to our clients . Taking a look at Satmetrix’ NPS Benchmark , one of the co-creators of this popular customer success metric, the NPS varies widely by industry, however, the average always stands between 0 and 60. An NPS of over 80 points is achieved only by those companies that provide a highly customized service, in which a big portion of the clients are satisfied, and it’s rare to have detractors, and this is exactly where we are standing. Auditing our process by PricewaterhouseCoopers (PwC) Apart from providing a high-quality service, we also strongly believe in transparency and professionalism . We apply these same values, that we consider a must-have with our clients, internally as well. That’s why we decided to have our Net Promoter Score survey process audited by PricewaterhouseCoopers , one of the big four. The team at PwC has conducted a deep examination of the calculation of our Net Promoter Score and survey process, ensuring that each aspect of the process was well executed. After months of investigation, they approved it, providing us a written certification of their conclusions. Our next challenge is to transform Moove It into a Global Software Innovation firm by 2025. Reaching such an ambitious goal requires us to focus on providing a global service to our clients. We work with world-class companies that force us to be a best-in-class provider. We are flexible, and we learn from our mistakes. We are always pushing for ways to improve quality, and, as a client-centric company, we want every single client to feel that they are working with a company that is providing them with the technical advantage needed to make a difference in their industries.", "date": "2020-12-28"},
{"website": "Moove-It", "title": "strategic-investment-artificial-intelligence", "author": [" Ariel Luduena Karhs "], "link": "https://blog.moove-it.com/strategic-investment-artificial-intelligence/", "abstract": "Moove It was created with delivering quality as its fundamental principle. Our vision has always been to build a consulting firm to deliver the highest quality services possible in the software industry, regardless of which services we would gradually include in our portfolio, and which technologies we would apply to achieve our clients’ goals. Almost 15 years later, we continue to be guided by that same vision, always striving to deliver the highest quality services and generate exceptional results for our clients, no matter the area of expertise. Taking a step into the Artificial Intelligence world was a logical choice for Moove It . We are, at the core, a team of innovative software engineers, who are always investigating next-generation solutions and many of us had already experimented with the technology prior to the discussion of adding it as a practice area. The maturity of the technology stack combined with the growing demand from our clients, and the industry in general, made the decision simple; we needed to add the practice into our portfolio. The real challenge was determining the best strategy that would allow us to deliver a quality service offering from day one. Marvik is an Artificial Intelligence consulting firm founded by Paula Martínez and Rodrigo Beceiro in 2016 with the same philosophy regarding quality as Moove It but laser-focused in Artificial Intelligence. The company has worked in several industries, providing services to clients in areas such as Computer Vision, Natural Language Processing, and Predictive Analysis. The investment will allow Moove It and Marvik to join forces, leveraging our combined expertise, in order to take on greater challenges together. Our joint goal is simple: to create one of the best Artificial Intelligence engineering teams in the industry, focused on providing high-value, strategic consulting services to our clients.", "date": "2020-08-05"},
{"website": "Moove-It", "title": "moove-it-certified-as-a-great-place-to-work-with-outstanding-results", "author": [" Florencia Sigalotti "], "link": "https://blog.moove-it.com/moove-it-certified-as-a-great-place-to-work-with-outstanding-results/", "abstract": "The process Moove It is all about teamwork. Among our core values are “strong relationships” and “happiness and flourishing work environment”, so we are always trying to find ways to improve as an organization. We pride ourselves on having a powerful work ethic, along with principles that are focused on creating long-term partnerships with our employees.  That’s why, even in this challenging year, we decided to get involved in the Great Place to Work® certification process. Great Place to Work® is a well recognized international company specialized in organizational climate and culture. It aims to help companies to build excellent workplaces through the analysis of their 5 dimensions:  Credibility, Respect, F airness, Pride, and Camaraderie. From the very beginning, it was a powerful introspective process, where we had to examine and review our way of doing things. A lot of the questions that came up were the following, among others: Are we being true to our culture? To our values? Are Moove It collaborators happy with our company? What do we want to change? This process had many parts. From analyzing the tool and the benefits it could have for us and our organization, to carefully planning the execution of the process and the communication within the company. It was crystal clear to us that we wanted all our team members to feel part of the process every step of the way. To ensure that the team was fully involved in the survey, we relied on strong communication from the start. We explained the reason for filling in the survey and what our main goal was, then clearly communicated the progress we made in each step we took. It was paramount that all participants felt assured we were going to listen to their opinions. It was a journey that required great commitment: commitment on our part, to use the inputs from the survey to truly improve; and excellent participation and commitment from the collaborators, for their willingness and openness in completing the survey with interest and sincerity. We reached an impressive score! After we finished the process, the results came in. We were frankly astonished! We reached 96% in the Great Place to Work® statement. The average on the platform is 85%! And that’s not all: we had a 91% score on average in all 60 statements in the survey ( vs. 63% of other companies). Beyond the scores, the most important thing is that we found valuable opinions from our employees, who sought to take the time and contribute so that Moove It remains an excellent place to work. It was a process that generated an even deeper insight into the organization, which allowed us to think and rethink our culture. Above all, it reaffirmed that, in line with our values, the best results are always achieved through the effort of teamwork.", "date": "2020-12-21"},
{"website": "Moove-It", "title": "hands-on-react-navigation-5", "author": [" Seba Paz "], "link": "https://blog.moove-it.com/hands-on-react-navigation-5/", "abstract": "All you need to know about React Navigation 5 We are big fans of experimenting with the latest, cutting-edge technologies. And the most popular navigation library for React Native, is no exception. In this article, we will dive into the core features of this big update, as Sebastián Paz tells us about his experience when upgrading from an old version of the library. With this new version, we get an improved navigation component, smoother UI effects and transitions, and a new built-in dark theme. Let’s jump ahead and get started! Component-based navigation It’s well known that the navigation concept in React Native tends to be wrongly conceived as one or many JavaScript objects and functions working accordingly, as its declaration syntax used to suggest. In previous versions of React Navigation, setting up the navigation component was a matter of declaring one or many JavaScript objects and passing them to functions that returned the entire navigation component. This was the standard method for a long time, and while it worked fine at meeting most project’s requirements, it always had its caveats. Probably the most critical one was that developers were unable to alter the navigation object at runtime without using some shady workaround or alternative way to make things work as expected. This major update brings the ultimate solution for this concern: now it’s possible to declare the navigation object as a React component! This enforces developers to understand that the navigation system is built upon components, just like everything else in React. This just makes things better. We are now defining the navigation component dynamically, which opens up a wide range of possibilities. For instance, you may want to add Redux here and access some store property at runtime that could hide or show tabs in tab navigation, or screens in a stack-based navigator. Debugger Integration Another big feature is the debugger integration. After a while developing projects that ran previous versions of React Navigation, sometimes navigating elsewhere produced undesired results. Gathering insights about a navigation error was a really tough thing to do, particularly because there was no available data or error trace to analyze. This change gives us just that. We can now attach any React Debugger and the entire navigation object will show up available to inspect. For example, using React Native Debugger, you can see the trail of dispatched navigation events. Whereas on the right, there’s the whole navigation state object, containing detailed information about routes and the parameters that are being sent from one route to another. Themes Thanks to its redesigned Theme API, React Navigation 5 enhances adding themes to your project. Projects now can be decorated with multiple themes that can dynamically change, just by writing a few lines of code. In React Navigation 5, you can specify a theme by writing a JavaScript object defining a few properties such as the primary, background and text colors, as in the image below: Then, adding the Theme object to the parent navigation container should do the trick: What do we have so far? The theme object is now created and included in the navigation container, overriding the default application theme. The final step is to paint our components with the colors we just set: And that’s it! The button text is now accordingly styled as the theme we just created. Notice that this powerful feature can be replicated throughout the rest of your components and the whole app color set will respond to a single theme object that can be modified at any time. But what about dark mode? Lately, there has been a lot of hype surrounding dark mode, a reversed color scheme that utilizes light typography, iconography and UI elements on dark backgrounds. The number of apps that support it is constantly increasing as time goes by, and more projects are now starting to consider it as an important requirement to meet. Setting up a dark theme was already possible in previous versions of React Navigation. However, now it’s much easier to add dark mode to a project, as it includes a built-in, predefined theme object that brings the native dark mode color scheme. You can create your own theme overriding this object or use it right after importing it. Look how simple it is. First import the built-in dark theme from the React Navigation package: Then, include a theme prop in the main navigation container component with the dark theme we just imported: The steps above, combined with some extra logic to show up one theme or another based on the system settings, produces the following results: Upgrading This is a big update. The main part of the library core was rewritten, which caused significant changes to the API. I had the opportunity to upgrade one of our existing React Native apps, which was running React Navigation 3. Luckily, there were just a few changes in the API between v3 and v4, so the upgrade guide from v4 was highly useful to me, even when migrating from an older version.  It did become a harder task, though, as there was navigation-related business logic that had to be rethought in terms of the new library version. This was one of the hardest things to figure out since I had to rewrite all the navigation components (particularly, a drawer navigator with a bunch of nested stack navigators) and as React Navigation 5 defines navigation screens dynamically and with React components, the createSwitchNavigator function was removed , as well as pop and popToTop functions, which were frequently used assets in projects with older versions of the library. The project size plays its role as well: the larger the project, the harder it to upgrade. On most occasions, upgrading is a long and challenging task. However, existing apps are worth upgrading. But if you’re planning to do so, make sure you have enough time. And if you don’t, consider starting new projects with the new library version. Conclusion Thank you for reaching the end of this post! We know that starting a new React Native project from the ground up can be overwhelming at its early stages. That’s why we have built a React Native Template with everything you need to quickly start a new project and begin working on what’s important. And, of course, we already updated it to the latest version of the library. React Navigation 5 defines a new paradigm that came here to stay and is prone to be updated in future releases. But one thing is true: navigation is now better in React Native. I can’t wait to see the amazing apps you build with it! Becoming a React Native Development Company has been a long way, and we can provide one definitive piece of advice regarding this challenge: practice, study and use all the available material on the internet to get better at it. Do you want to know more about Moove It? Check out our React Native developments here .", "date": "2020-04-13"},
{"website": "Moove-It", "title": "nginx-ssl-security", "author": [" Jared Selcoe "], "link": "https://blog.moove-it.com/nginx-ssl-security/", "abstract": "SSL certificate security has never been more important than it is today. Browsers have begun to show users a warning on sites that don’t use HTTPS, more confidential information than ever is being communicated via web applications, and data-snooping criminal activity continues to grow year over year. As such, today I am going to walk you through the process of hardening your server’s SSL certificate security via NGINX configurations. To do this you will need a server with: Ubuntu v16.04 or greater (required for http2) OpenSSL v1.0.2 or greater NGINX v10.13 or greater (required for TLSv1.3) If you have an older version, this is a good guide for updating your NGINX installation . Please note that the apt-get dist-upgrade command that it instructs should actually start with sudo . An SSL certificate already installed Okay – let’s begin! Basic Setup Though you are likely already doing this, first let’s make sure you are: Forwarding all traffic from port 80 to port 443 (the HTTPS protocol’s port) Using http2 Using your SSL certificate To forward all traffic to port 443, modify your /etc/nginx/sites-available/default file so that it reads: server {\n  listen 80 default_server;\n  listen [::]:80 default_server;\n  # Redirect all HTTP requests to HTTPS with a 301 Moved Permanently response.\n  return 301 https://$host$request_uri;\n} 1 2 3 4 5 6 server { listen 80 default_server ; listen [ :: ] : 80 default_server ; # Redirect all HTTP requests to HTTPS with a 301 Moved Permanently response. return 301 https : //$host$request_uri; } Because this file is imported in your /etc/nginx/nginx.conf it will be applied for all requests to all domains that your server manages, so long as you have it symlinked in /etc/nginx/sites-enabled . To use http2 and your SSL certificate for a specific domain, modify your etc/nginx/sites-available/YOUR_FILE (could be default if you’ve kept things simple) so that it reads: server {\n  # SSL configuration\n  # Use of default_server here is optional, but makes explicit this is the default\n  # server for this port\n  listen 443 ssl http2 default_server;\n  listen [::]:443 ssl http2 default_server;\n  server_name YOUR_SERVER_NAME;\n\n  ssl_certificate PATH_TO_YOUR_CERT;\n  ssl_certificate_key PATH_TO_YOUR_CERT_KEY;\n\n  # You may have also had “ssl on;” in this file - specifying “listen 443 ssl”\n  # above has the same effect so you can remove “ssl on;”\n\n  # You should disable gzip for SSL traffic if you have it enabled to avoid the BREACH vulnerability\n\n  # The remainder of your server config goes below\n} 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 server { # SSL configuration # Use of default_server here is optional, but makes explicit this is the default # server for this port listen 443 ssl http2 default_server ; listen [ :: ] : 443 ssl http2 default_server ; server_name YOUR_SERVER_NAME ; ssl_certificate PATH_TO_YOUR_CERT ; ssl_certificate_key PATH_TO_YOUR_CERT_KEY ; # You may have also had “ssl on;” in this file - specifying “listen 443 ssl” # above has the same effect so you can remove “ssl on;” # You should disable gzip for SSL traffic if you have it enabled to avoid the BREACH vulnerability # The remainder of your server config goes below } Enable HTTP Strict Transport Security (HSTS) By default, when modern browsers detect HSTS headers they automatically stop attempting to use anything but HTTPS for the length of time specified in the header. We use this in conjunction with the port redirecting that we have already configured so that normal users will receive the proper idiomatic HSTS headers, and bad actors will be forcefully redirected. Update /etc/nginx/nginx.conf ’s http {} block to contain: # Add HSTS header to all requests to force browsers to use HTTPS\n# Do \"max-age=15768000; includeSubDomains\" instead of just \"max-age=...\" if using subdomains\nadd_header Strict-Transport-Security \"max-age=15768000\" always; 1 2 3 # Add HSTS header to all requests to force browsers to use HTTPS # Do \"max-age=15768000; includeSubDomains\" instead of just \"max-age=...\" if using subdomains add_header Strict - Transport - Security \"max-age=15768000\" always ; Restrict Protocols and Ciphers Now, let’s make sure you are: Prioritizing the SSL protocol TLSv1.3, with 1.2 as a fallback, and rejecting all others Restricting the permitted ciphers The following changes will all be made to your /etc/nginx/nginx.conf file’s http {} block because we want these changes to apply to all traffic, not just a particular server: ssl_protocols TLSv1.2 TLSv1.3; # Prefer 1.3, fallback to 1.2, reject all others\nssl_prefer_server_ciphers_on; # Use the server’s cipher preference, not the client’s\n\n# A list of all ciphers to permit, sorted most to least preferred. ! indicates ciphers to reject.\nssl_ciphers \"EECDH+AESGCM:EDH+AESGCM:ECDHE-RSA-AES128-GCM-SHA256:AES256+EECDH:DHE-RSA-AES128-GCM-SHA256:AES256+EDH:ECDHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-SHA384:ECDHE-RSA-AES128-SHA256:ECDHE-RSA-AES256-SHA:ECDHE-RSA-AES128-SHA:DHE-RSA-AES256-SHA256:DHE-RSA-AES128-SHA256:DHE-RSA-AES256-SHA:DHE-RSA-AES128-SHA:ECDHE-RSA-DES-CBC3-SHA:EDH-RSA-DES-CBC3-SHA:AES256-GCM-SHA384:AES128-GCM-SHA256:AES256-SHA256:AES128-SHA256:AES256-SHA:AES128-SHA:DES-CBC3-SHA:HIGH:!aNULL:!eNULL:!EXPORT:!DES:!MD5:!PSK:!RC4”;\n\nssl_session_cache shared:SSL:5m; # Share the cache with all worker processes across cores; Name the cache SSL; Set to 5 min\nssl_session_timeout 1h; # The length of time a client can reuse session parameters 1 2 3 4 5 6 7 8 ssl_protocols TLSv1 . 2 TLSv1 . 3 ; # Prefer 1.3, fallback to 1.2, reject all others ssl_prefer_server_ciphers_on ; # Use the server’s cipher preference, not the client’s # A list of all ciphers to permit, sorted most to least preferred. ! indicates ciphers to reject. ssl _ ciphers \" EECDH + AESGCM : EDH + AESGCM : ECDHE - RSA - AES128 - GCM - SHA256 : AES256 + EECDH : DHE - RSA - AES128 - GCM - SHA256 : AES256 + EDH : ECDHE - RSA - AES256 - GCM - SHA384 : DHE - RSA - AES256 - GCM - SHA384 : ECDHE - RSA - AES256 - SHA384 : ECDHE - RSA - AES128 - SHA256 : ECDHE - RSA - AES256 - SHA : ECDHE - RSA - AES128 - SHA : DHE - RSA - AES256 - SHA256 : DHE - RSA - AES128 - SHA256 : DHE - RSA - AES256 - SHA : DHE - RSA - AES128 - SHA : ECDHE - RSA - DES - CBC3 - SHA : EDH - RSA - DES - CBC3 - SHA : AES256 - GCM - SHA384 : AES128 - GCM - SHA256 : AES256 - SHA256 : AES128 - SHA256 : AES256 - SHA : AES128 - SHA : DES - CBC3 - SHA : HIGH : ! aNULL : ! eNULL : ! EXPORT : ! DES : ! MD5 : ! PSK : ! RC4 ” ; ssl_session_cache shared : SSL : 5m ; # Share the cache with all worker processes across cores; Name the cache SSL; Set to 5 min ssl_session _ timeout 1h ; # The length of time a client can reuse session parameters Create A Stronger Diffie-Hellman A Diffie-Hellman key is used for our SSL handshake with clients. By default, this key is 1024 bits. We want to make sure that if we are using a 2048+ SSL certificate we do not diminish its security by using a 1024 bit key during our key exchange/handshake. So, let’ s create a stronger Diffie-Hellman key. In the terminal, enter openssl dhparam -out /etc/ssl/dhparam.pem 4096 – this will take a while! Then add the following lines to the /etc/nginx/nginx.conf file’s http {} block: ssl_dhparam /etc/ssl/dhparam.pem; # Use our custom strong Diffie-Hellman for handshakes\nssl_ecdh_curve secp384r1; # Secure it with an elliptic curve algorithm instead of RSA 1 2 ssl_dhparam / etc / ssl / dhparam . pem ; # Use our custom strong Diffie-Hellman for handshakes ssl_ecdh_curve secp384r1 ; # Secure it with an elliptic curve algorithm instead of RSA Confirm Everything Works First, let’s make sure there are no syntax errors with our configurations: sudo nginx -t Now let’s run our configuration tests: sudo /etc/init.d/nginx configtest Then, restart NGINX: sudo service nginx restart Now that everything is configured, run the Qualys or Comodo testing suites and look at your fancy A+ score! At Moove It we work to stay on top of the latest developments in online security to ensure that our clients and their data are kept safe in an increasingly complex and dynamic digital environment. I hope that this guide helps you to bring some of that same sense of security to your next application! Sources: https://ubuntu101.co.za/nginx/upgrade-nginx-1-12-ubuntu-16-xenial/ https://www.digitalocean.com/community/tutorials/how-to-set-up-nginx-with-http-2-support-on-ubuntu-16-04 https://raymii.org/s/tutorials/Strong_SSL_Security_On_nginx.html http://nginx.org/en/docs/http/ngx_http_ssl_module.html#directives", "date": "2019-05-09"},
{"website": "Moove-It", "title": "how-to-set-up-a-google-cloud-cdn-for-serving-images", "author": [" Jared Selcoe "], "link": "https://blog.moove-it.com/how-to-set-up-a-google-cloud-cdn-for-serving-images/", "abstract": "The Problem Images are an integral part of many websites’ user experience, but storing and serving images can be a deceptively challenging problem. If you store your images on a server filesystem, the server and codebase need to be optimized for rapid file reads. If you store your images in a database, space can rapidly become an issue. What’s more, if you try to send images to the UI via the standard “data on the wire” approach used for most UI-server data exchanges, you likely have to deal with image encoding, are going to load up your server bandwidth, and will find that the UI can rapidly slow down as it decodes and loads all of the images being sent. Surely there must be a better way. The Solution Enter the CDN: your handy dandy globe-spanning network of servers whose whole job is to serve files rapidly. If you would like to learn more about CDNs, Cloudflare has an excellent overview , and SolarWinds as a great practical explainer of CDN considerations . Cloudflare also has an awesome CDN, but we won’t be talking about their CDN today. Today we will go through the process of setting up a Google Cloud CDN. We are using Google’s CDN for several reasons. They consistently have a 10% faster response time than Amazon or Cloudflare , both of which are blazing fast already. Their CDN is part of a larger ecosystem of web hosting tools which you may already be using. If you aren’t using any Google Cloud products, they are currently offering a $300 credit to entice you to start. Frankly, their documentation is inscrutable, their UI is clean but infuriatingly vague at times, and I had a hard time finding any good resources for this process. So this seems like a good opportunity to help people navigate their system. Let’s dive in! Create An Account First let’s set up a Google Cloud account. If you have already done this, move on to the next step. Go to the Google Cloud Dashboard page and sign in . Once the dashboard loads, you’ll be greeted by a lovely message telling you to create a project Click the “CREATE” button and go through the creation form. Upon completion, you’ll be greeted by a fairly cacophonous dashboard showing you a whole lot of nothing: Save that money! Check out the free $300 voucher offer at the top of the page. Sign up for it. That’s a lot of moolah! If you don’t do this, you will still need to set up billing before you can continue. Set Up a Load Balancer – Getting Started Google handles a lot of network management for you. This is a blessing and a curse, because the purpose of some of the abstractions they’ve created can be unclear. On the menu on the left side of the screen, under the Networking section, click “Network Services.” You may see a message that the Compute Engine is getting ready. This is because you just set up a billing mechanism for your account, so they are now spinning up the infrastructure for their services. Once the message disappears, you should see a prompt to create a Load Balancer: Select your desired load balancer. We will be using the “HTTP(S) Load Balancing” option because we will eventually display our images in our app via <img> tags that point to files stored in our CDN. You now need to do 4 things to create your load balancer: give your load balancer a name, create a backend, create host/path mappings, and create a frontend. Set Up a Load Balancer – Name Make the name whatever you want – I will simply name mine “assets.” Set Up a Load Balancer – Create a Backend The backend will be our bucket where we dump our image files. First, select the “Create a backend bucket” option On the form that appears, give your bucket a name and then click the “Browse” button under “Cloud Storage Bucket.” On the form that appears, click the icon to create a new bucket. Yes, this is slightly confusing. You are creating a bucket which will then be assigned to… a bucket. Their terminology is a bit muddy here. As far as I can tell the parent bucket is itself just a collection of globally-uniquely-named buckets. Yet another form will appear – we are basically at Inception-levels of nested forms at this point. Let’s keep it simple. Give your bucket a name that is totally unique – as in, nobody else in the world can have the same name. I will call mine “moove-it-site-images”, and keep all other settings the same because they are fine for our needs, not to mention cheap! Note the “Access control model” setting at the bottom. Feel free to check bucket-level or object-level permissions based on how you will be using this CDN. Bucket-level is more simple, so that may be your best bet. Once created, you will go up one level to the parent form. Select your new bucket at the top of the form, then click the “Select” button at the bottom of the screen. Once selected, you’ll now be nearly free of this nested form madness. Make sure you check the “Enable Cloud CDN” button. To quote Google – “Cloud CDN caches HTTP(S) content closer to your users so content delivery is faster while also reducing serving costs.” – why wouldn’t you do it? Once you’ve done this, click the “Create” button to escape. Set Up a Load Balancer – Host/Path Mappings The host and path are the components of URLs that map to your various backends. For our purposes we are going to keep it simple and map all traffic to our “image-bucket” backend. The UI should have auto-selected your new “image-bucket” backend automatically. If it didn’t do so now. Set Up a Load Balancer – Create a Frontend The frontend is the portion of the load balancer that faces the outside world. It consists of an IP address and SSL cert. Begin filling out the frontend creation form with the following info. Name it whatever you want. Get a static IP address – we will add this IP address to an A record in our DNS configuration a little bit later. Name it whatever you want. Under the “Certificate” section, create a new certificate Time for another nested form! On this form, make sure to give your certificate a descriptive name, and just let Google handle the rest with the “Create Google-managed certificate” option. It’s free and easy, so why not? Lastly, make sure the domain you point to exactly matches the domain you will be setting in you is one you have access to. Your completed frontend should look something like this. Click “Done”: Before we move on, let’s add our new static IP address to an A record in our DNS configuration. If we do not do this, the certificate we created will fail as it gets provisioned because it will not have a valid record that it corresponds to. The process for doing this will vary based on who is hosting your domain. The important things to ensure are that the domain specified exactly matches the domain you specified for the SSL cert (“cdn.moove-it.com” in our case), and that the IP address exactly matches the new static IP address we created (“34.98.85.214” in our case). With that done, go back to the Google Cloud UI and click the “Create” button. It will take a moment to create the configuration. Once complete, wait for your SSL cert to get provisioned. You can check its status by hovering over its info icon as you can see below. Store Files On Your CDN You can transfer files to your new bucket via the gsutil that Google provides , or do so directly through their UI. Let’s do the latter. On the main menu shade on the left of the screen, go to “Storage” > “Browser.” Select your new bucket from the list that appears. Click the “Upload files” button and pick your files. The folder structure and names of files you upload will correspond directly to the paths you use in the URLs that point to your CDN content. Now we need to make the files public. You can do this by following Google’s documentation – specifically the “Making groups of objects publicly readable” section on the bottom half of the page. View Your Files! Wait for the status of your certificate to be “VALID” – once it is you can see your files by visiting your.cdn.domain/path/to/file.jpg. Wrapping Up Now that you have a CDN for serving images (and anything else you want) you can do some pretty neat stuff like send images in emails without having to attach those images and load multiple images in parallel in your UIs via <img> tags. Next steps could include creating a Node service to manage writing files to your CDN via Google’s gsutil npm package , too! Here at Moove It we love building elegant, maintainable, scalable software, which requires mastering an ever-changing set of technologies. CDNs are far from the only technology we need to understand, but they are an important one. I hope that this walkthrough helped you become a little more conversant in Google Cloud’s vernacular and UI, as well as the concepts underpinning CDNs. If you’re a developer looking for an exciting new opportunity to do varied work at a company that values learning and growing, or are someone searching for a team of passionate developers to help turn your vision into a reality, you know where to find us. Until next time! Sources https://www.cloudflare.com/learning/cdn/what-is-a-cdn/ https://www.pingdom.com/blog/benchmarking-cdns-cloudfront-cloudflare-fastly-and-google-cloud/ https://www.cdnperf.com/cdn-compare?type=performance&location=world&cdn=aws-cloudfront-cdn,cloudflare-cdn,google-cloud-cdn https://console.cloud.google.com/home/dashboard https://cloud.google.com/storage/docs/access-control/making-data-public https://github.com/googleapis/nodejs-storage", "date": "2019-06-26"},
{"website": "Moove-It", "title": "our-meaningful-end-of-year-gifts", "author": [" Mercedes Acevedo "], "link": "https://blog.moove-it.com/our-meaningful-end-of-year-gifts/", "abstract": "Gifts with meaning At the end of each year, we enjoy giving thoughtful gifts to our clients and friends. This time around we wanted to give something both meaningful and material, something that represented us and conveyed our values. We realized that books are the perfect way to do exactly that, so we asked our teammates to recommend books that have a special meaning to them, and to tell us why they recommended each one. After that, we sent each of our clients and friends a gift box filled with the books that we had all hand-picked, including the teammates’ name, a mini-biography, and the reason why that specific book had been chosen. They got something like this: A box including the recommended books . Some of the recipients decided to create a library in their offices. Others shared books among themselves. One of our clients even told us they felt so inspired that they are planning on starting an office book club! Here are a few of the book recommendations we sent out: To make things more interesting we randomized the selection of the books we sent in each box so nobody would know beforehand who was going to receive which book. This added a little bit of mystery and fun! Thanks to the se gifts we not only got a big ‘thank you’ from all of our clients but also had an opportunity to start a conversation about tastes and interests, issues that unite us beyond the daily hustle and bustle of work. If you want to see the complete list of books we sent, click here to download it. And if you’re curious about the letters that were sent with the books, here . On behalf of everyone in the Moove It team, we wish you a Merry Christmas and a happy New Year.", "date": "2019-12-24"},
{"website": "Moove-It", "title": "our-experience-drawing-a-ruler-with-spritekit", "author": [" Federico Ojeda "], "link": "https://blog.moove-it.com/our-experience-drawing-a-ruler-with-spritekit/", "abstract": "Introduction This spike was aimed at gathering knowledge and practical experience on SpriteKit through carrying out an exercise of  (partially) porting the code of a ruler I had it implemented in Swift to SpriteKit. As a brief background, this ruler increases and decreases its size when scrolled (through a pan gesture). The rationale behind this migration was the promise that performance would increase if the ruler was implemented with SpriteKit compared with UIKit. The exercise took me about an afternoon to complete, including the task of writing this doc. Main lessons learned Using it in a non-game app Contrasting with what one might expect from a framework that is heavily oriented to build games, SpriteKit shows a great versatility, making it possible to create views in non-game apps. Unexpectedly, its learning curve is not steep for a developer who has no experience in game development. Main structure The steps to creating a SpriteKit view are: Creating the view Creating an outlet Setting up the scene Updating the scene Creating the view A SpriteKit view (SKView) is just another type of view that can be added through the Storyboard or via code. Creating an outlet The next step to start using SpriteKit is was create an outlet. How do we do that? We have to create a SKScene. To do that, it’s as simple as subclassing the class SKScene and define several methods. Setting up the scene To use the scene we just created, we present it in the SKView as follows: Updating the scene What methods should be redefine in the custom Scene? Even though there’s a lot of methods that provide granular options to manage the screen, I found that the following are the most important ones: didMove(to view: SKView) This method is the initialization point. It means it finished moving to a SKView. This is called exactly once, and it’s a good place to set up everything. In my case, I created nodes, added gestures to the SKView, and other setup Update This method is called everytime a frame is rendered (take into account that it renders 60fps in normal devices and in 120fps in promotion devices). Here we can do some validation regarding the state of the view, and make the required updates so the screen has the correct information Nodes Every SpriteKit view has nodes. Each node (SKNode) is what can be shown on screen. For example, if we are in a game, a character can be a node (from a sprite), a text can be a node, a line can be a node. Every singular element you want to show in the SKView will be a SKNode (or a subclass of it). How to draw custom shapes? As the name states, SpriteKit is oriented to drawing sprites . Sprites are just simple images that can be rotated, moved, and even collisioned using SpriteKit. The framework provides several tools to do that, such as many geometry functions and a fully-fledged physics engine. To draw sprites, the framework provides several nodes, including SKSpriteNode — the most used one. Nevertheless, in my case I didn’t want to use this feature because drawing a ruler demands drawing lines basically. Unfortunately, and as far as I learned during my brief research, using SpriteKit to draw arbitrary shapes in regular mobile apps is not a piece of knowledge one could easily find in blogs and available documentation provided by the SpriteKit community. How can we do that using SpriteKit? What I found was that it is possible to draw arbitrary shapes using a special node called SKShapeNode. This allows us to create a shape on the screen by giving it a UIBezierPath, something that every iOS developer has used to draw on screen: Performance We wouldn’t be testing a framework if we didn’t test its performance. After finishing the proof of concept, I got some interesting data from the Xcode Profiler: Unexpectedly, it seems that CPU-usage is still very high, and that it is not using a lot of GPU. I still couldn’t define if this is due to the framework or if this phenomenon can be attributed to the way I am drawing lines and text on the screen. In a nutshell, I’m gladly impressed with SpriteKit’s versatility and I see a lot of potential, but I still need to investigate further to evaluate if it is the right tool for my business domain.", "date": "2019-09-19"},
{"website": "Moove-It", "title": "code-review-how-do-we-do-it", "author": [" Pablo Ifran "], "link": "https://blog.moove-it.com/code-review-how-do-we-do-it/", "abstract": "We are rigorous with the code we deliver. Just because? No. We are strict because we are what we deliver. Trustworthy, reliable and clean code make products scalable, clients happy and developers’ experience better. Since we started writing code more than 10 years ago, we have been refining a well-known process called “code review”. It is useful to deliver neat code, to learn from peers, to improve coding processes, and to transfer knowledge between teams and within the entire company. Even though code review seems to be a technical and low-level task, reality is, quite the opposite. We feel comfortable saying that it is a human communication process. The 3-step process for code review We use Github for code review. It provides a set of tools that assist in this process, such as Pull Request, comments, and even notifications regarding missing or recently added code. 1. The developer who wrote the code creates a pull request (PR) and lets the team know that the PR was created. (A Slack bot might be created to notify everyone that a PR was created). 2. Once the PR is created, someone else starts reviewing the code, in accordance with a guide providing rules, standards and processes. The guide suggests primitives to be nice to the person whose code is under revision: “Ask questions; don’t make demands”, “What do you think about naming this: user_id?”” or “Avoid using terms that could be seen as referring to personal traits. Assume everyone is attractive, intelligent, and well-meaning.” 3. After one or two developers have reviewed the code, one or both steps should be followed: The code needs improvement. Then, the developer could either start a discussion regarding the best way to solve the problem, or just follow the suggestions. This step is repeated until all developers are involved and agree on the fact that the code is good enough. The iterations stop and the code can be merged back into the development branch. If the code is good enough it can be merged into the development branch. Why is it important? Being transparent and consistent with this process has made the company more robust and horizontal. Knowledge is shared, thus improving quality, by assuring the code meets a standard which results in much more reliable deliverables. Since we started implementing the process the impact has been tremendous. Which is the result? At the beginning, the productivity seems to be slowed down, but as code review grows and multiplies among the team members, productivity actually increases. Bugs are found sooner, more ideas, tips, and solutions are shared, and, therefore, the quality of the delivery gets better and better. Besides, as the whole team is involved in the process, everybody will get to know the code and the architecture in greater depth. This leads to more accurate solutions, more maintainable products, faster bug fixing, and feature development. A win-win practice As mentioned before, we are big fans of doing code review. We have learned that it is important for our team members and for our clients. Fewer bugs, faster feature development, shared knowledge and better and deeper involvement. It is not just a practice, it is part of our DNA.", "date": "2019-02-21"},
{"website": "Moove-It", "title": "our-recruiting-process", "author": [" Martin Cabrera "], "link": "https://blog.moove-it.com/our-recruiting-process/", "abstract": "Our culture is our people. Products are designed and implemented by the team. Code is written, tested and deployed by our crew. And guess who is solving our problems and hacks…no wonder it is our staff. One of the most important tasks we have in Moove It is recruiting. In this article, we will try to explain how we do it, what we look for in technical & operative candidates and common mistakes candidates make. Furthermore, we will share some learnings, insights, and recommendations. Junior vs Senior candidates There are two big groups of people we look for: junior and senior professionals. The first group includes advanced students who are looking for a first or -even better- second work experience. We prefer individuals who are eager to learn, work hard, make mistakes, find solutions and investigate while getting involved with projects and teammates. As part of the recruiting process, we give developers some technical tasks to solve on their own. Sometimes, we want candidates to solve a complex challenge within the frame of a technology they feel comfortable with, or an easy one in a technology they are not very familiar with yet. To be honest, we do not really care if they have reached the best solution. We pay attention to the thinking process they are using while trying to solve the problem. We try to understand how they use design patterns or tests. After that, we schedule an onsite meeting to revise the questions they ask and the conclusions they arrive to. While trying to fill senior positions, the research is a bit different. We look for people with proven experience and deep and broad technical knowledge; 6-8 years of work experience as background is very much appreciated. Senior professionals also bring new processes, methodologies and ideas to the table. We encourage new hires to share good practices with the team. Even though we expect different things from junior and senior developers, we still expect a similar attitude. Being a team player is a must. Flexibility is also essential as well as the desire to grow and evolve. Ego? The exact opposite. Leadership and communication skills (in Spanish and English) are very welcome. Referrals Conversion rate from candidates to hires is pretty low: we always look for the best talent. So, when candidates are not a perfect fit for the company’s culture, we prefer not to move forward. We have made mistakes in the past, and learned from them. The situation is exactly the opposite when candidates are referred by current or past employees. In such cases, the hiring rate is extremely high. People who know our culture, methodologies and team generally refer people who are a good match. So, if you want to be part of our team, get to know our team, being referred is always a good strategy! Mistakes and Recommendations Something that not all candidates understand is that everything counts. Everything; emails, answers, the scheduling process, interviews, energy. Taking care of details is something we appreciate and thank During the interviews, what candidates say is as important as how they say it, body language speaks. Tone, silences, questions, reactions, everything is important. – Once, a candidate came to the interview with his mom. It might not be wrong, but it is, at least, weird. – It happens every once in a while that a candidate sends the CV dozens of times in just a few days, by means of different channels. Note: we receive all e-mails . If we do not answer it is because we are not looking for someone with your profile at the moment. A place to Grow Here, we can proudly say that people grow, evolve and learn. We design an individual career path for all of our team members, which is periodically analyzed and redefined. We understand the importance of being able to see the future, thus we plan, implement and deliver as expected. Not just with clients, but also with employees. Are you looking for a place to grow? This is the place for you. What do we offer? Being a key member of our growing team. Continuous technical challenges. An opportunity to contribute with your experience and ideas to exciting projects, that impact on the lives of millions of users. Growing as a professional and an individual. An awesome team culture. If you want to be part of our team do not hesitate and send your resume to jobs@moove-it.com or apply HERE . We are probably looking for someone like you.", "date": "2019-02-06"},
{"website": "Moove-It", "title": "a-horizon-expanding-experience", "author": [" Rafael Fremd "], "link": "https://blog.moove-it.com/a-horizon-expanding-experience/", "abstract": "Collision is North America’s fastest-growing tech conference. It brings together the people and companies redefining the global tech industry. We’re so happy we chose to be a part of this epic event by both attending and sponsoring it. A new mindset. We’ve sponsored many conferences including Ruby Conf , Node Summit , and Railsconf . They each focus on specific technologies, and as such are a great way to discover new developments in a relatively narrow set of topics, as well as new team members, hacks, and networking connections. We went to Collision with a completely different mindset that reflected our company’s shifting business model. Until recently we’ve focused a majority of our time on programming and consulting with clients to provide technical recommendations. More recently our approach has become more holistic as our clients’ needs and our capacities have grown. Development work is still at the heart of what we do, but over time we’ve developed many more competencies. We now also work with our clients on product ideation, and design, while considering and analyzing their broader organizational and competitive context. In short, we don’t simply create products – we create the right products. This shifting organizational approach demands a new mindset for what we look for in conferences that moves beyond the purely technical to the innovative and solution-oriented. With its expansive variety of attendees and sponsors across industries and disciplines, Collision was the perfect fit. The conference experience. 3 days. 30,000 attendees. 1,000,000 square feet. We knew these numbers going in, but you have to experience such a massive scale to truly appreciate it. Booth after booth overflowing with new and exciting ideas. A dozen stages buzzing with light shows and crowds. Countless workshops ranging from meditation to server deployment. The airy hum of countless conversations and activities reverberating around the cavernous interior of the Enercare Center. From the moment you entered the sprawling event space, the sense of engagement and excitement was palpable. Our booth had visitors nonstop. CEOs, CTOs, entrepreneurs, politicians, lawyers, students, developers, vendors – they all eagerly stopped by to chat with us. With so many visitors, our free gifts flew off our tables! Our visitors were kind and their conversations were engaging. So while we did not see many talks on the main stages, as we said previously, the “ best talks didn’t happen on stages but in the halls and booths .” We came back with lots of new leads and just as many new ideas, spurred by everything we saw and learned. Unexpectedly, we also walked away with a network of people ready to help us expand our operations into countries across the globe. We may not make that move quite yet, but the idea is exciting all the same! We loved Collision for many reasons: an utterly lovely host city (we miss you Toronto!), new experiences, and the chance to see exciting new technologies and ideas. What made it truly incredible though was the people. To see such a diverse group of people across so many disciplines coming together to share their experiences and ideas so enthusiastically and thoughtfully was eye-opening. The future is looking good. Optimism is more than a mindset – it’s a driver for real, concrete changes in the world. Companies like ACORN are born of optimism about what science can do, and by extension, what people can achieve. They collect your hair, transform it into stem cells, and then cryopreserve it for you to use decades from now when medical science has advanced far enough to utilize those young, healthy cells to create new organs, limbs, medicines, and more. Countries, while seemingly impersonal for their sprawl and scope, are ultimately comprised of people who are optimistic too. Many countries had stands at Collision promoting investment, cross-country collaboration, and offering legal, logistics and tax consulting. They were as excited as any startup co-founder to share their ideas and discuss their homelands’ potential. The optimists of the world spur change through their passion and vision for something better than the status quo, and Collision brought thousands of them together to share ideas and experiences. It gave us a glimpse of what waits in store for the world writ large, and we couldn’t be more excited. Thanks, Collision. See you soon.", "date": "2019-09-23"},
{"website": "Moove-It", "title": "methodology-waterfall-vs-agile", "author": [" Liz Parody "], "link": "https://blog.moove-it.com/methodology-waterfall-vs-agile/", "abstract": "In 2000, the FBI started developing a criminal case management software system called the Virtual Case File. It was a complete failure. The cost of this project was estimated to be over $170 million dollars and after 5 years of developing, prototyping and designing they had to abandon the project with nothing to present. Why did a project so large, with so many stakeholders and so much planning, fail so badly? For many reasons; the absence of enterprise architecture, a poorly designed system, lack of project management skills and a strong dependence on external contractors. But most of all, due to the methodology. They used the classic Waterfall methodology, where a 300 person team spent 6 months creating the requirements, with a grand design upfront, that resulted in 600 pages requirements, 400 documented ‘change requests’, 700,000 lines of code that were written and re-written time and time again, and no useful results. The Waterfall Methodology This doesn’t mean that every project that uses the Waterfall Methodology will fail. This methodology focuses on a more traditional and linear approach , where projects start at the first phase and only progress to the next when everything in the previous phase has been completed. All requirements must be gathered before any development occurs, (relying on heavy up-front analysis and documentation about the needs and problems of the client), followed by the process of design. Only once this is finished, will the implementation and development take place. In the final phases, there is the verification and maintenance stage, where the customer already has the end product and can test it according to the specifications only when the product is totally finished. The developers will then maintain the code, fix any bug that appears and keep it updated. For many years, this was the most widely used methodology and a large number of projects succeeded with it. When you see a large bridge, an airplane, a building of +50 floors, it most likely was build using Waterfall Methodology, taking into account that the client just won’t change the requirements middle way, since you would have to demolish the building or dismantle the airplane and that would cost money. Those projects must have a very long period of planning and up-front analysis to succeed. But, there is another kind of project where iteration, flexibility, constant feedback, and adaptation are essential. This is where the Agile Methodology comes in. The Agile Methodology The Agile Methodology is based on iterative and incremental development instead of a linear approach. It does not build an entire system at once, but rather develops it incrementally. Less time is invested upfront for documentation and analysis, as clients are constantly seeing and testing the product and providing feedback. The development and feedback process adds accountability (tangible milestones of completed work, not just documentation), and tends to improve client satisfaction by allowing ongoing input. Agile relies on a very high level of customer involvement throughout every phase of the project. The planning, design, develop, testing, release and feedback are in a constant cycle in a defined period of time. Instead of segmenting projects in stages, Agile development tends to address the projects as a whole. This model emphasizes the rapid delivery of a completely functional application components. Rather than creating tasks and schedules, all time is separated into phases called “sprints.” Each sprint has a specific duration (usually in weeks) with a list of deliverables, planned at the beginning of the sprint. Deliverables are prioritized by business value as determined by the customer. If all planned work for the sprint cannot be completed, work is reprioritized and the information is used for future sprint planning. This approach is more flexible. The requirements may change along the way and the team must be able to adapt easily, (the teams are usually smaller). There is greater transparency between the customer and developers, and the schedule and cost are predictable. After the failure of the FBI system, they had to implement a different approach in order to get different results. In 2010 the project was internalized, the FBI CIO took ownership and Agile was adopted as the project framework. The design was broken into 670 user stories, there were self-organizing teams, 45 staff (not 300 as previous) the product owner prioritized the work, and they had two weeks sprints (instead of 6 months sprint) with a demo in every sprint. The experience was successful, they reduced the cost in half, the agents started using the system on real cases, exceeding all expected targets. PROS AND CONS Waterfall Pros It is easy to understand and manage as stages are clearly defined. Meticulous record keeping and documentation. The client knows what to expect. The client will have an idea of the size, cost, and timeline for the project. The client will have a definite idea of what their product will do in the end. In the case of employee turnover, Waterfall’s strong documentation allows for minimal project impact . Waterfall Cons It often becomes rigid and resistant to change. It relies heavily on initial requirements. However, if these requirements are faulty in any manner, the project is doomed. The whole product is only tested at the end. If errors are discovered late in the process, their existence may have affected the rest of the project. The plan does not take into account the client’s evolving needs throughout the project cycle. Agile Pros It allows for changes to be made after the initial planning stage. It follows the client’s requirements changes. It is easier to add features that will keep the product up to date with the latest developments in the industry. At the end of each sprint, project priorities are evaluated. This allows clients to add their feedback so that they ultimately get the product they desire. The testing at the end of each sprint ensures that the errors are caught in each cycle. Agile Cons This dynamic methodology is not suitable for processes that require a complex decision making of formal planning such as construction, manufacturing, military, health care system among others. As the initial project does not have a definitive plan, the final product can be grossly different from what was initially intended. Moove It At Moove It , our main focus is on producing high-quality value for our customers. Create a beautifully-designed and functional components application in a short period of time, with small teams that can adapt easily and be flexible with the client needs and problems. Synchronize with the customer, adapt, iterate, deliver, again and again until getting the best possible outcome for the customer. Every client is different, the sprints, schedules, complexity, tools, teams, and technologies may change from customer to customer, but there is one common concept that applies throughout our projects; we use Agile methodology. And the reason is because it has been proved with multiple studies (and with our own experience), that for software development, the Agile methodology is more likely to help projects to succeed since it belongs to an industry that is constantly changing, and continuous improvement is highly valued, as is shown in the following image: Ambysoft’s 2013 Project Success Rates Survey The development team that uses Agile methodology has 64% success rate, compared to just 49% for the Waterfall model. Also, Ambysoft’s survey analyzed the main factors that contribute to a project’s success or failure and found the following: Agile methodologies are better suited for product quality, stakeholder value, ROI and Time/Schedule. Statistics clearly show that Agile methodologies consistently deliver more successful projects and fit better for software development teams than other methodologies. This is seconded by the 2017 study conducted by PWC, which also indicates that Agile projects are 28% more successful than traditional projects. This is why we apply this kind of methodology, not only statistics confirm Agile success, but also our team and happy customers. Conclusion Both of methodologies are widely used and many projects have succeeded with them. The Waterfall Methodology is more traditional, and for projects that need a big planning and a robust structure with no requirements being modified constantly, this is a great methodology to follow. In the other side, the Agile Methodology is great for teams that need to adapt constantly according to the client’s needs, focused on continuous improvement, flexibility, getting results rapidly and business value. However, Waterfall is not bad — the transition to Agile was largely dictated by the expectations of modern businesses, the nature of specific industries and the way projects are approached nowadays. Waterfall is still very useful (and the only methodology suitable) for many industries. At the end what it matters is the result, and Agile is thriving in software development because it works.", "date": "2019-02-15"},
{"website": "Moove-It", "title": "software-development-recipe", "author": [" Jared Selcoe "], "link": "https://blog.moove-it.com/software-development-recipe/", "abstract": "“It is easier to prevent bad habits than to break them.” – Benjamin Franklin The elegant software we love to build doesn’t arise magically. We hire talented developers, but people are only one part of our recipe for creating quality code. Equally important are the processes our team uses. We have a set of best practices to ensure that: development moves swiftly, nothing slips through the cracks, the work we do can be easily understood by future developers joining the project, and we are using the best tools for the job. Linting Linting is the term of art used for programs that identify style discrepancies and structural errors in code. Teams will define what styles they want their project to adhere to (for example, no more than 100 characters per line, 2 spaces per indentation, etc), and then the linter will either highlight or automatically fix those discrepancies. This consistency makes moving between files and codebases more seamless and improves the overall readability of the code. What’s more, you can even have rules that identify common inefficient, dangerous, and broken code snippets, helping you write more performant and bug-free code. Fancy! The linting packages we frequently use in our JavaScript repositories are: Eslint – The linting tool (highlights issues, instead of saving fixes) eslint-config-airbnb – Widely used configurations for the tool eslint-config-prettier – Widely used configurations for the tool Prettier – The tool for automatically fixing the code when issues are found An in our Android (Java and Kotlin) projects: Android Studio’s built-in linter – Java detekt – Kotlin klint – Kotlin Hot Reloading Hot reloading is a nifty feature available to JavaScript and other languages that use just-in-time compilation. It allows you to rebuild your application the moment changes are made to the code, and in advanced cases, can even be used to update only the piece of the app that was modified . This has had a huge impact on the velocity of our teams, allowing us to iterate more quickly and see up-to-the-second impacts that changes in our code create. Some tools we use that either support or provide hot reloading are: Nodemon – A tool for adding hot reloading functionality to Node processes Webpack – A tool for building and hot-reloading web apps Code Review One of the most important parts of our code creation process is code review. We never leave a developer on their own, writing and pushing code that is unreviewed. On every project, we always have at least 1 developer working directly in the codebase, with another developer to review the code. During this process, the reviewer may find bugs or more optimal solutions. However, even if no issues are found, something equally important happens: the transfer of codebase knowledge to a second developer. This ensures that no matter what, more than one person knows about the code being written, which in turn improves accountability, builds camaraderie amongst our team, and guarantees that a project never lives and dies by a lone developer. The review process typically occurs in Github’s reviewing interface , as well as on the reviewer’s computer as they pull the code and run it themselves. A common checklist we use when reviewing code is: Are there any typos? Are there new tests for the code? (more on that below) Do the continuous integration checks pass? (more on that below) Is the code easy to read and understand? Would a simple comment add clarity? Is the same code copied in multiple places? If so DRY it up. Would it be difficult to modify or extend the code if new requirements arise? Are all of the acceptance criteria met? (more on that below) Are any of the algorithms or data structures inefficient? Consider Big O notation. Are the SOLID principles followed? Check out our blog post about code review for more detail about our process. Testing Testing is the complement to code review, as it provides a mechanism for ensuring that the code behaves as it appears. Over time as more tests are written, it also serves as a form of regression testing, guaranteeing that new changes to code do not break old features. A common approach to writing software is called test-driven development, whereby you first write a test to check the behavior of a piece of code, and then write code to be tested. This guarantees that every incremental step of your code works, and that your code is thoroughly tested. What’s more, that flow encourages problem deconstruction and more focused, compartmentalized solutions as you write individually testable pieces of code. It really is a win-win. We do not have a one-size-fits-all approach to testing, as each project’s resource and timeline considerations can vary, but always prefer to have at least baseline tests to verify basic functionality of core features. A common checklist we use when writing tests is: Do I test that the various new pieces of code work individually (unit tests) and together (integration tests)? Do my tests verify that errors are properly handled? Do my tests individually handle setting up and tearing down any external requirements? Was it difficult to set up my tests? If it was, could it be because something in my code was poorly written, or relies too heavily on non-deterministic behavior? Some common testing tools we use are: Selenium or Cypress – Generic web applications RSpec , Minitest , Cucumber or Capybara – Ruby on Rails projects Karma with Jasmine or Mocha – AngularJS applications Jest or Enzyme – React and React Native Mockito , JUnit – Java applications XCTest and OCMock – iOS applications Robolectric or Espresso – Android applications Continuous Integration Pipelines A robust CI pipeline is the glue that binds code review and testing together. It handles listening for pull requests against repositories, and then running a set of scripts – such as the linting and testing scripts – against those pull requests to quickly and automatically verify that the changes they propose are acceptable. This tooling can require a little bit of up front configuration but is ultimately worth the investment as it not only guarantees code quality, but also speeds up the code review process. This is possible because the results of the CI analysis can be integrated directly into Github or other repositories, allowing reviewers to see whether the code they are reviewing passes the required tests without having to run those tests themselves. What’s more, CI pipelines can be seamlessly integrated with Continuous Deployment tools to automatically deploy the now-verified code once the pull requests are merged into specific branches. We frequently use the following CI tools: Jenkins CircleCI Documentation Documentation comes in many forms including research, meeting notes, tickets, and readmes. Depending on the project, we will work with our clients on each of those forms to varying degrees. No application gets created without thorough research and planning. Sometimes our clients come to us with a full set of technical specifications and all of the research already done, while others seek a collaborative process where we work with them to gather requirements, create user stories, and plan out the architecture for a project. We love digging into the details, and so are always happy to help with or take ownership over this process. When that happens, we will create a clear papertrail of the facts and conclusions, ensuring that all stakeholders are on the same page. We like to use Confluence for this process, as it provides a deep set of features for creating, linking, and organizing the mountain of information we produce. Regardless of whether we are conducting research, there are always various meetings over the course of a project. In the words of management author Patrick Lencioni: “The majority of meetings should be discussions that lead to decisions.” If nobody is writing those decisions down, they’re bound to be forgotten or misremembered, which can slow down the whole team, or worse yet, lead to incorrect courses of action. We build successful long term relationships by taking an active role in our partnerships, and that means doing whatever we can to make sure those sorts of mistakes don’t happen. So when clients prefer it, we will keep notes on the major takeaways from our meetings and send them out to the team afterwards. Tickets define the tasks to be completed, and are an essential part of keeping a development team moving swiftly through the countless discrete and interconnected steps of building an application. The most widely-used tool for creating and managing these tickets is Jira . Some of our clients prefer to create these tickets themselves so that we can just focus on writing great code, while others seek our input during every phase of the ticket research, writing, and organizing process. We’re happy either way, as long as quality work gets done. Last but certainly not least is the humble readme. Even the most clearly-written application requires a readme, so as we create and maintain projects we strive to produce readmes that describe the application’s purpose, explain how to get it running, and document relevant APIs and architectures. This in turn reduces onboarding time and improves developers’ ability to leverage the application, while providing a single source of truth about expected application behaviors. In the aggregate, this documentation improves the efficacy of our teams and gives our partners a sense of true ownership over the technology we create, as it ensures all stakeholders are aware of the process every step of the way and empowers any future developers to onboard onto the project as quickly as possible. Let’s Chat Our tools and processes help us consistently produce high quality code as quickly as possible while ensuring full buy-in from our clients. If you’re looking for a partner to help make your project a reality, or want to join a team of passionate devs that take pride in their work, then we’d love to chat. You can contact us through our site , or at hello@moove-it.com .", "date": "2019-10-08"},
{"website": "Moove-It", "title": "apple-wwdc-2019-recap", "author": [" Federico Ojeda "], "link": "https://blog.moove-it.com/apple-wwdc-2019-recap/", "abstract": "It’s been a week since WWDC 2019 – Apple’s Worldwide Developer Conference. That may sound like a long time, but it hasn’t been enough to fully grasp everything that was announced given that this was one of the densest and exciting WWDCs in years. A variety of new products were revealed, such as the new Mac Pro, but more importantly, for developers, a bevy of updates was announced for iOS and macOS. We always stay up to date with iOS and macOS developments because they create opportunities to bring exciting new features and improvements to our existing apps. After a week of investigation, evaluation, and testing, we want to show you what we are most excited about, and why. SwiftUI We are most excited about SwiftUI – a new UI framework, written completely in Swift, that brings reactive programming to iOS. The past few years of UI development have seen rapid change, and this framework distills the major lessons learned during that time to provide a much more clean, simple, and understandable way to write UI code. It has 3 important characteristics: Declarative: We define the code declaratively, as a function of our state. All code: No more storyboards and complicated file formats – the UI is just code. All Swift: There’s no under-the-hood magic, this framework uses and is written in Swift. Why is it such a big deal? Less code: A variety of constructs have been simplified in SwiftUI. For example, writing a List in the past meant writing a lot of boilerplate; a complicated mess of delegates and data sources taking at least 50 lines of code. With SwiftUI, we can create a List with just 5 lines of code. More predictable code: SwiftUI is built upon the paradigms of reactive programming and declarative UIs, which bring with them the notions of dataflow and a single source of truth for our data. This means our apps’ states can be modeled and understood more easily, which results in easier development and fewer bugs. A unified framework for several devices: SwiftUI can be used to create apps for Apple TV, iPhone, macOS, Apple Watch, etc. Having a unified UI framework makes switching between platforms an easy task. After trying it out, we can confidently say that SwiftUI is the future of iOS programming. Nevertheless, this beta release is still really buggy and incomplete, so it’s going to be a while until we can use it in production. Sign in with Apple Another significant announcement was the new Sign In With Apple service. It allows users to log in to third-party apps with their Apple account, just like Google or Facebook. Apple will work like any other authentication provider, using OAuth 2.0 to do this. Biggest features: UI: Provides a nice button that says “Sign in with Apple.” OS Integration: On iOS devices, it’s completely integrated with the OS so you can use Face ID/Touch ID to authenticate. Similarly, on Mac Safari you can authenticate with Touch ID. Security: Apple provides a lot of security features with its login implementation. Data selection : The user can select which information they want to share with the third-party app they are logging into. Hide your email : If you don’t want to share your real email, Apple will create a fake one for you, that will act as a proxy and re-send all the emails to your real one. If you feel like deleting one of these fake emails, you can easily delete it. One detail that should not be ignored is that Apple will make it mandatory to introduce this to apps that use third-party authentication services. There’s no timeline for this yet, but they stated that it will be required later this year. iPad apps on macOS The other big consumer-facing news to come out of the conference was project Catalyst. This is the codename for the project that allows iPad apps to be easily ported to macOS, and when I say easy, I mean “click a button” easy. Catalyst was introduced as a demo last year but was otherwise kept under wraps. Now it’s out in the world, and new apps for iPad (iOS 13 and up) can be easily configured to run on macOS. Why is this such a big deal? Unified codebase: In the past, developing an app for macOS and iOS meant having two different codebases. This almost always meant that teams maintained an iOS app and not a macOS app. Better user experience: Whenever a macOS app was needed, companies would rely on web technologies such as electron, providing inferior and inevitably inconsistent experiences compared to native applications. Project Catalyst is going to significantly impact how developers create and maintain apps, and how their customers use them. There are already a few apps using it, including Jira, and we expect many more to follow. New frameworks and improvements Of course, since it’s a developer conference, there were new updates on Swift (Apple’s favorite language) and iOS in general. Some of the highlights were: New Swift version: Swift 5.1 was announced, with new features that make developing apps in Swift a breeze. One of them is ABI stability, which means that, in the future, apps compiled in different versions of the language (5.1 and 6 for example) will be able to communicate. No more pain updating the dependencies! Combine – a new reactive framework: A new framework called Combine was introduced. Many new frameworks and libraries rely upon the reactive programming paradigm, and Apple’s new framework is no different. At Moove It we’ve been using reactive programming for a while, not only by using React and React Native, but also using RxSwift in our iOS apps. We’re happy that we chose wisely! For experienced RxSwift users, you’ll find Combine familiar. The transition for us is going to be a quick one 🙂 Summary This was one of the densest and exciting WWDCs in years. We have only just begun to unpack everything that was announced, and are already excited. The Apple ecosystem is evolving for the better, which will enable us to build more elegant, feature-rich applications. We can’t wait to see what next year brings!", "date": "2019-06-18"},
{"website": "Moove-It", "title": "react-native-workshop", "author": [" Federico Ojeda "], "link": "https://blog.moove-it.com/react-native-workshop/", "abstract": "Here at Moove It, we like to stay updated on the latest technologies. One of these is React Native, the Javascript framework that is taking over the mobile world by storm. Using it to develop an application for both iOS and Android is a breeze! As a React Native Development Company , and considering it may be difficult to level the performance of our staff, we like to deliver internal workshops addressed to all our developers in order to spread the knowledge! Motivation The main motivation behind the idea of preparing a workshop was the slight difference between React and React Native. Except for some specific parts, the coding is almost the same! Since a lot of people here at Moove It work with React (and pretty well, may I say!), it was a match made in heaven. Something that also motivated us was that we consider mobile applications as an important part of the future (and present) of software development. Enterprises and startups are using mobile applications as part of their business more and more. That’s why we believe that the ability to develop a simple mobile app is a skill that’s enriching for everybody. Ideation Process The team gathered on Fridays for a few weeks (during out Open Friday’s space) to brainstorm and create each lesson. We decided what we wanted people to learn, defined the topics that had to be taught, and some other conditions. We decided several things: The course would be for people who already knew React & Redux. Otherwise, it would be too much information for a single workshop. We would be creating an entire app throughout the course, lesson by lesson. Topics would focus on aspects that web developers do not take into account when going mobile (human interaction, offline databases, how React Native works under the hood, etc.). We decided how the lessons would develop and the topics to be addressed in each one. Lessons So the course started! A lot of people were interested so we had a full house. The workshop was held over 5 weeks, for 3 hours each Wednesday. The topics we covered throughout the workshop were the following: Introduction to React Native: We explained how React Native emerged and discussed the differences between this and other solutions. Then, we explored how React Native works under the hood. UI Basics: We showed how to create a React Native project from scratch. People learnt how to use the essential UI components and how to style them. We focused on the patterns that affect mobile applications in comparison to web applications. Finally, we ran our application on the iOS Simulator and Android emulator. Responding to user interactions: We started by adding Redux to our application. Then, we showed how to debug in React Native and the differences between  JS runtimes while debugging. Navigation: Navigation is all about showing the user where to go and how to get there. We presented and discussed the main patterns (and anti-patterns) that are common to both platforms. We also discussed the different solutions available and our pick for this course. Finally, we integrated the navigation solution to our project. Networking: It was time to show real data in our application. We integrated the libraries & structure needed to handle network requests. Finally, we discussed how to handle the Registration & Authentication flows. Persistence: How can we store data in the device itself? We discussed the importance of storing data in the mobile device, available solutions and the best fit for each use case. Device native features: We started by adding autologin and a settings tab to our app. Then, we learnt how to use the device native features, such as the camera & GPS. Publishing an app to the mobile stores: It was workshop wrap-up time! We showed the process of building production-ready binaries to upload to both the Apple App Store and Google Play Store. It was a pretty intense schedule, but in the end it was all worth it! Lessons Learned & Summary After the workshop ended, we felt very proud of how it turned out. People showed real interest and focused on the lessons, even working on the app at home, which made it really easier for us the teachers! This was the first time that the workshop was done here, so, of course, there were several lessons learnt and opportunities for improvement. For example, we learnt that we should allocate more time to setting up the environment (people had a tough time with that). Also, we realized that in the future we could create a more down-to-earth app to make the learning experience even better. On this instance, we just created a fake To-Do app. All in all, we are really happy with the workshop and are looking forward to doing it again. Several people expressed interest in continuing to work with React Native, which is amazing. We want to thank everyone who assisted and hope they learnt a lot! Further Information For additional details, you can check out the slides that we used during the workshop. These sum up the main lessons and include a collection of links with further information. You can access the folder here .", "date": "2019-03-05"},
{"website": "Moove-It", "title": "bringing-react-hooks-modals", "author": [" Gavin Haynes "], "link": "https://blog.moove-it.com/bringing-react-hooks-modals/", "abstract": "React Hooks Recently with React 1.16.7 alpha react announced it would be releasing “hooks” to allow you to use state outside of a class component ( hooks intro ). This has large implications for DRY coding and reusing stateful logic. Before when you had components that overlapped functionalities you could define getters/setters which could clunkily fit into each component. This post won’t be about hooks but will be a simple use case of extracting logic from a common library. Modals and popovers The first thought that came to me when I heard about this is reusing one of the most common react component’s functionalities – modals and popovers. Whenever I am introducing a custom modal/dropdown/popover I always have relied on one of the many libraries for handling clicking outside of my components scope (usually react-onclickoutside because the 0 dependencies). Recently I have decided to take it into my own hands and just implement it myself using document event listeners a reference to the modal content (source: article ). But I thought I could take this time to implement this using these brand new react hooks. Code I am going to do a comparison between two applications with the exact same purpose: Render a button to open a modal – and clicking anywhere not inside the modal should close the modal. Base modal app It’s important to note the size/complexity of not only the component we are focusing on but the controlling component above the modal where we are controlling whether to render the modal or not. I pass a function setIsOpen as a prop to the modal component, and conditionally render it using isModalOpen . From the library react-onclickoutside I defined the function handleClickOutside which calls the setIsOpen function. Using Hooks Since we don’t have to define a constructor , or handleClickOutside we can use functional react components. We create a hook using useState to control isOpen and create a handler setIsOpen . In the Modal component, we use useRef to create and access a ref for the modal content and determine if the click event was contained inside the modal. useEffect is defined as the custom useClickOutside hook. And the custom useClickOutside hook: Just takes in a ref and the callback that is utilized when the click isn’t contained inside the ref ( onModalClose ). For our case setting the modal to close, but it could be anything since it is just a callback function that is passed in as an argument to the hook. Takeaway To me, this seems like a positive improvement for separating common logic. Only time will tell how widely adopted this approach of using hooks to reuse stateful component logic will become. In my opinion, it sure is nice reusing the same hook for any component that needs to detect clicks outside of its scope.", "date": "2019-01-31"},
{"website": "Moove-It", "title": "gdpr-the-new-normal-of-software-development", "author": [" Hayley Parks "], "link": "https://blog.moove-it.com/gdpr-the-new-normal-of-software-development/", "abstract": "The European Union’s General Data Protection Regulation (GDPR) goes into effect this week. While the regulation affects all organizations accessing personal data of European Union (EU) citizens, its implication for software development companies is far reaching. This is because personal data – the focus of the regulation – is at the very core of software development. This means that those of us in the industry need to understand GDPR and position ourselves to operate in compliance with the regulation. Failure to comply has significant implications, as we will find out below. But first, let’s get started with the basics . What is GDPR and What has Changed The GDPR is a regulation that replaces the 1995 Data Protection Directive (DPD) of the EU. It sets minimum lawful standards, which organizations who benefit from personal data (“controllers,” under GDPR), and companies processing on their behalf (GDPR “processors”), must adhere to when processing personally identifiable data of EU Member State citizens (GDPR “data subjects”). The goals are to increase protection of, control over, and transparency of personal data.  To fully understand how this regulation is different from past practices, consider these key differences between GDPR and the DPD: Opt-out vs. Opt-in Consent: Under the previous directive, organizations could consider a person’s failure to opt-out as an indication of consent, such as failure to untick a pre-ticked box. U nder GDPR, organizations must gain explicit consent from the data subject. Explicit consent requires an clear, simple contract regarding their information and its intended use.. In order to be considered a valid consent, the controller must be able to show consent has been given by the data subject. If the consent is written, the verbiage in the consent contract must be written in clear, understandable language. Consent may be obtained for only for the personal data required for performance of the contract. Additionally, subjects have the right to easily withdraw consent at any time. The Definition of Personal Data: While the DPD applied only to information that can be used to identify a person and their sensitive personal details, GDPR covers any data or data sets which might be traced back to an individual, such as an IP address. Reporting a Data Breach: With the DPD,  organizations were merely encouraged to report when there has been a breach of their data. Under GDPR, organizations are under obligation to report such breaches to the proper authorities within 72 hours of the incident. Data Protection and Accountability: GDPR requires explicit accountability for data protection from organizations. Any company employing more than 250 people or processing more than 5,000 profiles annually must appoint a dedicated data protection officer. Companies are also expected to commit to mandatory activities, such as staff training, internal data audits, and maintaining compliance documentation. None of these requirements were stipulated under the old rules. Penalties and Compensation: Non-compliance with GDPR could impose costly penalties on companies found in violation of the regulation. Under DPD, violations could lead to a maximum fine of 500,000 euros or one percent of a company’s annual revenue. The maximum fines under GDPR are considerably larger: 20 million euros or four percent of annual revenue, whichever is higher. Furthermore, GDPR allows data subjects to seek compensation for both material and non-material damage, whereas DPD was material only. Definition of Processing: Under GDPR data “processing” has a much broader meaning, defined as: “A ny operation performed on personal data, whether or not by automated means, such as collection, recording, organizing, structuring, storage, alteration, retrieval, consultation, use, disclosure by transmission, dissemination or otherwise making available, alignment or combination, restriction, erasure or destruction. ” Scope: The 1995 EU Data Protection Directive was a guideline for EU Member States to pass and implement national legislation to effect change regarding data protection. The GDPR is much more than a directive. It is a legally forceable and binding piece of legislation meant to unify data regulation across the EU. It is more far-reaching, including Member States and any organization handles data of EU citizens, which means its scope extends to any software application involving EU citizen data. GDPR Subject Rights Means Responsibilities for Developers Protecting the privacy rights, ensuring transparency, and providing control over personal data of EU citizens is central to GDPR.  The requirements of GDPR places obligations on organizations to take steps to fulfill the new rights of data subjects, and thus it is imperative that developers keep these rights in mind when creating new GDPR-compliant software applications: Information and Transparency: When requested by data subjects, controllers must provide any information relating to their data processing to the subject in a concise and transparent form using clean and plain language. Article 13 and Article 14 establish what information shall be provided. Rectification : Provides data subjects the right fix inaccurate or incomplete information and without undue delay. Restriction of Processing: In lieu of erasure, data subjects can prevent a controller from processing their information if the data’s accuracy is contested, processing is unlawful but subject opposes erasure, or the controller must maintain the personal data for legal reasons. Erasure (or ‘Right to be Forgotten’): Data subjects can request deletion of personal data, of which the organization must oblige without undue delay (within the law). Notification Obligation : The entity collecting data must notify third parties of any erasures, rectifications, or restrictions of processing. Data Portability : Data subjects have the right to transfer their personal data between controllers, i.e. to move account details from one beneficiary of the data to another. Objection : While controllers must have a lawful basis for processing personal data, i.e.  public interest or legitimate interests, lawful bases are not absolute and data subjects have a right to object to such processing. Profiling : Data subjects may elect to not be subject to automated decision-making based on their personal data, which may produce legal effects. Controllers must implement safeguards to protect the data subjects’ rights, freedoms, and legitimate interests. Governing Principles for Software Development Under GDPR: Another way in which GDPR affects developers is Article 25: Data Protection by Design and by Default. What does this mean? It means that organizations must build data protection safeguards into their products, processes, and services from the earliest stage of development. In other words,  privacy and security considerations should not be an afterthought, but rather inherently built into each product, from the first stages of design and throughout the entire development process.. There are six key principles defined by the regulation that govern how organizations can collect, store, and process personal data. To be in compliance, ‘by design and by default” software developers should keep these principles in mind: Lawfulness, Fairness, and Transparency : GDPR requires processing of personal data to be lawful and fair. Communication relating to the processing of personal data should be transparent, accessible and easy to understand by the person whose data is being processed and include the identity of the controller and the purposes for which the data is being processed. Such communication should also disclose risks, rules, safeguards, and rights regarding the processing of personal data and how to exercise those rights.  (See Article 5 and Article 6 for compliance requirements.) Purpose Limitation : Personal data may only be collected for specified, explicit, and legitimate purposes, as determined at the time of collection. Data Minimization : Personal data should be limited to only what is relevant and needed for the purpose of which it is being processed. Accuracy : Personal data must be kept up-to-date and inaccurate data must be immediately deleted or rectified with accurate information. Every reasonable step must be taken to ensure information accuracy. Storage Limitation : Storage of personal data must be kept to a strict minimum and once the information is no longer necessary for processing, it should be erased. The entity collecting data should establish time frames for deletion or for a periodic review to ensure limitation. Integrity and Confidentiality : GDPR requires that personal data be processed only if the purpose of processing it could not reasonably be fulfilled by other means and in a manner that ensures security and confidentiality. In order to achieve a smooth transition in creating GDPR-friendly software that prioritize data protection, developers will need to devise strategies, policies, and procedures regarding data protection, subject control, and transparency. And now that we’ve covered the basics, we have a foundation of understanding and how we might approach software development that complies with GDPR. If you have any questions regarding this topic, leave us a comment.", "date": "2018-05-21"},
{"website": "Moove-It", "title": "a-gentle-guide-to-blockchain", "author": [" Juan Andrés Zeni "], "link": "https://blog.moove-it.com/a-gentle-guide-to-blockchain/", "abstract": "Thanks to Bitcoin and its recent surge in popularity, the terms “blockchain” and “blockchain technology” have entered into our common consciousness and become a big part of everyday vocabulary. Hardly a day goes by without some major publication talking about blockchain technology and how it’s going to change the world . For the most part, the media coverage targets a particular demographic: tech savvy, investment-types who have known about cryptocurrencies and blockchain long before the rest of us ever heard those terms. But what about the vast majority of people who aren’t quite sure what blockchain is and why it is such a big deal? Read on for an easy-to-understand introduction and some guidance to help businesses decide when it’s appropriate to adopt a blockchain solution. So what exactly is blockchain? Because blockchain first became popular as the technology behind Bitcoin, many have come to regard the two as interchangeable. However, there are clear distinctions between the two: Bitcoin is a cryptocurrency, which was specifically designed to facilitate payments without third-party intermediaries, such as banks or governments getting involved. Blockchain on the other hand, is the technology that underpins Bitcoin. Let’s illustrate this with an everyday analogy: Blockchain is the operating system while Bitcoin would be an app that runs on it. In much the same way that several apps can run on one operating system, several applications can run on blockchain, and cryptocurrencies are just one example. Having made that distinction, let’s now focus on how blockchain works. How does blockchain work? When transactions are conducted in Bitcoin (and other cryptocurrencies), those transactions are recorded in “blocks.” Each block represents the most recent transaction, like a new page in a ledger. That ledger is the blockchain (so called because, well, it’s a chain of blocks!). As new transactions are conducted, new “pages” (blocks) are added to the ledger. Unlike a traditional ledger, which is kept in a central repository, blockchain is decentralized . This means that all participants (nodes) in the blockchain’s network have access to the ledger and can conduct and validate transactions on it. This decentralized nature of blockchain is its main advantage because, while centralized data is easier to control, it is also easier to manipulate. By decentralizing data, blockchain makes that data transparent to everyone involved, and eliminates the likelihood of fraud. With this simplistic description of blockchain in mind, let’s now try to understand what makes this technology so special. 1. The immutable nature of blockchain data Blockchains are continually growing as new blocks are recorded and added in chronological order. E ach block is linked, assembled and secured with the use of a peer-to-peer network where members adhere to a protocol for making and validating new blocks. Blocks hold batches of valid transactions (or facts) that are hashed and encoded. Those facts can be anything from monetary transactions to content signatures. Each new block includes the hash of the previous block in the blockchain, linking the two and forming an immutable chain. This iterative process confirms the integrity of the previous block, all the way back to the original genesis block. This means that the data in any given block cannot be altered retroactively without altering subsequent blocks, which requires the collusion of the rest of the network. This is why data stored on blockchain is generally considered incorruptible, making it suitable for use where data integrity is important. For example:  medical records, identity management, transaction processing, voting, food traceability, establishing the provenance of documents and recording of events. 2. The consensus model of verification When a new transaction or an edit is made to an existing transaction in a blockchain, the different nodes within the network execute algorithms to evaluate and verify that transaction. When all nodes reach a consensus about the validity of the incoming transaction, the new block is accepted and merged into the shared ledger. If there’s no consensus, the transaction is not merged, but a record of the intent is registered for record purposes. This consensus model is what allows blockchain to run as a distributed ledger, without the need for a central authority to determine which transactions are valid and which are not. 3. The transparent nature of blockchain processes One of the key attributes of blockchain is its ability to facilitate transactions between entities with a mutual distrust of each other. Such entities can exchange value and transact without relying on a trusted third party because blockchain’s two-tier transaction verification process, validates the integrity of their transaction and promotes transparency. One set of participants, verifiers , confirm transactions while other participants, observers , detect changes in the ledger and confirm that all transactions were properly executed. Eventually, all participants have the same view on their “copy” of the ledger. 4. Fueling efficiency Redundancy is one of the major disadvantages of a centralized system as information has to be replicated on different physical servers, which is both time and cost inefficient. With blockchain, the ledger is replicated across all the writers automatically. Types of blockchains Blockchains can exist as permissionless or permissioned blockchains. As the name implies, permissionless (or public) blockchain networks do not require access control. This means applications can be added to the network without the approval or trust of others. A permissioned blockchain on the other hand, uses an access control layer to govern who has access to the network, and this makes them more suitable for business networks. Blockchain for business With all of these advantages, blockchain technology lends itself to multiple uses for businesses. There are a variety of reasons why businesses should adopt blockchain technology. One reason is the opportunity to transform the way we do business by providing a standard architecture that addresses various business and organizational problems. For instance: Blockchain can drive business efficiency . Because transactions are executed directly between the relevant parties with no intermediary, settling transactions can be quick, reducing  time and cost of each transaction. Distrust is eliminated , as all the parties involved in a transaction only have to trust the technology. Auditability is improved . Since each transaction is recorded sequentially and indefinitely, an indelible audit trail is created for the life of an asset between parties. This is especially important if source data is essential in verifying an asset’s authenticity. Similarly, accountability is guaranteed . As all parties to a transaction can view the distributed ledger, everyone can agree on how the transaction is progressing while it is ongoing and how it went once it is completed. Traceability is enhanced throughout the supply chain, especially for those companies that subscribe to standards that focus on the traceability of components. With blockchain, i nformation relating to each component can then be relayed to or from the new owner for possible action . Companies using blockchain Successful, forward-thinking companies are turning to blockchain technology to improve efficiency and decrease the risk of fraud or unnecessary tampering with information. Such companies include: Walmart teamed up with IBM in 2016 to improve the process of tracking the meat and poultry inventory. The blockchain technology it utilizes tracks all information (through QR code scanning) at every step of the process from the farmer to Walmart’s shelves. Maersk and IBM have devised an end-to-end shipping solution that provides all global trade organizations a single view of cargo location and allows authorities to give electronic approval for its movement. British Airways hopes to reduce conflicting flight information coming from gate monitors, flight apps, and the airline’s website by using Blockchain to build a single unchangeable history source. UPS joined the Blockchain in Trucking Alliance (BiTA) aiming for increased transparency among all groups involved in the supply chain. The group is working to develop blockchain standards for the freight industry. When to use blockchain for a business Now that we understand blockchain, how do we know when it’s good for business? Using Blockchain only makes sense when multiple, mutually mistrusting entities want to interact and change the state of a system, and are not willing to agree on an online trusted third party. A blockchain network with only one participant would be a waste of resources and add an unnecessary layer of complexity. If the network is composed of multiple participants, then the next question would be whether there is a trusted, always-available third-party responsible for verifying all of the state transitions of the ledger and notifying of changes. If all the network participants are known and trusted, then there is no need to use blockchain and a conventional business network would suffice. Otherwise, using blockchain would be a good option, then necessitating a decision as to which type of blockchain is better in that scenario. The following flow chart is designed to make the decision-making process easier: Conclusion The uses and benefits of blockchain are much wider than virtual currencies. It can be applied in countless areas, including in business, regardless of the size of the entity. While early adoption of new technologies can be a gamble, blockchain has shown itself to be so versatile and advantageous that many businesses have become eager to explore how they can use the technology to their advantage. The pioneering businesses that start engaging with blockchain technology now will have the opportunity to shape how the rest of their industry uses blockchain, and thus gaining a competitive advantage over late adopters. We hope this helps you understand blockchain and its use cases. If you have any questions, leave us a comment and we can discuss!", "date": "2018-05-17"},
{"website": "Moove-It", "title": "template-creating-scalable-swift-apps", "author": [" Mauricio Cousillas "], "link": "https://blog.moove-it.com/template-creating-scalable-swift-apps/", "abstract": "At Moove It we work hard to create beautiful, scalable and maintainable iOS apps. To achieve this, we have tried different architectures over time, such as the classic Model View Controller (MVC); View, Interactor, Presenter, Entity and Router (VIPER); Model View Presenter (MVP); and Model–View–ViewModel (MVVM). We have also tried several third party-frameworks, which all come with their advantages and disadvantages. We wanted to define a baseline for any new project, using all our previous experience to streamline and reduce the number of decisions that a developer has to make before beginning to develop a new project. With that in mind, we defined a project template that works as a starting point for iOS applications; providing a base architecture, core frameworks, and helpers to jumpstart development. In this blog post, we introduce the fundamental concepts and tools included in the project. If you just want to check out the code, you can go directly to our GitHub repo and access the template. Download template Base tooling The following list includes all of the basic frameworks that come bundled with the project and their purposes: R.Swift is used for xcode asset management SwiftLint is used for style checking RxSwift is used for data flow management Alamofire + Moya/Rx are used for networking RealmSwift + RxRealm are used for database management Whisper is used for in app notification-style messages Architecture We are going to use MVVM + routers (R), but with some caveats: Models The models won’t store business logic. They will only act as data stores (Realm objects when using a local database, and Structs in other cases). Views The responsibility of Views (or ViewControllers in this case) is to display the data provided by its ViewModel and forward all events to their respective ViewModel . ViewModel The ViewModel is the component in charge of managing the state of each view and any processing necessary for the displayed or submitted data. Moreover, the ViewModel communicates with Controllers to fetch the data necessary for its view, and uses the Router to forward navigation actions. Controllers, Services and DataManagers The Controllers are in charge of managing the Models . This means that ideally you should have one Controller per Model (unless you need something like a session controller that can manage more than one model such as a user model) and another model for storing session information. The Controllers use two types of support classes; Services and DataManagers for networking access and database access, respectively. If you don’t have one of them (for example apps that only consume API data and save nothing locally), you can replace your Controller with a Service or a DataManager and use that directly in your ViewModel . Router The router is the component in charge of handling the navigation stack for your entire application. For this, the router keeps track of your rootViewController and your currentViewController (the one currently visible). To keep everything tidy and isolated, the router does not know how to instantiate the screens that it presents. This is defined separately using the Route protocol. A Route is a component that encapsulates all of the necessary logic to instantiate a view , with its corresponding viewModel and any other parameters forwarded from other routes. Apart from the Route , you can set a TransitionType when you navigate to a route. This tells the navigator how the screen should be presented (modally, pushed, resetting the stack, etc.). So, to call a navigation action, all you need to do is call your Application’s Router and call .navigate with the corresponding Route and TransitionType. It’s as simple as that! Usage Option #1: Copy the core files to your project If you already have a running project and you want to use this project’s core modules, you can simply copy the source files into your project. Those are available in the /Core folder. This can come in handy with projects where you want to start by refactoring some parts of the code instead of starting from scratch. Option #2: Start from scratch Clone this repository to your machine. Change the git project remote URL to one of your own with git remote set-url origin https://github.com/USERNAME/REPOSITORY.git . Make all of the following scheme, target and ID changes, to update the necessary information about your new app. Change all schemes to be named after your new project. Change the bundle ID to one of your own. Set up certificates (or let xcode do it automatically). Update the TargetType+BaseProject.swift to be named after your project. Check the Constants.swift file and update the baseUrl properties to point to your backend services. Go to your Podfile and change the target names to the new ones that you have set up. Run pod repo update and pod install . Build both targets to confirm that everything is running properly. Configuring Environments We’ve all faced the same problem when defining production and development constants, such as the API URL, setting up crash logging only in production, etc. Here at Moove It, we decided to use two separate build targets, Production and Development, to differentiate between these two environments. Using different build targets allows us to use Build Flags to see in which environment we are running. If you don’t know how to set up new Build Flags on your project, a detailed step-by-step guide may be found in this article . Additionally, running two separate targets of your app has other advantages, such as the ability to release both separately and having better management of early access for each environment. If you want to dive deeper into the topic, you can read this post which shows how to set up your targets and what can you do with them. Conclusions We have defined a starting point for any iOS project that we could imagine. Having a standard for any project is always an advantage, both for developers starting work on a project from scratch, as well as for newcomers who diving into a pre-existing project. It’s always easier to step into a project knowing where everything should be, which frameworks are being used, and other basic development information. One question that we asked ourselves while building this was: Can we use this for any type of software project. The answer is yes! One of our main goals was to keep this project as small and flexible as possible so that it could be used in as many cases as possible – and we succeeded. We hope it helps you succeed too!", "date": "2018-04-13"},
{"website": "Moove-It", "title": "our-new-brand-identity", "author": [" Ariel Luduena Karhs "], "link": "https://blog.moove-it.com/our-new-brand-identity/", "abstract": "Big news! We’re rebranding, and with that comes a new website, logo, colors and font that reflect the success and transformation our company has experienced in the last 12 years. Back when we started out in 2006, old-fashion software dominated the software industry, waterfall development methodology was all the rage and Nokia 1100 devices were a thing. We’ve come a long way since then. Like the software industry, which is dynamic and ever‑changing, our brand has evolved over the years, with slight modifications to our logo, website and messaging. And while we think the current identity has carried us far, it hasn’t quite evolved in the right direction and does not represent a good foundation for the new era of our company. Through all the past and current changes, some things have remained the same: Our commitment has always been to offer quality service, be recognized for it, and do it with the help of great people. It’s wonderful to see that those three ideas, along with the same values, are still driving our growth a decade later. Thanks to our clients Our clients played a vital role in the rebranding process. They helped identify our key differentiators and gave us important feedback. Not only that, several clients provided amazing written testimonies, and others invested almost a full day to be interviewed and filmed by our team. Our gratitude to them knows no bounds. We feel so proud and lucky. Thanks to our team This could very well have been a painful experience, and we thought it would be; after all there was a lot of emotional attachment to the current identity. But our whole team made it all so easy. They encouraged the change and give honest and valuable feedback. Our awesome company culture made this potentially difficult process something really enjoyable, and we all embraced the change and did it together. Our new tagline: Developing impact. Our new tagline perfectly summarizes our purpose: empowering organizations to make positive global impact through high-quality digital experiences. Moove It uses software development and design techniques as tools to make positive impacts on society.  Our focus is on improving lives through digital experiences, and this concept drove our full rebranding process. It served as a guide for how we talk about our approach, experiences and success stories. The Moove It logo It took us seven iterations to come up with the definitive logo. It wasn’t an easy process, but in the end, it was worth it. The plan was to build a totally different logotype and mark, while maintaining a connection with the past. Something that shows evolution. We also wanted the new logo to reflect the fact that our company is both strong and approachable at the same time. The “M” and the “e” give the approachable touch. The “e” is the only lowercase letter and acts as a binding between the new logotype and the current one. Logo evolution Colors As you may have noticed, dark blue is our new orange. The primary palette captures the majority of our color needs. White and dark blue are used as the brand canvas, to capture the majority of all visuals. The coral is used sparingly to add interest to specific areas and bring impact. The secondary palette defines three tone-on-tone values for maximum flexibility without having to double the color selection. Primary palette Secondary palette Iconography We now have a system of distinctive icons to  represent different functions. Each icon is made up of three simple components. Three represents the number of forms in the mark, creating a subtle visual relationship within the system. Made up of linear elements from a square and circular elements, we have an iconography system with endless possibilities. The Moove It icon style marries together innovativeness and energy, while maintaining a polished and professional look. These new icons fit seamlessly within the brand: The future Moove It will continue to pour its skills and expertise into creating a sustainable future for all. Now more than ever, as we help companies develop high-quality technological products and services that change lives in meaningful ways, our commitment to making positive impacts will continue to inspire our journey. Here’s to the future! Ariel Luduena, CEO @ Moove It", "date": "2018-07-31"},
{"website": "Moove-It", "title": "bringing-women-world-technology", "author": [" Lucía Carozzi "], "link": "https://blog.moove-it.com/bringing-women-world-technology/", "abstract": "Last year, I attended an interesting event that was designed to encourage women to go into programming. It was held in my hometown of Montevideo, Uruguay and organized by Django Girls , a non-profit organization that holds free Python and Django workshops for women. Run by volunteers, Django Girls is making technology more accessible by providing tools, resources and support that help women organize free, one-day programming workshops. Events are held in all corners of the world, from Tampere, Finland to Akombo, Ghana and Brisbane, Australia. Each event sees 30-60 women gather together to build their first web application using HTML, CSS, Python and Django, by following the steps from a straightforward tutorial. Since its start in 2014, Django Girls has attracted nearly 1,500 Volunteers and held 645 events for over 15,000 participants in more than 400 cities around the world. First steps in programming I got involved with Django Girls when an acquaintance asked me if I would consider participating in a Django Girls event as a mentor. Even though I was not familiar with Django or Python, the idea of helping other women take their first steps in coding appealed to me, and I accepted the invitation. Viky, my friend and mentor at Moove It also joined. I saw the invitation as an opportunity to help other people start out on a journey that has become such a meaningful part of my life. When I started coding back in 2010, I didn’t know what coding was. But now – having graduated as a computer engineer in 2016 and worked with Moove It for almost three years – I don’t see myself in any other profession. Viky and I participated in a kickoff meeting with the other mentors the day before the launch. There were seven mentors altogether – four women and three men. We learnt that the workshop participants were between the ages of 16 and 50, and that there would be four to five mentees per mentor. Group members from different backgrounds The event itself took place on December 16, 2017, and was well-attended. Over the course of  10 hours we, the mentors, worked with our designated groups of women. My group consisted of five girls between the ages of 16 and 30, who all came from different backgrounds. Some were in high school, others in art school, and while some already had some coding experience, others were complete novices and didn’t know what a console was. Not only did we all have to follow the workshop plan, but the mentors also had to follow a “Code of Conduct”, which I sometimes found difficult. For example, we were not allowed to touch the mentees’ keyboards, and I had to restrain myself from the inclination to keep saying “Hey, let me show you how to do it”. We were also encouraged to use female pronouns like “nosotras” and “todas” – as opposed to masculine pronoun like “nosotros” and “todos” which we would normally use for a mixed group of people. (In Spanish, all pronouns and adjectives have feminine and masculine variations). Being used to working mostly with men, I found it almost impossible to use female pronouns even though my group was all female. Successful workshop Overall, the workshop was a great experience, made even better by the fact that we had a successful outcome. Three of my mentees were able to finish the entire workshop, and I was very proud of their level of motivation to follow through and understand the steps of the task. One of the girls even told me she was seriously considering studying technology! To any ladies out there thinking of joining Django Girls as a mentor or mentee, I warmly recommend it. The experience will be worth it! Read more about Open positions", "date": "2018-08-08"},
{"website": "Moove-It", "title": "what-is-apple-mfi-how-use-it", "author": [" Federico Ojeda "], "link": "https://blog.moove-it.com/what-is-apple-mfi-how-use-it/", "abstract": "As you may already know, handheld devices these days are able to do a lot more than just show basic applications. Mobile devices can do a wide range of things, from accessing your bank account to managing your house and controlling external hardware and peripherals. In this blog post, we’ll focus on the latter. Since the beginning, iOS has been particularly open to allowing apps to communicate with external hardware by providing several Software Development Kits (SDKs) to do the task. But first, let’s clarify what exactly is MFi. Allows manufacturers to create peripherals MFi program (Made For iPhone/iPod/iPad) is a licensing program created by Apple. It’s main purpose is to allow manufacturers to create hardware and software peripherals that can work with Apple’s devices. This program involves any device that can be connected through the headphone jack, the original 40 pin dock connector, or the lightning connector. It also involves devices that can be accessed through wireless connections, such as bluetooth. How to interact with an MFi device The question probably every developer has about MFi is “How do I code something to interact with hardware?” As always, Apple provides excellent SDKs to perform everything related to communication. This SDK is called ExternalAccessory , and it already comes with the platform, which means  no extra steps are needed to install it (just remember to add the key to the info.plist ). This time we won’t go into details about the implementation of this SDK, but believe us, it is pretty straightforward. There’s only one thing you need to know about the communication: The single thing you can send and receive through ExternalAccessory are bytes through a stream. What you do with the bytes you receive is up to you! The SDK is only responsible for sending and receiving the data. Our curated list of available documentations Even though Apple provides, as always, an official documentation, there’s not a lot of information around the internet on how to develop an application for the MFi program. That’s why we created this curated list of documents, examples and tutorials to follow: MFi general information: MFi program Documentation: Official docs External topics Examples: List of connected devices Detecting connected hardware Example of a pod that connects to Verifone barcode scanner Tutorials: O’Reilly: Building iPhone accessories Google Books: Frameworks Building an iOS hardware app Integrating hardware devices with iOS apps is not an easy task. There are a lot of challenges that arise when working with hardware, such as protocols and connections. If you have any questions, don’t hesitate to ask us!", "date": "2018-10-04"},
{"website": "Moove-It", "title": "custom-software-development-austin-silicon-hills", "author": [" Hayley Parks "], "link": "https://blog.moove-it.com/custom-software-development-austin-silicon-hills/", "abstract": "Beginning with chips and semiconductors in the 1970s and Apple computers in the 1980s, the 1990s gave birth to the Internet and a wave of tech startups – like Amazon and eBay – cementing Silicon Valley as the world center for technology research and development. Located in the San Francisco Bay area of California, Silicon Valley – named after the Santa Clara Valley – emerged due to a concentration of available venture capital and its entrepreneurial ecosystem, which welcomed disruptive innovation. However, being the center of tech startups and global technology companies comes at a cost – namely the increased cost of real estate. With real estate prices in the Silicon Valley at a record high, many tech companies are turning to Austin. Nicknamed the ‘Silicon Hills’ – for the Texas Hill Country, west of Austin – a combination of factors makes it attractive to tech companies. These factors include affordable living, low unemployment, high tech salaries, venture capital funding, and the presence of existing major tech companies, such as Dell, Intel, and Samsung. In addition, the Kauffman Foundation recently named Austin – in its entrepreneurial spirit – the “No. 1 city in the U.S. for startups two years in a row.” The culture of Austin is ripe for technology startups ready to disrupt with innovation. It is renowned for its support of small businesses and unique conventions, staying true to the town’s slogan to “Keep Austin Weird.” CNBC also named Austin the Best Place to Start a Business in 2016, due to “the cost of doing business, quality of life, labor force and diversity.” Austin also ranks as one of the top areas for venture capital investors with over a dozen startup incubators around the metropolitan area, including Moove-it’s first Austin home,  Capital Factory. Since our expansion in January 2016, we have enjoyed a mix of corporate and startup clients, working among the best innovators in the United States. We are thrilled to be celebrating our two year anniversary of developing software in the heart of the Silicon Hills! Stateside and near-shore, our team is ready to help innovators bring their concepts to reality through expert software development.", "date": "2018-01-09"},
{"website": "Moove-It", "title": "what-are-cloud-native-applications", "author": [" Rodolfo Pilas "], "link": "https://blog.moove-it.com/what-are-cloud-native-applications/", "abstract": "Cloud computing has a substantial impact on how companies acquire, manage and maintain infrastructure for their applications. Cloud computing opened a new path for applications ranging from new programming languages, automation and deployment processes to new forms of architecture. We are living in a time of immense changes in technology: today’s standards were simple ideas just three years ago. Why develop cloud native applications? Cloud computing is elastic, resilient, managed on demand and maintains a number of sensors that collect and store information about its operations. Not all applications managed in cloud computing make use of these features. Some are ready to scale, but they are not resilient, while others do not adapt their operation to the data that is being surveyed by the cloud, they simply ignore it. Cloud native applications will change how applications are linked to cloud computing. These are applications that incorporate the characteristics of cloud computing in themselves, achieving optimal use of technology and resulting in a solution that optimizes investment costs, with greater reliability and security. The four ages of server infrastructure In modern application development, we have gone from monolithic to microservices applications, from on-premises servers to cloud computing infrastructure. Let’s discuss what those entail: 1. Static age: the classic server A monolithic application is installed on the physical hardware on-premises server, which causes laborious deployments, and updates usually result in downtime of service. Also, the backups are made on tapes and recovery processes are long. In the case of an increase in the service needs of an application, the hardware is changed for one of higher performance. Consequently, the solutions (infrastructure and applications) aim to be stable, fixed, and solid. Any changes would be rejected because a change requires effort and planning and imposes a risk for the application, its data, and the business continuity. 2. Virtualization age: the consolidation Virtualization resulted in massive consolidation processes into extremely oversized physical servers. They run software that emulates and virtualizes hardware to provide virtual machines to which install operating systems, services and applications. Here we observe the first paradigm shift: the servers and their applications become “files” that can be copied, backed up and moved to another physical hardware. Therefore it becomes easy to allocate more virtual hardware to meet increases in demand. In general, the administration of virtual servers, deployments, and upgrade of applications remains unchanged, in the same way as in the classic server age. Here, however, it is possible to view the use of configuration management tools and automatic application deployment tools. 3. The cloud age: a new infrastructure for the business Today all the resources for a solution are available “as a service”. The applications are on separate layers running on separate virtual machines (instances), which can grow or decrease horizontally in quantity, according to the service needs of that particular layer. In addition to infrastructure, platforms are also available as a service, prepared to receive the applications and put them to work. At this age, the 12 factors are established to assist applications to be ready for cloud computing. Consequently, the automation chains of deployment processes and solution maintenance are developed, such as continuous integration, infrastructure as code, continuous deployment, delivery pipeline, and orchestration. As a result, DevOps engineering is introduced to deal with all these elements. Despite these changes, the solutions (infrastructure and applications) are closely linked to the provider of cloud computing. Once the applications are deployed in both IaaS or PaaS , changing providers becomes difficult and, sometimes, unfeasible. 4. The Cloud Native age: applications with cloud properties Cloud native is a new relationship between applications and cloud computing. Elasticity, resilience, operability, and observability are the four key characteristics that become intrinsic parts of the application: Elasticity is meant to satisfy demand and is used to execute rapid deployments and fast iterations, while engineering simplifies complexity in smaller units: microservices. The application is broken into small parts that can be maintained by different teams and scaled independently. Resilience welcomes failures instead of trying to prevent them; it takes advantage of the dynamic nature of running on a cloud computing platform to adapt and absorb problems. Operability adds control of application lifecycles from within, instead of relying on external processes and monitors. Observability exposes facts for the business to make informed decisions. Native cloud applications take responsibility for managing and automating their health. As a result, developers can expose the state of health (following business rules) and create alerts when considered necessary. Furthermore, they also provide information on the status of the application at all times, keeping historical records. The application continuously reports telemetry data allowing decision-making: how many requests are answered per minute? What are the slowest operations? What are the errors and the average response time, etc? Finally, this data is cross-referenced with data from cloud computing to have a global idea of performance. Definition The definition of “cloud native” is a work in progress, but the Cloud Native Computing F oundation (CNCF) says : Cloud native technologies empower organizations to build and run scalable applications in modern, dynamic environments such as public, private, and hybrid clouds. Containers, service meshes, microservices, immutable infrastructure, and declarative APIs exemplify this approach. These techniques enable loosely coupled systems that are resilient, manageable, and observable. Combined with robust automation, they allow engineers to make high-impact changes frequently and predictably with minimal toil. Conclusion At Moove It we aim to understand the needs of our clients when defining the best solutions that effectively help their business, resulting in the most optimal investment allowing for the natural growth of the company. Therefore, in addition to offering quality software and applications , we aim to promote the feedback these applications bring to the business; we are interested in continuous changes in technology to avoid locking suppliers while taking into account the stability and reliability of the solutions. References Computer Foundation, Cloud Native. “CNCF Cloud Native Definition v1.0.” GitHub , 6 June 2018, github.com/cncf/toc/blob/master/DEFINITION.md . Murphy, Niall Richard, et al. Site Reliability Engineering: How Google Runs Production Systems . O’Reilly, 2016. Garrison, Justin, and Kris Nova. Cloud Native Infrastructure: Patterns for Scalable Infrastructure and Applications in a Dynamic Environment . O’Reilly Media, 2018. Hoffman, Kevin, and Daniel P. Nemeth. Cloud Native Go: Building Web Applications and Microservices for the Cloud with Go and React . Addison-Wesley, 2017.", "date": "2018-10-11"},
{"website": "Moove-It", "title": "end-end-testing-puppeteer-jest", "author": [" Martin Rifon "], "link": "https://blog.moove-it.com/end-end-testing-puppeteer-jest/", "abstract": "In Software Engineering, deploying to production is always a risk, but at the same time, it is always necessary. How else would you ship new features or bug fixes? Bugs are frequently introduced while deploying new features or fixing other bugs. We are only human, after all. Given this scenario, it seems wise to invest time learning mechanisms that allow you to minimize the risk of introducing new bugs during deployment. In this blog post, we will take a look at one such tool, called Puppeteer , and how we can use it to implement end-to-end testing, in order to catch bugs before sending them to production. Along with Puppeteer we also use Jest , to facilitate testing. End-to-end testing and Puppeteer To start with, it might be a good idea to discuss what exactly is end-to-end testing? It actually is nothing more than a particular type of testing, where a flow from the system is evaluated and checked from start to finish. For example, in an e-commerce platform, an end-to-end test could be “Add item to cart”. The process starts with the user picking an item in the online store, selecting the option to add to cart, opening the cart details, and finally checking that the item was added to the cart. And what is Puppeteer? It is a Node library made by the Chrome DevTools team , offering a high level API used to control either Chrome or Chromium. It can do a fair amount of things, but here we’ll focus on how it helps end-to-end testing. To set up Puppeteer, you can either create an entirely new project or use your existing application. Once that’s done, all that remains is to write the actual tests and fire them away. Setting up a project with Jest and Puppeteer Aside from installing Jest and Puppeteer, you should also add the preset jest-puppeteer to your jest config file. We’ll be using Mapright in this example, one of our amazing clients. The Puppeteer test here describes a simple login function: // Let's increase the timeout in order to be safe. jest.setTimeout(120000); describe('Login', () => { beforeEach(async () => { await page.goto('https://mapright.com/users/sign_in'); }); it('should fail login', async () => { // Remember you can take screenshots in order to debug issues. // For instance: // await page.screenshot({ path: 'step_1.png' }); await page.type(\"input[name='email']\", 'john.doe@example.com'); await page.type(\"input[name='password']\", 'password'); await page.click(\"button[type='submit']\"); await page.waitForSelector('.sign-in-form__error'); const signInError = await page.evaluate(() => ( document.querySelector('.sign-in-form__error').innerText )); expect(signInError).toContain('Invalid email or password'); }); it('should login successfully', async () => { await page.type(\"input[name='email']\", '[VALID_EMAIL]'); await page.type(\"input[name='password']\", '[VALID_PASSWORD]'); await page.click(\"button[type='submit']\"); // Wait for the request to be completed. await page.waitForNavigation({ waitUntil: 'networkidle0' }); // Check that login was a success. expect(page.url()).toContain('dashboard'); }); }); Mitigating risk of missing bugs There’s one major caveat in this approach; Puppeteer only handles Chrome. So, by running your end-to-end tests only with Puppeteer, you run the risk of missing bugs that only happen in other browsers, such as Firefox or Edge. This risk can be mitigated by ensuring good cross-browser compatibility, or by using a tool such as Selenium that offers an API supporting all major browsers. Selenium is a great tool but as it is built to support all modern browsers it is prone to performance implications. Your road to Puppet Master We’ve only scratched the surface of what can be done with Puppeteer and how it can be integrated with a development workflow. Here are some ideas for your road to becoming the Puppet(eer) Master: Use Puppeteer to generate PDF reports (yes, Puppeteer can create PDFs from the content of the browser). Place it into a continuous integration workflow. Follow the official Jest guide for integration with Puppeteer. Crawl a single page application and use it to generate server side content. Help diagnose performance issues. And many more! In this post, we learned what Puppeteer is and what it can be used for, how to use it in a real world application , and how to make further use of it. I hope you find this testing tool as useful as we have. We at Moove It keep seeking new ways to help our agile software development and day-to-day activities. New technologies emerge frequently and staying on top of trends is key for us. Stay tuned for more of our informative software blog posts!", "date": "2018-09-27"},
{"website": "Moove-It", "title": "turning-global-healthcare-data-usable-insight", "author": [" Rafael Fremd "], "link": "https://blog.moove-it.com/turning-global-healthcare-data-usable-insight/", "abstract": "We live in an information era. Never have we had as many available resources as we do now: libraries, universities, databases, apps, informatic systems, and technology, to name a few. Nowadays, information multiplies at an incomprehensible rate. According to a recent study by IBM , the world creates 2.5 quintillion bytes of data every day; the equivalent of recording 50 million CDs every 24 hours! Government departments and agencies, scientists, and entrepreneurs have released extensive amounts of data via information repositories, such as Wikileaks , patents, like Tesla’s , and architectural plans, such as the Elemental in Chile. At the same time, researchers continue to develop methodologies to collect new information nearly free-of-charge, while technology generates it automatically. These factors contribute to database growth rates that are inconceivable and unpredictable. The problem is that the data is not always usable because it is often difficult to understand, digest, or consume. And thus, the data can seem like both a blessing and a curse. A big problem & relevant challenge Health is an especially sensitive area for data storage and sharing. While innumerable service providers, laboratories, pharmacies, hospitals and researchers generate information constantly, unfortunately, everyone shares information in their own way. The data variations prohibit the users from combing their data in order to recognize trends or simply understand the information. Having realized the problem, karmadata , a healthcare data company, set out to tackle the challenge. karmadata’s purpose is very simple: to standardize, connect and transform the world’s health information. In a nutshell, karmadata is transforming unreadable data into usable information. For this, karmadata has developed a platform that – through APIs, Machine Learning and Artificial Intelligence processes – produces and delivers billions of standardized healthcare related datasets per year. karmadata’s platform allows developers to use their aggregated data to create their own systems or applications in a simple, intuitive, and agile way. Organizations in the US – and soon, worldwide – can build their Healthcare systems on top of karmadata easily, by consuming the API directly or by using the component library, which allows developers to build interfaces faster and display valuable information in just hours. Note: If you are interested in learning more about the React.js library click here. An infinite ecosystem. A powerful example Moove-it helped karmadata create map.healthcare , an application that runs entirely on top of karmadata’s platform. Map.healthcare provides targeted geospatial search and insight into physicians, hospitals, pharmacies, payers, patient populations, economics, and other healthcare data from the United States, effortlessly. As karmadata’s software development partner, Moove-it is responsible of assisting and helping karmadata on its technological challenges. With the innovation begun, the possibilities are infinite!", "date": "2018-04-25"},
{"website": "Moove-It", "title": "boosting-access-important-healthcare-info", "author": [" Rafael Fremd "], "link": "https://blog.moove-it.com/boosting-access-important-healthcare-info/", "abstract": "The healthcare system is not perfect: It can be unfair, slow, expensive – even incomprehensible. It is so vast and complex that it may seem as though there is nothing that can be done to improve it. Some might think: If hundreds of millions of dollars, different administrations, professionals and organizations have been unable to improve it, there is nothing that could be done. Not the karmadata team; instead it boosted their motivation to tackle this complex problem and find a resolve. The latest development, now widely available Since its founding in 2010, karmadata has been dedicated to compiling, organizing, and delivering information regarding the global healthcare technology system in a useful, secure, and quick way. Their platform processes and produces billions of pieces of data every day – literally. Completing karmadata’s initial platform was not the finish line. With the help of our software development team here at Moove-it , karmadata built its latest integrated platform application: map.healthcare . “This new application has the potential to change the way people interact and understand health-related information!” boasted Brendan Kelleher , Chief Data Scientist at karmadata. What is it and what is it for As a responsive web application, map.healthcare provides targeted geospatial search and insight into physicians, hospitals, pharmacies, payers, patient populations, economics, and other healthcare data. Examples of questions that could be answered using the app include: How many medical specialists are in your area? What is the density of pharmacies in a specific state? What are the areas of the United States with the fewest hospitals? Users of map.healthcare can select various healthcare topics, drill into geographical regions, or simply search by place or address to receive hyper-local healthcare market insights. The application serves multiple user groups , such as: Healthcare analysts and data scientists who need quick facts and trends on targeted geographical markets. Corporate users providing products and services to the healthcare markets that need access to the highest quality data on integrated delivery networks, hospitals, physicians, physician groups, pharmacies, payers, and other healthcare entities. Government agencies, non-profits, and corporates that need a place to publish their data, either as open data or in private pages for internal use. Free information for everyone Currently, karmadata is using a freemium business model. All users have access to the organized, secure, and up-to-date information free-of-charge. Those who need more detailed insights can easily access the app via a paid subscription service. As in this case, our focus is to keep working and partnering with companies that, through innovative technology, make a real, positive impact on people’s lives. We believe technology can make the future better. What do you think?", "date": "2018-04-26"},
{"website": "Moove-It", "title": "template-building-react-native-mobile-app", "author": [" Federico Ojeda "], "link": "https://blog.moove-it.com/template-building-react-native-mobile-app/", "abstract": "React Native is taking over the mobile world, from the very start of being released to the general public. Being a newcomer in the industry, there isn’t a proper consensus for the structure of a React Native based app, so we’ve decided to implement our own. A main reason we want to define a baseline for a new project is to reduce the number of decisions a developer has to make before developing a new project. Based on our previous experiences in several other applications, we have defined a project template that works as a starting point for React Native projects, providing a base architecture, a core framework, and helpers to jumpstart your development. Here, we introduce the fundamental concepts and tools included in the project. To just view the code, you can skip to our Github repository and access the template directly. Download template Base tooling As you may be well aware of, React Native uses Javascript , so we can use the NPM (Node Package Manager) ecosystem and all the libraries/frameworks that it provides. The following list includes all the basic packages that come pre-installed in the project with their purposes.  Most are explained later below, too. Axios for networking. PropTypes to type-check components’ exposed properties. React-Native-Config to manage environments. React-Native-Navigation as native navigation library. React-Native-Localization for string localization. Redux for state management. Recompose for utilities. Redux-Persist as persistence layer. Redux-Thunk to dispatch asynchronous actions. Reselect the selector library for redux. Jest and Enzyme for testing. Architecture Container – Presentation Even though not defined explicitly in the template, and being more of a team decision, we prefer to structure our components following the container-presentation pattern. For those not familiar with this, a container-presentation pattern consists of dividing your components into two types – container and presentation.  Container components manage logical state and any logic involved (such as data fetching). The presentation components are exclusively responsible for the UI. This can bring a lot of advantages in terms of reusability and separation of responsibilities.  To delve further into this, check how to structure the components and why containers are useful.  These posts explain with much more detail. Folder Structure Having an organized folder structure is probably the most important facet in a JS based application.  Here are more defined guidelines on how to use this structure with the files and folders. src : This folder is the main container of all code inside the application. actions : This folder contains all actions that can be dispatched to redux. assets : Asset folder to store all images, vectors, etc. components : Folder that contains all application components. Common : Folder to store any common component that you use through app (such as a generic button, text fields, etc). MyComponent : Each component should be stored inside its own folder, and inside it a file for its code and a separate one for styles. The index.js is used only to export the final component that will be used on the app. MyComponent.js styles.js Index.js helpers : Folder to store any kind of helper you may have. reducers : This folder should have all your reducers and expose the combined result using its index.js selectors : Folder to store your selectors for each reducer. controllers : Folder to store all network and storage logic (you should have one controller per resource). App.js : Main component that starts the app. index.js : Entry point of your application as per React-Native standards. State Management As most agree, Redux is a great option to handle the state of a React (native or not) application and rightfully so.  It’s popular, widely used, full of support, and provides a clean way to store in-memory data. But, like every other framework, Redux has its caveats.  If used incorrectly, you can end up having too much boilerplate code, which is confusing and makes the code base just messy.  You can review the source code of the template and see how we solved these issues. Navigation In order to perform navigations between screens, we went with react-native-navigation .  It’s a native library, so all navigation is done on the native side.  This is why it is extremely fast and performant. Also, it has an amazing API that’s intuitive and integrated with Redux .  Creating custom navigations is a bit tricky, and react-native-navigation is probably the best, most neat solution we have found till date. Networking There is no application on the market that doesn’t use a network request.  Every app generally needs to make a request through the internet. Finding a correct library for making HTTP calls is important.  We use Axios , and we love it, since it has huge support and popularity in the community.  It also provides a lot of cool helpers & features that just make life easier! Persistence It’s really common storing some data, starting from a small token to having multiple entities with complex relationships amongst them. This is why we needed a solution for this recurrent situation. For simpler cases, such as saving a token or smaller pieces of data, we use Redux-Persist .  It’s a library that’s built on top of Redux , and provides a clear, almost transparent way of saving the information that’s inside Redux ’s store. Whenever a more powerful tool to express a complex relationship, or for transactional operations over data is needed, we are definitely in favor of using Realm .  It’s a robust and powerful database that provides a simple SDK to create, maintain and operate databases on mobile devices. Standards As in any larger project, following a consistent coding standard helps improve the quality of the overall software.  Javascript’s ecosystem has some wonderful tools to enforce several guidelines. We went with ESLint , the default library in the JS ecosystem. About the rules, we didn’t want to reinvent the wheel so to speak, so we use ESLint rules made by Airbnb, based on their style guide.  Some exceptions were made to these rules, which you can see in the .eslintrc.json file. Testing No app should be coded without testing.  That’s why, from the beginning, our team began developing this template, with a mindset of a testing environment to be implemented, a priority really.  There are too many possibilities, but we liked most the combination of Jest and Enzyme . Jest provides a truly complete testing platform for React that requires really a small amount of configuration, and Enzyme allows us to perform tests over the DOM elements that are rendered and ensure that certain aspects of the UI are carried out. Configuration To best manage the configuration,  we are using react-native-config to handle all important configurations in one central place. How to use it There are two options to set up a new project using the template: Option 1: Copy the files This is the easier option, where you copy everything under the /src and the index.js folders into a new react-native project.  Naturally, dependencies are defined on the package.json file, so you can copy them to the dependencies of the newly created package.json. Option 2: Use react-native-rename Clone this repository to your machine. Change the git project remote URL to one of your own with git remote set-url origin https://github.com/USERNAME/REPOSITORY.git Install the npm package react-native name Run the following command in the root of the project: react-native-rename <newName> Bingo! The project’s been renamed with the new name Push it to the repository Wrapping Up At Moove It we have defined a starting point for any React Native project possible. Having a standard and base for any project is always an advantage, both for developers starting work, as well as for the newcomer diving into an existing project. React Native can get complicated for those with no experience. We feel is our responsibility, as a React Native Development Company , to share our knowledge and help those who are just beginning to use this technology. It’s always easier to step into a project knowing where everything is, which frameworks are being used, and other basic development information. New technologies like this can have a chaotic start with a lot of future changes. This template provides a big advantage and a lot of stability! One question we asked ourselves while building this was if we can use this for any type of software project. The answer is yes, hands down! One of our main goals is to keep this template as small and flexible as possible, completely user friendly. We hope it can be used in as many cases as possible. And we see it happening, we hope it helps you too!", "date": "2018-05-25"},
{"website": "Moove-It", "title": "framework-use-react-native-flutter", "author": [" Federico Ojeda "], "link": "https://blog.moove-it.com/framework-use-react-native-flutter/", "abstract": "Mobile development has won over the world, with companies shifting their priorities to their mobile products. With this global domination of sorts, companies have been finding and developing newer, better ways to progress. Frameworks, although started out with the development of hybrid apps showing a website inside an application, have been evolving into creating fully native apps that are spread into hybrid technologies – truly amazing! Presently, the two most popular frameworks out there are React Native and Flutter. While they both have similar (if not the same) goals, their structure and strategies are quite different. They both are used to develop iOS and Android applications while using only one codebase, and the following passages explain how each of these frameworks achieves what they do, as well as point out some pros and cons. What is React Native? React Native is a framework developed by Facebook that lets you write mobile applications using Javascript. Specifically, it uses React , a popular web framework, to render the UI. There are a lot of frameworks that have used Javascript for applications, such as Phonegap or Cordova. The main difference between these and React Native though, is that the latter outputs a completely native UI in 60fps with native components. The others use web views for the UI too, usually causing performance and user experience issues. But how does Javascript actually provide a native UI? Think of it this way. Your entire application runs in what’s called a Javascript realm, a native thread that runs the Javascript application logic.  Whenever something needs to be rendered, such as a change in the size of a view, the Javascript realm communicates to the native realm through a Javascript bridge. This bridge is responsible for serializing and sending every event through. You now have a Javascript application running, and whenever there’s an event that needs some native action, it’ll notify you with an event through the bridge. Easy! What is Flutter? Flutter, too, is a newer framework developed by Google and provides you the tools needed to develop a fully native application for both iOS, Android, and other platforms. Although the end goal is the same, Flutter is significantly different from React Native. A big, perhaps the biggest, difference is that Flutter compiles the application to native code. It doesn’t consist of another application running which then communicates it to a native project.  Basically, this means that whatever program you write will end up compiled into a native language that both iOS and Android platforms will understand immediately. In most specific scenarios, this may lead to a better performance of the application as a whole. This also entails that everything (yes, every single thing) is controlled inside the framework.  Flutter doesn’t use the native widgets provided by each platform but instead, offers its own widgets that resemble the standard ones. For example, if you want to use a button, Flutter will provide one that looks the same as the native iOS button, but really provided by the framework. This does make it easier to provide your own design for widgets such as lists and buttons. On the other hand, it also makes it harder for the framework to stay current with new native components released. Dart is the language used by Google for Flutter. It probably doesn’t sound familiar, but it looks similar to what anyone has used in Java or Javascript before. It’s not just basic and definitely worth getting to know. How do they compare? You now have a general outlook to compare between React Native and Flutter, but there’s more. Usually, when you need to select a platform to develop an application, the decision is not solely based on the architecture or if the framework is bright and new. There are a lot of other aspects to analyze too, such as community, the existence of third-party solutions to common problems, and platform maturity. Thanks to our experience as a React Native Development Company is that we can provide an overview of what is currently working better. As of now, React Native is more established, and a more well-settled platform. It has been around for a longer time and has grown into one of the largest communities, providing solutions to just about any problem out there! Developer adoption and reception are wonderful, too. On the other hand, Flutter looks promising, but as a forecast for the future. Its core concepts are truly interesting with its beta version released not too long ago. But being newer doesn’t always mean being better, it means more of an analysis that will pass with time. In the end, our vote’s with React Native !", "date": "2018-09-21"},
{"website": "Moove-It", "title": "5-non-obvious-principles", "author": [" Rafael Fremd "], "link": "https://blog.moove-it.com/5-non-obvious-principles/", "abstract": "“Product discovery” is how we name the process in which we transform client’s ideas, thoughts, problems and insights into a high-fidelity prototype. Generally, the entrepreneur or product owner has an idea in his mind. We facilitate the process of taking those thoughts, understand the possible users, get deeply immersed in the market, and then transform all that information into a usable, self-explanatory prototype that looks good and is easy to use and understand. It is important to share and validate the concept, receive feedback, test the idea and learn from real users. Implementing this process before developing the actual solution it is not just economical and fast, it is also smart. The prototype will help product managers to understand better if the need is a real need, giving users the chance to share their opinion around the solution. This information should be used to make decisions about priorities and the possible future roadmap. Even though iterating is generally good, doing it early is double as important. The other relevant element is the iteration pace: doing it during the design phase is cheaper, quicker and easier than iterating when the actual product is being built. The objective of this article is to share some of the most important not obvious learnings we have had while designing products for our clients and for us. Dozens, literally dozens in the last few years. 1. Product discoveries are not all the same. At first glance, all product discovery processes seem to be similar. Some words are repeated once and again and again: wireframes, mockups, components, prototype, screens, user stories, pain-points, etc. Reality is quite the opposite: every product we have built so far, have something different to discover, design or implement from scratch. 2. Processes are made to be followed; and challenged. Let’s be clear: processes are made to be used and followed. The team leader is responsible for strictly protecting processes. Every step is important for the next one and for the overall goal. But, as said before, every client and product is unique. When our guts are telling us something different than the process is saying, we should at least stop and listen to them – and think. We welcome thoughts such as: “Ok, according to the process we should now implement some specific exercise… But… what if instead, we go two steps ahead or go back to the starting point?” It might enlighten the team: helping understand the big picture or showing where should the focus be directed. 3. Breaks make the work faster and better. Some might think that if they invest many hours in front of screens or make lots of wireframes and tests, the goal will be achieved fast and the prototype will be ready soon. Experience has taught us it does not work like that. Sometimes brakes are necessary: asking for feedback on what we have done so far to external people, looking for references outside the client´s market, conducting research of trends, for example, are powerful brakes. At a first glance, interruptions seem to be distracting, but when they are planned, they are not. They add perspective to the team, feedback highlight errors and the research process is generally enlightening, not just because of the findings, but also because the exercise feels like a breath of fresh air. Breaks are generally a positive and effective boost. 4. Iterations are necessary, but not infinite. Time is finite. The product discovery phase generally takes between four and six weeks. It involves listening to stakeholders, analyzing features competitors, and trends, making presentations, receiving feedback, adjusting designs accordingly, etc. We could invest months looking for the perfect product trying to satisfy all people involved, iterating once and again and again; the truth is that the most – maybe only- important stakeholder is the real user. Thus, finishing the product discovery phase is important, even when the product is not 100% defined. Previous assumptions and ideas are worth zero, comparing to real user’s feedback and insights. Product discovery processes should finish, because that’s when the real learnings begin. 5. The more we know, the less we know. Product discoveries processes aim to find something unique: a powerful insight, some gesture, an innovative product. For that, in Moove It we have incorporated practices from different parts of the world and universities, we have studied multiple authors, gained experience from clients and colleagues and we have implemented dozens of product discoveries. And yet, we still get surprised every time on how we feel every time we start a new process: that feeling of not knowing where are we heading, that uncertainty, boost our spirits and make us enjoy the ride.", "date": "2018-11-21"},
{"website": "Moove-It", "title": "2017s-edition-of-railsconf", "author": [" Hernán Daguerre "], "link": "https://blog.moove-it.com/2017s-edition-of-railsconf/", "abstract": "The last week of April marked Moove-it’s participation, for the second time in a row, as a sponsor of the ‘17 Edition of RailsConf . This initiative continues to represent a fruitful experience for our practitioners, and, as a whole, it entails another exciting opportunity to boosting and broadening our name across the US and International Ruby / Rails communities. These three intense days were all but sudden and casual networking opportunities, comrade activities, hallway chitchat and biz cards’ exchanges, while, some of the most technically savvy attendees indulged from the keynotes and more technical talks concurrently taking place within the Phoenix Convention Center’s premises. At the exhibitor area, many curious folks swung by our booth avid to learn more about our business, stacks, and culture. While some of them just passed through to pick up some of our souvenirs, many others hanged in there for a while… We were stunned upon the amazing traction from a multitude of craving techies who immensely enjoyed the Ruby / Rails / Javascript trivia contest for the chance to win a DRONE!! Many just stayed there for almost an hour, while others kept coming back over and over again!! In the end, another productive and worth-to-repeat experience. Special thanks to everyone who stopped by and the folks at the organization for their continuous support throughout the event. During the next few weeks, we will be bringing in more exciting news about upcoming participations in other renown technology events. If you haven’t yet, follow us on Twitter ( @moove_it ) to stay current on the latest…", "date": "2017-05-13"},
{"website": "Moove-It", "title": "kaigi-retrospective-tool-distributed-teams", "author": [" Daniel Garrison "], "link": "https://blog.moove-it.com/kaigi-retrospective-tool-distributed-teams/", "abstract": "Agile methodologies, nowadays, are the de-facto development framework for software projects in the industry. Moove-it firmly believes in the benefits of using Agile, and over the years has been developing its version based on hundreds of successful and not so successful in house experiences. One of the key elements of Agile is fluid communication among team members and stakeholders. In a distributed environment, which is getting very common these days, developers, project managers, product owners and stakeholders work remotely, and communication is becoming a challenge. Based on our experience, communication is the key element when it comes to determining if a project was successful or not. Retrospectives The Retrospective is one of the most important activities in the Moove-it development framework. The Retrospective meeting is there to help direct the next sprint by creating action items at the end of this period. This section starts with a timed round where everyone involved in the process writes down what went well on green sticky notes, what went bad on red sticky notes and ideas on how to improve the next process on blue notes. After the round, the team is then given several votes each to place among the sticky notes. These votes are to represent how much the team thinks they need to focus on the topic presented by that sticky. The discussion section of this phase is where the team talks about each sticky, and then the group creates action items based off of their deliberations that can be put into following sprints or be conversation pieces with clients to get a better understanding of what they want. This last portion can get messy even when everyone is in the same room. The streamlining and polishing of this process is why Kaigi was designed. Why we built Kaigi Nowadays working remotely isn’t a deal breaker, but it does present some challenges when using some of the tools of the Agile methodology. Particularly with the distance involved, the Retrospective meeting can get muddled by messiness. Distance can lead to a difficulty in defining priorities, expressing ideas and record keeping on the various points of the Retrospective. While all of this can be bypassed by a keen ear, sharp mind, and fast/precise penmanship, we at Moove-it know that can be taxing and even when things are executed well, sometimes slip through the cracks, which is why we made Kaigi . Kaigi is developed by Agile teams for Agile teams. We chose the name Kaigi in true Agile fashion. Most things in the Agile environment use Japanese names. Kaigi means meeting in Japanese. We hope it leads you down a clean and efficient path. Kaigi started off as a small in house project to streamline and keep track of the Retrospective phase in an easy to reference and permanent way. Kaigi revolves around creating virtual stickies with a slick interface and everything has you’ll need to make these review periods as productive as possible. The Moove-it way The Retrospective is an opportunity for the Scrum Team to inspect itself and create a plan for improvements to be enacted during the next Sprint or period. This definition, provided by the Scrum Guide , is broad enough to come up with different formats and agendas, depending on the project, the roadmap, and the circumstances. During the Sprint Retrospective, the team discusses what went well, what could be improved and what will they commit to improve during the next Sprint. Every team in the world has their own way to run Retrospective meetings, defining stages that fits their needs.  At Moove-it, after many iterations, we settled on the five step process described below. Kaigi streamlines each of these five steps and creates an efficient workflow for the Moove-it way of having Retrospectives. Create Stickies During the first stage we differentiate three classes of comments: what went well, what didn’t go well and ideas to improve our development process and deliverables. Each one of the items gets typed out onto a particular color of electronic sticky, green for good things, red for bad things and blue for ideas. Adding a sticky in Kaigi is pretty simple: first, click over the color you are going to use and then, type the text. By pressing enter, you’ll add a new sticky in the list that will be added to the catalog of stickies below. In Agile software development, a timebox is a defined period of time during which a task must be accomplished. Timeboxes are commonly used to manage software development risk. The sticky creation stage comes with its own built in timer that you set before the process begins. That way the only thing you have to worry about voicing your thoughts and opinions, and you are time boxed to a limited amount of time. Grouping Stickies We have now made our stickies by type, but they are still unorganized. During this step, the team group the stickies by concepts or ideas. This is accomplished easily. First, you click on the drop down menu and add the groups you would like, then you just drag and drop each sticky into each pile. Vote Groups Each team member gets a predetermined number of votes to split up among the groups that we created in the previous stage. This voting process determines the priority each category of sticky has in the Retrospective meeting. Discuss Stickies Now we get to the meat of the Retrospectives. It is time to discuss as a group our most highly rated category of stickies and create the actions items that will help smooth out and improve the process. This step is also timed like the defining stickies step. Action items are set up by typing them into the field at the top of the page. Once created they can be assigned to specific members of the meeting via the drop-down menu to the right of the action item field. If you finish discussing the most popular group within the time limit, you can click on the title of another group to switch your view to that category. This makes an easy to see and organized way of dealing with the issues that have come up over the course of the Retrospective. Summary The final step of our process reviews our Retrospective in a nice itemized list and saves you action items to your GitHub account. Making it not only easy to read and review what your team decided on but easy to find in the future. Kaigi makes remote tasks like the Agile Retrospective much less unwieldy than before. If Kaigi interests you and your team, please follow this link for a beta key code! At Moove-it we strive for constant improvement. Please feel free to reach out to us with any feedback and comments you may have!", "date": "2017-11-14"},
{"website": "Moove-It", "title": "our-first-experience-with-anima", "author": [" Lucía Carozzi "], "link": "https://blog.moove-it.com/our-first-experience-with-anima/", "abstract": "What is Á NIMA? Á nima opened its doors in February 2016 as a new educational center with an innovative proposal for the three last high school years. They offer two orientations: 1. Technological Baccalaureate in Administration and 2. Information and Communication Technology (ICT). Why innovation? Because they believe that much of students’ knowledge is learned not only in the classroom but also on the job, the program offers to teach theory in the classroom and accompanied by professional training at a company. With this dual approach, Á nima wants to contribute to providing the connection between education and the real world, waking the interest of young people in those areas. What does Moove-it have to do with ÁNIMA? We are proud to be one of the companies that offer a working experience for Á nima’s students! It started early this year and we had meeting after meeting, planning: When do the apprentices start? What should they do? What do they already know? What would we need to teach them? Are they going to adapt to the working environment? Are we going to be adapted to teenagers? We had lot of questions, fears and hopes heading into this experience. So we decided to stick to the basics. We knew we wanted them to experience our work to its fullest – dealing with clients and deadlines – but we also didn’t want to scare them at their first job! We would first analyze where we stood and then build from there. When the start date arrived, we introduced Valentina and Matias to Moove-it. They had their first-day inception: getting to know everyone in the office, learning where the coffee and cookies are, and becoming familiar with where they were going to spend the rest of the year working. We talked about what programming languages they already knew and decided to begin with some tutorials and a basic training on git and how to work collaboratively. After that, they would learn Agile methodologies and then jump right into one of our in-house applications doing front-end work. We would have Moove-it’s Sofia and Martin as “clients” and Cristian and I would work as their mentors. Valentina and Matias would come for four hours every Tuesday, Wednesday, and Thursday. We created a big board where we added everything that we wanted to work on; Matias and Valentina tested the application and added tickets to fix the bugs they found. At first, the experience was similar to a classroom, like asking to use the restroom or make coffee, while the coding experience started slowly. Their first commits where little code with a huge amount of comments and code review; it wasn’t easy to achieve our standards! Estimations were off too – by a lot. We wanted to fix everything possible in one week (that was eventually translated to a month of work.). Slowly but surely, we were making changes and coming closer to our first deploy! Of course, thanks to Murphy’s law, our first deploy crashed… We notified them and the reality kicked in strong: We needed to fix it! They managed to not surrender to the stress, found the source of the crash, and fixed it accordingly. We felt so proud! Weeks passed by smoothly and, since day one, we knew that we wanted them to experience everything that is involved with working in the tech industry. We made sure they got to know not only Cristian and me, but also join other team members in their planning sessions and retrospectives – all new experiences to them! Fast forward to today… Valentina and Matias finished their first year of working experience with Á nima and Moove-it. We all enjoyed the experience so much, that when they asked us if they could continue to work at Moove-it until their next school year starts, we were thrilled! To execute this idea, we needed to make a new plan. Now that we will have more time, we want them to go through the whole mentoring process, which is the same training provided to every new Moove-it team member. We consider Valentina and Matias as a part of the Moove-it team. They worked side-by-side with us, they shared after-office gatherings, and even wore Halloween costumes! We knew that we wanted to participate in what Á nima was doing, but we never expected to get to know two amazing people so aligned with our core values, eager to learn, open to criticism, fearless of new experiences, and happy to come to work every day.  At the end, they motivated us to do our best and reminded us why we do what we do – because we love it! Thank you guys, see you in a few weeks.", "date": "2017-11-27"},
{"website": "Moove-It", "title": "the-clean-architecture-on-android", "author": [" Juan Manuel Pereira "], "link": "https://blog.moove-it.com/the-clean-architecture-on-android/", "abstract": "At Moove-it we work hard to create beautiful, scalable and maintainable Android apps. To achieve that goal, over the years we have tried different architecture approaches (MVP, MVVM, MVC and some custom architectures) until we finally encountered the Clean Architecture . In this post we introduce the Clean Architecture and then explain the approach we took to use it on Android. If you are a show-me-the-code person, you can jump directly to our example on GitHub . About the Clean Architecture The Clean Architecture was introduced by Uncle Bob. Its main goal is the separation of concerns by diving software into layers (see image). What makes it work is the dependency rule , which basically states that source code dependencies can only point inwards. So the idea is that inner circles are not aware of the outer circles and only the outer circles know that the inner layers exist. Following this rule, we can build software holding the following quality properties: Independency of Frameworks. Testability Independency of the UI Independency of the Database Independency of any external agency On Android We decided to separate our logic into three layers: Domain : This is the layer where all the business logic occurs, we do not have any Android dependency here. Data : It is in charge of getting the data for the app. Presentation : Everything related with the UI logic and the render. Presentation Layer We use MVP to develop our presentation layer. View The view is a set of interfaces that could be implemented by any Android view, such as Activities, Fragments or Custom Views. These views are dummy in the sense that they only act in response to orders from the presentation layer. Presenters Serve as a middleman between views (abstractions over Android specific components) and the business logic ( interactors ). Domain layer Here we introduce the interactors . Each interactor is a reusable component that executes a specific business logic. In the GitHub example, the use cases are: “Add a pet”, “Edit a pet”, “Delete a pet”. It fetches the data from a repository, executes the business logic and returns the result to the presenter. Data layer We use the Repository Pattern to create an abstraction of the datasources from which the interactors get the data to act upon. This example uses a database as a local datasource and cache mechanism and also a remote datasource that syncs with the server using a REST API. Initially, the repository persists the changes locally and then it syncs with the server. It uses the local data unless it’s told that the cached data is stale, thus needing to sync with the server. The syncing mechanism is pretty simple: wipe the database and store the remote items again — this could be improved. Communication Our first approach to this architecture was communicating the layers using callbacks. We had nested callbacks to communicate through the view to the interactor and back to the view again. Instead of using callbacks to communicate between layers we use the power of RxJava to provide the data upstream. Each of the inner layers can transform the data in a way the outer layer can understand it. Testing Unit testing Presenters , interactors and repositories are unit tested by using JUnit and Mockito . They are all framework-agnostic components that use plain Java objects and do not reference any Android specific code, thus making unit tests really easy to build and maintain. UI testing We use Espresso tests together with Mockito for integration and UI tests. Mockito allows us to mock the Observables returned from the repository to be able to test different states of the UI, like success, failure and progress. Dependency Injection We use Dagger 2 as a dependency injection framework which allows us to have control of the dependency graph and inject mocked dependencies while testing. We organize the dependencies in modules in a way they can be easily interchangeable with mock modules while testing. We use Dagger’s recommendation to organize the modules . Conclusions We use the Clean Architecture approach for developing apps that are easy to maintain, test and — why not — to reuse the business logic in other projects. But, most importantly, we are doing a favor to future developers that are going to work on the code. Getting your hands on tightly coupled code — where you are afraid to change even one line of code — is painful! Through the Clean Architecture approach, we can lead the world of Android development to a happy place :). One question we have asked and discussed with my colleagues is whether we can apply this architecture to all cases. My opinion is: yes . No matter how small the app is, it may grow in the future, so why not do things right from the beginning? Code You may want to take a look at our example on GitHub where we use the Clean Architecture in a small app for pets. References Clean architecture: https://blog.cleancoder.com/uncle-bob/2012/08/13/the-clean-architecture.html Dagger: https://dagger.dev/ RxJava: https://github.com/ReactiveX/RxJava Espresso: https://developer.android.com/reference/android/support/test/espresso/Espresso.html Mockito: https://github.com/mockito/mockito Roboelectric: https://github.com/robolectric/robolectric Next What are your thoughts on this architecture? Do you have any experience with it? Let us know. In the next post, we are going to build a step-by-step application applying the Clean Architecture approach using Kotlin . Stay tuned!", "date": "2017-04-28"},
{"website": "Moove-It", "title": "solving-memory-leak-jruby-1-7-ruby-1-9-redis", "author": [" Martin Rifon "], "link": "https://blog.moove-it.com/solving-memory-leak-jruby-1-7-ruby-1-9-redis/", "abstract": "The Problem On a recent project running on jRuby 1.7.23, mizuno 0.6.8 and the Rest Service Framework Angus 0.0.13, we were faced with a substantial memory leak. During our initial troubleshooting, we found that restarting mizuno weekly prevented the process from crashing. This meant that while we were debugging, we were also scheduling restarts every Monday night in order to prevent any downtime for the 500+ requests per minute. While this worked, it was a short term solution, because: 1. it meant a team member would have to be present every Monday evening for the restart ( and what’s the fun in that?!) and 2. changes in system demand would affect the schedule, and we wouldn’t know until it was too late. For example, if demand suddenly spiked, it may cause the system to crash; fixed only by more frequent restarts. We needed to find our long term solution. Troubleshooting jRuby To begin troubleshooting, we started by dumping the heap right before doing a restart by using the command: jmap -dump,format=b,file=heap_before_restart.bin [pid] 1 jmap - dump , format = b , file = heap_before_restart . bin [ pid ] We waited a full day and dumped the heap again, in order to get a more realistic heap benchmark. Then we ran both dumps through VisualVM (which BTW is an excellent tool), generating the following results: Figure 1: VisualVM results: Before mizuno restart on jRuby 1.7.23 Figure 2: VisualVM results: After mizuno restart We noticed an odd return: org.jruby.RubySymbol had dramatically increased in size from one memory dump to the other. A closer inspection of this class revealed the following: Figure 3: Wow, that’s a lot of jRuby symbols! We then took a look at some of the latter ones: Figure 4: First symbol of the last set Figure 5: Last symbol of the last set They all seemed to follow the pattern: current-[number] . It was very unusual to see a large number of symbols being defined and not garbage collected. We researched garbage collection in Ruby, which revealed that the garbage collection process does not mark symbols for deletion, at least until Ruby 2.2. The jRuby 1.7.23 that we were using is based on Ruby 1.9.3. It looked like we had found the culprit! Somewhere in the code, symbols were being defined over and over again, using dynamic values. Banking on that idea, we performed a search in the codebase, yielding the following line, from the gem connection_pool : @key = :\"current-#{@available.object_id}\" 1 @ key = : \"current-#{@available.object_id}\" This pattern matched exactly the symbols that were showing up in the memory dump. We were getting closer to the solution. Investigating the codebase further resulted in the following: module Services\n class RedisStore\n\n   .\n   .\n   .\n\n   def redis\n     @redis ||= ConnectionPool.new(@@pool_settings) { Redis.new(@@redis_settings) }\n   end\n end\nend 1 2 3 4 5 6 7 8 9 10 11 12 module Services class RedisStore . . . def redis @ redis || = ConnectionPool . new ( @ @ pool_settings ) { Redis . new ( @ @ redis_settings ) } end end end The class RedisStore defined a new ConnectionPool upon initialization. And for every new request for a specific resource, this class was initialized. This led to a new symbol, with the dynamic name. Now, we had a strong hypothesis, and it was time to test a countermeasure. The Solution To test our theory, we set a trap by changing the code so that the redis variable became a class variable (instead of an instance one). After that, it was time to upload the change into our testing environment and start hammering requests for the specific resource. Memory dumps in between the symbol table started growing larger and larger. Bingo! We had found the solution. A set of smoke tests were run in order to ensure that everything was still working correctly. After that, it was a matter of scheduling and executing a deploy, then monitoring the memory usage periodically. During monitoring, we generated dumps every day for about two weeks. Below are some examples of what we found. While monitoring, we found that the symbol table did grow, we also found that the increase was normal and the scores of current-[number] symbols were nowhere to be found. This meant that the problem was solved – and the team was happy! Figure 6: First jRuby HeapDump post fix Figure 7: Second jRuby HeapDump post fix Figure 8: Last jRuby HeapDump post fix", "date": "2017-09-04"},
{"website": "Moove-It", "title": "machine-learning-javascript-part-neural-networks", "author": [" Lucas Aragno "], "link": "https://blog.moove-it.com/machine-learning-javascript-part-neural-networks/", "abstract": "A couple of months ago I gave a talk at the NodeSummit about my experience using machine learning with JavaScript and a brief introduction to neural networks. In this post, I will focus on Neural Networks – reviewing additional examples of neural networks using synaptic, which is a library for neural networks for both NodeJS and the browser – before delving more into Machine Learning. The Neuron First, let’s do a quick recap of what a Neural Network is and how it is used. For this, we will start from the basic component of the neural network: The Neuron. This is a neuron. As you can see, a neuron has a set of inputs with its own weights plus a special input called a “Bias” which always has a value of 1. The goal of this structure is to establish that if all of the inputs of the neuron are 0, then the neuron will still have something after the activation function is applied. Before moving onto activation function, we need to know how to compute the neuron state. We can accomplish this by adding every input multiplied by its weight, like so: L0 * W0 + L1 * W1 + […] + Ln * Wn + B * Wn+1 When we activate the neuron, it computes its state and runs the output through the Activation Function. The idea of the activation function is to “normalize” the result (usually between 0 and 1). The Activation Function is typically a Sigmoid Function: There are a variety of activation functions, including: Logistic Sigmoid Hyperbolic Tangent Hard Limit Functions Rectified Linear Unit Here are additional resources to learn more about these activation functions: “ Logistic Regression: Why Sigmoid Function? ,” “ Neural Networks 101, ” and neural networking activation methods . Neurons can perform beyond an activation function and can also project their output to another neuron: In this example, we are using the Neuron class from synaptic to create two networks: A and B. Then we project A to B: the output of A is going to act as the input of B. When we prompt A.activate(0.5) , we are sending 0.5 as the output of the A neuron and it’s going to get multiplied by a weight (which will be random at first). It is then applied to an activation function on B to generate an output ( 0.3244554645 ) when B is activated. It is important to note, as in the example, that if we call to activate with a value, that means we want that value to be the output of the neuron. If we call to activate without any value, it is going to perform the activation function with the neuron state. Propagation At this point, we have made a neural network composed of two neurons, but how does a network learn? There is another operation that a neuron can perform: Propagation. When we propagate, we “adjust” the random weights using the error, in order to get a desired output based on an input and its error. From this, we can determine how far off from our desired output we were based on the output that we got for that input. Sounds a little confusing, right? So let’s do an example: Let’s train our little 2 neuron network so that whenever we get a 1 as the input, it will output a 0. For this example, let’s assume that an input of 1 represents “nice things” and an input of 0 represents “bad things.” On the other hand, an output of 0 represents “nice” and output of 1 represents “bad.” From there, it becomes a simple logical equation: If we input a 1, then we get an output of 0, therefore meaning to our network that “nice things are nice.” Here we have the code. Lines 1 through 5 are basically what we did in our previous example: We create the A neuron and the B neuron and project A to B. Then, we set a learning rate. This is one of our hyper-parameters that’s going to help us to reduce the “cost function” of the neural network by adjusting those weights to get the desired output. For now, this parameter may be kept as is and tweaked as you see fit, in order to achieve the desired results. (You can learn about the math behind that here .) The next step is to loop the steps: A.activate , B.activate and B.propagate 20,000 times through a block of code that’s going to be our “training session:” To begin, activate A with 1, meaning that 1 x [randomWeight] is going to be the input of B. Then we activate B. Note that the first time this is done, you will likely get a higher number than 0 (perhaps even a 1), depending on the first randomWeight . This is okay and it is why we propagate after the activation. Using the learning rate and the desired output for the previous input, it calculates the error from the previous activation and the 0 that we want to achieve. It then propagates that back to the connection between the neurons, adjusting the weights to get closer to the desired output: This is called backpropagation. This will be done 20,000 times; adjusting and re-adjusting the weight 20,000 times, so that whenever we have a network input of, we get a 0 output. After the “training session,” if we activate A with a 1, then activating B should result in a number very close to 0. And voila! , our network just learn how to respond 0 whenever it gets a 1! Neural Layers Of course, this is just a basic example, because a real neural network is composed of many neurons adjusted in various layers. The first layer is called the Input Layer. This layer is comprised of the inputs provided by the engineer. The last layer is called the Output Layer, which will output the results of the activation of the network. In between those two layers, there may be anywhere from zero to countless layers, which are called Hidden Layers. These layers can serve a variety of purposes, but their main objective is to get a better output on the neuron and number of Hidden Layers. The amount of neurons in Hidden Layers need to be tweaked depending on the case usage. You can learn more about Hidden Layers layers with “ What’s Hidden in the Hidden Layers? ” and “ What does the hidden layer in a neural network compute? “ Building a Complex Network Now that we’ve covered the basics of neural networks, let’s build a more complex network. This new network will be able to solve an XOR operation, wherein two inputs have one output. If you are unfamiliar with XOR, it looks like this: To perform this function, we need to train our network to understand the XOR output of each set of inputs. Before getting into code, let’s review our problem and define the amount of layers and neurons required. We will need: Our Input Layer, with 2 neurons (since we have 2 inputs) Our Output Layer, with 1 neuron (since our output is just 1 number) A Hidden Layer (to improve the results) with 2 neurons (since the rule of thumb is that the amount of neurons on the Hidden Layer is a number between the output and the input) That’s how it would look on Synaptic. We can call directly the Layer and Network classes to create the layers with the amount of neurons that we want, then project the layers in the order that we need, creating the network with our layers in it. It would look something like this: Now that we have our network, we can ‘train’ it by writing another ‘training session.’ As seen here, networks have the same API as the neurons’ input value we activate it with, propagating the error using the desired output to adjust its weights. After all that training, we should be able to calculate the xorNet with values from the table and get the expected results. For example: If you want to change the network results (for better or worse), you can adjust the learning rate and the number of loops per training session (hyper-parameters) to see what happens. You might also want to try Synaptic counts with other classes, such as an Architect (to build different types of networks), a Trainer (to create those training sessions easily), and more utilities that could save you time. For more on these, I suggest reviewing the Synaptic Wiki . This concludes the basics of Neural Networks with JavaScript. Stay tuned, as we will release another blog post to delve deeper into Machine Learning soon!", "date": "2017-12-18"},
{"website": "Moove-It", "title": "getting-started-preact-progressive-web-apps", "author": [" Lucas Aragno "], "link": "https://blog.moove-it.com/getting-started-preact-progressive-web-apps/", "abstract": "Are you concerned about accessibility? Do you want your site to be blazingly fast and accessible, even from some random guy with a 2G connection on the North Pole? Are you afraid about being sued by Facebook ? (JK probably not happening tho.) Then you might wanna take a look to Preact , a lightweight alternative to React — and why not add learning about progressive web apps on the mix? This blog post attempts to show you how these two things works together by creating a little webapp to search TV shows. Before we get started with Preact, lets start with the project configuration. Setting up Babel and Webpack 2 A little note here. There are some Preact boilerplates / starter kits out there, but most of them are meant for complex apps and thus they install a lot of useful-yet-not-always-needed libraries and modules. I asked @_developit if he knew about a minimal set up for Preact apps and he came back to me with this helpful piece of art . You can skip this step and use his approach if you want to. If you stick to my approach, first let’s create our project structure: .\r\n├── index.html\r\n├── package.json\r\n├── public\r\n├── src 1 2 3 4 5 . ├── index . html ├── package . json ├── public ├── src The directory src / will contain the source code for our Preact app, public / will hold all the builds for JS and CSS, index . html will contain the app barebones and package . json is all about our dependencies (and maybe some scripts). Also, let’s get started by filling our index . html with some code like this: <!DOCTYPE html>\r\n<html>\r\n  <head>\r\n    <title> TVApp </title>\r\n  </head>\r\n  <body>\r\n    <div id=\"root\">\r\n    </div>\r\n  </body>\r\n</html> 1 2 3 4 5 6 7 8 9 10 < ! DOCTYPE html > < html > < head > < title > TVApp < / title > < / head > < body > < div id = \"root\" > < / div > < / body > < / html > Now let’s add some modern-js-fatigue-frontend-tooling. Read this carefully because your brains may fall out: yarn add -- dev webpack babel - loader webpack - dev - server babel - core babel - preset - es2015 babel - plugin - transform - react - jsx Notice that I’m using Yarn, because I like live on the edge. You can use npm install -- save - dev if you are not ready for this level of awesomeness yet. If you are already a pro setting up your webpack and babel configs, feel free to skip to the next section. If you are not ready yet, after this you might end up using webpack a lot, or starting your very own js fatigue post. Either way you are gonna get something from it. Let’s start creating the infamous webpack.config.js file: JavaScript var webpack = require('webpack') // require webpack of course\r\nvar path = require('path') // path is nice to have\r\n\r\nmodule.exports = {\r\n  entry: path.join(__dirname, 'src/index.js'), // our entry file\r\n  output: {\r\n    filename: 'bundle.js',  // we are creating this file after bundling\r\n    path: path.join(__dirname, 'public'), // this will be the folder for bundle.js\r\n    publicPath: '/static/' // we are gonna serve the bundle from here using webpack-dev-server\r\n  },\r\n  module: {\r\n    rules: [\r\n      {\r\n        test: /\\.jsx?/i,\r\n        loader: 'babel-loader',\r\n        options: {\r\n          presets: [\r\n            'es2015'\r\n          ],\r\n          plugins: [\r\n            ['transform-react-jsx', { pragma: 'h' }]\r\n          ]\r\n        }\r\n      }\r\n    ]\r\n  }\r\n} 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 var webpack = require ( 'webpack' ) // require webpack of course var path = require ( 'path' ) // path is nice to have module . exports = { entry : path . join ( __dirname , 'src/index.js' ) , // our entry file output : { filename : 'bundle.js' , // we are creating this file after bundling path : path . join ( __dirname , 'public' ) , // this will be the folder for bundle.js publicPath : '/static/' // we are gonna serve the bundle from here using webpack-dev-server } , module : { rules : [ { test : /\\.jsx?/i , loader : 'babel-loader' , options : { presets : [ 'es2015' ] , plugins : [ [ 'transform-react-jsx' , { pragma : 'h' } ] ] } } ] } } I tried to keep things minimal, so no fancy configs, but this will do the work. The only “confusing” thing here is the path vs publicPath thing. For now just think this: for the final (production) bundle we are throwing that file on the public directory, but for webpack-dev-server (a little server that we are gonna use to work on dev) we are serving from / static / . Now we need to tweak a little bit our package json in order to look like this: JavaScript {\r\n  \"name\": \"moovieapp\",\r\n  \"private\": true,\r\n  \"dependencies\": {\r\n  },\r\n  \"devDependencies\": {\r\n    \"babel-core\": \"^6.23.1\",\r\n    \"babel-loader\": \"^6.3.2\",\r\n    \"babel-preset-es2015\": \"^6.22.0\",\r\n    \"webpack\": \"^2.2.1\",\r\n    \"webpack-dev-server\": \"^2.3.0\",\r\n    \"babel-plugin-transform-react-jsx\": \"^6.23.0\"\r\n  }\r\n} 1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \"name\" : \"moovieapp\" , \"private\" : true , \"dependencies\" : { } , \"devDependencies\" : { \"babel-core\" : \"^6.23.1\" , \"babel-loader\" : \"^6.3.2\" , \"babel-preset-es2015\" : \"^6.22.0\" , \"webpack\" : \"^2.2.1\" , \"webpack-dev-server\" : \"^2.3.0\" , \"babel-plugin-transform-react-jsx\" : \"^6.23.0\" } } We have just added the “babel” key telling babel “hey we are using this preset”. You can move this thing from here into a . babelrc file if you want to. And we are done. Yes, that was it, at least for now. A really painful experience right? (</sarcasm>) So, let’s test our setup! we can create an index . js file on our src / folder with the following content: JavaScript document.addEventListener('DOMContentLoaded', event => (\r\n console.log('test')\r\n)) 1 2 3 document . addEventListener ( 'DOMContentLoaded' , event = > ( console . log ( 'test' ) ) ) I use DOMContentLoaded to wait for everything to load. You can use whatever you want (e.g. If you are into lispy things: ( ( ) = > ( console . log ( 'test' ) ( ) ) ) Next, let’s tweak our index . html : <body>\r\n    <div id=\"root\">\r\n    </div>\r\n    <script src=\"/static/bundle.js\"></script>\r\n  </body> 1 2 3 4 5 < body > < div id = \"root\" > < / div > <script src = \"/static/bundle.js\" > </script> < / body > Here we just added the < script > tag to load bundle . js from / static / . Let’s take this for a run by doing node_modules / . bin / webpack - dev - server on our console: Webpack should be running a server on your port 8080 in localhost and you should see the test message on your console! Pretty neat, huh? That’s it for the configs, at least for now. Let’s step into the preact app now! Getting started with Preact We already have all the configs that we need for running Preact on our webpack config on the rules key: ...\r\n      {\r\n        test: /\\.jsx?/i,\r\n        loader: 'babel-loader',\r\n        options: {\r\n          presets: [\r\n            'es2015'\r\n          ],\r\n          plugins: [\r\n            ['transform-react-jsx', { pragma: 'h' }]\r\n          ]\r\n        }\r\n      },\r\n   ... 1 2 3 4 5 6 7 8 9 10 11 12 13 14 . . . { test : / \\ . jsx ? / i , loader : 'babel-loader' , options : { presets : [ 'es2015' ] , plugins : [ [ 'transform-react-jsx' , { pragma : 'h' } ] ] } } , . . . You could move all the presets/plugins stuff into a babel . rc file if you want to, but im trying to keep things (and files) minimal here. With this config we can use ES6 and Preact components. If you are wondering about that 'h' after the keyword pragma , we will see that soon. Let’s write our first/main component under src / compontents / App . js : import { h, Component } from 'preact'\r\n\r\nexport default class App extends Component {\r\n  render () {\r\n    return (\r\n      <div>\r\n        Hi\r\n      </div>\r\n    )\r\n  }\r\n} 1 2 3 4 5 6 7 8 9 10 11 import { h , Component } from 'preact' export default class App extends Component { render ( ) { return ( < div > Hi < / div > ) } } And now let’s get that rendered on our page by updating the index . js file so it looks like this: import { h, render } from 'preact'\r\nimport App from './components/App'\r\n\r\ndocument.addEventListener('DOMContentLoaded', event => (\r\n render(<App />, document.getElementById('root'))\r\n)) 1 2 3 4 5 6 import { h , render } from 'preact' import App from './components/App' document . addEventListener ( 'DOMContentLoaded' , event = > ( render ( < App / > , document . getElementById ( 'root' ) ) ) ) Now if you reload the page you should see our App component saying hi! As mentioned earlier, you might have noticed that h here and there on the configs and the components. That’s a pretty common thing on Preact. You can read more about it here . Working on our App Now that everything is up and running, we can get down into some code. For this app I’ve just created a few components. Let’s go through each one: This is how App . js looks now: import { h, Component } from 'preact'\r\nimport SearchBar from './SearchBar'\r\nimport SearchesList from './SearchesList'\r\n\r\nexport default class App extends Component {\r\n  constructor () {\r\n    super()\r\n    this.state = {\r\n      searches: []\r\n    }\r\n    this.addSearch = this.addSearch.bind(this)\r\n  }\r\n\r\n  addSearch ({search, results}) {\r\n    this.setState((prevState) => {\r\n      const { searches } = prevState\r\n      searches.push({search, results})\r\n      return {searches: searches}\r\n    })\r\n  }\r\n\r\n  render (props, state) {\r\n    return (\r\n      <div>\r\n        <SearchBar\r\n          addSearch={this.addSearch}\r\n        />\r\n        <SearchesList\r\n          searches={state.searches}\r\n        />\r\n      </div>\r\n    )\r\n  }\r\n} 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 import { h , Component } from 'preact' import SearchBar from './SearchBar' import SearchesList from './SearchesList' export default class App extends Component { constructor ( ) { super ( ) this . state = { searches : [ ] } this . addSearch = this . addSearch . bind ( this ) } addSearch ( { search , results } ) { this . setState ( ( prevState ) = > { const { searches } = prevState searches . push ( { search , results } ) return { searches : searches } } ) } render ( props , state ) { return ( < div > < SearchBar addSearch = { this . addSearch } / > < SearchesList searches = { state . searches } / > < / div > ) } } If you know React, this should be pretty straightcoforward: we have a constructor function and we keep a searches collection on the app state. The addSearch method gets and object with a search and result keys as a parameter (thanks to destructuring) and we use the setState function to push the new search with its result to the new state. Finally our render it’s just a < div > with 2 components SearchBar which will use addSearch as a callback later and SearchesList which will display the searches on our state. Now, let’s create all the other components on the src folder. The file SearchBar . js should look like this: import { h, Component } from 'preact'\r\nimport request from 'superagent'\r\n\r\nexport default class SearchBar extends Component {\r\n  constructor () {\r\n    super()\r\n    this.doSearch = this.doSearch.bind(this)\r\n  }\r\n\r\n  doSearch () {\r\n    const { addSearch } = this.props\r\n    const { text } = this.state\r\n    const url = `http://api.tvmaze.com/search/shows?q=${text}`\r\n    request\r\n      .get(url)\r\n      .end((err, res) => {\r\n        if (!err) {\r\n          addSearch({\r\n            search: text,\r\n            results: res.body\r\n          })\r\n        } else {\r\n          console.error('Something is wrong with the API :(')\r\n        }\r\n      })\r\n  }\r\n\r\n  render (props, {text}) {\r\n    return (\r\n      <div>\r\n        <input value={text} onInput={this.linkState('text')} />\r\n        <button onClick={this.doSearch}>\r\n           Search\r\n        </button>\r\n      </div>\r\n    )\r\n  }\r\n} 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 import { h , Component } from 'preact' import request from 'superagent' export default class SearchBar extends Component { constructor ( ) { super ( ) this . doSearch = this . doSearch . bind ( this ) } doSearch ( ) { const { addSearch } = this . props const { text } = this . state const url = ` http : //api.tvmaze.com/search/shows?q=${text}` request . get ( url ) . end ( ( err , res ) = > { if ( ! err ) { addSearch ( { search : text , results : res . body } ) } else { console . error ( 'Something is wrong with the API :(' ) } } ) } render ( props , { text } ) { return ( < div > < input value = { text } onInput = { this . linkState ( 'text' ) } / > < button onClick = { this . doSearch } > Search < / button > < / div > ) } } This component is an text input with a button to perform searches. It’s in charge of dispatching an API call and setting the results on a search within a searches collection on the App state. For that, we have the doSearch method that just gets the addSearch function from the props (the one that we sent from the App component) and the text input from the state. Then, it performs an Ajax call to tvmaze’s API. After that, if everything was ok, we call addSearch with our text as the search key and the results . You can see that I’m using superagent to handle Ajax calls, mostly because I’m super lazy right now. Feel free to use whatever you want, but if you want to use super agent as well, just run yarn add superagent on your terminal. The render method has the input tag with the text from our state as a value and a really nice feature from Preact called linkState , which allows us to update the text key of our state on each change on our input element. And we have also a button that triggers the doSearch function when it’s clicked. Notice that we had to bind that function to the component’s this on the constructor function so it could access the state and props of the component. The file SearchesList . js is simpler: import { h, Component } from 'preact'\r\nimport SearchItem from './SearchItem'\r\n\r\nconst SearchesList = ({\r\n  searches\r\n}) => (\r\n  <div>\r\n    {\r\n      searches.map((search) =>\r\n        <SearchItem search={search} />)\r\n    }\r\n  </div>\r\n)\r\n\r\nexport default SearchesList 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import { h , Component } from 'preact' import SearchItem from './SearchItem' const SearchesList = ( { searches } ) = > ( < div > { searches . map ( ( search ) = > < SearchItem search = { search } / > ) } < / div > ) export default SearchesList Since it is just a presentational component, it doesn’t need state or lifecycle functions so we can just write it as a plain function that gets our searches from the state and maps them into a SearchItem . And SearchItem . js : import { h, Component } from 'preact'\r\n\r\nconst SearchItem = ({\r\n  search\r\n}) => (\r\n  <div>\r\n    <h1> {search.search} </h1>\r\n    <ul>\r\n      {\r\n        search.results.map(({show}) => (\r\n          <li>\r\n            <h5> {show.name} </h5>\r\n            <img src={show.image.medium} />\r\n            <div dangerouslySetInnerHTML={{__html: show.summary}} />\r\n          </li>\r\n        ))\r\n      }\r\n    </ul>\r\n  </div>\r\n)\r\n\r\nexport default SearchItem 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import { h , Component } from 'preact' const SearchItem = ( { search } ) = > ( < div > < h1 > { search . search } < / h1 > < ul > { search . results . map ( ( { show } ) = > ( < li > < h5 > { show . name } < / h5 > < img src = { show . image . medium } / > < div dangerouslySetInnerHTML = { { __html : show . summary } } / > < / li > ) ) } < / ul > < / div > ) export default SearchItem This is also a presentational component so it’s also a function. Here we render a < div > with a title that should be the searched text (that’s what serach . search contains, sorry about the confusing naming) and we map the results inside a list showing the show name , image and summary (we use dangerouslySetInnerHTML because it comes down with it’s own html tags). With all this new files and changes let’s go back to our localhost : 8080 and we should see our app up and running: https://blog.moove-it.com/wp-content/uploads/2017/02/Feb-19-2017-4-07-AM.webm Going offline We have our app working nicely despite not having any styling, but what happens if we want to remember the description of a show we looked up before on the subway and we don’t have any connection? That’s a shame we want our app to be available anywhere! That’s where service workers shine. Let’s get started making our app progressive. First thing about progressive web apps is that they use a manifest . json file where we store some data about the app, much like when you work on chrome extensions. Ours should be something like: {\r\n  \"name\": \"TVApp\",\r\n  \"short_name\": \"TVApp\",\r\n  \"start_url\": \"/\",\r\n  \"scope\": \"/\",\r\n  \"display\": \"standalone\",\r\n  \"background_color\": \"#2196F3\",\r\n  \"theme_color\": \"#2196F3\"\r\n} 1 2 3 4 5 6 7 8 9 { \"name\" : \"TVApp\" , \"short_name\" : \"TVApp\" , \"start_url\" : \"/\" , \"scope\" : \"/\" , \"display\" : \"standalone\" , \"background_color\" : \"#2196F3\" , \"theme_color\" : \"#2196F3\" } And that manifest should be located on the root of our app. You could set more things in here like icons and such… feel free to tweak it around! Next step, add the manifest on the < head > of our index . html like so: <head>\r\n    <title> TVApp </title>\r\n    <link rel=\"manifest\" href=\"/manifest.json\">\r\n  </head> 1 2 3 4 < head > < title > TVApp < / title > < link rel = \"manifest\" href = \"/manifest.json\" > < / head > Now we can add our service worker to cache our assets, let’s create a sw . js file also on the root of our app. var CACHE_NAME = 'tv-app-v1'\r\nvar urlsToCache = [\r\n  '/',\r\n  '/index.html',\r\n  '/static/bundle.js'\r\n]\r\n\r\nself.addEventListener('install', function (event) {\r\n  // Perform install steps\r\n  event.waitUntil(\r\n    caches.open(CACHE_NAME)\r\n      .then(function (cache) {\r\n        console.log('Opened cache')\r\n        return cache.addAll(urlsToCache)\r\n      })\r\n  )\r\n})\r\n\r\nself.addEventListener('fetch', function (event) {\r\n    event.respondWith(\r\n      caches.match(event.request)\r\n        .then(function (response) {\r\n          // Cache hit - return response\r\n          if (response) {\r\n            return response\r\n          }\r\n          return fetch(event.request)\r\n        }\r\n      )\r\n    )\r\n}) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 var CACHE_NAME = 'tv-app-v1' var urlsToCache = [ '/' , '/index.html' , '/static/bundle.js' ] self . addEventListener ( 'install' , function ( event ) { // Perform install steps event . waitUntil ( caches . open ( CACHE_NAME ) . then ( function ( cache ) { console . log ( 'Opened cache' ) return cache . addAll ( urlsToCache ) } ) ) } ) self . addEventListener ( 'fetch' , function ( event ) { event . respondWith ( caches . match ( event . request ) . then ( function ( response ) { // Cache hit - return response if ( response ) { return response } return fetch ( event . request ) } ) ) } ) There’s a lot going on here. Let’s split that into different parts: First we declare the cache name, that can be whatever you want. After that, we declare an array of files that we want to store in our service worker cache. Since we are using the webpack dev server, we cache the / static / bundle . js , but if you are in production you probably want to add / public / bundle . js here. Second, we open the cache and add all our url’s on the installation step of our service worker. Third, every time our app does a fetch , we check if our cache contains the thing that the app is trying to fetch. If it does, we return the cached content. Otherwise, we retrieve the data from the network. That’s it for now! Now we just need to install this service worker, in order to do that let’s add this on our index . html . <!DOCTYPE html>\r\n<html>\r\n  <head>\r\n    <title> TVApp </title>\r\n    <link rel=\"manifest\" href=\"/manifest.json\">\r\n  </head>\r\n  <body>\r\n    <div id=\"root\">\r\n    </div>\r\n    <script>\r\n    if('serviceWorker' in navigator) {\r\n      navigator.serviceWorker.register('/sw.js', { scope: '/' })\r\n        .then(function(registration) {\r\n              console.log('Service Worker Registered');\r\n        });\r\n      navigator.serviceWorker.ready.then(function(registration) {\r\n         console.log('Service Worker Ready');\r\n      });\r\n    }\r\n    </script>\r\n    <script src=\"/static/bundle.js\"></script>\r\n  </body>\r\n</html> 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 < ! DOCTYPE html > < html > < head > < title > TVApp < / title > < link rel = \"manifest\" href = \"/manifest.json\" > < / head > < body > < div id = \"root\" > < / div > <script> if ( 'serviceWorker' in navigator ) { navigator . serviceWorker . register ( '/sw.js' , { scope : '/' } ) . then ( function ( registration ) { console . log ( 'Service Worker Registered' ) ; } ) ; navigator . serviceWorker . ready . then ( function ( registration ) { console . log ( 'Service Worker Ready' ) ; } ) ; } </script> <script src = \"/static/bundle.js\" > </script> < / body > < / html > Here, we ask if the browser supports serviceWorker and if it does we grab our sw . js and we install it. Let’s take it for a ride! If you are using Chrome you can reload the page, then go to your dev tools on Application -> Service Workers and there you’ll see your service worker registered. Check that offline box and see how our app now even loads without connection. But we still have a problem: we can see our app assets and if we perform a search without connection it’s gonna fail because it will try to use the network. Fixing that up is not really hard. We just need to make some tweaks on 2 files. First on sw . js : self.addEventListener('fetch', function (event) {\r\n  var dataUrl = 'http://api.tvmaze.com/search/shows?q='\r\n  if (event.request.url.indexOf(dataUrl) > -1) {\r\n    event.respondWith(\r\n      caches.open(CACHE_NAME).then(function (cache) {\r\n        return fetch(event.request).then(function (response) {\r\n          cache.put(event.request.url, response.clone())\r\n          return response\r\n        })\r\n      })\r\n    )\r\n  }\r\n  else {\r\n    event.respondWith(\r\n      caches.match(event.request)\r\n        .then(function (response) {\r\n          if (response) {\r\n            return response\r\n          }\r\n          return fetch(event.request)\r\n        }\r\n      )\r\n    )\r\n  }\r\n}) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 self . addEventListener ( 'fetch' , function ( event ) { var dataUrl = 'http://api.tvmaze.com/search/shows?q=' if ( event . request . url . indexOf ( dataUrl ) > - 1 ) { event . respondWith ( caches . open ( CACHE_NAME ) . then ( function ( cache ) { return fetch ( event . request ) . then ( function ( response ) { cache . put ( event . request . url , response . clone ( ) ) return response } ) } ) ) } else { event . respondWith ( caches . match ( event . request ) . then ( function ( response ) { if ( response ) { return response } return fetch ( event . request ) } ) ) } } ) In our fetch event listener, we added an if, so if the request is for our API, we open the cache and then we store the result in there. Otherwise, we proceed as before returning assets from our cache or the network. Now we have our requests on our cache, let’s read them from there. For that, let’s go to our SearchBar . js on the doSearch method: doSearch () {\r\n    const { addSearch } = this.props\r\n    const { text } = this.state\r\n    const url = `http://api.tvmaze.com/search/shows?q=${text}`\r\n    if ('caches' in window) {\r\n      caches.match(url).then((res, err) => {\r\n        if(res) {\r\n          res.json().then((json) => {\r\n            addSearch({\r\n              search: text,\r\n              results: json\r\n            })\r\n          })\r\n        }\r\n      })\r\n    }\r\n    request\r\n      .get(url)\r\n      .end((err, res) => {\r\n        if (!err) {\r\n          addSearch({\r\n            search: text,\r\n            results: res.body\r\n          })\r\n        } else {\r\n          console.error('Something is wrong with the API :(')\r\n        }\r\n      })\r\n  } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 doSearch ( ) { const { addSearch } = this . props const { text } = this . state const url = ` http : //api.tvmaze.com/search/shows?q=${text}` if ( 'caches' in window ) { caches . match ( url ) . then ( ( res , err ) = > { if ( res ) { res . json ( ) . then ( ( json ) = > { addSearch ( { search : text , results : json } ) } ) } } ) } request . get ( url ) . end ( ( err , res ) = > { if ( ! err ) { addSearch ( { search : text , results : res . body } ) } else { console . error ( 'Something is wrong with the API :(' ) } } ) } Here, we aso added another piece: checking if we have the request on our cache. If we do, we add the search from there, otherwise we go to the network again. Let’s try our app again! So, let’s say we have internet connection so we search for Batman shows: But 5 hours later you are talking with your friend about the old Batman show while climbing a mountain and you don’t remember who was interpreting Batman on that show (how could you?). Now, you can check that on our app even without connection! (if you don’t have any mountain near you, you can try clicking the offline box on your chrome dev tools.) That’s it! You’ve got your first Progressive Preact app up and running using Webpack 2! Probably there are bugs and things to improve. Feel free to hack around the repo and send PRs! Here you can find the source code. Some final thoughts about Preact and Progressive web apps. At first, I wasn’t really sure about Preact (or any other React alternative that matters), but after using it a bit I found it really useful. Specially when you want the React goodies, but you can’t / shouldn’t afford the bandwidth to send React on your build, like working on widgets or landing pages. Also, it plays really nice with progressive web apps and I found that the community is great! And, for progressive web apps, I really thing that’s gonna play a major part on the future of web development. There are a lot of things to use on the ServiceWorker API, but it’s not standard yet. Keep an eye on this page to stay tuned.", "date": "2017-02-24"},
{"website": "Moove-It", "title": "railsconf-2017", "author": [" Andreas Fast "], "link": "https://blog.moove-it.com/railsconf-2017/", "abstract": "Earlier this year, we continued our sponsorship of RailsConf and this time in Phoenix, Arizona. Once our sponsorship was confirmed, we immediately went to work planning what we wanted to do: swag to bring, cool games to share. Of course, you never want to be short on company t-shirts and stickers! And it’s a good exercise as technologists to think about different games and topics that will separate you from the competition, to catch people’s eyes, or get the conversation going. Being a company with multinational offices, we came together in Phoenix from South America, Austin, and New York. Once at the hotel, we started preparing our things for the conference – including tracking down a package to one of the hotel warehouses! Day 1: Setting Up This is always one of the more exciting days: go early, register, explore the convention center and attend the keynote. After checking out a few talks, it was time to prepare our booth. A lot of design work had gone into preparing and we were proud of the results and layout: One of our main goals at the booth was to engage with people: talk about what they were doing and our own software development. We’re always looking for people who are interested in joining our team, as well as for people looking to hire our software consulting and development services. At day’s end, we were ready for the exhibit hall opening. We had our t-shirts, mugs, and stickers to give away, as well as two drone quadcopters as giveaway prizes. (Fun Fact: The head of security came to talk to us to make sure we weren’t going to fly the drone inside the building; an alarm would go off and everyone would have to be evacuated! But no, they were only there for display.) After setting up the booth we went back in time to watch the keynote of the day and head out to the night’s party. It was a packed bar called The Duce in Phoenix’s Warehouse District. We met a lot of fun people from across the US and Europe! Day 2: Trivia Time! To begin Day 2, at 10am the conference opened and a tsunami of developers washed over us. In a matter of minutes, hundreds of people descended upon the exhibit hall and we could barely finish a conversation with one person before the next was asking questions or wanting swag! After about 15 minutes, things calmed down and we were able to have more meaningful discussions with people. As a fun way to engage with other developers, we had brought trivia questions about Ruby and JavaScript. Our entire office got together a couple of months before the conference to collaborate, creating questions that challenged Ruby and JavaScript knowledge. Both languages have some unexpected behaviors under specific circumstances. Our 30 question/answer cards were quite the success! Attendees absolutely loved it and a lot of them came back during the quiet hours to sift through all of them and challenge each other to see who knew the answers. On this day we also met Aaron Patterson , one of the most notable developers in the Ruby Community. He’s part of the Ruby Core and the Rails Core, so he constantly works on the software that we use to run a lot of our clients’ applications. Day 3: Drones’ Giveaway On our last day of RailsConf, between restocking our swag and chatting with new people, we were giving away the drones. We had a long list of people who had applied to win. Around noon we drew names and our winners were Tori Brown (@vibro15) and Gaurish Sharma (@gaurish)! Here are the happy winners: We had one other big event left for the afternoon: Our sponsor presentation! As a Ruby on Rails Development Company and part of the Ruby community, Moove-it loves and embraces open-source. We’ve put out a number of libraries and are actively maintaining Sidekiq-Scheduler: A gem (library) that enables scheduling of recurring background jobs working as a plugin for Sidekiq (the most popular background job processing gem). We also took a picture with the author of the Sidekiq gem. Until next time! To close out the conference, our team went out to eat before going our separate ways: some back to Uruguay, others to Tennessee and Texas.", "date": "2017-12-01"},
{"website": "Moove-It", "title": "ruby-full-stack-sw-development-austin", "author": [" Hayley Parks "], "link": "https://blog.moove-it.com/ruby-full-stack-sw-development-austin/", "abstract": "As we near the second anniversary of our Austin office, we reflect on the founding of a little software company back in 2006. When we began, we wanted to have a focus on mobile applications. At that time, Apple iPhones and Android smartphones didn’t exist and the mobile industry was dominated by Nokia – chiefly the, now renowned, Indestructible . This meant that building applications for that phone required working with Java Mobile Edition (J2ME). And that’s where our name came from: The ‘move’ gave us a sense of mobile, moving – going forward. And the double ‘o?’ Well, Google was pretty big in tech, so we borrowed the double o’s. Now, after a decade in business – and a lot of questions about the origin of our name we finally ended up building Internet software products – and most of them mobile. From a small group of founders to our current team of over 70, we now offer full-stack mobile and web application development services. Our breadth of work covers a variety of languages and frameworks serving a range of industries, such as entertainment, healthcare, finance , education , real estate , and IoT. Our success spans the Americas and is due, in part, to our popularity among the Ruby enthusiasts. In 2016, Moove-it expanded its operations into Austin, Texas. The move positioned us to grow our foothold in the US market, as well as increasing proximity to our clients. We chose Austin, for our new office for a number of reasons. In addition to our numerous clients in central Texas, Austin is an alluring city, in regards to culture, community, nature , and climate. As well, it is quickly becoming one of the most important Startup and Tech Hub in the US – recently named best tech city in the world. After researching a variety of Austin office space options, we decided Capital Factory was a great place for us to start. From day one, it provided quality services and a central downtown location, all while being surrounded by likeminded of entrepreneurs and startups. The culture of Austin is also incredibly inviting; the community in Austin is ripe for companies and startups with a welcoming and helpful culture. Although the city is growing at a rate of over 150 new transplants per day, the community remains tight – and weird. As for our Austin developers, they come from a variety of backgrounds – and states – and we are hard at work cultivating our Austin team to reflect the principles of Moove-it and the culture of our new city.", "date": "2017-11-14"},
{"website": "Moove-It", "title": "product-scope-definition-workshop", "author": [" Gustavo Armagno "], "link": "https://blog.moove-it.com/product-scope-definition-workshop/", "abstract": "In today’s landscape, vague ideas are the starting point for multi-million dollar businesses. However, before being shaped into real products, ideas must cross a tangled jungle full of detours, dead-ends, and dangers of all kind. Only the most experienced guides, aimed with the right maps, tools, and knowledge, can lead the journey to success. To guide business in the right direction, we conduct Inception Workshops with our clients. During an Inception Workshop we: validate the proposed business model against its potential market, refine ideas, establish formal product descriptions, and create feasible software development project scopes. A recent Inception Workshop was with of one of our clients, YouScience. YouScience identified a new business opportunity for expanding their current service — namely, helping students select the right career path based on their interests and aptitudes — to a new audience. The innovation provides a new tool to assist high-school career counselors in helping students make smart decisions about their career options. The idea grew out of years of working with students, high-school administrations, college representatives, and employers from all over the United States. The information was compiled to form a full understanding of their needs and concerns, in order to create a product that address these issues. When YouScience came to us, they had already validated their business model, but required our technical expertise to execute the enterprise software product. Due to the nature of enterprise software development, each of our Inception Workshops is carefully planned, structured, and tailored to the needs and requirements of each project. YouScience’s Workshop included: defining the project scope, identifying the main critical risks and obstacles compromising the realization of the product, defining the architectural approach to integrate the new product with the existing one, and estimating the timeline and cost of overall effort. After defining the agenda, Moove-it’s YouScience team traveled to the YouScience office in Austin, Texas, to participate in the workshop. The Workshop was focused on analyzing and internalizing the product’s vision, goals, risks, roles and use cases, establishing a high-level definition of the system architecture, and beginning the process of defining the technical solution proposal. The agenda was organized into three phases, each focused on a particular part of the solution discovery process: 1. the product discovery, wherein high-level product concept and goals were discussed, risks analyzed, and the main personas, roles, and activities identified, 2. the requirements definition, wherein the main use cases were defined, and main flows of the current system analyzed, and 3. the system architecture definition, wherein the new system architecture, and the main functionality flows were established. The Workshop enabled both teams to come together, and focus on the new project. Through analyzing, discussing and internalizing the new requirements together, the entire team started out on the same page, ensuring long-term project success. This also enabled the development team and other roles to begin working on the new solution immediately, and with the full confidence of our client.", "date": "2017-10-07"},
{"website": "Moove-It", "title": "how-to-be-awesome", "author": [" Marianne Maisonneuve "], "link": "https://blog.moove-it.com/how-to-be-awesome/", "abstract": "DevSnack #46: All humans are different. However, we have some things in common. One of them is: “everyone wants to be awesome”. Here are some tips regarding several aspects of awesomeness. #1 – 7 Scientifically Proven Ways to Achieve Better Success in Life One of the things we consider part of being awesome is success. We want to be successful in life and we all trust science, so in this article by Christina Desmarais ( @ salubriousdish ),  7 scientifically proven tips to be successful are described. #2 – How To Be Successful In Life: 13 Tips From The World’s Most Successful People We all look up to people we consider awesome. In her article, Kara Heissman ( @ KaraHeissman ) shows 13 tips given by real awesome people who achieved success in what they do/did and are worth our admiration for it. #3 – The pursuit of excellence in programming Now, let’s go to what’s interesting to us: programming. Antonio Cangiano delights us with his insights on how to be awesome as a programmer. In his article, he describes aspects that are important to achieve excellence. #4 – Programming Achievements: How to Level Up as a Developer Jason Rudolph ( @ jasonrudolph ) provides some practical and concrete steps to become a “really good developer”. Even though he explains that goal is not measurable, he gives us some things we can do in several areas to be a step closer to it. #5 – 15 Characteristics of a Good Programmer Finally, it would we good to introduce you to 15 features that every awesome programmer should have. We have to remember that there aren’t magical recipes, the only thing we have are tips to be closer to achieving our goal of awesomeness. DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2016-06-24"},
{"website": "Moove-It", "title": "accessibility-on-the-web", "author": [" Santiago Estragó "], "link": "https://blog.moove-it.com/accessibility-on-the-web/", "abstract": "DevSnack #56: Ideally, web applications should be accessible to everyone. That means including people with disabilities. There are four main areas of disabilities to consider when building an application: visual, hearing, mobility and cognition. Front-end development has ways and tools to achieve this. There’s a standard on how to make web apps and content more accessible to people with disabilities, called ARIA (Accessible Rich Internet Applications). #1 – Accessibility: It’s not as hard as you think In this post, Chris Northwood ( @cnorthwood ) lists some common mistakes and how to fix them. He also mentions that developing for accessibility is not as hard as it seems, and establishes a quick checklist to know if your web app is accessible or not. #2 – The web accessibility basics There are aspects that change accessibility in many page components and parts, like images, forms, fields, tables, lists, headings and things like color contrasts. Marco Zehe ( @MarcoZehe ) explains ways of improving accessibility in each of these aspects. #3 – Accessible UI Components For The Web Addy Osmani ( @addyosmani ) wrote this interesting post about creating Accessible UI Components. First off, he presents some key conditions a components must have to be accessible and provides a few examples. Then he presents a lot of useful tools, to test and debug the accessibility of your visual components. #4 – The Modern Web is Broken for People with Disabilities In this post, Coleman Collins ( @ COLEMANICEWATER ), criticises the current state of the web and Single Page Applications. It shows some examples on why current web apps are not being accessible and gives some tips and resources on how to fix that. #5 – Getting Started With Angular And Accessibility Angular is one of the most used JavaScript frameworks, although so many others has gained popularity in the last few years. This is a quick guide wrote by Ben Teese ( @ benteese ) on how to take accessibility into account when developing with Angular. DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2016-09-02"},
{"website": "Moove-It", "title": "state-art-trello-extensions", "author": [" Agustín Pazos "], "link": "https://blog.moove-it.com/state-art-trello-extensions/", "abstract": "DevSnack #59: Trello is an excellent card-based project management application, simple and easy to use. I regularly use Trello not only at work but also in my personal projects. In this DevSnack, I’ll show you some interesting extensions to become a Trello master. #1 – Scrum for Trello Do you use Scrum? This extension allows you to use Trello in Scrum projects. It provides a board, facilities for estimation of tasks and Burndown charts. #2 – Trello Chrome Extension This Chrome plugin makes it faster for you to quickly access Trello boards and also to easily add new cards. #3 – Toggl integration This extension puts a timer in Trello and allows us to track our productivity in Toggl. Another option for time tracking, if you don’t have a Trello account, is Punchti . #4 – CardCounter for Trello Do you want to know how many cards are in a list? This extension shows the number of cards in every list in Trello. #5 – Trelabels for Trello This extension permit to customize the labels style on any Trello board. #6 – Trello Card Numbers If you want to see the ID of a Trello card easily, this extension is for you. It adds card number to Trello cards. Each board has a different sequence number. #7 – Ultimello Ultimello adds a lot searching features. Allow to sort the list by (Due Date, Title, Votes, Labels, Creation Date and Date of appearance on the list). #8  – Export for Trello Do you want an Excel report from Trello? This extension is for you! Export for Trello allows you to move a board into an Excel sheet in an easy way. #9  – Elegantt The Elegantt plugin automatically generates a Gantt charts from a Trello board. DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2016-09-23"},
{"website": "Moove-It", "title": "expanding-our-reality", "author": [" Lesly Acuña "], "link": "https://blog.moove-it.com/expanding-our-reality/", "abstract": "DevSnack #52: Earlier this year, Virtual Reality (VR) headsets and applications started to become more accessible to everyone with a modern smartphone. In the last month, the world was witness to the success of Pokemon Go and Augmented Reality (AR) is now on the spotlight. But what happened before this? What’s the difference between VR and AR? In this week’s 1 year anniversary DevSnack, we cover some aspects in order to understand more these technologies. #1 – The difference between Virtual Reality and Augmented Reality Before jumping into the rest of the links we need to know what are both AR and VR about. Eric Webster ( @ericwebster ) and Jonathan Ronzio ( @jonathanronzio ) discuss the differences between both technologies, mentioning the latest trends in each one. #2 – Virtual Reality Is the Most Powerful Medium of Our Time Molly Gottschalk ( @mollygottschalk ) narrates her experience using virtual reality and discusses how artists and designers are using this new reality as a medium. #3 – The technology behind Pokémon GO: Why augmented reality is the future In this really interesting post, Alexandra Tregre ( @alextregre ) goes through a brief story of augmented reality, and also comments on the possibilities this technology may have in the future. #4 – Augmented reality turns a sandbox into a geoscience lesson One of the huge possible applications of AR is as a learning resource. This post from EOS mentions details of a project on using AR as a platform for teaching geoscience. #5 – Medical products as a platform for augmented reality What can be done in the medical field in order to help doctors and even patients diagnose and even treat using augmented reality? In this post by Christoph Bichlmeier ( @ MedicalARBlog ) we are introduced to what can be done by including AR to already existing medical products. DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2016-08-05"},
{"website": "Moove-It", "title": "devsnack-61-mvvm-android", "author": [" Jeasmine Nahui "], "link": "https://blog.moove-it.com/devsnack-61-mvvm-android/", "abstract": "DevSnack #61: When building a new application, developers must consider which architecture to use. This decision will affect tests capabilities, modularity, extensibility and a bunch of other characteristics. In the past, there were not that many options for developers to choose from. There was a time that the only options were MVC or MVP. But now, with the introduction of Data Binding on the Google I/O 2015, a new approach for Android became available, the MVVM architecture. Let’s see how can we use this on Android #1 – Data Binding The main reason for using Data Binding will be to avoid boilerplate code that doesn’t require any brainpower. Yigit Boyar ( @yigitboyar ) and George Mount ( @ georgemount1 ) explain how to use Data Binding on Android projects. They also describe the way the code gets reduced, by setting the events handlers, data, and view logic on the layout, instead of the fragment or activity view. #2 – MVVM Approach In his post, Joe Birch ( @hitherejoe ) explains what MVVM is and all of its components. For making it easier for everyone, he shared an working example on Github where he uses Data Binding, so then we can learn how to use it with MVVM and why it makes MVVM possible. #3 – Using MVVM on a project In this link, Ross Hambrick ( @rosshambrick ) describes how to set up a project with Binding and ViewModel declarations, and bringing it all together. So, what are their limitations, and what is it still missing? Regarding the missing points, two-way binding functionality is currently available by including @={} instead of @{}. #4 – From MVP to MVVM Frode Nilsen outlines the main differences between MVP and MVVM. He also explains the reasons why it is not necessary to use certain frameworks like ButterKnife. Your code will be testable as long as you mock the Fragment using Robolectric and Mockito. It will not leak memory unless you start referencing ViewModels where you should not. Finally, you should not be using things that contain the line import android.content. If you are, you’re not supposed to. #5 – MVVM For testing Frank explains why this approach is perfect for testing. Because view model does not know which activity or fragment to use, it does not rely on them, so that we can test only the logic that matters. DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2016-10-07"},
{"website": "Moove-It", "title": "avoid-the-craziness", "author": [" Lucía Carozzi "], "link": "https://blog.moove-it.com/avoid-the-craziness/", "abstract": "DevSnack #50: Are you the kind of person that keeps a thousand open tabs in your browser? Do you use several browsers and find yourself lost between old posts that you, for some reason, don’t want to lose? Have you ever tried to bookmark something you already bookmarked months ago but you have still not read? I know by experience that this is a big discussed topic around developers, so let’s start avoiding the browser craziness and start fighting the tab hoarding (and gain in productivity) with this five posts! #1 – Make it stop In this post @ elmoonio explains us why it is better to moderate the amount of tabs we have opened in our browser. #2 – L ife savers In this post from @ProductHunt we are introduced to a lot of life-changing chrome extensions, starting with OneTab and through some non-work-related but useful extensions like Smart Tab Mute. #3 – OneTab Try it if you feel brave enough to say goodbye to ALL your tabs. Here @deffjunn shows that OneTab does not only clean your browser but also improve its performance. #4 – P ocket (N ot only a fancy bookmark app ) Is it often that you go back and read the articles you have populating your browser? You can save them and read them offline later. @ denharsh gives us his best offline productivity tip with Pocket app that I also use and recommend to everyone. (While I’m writing this post I’m also collecting A LOT of reading material in my Pocket account). #5 – Todoist If what you are looking for is to increase your productivity you can find this app useful. This post written by @ jbradleybush gives us 10 reasons to use Todoist app to get things done, specially if your life goes around a to-do list. DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2016-07-22"},
{"website": "Moove-It", "title": "transforming-information-into-an-experience", "author": [" Guzman Iglesias "], "link": "https://blog.moove-it.com/transforming-information-into-an-experience/", "abstract": "DevSnack #24: Data Visualization is the technique that transforms information into an experience. It is the modern equivalent of visual communication. This technique takes some raw information into a scene where analysis flows spontaneously and conclusions are seen explicitly. This concept is pretty old. However, powered by the Internet, the amount of information is growing faster than ever. Within this context, a perfect environment for the growth of beautiful artworks is being generated. #1 – We Feel Fine Multi awarded and talented Jonathan Harris ( @jjhnumber27 ) and Greg Hochmuth ( @grex ) created ‘We feel fine’ in 2006, an exploration of human emotion in six movements. The data is taken from people’s feelings. The project presents a collection of posts from weblogs that have the phrase ‘I feel’ or ‘I’m felling’.  This couple of artist-developers have created other similar projects since then that it worth a mention and nobody will regret if after visiting it . #2 – Music Time How popular was each music genre over the time? In this visualization, Google ( @google ) shows the evolution of the popularity of each music genre over the last 60 years. #3 – Close the Gap In this project, StudioMetric ( @studio_metric ) explores the gaps between women and men in three different social aspects: 1) Labour force participation; 2) Secondary education participation; and 3) Parliamentary participation. #4 – ‘Scrollytelling’: Pinellas Failure Factories Data Visualization is a powerful tool for journalism, and this work can prove it. This project, created by Tampa Bay Times ( @TB_Times ), tells a story about an specific educational situation on the Pinellas County (Florida, USA), with graphics that causes an effective impact on reader. #5 – Selfiecity Selfies are being an extraordinary social phenomenon and this project contributes with an valuable investigation work. Selfiecity ( @selfie_city ) investigates selfies using a mix of theoretic, artistic and quantitative methods. Then, represents their findings about the demographics of people taking selfies, their poses and expressions. Rich media visualizations assemble thousands of photos to reveal interesting patterns.", "date": "2016-01-22"},
{"website": "Moove-It", "title": "off-the-rails", "author": [" Ashley Lewis "], "link": "https://blog.moove-it.com/off-the-rails/", "abstract": "DevSnack #48: Rails is a wonderful tool, but day-to-day it can be tempting to lean on a lot of the magic it provides without really understanding it. But Ruby started out as, and still is, a powerful scripting language with a vibrant community. Having been designed from the ground up with “programmer happiness” in mind, digging into pure Ruby can reveal some delightful gems (pun absolutely intended). I have also found that when I pause to dive head first into a pure-Ruby project, I come back to Rails more grounded, with a clearer sense of the mechanics under the magic. Part of our evolution as a Ruby on Rails Development Company is learning to focus not only on using the best programming languages out there but also on sharing our knowledge with the community. #1 – 23 Years of Ruby with Matz (Yukihiro Matsumoto) It’s easy to forget that Ruby existed for over a decade before it was popularized by Rails. This fantastic interview with Matz himself by the guys at the Changelog ( @changelog ) goes deep into the history and influences of Ruby. #2 – Nokogiri as a command-line tool Web scraping, while messy, is still very much a viable data-collection tool. In this tutorial, Ruby developer Rob Miller ( @robmil ) shows how to leverage Nokogiri’s command-line interface and common Bash commands to scrape and parse data from a website without a public API. #3 – About the Ruby squiggly heredoc syntax One could do worse than follow the musings of Avdi Grimm ( @avdi ). His enthusiasm and curiosity for Ruby is infectious. In this tasty morsel, he explains the reasoning for a rather exotic bit of new syntax in Ruby 2.3. Like it or love it, it’s a trip. #4 – Blocks, Procs, and Enumerable Yes, back to the basics. So much of what distinguishes a Ruby-flavored method from its equivalents in other languages can be found in the Enumerable module. In this post from Joel Quenneville ( @joelquen ), take a deeper dive into what makes this module tick and makes all those yummy one-liners possible. #5 – Rails vs. Sinatra Time to spin up a new app? Before you tap out rails new yet again, consider the under-rated, bare-bones Ruby web framework, Sinatra. This compare/contrast piece by PJ Hagerty ( @ aspleenic ) walks through the process of getting started and tapping into an external API with each framework. If nothing else, Sinatra’s bare-bones approach can be a great training ground, and help to develop a deeper understanding of some of the tools we take for granted in Rails. DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2016-07-08"},
{"website": "Moove-It", "title": "sponsoring-ruby", "author": [" Ariel Luduena Karhs "], "link": "https://blog.moove-it.com/sponsoring-ruby/", "abstract": "Once more, Moove-it proudly sponsors two of the most relevant events of the Ruby community: KeepRubyWeird 2016 in Austin, Texas, and RubyConf 2016 in Cincinnati, Ohio. Early this year, Moove-it made a huge leap forward by opening its first development office in the US. This step was motivated by three factors. First, we wanted to give our clients and partners more opportunities to have Moove-it staff working onsite — a nice to have at certain periods of the development process. Second, this is part of our strategic plan to increase our presence in the US, giving new clients an opportunity to work with a talented team of professionals working in different areas, including web app development, mobile development, enterprise integrations and UX/UI design . Third, we strive to give back to the community behind the languages we use, including Ruby, what the community has given us. We truly care about Ruby Keeping his principle of reciprocity in mind while we grow as a Ruby on Rails Development Company helps us to plan, coordinate, and support activities and events involving Ruby and Rails. For instance, we are carrying out the third version of the Ruby Apprenticeship , a 3-month Ruby on Rails development apprenticeship. We have sponsored large international conferences, like RailsConf 2016 in Kansas City, or RubyConf 2015 in San Antonio, Texas. We have organized or sponsored meetups in the US and Latin America, related to Ruby and other areas of expertise. And, we are currently supporting, for the second year in a row, KeepRubyWeird next Friday, October 28, and the International Ruby conference (RubyConf) on November 10-12. Stay tuned During the current and following weeks, we will be giving more updates about these and other events. If you haven’t done it yet, follow us on Twitter ( @moove_it ) and stay tuned during KeepRubyWeird and RubyConf!", "date": "2016-10-24"},
{"website": "Moove-It", "title": "material-design-for-everyone", "author": [" Nicolas Gonzalez "], "link": "https://blog.moove-it.com/material-design-for-everyone/", "abstract": "DevSnack #27: in this week’s DevSnack we bring 5 interesting links on Material Design for Android. Learn about existing tools to apply Material Design principles to your app, how to style your app to engage your users and how to master some of the features of the new Design Support Library. Also, check out a few showcase apps and a must take Udacity’s free online course. Enjoy! #1 – Material Design for Developers: Getting Started This link should be the starting point of every Android Developer ( @AndroidDev ). Learn how to get started with Material Design. Also, how to apply the material theme and use system widgets to create consistent layouts, following material design guidelines. # 2 – Android Design Support Library Consequently to the release of Android Lollipop 5.0 and the introduction of Material Design , Google I/O 2015 presented a new support design library with the intention to provide developers a consistent way to include material design components, keeping backward compatibility down to Android 2.1. In this post Ian Lake ( @ianhlake ) showcases how to use the official Android Design Support Library to add Navigation Views, Floating Action Buttons, Snackbars, motions and scrolling techniques to our app. Furthermore, checkout Chris Bane’s ( @chrisbanes ) Cheesesquare app available on Github , which demos the usage of this support library. # 3 – Mastering the CoordinatorLayout In this post Saul Molinero ( @_saulmm ) teaches us how to master one of the core features of the Android Support Library: The CoordinatorLayout . Learn how to integrate different widgets such as AppBarLayout and CollapsingToolbarLayout to achieve material design patterns and scrolling techniques (parallax, quick return, etc) and also customise behaviour dependencies and interactions between siblings Views and/or its parents. # 4 – Plaid: Android Material Design Showcase App Check out this showcase app created by Nick Butcher ( @crafty ), an Android Developer Advocate at Google. It makes use of material design principles to create “tactile, bold and understandable UIs”. You can check out the code at Github and install the app from the play store (beta testing) # 5 – Udacity’s free online course: Material Design for Android Developers In this course created by Google in conjunction with Udacity, Nick Butcher ( @crafty ) and Roman Nurik ( @romannurik ) show us how to leverage your app in order to include some platform-specific design patterns to keep your users engaged and improve your UX. A must among Android Developers/Designers DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License . Read more about our Ruby Development Company .", "date": "2016-02-11"},
{"website": "Moove-It", "title": "accessibility-ios-apps", "author": [" Maximiliano Casal "], "link": "https://blog.moove-it.com/accessibility-ios-apps/", "abstract": "DevSnack #60: When building a new application sometimes developers must consider that it is going to be used by plenty of people, in different situations, some of them with special needs. iOS provides a fantastic opportunity to deliver apps with an excellent user experience for everybody. Let’s see how to use Accessibility in your iOS application! #1 – Accessibility for iPhone and iPad apps Matt Gemmell ( @mattgemmell ) explains that the first thing we should decide when adding accessibility in our application is which type of accessibility we want. He describes three types: Basic , Advanced and Assistive Hardware . Once we define which type we want, in the interface builder, we have to use three properties: accessibility label , accessibility hint , and one or more accessibility traits . With those three options, we will help the user to navigate through the app with an excellent user experience. #2 – What iOS Traits Actually Do? In this post, we will understand the main purpose of UIAccessibilityTraits , the different options we have and some tips. An Accessibility Trait allows us to choose the description for what an element in your application does. In some cases, a control may have more than one trait but we should be very careful to use combined Traits that are compatible. For example, we should not use Text Field and Label if it is a button that opens a web browser. #3 – How to check for accessibility labels In this link, Lukas explains what are the accessibility labels, why they make our application testable and some good practices to follow when setting them. When developing an iOS app we should follow some basic guidelines that will help us for our automation and testing process. #4 – How to use VoiceOver When using VoiceOver, there are different gestures to navigate through the elements shown on the screen. In this post, we will learn all the gestures to make it easier for visually impaired individuals to use our application in a better way. The most used gestures are Single-tap, Double-tap, use three fingers, Double-tap with three fingers and two fingers double tap. #5 – Tips for making your iOS app accessible Alister Scott ( @watirmelon ) explains some tips on making our iOS application accessible. The first is to enable form field tabbing; the main idea is that we should make easy for VoiceOver to move between fields and also making a return action for those elements that moves to a new form. Another tip is to make embedded UIWebView accessible. Once we have done this, VoiceOver will read the content of all the HTML elements in the same way it does on a web page in Safari. DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2016-09-30"},
{"website": "Moove-It", "title": "lets-talk-about-bad-habits", "author": [" Lesly Acuña "], "link": "https://blog.moove-it.com/lets-talk-about-bad-habits/", "abstract": "DevSnack #30: Developers strive to always deliver nice, clean code that works as expected. However, we are humans and there are bad habits we have that even the best of the good practices lists we read cannot take away. This week’s DevSnack compiles 5 links that provide some food for thought about our coding habits. # 1 – Am I really a developer or just a good googler? How many times have you searched for a solution in google, or used a code snippet from StackOverflow? It is argueably a bad habit, but it is one that many of us are guilty of. This blog post from Scott Hanselman ( @shanselman ) has some interesting reflections on that, and also provides some tips on how to start depending less on the internet and more on our own knowledge. # 2 – 9 bad programming habits we secretly love From yo-yo code to using the infamous goto statement, in this post Peter Wayner ( @peterwayner ) lists typical habits programmers have, offering a curious point of view on why some of these habits are useful if used under the right circumstances. # 3 – 7 bad habits of highly ineffective software engineers Habits do not only concern coding, attitude is a vital part of our lives. This post from John Mello ( @jpmello ), ennumerates seven poor attitudes from software engineers that lead to a bad working environment and a bad code quality. # 4 – Null Dilemma: The internet refuses to believe this man exists thanks to bad SQL coding Usually bad coding habits lead to bad code that has consequences on the end user. Mary-Ann Russon ( @concertina226 ) describes the case of Christopher Null ( @christophernull ), and how his “reserved” last name caused him problems due to bad programming decisions made on some sites he used. # 5 – The dangers of spaghetti code As we saw in the previous link, bad habits lead to real life consequences, and there are times those consequences are serious. This interesting post by Natali Vlatko ( @natalisucks ) uses Toyota’s unintended acceleration case from 2009-13 as an example of how spaghetti code can be more dangerous than we think it can be. DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2016-03-04"},
{"website": "Moove-It", "title": "personal-software-process", "author": [" Maite Mañana "], "link": "https://blog.moove-it.com/personal-software-process/", "abstract": "DevSnack #28: in this week’s DevSnack we bring 5 interesting links on Personal Software Process (PSP). Get a general overview on how the process was born by talking to its creator and take a pick at its fundamental basics. Hear some opinions from those who are not really keen on this type of processes and finally, if it caught your attention, take a look at PSP phase 2: TSP . Dig in! #1 – Personal Quality Management with the Personal Software Process If you’re feeling doubtful regarding the needs that Personal Software Process addresses, @AlanSKoch gives you a brief yet powerful introduction to the reasons why PSP may be exactly what you need. # 2 – Personal Software Process In this article, @gwareddm talks about the PSP from an agile-developer perspective and explains why – sometimes – the grass is greener on the other side. # 3 – An Interview with Watts Humphrey Because it’s always better to go straight to the source, @Grady_Booch interviews Watts Humphrey – PSP creator and walks us through the development of this process. # 4 – Introduction to the Personal Software Process Lois N. L. Akoto explains PSP in a nutshell, providing a general overview of Introduction to Personal Software Process – the PSP bible. # 5 – Implementing TSP process to build better software products Do you like to push things further? @anandsesh_s tells you how, by implementing TSP after PSP. DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2016-02-19"},
{"website": "Moove-It", "title": "announcing-the-mastering-ruby-program", "author": [" Ariel Luduena Karhs "], "link": "https://blog.moove-it.com/announcing-the-mastering-ruby-program/", "abstract": "After almost two years of successfully running our induction process and improving it, we asked ourselves: why don’t we design a training program, inspired by Moove-it’s Induction —as we call it—, aimed at developers who want to convert to Ruby? Which is what we did. Some of our new talents are experienced Rubyists. However, most of them are developers whose main experience lies in other programming languages. They have heard about Ruby, or have taken a Ruby tutorial, they’ve fallen in love with the language, and now want to make a career shift towards Ruby. During the first month, working at Moove-it, newcomers have to go through a challenging, mentor-based learning/training/leveling program. This is part of our regular induction process, regardless of the trainee’s previous experience. This Induction encompasses, not only exercises focused on a particular programming language and its ecosystem, but it also covers Agile, communication and management tools, good technical and consulting practices and introduces our trainees to Moove-it’s culture and values. It normally takes one month, more or less, working full time, to complete all the assignments. After finishing this period, some people are ready to jump straight into a client project and others need to split their time between more induction and working on a real project. During this phase, which can take between a month and a half and three months, both parties have the chance to evaluate each other and decide whether they want to keep working together, or not. So, we are delighted to announce the launch of Mastering Ruby , a Ruby on Rails learning program based on Moove-it’s Induction , open to skilled developers, who want to kick-start or change their careers. Mastering Ruby is totally free and is initially intended for developers living in Austin and its surroundings, or who are planning to move to Austin. Applicants are expected to showcase solid programming foundations, in terms of key principles, practices and skills. No prior knowledge of any particular language or framework is expected or required. Towards the end of the second month of the program, mentees will get the chance to land their next job at Moove-it, should they share Moove-it’s core values and have excelled in the completion of Mastering Ruby’s milestones. This first edition starts on April 4, 2016 and will be held on site at Moove-it’s Austin Office. The call for participants ends on March 30, 2016. We have a special perk for the best programmer to successfully complete the first month of the program: a free pass to RailsConf 2016 in Kansas City, Missouri, on May 4 (flight tickets included). Further information, including instructions on how to apply, can be found at masteringruby.com.", "date": "2016-03-22"},
{"website": "Moove-It", "title": "about-content-management-systems", "author": [" Juan Andrés Zeni "], "link": "https://blog.moove-it.com/about-content-management-systems/", "abstract": "DevSnack #37: Deciding whether to use a platform for managing the content of a website is a decisive choice to make at the beginning of a web project. We should consider time, budget and client flexibility around requirements. Today’s DevSnack will explore when it’s convenient to use a Content Management System (CMS), and will present some options available in the market. # 1 – When use a CMS? In this article, @ joshuakrohn describes those situations where it’s convenient to use a CMS based on the specifics of the project. For blogs, sites of news or e-commerce portals, given that the content needs to be updated frequently, a CMS is useful. Not using a CMS probably means that the user should edit the source code each time he want to publish new content. # 2 – Why use a CMS? In this post, @ Jeff_A_Kline lists some of the benefits of using a CMS in a web project. He highlights that the content is an independent layer to the design, so when a redesign is needed the work is much easier. He also points up that the access to most up-to-date features on the market usually are just one update away. # 3 – How CMS works This link is a great resource for understanding how Content Management Sytems works. It briefly explains how the actors (users) participate in the updating process, and highlight how the webmaster is not involved. #4 – Scrivito, a CMS for Ruby on Rails Projects The market is full of options, but none of them are based on Ruby on Rails. Scrivito is the first-ever 100% Ruby on Rails CMS solution to not only meet but surpass the feature sets of WordPress or Drupal. Lack of features is consistently among the top reasons given by Ruby on Rails developers for choosing WordPress over a Ruby CMS. By delivering WYSIWYG editing, large asset management support, plus all the benefits of the cloud, Scrivito is simply the best Ruby CMS on the market today. # 5 – Thinking on coding you own CMS? In this article @ danielthrelfall analyzes the cost of creating your own CMS. The author also evaluates the possibility of modifying an existent CMS and customizing it according to the needs of the project, in order to optimize the development time and the quality of the the product. DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2016-04-22"},
{"website": "Moove-It", "title": "our-second-experience-in-a-rails-rumble-2", "author": [" Marianne Maisonneuve "], "link": "https://blog.moove-it.com/our-second-experience-in-a-rails-rumble-2/", "abstract": "It all began when a peer at Moove-it asked me if I would be keen to participate in the Rails Rumble. The title rang a bell but I was not familiarized with most of the competition’s specifics. However, I was eager to join a programming contest, so, without a shadow of a doubt, I said yes. What is a Rails Rumble? Rails Rumble is a two-day programming competition where participants (generally teams comprised of anything in between 1 to 4 members) from all of the world are to deliver a compelling and catchy app. The two paramount premises are: it has to be developed leveraging a Rack-based framework, and it is expected to be implemented in Ruby. Once the 48-hours effort is over, jury is to vote for the app that impressed them the most. Finally, the ballot’s results are to be published in the form of a web ranking. The process A couple of weeks before the competition kicked off, we organized the team and debated the available options, dividing them into areas of interest. We evaluated the pros and cons of each one. We identified diverse target markets, e.g. Health, Society, Software Development, Work Productivity, etc. At the beginning some of us would lean toward Health, nevertheless, in order to deliver something useful and appealing in the Health sector (as any other humongous and complex industry), one should be truly immersed into the subject in question. None of our teammates joining this year’s edition really owned such deep grasp of the Health context, though; therefore we rapidly discarded that idea. Kick-off meeting discussing alternatives We also thought about giving our project a social purpose by putting together something that could ultimately support the needs of the general public. Even though most of us were very enthusiastic about this approach, figuring out something meaningful to us in such short time was kind of unrealistic. Another approach was to build something in line with the methodology and the processes that we promote here at Moove-it. Perhaps something in line with what was conceived in the ’14 Rails Rumble: Kaigi, a web app aimed to streamline remote retrospectives’ execution. Kaigi is indeed popular within Moove-it, since part of our team is scattered across Argentina. Moreover, the great majority of our customer base resides in the US. With our eyes set on improving our process, the experience and the communication we keep with our clients, we had originally decided to implement an app for tracking our daily meetings. It mainly consisted of a Hangouts and GoToMeeting integration app, where we could write e.g. action items, sidebars; very much every aspects tied to the event itself, for every organizer and attendees to have. This one was our best shot for a while, nevertheless, it didn’t last long… A sudden change in plans A few days afterwards we had a follow-up meetup to finalize the idea and turn it into a tangible project. However, new views were thrown out to the table, making us realized we were not as enthusiastic as we were before, which then led us to scrap out our original idea and take a whole new direction. We were now all seeking for something, not only innovative and challenging, but also something that thrills us, something we could fall in love with and made us feel proud of. We decided then to lean towards Entertainment. That was indeed the choice that suited us best, as we got rapidly flooded with clever ideas coming from everywhere; “Let’s do an arcade game…”, “It would be cool to bring zombies in…”, “Yes, a runner…”. At the end of the meetup, we had entirely flipped the subject and agreed to build a Ruby-on-Rails “Zombie Runner” arcade, entirely operated from our mobile phones. The preparation It was now time to dive into the nitty gritty of the product we were about to build. We held a kick-off meeting with the UX/UI designer, so that we could shape the game’s workflow and overall behavior. We all reached to a common ground on the game’s modality: it will need to support single and multi-player. Besides this, the whole scope still remained to be nebulous. By the time the kick-off was over, we had finalised the views, UI components and a draft version of the game’s interaction design. Moreover, we had reached an agreement on where and how buttons and transitions were intended to behave. In the end, a rough whiteboard diagram was our only asset, serving as our single input for our later identification and organization of user stories and tasks. With only one week to go, we held a new gathering aimed to ensure everything was clear and ready for us to only stick to coding during those two intense days. We agreed to have two of us doing the backend, and the other two, the frontend piece. We also shared our views about the technologies to be used, how we were going to make the mobile phone behave as a joystick, and get everything seamlessly integrated with our Ruby-on-Rails app. And the day finally came in… I guess it was a mix of anxiety, enthusiasm and, at the same time, profound commitment towards this cause, what I could felt by only staring at each others that day. We were thrilled to be working together on something we truly liked. When you only have two days to achieve such an ambitious goal of building from the ground an app, you need to be, more than ever, self-directed and proactive. Besides, you should pick your tools and process right at the very start. An over-structured approach may lead to unnecessary overhead on coordination and planning, making your entire project to fail. On the other hand, lack of a framework and consistent practices may also wind up in a wreck. The team in action As a consequence, we decided to follow a Kanban-ish approach as a minimum viable process. Basically, we set up a whiteboard and divided it in two columns where we would place sticky notes. Left-side column for the TODO tasks, and the right-side area for the DONE ones. Since the four of us were physically based in the same room, we perfectly knew who was doing what, so we didn’t really need a IN PROGRESS column. We strived to set achievable short-term objectives to keep us on track and make us feel as we were progressing at a good and steady pace. That helped us keep team’s morale where it had to. Every now or then, our peers from Moove-it showed up to cheer us up -e.g. delivering pizza, burgers and other treats- making us feel extremely supported and comfortable; an instrumental factor in keeping us engaged and motivated. These were two intense days of hard work and all-nighters. The first night, two of us did not get any sleep at all, while the two others passed out for just an hour. However, knowing that the whole team was behind us really payed off for all that lack of sleep. As we got closer to an actual usable version, things suddenly become more and more exciting and fun. The first few test-runs were extremely encouraging. In fact, everyone who stopped by for a quick visit ended up playing with it along with bringing up suggestions. The Grand Finale We wrapped up 10 minutes before the official deadline. We rapidly delivered a fully-functional game which inevitably has lots of stuff to be polished, but, at the same time, looks good and entertains its audience. We faced several issues closed to the end, though. Three hours to run and we still did not have the multiplayer modality ready. Essentially, we could not get to come up with a proper way to create the routes in order to sync the players. Anyways, in the end we were able to pull it off and proudly deployed, at last! We also learned a lot. Understanding the key role of aesthetics in the Entertainment world was new for some of us. Furthermore, we got to explore the Pixi.js library (a 2D webGL renderer) to generate animations. I was even able to get a deeper understanding on how websockets work. Even though we knew it entailed an ambitious endeavor, when completed, we felt very happy and proud to behold the result. We were able to meet all of our set goals in less than 48 hours, because of hard work and, most importantly, teamwork, discipline and commitment. Overall, it was a great experience which I would certainly repeat. Testimonials This was my second time on the Rails Rumble. You know… when they say ‘Second parts were never as good as first ones’, well that’s not true in this case. I had a blast! On the first, I was more concerned about finishing the product than having fun and enjoying the ride. Also, the team decided to build a product a lot more compelling and more enjoyable than the first one, IMO. So if the question is: will you do it again? I’ll most definitely will. — Adrian Gomez, participant. It was a great experience where we were able to work under pressure to release a great product. Given the nature of our app—a game—, it was even more enjoyable to develop and test. I will definitely do it again. — José María Aguerre, participant. My experience in the Rails Rumble was very rewarding from all points of views. Sometimes, it is difficult to assemble a team to diligently work for a common goal, but in this case we could. Communication was key, due to the fact that we didn’t have much time and it also required a full time dedication. As a consequence, commitment was essential. To conclude, I can stress that it was indeed an enriching experience I would like to be part of again — Maximiliano Arcia, participant. Play the game! Zombie Runner is available online . I encourage you to write down your comments, ideas and questions. Snapshot of Zombie Runner", "date": "2016-02-24"},
{"website": "Moove-It", "title": "open-source-slack-integrations-for-agile-teams", "author": [" Juan Manuel Pereira "], "link": "https://blog.moove-it.com/open-source-slack-integrations-for-agile-teams/", "abstract": "DevSnack #38: At Moove-it we love agile methodologies, Slack and open source code. So, like Captain Planet and the Planeteers, combining all three we present you: Must-have open source Slack integrations for agile teams ! #1 – MorgenBot “Can you hear me? Nope, I can’t”, “I can’t see you!”, “I think your microphone is not working”. These are some of the things you may hear when having a daily meeting with a remote team. Stop wasting time and use this cool integration that handles the daily meeting for you. #2 – SlackSlime “ United we stand divided we fall “. Who is not hyped about the upcoming Captain America: Civil War movie? If Tony Stark has to choose which tool to use to communicate, I’m pretty sure about the answer: SlackSlime . This integration allows you to communicate to cross-teams or organizations easily. So my support goes to #TeamIronMan and SlackSlime! #3 – GitSlack Looking for a quick code review? There’s no need to tell your colleagues to do it, just add this integration and bend them to their knees with notifications. #4 – Slack timezone converter Distributed team around the world? Meeting issues and wasting time due to timezones? Now there is an easy way to remind everybody when they need to be ready. #5 – TeamBot Having a channel with a client sometimes can be tricky; you don’t want to bother them every time you need to notify your team. Use TeamBot to notify only those you want with the @team annotation! Bonus : An agile team also relies on motivation. With the hi5bot give and receive high-fives whenever you need to celebrate! https://github.com/hlian/hi5bot DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2016-04-29"},
{"website": "Moove-It", "title": "vagrant-multiple-projects", "author": [" Gustavo Armagno "], "link": "https://blog.moove-it.com/vagrant-multiple-projects/", "abstract": "This post is intended for automation lovers, who want to automate the setup of the development environment in microservice-based projects. One option is to use Vagrant. If you are setting up a project based on microservices (or any project organized into a few distributed, self-contained components) you will probably end up hosting them in separate Git repositories. This is to allow for division of work, keep an organized workflow, prevent bottlenecks, etc. Some of these components may share the same development environment configuration and may even be designed to run on the same machine. A quick digression We have different approaches to automating the development environment configuration for our dev team. One option is to use a custom bootstrap script to setup our machine. This option has some drawbacks, which include constraining the variety and types of workstations our developers can use and making it difficult to select and install specific versions of our project’s dependencies, due to project restrictions — e.g. when the project requires PostgreSQL 9.2, but we have 9.4 installed on our machine. A second option is to use Vagrant. Vagrant allows us to create and configure guest virtual machines (VMs) easily on our developer workstations, using a single description file. This file is called Vagrantfile and is normally located in the project’s base directory. Using this file, we can tell Vagrant to choose a variety of VM providers (e.g. VirtualBox or AWS), to define a list of tasks to be run once the VM has been provisioned, to configure the network, and so on. Then, Vagrant will quickly spin up a VM containing our development environment and keep it in sync with our source code in the host OS. Changing the source code in the host OS will automatically reflect in the VM, and vice versa. One of the benefits of this mechanism is that we can edit the source code with our preferred IDE installed in the host OS (e.g. OS X) and run/debug/test the application in the VM (e.g. an Ubuntu Server 12.04, including a particular set of dependencies). Team members using Vagrant can reproduce exactly the same configuration on their machines. A third option is Docker. Docker is a more efficient alternative to using VMs. In a nutshell, Docker’s service layer runs over the host OS and creates isolated containers that include the app under development and all of its dependencies. VMs, on the other hand, don’t share the same OS instance. For example, two apps placed in two separate Docker containers will run on the same host OS. Conversely, two apps placed in two separate VMs will run on two different OS instances — and will therefore consume more resources. Problem To define the scope of the problem without losing generality, our goal is to automate the development environment’s installation and configuration for a project that has the following constraints: It is organized in many Git repositories. Each repository contains the source code of an individual component (e.g. a service, an app, the project’s core). Components run independently from each other. Components can communicate between each other (e.g. through RESTful endpoints). Components (may) share the same development environment. Components (may) run on the same (virtual) machine. The environment setup should be easily portable. The solution should encompass at least Linux or Mac workstations. The solution shouldn’t constrain any development, communication or management tool that developers are used to. Solution Our solution involves using Vagrant and Git subtrees to reference and checkout the external components from a single repo. We are not using Docker-native on the host machine, this time, because we want our solution to be as generic as possible: Docker would narrow our solution down to only using Linux machines and we want to be nice to OS X devs. If we still want to use Docker, Vagrant comes with a Docker provisioner out of the box that fits in with our solution (see Vagrant’s Docker provisioner ). Let’s call this single repo dev - env . dev - env will contain the Vagrantfile with all the development environment configuration, a Readme file, including any manual setup required after provisioning the VM, and the subtrees. Git subtrees are great because they allow us to work on our external repositories, right from our super project — dev - env , in our example — with relative ease. You only have to learn a new merge strategy — i.e. subtree — and be a little disciplined when pulling and pushing. To simplify the explanation, let’s suppose we have three different repos, service - auth , service - notifier and core , each one containing the components we want to configure. As a first step, we will create the repo and initialize it, using Git: Shell $ mkdir dev-env\r\n$ git init 1 2 $ mkdir dev - env $ git init Then, we are going to add a Vagrantfile and a Readme file to dev - env ‘s base directory. (The Readme is not strictly required, but it is recommended that we devote some time to writing one.) Assuming that our components will listen to different TCP ports, our Vagrantfile would end up looking like this: Ruby VAGRANTFILE_API_VERSION = \"2\"\r\n\r\nVagrant.configure(2) do |config|\r\nconfig.vm.box = \"ubuntu/trusty64\"\r\nconfig.vm.network :forwarded_port, host: 8080, guest: 8080 # core\r\nconfig.vm.network :forwarded_port, host: 8081, guest: 8081 # service-auth\r\nconfig.vm.network :forwarded_port, host: 8181, guest: 8181 # service-notifier\r\nend 1 2 3 4 5 6 7 8 VAGRANTFILE_API_VERSION = \"2\" Vagrant . configure ( 2 ) do | config | config . vm . box = \"ubuntu/trusty64\" config . vm . network : forwarded_port , host : 8080 , guest : 8080 # core config . vm . network : forwarded_port , host : 8081 , guest : 8081 # service-auth config . vm . network : forwarded_port , host : 8181 , guest : 8181 # service-notifier end At this point, we are ready to set up the subtrees. The first step is to add one Git remote per external component. This is a trick that will allow us to refer to the subtrees in a shorter form: Shell $ git remote add core git@github.com:moove-it/core.git\r\n$ git remote add service-auth git@github.com:moove-it/service-auth.git\r\n$ git remote add service-notifier git@github.com:moove-it/service-notifier.git 1 2 3 $ git remote add core git @ github .com : moove - it / core .git $ git remote add service - auth git @ github .com : moove - it / service - auth .git $ git remote add service - notifier git @ github .com : moove - it / service - notifier .git In the dev - env base directory, we will add the subtrees in their respective prefix folders, referring to the remotes we have just created: Shell $ git subtree add --prefix core core master --squash\r\n$ git subtree add --prefix service-auth service-auth master --squash\r\n$ git subtree add --prefix service-notifier service-notifier --squash 1 2 3 $ git subtree add -- prefix core core master -- squash $ git subtree add -- prefix service - auth service - auth master -- squash $ git subtree add -- prefix service - notifier service - notifier -- squash (We should only have to do this once, or just each time we want to include a new external component in our setup, which won’t happen very often. The general command to add a new subtree is git subtree add -- prefix [ prefix_folder ] [ remote_name ] [ branch ] -- squash .) Note that we are using the -- squash modifier. This is to avoid storing the entire history of the components’ repos in dev - env . After adding the subtrees, our commit log will look something like this: Shell 9298f0a Merge commit 'b376ce343d7ebfa47811e1fb98ced870ce346a4c' as 'service-notifier'\r\nb376ce3 Squashed 'service-notifier/' content from commit 3d51460\r\na5a0407 Merge commit '56c06a59c1fc79d1a60a6712f467517c22fbdc05' as 'service-auth'\r\n56c06a5 Squashed 'service-auth/' content from commit 1a7d7a3\r\n1ce0b38 Merge commit '049d2492ae3fe15ec65d3663745e758638746414' as 'core'\r\n049d249 Squashed 'core/' content from commit f51b5ba 1 2 3 4 5 6 9298f0a Merge commit 'b376ce343d7ebfa47811e1fb98ced870ce346a4c' as 'service-notifier' b376ce3 Squashed 'service-notifier/' content from commit 3d51460 a5a0407 Merge commit '56c06a59c1fc79d1a60a6712f467517c22fbdc05' as 'service-auth' 56c06a5 Squashed 'service-auth/' content from commit 1a7d7a3 1ce0b38 Merge commit '049d2492ae3fe15ec65d3663745e758638746414' as 'core' 049d249 Squashed 'core/' content from commit f51b5ba Through the git subtree command, we can keep our individual components up to date and push changes for review. We will then use git subtree pull to update the components. For example, to update core ‘s master branch: Shell $ git fetch core master\r\n$ git subtree pull --prefix core core master --squash 1 2 $ git fetch core master $ git subtree pull -- prefix core core master -- squash If we are working on a feature branch, we will then use git subtree push to push our feature branch: Shell $ git subtree push --prefix=core core feature 1 $ git subtree push -- prefix = core core feature Finally, we can push dev - env to our remote repo. Discussion One of the benefits of this solution is that the team maintaining the original component’s repo doesn’t necessarily have to be aware of the existence of dev - env and all the infrastructure we have created to automate the development environment setup. This solution, to set up a common development environment for autonomous components living in separate repositories, may or may not make sense, depending on your organization’s policies and/or the project’s complexity. Scenarios that may benefit from this approach include: projects that are difficult to set up, requiring specific dependency versions, overwriting certain libraries or copying certain files to specific folders; or, projects that require distributing the development environment on separate machines. Some developers may benefit from using the same Vagrant setup across a lot of projects (for example, WordPress sites), having a single custom base box, so they can update them from a single place. You will be able to tell if this solution suits your needs. This blog post follows a discussion I started on Stack Overflow . There, you will find alternative solutions that may be a better fit for your project. Conclusion Vagrant is an excellent solution for automating your project’s development environment setup. When the project’s architecture is organized in several components, living in separate repos, and you need to run them in the same guest machine, while still keeping the ability to edit/debug/test/interact with them from your host machine, you will want to avoid adding your Vagrant setup to each project repo. Instead, you need to find a way to end up with a single Vagrant folder pointing to the sub-projects’ folders. In addition, you will want a solution that keeps the folders in the guest machine synchronized with the ones in the host machine. The approach outlined in this article describes creating a Git repo, containing a single Vagrantfile and Git subtrees, each one linked to an external project. Once the VM has been provisioned and loaded, the external projects will be placed in Vagrant’s default directory in the VM, where they can run in the proper environment. Featured image source .", "date": "2016-04-14"},
{"website": "Moove-It", "title": "building-modern-web-apps-with-jhipster", "author": [" Agustín Pazos "], "link": "https://blog.moove-it.com/building-modern-web-apps-with-jhipster/", "abstract": "DevSnack #35: JHipster is a Yeoman generator, used to create a Spring Boot + AngularJS project. A few days ago, the Java Hipster team released the version 3.0 adding support for project generation with a complete Microservices architecture. Let’s review JHipster through the following links! # 1 – JHipster introduction A great introduction to the Java Hipster project by its lead developer @juliendubois. # 2 – Get Hip with JHipster: Spring Boot + AngularJS + Bootstrap @mraible talking about how JHipster integrates popular frameworks like Spring Boot, Bootstrap and Angular to build modern web applications. # 3 – JHipster video tutorial This 20-minute tutorial created by @mraible shows how to quickly build a blog web app with a JHipster application. # 4 – JHipster 3.0 Microservices architecture This post by @java_hipster describes the components of the JHipster 3.0 Microservices architecture. # 5 – JHipster first book Finally, for a more complete reference, I strongly suggest reading the first and unique free book about Java Hipster. Hope you enjoy it! DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2016-04-08"},
{"website": "Moove-It", "title": "lets-talk-microservices", "author": [" Adrian Gomez "], "link": "https://blog.moove-it.com/lets-talk-microservices/", "abstract": "DevSnack #29: in this week’s DevSnack we bring 5 interesting links on microservices architecture. Some dos and don’ts, tips and more that might help you decide if microservices is the right architecture for your app. # 1 – Five considerations for delivering microservices 5 questions you should ask yourself in order to find out if microservices is good for your application by  Anna Gerber ( @annagerber ). # 2 – Get Small To Get Big Through Microservices Matt Miller ( @mcmiller00 ) brings some tips about microservices extracted from the Sequoia’s Microservices Summit. # 3 – Architecting Rails Apps as Microservices In this post Leigh Halliday ( @leighchalliday ) gives a high level example for building microservices on top of rails. # 4 – The Seven Deadly Sins of Microservices (Redux) Daniel Bryant ( @danielbryantuk ) talks about 7 common anti-patterns when implementing microservices. # 5 – Adopting Microservices at Netflix: Lessons for Team and Process Design To close lets get some insights from applying microservices on Netlflix byTony Mauro. DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2016-02-26"},
{"website": "Moove-It", "title": "railsconf-excellence-through-diversity", "author": [" Andreas Fast "], "link": "https://blog.moove-it.com/railsconf-excellence-through-diversity/", "abstract": "On May 2nd I started my journey to RailsConf. It was the first time for many things, but not for everything. It wasn’t my first trip to the US, but it was my first time in Kansas City. It wasn’t my first conference nor my first talk, but it was my first RailsConf as a speaker and attendee. It took me 4 planes, 2 countries, 3 states and 27 hours, with a lot of waiting, to get there. As you can imagine I was quite tired. But it was going to be my first speaker dinner, so nothing was going to stop me from going. I got there a bit late so I didn’t get to meet a lot of people, but we had a great time with @joeddean and @misbehavens . Getting sneak peaks into our talks and trading life experience. The next day was the day. First day of RailsConf and my talk was up in the afternoon. I had prepared a lot. Presenting to my co-workers many times. Every time I was extremely nervous, even more than I had been for talks on previous conferences. Clearly I cared about this one even more. During the day, leading up to my presentation, I cycled through moments of peace and quietness to a lot of nervousness, when even my pulse would get faster. As usual, when it finally started, I forgot everything else and completely focused on presenting. I knew it by heart and it’s something I care a lot about, and it’s often easier to talk about these. Globalization has brought different cultures and ethnics closer than ever before. Specially in programming we see many different people interacting with each other. It happens time and again that certain ways of doing things, or certain behaviors, or certain skills, or interactions are praised over others. It often happens that clients or Product Owners get the impression that certain people “perform better” than others and start asking that everyone be the same. It has been proved time and again that homogeneity is a bad recipe for a great team. A diverse team has much more potential to outperform. Unity makes strong teams, but unity isn’t homogeneity. It’s time for us to cherish our differences and leverage them to build great teams that deliver great products and don’t self-destruct in the process.", "date": "2016-06-07"},
{"website": "Moove-It", "title": "going-from-mvc-to-mvp-on-ios", "author": [" Maximiliano Casal "], "link": "https://blog.moove-it.com/going-from-mvc-to-mvp-on-ios/", "abstract": "Every time we start a fresh project, we are compelled to choose from many patterns, for us to be able to ultimately architect our solution in a way that it primarily stays maintainable and scalable. However, we may feel stunned because of the number of options and variations. By the time are done with this post, you will hopefully have gained the necessary knowledge for you to drill even further to implement two of the most common (and known) patterns: Model View Controller -MVC- and Model View Presenter -MVP-. To learn the key concepts, I decided to include a Pokedex starter project which implements the MVC pattern. The idea is that you make your own edits to the project and, by the end of the tutorial, you will have the same Pokedex –but following a MVP approach–, while gaining some experience on how to migrate MVC apps to MVP! I will first walk you through a quick overview of MVC and MVP, and then compare them against each other. MVC & MVP Architecture MVC defines three roles: the controller, the view and the model; however, this pattern not only describes the roles, it also outlines the interactions between the components. The Controller is only a mediator between the View and the Model, preventing them from “knowing” each other. If you are looking for a more exhaustive explanation, you may want to take a look at Apple’s MVC definition . In theory, it looks very straightforward, but in reality, it does not. The number of Controllers tend to grow significantly, when you suddenly realise that your script counts more than 1,000 lines, or even more! That’s why you may hear people referring to MVC as the “Massive View Controller”. This title results from their tight dependency with the View’s life cycle, making it hard to treat them as separate entities. In the end, the View Controller turns out to be responsible for everything. On the other hand we have the MVP pattern that is an “evolution” of MVC. In here we also deal with three components: the Presenter (UIKit independent mediator), the Passive View (UIView and/or UIViewController) and the Model. This pattern defines Views as recipients of the UI events, which then call the appropriate Presenter as needed. The Presenter is, in fact, responsible for updating the View with the new data returned by the Model. Migration Tutorial At this point you should have gotten a fairly decent idea of what both the MVC & MVP patterns entail, so let’s move on now with our brief tutorial. 1. First run The first thing you have to do is to download the Starter project from Pokedex MVC . Then you need to build and run the project. You should see something like this: 2. Creating the files needed Now let’s go ahead and migrate our app’s architecture from MVC to MVP. First of all, you have to create the Views and the Presenters. Create these files: PokemonListView, ItemsListView, PokemonPresenter & ItemsPresenter. Now your project should look like this: 3. Creating the Views Now let’s begin with the Views. The idea is that you need to define the methods that the UIViewController implements in order to display the information. A good practice is to first think about the goal of the UIViewController, and the method(s) it probably needs implement to meet such goal. For example, in the case of the PokemonListViewController, the main purpose is to render a list of existing Pokemons and be able to search for a given one. Also, as we fetch the data from the API, we should have a visual aid depicting that the request is still processing, and, at the same time, prompt a message in case the request fails for any reason. PokeListView code Swift import Foundation\n\nprotocol PokeListView {\n  func addPokemon(pokemon: Pokemon)\n  func getInitialPokemons()\n  func showLoadingIndicator()\n  func hideLoadingIndicator()\n} 1 2 3 4 5 6 7 8 import Foundation protocol PokeListView { func addPokemon ( pokemon : Pokemon ) func getInitialPokemons ( ) func showLoadingIndicator ( ) func hideLoadingIndicator ( ) } So, as a recap, you should first think about the main goal of the PokeItemsView, and then implement the class. 4. Creating the Presenters The Presenters shall handle the calls to the APIManager to pull the information from the services. Swift import Foundation\n\nclass PokeListPresenter {\n\n  private var pokemonView : PokeListView?\n\n  init() {  }\n\n  func attachView(view: PokeListView) {\n    pokemonView = view\n  }\n\n  func detachView() {\n    pokemonView = nil\n  }\n\n  func getInitialPokemons() {\n    for id in 1...5 {\n      APIManager.sharedInstance.retrivePokemon(id, completionHandler: { (pokemon) in\n        self.pokemonView?.addPokemon(pokemon)\n      })\n    }\n  }\n\n  func getNextPokemons(range: Int) {\n    for id in range...range+5 {\n      APIManager.sharedInstance.retrivePokemon(id, completionHandler: { (pokemon) in\n        self.pokemonView?.addPokemon(pokemon)\n      })\n    }\n  }\n} 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import Foundation class PokeListPresenter { private var pokemonView : PokeListView ? init ( ) { } func attachView ( view : PokeListView ) { pokemonView = view } func detachView ( ) { pokemonView = nil } func getInitialPokemons ( ) { for id in 1 ... 5 { APIManager . sharedInstance . retrivePokemon ( id , completionHandler : { ( pokemon ) in self . pokemonView ? . addPokemon ( pokemon ) } ) } } func getNextPokemons ( range : Int ) { for id in range ... range + 5 { APIManager . sharedInstance . retrivePokemon ( id , completionHandler : { ( pokemon ) in self . pokemonView ? . addPokemon ( pokemon ) } ) } } } Please ensure you create the PokeItemsPresenter! 5. Using the Presenters in the UIViewControllers The first thing you have to do is to extend the ViewController to implement the View (e.g. our PokeListViewController implements PokeListView). Swift extension PokeListViewController : PokeListView {\n\n  func getInitialPokemons() {\n    presenter.getInitialPokemons()\n  }\n\n  func addPokemon(pokemon: Pokemon) {\n    lastPokemon = pokemon.id!\n    pokemons.append(pokemon)\n    pokemonTableView.reloadData()\n  }\n\n  private func getNextPokemons() {\n    if pokemons.count == lastPokemon {\n      let range = lastPokemon+1\n      presenter.getNextPokemons(range)\n    }\n  }\n} 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 extension PokeListViewController : PokeListView { func getInitialPokemons ( ) { presenter . getInitialPokemons ( ) } func addPokemon ( pokemon : Pokemon ) { lastPokemon = pokemon . id ! pokemons . append ( pokemon ) pokemonTableView . reloadData ( ) } private func getNextPokemons ( ) { if pokemons . count == lastPokemon { let range = lastPokemon + 1 presenter . getNextPokemons ( range ) } } } Once you have built that PokeListView protocol, you then need to create a PokeListPresenter class in your PokeListViewController. It should look like this: Swift class PokeListViewController: UIViewController {\n\n  @IBOutlet var loadingView: UIView!\n  @IBOutlet var pokemonTableView: UITableView!\n\n  private let kPokeCellIdentifier = \"kPokeCellIdentifier\"\n  private var pokemons = [Pokemon]()\n  private let searchController = UISearchController(searchResultsController: nil)\n  private var filteredPokemons = [Pokemon]()\n  private var lastPokemon = 1\n  private var presenter = PokeListPresenter()\n\n  override func viewDidLoad() {\n    super.viewDidLoad()\n    setupSearchController()\n    presenter.attachView(self)\n    presenter.getInitialPokemons()\n  }\n} 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class PokeListViewController : UIViewController { @ IBOutlet var loadingView : UIView ! @ IBOutlet var pokemonTableView : UITableView ! private let kPokeCellIdentifier = \"kPokeCellIdentifier\" private var pokemons = [ Pokemon ] ( ) private let searchController = UISearchController ( searchResultsController : nil ) private var filteredPokemons = [ Pokemon ] ( ) private var lastPokemon = 1 private var presenter = PokeListPresenter ( ) override func viewDidLoad ( ) { super . viewDidLoad ( ) setupSearchController ( ) presenter . attachView ( self ) presenter . getInitialPokemons ( ) } } 6. Final run Final project Now, be proud of yourself! It’s there now!! Congratulations ! If you feel like downloading our solution and compare against your own version, please feel free to download ours from Pokedex MVP . And if you want, you can also read all about our Software Development Firm .", "date": "2016-10-26"},
{"website": "Moove-It", "title": "where-to-start-with-ruby", "author": [" Lucía Carozzi "], "link": "https://blog.moove-it.com/where-to-start-with-ruby/", "abstract": "DevSnack #25: When it comes to learning something new, people might feel a little bit lost. Not knowing where to start can be scary. Let’s fight those feelings and show the path on where to start when learning Ruby programming language. #1 – GitHut & Redmonk First, why Ruby? In GitHut , @littleark shows us “the complexity of the universe of programming languages”. The app provides a visualization of different programming languages across GitHub. Our goal, Ruby, is in the top 6. On the other hand, @sogrady does something similar but correlates discussions, via StackOverflow, and usage, via GitHub. In this ranking , Ruby is in the top 5. So, where do we start? #2 – Try Ruby In this site, @eallam and @nickawalsh introduce us to a console prompt and provide us basic lessons to get to know Ruby syntax and simple Ruby functions. #3 – Ruby in 20 minutes It is always good to start with the official quickstart. Here we are guided through a 20-minute tutorial where we are introduced to methods, objects and some basic algorithms. #4 – Ruby Koans @neoinnovate will push you further in understanding most of Ruby’s features, expecting you to answer correctly to move on to the next koan, introducing you also into testing Ruby. #5 – Warrior Maybe you just want to have fun experimenting with Ruby, here @trybloc shows us an entertaining way of learning. Have fun and good luck! Last but not least, if you somewhere down the road decide to start this journey of learning Ruby, its doc will become your bible. DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2016-01-29"},
{"website": "Moove-It", "title": "what-makes-phoenix-a-good-catch", "author": [" Santiago Noguera "], "link": "https://blog.moove-it.com/what-makes-phoenix-a-good-catch/", "abstract": "DevSnack #58: When selecting a web development framework we look for something that offers high productivity levels, maintainability, and scalability. Phoenix, the main framework of Elixir, aims to be our best friend, offering more than what we’re looking for. This DevSnack explains why it has piqued the interest of several people around the community. Let’s see what makes Phoenix a good catch! #1 – Elixir and Phoenix: The Future of Web APIs and Apps? Christian Nelson ( @xianpants ) explains why Phoenix and Elixir piqued his interest. He also shares some thoughts about his experience, after giving a try to this framework by rewriting one of their in-house web applications. #2 – Why Phoenix is exciting for the modern web? In this post Hjörtur Hilmarsson ( @hjortureh ) walk us through many aspects of Phoenix, Elixir and Erlang VM. He also shows a few challenges of web development like concurrency, real-time communication and distribution and shows how these tools face them. #3 – An Introduction to APIs with Phoenix Micha Woods ( @mwoods79 ) gives a brief and nutritious introduction to API building with Phoenix. She also describes some of the tools used around the ecosystem of this framework. #4 – How fast is Phoenix/Elixir? Yes, Phoenix seems to be fast, but… show me the numbers! In this article Fabio Akita ( @AkitaOnRails ) shares his performance analysis. #5 – Guts of Phoenix web socket channels Channels are a very powerful feature of Phoenix. In this link Zohaib Rauf ( @zabirauf ) shows us the way Phoenix channels are designed and how Phoenix web socket layer works. DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2016-09-16"},
{"website": "Moove-It", "title": "best-practices-js-stack", "author": [" Agustin Daguerre "], "link": "https://blog.moove-it.com/best-practices-js-stack/", "abstract": "DevSnack #57: JavaScript is everywhere, it has conquered the web and have also started to break into the desktop world. Gone are the days where being a JavaScript developer meant that you could only work on the front end of a web application. With Node.js those days are behind us and a new era of JavaScript development is just beginning. In this post, we’ll review some best practices for JavaScript and some of its more popular frameworks. #1 – 5 Principles that will make you a SOLID JavaScript Developer In JavaScript we are not used to apply the SOLID principles as much as in other languages that are more traditionally used in the back end of our applications. In this post Dor Tzur ( @dortzur ) show us how to become a SOLID JavaScript developer. #2 – Checklist: Best Practices of Node.JS Error Handling Here Yoni Goldberg ( @goldbergyoni ) shares with us a checklist that encapsulates every aspect regarding error handling that we should have in mind when writing code with Node.JS. #3 – React.js Best Practices for 2016 React has experienced a massive growth in popularity for the last couple of years. Péter Márton ( @slashdotpeter ) recaps the lessons learnt from past experiences and leave us with a set of best practices to look forward when developing applications with React. #4 – Promise Patterns & Anti-Patterns In this post Dave Atchley ( @tuxz0r ) walk us through some Best Practices, Patterns and Anti-Patterns to be aware of when handling asynchronous request on ES6. #5 – Untangling Spaghetti Code: How to Write Maintainable JavaScript How many times we have come across some old legacy code which we just want to throw away and start from scratch? In this article Moritz Kröger ( @morkro ) will show us some steps to tackle legacy code and keep a maintainable code. DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2016-09-09"},
{"website": "Moove-It", "title": "what-is-elixir-programming-language", "author": [" Lucas Aragno "], "link": "https://blog.moove-it.com/what-is-elixir-programming-language/", "abstract": "DevSnack #49: Learning a new programming language can be hard. Even if the syntax of the new language looks familiar, you need to figure out how that new programming language’s paradigm fits with your knowledge. Apart from that, you need to learn about the tools, libraries, modules and frameworks in the language ecosystem and how they work together. This week’s DevSnack has 5 different links to talks that I’ve found useful when I was starting to work with Elixir programming language for web development. #1 – Introduction to Elixir for Rubyists (Josh Adams) In the first half of this talk, Josh Adams guides us trough some Elixir basics like data types, data structures, modules, processes, matching, functions and how to call them. This part is really useful if we are a newcomer into the Elixir world. The second half shows a demo of Elixir on distributed robots by Robby Clement. #2 – Writing Command Line Applications (James Smith) Now that we are all set with the Elixir syntax and how it works, we can listen to this talk were James Smith shows us how to create command line apps using Elixir. Here we are going to learn a lot about Mix, Mix projects and tasks and dependency management for an Elixir project. #3 – Using OTP in production (Martin Schurrer) One of the reasons to use Elixir is because it runs on the BEAM, the Erlang’s VM which allows Elixir to get all the good stuff from a language that has 30 years of work on top of it. OTP is the Open Telecom Platform that comes with Erlang. On this talk we will learn how to integrate OTP with Elixir using supervision trees. #4 – Phoenix, a web framwork for the new web (Jose Valim) This talk is about Phoenix as a web framework for Elixir. In the video, Jose Valim doesn’t talk about how to use it, he talks about why Phoenix is one of the best choices for our app and why it’s important for the new generation of the web. #5 – Rise of the Phoenix (Chris McCord) In this video, Chris McCord gives us a tour around Phoenix features, the overall architecture, and how to use it to build awesome apps. DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2016-07-15"},
{"website": "Moove-It", "title": "diving-into-functional-programming", "author": [" Mario Souto "], "link": "https://blog.moove-it.com/diving-into-functional-programming/", "abstract": "DevSnack #53: Scala, Elm, Elixir, Clojure and the list goes on. The software industry is adopting a lot of functional concepts by creating functional programming languages or adding functional support to existing programming languages. If you are not using a functional programming language per se, it is highly likely that you are using one of those concepts in your application. But, what is functional programming? How different is from object oriented programming? Let’s talk about some aspects of this paradigm. #1 – What functional programming is and why it makes you better First, why should I care about functional programming? This post by Charlie Gower ( @charliejrgower ) answers this question by listing some benefits for both the programmer learning this paradigm, and the application built using this approach. #2 – Learning functional programming with JavaScript Anjana Vakil ( @anjanavakil ) talks about functional programming, pure and impure functions, high-order functions and some JavaScript libraries for efficient immutability. #3 – Object Oriented Programming vs. Functional Programming Bill Gathen ( @epicpoodle ) goes through the differences between Object Oriented Programming and Functional Programming using a short Ruby example. #4 – Functional Programming is finally going mainstream. Why now? Functional Programming is not a new concept at all, in fact it has been around for a long time. But why all of a sudden it has become so popular? Matthew Gertner ( @plasticmillion ) talks about this. #5 – Developing a Purely Functional Web Application Finally, Michal Plachta ( @miciek ) develops a purely functional web application using Elm and Haskell for the frontend and backend respectively. DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2016-08-11"},
{"website": "Moove-It", "title": "fast-cssconf-argentina-review", "author": [" Guzman Iglesias "], "link": "https://blog.moove-it.com/fast-cssconf-argentina-review/", "abstract": "Last Sunday August 7th, I attended the first South American CSS Conference (the CSSConf Argentina), which took place in Argentina, at Escuela Superior de Comercio Carlos Pellegrini, Buenos Aires. By the way, I was not alone; three other Moove-it folks were there: @aragno157 , @gusability and @Mefistos0 . It was a great opportunity to get close to the community and expose ourselves in front of high qualified technical talks. CSSconf Argentina promised an interesting trip trough several CSS disciplines presented by high-valued speakers. My expectations were pretty high. Once we arrived at the venue and walked through the facilities enjoying a nice breakfast, we realized that the organizational aspects were taken care of and we would have a great day. I must confess I was looking forward to seeing a presentation of a revolutionary technic, a different theoretical approach or maybe learn about new inputs referred to the future of CSS capacities. Unfortunately I wasn’t able to attend one of the workshops offered by the organization before the conference. Maybe that was the opportunity to go deeper on a specific practice and learn more insights. All in all, I really liked the talks. The variety of topics was remarkable and in general the talks totally engrossed me. 3 talks that I specially enjoyed: Werther Azevedo. Game Dev for Web Designers: Play with the DOM like you never did before! This talk was about exposing an alternative approach to game development by using CSS as a key element for DOM manipulation. The work of a CSS architect is normally constrained to developing web apps and involves interpreting PSDs, analyzing data architectures, following conventions and others technical, sometimes boring, tasks. Werther’s talk was a breath of fresh air, showing a work context I personally would like to explore. Aurelia Moser. Slippy Map Style: Geo-canvas design with CartoCSS . This talk showed that CSS and its way of thinking about styles can have its very own place in cartography, through CartoCSS. The talk wasn’t about how to use CartoCSS, but a walk through of a variety of pretty awesome examples, showing that CSS is something to have in mind when designing and customizing a map. Harry Roberts. Refactoring CSS Without Losing Your Mind . It was the last talk, probably generating the highest expectations on attendants. Harry Roberts showed, once again, his ability to come up with new and smart approaches to the CSS world. This time was not an exception. His talk landed on the field of CSS refactor. Refactoring working-but-awful code is always tempting and sometimes inevitable. Harry’s talk gave me a bunch of helpful tips and tricks I’m sure I will be able to put into practice next time an opportunity to refactor the stylesheets arrives. It’s worth mentioning the devotion of time and energy of some speakers to their slides, creating truly professional artworks that enhanced the overall experience. The excellent sound and screen visualization also contributed to the cause. Conclusion I enjoyed CSSconf Argentina. In future CSSconfs I would like to see more new CSS features to study and apply immediately. In any case, we always try to keep up with new CSS best practices and that’s why we never stop traveling to conferences and events around the world.", "date": "2016-08-29"},
{"website": "Moove-It", "title": "thinking-beyond-design", "author": [" Nicolas Suarez "], "link": "https://blog.moove-it.com/thinking-beyond-design/", "abstract": "DevSnack #26: in this week’s DevSnack we bring 5 interesting links on design. Design principles, design psychology, semantic animations and design ethics. Enjoy the read! #1 – Design Principles To Evaluate Your Product A company proves that it has a strong creative process by developing successful products repeatedly. We see this in companies like Apple, BMW and Google. In this article, Dave Schools brings the six core product design principles, and explains how to apply them with the aid of 15 product examples. # 2 – Why Product Thinking is the next big thing in UX Design Interesting read by Nikkel Blaase : When thinking of User Experience, we often think of a simple, beautiful, and easy to use feature-set of a product, that makes the user’s life easier. But as a matter of fact, features are merely a small, fragile part of the product. # 3 – Combining UX Design And Psychology To Change User Behavior Have you ever wondered why your users do not interact with your product the way you hope? Looks like Nadine Kintscher might have a clue. # 4 – Motion with Meaning: Semantic Animation in Interface Design Animation is fast becoming an essential part of the design of the interface, and it’s easy to see why. It gives a new dimension to play with time. This creates opportunities to do better at all levels our interfaces: you can make them easier to understand, more pleasant to use, and more pleasing to the eye. This article by Amin Al Haxwani and Tobias Bernard explains the importance of animating our interfaces. # 5 – Design, White Lies & Ethics Unless you’re a fan of dark patterns or turbid, it is likely that occasionally have problems with integrity in their practice of design: balance between the wishes of the parties against the user’s needs, for example, or guiding hero road users while also grants them freedom to explore. Dan Turner ‘s take on ethics in design. DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2016-02-05"},
{"website": "Moove-It", "title": "build-made-easy", "author": [" Diego Muracciole "], "link": "https://blog.moove-it.com/build-made-easy/", "abstract": "DevSnack #34: Far behind are the times where front-end projects do not involve the use of JS compilers, complex dependencies and testing frameworks (just to say some). As these technologies appeared to the scene, it was necessary to find a way to integrate them in the most transparent way. These next 5 links introduce us to the concept of build tools and what options are out there. # 1 – What are we talking about? Build tools and task runners are treated by most literature as synonyms, but they refer to two different (but not conflicting) concepts. @nadavspi explains in a very neat way what are the differences between them and what gobble is. # 2 – What options are out there? We always have many flavors to choose from, and build tools are not the exception. @bebraw summarizes many of them and explains their weaknesses and strengths. # 3 – How to choose one build tool? Got overwhelmed by all the different options? Don’t panic! As there isn’t a silver bullet product, @james_k_nelson gives us a little help on what build system should we use for a specific project. # 4 – Because the most used is not always the best @keithamus gives us a his take on one of the most used task runner tools, and explains why, in his opinion, we should avoid using it. # 5 – Will you be coming to brunch? Finally, @alxhill introduces Brunch , a very simple (not to say the simplest) and yet powerful build tool that will make your life easier in most of your projects. 100% guaranteed! DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2016-03-31"},
{"website": "Moove-It", "title": "rails-actioncable", "author": [" Emiliano Coppo "], "link": "https://blog.moove-it.com/rails-actioncable/", "abstract": "DevSnack #40: Everything changes. It’s a fact. A few years ago web development was very rudimentary, it just comprehended to serve a bunch of files from a web server which then the browser would interpret and render as a web page. But as time goes on, new technologies and also new requirements have emerged, transforming web pages from their ‘static’ origins to very rich, interactive, real-time complex applications. With Rails 5, ActionCable promises to help developers to include these real-time features seamlessly into any Rails app. Let’s learn about it and discover all the cool things we can do with it! #1 – Implementing WebSockets in Rails 5 with Action Cable Sophie DeBenedetto ( @sm_debenedetto ) brings a great introductory article about how WebSockets and ActionCable work in a Rails app. It features a standard chat room example using Rails, and also explains how to deploy our Rails + ActionCable app to Heroku. #2 – Rails ActionCable — The good and bad parts Everything can’t be perfect in the Rails wonderland, in this great article, Matthew O’Riordan ( @mattheworiordan ) meticulously lists the strengths and weaknesses of ActionCable as a real-time solution in the real world. #3 – Using ReactJS with Rails Action Cable Wondering how to integrate ActionCable with new frontend frameworks? This post shows a basic ActionCable + ReactJS example which would be very useful for our frontend friends! By Vipul Amler ( @vipulnsward ). #4 – ActionCable Examples For those of you who prefer examples rather than reading an article, the Rails team has put together this repository with a nice collection of ActionCable examples. Check it out! #5 – How to Deploy ActionCable and Rails 5 To Production In this screencast, the guys from Go Rails ( @GoRails ) show a complete example of developing a Rails app with ActionCable and specifically how to deploy our code to production using Passenger. Nice video! DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2016-05-13"},
{"website": "Moove-It", "title": "how-to-improve-agile-teams", "author": [" Santiago Estragó "], "link": "https://blog.moove-it.com/how-to-improve-agile-teams/", "abstract": "DevSnack #33: We all love Agile methodologies and they considerably help development teams to achieve their goals and be ready to respond to change. However, in every team, there’s always room for improvement and many things that need enhancement. In this post we’ll share some interesting links that can help you boost your team productivity and “ agility “. # 1 – Improve Agile Team Velocity In this post @nyike lists five ways to increase velocity and how to accelerate the team to achieve a critical deadline. # 2 – Tips for Iteration Planning @mberteig recommends 17 tips for Iteration Planning in this post. He mentions some different aspects of planning, like task breakdown, iteration duration and inviting experienced people from outside the project. # 3 – Your team is likely violating the first principle of Agile The first principle of the Agile Manifesto states “ Individuals and interactions over processes and tools. ” @gammons explains in his post why this principle is so easy to break and how many teams are breaking it. He also recommends how to slim down the process and tools to focus on individuals and interactions. # 4 – 7 Step Agenda for an Effective Retrospective Retrospective is a very productive ceremony for Scrum teams if done correctly, and can be used on other Agile methodologies. In this seven-step agenda, @caetano_tc and @paulocaroli catalogue many ideas and activities for retrospectives. # 5 – When is a User Story “Done?” In this blog post, @hey_AW explains some aspects of Acceptance Criteria and the “Definition of Done”, and answers the question “What does it mean to be done ?”. DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2016-03-24"},
{"website": "Moove-It", "title": "react-pleased-meet-part-1", "author": [" Diego Muracciole "], "link": "https://blog.moove-it.com/react-pleased-meet-part-1/", "abstract": "At Moove-it we have always cared about the community. This time, we wanted to create a space where we could learn and discuss about new technologies, share experiences and have a good time with anyone interested in the subject. With this in mind, it is how we celebrated our first-ever Moove-it Tech Meetup last July 27th: 3 talks, more than 70 guests, music, pizza, beer, and a lot of React! So, why React? First of all, we already had experience working with it and we wanted to share it with our team. Also, we knew there was a huge interest for knowing what all the fuss with React was about. Even though it’s open-source since 2013, it was not until these last few months that we started hearing more and more about new libraries and people using (and loving) React. We wanted the event to be dynamic and participative, so we divided it in three talks, 30 minutes each, with an open space for questions and debate at the end. The first one was a quick and yet deep introduction to React and Redux. These are subjects that typically are treated separately, but we wanted to give an insight on both since it’s more likely to be using them together when creating an app. The second one was about imperative vs. declarative JavaScript, explaining the key differences that structuring and writing code with React have over some other libraries such as jQuery. Finally, and to sum up the first two, the last talk was about explaining how React was used in a real project at Moove-it. This is the first part of a series of posts that aims to give a deep insight of what is and how to build a React app, sharing what the talks were about. So, let’s start from the beginning. React Even though many who came to the meetup had previous experience and knew what React was, others didn’t. So the obvious starting point was to answer the following question: What is React? React is a JavaScript library for building user interfaces. Not bigger than that. Nothing more. It is developed and maintained by Facebook, which used it internally for awhile until 2013 when they open-sourced it. More and more, companies have been adopting it and libraries have been built on top of it. One of the first questions a developer asks is what difference does it have with other frameworks that they already know. It’s natural: we use to compare new concepts to familiar ones in order to understand them. However, comparing React with Angular, Ember or whatever other framework, will probably lead to confusion. This is mostly due to the fact that React isn’t a framework by itself ; the only thing that React does is output HTML and nothing more. React does not have features such as native event system or AJAX calls. This means that you can’t build a dynamic web app with React only . This is one of the first, but yet very important facts that you have to know before even starting to use React. Many actors come into play in a React app, which can be a bit frustrating if you’re just starting with it. It is frequently said that learning React isn’t hard; what’s hard is learning all the tools needed for building a React app. This leads to our next question: Why is React so compelling then? Declarative React wraps an imperative API into a declarative one. This means that we don’t tell React exactly how to do things, but we declare what we want to do. The easiest way to see this is by looking how React manages DOM changes: it removes every direct interaction with the DOM. No CSS selectors of any types are needed. Just change some data, and React handles the operations needed to see that change reflected in the DOM. The way to “change that data” isn’t trivial, and we will get to that further in this article. This could be a bit ambiguous for someone who did not see yet a single line of React code, so we will resume this subject with more details and examples on the second part of this post. Composable React is all about Components. You can think of a component as any UI piece of the site that has a meaning, can be reusable and nested inside other component. In this way, we can create a mental model of the app based on Components, and not in HTML elements. This DOM abstraction comes with great benefits when we start to write code. It’s also important to notice that a React Component is not just the markup of a piece of the site, but also the logic (JS code) and styles (CSS). This mixture of concerns within one single block is one of the most criticized points of React, but it gives us the advantage of knowing how a component will render in any possible scenario by just looking at one source file. No more callbacks spreaded all over the project that change the site. Isn’t that awesome? Works great with teams It’s very easy to split Components into different members of the team. Each Component is independent from others and has a defined interface which must follow. Having defined that, all the developers can work in parallel and then merge their progress pretty smoothly. Virtual DOM Before jumping into React code, it’s important to understand the concept of VirtualDOM. In any app, we will have some kind of model that will have to sync with the view layer (scope variable in Angular for example). Rendering all the app in each model update would work, but certainly wouldn’t be efficient. What React does is: it maintains an intermediate model between the state of the app and the real DOM, that preprocesses any change on that state and talks to the real DOM to apply those changes in an efficient way. But, why don’t we just apply directly those changes into the DOM? Simply because DOM operations are very expensive. A DOM mutation can trigger many redrawing, layouting and painting operations that could be very time-consuming. What VirtualDOM does for us is to calculate and apply the minimal set of operations needed to switch from one state to a new one, reducing the bottleneck of DOM handling. Let’s see this in action: Imagine a site that has a primary heading, a paragraph and an empty unordered list. The VirtualDOM tree would look something like this: At some point, a response comes from the server with some useful data and we want to populate the list with it. What React does then is creating a new VirtualDOM tree and apply the needed changes to it: Now that React knows the old and the new state of the app, he can compare them and calculate the minimal set of steps needed to pass from one state to the other: All it’s left to do is applying them into the real DOM and see the changes on the screen! What about the code? Congratulations! If you made it this far, it means you now know the basics of why to learn React and you’re ready to see some actual code. Render method The render method represents the starting point of any React app. It’s basically responsible for taking some React Element and render it into the page. We can see this process as a translation between the Virtual DOM to the real one. ReactDOM.render(\r\n  <h1>Hello, world!</h1>,\r\n  document.getElementById(root)\r\n); 1 2 3 4 ReactDOM . render ( < h1 > Hello , world ! < / h1 > , document . getElementById ( root ) ) ; It takes two arguments: A React Element. In the example, it’s just a heading, but as we will see later on, it can be any type of element that we might have defined. A root element used to render the value passed in the first parameter. Note how the render method is part of the ReactDOM object. This is because React is shipped in many modules. For the web, we will need the React package as well as the ReactDOM package. While the first one is responsible for tasks from the library itself, the other one handles web-specific tasks. Components “A React Component is a highly cohesive building block for UI, loosely coupled with other components” We can think of Components as the fundamental blocks from which we will build the app’s UI. They represent the proper separation of concerns within the application. The easiest way to think about Components is to compare them with the HTML elements we already know. Let’s pick the a element for example: it has a specific meaning (render a link), has some styling (by default, blue), receives parameters (href in this case) and we could say that it even has some logic (underlined at hovering for example). React lets us define custom elements and treat them just like you would do with a link element in plain HTML. Of course, this is a very raw simplification, because React Components are extremely powerful and more extensible than regular HTML elements, but they have many things in common. Component definition There are two ways of defining a React Component, depending on whether you’re using ES6 or not, but they are essentially the same: // Using createClass method\r\n\r\nvar Title = React.createClass({\r\n  render: function() {\r\n    return <h1>Hello world!</h1>\r\n  }\r\n}); 1 2 3 4 5 6 7 // Using createClass method var Title = React . createClass ( { render : function ( ) { return < h1 > Hello world ! < / h1 > } } ) ; // Using ES6 classes\r\n\r\nclass Title extends React.Component {\r\n  render() {\r\n    return <h1>Hello world!</h1>\r\n  }\r\n} 1 2 3 4 5 6 7 // Using ES6 classes class Title extends React . Component { render ( ) { return < h1 > Hello world ! < / h1 > } } Each component must implement a render method. This method will return how that Component will be displayed in the screen (in this case, just by a heading). React.createClass method takes by only parameter an object that specifies the Component, with the render method within it. React.Component works as a base class from which we should extend to create the Component. To sum up We now know how to create a Component, and how to render it to the DOM. Let’s see them in action: var Title = React.createClass({\r\n  render: function() {\r\n    return <h1>Hello world!</h1>\r\n  }\r\n});\r\n\r\nReactDOM.render(\r\n  <Title/>,\r\n  document.getElementById('root')\r\n); 1 2 3 4 5 6 7 8 9 10 var Title = React . createClass ( { render : function ( ) { return < h1 > Hello world ! < / h1 > } } ) ; ReactDOM . render ( < Title / > , document . getElementById ( 'root' ) ) ; Result: Voilà! We now have our first Component rendered in the page! Protip: be careful with what render returns! It should always be one single element. JSX Did you notice some sort of weird syntax in the code above? Let’s say for example… HTML INSIDE JS?! Well, you are not alone: this is called JSX. “JSX is an optional preprocessor to use HTML-like syntax in our code.” What a Component returns via its render method is called a React Element and it’s natively created by calling the createElement method. What JSX enables us to do is writing this kind of statements in a way that feels so much familiar and similar to HTML syntax, so something like this: React.createClass({\r\n  render: function() {\r\n    return <h1>Hello world!</h1>\r\n  }\r\n}); 1 2 3 4 5 React . createClass ( { render : function ( ) { return < h1 > Hello world ! < / h1 > } } ) ; Becomes into something like this: React.createClass({\r\n  render: function() {\r\n    return React.createElement(\r\n      'h1', // Element type\r\n       null, // Props\r\n       \"Hello world!\" // Children\r\n    )\r\n  }\r\n}); 1 2 3 4 5 6 7 8 9 React . createClass ( { render : function ( ) { return React . createElement ( 'h1' , // Element type null , // Props \"Hello world!\" // Children ) } } ) ; createElement takes three arguments: The type of the element. Could be some HTML basic type or one defined by us. The props to pass to the element, i.e. any argument that the element needs. The children of that element. Notice that this could be another createElement invocation. So, we now see that what we wrote in our first Component isn’t actually HTML code but JS code with HTML syntax. Like we said before, the use of JSX isn’t mandatory, but it’s recommended if you don’t want to lose your mind when your Components start to grow. Protip: because words such as “class” and “for” are reserved in JS, we must use “className” and “htmlFor” instead when we write code in JSX Component Props Props are read-only properties passed to a React Component via its attributes and accessible inside the Component via the props object. Unlike HTML elements that can receive string attributes (such as class or href), React Components can receive many other data types, such as numbers, booleans and even functions. // The element\r\n\r\n<Title \r\n  color=\"red\" \r\n  fontSize=\"2em\"\r\n  hidden={true}\r\n  />\r\n\r\n// It's props\r\n\r\nthis.props = {\r\n  color: 'red',\r\n  fontSize: '2em',\r\n  hidden: true,\r\n  // ...\r\n} 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // The element < Title color = \"red\" fontSize = \"2em\" hidden = { true } / > // It's props this . props = { color : 'red' , fontSize : '2em' , hidden : true , // ... } PropTypes React gives us the ability to define what attributes a Component can receive and what type they should be. In a way, they allow us to define which interface is needed to use a Component. This comes very handy when someone else will use our Component. The way we specify PropTypes in a Component may vary, depending on whether we are using ES6 classes or not, but they are basically the same: // Using createClass method\r\n\r\nvar Serie = React.createClass({\r\n  propTypes: {\r\n    name: React.PropTypes.string.isRequired,\r\n    seasons: React.PropTypes.number\r\n  },\r\n  \r\n  render: function() { /* ... */ }\r\n}); 1 2 3 4 5 6 7 8 9 10 // Using createClass method var Serie = React . createClass ( { propTypes : { name : React . PropTypes . string . isRequired , seasons : React . PropTypes . number } , render : function ( ) { /* ... */ } } ) ; // Using ES6 classes\r\n\r\nclass Serie extends React.Component {\r\n  render() { /* ... */ }\r\n}\r\n\r\nSerie.propTypes = {\r\n  name: React.PropTypes.string.isRequired,\r\n  seasons: React.PropTypes.number\r\n} 1 2 3 4 5 6 7 8 9 10 // Using ES6 classes class Serie extends React . Component { render ( ) { /* ... */ } } Serie . propTypes = { name : React . PropTypes . string . isRequired , seasons : React . PropTypes . number } Protip: bare in mind that React does not throw an error if a PropType is violated. All it does is logging a warning in our development environment. DefaultProps DefaultProps are almost the same as PropTypes but they define what value a prop must have in case it wasn’t passed in. // Using createClass method\r\n\r\nvar Serie = React.createClass({\r\n  getDefaultProps: function() {\r\n    return {\r\n      name: 'Game of Thrones'\r\n      seasons: 1\r\n    }\r\n  }\r\n  \r\n  render: function() { /* ... */ }\r\n}); 1 2 3 4 5 6 7 8 9 10 11 12 // Using createClass method var Serie = React . createClass ( { getDefaultProps : function ( ) { return { name : 'Game of Thrones' seasons : 1 } } render : function ( ) { /* ... */ } } ) ; // Using ES6 classes\r\n\r\nclass Serie extends React.Component {\r\n  render() { /* ... */ }\r\n}\r\n\r\nSerie.defaultProps = {\r\n  name: 'Game of Thrones'\r\n  seasons: 1\r\n} 1 2 3 4 5 6 7 8 9 10 // Using ES6 classes class Serie extends React . Component { render ( ) { /* ... */ } } Serie . defaultProps = { name : 'Game of Thrones' seasons : 1 } See it in action Component State The state consists of any piece of data that the Component needs in order to function properly. The Component may eventually adopt any given state while it is rendered in the page. Like props , states are accessible within the Component via an object this.state. This object is just a regular JS plain object, so we can define as many attributes as we want. It is said that Components are state machines, that take a current state and render markup based on them. When the state changes, the Component re-renders to reflect that change in the screen. Setting an initial state Since the Component will use the state to generate some piece of UI, it’s obvious that we must set some initial values. For example, Which tab should be active by default? Or what price should I render when it is not specified? Like in the previous examples, we have two different flavours for setting the initial state: // Using createClass method\r\n\r\nvar Serie = React.createClass({\r\n  getInitialState: function() {\r\n    return {\r\n      likes: 0,\r\n      // ...\r\n    }\r\n  },\r\n  \r\n  render: function() { /* ... */ }\r\n}); 1 2 3 4 5 6 7 8 9 10 11 12 // Using createClass method var Serie = React . createClass ( { getInitialState : function ( ) { return { likes : 0 , // ... } } , render : function ( ) { /* ... */ } } ) ; // Using ES6 classes\r\n\r\nclass Serie extends React.Component {\r\n  constructor(props) {\r\n    super(props);\r\n    \r\n    this.state = {\r\n      likes: 0,\r\n      //...\r\n    }\r\n  }\r\n  \r\n  render() { /* ... */ }\r\n} 1 2 3 4 5 6 7 8 9 10 11 12 13 14 // Using ES6 classes class Serie extends React . Component { constructor ( props ) { super ( props ) ; this . state = { likes : 0 , //... } } render ( ) { /* ... */ } } Changing the state Ok, so we now have all the data we need to render the Component. Now, we just need a way to mutate this data in order to make our Component dynamic. For example, we would like to change the active tab. Luckily, React comes with a very handy method called setState : var Serie = React.createClass({\r\n  getInitialState: function() {\r\n    return { likes: 0, /* ... */ }\r\n  },\r\n  \r\n  like: function() {\r\n    this.setState({\r\n      likes: this.state.likes + 1\r\n    })\r\n  },\r\n  \r\n  render: function() { /* ... */ }\r\n}); 1 2 3 4 5 6 7 8 9 10 11 12 13 var Serie = React . createClass ( { getInitialState : function ( ) { return { likes : 0 , /* ... */ } } , like : function ( ) { this . setState ( { likes : this . state . likes + 1 } ) } , render : function ( ) { /* ... */ } } ) ; setState takes as a parameter an object with the new state that we want to set. What React does under-the-hood is merging this new state object with the current one, so any other state attribute that’s missing from the new object wouldn’t be affected. Other than changing the state, setState is also responsible for another task: triggering UI updates. Remember how we said that once the state is changed, the view is updated? Well, it is setState who does this automagically for us. What it really does, is executing all the VirtualDOM cycle where the minimal set of changes are applied, making it very easy and yet efficient to pass from one state of the app to another. Protip: When you’re in doubt about whether something should be part of the state or a prop, try to ask yourself who is responsible for handling that data. Remember: props are read-only and come from outside the Component. State is internal to the Component and it’s editable. See it in action Component lifecycle Last but not least, we have one more concept to talk about: lifecycle. A Component transits along the app through many states, starting before it is mounted into the DOM and ending when it is deleted from it. React makes it possible for us to define a variety of callbacks throughout a Component’s lifecycle. It is not the aim of this article to explain them all ( there are many ), but to give a brief description of the most frequent ones and possible use cases: componentDidMount: called only once in the life of the Component right after it was added to the DOM (aka mounted). This callback is usually appropriate for making AJAX requests or setting timeouts or intervals. shouldComponentUpdate: this callback is invoked every time the state or a prop is changed. It should always return a boolean value, that defines whether the Component should re-renders (this means, a call of the render method) or not based on the current state/props and the new ones. This allows us to have a very fine-grained control on the Component output and also on the performance of the app, avoiding unnecessary renderings. componentWillUnmount: invoked once before the Component is removed from the DOM, or unmounted. If you define timeouts on the componentDidMount method, it is a good idea to close them here. See it in action The end These are the basics of React. You can now jump straight on creating your custom Components. Be creative with them! Just remember: a Component it’s just an object that renders HTML on the screen based on some arguments (props) and its state. Keep that always in mind. However, there are important topics that this article does not cover that are crucial when creating a web app. For example, how do I structure a React project?, Where do I store global app data? These and some other questions will be answered on future posts to come. Stay tuned!", "date": "2016-09-23"},
{"website": "Moove-It", "title": "continuous-delivery", "author": [" Germán Barros "], "link": "https://blog.moove-it.com/continuous-delivery/", "abstract": "DevSnack #36: Are you afraid of releases? Too many things can go wrong? Too many features involved when releasing? Lack of coordination between departments? Continuous delivery aims to remove these fears. It’s a software engineering approach that will help you produce software in shorter cycles ensuring that it can be released at any point in time with high quality standards. # 1 – What is, and why continuous delivery? As @jezhumble explains in his books Continuous Delivery and Lean Enterprise, it is often assumed that if we want to deploy software more frequently, we must accept lower levels of stability and reliability in our systems. However, applying continuous delivery practices will invalidate this assumption. # 2 – Benefits and challenges Great article on the main benefits and challenges of applying continuous delivery. # 3 – How to apply it? Martin Fowler ( @martinfowler ) explains that you achieve continuous delivery by continuously integrating your software, building executables, running automated tests and pushing those executables to production-like environments. # 4 – What’s a continuous delivery pipeline? The pipeline breaks down the software delivery process into stages. Each stage is aimed at verifying the quality of new features from a different angle to validate the new functionality and prevent errors from affecting your users. # 5 – Building the pipeline with jenkins Andrew Phillips and Kohsuke Kawaguchi on how to implement a delivery pipeline using Jenkins. Let’s do it!!! DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2016-04-13"},
{"website": "Moove-It", "title": "about-motivation-productivity-concentration", "author": [" Guillermo Chiacchio "], "link": "https://blog.moove-it.com/about-motivation-productivity-concentration/", "abstract": "DevSnack #39: Motivation, inspiration, productivity, concentration… all those are common words we are used to hear everyday. But, what we really know about those concepts? Let’s take a quick tour to understand them better and discover some tricks and tips. I’ve personally found much of this stuff really helpful in different stages of my own career, and I hope you find the tools that work best for you! #1 – Feed Your Fire A non-technical talk with the goal of giving you a new idea, some battle-won advice, and leaving you excited and energized. I hope you enjoy! #2 – Inspiration vs. Motivation It may seem like a subtle distinction, but the worlds of motivation and inspiration are millions of miles apart. A lot of people use the words “motivated” and “inspired” interchangeably. But I’ve found something different to be the case. #3 – Help Others Develop Self-Pride At some point, we need to make the transition from looking for external praise to the ability to praise ourselves and to develop self-pride. If we have never made the transition, we are stuck. If we have made that transition, we need to help others make it too, and here’s why. #4 – Start A Productive Day Make your intentions clear to yourself and the universe is by asking yourself the same ten questions every morning and reviewing how the intentions worked out before you go to sleep at night. #5 – Skyrocket Your Concentration For Success At Work There seemed to be all the time in the day to get things done, and all the entertainment in the world to make sure that they weren’t done immediately. Sensory distractions – such as if your Facebook tab is flashing while you are trying to write a report,  and emotional distractions – such as anxiety, fear, worry or anger – are the biggest obstacles that prevent you from concentrating. DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2016-05-06"},
{"website": "Moove-It", "title": "whats-your-posture", "author": [" Ines Saint Martin "], "link": "https://blog.moove-it.com/whats-your-posture/", "abstract": "DevSnack #47: The cliché of the programmer hunched over their desk typing away on their laptop is alive and well in many offices. Is there a way to avoid pain down the road? Try some of the tips, such as correct posture and stay healthy. #1 – The importance of Good Posture for Software Developers What is the problem with poor posture? What are the long term effects of poor posture? In this post by Layne McIntyre we can read some of the negative effects and how to check our posture! There is also an animated guide with the basics of good posture. #2 – What Your Hardware Needs to Do Whitson Gordon tell us how to set up a healthy, ergonomic workspace to keep you comfortable and injury-free. What you really need to do is make a few changes around your workspace. #3 – Programming Your Hands Jeff Atwood ( @ codinghorror ) shares some hand exercises and the importance of trying to change things on a regular basis. Now, try some! #4 – 10 exercises and ergonomics for software developers You’re slouching right now, aren’t you? Don’t worry, it’s something we all do. In this post Mohammed Lakkadshaw wirtes 10 exercises to do at office and improve our posture. #5 – 10 tips to stay healthy as a developer It’s not all about posture, being a software engineer is definitely not a very healthy profession. Takes a look to this article by Shubham Sharma ( @ mailjet ) and try to be healthier. DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2016-07-01"},
{"website": "Moove-It", "title": "highlights-rubyconfbr-2016", "author": [" Lucas Aragno "], "link": "https://blog.moove-it.com/highlights-rubyconfbr-2016/", "abstract": "Last week I had the honor of giving a talk at RubyConf BR in São Paulo, Brazil. Here are my notes on the talks I enjoyed the most. I had the opportunity to give some talks in the past, but this one was my first talk in such a large event. I enjoyed it a great deal. I met a lot of awesome (and really smart) people who were really passionate about technology and specially about Ruby. They not only love working with and improving Ruby, but also exploring new technologies to improve their software development skills. That’s one of the reasons why I think the ruby community is one of the bests. Talks where awesome. Excellent keynotes and great speakers in general. The videos of the talks are not online yet , but I hope they will be soon. There were multiple tracks, so here are my highlights from the talks I chose to attend: Day 1 Trailblazer: One year after 1.0 release (by @celsovjf ) In his talk, Celso showed all the news about the Trailblazer world. He showed patterns from his 2 year-old app which is actually using Trailblazer in production! Also, he showed that Trailblazer is not just constrained to Ruby, talking about other languages that can benefit from using it. Slides Progressive web apps: the best of the Web, Appified (by @sergio_caelum ) In his talk, Sergio showed how to approach progressive web apps with awesome examples. He even uploaded the demo app to GitHub so you can test it on your phone! Slides Phoenix for Rubyists (by @michaellnorth ) Michael’s talk was a great introduction to Phoenix. He showed how easy is for Rails developers to get started into the Elixir/Phoenix world. Slides Ruby’s Concurrency Journey [Closing keynote] (by @jerrydantonio ) Jerry’s talk was very insightful. He talked about how much he loves ruby and also the present and future of the concurrency model in Ruby. Slides Day 2 Crystal for Rubyists (by @asterite and @bcardiff ) Ary and Brian are members of the Core Crystal team. They gave an awesome introduction to Crystal, a language that has a growing community and it seems to have a great future. Slides Kemal: building lightning fast web apps with simplicity (by @sdogruyol ) Following the previous talk, Serdar talked about Kemal, a Crystal web framework developed by him. He showed some really impressive benchmarks. He have also written a free book about it! ( crystalforrubyists.com ) Slides From monoliths to services: gradually paying your technical debt (by @dlitvakb ) In his talk, David told us about technical debt, why do we have it on our projects and how can we reduce it by improving our architecture. Slides Following the Trailblazer’s path [The Trail we blaze] (by @aragno157 ) Some self-promotion here… ahem. This was my talk about Trailblazer. Although Celso and I shared the same topic, mine was focused on the service oriented architecture of Trailblazer and how it can help us refactor our code, pay our technical debt and accept, and even embrace, our monolithic architecture. Slides Why (still) ruby? [Closing Keynote] (by @a_matsuda ) Akira’s talk closed the conference. It was about some really great things that came up in the last Ruby kaigi, what are people using Ruby for (besides building Rails web apps) and what to expect from Ruby 3. Slides", "date": "2016-09-30"},
{"website": "Moove-It", "title": "5-starter-kits-for-a-great-react-experience", "author": [" Patricio Maite "], "link": "https://blog.moove-it.com/5-starter-kits-for-a-great-react-experience/", "abstract": "DevSnack #45: Are you about to get your hands dirty with React and you don’t know which is the best way to set up a project structure that suits your needs? Check these approaches and share your experience! #1 – Start your next react project in seconds with React Boilerplate In this snack Max Stoiber ( @mxstbr ) presents a highly scalable offline-first foundation with the best developer experience and a focus on performance and best practices. #2 – React Hot Boilerplate Dan Abramov ( @dan_abramov ) presents the minimal dev environment to enable live-editing of React components. #3 – Get started with React, Redux, and React-Router! David Zukowski presents his starter kit designed to get you up and running with a bunch of awesome new front-end technologies: React Redux Starter Kit. #4 – React Starter Kit — “isomorphic” web app boilerplate This is an opinionated boilerplate for web development built on top of Node.js , Express , GraphQL and React . By Kriasoft . #5 – React Redux Universal Hot Example A starter boilerplate by Erik Rasmussen ( @erikras ) which helps you to set up a universal webapp using express, react, redux, webpack, and react-transform! DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2016-06-17"},
{"website": "Moove-It", "title": "dear-programmers-namaste", "author": [" Matías Masnú "], "link": "https://blog.moove-it.com/dear-programmers-namaste/", "abstract": "DevSnack #54: As software developers we are constantly training our brains and learning new things on a daily basis. We face challenges all the time and in order to solve them we use our creativity, our mind. But what happens to our bodies? I personally believe that we should cultivate both mind and body as a whole, and truly think that Yoga can help us in reaching that goal. Therefore, as we take care of our bodies we’ll be taking care of our brains: #1 – Powerful Reasons Coders Need Yoga Ciara Byrne ( @deciara ) – “While the mind lies all the time, the body never lies. When you are upset or angry, even if you are trying to ignore it, your body tenses up, your breathing gets faster. So if you really want to know how you feel about something, check in with the body” #2 – Yoga Poses Every Programmer Should Try Daragh Byrne ( @daraghjbyrne ) – “Programmers need to actively work to protect and strengthen their body due to the sedentary nature of their work. Healthy minds exist in healthy bodies. Modern yoga is a blend of ancient Eastern practice and philosophy with more recent techniques from Western gymnastics training. It can be a great tool for programmers, or anyone who spends a long time seated.” #3 – How Yoga Nurtures Your Programming Skills “At first glance, yoga and technology seem like uneasy fellows. Yoga’s spiritual image appears at odd with the highly scientific mindset of the software developer. But looking in deep and analyzing cause and effect of rigid work routines, yoga is a powerful way of re-centering oneself and putting oneself back in the body.” #4 – Practicing Yoga Helps Coders to be More Successful Advize Health ( @advizehealth ) – “After working all-day and coding for hours, it’s vital to relieve your stress and yoga is the best option for this. Yoga relaxes your body and helps to relieve stress while still providing a workout as well as an emotional health boost to recharge after a long day.” #5 – Why Yoga? “The nice thing about yoga is that, like the best RPGs, it has as much or as little content as you want. You can explore the rich mental and spiritual aspects of yoga, using the moving meditation to develop your mind and spirit and transform your life in ways you’ve never imagined. Or, press B and skip all that, using it simply as hardcore calisthenics to tune up your muscles and joints.” DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2016-08-18"},
{"website": "Moove-It", "title": "gearing-up-for-devops", "author": [" Lesly Acuña "], "link": "https://blog.moove-it.com/gearing-up-for-devops/", "abstract": "DevSnack #22: There’s been an awakening. The walls between Dev teams and Operation teams are disappearing. Small and big organizations are finding that their current development process is not effective. These days the term has been used by everyone, but what do we talk about when we say DevOps? #1 – What is DevOps (yet again)? There’s more to DevOps than we think. In this article Mike Loukides ( @mikeloukides ) talks about why and how the DevOps movement started and how the practices don’t just apply to technical teams, but to whole organizations. #2 – DevOpsCulture Using automation tools and agile practices is not enough to implement DevOps. This post by Rouan Wilsenach ( @rouanw ) emphasizes the importance of the culture in DevOps. #3 – 7 signs you’re doing devops wrong Adam Bertram ( @adbertram ) points out some common misconceptions companies and engineers have on the DevOps movement, while highlighting the philosophy behind it. #4 – 7 DevOps Tips From Gene Kim Deploy more frequently! Test in production! Some may sound outrageous and others may sound familiar, this post by Fredric Paul ( @TheFreditor ) lists a series of technical advices for implementing DevOps in all kinds of organizations given by Gene Kim, one of the biggest DevOps experts in the world. #5 – DevOps: The skills that matter What do we really need for being an engineer in a DevOps organization? From how we deliver code to communication skills, Dustin Collins ( @dustinmm80 ) lists the three DevOps skills he considers most important for engineers. Bonus track! At Disney, the DevOps movement awakens. In a session from last November’s Structure Conf Jeff Wile ( @jeffwile ) from Disney talks with Tom Krazit ( @tomkrazit ) about how a huge organization like Disney adopted DevOps and why they did it. DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2016-01-08"},
{"website": "Moove-It", "title": "10-5-talks-i-wouldnt-miss-at-railsconf-2016", "author": [" Gustavo Armagno "], "link": "https://blog.moove-it.com/10-5-talks-i-wouldnt-miss-at-railsconf-2016/", "abstract": "RailsConf 2016 in Kansas City is one of the Ruby community’s most important international events. Working for a company which contributes to Ruby-related events as part of its strategy to build high-performance software teams , I felt like it would be a nice idea to share which talks I’ll attend, and the reasons why I wouldn’t miss them, hoping to help you decide how to spend your time during the event. I tried not to be biased by who’s giving the talk, whether it’s sponsored or not, or any other factor — instead, basing my selection on the abstracts that I liked the most. About the kind of topics that I like, I often tend to mix “show-me-the-code” and “soft” talks when I go to conferences. I think both are important to being a professional developer. However, on this occasion I wanted to shrink my list down to only 10 options and I winded up just picking technical talks. So, here are the 10 talks, not ranked in any particular order, that I think you shouldn’t miss during RailsConf 2016. Or, putting it another way, if you were me you wouldn’t miss for anything. Talk #1 – Real World Docker for the Rubyist Docker is gaining popularity and promises to beat virtual machines (VMs) in the future. For those who haven’t heard of it, Docker is a more efficient alternative to using VMs. In a nutshell, Docker’s service layer runs over the host OS and creates isolated containers that include the app under development and all of its dependencies. VMs, on the other hand, don’t share the same OS instance. For example, two apps placed in two separate Docker containers will run on the same host OS. Conversely, two apps placed in two separate VMs will run on two different OS instances — and will therefore consume more resources. (I have recently written an article about how to use Vagrant to set up microservice-based projects hosted in separate repos , where I claim that Docker can be a better solution, if all your dev workstations are Linux machines). I would attend this talk because, as developers, we should all be aware of the importance of improving our development environment: making it portable, easy to configure across the team, and should help stop, once and for all, the reaction of blaming the machine when we cannot reproduce an error. Also, I would like to see how Docker and Ruby fit with each other. Talk abstract: Docker’s gotten a lot of press, but how does it fare in the real world Rubyists inhabit every day? Together we’ll take a deep dive into how a real company transformed itself to run on Docker. We’ll see how to build and maintain Docker images tailored for Ruby. We’ll dig into proper configuration and deployment options for containerized applications. Along the way we’ll highlight the pitfalls, bugs and gotchas that come with such a young, fast moving platform like Docker. Whether you’re in production with Docker or just dabbling, come learn how Docker and Ruby make an awesome combination. Related talk: How We Deploy Shopify Talk #2 – Going Serverless AWS Lambda allows developers to upload and run their code on a high-availability AWS infrastructure. It also takes care of managing and maintaining the server and operative system, scaling, memory balance, monitoring, logging, etc. Serverless, on the other hand, “is the application framework for building web, mobile and IoT applications exclusively on Amazon Web Services’ Lambda and API Gateway.” (see Serverless repo on GitHub ). AWS Lambda only supports Node.js, Java and Python, so I’m a little bit curious, in a good way, about what this talk has to do with Rails. Moreover, the topic sounds interesting, useful and I always try to keep an eye open for new languages, frameworks and technologies in general. Talk abstract: Serverless is a new framework that allows developers to easily harness AWS Lambda and Api Gateway to build and deploy full fledged API services without needing to deal with any ops level overhead or paying for servers when they’re not in use. It’s kinda like Heroku on-demand for single functions. Talk #3 – Inside ActiveJob At one point, every large and complex system requires some background job processing. Introduced in Rails 4.2, ActiveJob provides an abstraction layer to create and put jobs into background, interfacing with libraries like Sidekiq or Resque. The speaker promises to teach us how ActiveJob works internally, through building an asynchronous job processor from scratch, as a didactic approach. Talk abstract: ActiveJob made a huge impact when it landed Rails 4.2. Most job processors support it and many developers use it. But few ever need to dig into the internals. How exactly does ActiveJob allow us to execute performant, thread-safe, asynchronous jobs in a language not known for concurrency? This talk will answer that question. We’ll build our own asynchronous job processor from scratch and along the way we’ll take a deep dive into queues, job serialization, scheduled tasks, and Ruby’s memory model. Talk #4 – ActionCable for Not-Another-Chat-App-Please Action Cable extends Rails functionality with “real time” messaging through WebSockets. If you’ve read or listened to DHH criticizing JavaScript single page applications in favor of using ActionCable, but you haven’t had the chance to build a real app using it (like me), then this talk may be a useful guide. Talk abstract: RealTime updates using WebSockets are so-hot-right-now, and Rails 5 introduces ActionCable to let the server talk to the browser. Usually, this is shown as a Chat application — but very few services actually use chats. Instead, Rails Apps want to be able to update pages with new inventory information, additional products, progress bars, and the rare notification. How can we make this happen in the real world? How can we handle this for unauthenticated users? How can we deploy this? Talk #5 – Facepalm to Foolproof: Avoiding Common Production Pitfalls Let’s face it. This is me, every time I deal with an error involving the asset pipeline: Needless to say, expectations for this talk are high. Talk abstract: “WTF asset pipeline?” “What are all these errors?” “Why is my app running so slow?” If you’re new to Rails development, or just want some tips on deploying and running in production, this is the talk for you. Relying on real-world experience as part of the Heroku support team, we’ll talk through common issues (and a few funny ones) we see when people take their “but it works in development!” app to a production environment. Related talk #1: How Sprockets works Related talk #2: Saving Sprockets Talk #6 – The State of Web Security A couple of years ago, Najaf Ali gave an insightful speech about how to improve security in Rails apps , where he stated that, generally speaking, we all as developers, suck profoundly at security. In his own words: We suck so hard at security, that there’s a whole thriving industry (i.e. InfoSec) whose sole purpose is to document all the ways in which we suck at security. Among his various recommendations, the one that really stuck out, for me, was (quote): The only way to defend against attacks is to understand them in as much technical depth as possible. Since then, I have always tried to find some time for at least one talk about security trends — if there is one — when I go to tech conferences. Talk abstract: Join me for a wild ride through the dizzying highs and terrifying lows of web security in 2015. Take a look at some major breaches of the year, from Top Secret clearances, to medical records, all the way to free beer. We’ll look at how attack trends have changed over the past year and new ways websites are being compromised. We’ve pulled together data from all the sites we protect to show you insights on types and patterns of attacks, and sophistication and origin of the attackers. After the bad, we’ll look at the good – new technologies like U2F and RASP that are helping secure the web. Talk #7 – Secrets of Testing Rails 5 apps Rails 5 comes with a test runner out of the box, taking ideas from RSpec, Minitest and others. This new feature reduces running time and improves the output legibility. This talk promises to allow us to catch up with the new bin / rails test command. Talk abstract: Testing Rails 5 apps has become a better experience out of the box. Rails has also become smarter by introducing the test runner. Now we can’t complain about not being able to run a single test or not getting coloured output. A lot of effort has gone into making tests — especially integration tests — run faster. Come and join me as we commence the journey to uncover the secrets of testing Rails 5 apps. Related talk: RSpec and Rails 5 Talk #8 – Rails 5 Features You Haven’t Heard About We need to understand the tools we use. We never know when we are going to wind up using that small gem or hidden framework feature we heard about once in a meetup or a conference. I wouldn’t miss this talk about several of Rails 5’s minor, not very well-known, features. Talk abstract: We’ve all heard about Action Cable, Turbolinks 5, and Rails::API. But Rails 5 was almost a thousand commits! They included dozens of minor features, many of which will be huge quality of life improvements even if you aren’t using WebSockets or Turbolinks. This will be a deep look at several of the “minor” features of Rails 5. You won’t just learn about the features, but you’ll learn about why they were added, the reasoning behind them, and the difficulties of adding them from someone directly involved in many of them. Talk #9 – ActiveRecord vs. Ecto: A Tale of Two ORMs Elixir. I don’t know if it’s gonna pay my bills in the future, but I’m learning it. And I’m loving it. Also, it is impressive to see how fast, and inconspicuously, Elixir has slid into Ruby conferences. Even Matz said that Ruby 3, currently under development, has sought some inspiration from the way that Erlang/Elixir solves concurrency . This talk compares Rails ORM’s ActiveRecord with Ecto, its equivalent in Phoenix — Elixir’s framework, inspired by Rails. I’m currently learning Elixir, and the fact that it is apparently quickly escalating, makes me want to keep an eye on it. Talk abstract: They bridge your application and your database. They’re object-relational mappers, and no two are alike. Join us as we compare ActiveRecord from Rails with Ecto from Phoenix, a web framework for Elixir. Comparing the same app implemented in both, we’ll see why even with two different web frameworks in two different programming languages, it’s the differing ORM designs that most affect the result. This tale of compromises and tradeoffs, where no abstraction is perfect, will teach you how to pick the right ORM for your next project, and how to make the best of the one you already use. Related talk: Rails to Phoenix Talk #10 – Multi-table Full Text Search with Postgres The last talk in the list is related to full text search. Some projects involve text searching across multiple entities. For example, I’m currently working on a web app that allows users to search text across titles and descriptions of Promotions, Products and Categories. Depending on several factors, including the number of rows to be searched, the size and type of text columns, the number of tables and other efficiency related constraints, we may end up using Lucene, or Elasticsearch, or opt for less sophisticated solutions, like using SQL on your own database. If we opt for the latter, we generally want to tune and optimize our queries — and that’s what this talk seems to be all about. Talk abstract: Searching content across multiple database tables and columns doesn’t have to suck. Thanks to Postgres, rolling your own search isn’t difficult. Following an actual feature evolution I worked on for a client, we will start with a search feature that queries a single column with LIKE and build up to a SQL-heavy solution for finding results across multiple columns and tables using database views. We will look at optimizing the query time and why this could be a better solution over introducing extra dependencies which clutter your code and need to be stubbed in tests. Talk #10.5 – Excellence Through Diversity I excluded this talk from the list above, because I know the speaker well and he works a few feet away from me, so I guess I’m probably biased. However, if you read up to here — and, by the way, THANK YOU, for that! — I must sound fairly convincing and must have gained your trust. What I would say is that I’ve seen the talk being rehearsed here, at Moove-it. The topic is totally relevant, making you stop and think about this, so-called, “normal” social construct. Talk abstract: Great individuals often outperform their peers, and when going through school and applying for jobs this seems to be the most important aspect. But who is really outperforming whom? Also, what are we using to measure people? How do you know you’re being fair? Hiring is such a subjective topic and of utmost importance when building a team. Let’s explore how our strengths and weaknesses affect ourselves and the team and, how we need to look past ourselves when building a team. I hope this list helps you organize your time during the conference. Please feel free to chime in and recommend your own options. If you ever want to reach out during the event, our colleagues will be around the Moove-it booth. Have a great conf!", "date": "2016-04-27"},
{"website": "Moove-It", "title": "customize-your-agile-methodology", "author": [" José María Aguerre "], "link": "https://blog.moove-it.com/customize-your-agile-methodology/", "abstract": "DevSnack #51: The agile methodology provides a great framework for software development, but sometimes strictly following the guidelines is not productive. Here we have five links that could give us some interesting ideas to apply in our regular process. #1 – Why Do So Many Programmers Hate Agile? While we can all agree agile is better than waterfall development, the iterative methodology has a lot of detractors from it’s history of misuse. In this article, Anna Majowska from @dzone writes about sensations that programmers have while using an agile methodology. #2 – Scrum is not done yet, and neither are you Scrum has been around for the past 21 years, with Scrum.org fueling and supporting the methodology since 2009. In this post, Christina Mulligan ( @MulliganSDTimes ) tells us about the details of the Scrum SDK. #3 – Why you need to customize your agile methods It’s important that our agile process fit our workflow. In this link, Matthias Marschall ( @mmarschall ) give us some advices on how can we customize our process based on our team. #4 – 3 steps for customizing your agile processes Agile development is also meant to be creative and unique to each team. It’s meant to change, morph and be re-created by every team at every company. That’s why it is important to create a unique set of agile practices. This post by TechBeacon ( @TechBeaconCOM ) gives us three basic steps for doing so. #5 – Customizing Agile In this link, Scott Nonnenberg ( @scottnonnenberg ) teaches us how to exercise a new muscle: taking a standard cookie-cutter plan and tuning it for your specific team, project, or organization. DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2016-07-29"},
{"website": "Moove-It", "title": "useful-javascript-design-patterns", "author": [" Ignacio Villaverde "], "link": "https://blog.moove-it.com/useful-javascript-design-patterns/", "abstract": "DevSnack #43: Almost every developer has dealt with JavaScript in any of its flavors. However, not everyone spends the right amount of time analyzing their solutions, aiming for the best. Assuming we already know what a design pattern is, it is important to remark three main benefits of using them. As @addyosmani explains in his book Learning JavaScript Design Patterns : Patterns are proven solutions Patterns can be easily reused Patterns can be expressive This DevSnack supplies some design patterns which can be used or at least be considered for improving your JS code quality. #1 – The Module Pattern @ toddmotto exposes the Module Pattern in a vast way, being a must-know pattern for every JS developer. This post is of great help for beginners who don’t know much about it. #2 – The Prototype Pattern When coming from other languages, especially traditional object-oriented ones, prototype pattern can become a little bit confusing. @addyosmani wrote a whole chapter for it, guiding us through the road from zero to hero. #3 – The Bridge Pattern Although being one of the most underused design patterns, bridges are quite helpful in event-driven apps. This type of apps are common in JavaScript  so why don’t spending some time on it. @JoeZimJS has a great post for it in his blog, just read it. #4 – The Publisher Pattern In this post @ded shows his own publisher pattern, described by himself as an “Observer Reloaded”. If I were you I would take a look at it, it’s always nice to learn from the big fish. #5 – The Factory Pattern Being used by a lot of JS frameworks, it is really useful to understand the factory pattern in depth. @carldanley published a short article about it, which I highly recommend. DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2016-06-02"},
{"website": "Moove-It", "title": "the-power-of-women-in-tech", "author": [" Virginia Rodríguez "], "link": "https://blog.moove-it.com/the-power-of-women-in-tech/", "abstract": "DevSnack #31: Last March 8th, International Women’s Day was commemorated, so in this week’s dev snack we’re sharing five resources in the topic of women in tech. # 1 – Who was Ada? Probably you already heard of Ada Lovelace, often referred to as the first computer programmer . In case you want to know her history better, this short biography by @FindingAda is a good place to start. # 2 – The queen of code Probably less known than Ada Lovelace, computer programmer Grace Hopper helped the development of a compiler that was a precursor to the widely used COBOL. Hopper’s story is told in The Queen of Code, a short film brought to us by @FiveThirtyEight . # 3 – Famous women in computing From programming languages to interaction design to robotics, women have been contributing to the evolution of computing for over a century. Check this cool timeline by @anitaborg_org to know more about them. # 4 – Women making technology work for them In this recent article shared by @bbc , women from 11 countries share an insight into how they are using technology to make things happen for their communities, families and themselves. # 5 – Origins A written interview series by @IvanaMcConnell with some great women in the tech industry, and how they got their start – the exact moments when they fell in love with the web, engineering, and design. DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2016-03-11"},
{"website": "Moove-It", "title": "the-art-of-estimation", "author": [" mig "], "link": "https://blog.moove-it.com/the-art-of-estimation/", "abstract": "DevSnack #23: If you work in the Software Development field, especially in a company doing Agile Development, you most likely have come upon an estimation situation. Estimations are no piece of cake, but come handy when dealing with customer and product management expectations that will get you that limited budget you’ve been begging for. We rounded up five “You may like to read this” articles about the “art” of estimating. #1 – Purpose of Estimation Here @martinfowler shares a short dissertation on why and when estimations become a valuable part of a team’s workflow. #2 – Why is estimating so hard? Beautiful words, but, in case you are under the impression that nothing can go wrong with estimations, please take a look at this @unclebobmartin ’s article. #3 – What’s the point of Agile Points? There are many schools when it comes to units of estimation: hours, points, t-shirt sizes, and what not. My personal taste is to go with points, and I think this article by @mdavidgreen contains a fair explanation of what they are and how they work. #4 – Three powerful estimation techniques for Agile teams In order to be able to estimate in teams, criteria must be used and agreements must be reached. Many teams even don’t use any popular technique, but define their own based on others. In another article by @mdavidgreen we take a look at three of the most renowned ones. #5 – Velocity, Agile estimation and Trust There’s no point in using estimations if your expectations upon them are unrealistic. In this article, @noelrap makes some reasoning about estimations in Agile projects. May the points be with you. DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2016-01-13"},
{"website": "Moove-It", "title": "speed-up-your-database", "author": [" Mauricio Coniglio "], "link": "https://blog.moove-it.com/speed-up-your-database/", "abstract": "DevSnack #55: When building systems, we must consider a lot of things with regard to manipulating stored data, so that they perform efficiently. When domain/business data is little there is not much to worry about, however, as it grows, latency becomes a key factor to be cautious about, since it could potentially make our whole system to behave poorly, subsequently provoking an overall negative impact/perception towards our user base’s eyes. There are several things we can do to make our database perform better; from properly saving our data, to designing well-formed data structures to optimize our database operations. #1 – Do you know if your Database is slow? Every time we tackle any given problem, we first need to be able to thoroughly define it. Now, when it comes to boosting our system performance, we should first isolate whether the database is the one actually causing the slowdown. In this article, Gorjan Todorovsk ( @gorjant ) presents a few strategies to help us out in getting to the bottom of it. #2 – Database optimization techniques you can actually use In this post, Eran Galperin ( @erangalperin ) provides a good starting guide to optimize database performance. Here he outlines a variety of concepts, including some that are essential for determining exactly what you should be looking at, what to tweak in your database and how you should do it. #3 – Introduction to Data Normalization: A Database “Best” Practice One of the most relevant aspects to take into account while storing data is to follow and tailor standards and practices. In that sense, it is strongly encouraged to normalize tables and columns to reduce redundancy and to ensure data integrity. In this article from Scott Ambler ( @scottwambler ) , he explains the three most common forms of normalization. #4 – SQL Database Performance Tuning for Developers As developers, we should adhere to industry-wide, commonly-accepted ways of managing our database. We should enforce ourselves to efficiently design our queries, so that performance does not suffer while running unnecessary and highly-resource-consuming tasks and calculations. Another basic (but often overlooked) thing we should be doing at all times is creating indexes to speed up querying operations. In this article Rodrigo Koch elaborates on these concepts (and some others). #5 – Caching Techniques Another way to speed things up is to avoid running the same query multiple times. To achieve this, you may use caching techniques, so that the results of certain recurring queries get temporarily saved. This article by Jakov Jenkov ( @jjenkov ) talks about caching techniques, how they work, their advantages and practical usages. DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2016-08-26"},
{"website": "Moove-It", "title": "securing-ruby-rails-apps", "author": [" Guillermo Chao "], "link": "https://blog.moove-it.com/securing-ruby-rails-apps/", "abstract": "DevSnack #42: We all know the benefits of Ruby on Rails applications, but what about their security? In this DevSnack we will cover different aspects of Ruby on Rails applications from a security standpoint. It doesn’t exist a simple guide to follow in order to make the “impenetrable” web application, but with the following articles you will be able to, at least, build more robust RoR applications. #1 – How Do Rails Developers Keep Updated on Security Alerts? In the following article, Gavin Miller will cover different options to keep yourself updated with the latest security alerts. He will cover Mailing Lists, Common Vulnerability Exposures -CVE- reports, different tools to audit your application in terms of security. #2 – Preproduction Security Checklist for Rails app Heiko Webers is the author of “Rails Security Guide” and “Rails Security Strategy”. He made a list of security issues to check when developing Rails Apps. From the great old SQL Injection, to Cross-Site Request Forgery, he goes through 10 vulnerabilities, explaining them as well as how to avoid them. More articles from Heiko here . #3 – Using (and Testing) Rack::Attack to Improve the Security of Your Rails App In the following post, Hayley Anderson explains how to use the Rails gem “Rack::Attack” to test how secure your application is. Rack::Attack is a Rack middleware intended to make your Rails application more robust and secure – give it a try! #4 – Two Factor Authentication in Rails 4 with Devise, Authy and Puppies We all know that the simple combination of username and password doesn’t make a robust authentication method. Two-factor authentication helps to fight this issue. This is not the “one for all” solution, there is no such thing – but it definitely helps. Phil Nash covers this topic by building a Rails 4 application with the “Authy” gem to protect puppy pictures. #5 – More Useful Resources Ruby on Rails Security Project : This project indexes useful articles, tools and guides about Rails Security. Ruby on Rails Security Mailing List : This is the mailing list for all the security-related topics in Rails, a good place to search about the security state of releases. Breakman – Rails Security Scanner : This gem helps you scan your code statically to detect potentially unsafe code. Sadly, there is no perfect application in terms of security, no magic solution. As a community, we all need to contribute! “Security is sometimes a tough effort to justify because when it’s working you’ll rarely notice” – Gavin Miller DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2016-05-26"},
{"website": "Moove-It", "title": "reactive-programming", "author": [" JR González "], "link": "https://blog.moove-it.com/reactive-programming/", "abstract": "DevSnack #41: Want to learn what Reactive programming is? It’s become a very popular paradigm, increasing its importance among highly interactive websites like Netflix. In this DevSnack you will learn what Reactive programming is and see a few interesting implementations of it. #1 – What is Reactive Programming? Kevin Webber ‘s post introducing Reactive programming. To understand Reactive — both the programming paradigm and the motivation behind it — it helps to understand the challenges that are faced by developers and companies today compared to the challenges faced only a decade ago. #2 – Volt Check out this relatively new Ruby web framework by Ryan Stout that lets your Ruby code run on the client and the server. #3 – ReactiveX – asynchronous programming done right? Today applications need to be more and more asynchronous and process more information in parallel. Every programming language has its own way of dealing with asynchronous programming. Some are designed for such purposes (like Erlang) but the others, more popular ones, are not. Is there no hope for Java, .Net or JavaScript developers? I think there is. Post by Mateusz Łyczek. #4 – Introduction to ReactiveX on Android In this post by Adam Jodłowski you will learn how to setup and use basic ReactiveX components on Android. It’s a great tutorial, don’t miss out! #5 – Google Agera vs. ReactiveX There was a “big” announcement from Google: they’ve released their reactive programming library targeting Android specifically: Agera . Of course, one has to look into the details to get an accurate picture. Writing a reactive library is not an easy task and one can fall into a lot of mistakes if one is not familiar with the history and evolution of field. Let’s see Dávid Karnok ‘s take on this subject. DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2016-05-23"},
{"website": "Moove-It", "title": "new-flavors-javascript", "author": [" Lucas Aragno "], "link": "https://blog.moove-it.com/new-flavors-javascript/", "abstract": "DevSnack #32: Have you ever tried to do time travel debug, hot reloading, one-way reactive data flow or template literals with your everyday javascript framework? It’s too hard. That’s why the JS community created Redux, React, ES6, Babel, Webpack and some other rad utilities. This week’s DevSnack compiles 5 links that will tell you a little bit about these amazing tools. # 1 – Why Babel Matters @phpnode talks about why you should care about Babel, why it’s different from other compile-to-js systems and why it’s awesome. # 2 – Make javascript again Here you can check out some of the top features that ES6 adds to Javascript to make your dev life easier. Also, you can begin to argue about ES6 vs. ES5, mutability and transpilers on the interwebs. # 3 – Devs reacts to react @chibicode will help you to get started on React in a really quick and easy way. # 4 – Gotta learn ’em all In this tutorial David Chang will guide you trough the basics of Redux building a Pokedex. # 5 – Out with the old in with the new This Medium post by @jennschiffer talks about the process of learning and adapting to new web technologies. DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2016-03-17"},
{"website": "Moove-It", "title": "java-under-the-hood", "author": [" Enrique Rodriguez "], "link": "https://blog.moove-it.com/java-under-the-hood/", "abstract": "DevSnack #44: So, you know how to write Java applications, you are comfortable with a couple of frameworks, and you can solve the majority of your daily challenges. But, what’s going on under the hood? Maybe you are getting an obscure error that you don’t understand, or you are trying to optimize some already well-performing application, or maybe you are just curious. Even if you work with a JVM-based language like Groovy, JRuby, Scala, or Clojure, here you can find some very good snacks that will start feeding your hunger for knowledge. #1 – JVM Internals Let’s start with this one. In this article James D. Bloom ( @jamesdbloom ) will give you an introduction to the internals of the JVM that is very detailed but very easy to grasp at the same time. #2 – Java Garbage Collection Distilled In this snack Martin Thompson ( @mjpt777 ) explains how the JVM garbage collector works; i.e. generations, heap spaces, minor and major collections, among other topics. He also gives you an insight into the different trade-offs regarding performance and when to choose one collector over the other. #3 – Java multi-threading: volatile variables, happens-before relationship, and memory consistency Concurrency problems are becoming common these days, and they often narrow down to very fundamental concepts of computer science, like how a CPU works. In this post, Sayem Ahmed ( @say3mbd ) explains the Java volatile keyword and the consequences of using it. #4 – Reference types in Java: Strong, Soft, weak, Phantom Did you know that there are different types of references to an object? Even if you already knew, Chandrashekhar ( @olntutorlspoint ) will help you better understand the topic. #5 – Exploring Java bytecode Finally, if you think you know a lot about Java and the JVM, and want to go further, why not learn some Java bytecode? In this article Nish Tahir explores the bytecode basics and encourages you to continue studying it. DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2016-06-09"},
{"website": "Moove-It", "title": "how-i-test-ruby-on-rails-web-apps", "author": [" Pablo Ifran "], "link": "https://blog.moove-it.com/how-i-test-ruby-on-rails-web-apps/", "abstract": "In this post, I will describe different techniques I generally use for testing Ruby on Rails apps. I’m going to use as an example a simplified version of a blogging platform we have developed for a client. The app basically allows users to manage and publish stories, which are posts, but more complex – e.g. Stories are versioned. To simplify the examples I will leave complexities aside and talk about stories as if they were posts. Note: I’m assuming that you already know the importance of automated testing, so I’m not arguing its importance here. Diving into the details, let’s start with the Story model: Story class Ruby class Story < ActiveRecord::Base\r\n\r\n  scope :drafts, -> { where(draft: true) }\r\n\r\nend 1 2 3 4 5 class Story < ActiveRecord :: Base scope : drafts , -> { where ( draft : true ) } end Within the Story model we have a scope called “drafts”, that returns all the stories from our database which are drafts (have the attribute draft set in true). By default stories are not drafts. Now that we have defined the scope, we need to think about all the possible scenarios that need to be tested. If we take a look at the scope, it’s easy to realize that we have two possible situations: The first one is when the story is a draft and the other one is when it’s not. Hence, we can divide our tests like this: Ruby require 'rails_helper'\r\n\r\ndescribe Story do\r\n\r\n  describe '.drafts' do\r\n    subject { described_class.drafts }\r\n\r\n    context 'when it is a draft' do\r\n    end\r\n\r\n    context 'when it is not a draft' do\r\n    end\r\n  end\r\n\r\nend 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 require 'rails_helper' describe Story do describe '.drafts' do subject { described_class . drafts } context 'when it is a draft' do end context 'when it is not a draft' do end end end Now that we have a clearer idea of the possible scenarios, we can start adding in tests. In this case, since each scenario (context) will only have one test, we can take advantage of the “let!” method. Note: using the let! method might not always be considered as a good practice, because it generates a before hook and then executes and memorizes the content of the given block. Thus, for each test we will have the block content available, and if it’s a database query it will be executed several times, even if we don’t need it. In our case we can easily realize that we won’t have more tests per scenario, so we can use let! without worrying about the performance. Let’s create the tests: Ruby require 'rails_helper'\r\n\r\ndescribe Story do\r\n\r\n  describe '.drafts' do\r\n    subject { described_class.drafts }\r\n\r\n    context 'when it is a draft' do\r\n      let!(:story) { create(:story, draft: true) } # FactoryGirl with sugar syntax\r\n\r\n      it { expect(subject).to include(story) }\r\n    end\r\n\r\n    context 'when it is not a draft' do\r\n      let!(:story) { create(:story, draft: false) }\r\n\r\n      it { expect(subject).to_not include(story) }\r\n    end\r\n  end\r\n\r\nend 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 require 'rails_helper' describe Story do describe '.drafts' do subject { described_class . drafts } context 'when it is a draft' do let ! ( : story ) { create ( : story , draft : true ) } # FactoryGirl with sugar syntax it { expect ( subject ) . to include ( story ) } end context 'when it is not a draft' do let ! ( : story ) { create ( : story , draft : false ) } it { expect ( subject ) . to _ not include ( story ) } end end end In this example our tests are based on data, meaning that they rely on database queries. If our method implementation changes, the tests will still pass, as long as the result is the expected one. These tests are recommended for model methods handling data, changing the model status or, like in this case, for database queries. Now we can create stories and make them “draft” or “published”, but we are not done yet. Having a story without a cover photo will not look nice. So our next step will be adding the ability for a story to have one. We will do some photo processing, to make sure that all the cover photos follow the same standard: User model class Story < ActiveRecord::Base\r\n\r\n  scope :drafts, -> { where(draft: true) }\r\n\r\n  # More code here\r\n\r\n  # The cover is processed in the background. Before the transformations \r\n  #   finish, the original cover is returned. Once the process have\r\n  #   finished, a reduced version of the transformed cover is returned.\r\n  #\r\n  # @note The cover could not exist. In such a case, the fallback url will be \r\n  #   returned when we call cover.url.\r\n  def cover_photo_url\r\n    if cover_photo.is_processed?\r\n      cover_photo.normal.url\r\n    else\r\n      cover_photo.url\r\n    end\r\n  end\r\n\r\n  # More code here\r\n\r\nend 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 class Story < ActiveRecord :: Base scope : drafts , -> { where ( draft : true ) } # More code here # The cover is processed in the background. Before the transformations #   finish, the original cover is returned. Once the process have #   finished, a reduced version of the transformed cover is returned. # # @note The cover could not exist. In such a case, the fallback url will be #   returned when we call cover.url. def cover_photo_url if cover_photo . is_processed ? cover_photo . normal . url else cover_photo . url end end # More code here end This is a more complex situation: we have a few scenarios when the cover photo is processed and some more when it is not. Furthermore, the cover photo could even not exist. Let’s start with the contexts we need to define. A good approach could be defining one case for when the cover photo hasn’t been fully processed yet, and another case for when the background processing has completed. For the first case, we have two scenarios, depending on whether the cover photo exists or not. Contexts definition: require 'rails_helper'\r\n\r\ndescribe Story do\r\n\r\n  let(:story) { create(:story) }\r\n\r\n  describe '#cover_photo_url' do\r\n    subject { story.cover_photo_url }\r\n\r\n    context 'when cover exists' do\r\n      pending 'returns the fallback url'\r\n    end\r\n\r\n    context 'when cover does not exist' do\r\n      pending 'returns cover url'\r\n      pending 'returns cover processed url if is processed'\r\n    end\r\n\r\nend 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 require 'rails_helper' describe Story do let ( : story ) { create ( : story ) } describe '#cover_photo_url' do subject { story . cover_photo _ url } context 'when cover exists' do pending 'returns the fallback url' end context 'when cover does not exist' do pending 'returns cover url' pending 'returns cover processed url if is processed' end end Now that our contexts and test cases are well defined, we can start implementing them: require 'rails_helper'\r\n\r\ndescribe Story do\r\n\r\n  let(:story) { create(:story) }\r\n\r\n  describe '#cover_photo_url' do\r\n    subject { story.cover_photo_url }\r\n\r\n    context 'when cover exists' do\r\n      it 'returns the fallback url' do\r\n        expect(subject).to include('fallbacks/cover_photo/default.png') # This is the path to the default image.\r\n      end\r\n    end\r\n\r\n    context 'when cover does not exist' do\r\n      it 'returns cover url' do\r\n        story.update(cover_photo: File.new(Rails.root.join('spec', 'support', 'files', 'image.jpg')))\r\n\r\n        expect(subject).to include('story/cover_photo/image.jpg')\r\n      end\r\n\r\n      it 'returns cover processed url if is processed' do\r\n        story.update(cover_photo: File.new(Rails.root.join('spec', 'support', 'files', 'image.jpg')))\r\n\r\n        # Run all background jobs to process the avatars        \r\n        ...\r\n\r\n        expect(subject).to include('story/cover_photo/image_processed.jpg')\r\n      end\r\n    end\r\n\r\nend 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 require 'rails_helper' describe Story do let ( : story ) { create ( : story ) } describe '#cover_photo_url' do subject { story . cover_photo _ url } context 'when cover exists' do it 'returns the fallback url' do expect ( subject ) . to include ( 'fallbacks/cover_photo/default.png' ) # This is the path to the default image. end end context 'when cover does not exist' do it 'returns cover url' do story . update ( cover_photo : File . new ( Rails . root . join ( 'spec' , 'support' , 'files' , 'image.jpg' ) ) ) expect ( subject ) . to include ( 'story/cover_photo/image.jpg' ) end it 'returns cover processed url if is processed' do story . update ( cover_photo : File . new ( Rails . root . join ( 'spec' , 'support' , 'files' , 'image.jpg' ) ) ) # Run all background jobs to process the avatars . . . expect ( subject ) . to include ( 'story/cover_photo/image_processed.jpg' ) end end end This is, once again, the same pattern: tests based on data. Now our model is ready to create stories, but we still need to define an endpoint to allow our users to create a story. For this purpose, we will create a new controller. Testing controllers To test controllers we usually create “stubs”. Our controller will look like this: class StoriesController < ApplicationController\r\n\r\n  respond_to :html\r\n\r\n  def create\r\n    @story = CreateStoryService.run(current_user, story_params)\r\n\r\n    respond_with @story\r\n  end\r\n\r\n  private \r\n\r\n  def story_params\r\n    params.require(:story).permit(:content, :draft, :cover_photo)\r\n  end\r\n\r\nend 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class StoriesController < ApplicationController respond_to : html def create @ story = CreateStoryService . run ( current_user , story_params ) respond _ with @ story end private def story_params params . require ( : story ) . permit ( : content , : draft , : cover_photo ) end end The class “ CreateStoryService ” is responsible for creating a story for the given user, and then returning the newly created story. Note: We are assuming that the person who created this class, also wrote the proper tests for it. Once again, I will only focus on what I actually want to test and assume that the rest is safe and sound. Let’s continue by defining all the possible scenarios for this controller: spec/controllers/places_controller_spec.rb require 'rails_helper'\r\n\r\ndescribe StoriesController do\r\n\r\n  let(:user) { create(:user) }\r\n\r\n  describe '#create' do\r\n    # Here we use shared_examples_for because we can have multiples roles that \r\n    # behave the same way for this action, and we don't want to repeat the tests.\r\n    shared_examples_for 'a logged out user' do\r\n      pending 'should not be able to create stories'\r\n    end\r\n\r\n    shared_examples_for 'a logged in user' do\r\n      pending 'should be able to create stories'\r\n      pending 'should ignore incorrect params'\r\n      pending 'should correctly respond after a story is created'\r\n      pending 'should handle the errors correctly'\r\n    end\r\n    \r\n    context 'when user is not logged in' do\r\n      it_should_behave_like 'a logged out user'\r\n    end\r\n\r\n    # We can have other roles and the context will be similar but \r\n    # instead of sign_in the user we can sign_in an admin, for \r\n    # example, and have the same behaviour.\r\n    context 'when user is logged in as a normal user' do\r\n      # Assume that the sign_in method will login the user and stub that user \r\n      # on the controller. That way we can refer to it on the tests.\r\n      before { sign_in user } \r\n\r\n      it_should_behave_like 'a logged in user'\r\n    end\r\n  end\r\n\r\nend 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 require 'rails_helper' describe StoriesController do let ( : user ) { create ( : user ) } describe '#create' do # Here we use shared_examples_for because we can have multiples roles that # behave the same way for this action, and we don't want to repeat the tests. shared_examples _ for 'a logged out user' do pending 'should not be able to create stories' end shared_examples _ for 'a logged in user' do pending 'should be able to create stories' pending 'should ignore incorrect params' pending 'should correctly respond after a story is created' pending 'should handle the errors correctly' end context 'when user is not logged in' do it_should_behave _ like 'a logged out user' end # We can have other roles and the context will be similar but # instead of sign_in the user we can sign_in an admin, for # example, and have the same behaviour. context 'when user is logged in as a normal user' do # Assume that the sign_in method will login the user and stub that user # on the controller. That way we can refer to it on the tests. before { sign_in user } it_should_behave _ like 'a logged in user' end end end In this case, we will not base our tests on data. If we did so, we would end up duplicating the same logic several times. Instead, we are going to stub the method like this: Let me show you how. spec/controllers/places_controller_spec.rb require 'rails_helper'\r\n\r\ndescribe StoriesController do\r\n\r\n  let(:user) { create(:user) }\r\n\r\n  describe '#create' do\r\n    \r\n    # Here, we will use shared_examples_for because we can have multiples roles that \r\n    # behave the same for this action, and we don't want to repeat the tests.\r\n    shared_examples_for 'a logged out user' do\r\n      \r\n      it 'should not be able to create stories' do\r\n        expect {\r\n          post(:create, {story: {content: 'Some content', draft: true}})\r\n        }.to_not change(Story, :count)\r\n\r\n        expect(response).to redirect_to(root_url)\r\n      end\r\n    end\r\n\r\n    shared_examples_for 'a logged in user' do\r\n      it 'should be able to create stories' do\r\n        valid_params = {content: 'Some content', draft: true}\r\n\r\n        # With this we can make sure that the correct Service is being called \r\n        # with the correct parameters. We leave the job to the Service \r\n        # anyway to make sure that the service exists with those parameters.\r\n        expect(CreateStoryService).to receive(:run).with(valid_params).and_call_original\r\n\r\n        expect {\r\n          post(:create, {story: valid_params})\r\n        }.to change(user.stories, :count).by(1)\r\n        # In this test we can make sure a post is created and for the correct user.\r\n      end\r\n\r\n      it 'should ignore incorrect params' do\r\n        valid_params = {content: 'Some content', draft: true}\r\n        invalid_params = {some: 'Some'}\r\n\r\n        expect(CreateStoryService).to receive(:run).with(valid_params).and_call_original\r\n\r\n        post(:create, {story: valid_params.merge(invalid_params)})\r\n      end\r\n\r\n      it 'should correctly respond after a story is created' do\r\n        valid_params = {content: 'Some content', draft: true}\r\n\r\n        expect(CreateStoryService).to receive(:run).with(valid_params).and_call_original\r\n\r\n        post(:create, {story: valid_params})\r\n        \r\n        expect(response).to redirect_to(assigns[:story])\r\n      end\r\n\r\n      it 'should handling the errors correctly' do\r\n        params = {draft: true}\r\n\r\n        # Assume that the story must have a content.\r\n        expect(CreateStoryService).to receive(:run).with(params).and_call_original\r\n\r\n        post(:create, {story: params})\r\n        \r\n        expect(assigns[:story].valid?).to be_falsey \r\n        expect(response).to render_template(\"new\")\r\n      end\r\n    end\r\n    \r\n    context 'when user is not logged in' do\r\n      it_should_behave_like 'a logged out user'\r\n    end\r\n\r\n    # We can have other roles and the context will be similar but \r\n    # instead of sign_in the user we can sign_in an admin, for \r\n    # example, and have the same behavior.\r\n    context 'when user is logged in as a normal user' do\r\n      # Assume that the sign_in method will login the user and stub that user \r\n      # on the controller, that way we can refer to it on the tests.\r\n      before { sign_in user } \r\n\r\n      it_should_behave_like 'a logged in user'\r\n    end\r\n  end\r\n\r\nend 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 require 'rails_helper' describe StoriesController do let ( : user ) { create ( : user ) } describe '#create' do # Here, we will use shared_examples_for because we can have multiples roles that # behave the same for this action, and we don't want to repeat the tests. shared_examples _ for 'a logged out user' do it 'should not be able to create stories' do expect { post ( : create , { story : { content : 'Some content' , draft : true } } ) } . to_not change ( Story , : count ) expect ( response ) . to redirect_to ( root_url ) end end shared_examples _ for 'a logged in user' do it 'should be able to create stories' do valid_params = { content : 'Some content' , draft : true } # With this we can make sure that the correct Service is being called # with the correct parameters. We leave the job to the Service # anyway to make sure that the service exists with those parameters. expect ( CreateStoryService ) . to receive ( : run ) . with ( valid_params ) . and_call_original expect { post ( : create , { story : valid_params } ) } . to change ( user . stories , : count ) . by ( 1 ) # In this test we can make sure a post is created and for the correct user. end it 'should ignore incorrect params' do valid_params = { content : 'Some content' , draft : true } invalid_params = { some : 'Some' } expect ( CreateStoryService ) . to receive ( : run ) . with ( valid_params ) . and_call_original post ( : create , { story : valid_params . merge ( invalid_params ) } ) end it 'should correctly respond after a story is created' do valid_params = { content : 'Some content' , draft : true } expect ( CreateStoryService ) . to receive ( : run ) . with ( valid_params ) . and_call_original post ( : create , { story : valid_params } ) expect ( response ) . to redirect_to ( assigns [ : story ] ) end it 'should handling the errors correctly' do params = { draft : true } # Assume that the story must have a content. expect ( CreateStoryService ) . to receive ( : run ) . with ( params ) . and_call_original post ( : create , { story : params } ) expect ( assigns [ : story ] . valid ? ) . to be_falsey expect ( response ) . to render_template ( \"new\" ) end end context 'when user is not logged in' do it_should_behave _ like 'a logged out user' end # We can have other roles and the context will be similar but # instead of sign_in the user we can sign_in an admin, for # example, and have the same behavior. context 'when user is logged in as a normal user' do # Assume that the sign_in method will login the user and stub that user # on the controller, that way we can refer to it on the tests. before { sign_in user } it_should_behave _ like 'a logged in user' end end end If we repeat these techniques all over our app, we’ll be several steps closer to getting a well-tested and high-quality code. Hope you liked it! Next time, we’ll look at how to test AngularJS applications, stay tuned!", "date": "2015-12-09"},
{"website": "Moove-It", "title": "devsnack-9-devs-medley", "author": [" Agustín Cornú "], "link": "https://blog.moove-it.com/devsnack-9-devs-medley/", "abstract": "DevSnack #9: XP, the birth of a gem, new feature in Swift 2.0 and a bit of Haskell. A medley of dev topics, languages and experiences for this week’s DevSnack. #1 – Is TDD dead? Last year, DHH ( @dhh ) made a controversial statement about TDD. It triggered many replies from Agile and XP advocates, one of them was this post from Uncle Bob ( @ unclebobmartin ). #2 – Pairing with junior developers In this article Sarah Mei ( @ sarahmei ) gives some tips on how to train junior developers using pair programming. It’s focused on junior devs, but the same tips can be useful for teaching anything using pair programming. #3 – Pundit inception Pundit is a simple gem for authorization and Jonas Nicklas ( @ jonicklas ) describes how it works and why his team decided to build a gem from scratch. #4 – Mixins and Traits in Swift 2.0 Sometimes inheritance and interfaces are not enough – Matthijs Hollemans ( @mhollemans ) gives some cool examples and introduces a new feature from Swift 2.o. #5 – Haskell love Do you want to have fun while learning a new programming language or even fall in love with it? Zuzana Dostalova ( @ Jocinka ) describes how she sees Haskell from a rubist point of view and why she fell in love with it. DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2015-10-02"},
{"website": "Moove-It", "title": "patterns-and-principles-of-software-engineering", "author": [" Cristian Dotta "], "link": "https://blog.moove-it.com/patterns-and-principles-of-software-engineering/", "abstract": "DevSnack #17: Both patterns and principles are sometimes forgotten due to all the awesome frameworks and gems. The following list of articles will refresh these concepts and will expose the right way to apply them. #1 – SOLID Design Principles – Dependency Injection In this article Sandy Metz ( @sandymetz ) shows how one of the SOLID principles can turn the code more testable and beautiful. “Not as painful as it sounds…” . #2 – The Open Closed Principle This post reminds us one of the most important principles of software engineering: the Open Closed Principle (OCP). Robert Martin ( @unclebobmartin ) talks about the benefit of OCP, and when should we apply it. Avoid Shotgun Surgery ! #3 – 7 Patterns to Refactor Fat ActiveRecord Models Discover 7 patterns to apply on ActiveRecord models in this article written by Bryan Helmkamp ( @brynary ). #4 – Null Object Pattern in Ruby Pretty useful and simple pattern explained by Franzé Júnior ( @franzejr ). Don’t ask for nothing! #5 – The Strategy Pattern Doug Yun ( @dougyun ) makes a clean implementation of Strategy Pattern on Ruby. He also explains both when and why is useful and beautiful. DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2015-11-27"},
{"website": "Moove-It", "title": "announcing-ruy", "author": [" Gian Zas "], "link": "https://blog.moove-it.com/announcing-ruy/", "abstract": "We recently released Ruy, a Ruby library which we define as a lightweight rules evaluator . We readily admit that this definition doesn’t exactly explain what Ruy is about, or how it works, not because it’s hard to understand, but because we can’t really find the right form of words to describe it. The use case Some time ago, one of our clients challenged us to build a loyalty program system, capable of letting card issuers define their own programs, receive incoming transactions from consumers, calculate reward points and, finally, allow consumers to exchange the points they had earned for cash discounts. Card issuers would configure their loyalty programs via web forms, define which conditions transactions from consumers had to meet and how many points would be earned. Here are two example loyalty program configurations: 1 reward point every U$S 10 spent at a particular restaurant. For purchases at Vintage Clothing Bt, offer 30 reward points for purchases over U$S 200, and 12 reward points for purchases over U$S 100. Looking at the other side of a transaction, when someone has dinner at Mario & Luigi’s Restaurant, the system receives the payment details with information about the relevant merchant, amount spent and card details. The link between those two sides needs to be something that enables us to define all the conditions, receive the transaction information, evaluate the conditions, and then return the appropriate ratio of points to amount spent. That link is Ruy. The rise After considering all these requirements, we decided to build a library that provided a way to define boolean conditions, receive a context containing the information needed to evaluate the conditions, and produce the required result at the end. With Ruy, the first example turns into this: Ruby # 1 reward point every U$S 10 spent at a particular restaurant.\r\n\r\nrestaurants = Ruy::Rule.new\r\n\r\nrestaurants.eq 'Restaurant', :merchant_burden # matches purchases at restaurants\r\n\r\nrestaurants.outcome [1, 10] # points / amount ratio = 1 / 10 1 2 3 4 5 6 7 # 1 reward point every U$S 10 spent at a particular restaurant. restaurants = Ruy :: Rule . new restaurants . eq 'Restaurant' , : merchant _ burden # matches purchases at restaurants restaurants . outcome [ 1 , 10 ] # points / amount ratio = 1 / 10 The second example turns into this: Ruby # For purchases at Vintage Clothing Bt, offer 30 reward points for purchases\r\n# over U$S 200, and 12 reward points for purchases over U$S 100.\r\n\r\nvintage_bt = Ruy::Rule.new\r\n\r\nvintage_bt.eq 'VINTAGE-CLOTH-BT', :merchant_code\r\n\r\nvintage_bt.outcome 30 do\r\n  greater_than 200, :amount # 30 points when amount > 200\r\nend\r\n\r\nvintage_bt.outcome 12 do\r\n  greater_than 100, :amount # 12 points when amount > 100\r\nend 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # For purchases at Vintage Clothing Bt, offer 30 reward points for purchases # over U$S 200, and 12 reward points for purchases over U$S 100. vintage_bt = Ruy :: Rule . new vintage_bt . eq 'VINTAGE-CLOTH-BT' , : merchant _ code vintage_bt . outcome 30 do greater _ than 200 , : amount # 30 points when amount > 200 end vintage_bt . outcome 12 do greater _ than 100 , : amount # 12 points when amount > 100 end When a sales transaction comes in, it must also be evaluated against those programs. Ruby restaurants.call({ amount: 250, merchant_code: 'Mario & Luigi',\r\n                   merchant_buden: 'Restaurants' })\r\n\r\n# => [1, 10]\r\n\r\n\r\n\r\nrestaurants.call({ amount: 125, merchant_code: 'VINTAGE-CLOTH-BT',\r\n                   merchant_buden: 'Clothing' })\r\n\r\n=> 12 1 2 3 4 5 6 7 8 9 10 11 restaurants . call ( { amount : 250 , merchant_code : 'Mario & Luigi' , merchant_buden : 'Restaurants' } ) # => [1, 10] restaurants . call ( { amount : 125 , merchant_code : 'VINTAGE-CLOTH-BT' , merchant_buden : 'Clothing' } ) = > 12 At its core, Ruy is essentially no more that what is demonstrated, here. Of course, there are several condition methods besides eq and greater_than, custom attributes at rule level, and fallback values that can be defined when no outcome condition matches. Don’t forget to take a look at the project’s README . The expansion After using Ruy for a while, as part of the loyalty system previously mentioned, another client needed to process events coming from certain IoT devices and undertake certain actions, depending on what the owners of those devices had configured. Ruy seemed to be a good fit for that project, as well, so we decided to extract it from the project in which it was being incubated and take the opportunity to make it available to the wider community. From its inception to its current state, we strove not to divert Ruy from its core purpose, being a tool to define programmatically a set of conditions and outcome values, and evaluate a context against them. At the time of writing, Ruy is running as a key part of two systems, for clients in two different industries, processing large numbers of incoming events and payment transactions. The future We have a few ideas about the future direction of the library and it’d be great to hear your thoughts about Ruy. Its source code is relatively easy to follow, so please take a look at it and get back to us with any feedback you have. Links Project page Github repository", "date": "2015-10-16"},
{"website": "Moove-It", "title": "bulk-inserts-patching-multistasking-and-more", "author": [" Gabriel Fagundez "], "link": "https://blog.moove-it.com/bulk-inserts-patching-multistasking-and-more/", "abstract": "DevSnack #15: A medley for dev people. Learning about how to make the magic go away, build things good, fast and cheap, bulk inserts and more. #1 – Bulk Inserts in ActiveRecord Have you ever been in a situation where your app needs to insert hundreds of rows at a time? Since ActiveRecord doesn’t support it native, we can implement more than one workaround to fix this lack of ability. If we want to keep the code clean, we can also use this wonderful gem, created by Jamis Buck ( @jamis ). #2 – Please. Don’t Patch Like An Idiot. Modifying HTTP resources isn’t new. Most of the existing APIs provide a way to modify resources. They often provide such a feature by using the PUT method on the resource, asking clients to send the entire resource with the updated values, but that requires a recent GET on its resource, and a way to not miss updates between this GET call, and the PUT one. In this post, William Durand ( @couac ) offers a set of tips in order to use properly the PATCH HTTP verb. #3 – Make the Magic go away. In this article, Robert Martin ( @unclebobmartin ) advices about the implications of choosing a software framework. Basically, never buy magic! Before you commit to a framework, make sure you could write it. #4 – The Multitasking Misnomer When we learn to multitask, we’re not training ourselves to focus on more things at once, we’re just teaching ourselves to do things without requiring our focus. In this article Margaret Pagel (@ MargaretPagel ) share with us her experience multitasking! #5 – You Can Have It Good, Fast or Cheap. Pick Three! Most software developers have heard the line “You can have it good, fast or cheap. Pick two.” This idea relates back to the iron triangle of software development. Since we love quality, but we know that we need to build things fast and cheap due to the requirements of the market, we recommend this article of BrainsLink ( @brainslinks ) DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2015-11-13"},
{"website": "Moove-It", "title": "how-to-become-a-better-developer", "author": [" Matías Cubero "], "link": "https://blog.moove-it.com/how-to-become-a-better-developer/", "abstract": "DevSnack #12: As a software developer you need to constantly improve yourself. Learning more programming languages is only the tip of the iceberg. Communication, time management, and being a team player are also crucial aspects. In this DevSnack we’ll show you how to become a better developer. #1 – How to be a great developer Peter Nixey ( @peternixey , CEO of Coca-Cola funded, Twistilled.com, entrepreneur & YC alumn) establishes some interesting points to be a great developer. He affirms that “your value is in how you move your project forward and how you empower your team to do the same” . #2 – Time Management Tips for Developers Managing efficiently the time is crucial. The more efficient we are, the more goals we accomplish. Alexander Fedorenko ( @avfedorenko ) describes some really awesome practices about time management. Check it out! #3 – The 4 More Important Skills for a Software Developer Do you think that developers should be specialized in a particular technology? Well, John Sonmez has an argument against. He affirms that “if you can solve problems, learn things quickly, name things well and deal with people, you will have a much greater level of success in the long run that you will in specializing in any particular technology”. Do you have the same feeling, after John’s argument? #4 – Developers and Communication As developers we’re drawn to work that requires focus and isolation. Thus, communication isn’t typically one of our best skills. Sam Hernandez ( @sam_h ) analyzes how developers communicate and points out a few helpful points to improve our skills. #5 – 40 Blogs that Every Software Developer Should Be Reading Last, but not least, developers should always be up to date. On this post you can find a list of 40 blogs that every software developer should read! DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2015-10-23"},
{"website": "Moove-It", "title": "introducing-shakejake-ruby-and-iot", "author": [" Gustavo Armagno "], "link": "https://blog.moove-it.com/introducing-shakejake-ruby-and-iot/", "abstract": "ShakeJake is a wearable device that unobtrusively detects handshakes. It has been conceived and developed by Moove-it, who brought together a cross-disciplinary team of electronic engineers, an iOS developer, a Ruby developer and a UX designer for its creation. Although Moove-it’s main focus is providing Ruby, Java and mobile development services, we are always dreaming up new products to build. On this occasion, we wanted to take something innovative to RubyConf San Antonio , that combined both Ruby and IoT technologies. The guiding principle that led to building this particular wearable device was: “how to make networking less scary and more fun”. The product idea we came up with was a mobile app that automagically detects each time the user shakes someone’s hand, keeps track of handshakes and encourages both parties to exchange contact information. To solve the automagical thing, we decided to build an electronic wristband using capacitive touch sensors, the same principle as underlies the touchscreens used in most smartphones. When the wearable detects a handshake, it sends a bluetooth message to a smartphone connected to the device. A mobile app installed in the phone receives the bluetooth signal, which then triggers the app’s logic. In order to keep track of handshakes and save contact data, the mobile app connects with a server endpoint, implemented in Ruby. The fun part comes in the form of Jake, a digital narrator who chimes in when the system detects a handshake, encouraging both individuals to share their contact information. Let’s get into the technical details. The hardware Note: I know almost nothing about electronics. However, I asked the electronic engineers to explain the secret behind the magic to me and, after filtering out some of the more complex technicalities, it is pretty straightforward to understand. The circuitry embedded in the wristband is shown, below. It basically consists of a microcontroller (an Arduino), two metallic plates serving as a capacitor, a multiplexer, a sample-and-hold circuit, an analog-digital converter and stuffing circuits. The x pF capacitor represents the capacitance between the two plates at the wrist. One plate is connected to one of the microcontroller’s ground pins and the other to one of the inputs that are able to take analogic measurements. The sample-and-hold (S&H) circuit samples the varying voltage emitted by the input signal and holds its value at a constant level in the 14 pF capacitor until the reading is done. When the ADC finishes reading the sample, the 14 pF capacitor needs to be discharged in order to be able to take a new measurement. This is done by the multiplexor, which connects the capacitor’s input to ground, allowing it to get a 0V state again. ShakeJake works by measuring the capacitance between the two adjacent plates (x pF capacitor), using the internal sampling capacitor (14 pF) as a bridge measurement system. In order to measure the capacitance between the plates, the algorithm embedded in the microcontroller makes a two step iteration: charging and measuring . Step 1: Charging When the pull-up resistor is enabled (ON), the external capacitor (x pF) between the plates is charged up to 3.3V. In parallel, the multiplexer switches its input to ground and discharges the measuring capacitor (14 pF) in order to prepare it to take a new measurement. Step 2: Measuring Once both capacitors reach a steady state, the pull-up resistor is disabled (OFF) and the multiplexer connects the input signal to the ADC. The capacitors get connected in parallel, causing some current to flow from the external plates (x pF) to the internal capacitor (14 pF). The voltage read by the ADC depends on the gap between both capacitances. The presence of an object over the external plates (x pF) modifies the amount of energy that can be stored in them and, as a consequence, the electrical current that is going to flow from them to the internal capacitor. The crucial factor is that objects sharing the same material characteristics, like human skin, produce similar variations in the aforementioned amount of energy, so we can identify the presence of certain materials by measuring the amount of energy that flows from the plates. The algorithm running in the microcontroller performs this identification and classification process, allowing ShakeJake to detect the proximity of the hand wearing the wristband to any hand that is presented to it. The procedure defining the threshold that identifies a handshake is quite straightforward. First, the microcontroller takes a training data set, divided in two groups, one that represents handshake measurements, and the other representing the opposite. If we accept that each group follows a gaussian distribution, we can model its expression with its mean and variance. When we take a new measurement, we want to respond to the question “given this new value, is it probable that the user is shaking hands?”. Using the previously determined probability distributions, we can find the answer quite easily: if the distribution that corresponds to the handshake measurements is larger than the other one, when we evaluate them at the measurement value, then it is probable that we are shaking hands with someone. To make this process more efficient we identify the point at which those two distributions are equal at the calibration stage. Then, if the new measurement is greater than that point, we are shaking hands, otherwise we are not. The mobile app The wrist-wearable device uses Bluetooth Low Energy (BLE) to communicate with the smartphone (an iPhone). The main difference between BLE and Bluetooth Classic is that BLE’s standard version is much more limited in scope, when compared to Classic’s, and thus requires very low power to transmit messages. This feature makes BLE a better choice to use on devices with very limited power, like wearables. Another benefit of using BLE is that, unlike Classic, you don’t need to pass any special Apple certification to use the protocol. To create accessories based on Bluetooth Classic, you need to join Apple’s MFi licensing program and only use connectors and components authorized by Apple. Other than that, ShakeJake’s mobile app is a pretty standard Swift app (latest version) with only a limited number of views. The main job of the logic is to synchronize the mobile app with the wrist’s three states: PAUSED, CALIBRATING and SAMPLING. First, the user needs to connect the app with the wristband. This is done from the Sampling view, by tapping on the Settings option and, once in the Settings view, choosing to pair with the device. The app always makes sure that, when the user is in the Settings view, the wristband is in PAUSED mode. Once the app connects to the wristband through the BLE channel, the user has the option of calibrating the device, from the Settings view. If that option is selected, the app sends a “calibrate” command to the wristband to start the calibration process. This action puts the wristband in CALIBRATING mode. Authentication to device calibration’s flow Calibrating the system means shaking someone’s hand to correctly classify human skin under the existing environmental conditions at that location. This is fundamental, because changing weather conditions—and, in particular, humidity—affects skin conductance and may lead to an inaccurate classification. Once the device has been successfully calibrated, the app switches the wristband back to PAUSED mode. When the user leaves Settings, the app switches the device’s state to SAMPLING, which then allows it to start detecting handshakes. The user can go back to Settings at any time and the workflow will repeat itself. When the device is in SAMPLING mode and detects a handshake, it sends a message containing the word “HAND” to the app. When it detects the end of a handshake, it sends a “JAKE” and, in a new line, the duration of the handshake in milliseconds. This action allows the app to trigger Jake’s voice and open the Save Contact view. Handshake detection to save contact’s flow In the Save Contact view, the user can enter the other individual’s name, email and Twitter handle. The app then submits this data to the server, running in Ruby. The Ruby backend The backend API has been implemented using Angus , a lightweight open-source framework aimed at easily developing RESTful APIs. The API registers contacts and keeps track of handshakes. One of the benefits of using Angus is that it automatically generates the API specification straight from the code, without the need to write any additional documentation. ShakeJake’s API specs can be found at https://moove-it.com/shake_jake/doc/0.1. The number of handshakes is accumulated and stored in a database in order to be accessed and shown on ShakeJake’s landing page. This feature has been specially developed for the RubyConf. The goal here is to let the world know how many people we meet at the conference. The backend service uses Rusen to log errors and send them via email. Error notifications include: information about the current request, session, environment and a backtrace of exceptions. Conclusion The whole system is a minimum viable product (MVP)—which also served as a feasibility test—aimed to test the product’s value hypothesis, which is: people would feel more motivated to meet new people if they had an app that counted the number of handshakes they received and registered new contacts in a fun and interesting way . We believe that the RubyConf presents us with a great test scenario to evaluate this MVP, where we can learn from a real use case and interact with lots of potential adopters. Future work We foresee a lot of potential uses for this technology that go beyond networking during events. One potential use is to enhance the existing market for digital business card apps, by offering greater accessibility. The system’s main input is tactile and one of the main interaction channels is auditory. In the future, it could incorporate voice input and be used by visually challenged people as a substitute tool for similar apps already in existence. Another possible use could be, where two people wearing wristbands shake hands, turning the system into a secure and fun mechanism for them to exchange information. A third use we have thought of, is using a similar mechanism to detect different behavioural patterns and even to recognize people or objects that users have already touched before. This sounds more like science fiction, but the technology is out there and Disney Research has published a paper about this very topic. We are currently working on building something similar that can be transportable, so watch this space! Check out how many handshakes we’ve done so far at ShakeJake’s landing page.", "date": "2015-11-09"},
{"website": "Moove-It", "title": "custom-server-monitoring-tool-using-scala-akka", "author": [" Enrique Rodriguez "], "link": "https://blog.moove-it.com/custom-server-monitoring-tool-using-scala-akka/", "abstract": "The ability to monitor servers and applications is a key skill for any software development provider: it’s important to include it within the range of services we offer our clients, but it’s just as important for our own internal use. As ever, selecting the right tool for each task represents one of the most important elements for any successful software development project. And, in this case, we felt the need to develop our own flexible monitoring tool, made up of several different elements. In this article, we explain why we took up the challenge of developing our own monitoring solution, how we chose each of the tools that we are using, and how we made all the tools work together. Our motivation Monitoring is a peculiar task, because it’s by no means as generic and repetitive in different situations, as one might think. It has to deal with the challenges presented by different hardware, contrasting operating systems, applications created using diverse technologies, and so on. But that’s not all it has to deal with: different customers might be interested in monitoring different data, and might also want different analyses to be derived from that data. The configuration process also needs to be very intuitive, because once a monitoring solution is installed, it’s going to be maintained by administrators, not developers. That’s why a monitoring tool also needs to take into account user experience. Needless to say, installing and configuring monitoring tools is something you have to do regularly, so implementation needs to be really quick and easy. There are plenty of solutions already out there, that are very stable and reliable. However, this number reduces significantly, if you only consider the open source options and take into account all the challenges, mentioned above, that need to be tackled. As a result, we decided it would be worth creating our own tool, in order to investigate just how far a monitoring tool can go, in terms of its flexibility, following the open source principles. The requirements We wanted to: Select which hosts to monitor; and be able to start and stop monitoring, on demand Configure which metrics to monitor in each host, dynamically Configure collection frequency for each metric, independently Save all information collected for analysis, later Configure alerts to notify people when some condition is met regarding a metric The Architecture In the following sections we’re going to give a high level description of how our solution was designed and implemented. If you want to read this in greater detail, you can find the source code in github . Based on our requirements, we chose a fairly typical architecture for a monitoring solution. The architecture we chose is built on two base entities: Agent and Principal. Agent One agent is installed in each host we want to monitor. Agents provide us access to all kinds of metrics from within the machine. As we want to configure which metric we want to collect on each host, dynamically, it would be awkward if we had to deploy, modify configuration files, restart, or edit something directly in the monitored host.  As such, we designed agents so they can  change their behavior in runtime. They are even able to start collecting completely new metrics, without us having to change their code. Once installed, agents open a communication channel, via which only a principal can send instructions. These instructions tell the agent which metrics to collect and how often. Then, each agent periodically gathers these values and reports them back to their principal. In summary, agents do the following: Run on monitored hosts Dynamically load collection strategies Receive and react to directives from their principal Gather information and send it to their principal Principal There is one principal per agent (meaning one principal per monitored host). Each principal is responsible for the interaction with a single agent. To start with, all the principals can be run from one machine, but we should implement principals in a scalable way, so that their number can grow indefinitely, which we can support with horizontal scaling. For example, when we want to start collecting a new metric in a host, or configure an alert for when a metric is above a certain value, we tell the corresponding principal. The principal takes care of everything required: communicating the changes to the agent, and saving the newly created configuration (just to persist between system restarts). In order to do this, we use a REST interface so that we can configure every host, metric and alert. Interactions with the agent include receiving the metrics that have been collected, so it is important to note that the communication between agent and principal flows in both directions. All the information received from the agent is immediately saved into a reliable database, that has to be able to cope with several principals sending multiples metrics and also has to respond quickly to complex queries for real-time analytics. In addition, as new metrics arrive, the principal monitors alert configurations and checks to see if any conditions are met, in which case the principal notifies the relevant parties. So a principal’s work includes: Notifying their agent to start or stop collecting metrics Saving the host configurations, both metrics and alerts Reading and saving all metrics received from their agent Monitoring to see if a metric condition has been met and alerting the relevant parties Visualization Collected data is meant to be analyzed. For this, we have to provide a visualization tool that can help users not only query, but also visualize in a meaningful way, the data that is being collected. Basically, we need an application that can display charts for custom queries and create dashboards. Implementation Akka Given the communication requirements between agents and principals, and need for  simple scalability if we want to run a lot of principals, we chose the Akka framework to implement our requirements. Akka-remote provides a transparent communication API that helps to keep the amount of code to a minimum. Also, with Akka we can add more hardware resources to run the principals, while keeping the code untouched. Akka implements the actor model and it’s easy to map our reality to actors. That’s why, until now in this post, we have treated agents and principals as people, because we can directly model them as actors, and we refer to them as such. However, those aren’t the only actors we use in our implementation: agents spawn their own subordinates, called collectors, for instance – these are the actual units used for metric collection. Each collector collects one metric with a given frequency. On the principal’s side, we also have Watchers, who are responsible for comparing metrics being collected with the configured alerts. Now let’s see a more detailed description of how the system works, by looking at configuring a new host for the first time, called “Host A”. After installing the agent software in Host A, we have to register it in our configuration store. To do that, we use the configuration service. We call the REST interface, giving the host’s name. The system will react to that by saving the configuration into the store and also by creating a new principal actor. The principal will be responsible for communication with Host A. This process is shown in figure 1. Notice that saving configuration and creating the principal are two processes that run in parallel, and that’s why they are both numbered starting at 3. Figure 1 When a principal initiates, it sends a message to the configuration store, asking for the metrics to collect in Host A. There might already be metrics configured for this host, as this process happens after a system restart; or, it might be the case that we previously stopped collecting from Host A and now we want to start, again. In any case, the principal will connect to Host A via Akka-remote and tell the remote system to create a new agent actor, with the given configuration. The system in the remote host will respond with a reference to the newly created actor, for further communication. This flow is represented in figure 2. Figure 2 When the agent initiates, he calls the actor system to create one collector actor per metric to be collected. Then, each collector dynamically loads the collection strategy and schedules it for execution with the required frequency. Later on, other metrics can be configured and more collectors will be created, as a result. As shown in figure 3, when the collection strategy is finally triggered, the value collected is sent back to the principal and saved into the metrics store. Figure 3 Dynamic class loading In order to achieve the goal of changing the metrics being collected in runtime – without any downtime – collectors can dynamically load compiled classes. In this way, the strategy of collecting a new metric can be developed independently, dropped into a lib folder and loaded, without restarting anything. In addition to this extensibility-focused feature, we have also included a selection of basic metrics to be collected, such as: CPU, memory and disk usage. These depend on Sigar library, which includes native libraries that specifically deal with the details of the operating system and hardware. Task scheduling Task scheduling is managed by Akka. This is a very handy feature, letting us schedule recurring tasks with very little effort. Collectors schedule metric collection with a given frequency within the Akka framework. What they actually do, in reality, is tell the framework to periodically notify themselves, using a predefined message. When collectors receive this message, they fire up the collection strategy. Time series database When you look at the data being collected, it follows a pattern. All kind of metrics have a timestamp (the moment when the metric was collected) and a corresponding value for that timestamp. The value is typically numeric. So what we need is what is commonly called a Time series database (TSDB). For this task we chose InfluxDB , a very promising TSDB implementation, with a simple to use API. InfluxDB has no external dependencies, so it’s pretty easy to install and maintain. It also provides a very useful HTTP API which is straightforward and allows us to execute queries and write data. At present, there isn’t a Scala library to write and read from InfluxDB 0.9 (the latest, stable version), but given that the HTTP API so simple, we made it work using scalaj-http . REST service for configuration In order to configure the different hosts with their different metrics, we provide a REST interface, servicing operations to manage the three basic resources: hosts, metrics and alerts. Creating a host resource translates into instantiating an agent in the corresponding host. Deleting the host resource means that we stop collecting all metrics and kill the agent, along with all of their collectors. These two operations are independent from the agent software being installed on the host. This software is an extremely lightweight process, that just listens on the Akka-remote port and instantiates an agent, if needed. Below the host resource, following the REST protocol, we expose metrics and alerts. Creating or deleting metrics means starting or stopping metric collection in the host. The same applies for alerts; you create an alert resource to start monitoring a metric’s value, or delete an alert resource, to stop watching it. In this case, we chose Spray , which smoothly integrates with Scala and Akka, allowing us to define the REST services cohesively with our actor system. Persistence of configuration The hosts and their configuration need to be saved somewhere, so that they aren’t lost between system restarts. We chose to use one of our favorite databases: redis . Redis is not only blazingly fast, but also provides useful data structures that help us reduce the effort needed to manage our data. Alerts Although more types of alert can be added, for now, what we need is simply to send emails. Scala is relatively new, but it already has plenty of community contributions and there are libraries that cover almost any requirement you might have. There’s a library called courier , that covers emai ling. We chose to use courier because of its simplicity and stability. Visualization In order to display the InfluxDB data in charts, organized in dashboards, we found no better tool than Grafana . Grafana has an elegant and intuitive user interface. It integrates with InfluxDB, allows us to write the custom queries that we want to display, and mix charts with different data sources. Here’s a screenshot of a dashboard we created for a server that runs a Jenkins instance: Figure 4 Building and packaging It’s worth noting that the packaging process was tremendously simplified thanks to sbt-native-packager . Simply by configuring the desired options within sbt , you can then generate rpm and deb packages with a single command. These packages, once installed, configure a process to be run as a system service and generate all the necessary directory structure for binaries, configuration and logging, conforming to Unix standards. Testing As with any other project, our tool needs to incorporate automated tests. In the Scala world, you can choose between a couple of frameworks; we found Scalatest useful for this task. However, Akka actors require some infrastructure that can’t be easily created on the fly, without some help from the framework. That’s why we added the Akka-testkit to our test suites, which meant that we could finally cover every aspect of our code. Conclusions and future work Any monitoring tool needs to be designed to tackle multiple, diverse problems, and that makes it an interesting subject of study. We started by selecting the main tasks required to create a minimum viable product; we then designed a high-level architecture; and tailored it to our specific implementation. It’s still a work in progress, and more work is required to make it a complete product. For example: creating a UI for the host configuration that focuses more on UX; improving the flexibility of alert configurations; creating new alert channels, other than email; and defining failure management, should an agent ever fail. Akka is a modern framework that, when combined with Scala, resulted in a small and manageable code base, letting us focus on the monitoring problems rather than technology-related ones. Besides that, tools like InfluxDB, Redis and Grafana were integrated, in order to create a robust overall solution, based on great tools, each of which is focused on completing specific tasks.", "date": "2015-10-23"},
{"website": "Moove-It", "title": "its-all-about-architecture", "author": [" Ines Saint Martin "], "link": "https://blog.moove-it.com/its-all-about-architecture/", "abstract": "DevSnack #20: We know that writing quality software is hard and complex. It’s not only a matter of satisfying requirements, but the system should also be robust, maintainable, testable, and flexible enough to adapt to growth and change. In this DevSnack you’ll find 5 links discussing different software architecture types, and hopefully you’ll be able to choose the most appropriate for your Android application. #1 – Clean architecture In this article, Robert Martin ( @unclebobmartin ) explains the clean architecture concept and summarizes some rules to achieve this. Go ahead and read it carefully, it’ll save you a lot of headaches. #2 – Android architectures​ Thanos Karpouzis ( @Droid_Coder ) wrote this simple guide for MVC, MVP and MVVM on Android projects. #3 – MVP for Android One of the most popular architectures for Android applications is the MVP. In this post by Antonio Leiva ( @lime_cl ), you’ll find the most important features of this pattern. #4 – Goodbye Presenter, hello ViewModel! In this article Frode Nilsen ( @nilzor ) talks about how the Data Binding library changes everything. You’ll learn the advantages of using it, along with a code example. #5 – Code examples This repository showcases and compares different architectural patterns that can be used to build Android apps. DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2015-12-18"},
{"website": "Moove-It", "title": "ruby-testing-and-refactoring-goodies", "author": [" Adrian Gomez "], "link": "https://blog.moove-it.com/ruby-testing-and-refactoring-goodies/", "abstract": "DevSnack #7: Here are some tips and tools to help you improve readability, maintainability and reliability of your Ruby code and test suite. Enjoy our tips on Ruby testing. #1 – How good are your Ruby tests? Testing your tests with mutant If you think that having 100% code coverage in your tests is enough, think again. Andrzej Krzywda ( @andrzejkrzywda ) shows us how mutant can help us improve the reliability of our test suite. #2 – Feature testing with RSpec Feature testing is a key part of any testing suite, in this article Luke Morton ( @LukeMorton ) talks about his experience creating feature tests and shares some tips he found useful. #3 – Using RSpec Shared Contexts to Ensure API Consistency Some of the advantages of RSpec Shared Context are code readability and code reuse, but something that gets overlooked is that they can help in keeping things consistent! Read the article from Charlie Tanksley ( @charlietanksley ) to find out more. #4 – Extract a service object using SimpleDelegator Andrzej Krzywda ( @andrzejkrzywda ) strikes again with some useful tips on how to clean up those messy Rails controllers. #5 – Five Ruby Methods You Should Be Using In this article by Ben Lewis ( @fluxusfrequency ), you will find some less known Ruby methods that can help us improve our code. DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2015-09-18"},
{"website": "Moove-It", "title": "discover-the-awesomeness-of-d3", "author": [" Virginia Rodríguez "], "link": "https://blog.moove-it.com/discover-the-awesomeness-of-d3/", "abstract": "DevSnack #8: If it’s the first time you hear about D3 or if you are already working with it, but you think there is still a lot to learn out there, these five resources are worth taking a look! #1 – D3 Tutorials by Scott Murray If you want to start learning D3 and understand how it works, these tutorials by Scott Murray ( @alignedleft ) are a great place to start. From drawing basic things to binding data and working with axis and scales, you will really get D3 basics after reading this. #2 – For example In this article, based in a talk he gave, Mike Bostock ( @mbostock ) presents a selection of the favorite examples he built using D3 . Examples are a really good way to understand how things work and see what different kinds of visualizations you can build using this library. #3 – d3-legend Are you bored of making legends for your data visualizations? d3-legend is a a reusable D3 component wrote by Susie Lu ( @DataToViz ) that will help you. It provides easy generation of good looking legends (using colors, sizes and symbols) for your charts. #4 – God Help Me, I Hate d3.scale.category20 In this article Elijah Meeks ( @Elijah_Meeks ) talks about how from time to time you should spend some time thinking about color using in your visualization. He provides some good and simple advice on how to start on this topic. #5 – Getting to know Crossfilter Crossfilter is a javascript library that is useful for exploring large multivaritate datasets in the browser, and it is often used in conjunction with D3 for drawing charts. In this article, Peter Cook ( @animateddata ), explains the basic concepts behind it. DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2015-09-25"},
{"website": "Moove-It", "title": "introducing-architecture-design-reviews", "author": [" Gian Zas "], "link": "https://blog.moove-it.com/introducing-architecture-design-reviews/", "abstract": "The importance of introducing periodic architecture design reviews in your code review process. Stagnant design As a product evolves over several development cycles its design may become outdated. The design that seemed good enough during the first iterations may not be appropriate, as your understanding of the project and the underlying technology changes. Indeed, after several iterations, some design decisions that seemed reasonable during the early stages degrade over time. As a consequence, adding new features and maintaining the product start to become increasingly costly and error-prone. Only focusing on small and gradual changes, instead of challenging the high-level architecture, may generate unexpected issues, including cyclic dependencies between modules, duplicated responsibilities, unmanaged remote calls and cross-boundary violations. Design as an evolutionary process In an agile context, embracing change is a universal principle. We adapt to changes in requirements, priorities, processes, technology and so on. It’s unlikely that the initial design of a software product will remain acceptable after the surge of changes that a product faces during its entire life cycle. Given that software architecture design is part of a product, it must continuously evolve in order to adapt to these inevitable changes that happen during the life of every project. If the software architecture remains invariant throughout the development process, then the quality of the final product – including qualitative factors, like correctness, efficiency, accuracy, maintainability and flexibility – will most likely be diminished. You can’t see the forest for the trees When you start coding a new user story, you normally put yourself into ‘coding mode’ and start introducing changes to the existing codebase, taking the existing architecture design for granted. Sometimes, you need to stretch the boundaries of the architecture a little, but generally you are more focused on the immediate goals and lower-levels of abstraction. Even when you are in the refactoring stage within the Test Driven Development (TDD) red-green-refactor cycle, the underlying system architecture often remains unchanged. When another team member reviews the changes you have made to the codebase, the tendency will naturally be to focus in on the code being reviewed, and not to question the overall architecture. The reason for this is that, neither the TDD’s refactoring steps, nor conventional code reviews focus on architecture design. For example, let’s suppose we need to store two values that are very unlikely to change over time. Based on this assumption, the team decides to store them in a configuration file. Weeks later, the client realizes that it may be useful to allow a superuser to change these values via an admin panel. In response to this, the team decides to create a table to store these two values. Some time later, more and more configurable values are progressively required to be stored in the database. The team starts adding new columns to the table, but there’s a threshold beyond which the model becomes unmaintainable. At that point, it might be a better idea to step back, review the architecture design and introduce a key-value based structure. It is difficult to effect changes if you are just focused on reviewing the code. The sensation that something is going wrong with the architecture design may arise during regular code inspections. However, having periodic architecture design reviews allows the team to identify any problems, propose solutions and define a refactoring plan. The whole picture To incorporate incoming changes, the architecture design must first be inspected from a bird’s-eye view. Only by inspecting it in such a way will we observe dependencies between packages, classes and objects. This perspective allows to uncover design problems and elaborate what design refactoring is needed to improve the architecture. This point of view is generally not taken into account when the team is in coding mode. The code-refactor approach and the ideas that emerge from taking a more global perspective are normally richer than those coming out of a typical TDD cycle. Introducing Architecture Design Reviews into a project Architecture Design Reviews (ADRs) are complementary to conventional code reviews. The goal is, periodically, to assess the state of the system’s architecture design, to identify problems and opportunities for improvement, to propose solutions and to discuss an action plan. If the project has already started, and the idea of having periodic ADRs hasn’t been considered before, then a Retrospective meeting is probably the best place to propose the idea. The team can decide whether it is convenient to dedicate some time during the next sprint to an ADR and which members of the team will participate in the review. Architecture Design Review flow The first ADR starts by creating the architecture design diagrams, to catch up with the current state of the product. The first step in syncing the diagrams with the underlying code is to draw the codebase component and package diagrams . The resulting diagrams don’t need to conform with the UML specification. The whole point, is that the team can all recognize and understand the system architecture, so they can communicate using a common language. By reviewing the diagrams, other team members can spot design problems and collaborate in a more efficient way. If needed, the team can drill-down when reviewing these diagrams to review any hot spot in the codebase, without losing focus on the big picture. After identifying a list of design issues, the review team can decide which to tackle first. In the best case, if the list is short, they can decide to tackle them all. The next step is to discuss different solutions for each issue and agree on an action plan for actually refactoring the codebase. For any issues that are not too complex, the solutions can be implemented during the ADR. More complex changes should be converted into a user story and uploaded to the product backlog. Any changes to the codebase must be reflected back in the original diagrams. Having a shared and accessible place to store and update the diagrams is good practice. It doesn’t have to be too sophisticated: some sharpies, post-its and a wall can do a perfectly good job. However, it is important that the diagrams are kept updated. Outdated documentation is worse than not having any documentation, at all. Guidelines for Architecture Design Reviews If the team finds that the review has been helpful they can start to incorporate it into the development process. Four tips on how to conduct Architecture Design Reviews: Set how often you perform design reviews. Some teams choose to hold monthly reviews, while others prefer to hold reviews every two months. It is not necessary for the whole team to participate in every review. A pair of developers can be perfectly sufficient to carry out a review, successfully. It is recommended that one of them be an experienced developer. Rotate the review team. The whole team should be accountable for the system architecture. Make any required design changes during the current sprint. Only the first ADR should be see a significant number of changes. If the team undertakes regular reviews, the extent of the changes should decrease over time. Sustainable pace Carrying out regular Architecture Design Reviews prevents big code rewrites, helping programmers focus on creating user stories rather than struggling with the design. As a consequence, regular ADRs increase the odds that each sprint stays within the estimated deadline and allow the development velocity to be more predictable. Empowering all team members to identify problems, propose solutions and implement regular design reviews reduces the risk of miscommunication and fosters product ownership and commitment. Improving the system’s architecture design helps increase the product’s reliability and reduce the technical debt.", "date": "2015-08-26"},
{"website": "Moove-It", "title": "programming-games-with-ruby-and-html5", "author": [" JR González "], "link": "https://blog.moove-it.com/programming-games-with-ruby-and-html5/", "abstract": "DevSnack #11: If you’re not having fun… then you’re doing it wrong. How to build games with Ruby and HTML5 for this week’s DevSnack. #1 – How to make a video game (experience not required) Creating a video game is a daunting task. However, making a playable game is not as insane as you might think; it just takes a little bit of time and patience. Here’s a quick guide on how to make a (very simple) video game. No experience needed. See for yourself what Brandon Widder ( @ BrandonWidder ) has to say about it. #2 – Writing Games with Ruby Mike Moore ( @ blowmage ) states that the promise of making games is what brought many of us to programming. Games are fun, and writing games is fun. But isn’t game programming hard? Yes, it is. But also no. If you can code a web app you can make a game, especially with an awesome language like Ruby. #3 – Create stunning HTML5 particle effects with Phaser Particle Storm In this article, Emanuele Feronato ( @ triqui ) explains how to work with particle effects. There’s nothing as awesome as particle effects. That’s why you should absolutely get Phaser Particle Storm. #4 – Current state and the future of HTML5 games Browser games are, in the vast majority, developed using Flash technology, however more and more often we come across HTML5 productions. What is the reason behind the growing popularity of this technology, and why are game creators so willing to sacrifice Flash for HTML5? Check out this interesting article by Kamil Kaniuk & Mikołaj Stolars from Merixgames ( @ merix_studio ). #5 – Trends that will define the future of video games From the rise of gamer parents to transparent game design, Keith Stuart ( @ keefstuart ) & Jordan Erica Webber give us a step-by-step prediction of how games will be made over the next five years. DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2015-10-16"},
{"website": "Moove-It", "title": "what-is-coming-up-in-rails-5", "author": [" Pablo Ifran "], "link": "https://blog.moove-it.com/what-is-coming-up-in-rails-5/", "abstract": "While Rails 5 is still in the pipeline I went through its changelog and the source code to see what’s coming up. I got very excited about a bunch of new features I believe will make our lives easier! So, let’s start by looking at the changes. First of, the new version will require Ruby 2.2. So if you are thinking of upgrading your app to Rails 5, you should start with upgrading Ruby. Rake Tasks On previous versions of Rails, you have commands executed with rails and others with rake . In this new release, one of the features includes executing all Rake tasks using just Rails. The new format is similar to the older one: rails db : migrate . This move toward simplification will make both the framework and task automatization easier to learn. Action Mailer Regarding Action Mailer, the main change is that deliver and deliver ! methods were removed and replaced by deliver_later and it’s bang version. This change was introduced to make Action Mailer compatible with Action Job (see below). Also, all * _path methods have been removed to prevent introducing programming bugs when writing senders. Instead, Rails 5 will force us to use use the * _url methods. Another change is that templates now respects I18n fallbacks. In previous versions of Rails if you had a template called welcome . en - US . erb and another called welcome . erb even if you had set the en - US as the default language, the template welcome . erb would be rendered. Rails 5 will use the local option rather than the fallback one. So far, so good… and we’re just getting started. Action Pack Apart from Action Mailer, Action Pack also gets some interesting changes. Under development environment, when you browse to a non-existing route, Action Pack displays route information, letting the user filter the results. In previous versions this filtering was a pure Javascript regex trying to guess the route and was a little bit unreliable. Rails 5 improves this feature by getting accurate routing information directly from the Rails backend instead. Action Controller Now it is the turn for Action Controller. The novelty is a parameter of the protect_from_forgery method that allows prepending (or not) the forgery protection. As in previous versions, it also lets you add a conditional –e.g. like avoiding forgery protection in JSON requests. This is how: Allowing conditional protect from forgery Ruby protect_from_forgery prepend: false, unless: -> { request.format.json? } 1 2 3 protect_from _ forgery prepend : false , unless : -> { request . format . json ? } When this option is set to false, Rails will not prepend the protection. In the example, it will also turn the protection off for JSON requests. If true, the callback will be inserted in the first place of the callback chain. Action View There are some cool new stuff in the Action View, too. One of them is the possibility of naming partials using any character, not only alphanumeric ones. In previous versions, partials’ names must start with an underscore and then followed only by alphanumeric characters, numbers, or underscores. In Rails 5 we can use any character after the underscore. Another change is that helper methods like content_tag_for , div_for and so on were removed from the core and moved out to a separate gem, called record_tag_helper . Active Job Active Job, a great addition to Rails 4.2.0, also got some tweaks in the new version. Formerly a separate gem, Active Job was merged into Rails to serve as an adapter layer for the most popular queuing libraries around. Active Job allows us to change between inline jobs or delay jobs only by changing one line of code: From : Process jobs right away Ruby ActiveJob::Base.queue_adapter = :inline 1 2 3 ActiveJob :: Base . queue_adapter = : inline To : Process jobs using sidekiq Ruby ActiveJob::Base.queue_adapter = :sidekiq # Sidekiq for instance\r\n\r\n  # The following are the allowed adapters :backburner, :delayed_job, :qu, :que, :queue_classic,\r\n  #                                        :resque, :sidekiq, :sneakers, :sucker_punch 1 2 3 4 5 6 ActiveJob :: Base . queue_adapter = : sidekiq # Sidekiq for instance # The following are the allowed adapters :backburner, :delayed_job, :qu, :que, :queue_classic, #                                        :resque, :sidekiq, :sneakers, :sucker_punch In the new version, all the jobs inherit from a base class app/jobs/application_job.rb. This is to be consistent with the already existing structure of others components in Rails, like controllers, models and the like. The following is an example of how to use Active Job: app/jobs/welcome_email_job.rb Ruby class BackgroundProcessJob < ActiveJob::Base \r\n    queue_as :background_process\r\n\r\n    def perform(id)\r\n      # Perform the background process\r\n    end\r\n\r\n  end 1 2 3 4 5 6 7 8 9 10 class BackgroundProcessJob < ActiveJob :: Base queue_as : background _ process def perform ( id ) # Perform the background process end end Active Job is integrated with Action Mailer allowing us to easily send emails asynchronously. The methods deliver_now and deliver_later , and their bang versions, will use the preferred queue adapter. Active Record In my opinion, the most existing changes are in Active Record. First, let’s refer to belongs_to . Before Rails 5, given an Employee that belongs_to a Company, nothing prevented from creating a new Employee without specifying the Company it belongs to. In the new version, a validation error will be thrown if the associated record is not specified. If you migrate your app to Rails 5, this feature will remain disabled unless you specify the opposite. It will be enabled when creating new applications, though, or if you add the following line to the Rails configuration: This is the default for new rails applications Ruby config.active_record.belongs_to_required_by_default = true 1 2 3 config . active_record . belongs_to_required_by_default = true Second, another useful addition is ActiveRecord :: Base . suppress . This method prevents its class from being saved while executing the enclosing block. Example of suppress Ruby class Post < ActiveRecord::Base\r\n\r\n    has_many :followers\r\n    after_create -> { Notification.create!(post: self, followers: followers) }\r\n\r\n    def copy\r\n      # During the copy we want to avoid notifications from being saved\r\n      Notification.suppress do\r\n        # Perform a copy of the post.\r\n      end\r\n    end\r\n\r\n  end 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class Post < ActiveRecord :: Base has_many : followers after_create -> { Notification . create ! ( post : self , followers : followers ) } def copy # During the copy we want to avoid notifications from being saved Notification . suppress do # Perform a copy of the post. end end end Third, a much expected feature: now the or method is available for relations: Ruby Post.where('id = 1').or(Post.where('id = 2')) 1 2 3 Post . where ( 'id = 1' ) . or ( Post . where ( 'id = 2' ) ) And last, for migrations now it’s possible to specify a parameter : if_exists for dropping a table only if it exists. And there are a lot of bug fixes, and other features… I can’t wait to start using this new version of Rails! Looking forward to it!", "date": "2015-04-14"},
{"website": "Moove-It", "title": "future-css-guidelines-and-tools", "author": [" Nicolas Suarez "], "link": "https://blog.moove-it.com/future-css-guidelines-and-tools/", "abstract": "DevSnack #2: This week we bring you a collection of five links that will change your perspective on CSS. #1 – CSS Guidelines When working on large and complex applications with dozens of developers at the same time, it is important to have a unified standard to follow. We should always keep stylesheets maintainable, and a transparent and readable code. There are several techniques we must employ in order to satisfy these goals. CSS Guidelines is definitely an awesome document, full of recommendations, that will help us move in the right direction. This guide’s author is Harry Roberts ( @csswizardry ). We hosted Harry’s CSS workshop a couple of months ago. #2 – Quantity queries for css In this post, Heydon Pickering ( @heydonworks ) explains the importance of adapting our design to the content. He also shows how to make our site fit the content using pseudo classes. #3 – CSS Next As Maxime Thirouin ( @MoOx ), the author of @cssnext , states: “Prior to 2015, CSS was frustrating by not having any specification for features we were looking for. No variables, no math, no color manipulation and no customization. Things are going to change soon since a lot of work has been made by the W3C to write new specs to make our dev life easier. With cssnext , you can start using some of the new features today!” #4 – Styling SVG content CSS Sara Soueidan ( @SaraSoueidan ) wrote a remarkable in-depth article on how to style the contents of the SVG <use> element and overcome some challenges it brings. #5 – AXR Project As CSS crosses the boundaries of the mere cosmetics and penetrates into the lands of layout, one may ask the question: What’s the future of CSS and HTML, and what’s the reason for keeping them separate? AXR aims to unifying both into a new language, so called HSS. The platform is a little bit outdated and the website is begging for a redesign, but the idea is by no means less interesting. Maybe if we help the author, @veosotano , and contribute to the project we help improve the web! DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2015-08-14"},
{"website": "Moove-It", "title": "hybrid-mobile-development", "author": [" José María Aguerre "], "link": "https://blog.moove-it.com/hybrid-mobile-development/", "abstract": "DevSnack #19: Developing a mobile app could result in something difficult for someone with little to no background in mobile development. However, building a hybrid app with native navigation and web content simplifies the task. The following list of articles goes through some vital steps on this type of developments. #1 – Hybrid sweet spot: Native navigation, web content In this article David Heinemeier Hansson ( @dhh ), creator of Ruby on Rails and founder & CTO of Basecamp, explains the architectural decisions they took and what things changed between versions when building the Basecamp mobile app. #2 – WKWeb​View When developing for the latest versions of iOS (iOS 8 and later), developers at Apple recommend to use their latest class of Web Views, Mattt Thompson ( @mattt ) gives us a walkthrough between the differences of the older UIWebView and the new WKWebView , along with advice on how to communicate back and forth between Swift and Javascript, among other tips and tricks. #3 – WKWebViewTips Shingo Fukuyama ( @shingo_fuku ) shares some tips he learned while working with the Apple WKWebView. #4 – Android-Javascript Bridge On the Android side of things, Md. Minhazul Haque ( @mdminhazulhaque ) explains the basics of duplex communication between Android native code and Javascript running inside a WebView . #5 – A Concise Guide to Remote Debugging on iOS, Android, and Windows Phone Last but not least, as a developer you will have to make some debugging at some point. TJ VanToll ( @tjvantoll ) shows us different tools available to debug these hybrid monsters. DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2015-12-11"},
{"website": "Moove-It", "title": "js-frameworks-the-good-the-bad-and-the-ugly", "author": [" Patricio Maite "], "link": "https://blog.moove-it.com/js-frameworks-the-good-the-bad-and-the-ugly/", "abstract": "DevSnack #10: JS frameworks! The good, the bad, and the ugly. Tie-breaker comparisons for this week’s DevSnack. #1 – Why use JS Frameworks at all? JavaScript fundamentalists may argue pure JavaScript is better for a great number of reasons, while most of  the community prefers to delegate the deep-down code to Frameworks and just work on what they call ‘the real problem’.  See for yourself what David Walsh ( @ davidwalshblog ) has to say about it. #2 – Mithril vs The World Released in March of 2014 and having 83 contributors on GitHub, Mithril is worth to take a look at, as it claims to be the smallest, fastest and easiest JS Framework. Take a look at this post to see how convenient it may be for your purposes, compared to an Angular or React approach. #3 – Angular vs Ember Robin Ward ( @eviltrout ), tells us in his blog that Angular is simpler than Ember, but this simplicity comes along with some pitfalls he illustrates with examples. Check it out! #4 – Ember over Angular In this article , Paul Yoder ( @paulyoder ) explains how he found Ember, and why he feels that Angular let him down in some aspects. Not only a technical comparison but also an overview from a more human perspective. #5 – Showdown! Angular vs Backbone vs Ember Come along with Uri Shaked ( @urishaked ), while he contrasts the key features of these frameworks. Things like Backbone’s minimalism, Angular’s innovative approach and Ember’s performance. DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2015-10-09"},
{"website": "Moove-It", "title": "7-reasons-js", "author": [" Marianne Maisonneuve "], "link": "https://blog.moove-it.com/7-reasons-js/", "abstract": "DevSnack #16: A series of articles that follow the 7 reasons format and are meant to be food for thought. They expose different aspects of JavaScript and some of its frameworks: Meteor and MEAN Stack. This is why JavaScript. #1 – 7 Reasons Every Programmer Needs to Learn JavaScript Three things to do in your life: plant a tree, write a book and have a child. According to Dave Bush ( @ davembush ), if you are a programmer you should also include: learn JavaScript. #2 – 7 reasons why frameworks are the new programming languages This post encourages the use of frameworks by providing several reasons why they are the new programming languages. Read this post by Peter Wayner ( @ peterwayner ) and remember “Code is law”. #3 – 7 Reasons to Develop Your Next Web App with Meteor Discover 7 reasons to use Meteor framework in your JavaScript apps in this article written by David Turnbull ( @ dturnbull ). #4 – 7 Good Reasons to use MEAN Stack in your next web project Intended to promote the use of MEAN Stack, a full stack framework for JavaScript which combines Mongo, Express, Angular and Node; this article enlightens us with some of the advantages of using the framework. #5 – Which do you prefer and why: MEAN stack or MeteorJS? Not a 7 reasons article but serves as a conclusion to make you think and judge by yourself. This shows an answer to the question provided by Dan Dascalescu ( @ dandv ) about the differences between MEAN and Meteor. DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2015-11-20"},
{"website": "Moove-It", "title": "agile-software-design", "author": [" Agustin Daguerre "], "link": "https://blog.moove-it.com/agile-software-design/", "abstract": "DevSnack #21: With the Agile hype, software design has been somehow relegated in the priorities of the development team, sometimes having very little to none design invested in their code. In the following articles, we will see how to include design in the development process through some best practices and principles. #1 – You Aren’t Going to Need It Simplification is the key to good design and @ vkhorikov explains us how to apply the YAGNI principle to reach a good and simplistic design. #2 – Death by Over-Simplification​ In this article @hayim_makabee tackles the most common assumptions made by developers who tend to over-simplify their development processes. #3 – He who promises runs in debt Technical debt is a problem almost impossible to avoid, but as @ codinghorror shows us in this article, it can be cut down by periodically refactoring into a better design. Remember, interest on debts grow without rain . #4 – Keys to improve agile development Writing good code is very challenging. Writing good code in an Agile environment is even harder. Here @ chrisverwijs leaves us with 5 principles that all Agile developers must keep in mind to achieve code quality and agility. #5 – So is design dead? Is design really dead? In this paper @ martinfowler guides us through the changes suffered by software design to assure us that it is alive and well. DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2015-12-29"},
{"website": "Moove-It", "title": "2015s-edition-of-moove-its-agile-workshop", "author": [" Hernán Daguerre "], "link": "https://blog.moove-it.com/2015s-edition-of-moove-its-agile-workshop/", "abstract": "Between last Friday August 14th and Saturday 15th a new edition of Moove-it’s annual Agile Workshop was held. Initiatives of such nature go in line with our culture’s commitment to invest in our talent’s continuous improvement and growth. This year we welcomed Gabriel Ledesma and Gabriel Montero from Peregrinus as the Subject Matter Experts accountable for leading the hands-on coaching activities. This instance was particularly singular, as we proudly reached a record attendance of 20 in-house participants. From Web to Mobile Engineers, from UX Designers to Engagement Managers, everyone showed enthusiasm and a deep sense of commitment towards the collective and personal value this practice delivered. Throughout those intense and highly interactive two days, these practitioners displayed a wealth of experience in applying Agility ‘s best practices in an array of businesses and contexts to streamline the underlying processes and communication interactions immersed within software engineering environments. As part of the training dynamics, participants were introduced to several widely used techniques and artifacts leveraged by the developers while outlining their initial communication strategy. For example, during an Inception phase, the dev team will most likely lean toward modelling a Stakeholder Map to clearly identify, categorize and effectively manage communication across stakeholders, depending on the level of interest and influence in the outcomes of the project that they may have. Additionally, seeking early agreements and joining forces with the customer to, e.g. come up with a comprehensive Trade-off Slider (see Image 1) is always seen as a good practice, as it helps in setting up the right expectations upfront, enabling the team to gauge the key factors against each other, and, at the same time, allowing developers to acquire a better grasp of where the customer may or may not eventually have bandwidth for scope negotiation. Image 1: Trade-off Slider Example As a takeaway from the 2-days activities, among some of the most relevant tools and techniques presented over the course of the workshop, I’ve selected a few assets which deserve being referenced, and, furthermore, periodically revisited by the developers throughout the project’s cycle: The User Story Mapping is a technique/practice aimed to visually arrange and describe the flow of the envisioned product’s activities. The horizontal dimension represents a walking skeleton , a barebones but usable version of the product. Iterating through successive rows results in a more refined version of the product with additional functionality (see Image 2). Image 2: User Story Map Example The Vision Board is a simple, yet powerful, and easy-to-use visual artifact aimed to concisely convey core, yet summarised, information pertaining to the product to be modelled. The Improvement Tracking Theme (see Image 3) is another visual tool originated from the Lean principles, extensively promoted by highly effective teams which use Retrospective interactions as an opportunity to continuously seek for ways to improve on a given subject. The artifact should essentially be comprised of four quadrants: a) problem definition, b) desired state, c) short-term goals (at least two), and d) breakdown of necessary tasks to achieve those goals. Image 3: Improvement Theme Sketch As a closing thought, we would like to extend our appreciation to the Peregrinus team for their due diligence and contributions to foster Moove-it’s success story.", "date": "2015-08-28"},
{"website": "Moove-It", "title": "make-it-go-faster", "author": [" Maximiliano Arcia "], "link": "https://blog.moove-it.com/make-it-go-faster/", "abstract": "DevSnack #14: Web apps performance is crucial. If the app works fast the user experience is better, and this improves yours chances of success. Ruby on Rails code optimization and cleanup, Rack MiniProfiler and more on this week’s DevSnack. #1 – Web Performance is User experience Lara Swanson ( @lara_hogan ) explains how the efforts to optimize your website’s performance could have a great effect on the entire user experience. #2 – Web Performance Terms Alex Pinto ( @ ad3pinto ) created an amazing glossary with 50+ terms we need to understand on web performance. #3 – Ruby on Rails code optimization and cleanup Keeping your code clean and organized while developing a large Rails application can be quite a challenge, even for an experienced developer. Fortunately, Damir Svrtan ( @damirsvrtan ) outlines a whole category of gems that make this job much easier. #4 – The Secret weapon of Ruby and Rails speed Nate Berkopec ( @nateberkopec ) brings an exhaustive explanation about mini-profiler. Rack-mini-profiler (maintained by @samsaffron ) is a powerful Swiss army knife for Rack app performance. #5 – 20 Top Factors That Impact Website Response Time Three seconds may not seem like a long time, but it could be the difference between making the online sale and losing a customer! DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2015-11-06"},
{"website": "Moove-It", "title": "spicy-ux", "author": [" Sebastián Mantel "], "link": "https://blog.moove-it.com/spicy-ux/", "abstract": "DevSnack #4: Simplifying IoT, the future of design in tech and UX rubbish. Learn about this and other UX news on this week’s DevSnack. #1 – Simplifying IoT Richard Caddick ( @richardcaddick ) proposes an interesting perspective shift on the way the Internet of Things ( IoT ) has been thought. He proposes that the emphasis should be on the individuals, rather than things. He also argues that this conceptual change may enable the development of better IoT solutions. #2 – Zero UI In this article from FastCompany, @drcrypt , summarizes Andy Goodman’s ( @goodmania ) ideas on Zero UI, or interfaces that are no longer constrained by screens. #3 – The Future of Design in Technology Julie Zhuo ( @joulee ), Product Design Director at Facebook, proposes 10 predictions about where design in Silicon Valley will be in the next 10 years. #4 – What Is The Next Frontier For Human Computer Interaction? John Siebert ( @tranquilblue ) explains the future of human-computer interaction: “Imagine this – you are playing a video game and want to interact with the other character. But, instead of using the mouse, you simply rest your eyes on that particular character and almost instantly, you get a response from it.” #5 – UX bullsh*t Taken seriously, UX design’s philosophy, processes and principles can be a powerful force to maximize the probability for a product to be accepted by a target audience. Unfortunately, the term has become a buzzword that rarely serves its main principles. This hilarious site, created by @3oheme , captures the feeling of those who truly takes the discipline in a serious matter when they read mainstream articles about UX. DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2015-08-28"},
{"website": "Moove-It", "title": "reacting-with-react-js", "author": [" Santiago Noguera "], "link": "https://blog.moove-it.com/reacting-with-react-js/", "abstract": "DevSnack #13: React is a Javascript library for building user interfaces and can be used as the V in the MVC in a very efficient way. A virtual DOM, a component framework, and a Javascript syntax extension are part of React’s philosophy. More about this powerful Javascript library/engine, on this DevSnack! #1 – The React Way Peter Marton ( @slashdotpeter – CTO at RisingStack) explains how to start off with this tool and how we should think in the React way. Peter introduces the concepts of virtual DOM, component-driven development and isomorphic React, among others. #2 – The Secrets of React’s Virtual DOM In this article, Tony Freed ( @tony_freed – Senior Software Engineer at Webzai) analyzes the original DOM performance issues and the concept of virtual DOM. There is also an interesting talk on virtual DOM by Pete Hunt ( @ floydophone – who worked on React.js) – check it out ! #3 – JSX for the real DOM It’s not necessary to use JSX with React,  you can just use plain JS. However, using JSX has its benefits because it’s a concise and familiar syntax for defining tree structures with attributes. Christopher Chedeau ( @Vjeux – Front-end Engineer at Facebook) outlines the advantages of using JSX with React. #4 – React on Rails Is there an easy way to work with React in a Rails project? Apparently there is! In this post by @ learnreact , we can see how to setup a Rails project using React, with the react-rails gem – a ruby gem for automatically transforming JSX and using React on Rails. #5 – Sites using React Here’s a list of projects currently using React! DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2015-10-30"},
{"website": "Moove-It", "title": "android-pearls-face-detection-beacons-and-more", "author": [" Juan Manuel Pereira "], "link": "https://blog.moove-it.com/android-pearls-face-detection-beacons-and-more/", "abstract": "DevSnack #3: Facebook is getting rid of JSON in their Android app. Do you want to promote your Android apps? Google can help you! Learn about this and other Android news on this week’s DevSnack. #1 – Android Experiments Google has released a project called Android Experiments, a showcase with some of the most creative apps for Android devices. All projects are built using different platforms like the Android SDK and NDK, Android Wear, the IOIO board, Cinder, Processing, OpenFrameworks and Unity. Developers also have the possibility to submit their own work and be promoted on the webpage. #2 – Data Binding Library One of the most exciting features announced during the Google I/O event, was the Data Binding Library. With this tool you can write declarative layouts and minimize the glue code needed to bind the application’s logic and the layouts. @stablekernel explains how to start rocking this library. #3 – FlatBuffers FlatBuffers is a cross-platform serialization library from Google, created specifically for game development, which can replace the JSON format. Facebook have transitioned most of their Android code to use FlatBuffers as the storage format. @froger_mcs explains why FlatBuffers is so effective. #4 – Google’s Eddystone: BLE Beacon on Android In this post, @hitherejoe explains what we can do with Google’s recently announced BLE Beacon format and how to use this new API to manage beacons. #5 – Face Detection on Android Google has improved Face Detection on Android devices and made it super easy to use and way faster than before. @lmoroney shows how the release of Google Play services 7.8 made the Mobile Vision APIs better and faster. DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2015-08-21"},
{"website": "Moove-It", "title": "staying-current-with-js", "author": [" Santiago Estragó "], "link": "https://blog.moove-it.com/staying-current-with-js/", "abstract": "DevSnack #6: JavaScript is always moving forward, and everyday the number of front-end (not only web, but also mobile and desktop ones) and back-end applications that fully rely on it is increasing. Stay up to date with some of the JS trending topics in this week’s DevSnack. #1 – JavaScript Application Architecture On 2015 In this article, Addy Osmani ( @addyosmani ) summarizes some important aspects to consider when developing your front-end application using JS, without specifying a framework. It covers many topics such as composition and making apps work offline. #2 – Learn TypeScript! TypeScript is sounding louder recently since some known frameworks, as Angular 2 and Ionic 2 , are rooting for using it. If you like using languages that compile to JS, as CoffeeScript or Dart , it would be a good idea to learn TypeScript . In the linked post Andrew Chalkley ( @chalkers ) talks about some of the advantages of using it, some of its features, and provides some links to get started with TypeScript . #3 – Create cross-platform desktop apps with NW.js Nowadays, most of the applications are intended to be used on the browser or to be mobile apps. However, there are some cases in which a desktop application is necessary. With Node Webkit ( NW.js) , developing cross-platform desktop applications becomes really easy if you have experience with JS . In this article Alexandru Rosianu (Github: https://github.com/Aluxian ) does a fast walkthrough on using this runtime. #4 – How to Use EcmaScript 6 (ES6) for Universal JavaScript Apps Is ES6 ready for production? Eric Elliott ( @_ericelliott ) shows a lot of tools in order to make your ES6 code reliable. He also covers some issues like compatibility, code linting and more. #5 – If we stand still, we go backwards In this article, Jake Archibald ( @jaffathecake ) talks about the evolution of the web platform. He also points out how developers should improve his skills, and the impact that this practice has on the web. He also mentions that as a web developer you don’t need to know and use everything that is available out there. DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2015-09-11"},
{"website": "Moove-It", "title": "crash-or-monit", "author": [" Andreas Fast "], "link": "https://blog.moove-it.com/crash-or-monit/", "abstract": "As an application grows it starts demanding more and more work. At some point of the ongoing project, processing a task in the background becomes a necessity. One of the most popular tools to accomplish this in Ruby is Sidekiq. Now, did it ever happen to you that you get it all up and running on the server and the next thing you know is the server is down and you get a colorful variety of error reports yelling that stuff is not working? Meet Monit, a tool that’ll let you get ahead of trouble. It automatically restarts programs when they crash and can send email reports on failures! In case you want to try it out –and make yourself a favour– here’s some aid on installing, configuring and get it up and running. Install Monit Depending on your package manager this may differ: Install Monit apt-get install monit\r\nyum install monit\r\nbrew install monit 1 2 3 apt - get install monit yum install monit brew install monit Configure monit Edit / etc / monit . conf or / etc / monitrc Uncomment (or add) these lines: set httpd port 2812 and\r\n  use address localhost\r\n  allow localhost 1 2 3 set httpd port 2812 and use address localhost allow localhost Add Sidekiq entry Create the file / etc / monit . d / sidekiq . monitrc with: Shell check process sidekiq_application_production0 with pidfile /path/to/shared/tmp/pids/sidekiq.pid\r\nstart program = \"/bin/bash -c 'cd /path/to/app/current && source /home/user/.rvm/environments/ruby-2.0.0-p353@global && bundle exec sidekiq -e production -C /path/to/app/config/sidekiq.yml -P tmp/pids/sidekiq.pid -L log/sidekiq.log'\"\r\nstop program = \"/bin/bash -c 'kill -s INT `cat /path/to/shared/tmp/pids/sidekiq.pid`'\" with timeout 90 seconds\r\nif totalmem is greater than 500 MB for 2 cycles then restart # eating up memory?\r\ngroup sidekiq 1 2 3 4 5 check process sidekiq_application_production0 with pidfile / path / to / shared / tmp / pids / sidekiq .pid start program = \"/bin/bash -c 'cd /path/to/app/current && source /home/user/.rvm/environments/ruby-2.0.0-p353@global && bundle exec sidekiq -e production -C /path/to/app/config/sidekiq.yml -P tmp/pids/sidekiq.pid -L log/sidekiq.log'\" stop program = \"/bin/bash -c 'kill -s INT `cat /path/to/shared/tmp/pids/sidekiq.pid`'\" with timeout 90 seconds if totalmem is greater than 500 MB for 2 cycles then restart # eating up memory? group sidekiq Start Monit Let’s not forget to start Monit: / etc / init . d / monit start Now a sudo monit status should show the sidekiq_application_production0 entry. If you use capistrano you will notice the current and shared directories. If you don’t use capistrano just make sure the PID is stored in some place where the path doesn’t change to let Monit find it. This configuration will also be useful if you use capistrano-sidekiq, just make sure the check process name in the config matches what is used during deploy. This configuration includes a memory protection that will restart Sidekiq before it eats up all the memory in your server. Yes, I’ve got a server down more than once due to this issue 🙁 Configure email server To receive Monit’s status via email just add the following to / etc / monit . conf or / etc / monitrc Add the email server, e.g.: for gmail’s SMTP add in the following line to the config file set mailserver smtp . gmail . com port 587 username \"notifier@gmail.com\" password \"********\" using tlsv1 with timeout 30 seconds Subscribe To subscribe to all emails, including non-error emails, add the following line to the config file: set alert my_app_notifier @ gmail . com If everything is set up correctly, executing sudo monit reload should trigger an email. If you want to get emails for specific events or have more questions regarding Monit’s config check out https://mmonit.com/monit/documentation/monit.html . A word of warning, the doc is huge. If you are looking for something specific like email, alert or whatever you need, just search it. That’s pretty much it. Of course Monit doesn’t take care of starting the processes if the server restarts. You’ll have to look for another tool to do that. Reposted from http://blog.afast.uy/2015/03/keep-sidekiq-running-using-monit.html", "date": "2015-03-26"},
{"website": "Moove-It", "title": "memory-leaks-awesome-ruby-libraries", "author": [" Gabriel Fagundez "], "link": "https://blog.moove-it.com/memory-leaks-awesome-ruby-libraries/", "abstract": "DevSnack #1: A selection of five interesting articles shared internally in our #ruby Slack channel. As Ruby developers, we deal with thousands of issues every week. Since we love sharing our experience, this is the summary of the most interesting links we found this week. #1 – What I Learned About Hunting Memory Leaks in Ruby 2.1 We usually spend a bunch of time deep diving into some mystery memory leaks in Ruby when we are working at the office. They were persistent and annoying, though not devastating by any means—the kind we could get away with ignoring for a long while, but shouldn’t. Here’s an experience, an awesome story. #2 – Awesome Ruby A collection of awesome Ruby libraries, tools, frameworks and software. The essential Ruby to build modern Apps and Web Apps. Inspired by the awesome-* trend on GitHub. The goal is to build a categorized community-driven collection of very well-known resources. #3 – How to Call Bash (not Shell) From Ruby Is it possible to call Bash methods, not Shell ones from Ruby? In this article, the author explains how to do it! #4 – Just use double-quoted strings If you are a Ruby developer, you’ve heard this statement before: Use single quoted strings unless you need string interpolation. Of course, this statement looks consistent. When we instantiate strings using double quotes, the Ruby interpreter has to do extra work: perform the interpolation. Since extra work means reduced performance, it seems reasonable to avoid double-quoted instantiation unless it’s a necessity. In this article, the author makes some tests to analyze the impact of double quotes in the performance. #5 – Rubycritic RubyCritic is a gem that wraps around static analysis gems such as Reek, Flay and Flog to provide a quality report of your Ruby code. DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2015-08-07"},
{"website": "Moove-It", "title": "how-to-implement-multitasking-in-ios", "author": [" Lucas Diez de Medina "], "link": "https://blog.moove-it.com/how-to-implement-multitasking-in-ios/", "abstract": "Up until iOS 8, Apple didn’t allow applications to run in a multitasking environment, except for during very specific and limited amounts of time: that is, when an application was sent to the background, the application only had a few minutes to complete certain tasks, such as finalizing network requests. Apple was extremely careful to limit what they allowed to happen in the background without any time limits. Indeed, developers were only ever able to play audio, receive location updates, or provide VoIP services without time restrictions, while an app was running in the background. iOS 9 introduces new opportunities to run multiple, active applications, concurrently. To support multitasking, developers just need to apply some of the latest technologies, most of which have been available since Xcode 6. Allowed multitasking modes iOS 9 supports applications multitasking in two different ways: Split View and Picture in Picture . In addition, apps need to be compatible with Slide Over ; that is, they should be configured to appear in the side menu that the user slides from the right edge of the screen (or the left, in RTL languages). In order to be compatible, developers need to set up their projects as follows: Set the base SDK to “Latest iOS” Provide a single storyboard file for the launch screen (instead of a .png image in multiple sizes). Support all four interface orientations (Portrait and Landscape , both standard and upside down ) Split view Two applications can run at the same time sharing the screen space. In this scenario, the left side application is called the Primary App , and the right side app is called the Secondary App . The Primary App has some advantages over the Secondary App: It owns the status bar; It is able to work using a second physical screen; It can work with Picture in Picture automatic invocation; It can occupy two-thirds of the screen (in landscape orientation, only). Users are able to resize the Primary and Secondary App spaces by dragging the separator view. The Secondary App can fill either one third or one half of the screen, while the Primary App can take up either two thirds or half of the screen. What’s needed to support Split View? iOS 9 makes use of Size Classes to determine how to lay out an application in a split view environment. This diagram shows which Size Classes iOS 9 uses to render the application’s layout: In order to support Split View, developers just need to support Slide Over and use Size Classes in their storyboards or Xibs. That way, iOS 9 will be able to render the UI correctly as a user moves the slider. Most of today’s apps already use Size Classes, so only minor modifications should be required to add Multitasking support to apps. Picture in Picture (PiP) If an application plays videos, developers can add support for Picture in Picture. However, Apple doesn’t recommend supporting Picture in Picture, if the video feature is not a key part of the app. For example, if your app includes an introduction or demo video, using Picture in Picture wouldn’t be recommended; but, if the application is an educational app, that shows course videos, it would be great to support PiP. What’s needed to support PiP? First of all, the app should support Slide Over. After that, the following configuration is required: In the Capabilities view, enable Background Modes and select “Audio, AirPlay and Picture in Picture” mode. Ensure your app’s audio session uses an appropriate category, such as AVAudioSessionCategoryPlayback. In addition, the application should use one of the following classes for video playback: AVPlayerViewcontroller class: automatically displays a PiP button. AVPictureInPictureController class: Available in AVKit, can be used with the AVPlayerLayer class from AVFoundation. WKWebView class: Available in the WebKit framework, and supports PiP in iOS 9 by default. If a video is being played using one of those components, when the user presses the home button or taps a notification that takes them into another app, the video will continue playing in PiP mode. In addition, when a video moves to PiP playback, the application will potentially keep running in the background. It’s important for developers to avoid using unnecessary resources while an app is running in the background, such as in a multitasking environment: if the app that’s running in background is consuming too much resource, iOS will automatically terminate it. Final Considerations Adding support for multitasking is very straightforward if the latest technologies were used during app development. However, developers must take care to optimize their apps, in order to avoid performance issues. When two applications run at the same time, they share CPU and memory usage. In order to help optimize resources, Apple has also introduced new technologies, such as App Thinning and Slicing . We always recommend (and use) the latest Apple technologies, as they bring significant advantages to new versions of iOS or Xcode. Size Classes and Asset Catalogs might just seem like ways to organise and design your application in a clearer manner, but with Xcode 7 and iOS 9 they acquire a central role in supporting Multitasking and other new enhancements, all of which you can access without expending a huge amount of extra effort.", "date": "2015-10-09"},
{"website": "Moove-It", "title": "mobile-meetup-montevideo", "author": [" Michel Golffed "], "link": "https://blog.moove-it.com/mobile-meetup-montevideo/", "abstract": "August has been a very exciting month for mobile app developers in Uruguay. Last Thursday took place the first Mobile Meetup Montevideo during this year. It was organized by our own Juanma and sponsored by Moove-it and Kona . With 45+ developers, testers and tech aficionados, we met at @cowork_latam to share the latest trends in mobile development and enjoy some pizzas & drinks. Sebastián Mantel , Bruno Berisso and Damián Arrillaga presented mobile app UX design , Android automated testing and Swift . At Moove-it, we are really happy to be contributing to the growth of the mobile community in Uruguay. Join us for the next meetup!", "date": "2015-08-27"},
{"website": "Moove-It", "title": "how-we-test-java-apps", "author": [" Enrique Rodriguez "], "link": "https://blog.moove-it.com/how-we-test-java-apps/", "abstract": "Before a feature is rolled out to production, it has to pass through a sequence of test phases. These are executed in four different environments: the developer workstation, the continuous integration server, the development environment and the staging environment. Let’s see what happens during each step, to help us detect any problems in our Java apps. Phase 1: Developer workstation Unit tests We prefer to write our tests after we finish coding, rather than follow the Test Driven Development (TDD) model. There is a myriad of articles, both in favor of and against TDD, but we believe that enforcing its use may reduce productivity. So, we always write automated tests after developing a feature and we try to be smart when choosing what component should be tested and when testing is not worthwhile –for example, when the new development is covered by another test at a different abstraction level. Automated tests are part of the codebase and we strongly encourage team members to deliver code that is fully covered by tests. Automated tests not only dramatically reduce the chance of introducing new bugs when coding new features, but also provide valuable documentation. As such, they should be kept as simple as possible, with the aim that they always end up being extremely easy to read and understand. Here are the guidelines we follow, in order to write simple and efficient tests: Use plain JUnit runner and Mockito, not some fancy runner or framework. Both JUnit and Mockito are predictable, stable and easy to understand. Instantiate mocks/stubs explicitly as class fields: this way everything is visible for the reader without having to navigate through other classes and/or configuration files (see Figure 1). Make test suite names correspond to the tested class name (Figure 1). Write descriptive test names that make it clear what the outcome of the tested operation should be (Figure 1). Figure 1 – A real-world test scenario. Write one test for each possible workflow: starting at the Happy Path and continue down different paths that diverge from the main one. Systematically organize each test logic as “AAA” (Arrange, Act, Assert). If you want to know more about this pattern, read Kent Beck’s Test-Driven Development: By Example , page 97. Keep the logic as simple, as possible: restrict it to just few lines of code –don’t even use “if” statements in your unit test logic, if you can avoid it. Only if you absolutely need to use complex statements should you employ an extract method  –and, if you do, then use a name that describes what’s going on and don’t over-complicate it! It’s a code smell , so dividing the test in two parts would probably work out better (like in Figure 2, below, where “extractGroupIds” has been extracted from the test case). Figure 2 – A method has been extracted to reduce the complexity of the test. Make as few assertions in each test as possible: ideally, just make one assertion per test. If there is more than one assertion relating to the same condition, extract a method with a descriptive name that encompasses the assertions that are related to each other (see Figure 3). Figure 3 – A method has been extracted to group correlated assertions. Only reuse code when absolutely necessary: tests are not application code. When reading a test, you should already have all the necessary information to hand, without needing to navigate through more classes. Avoid using inheritance between test suites. The reason for this is analogous to the previous one: inheritance makes it more difficult to understand the purpose of the test. Avoid making tests dependent on their order of appearance, and also avoid making the result of one test dependent on the result of another. Test functionality, not how it was implemented: if the test checks how a class does something, every change to the class will impact on the test and, as a result, the test will end up repeating the app’s code. For example: don’t test that a DAO calls the database when creating a new object. Instead, test that after the object is created, it can be retrieved. Integration tests Once we’ve ensured that each class behaves as expected, then it’s time to analyze the interaction between different components, to validate that the system is working as a whole. It’s important to highlight that, just because two classes work correctly independently, it doesn’t necessary mean that they will work as expected when they have to interact with each other. This is where we rely on the framework we use. Spring allows us to instantiate a particular application configuration for the purpose of testing. Within this configuration we keep all the components exactly as they will run in production, except for the external dependencies, which are mocked or embedded in the same JVM. This is very important. One of the reasons for this, is that we want tests to run quickly, so sending packets across the network is not an option. Another reason, besides performance, is that it’s a good practice for tests to be encapsulated, which helps us avoid the frustration of tests passing on the developer’s machine, but not on the CI server. Following such practices enables us to write and run end-to-end functional tests. End-to-end testing seeks to confirm that integrated components of an application function as expected. For example, we can select a use case and test the whole flow, trying to reproduce it, programmatically. Another example would be sending a message to a messaging source and checking if the right command has been triggered: like a call to a web service or a database insertion. The same rules for writing unit tests apply when writing integration tests. Integration testing doesn’t mean writing over-complicated test cases or increasing the dependencies of one test on another. On the contrary, integration tests should be simple, independent and efficient. Practical advice: Run all tests periodically. Not just after you’ve finished coding a full feature. Organize your work into small steps that you can test individually. You may end up subdividing a larger task into several smaller ones, which is actually better practice. For example, if you have two branches that each pass their own tests, when you merge them: run all the tests again, before moving forward to the next step. Merging two correct changes doesn’t mean that the result of the merge will be correct! Running tests often will also force you to write efficient tests that don’t take too much time to complete. Phase 2: Continuous integration We employ a Continuous Integration (CI) environment in which all our tests, both unit and integration tests, are run in a common configuration. We integrate our CI server with Gerrit, our preferred code review tool. When a developer pushes a change to Gerrit, the CI server runs automatically and scores the result with a +1, if the tests pass, or a -1, if they don’t. If the change receives a -1 from the CI server, then the reviewing team doesn’t need to spend time reviewing code that isn’t working. In addition, we keep the CI status visible to everyone. It should be always green and the team should be aware of its status, at all times. For example, in one project, we decided to mount a screen on the office wall, to show the current status of the CI server for each sub project. This was in no way intended to shame people for failing tests, just to stress to the whole team how important it is for all tests to be showing as passed. We also add a code coverage report to the CI status, so that the coverage is also visible to everyone in the team. Sometimes we set a minimum percentage of coverage as a standard for every project. In this way, we can compare numbers and discuss how to achieve better results. Phase 3: Development environment We deploy to the development environment when the CI server is green. This environment is set up for developers to explore, try out new solutions but, mainly, to break things. Here, developers feel free to shut down services and deploy unstable features. We set up an environment, configured to be as similar to the production environment as possible: with the same operating system, software versions, etc. There, we can deploy the application, enabling us to test the deployment process and environment configurations that may have not been configured fully in the local testing environment. Once all the environment-related variables have been tested, we can start manually testing the new functionalities and make sure everything works as expected. Sometimes, we go one step further and simulate application users, employing automated scripts that can be left running, permanently. Simulating a real scenario helps to identify potential flaws that are difficult to find, just by inspecting the app manually. Furthermore, we can practice the deployment process and test failure scenarios, like service nodes failing, connection problems, or any test scenario that would take too much effort to simulate using unit or integration tests. Phase 4: Staging environment We deploy to the staging environment when the feature code has been reviewed and the developer considers phase 3 to be complete. The aim of the staging environment is to make sure that what we have developed is what the clients actually want. During this stage, we start our QA and acceptance tests, hand-in-hand with our clients. The staging environment is a phase that developers really don’t want to mess up! We want our clients to have a stable and predictable system that works. During this phase, we only deploy stable and fully tested features, so this phase should only be used to validate functionality, not to find bugs. Finally, and only once we have successfully completed all four of these phases, do we truly know that the application has been fully tested: unit by unit, with all units running together, and with all units working in two, separate production-simulating environments, one for developers and one for client acceptance. These phases are ordered so that problems can be caught as early as possible, at a stage when they are easier to resolve; and long before be released into production. The overall idea being: the sooner you catch a problem, the easier it is to solve.", "date": "2015-09-08"},
{"website": "Moove-It", "title": "wrapping-up-rubyconf-2015", "author": [" Gustavo Armagno "], "link": "https://blog.moove-it.com/wrapping-up-rubyconf-2015/", "abstract": "RubyConf in San Antonio was a perfect ending to 2015. The conference was well organized and, in general, the talks were educative, inspiring and provided some very helpful insights. There was a good balance between motivational and technical talks. The opportunities provided to network with peers worked really well, not to mention the spirit of openness fostered by the organizers, that allowed whoever wanted to, to chat with celebrities like Aaron Patterson and even Yukihiro Matsumoto. Finally, the location was excellent — next to the beautiful San Antonio River Walk — the service and food were both excellent, and there was time for fun, after hours. Must-see talks The conference was organized in three tracks. Me and my colleague, Michel, soon realized that our technical areas of interest differ somewhat so, for the most part, we attended different presentations. After each talk, we would regroup and discuss what we had learned. We found common ground over three talks we both attended: 1. How to Crash an Airplane by Nickolas Means . Nickolas narrates the minute by minute story of what happened inside aircraft UA-232 before it crash-landed. Against all expectations, most of the people on board survived the crash. This achievement, Nickolas explains, has been attributed to the way in which the flight crew as a team handled the emergency and landed the airplane without using conventional controls. This experience can be taken as a lesson about how software teams should be managed, in order to make smart decisions based on shared knowledge. 2. Hacking Spacetime for a Successful Career by Brandon Hays . IMHO, Brandon Hays’ talk should be turned into an academic paper and published in a journal of social psychology. Brandon proposes a framework — in the form of an arcade game — that analyzes people’s character (“how we are wired”) and then fast-forwards time to see what happens with their careers. The outcome resulting from playing with the framework is a personal platform that helps you localize yourself in your career and evaluate whether you are on the right track, or not. 3. Keynote and Q&A session by Yukihiro Matsumoto (Matz). Matz starts his speech with an intriguing — and maybe a little disturbing, if taken seriously — “I’m the creator of this Universe”, to perhaps suggest that he is in control of Ruby and its future. Then, he focuses on the, mostly non-technical, challenges of releasing a new version of a programming language and getting it adopted by the community. Javascript ES 4, Perl 6 (still in development), PHP 6 and Python 3 all failed to substitute their predecessors. Ruby 3, which is currently being designed, could be doomed if it can’t persuade programmers that it’s really worth the effort to migrate to the new version. There’s a lot of useful and interesting material that can be taken from this RubyConf. If you weren’t able to make it, all the talks can be found at http://confreaks.tv/events/rubyconf2015 . A lean experiment We used the breaks between talks to validate ShakeJake, a wearable device created by Moove-it that detects handshakes and is aimed at making networking easy . While we are still processing the results, one of the things we learned is that ShakeJake gets people’s attention and that, with a few, minor improvements, it may even change the way people exchange contact information during events. Reflection In conclusion, the 15th Annual RubyConf in San Antonio, with its nearly 800 attendees, proved that the Ruby Community is in a very good shape. I perceived an enthusiastic, energetic and optimistic atmosphere, and a general commitment to keep growing and doing the right things. We are very proud to participate actively in the Ruby Community, to keep contributing and to see how it continues to grow.", "date": "2015-12-29"},
{"website": "Moove-It", "title": "dev-souffle", "author": [" Andreas Fast "], "link": "https://blog.moove-it.com/dev-souffle/", "abstract": "DevSnack #5: Google’s new logo, Phoenix on Elixir, scaling with a nomadic team, vim productivity and speeding up rails. A wide range of topics that converge in one: performance. #1 – Google’s new logo is not about legibility? Jack Self ( @jack_self ) analyses Google’s new logo and argues that the new font isn’t really about legibility but about austerity. He makes and interesting case about the recent global events and their effects on our psyche and it’s ultimate effects on tech and design. #2 Phoenix, the framework for the modern web! Chris McCord ( @chris_mccord ) introduces Phoenix 1.0. Phoenix is a web framework for Elixir. Elixir is an exciting new language that runs on the Erlang VM and leverages it’s incredible performance. Phoenix uses Channels to make real time communication on the web trivial. It’s at an early stage but it is a promising technology! #3 Scaling ruby with a nomadic team at GitHub Derrick Harris ( @derrickharris ) interviews Sam Lambert discussing how the service is able to keep on scaling with a relatively simple technology stack. He also talks about GitHub’s largely officeless workplace where about 60 percent of its employees work remotely. #4 JSHint Vim plugin I like to use vim for development and recently I joined a project where we have a bunch of node applications. So I spend my workday writing and editing a lot of Javascript. Everyone knows how shady Javascript can be and JSHint is a great tool that helps in writing better Javascript. Nikolay Frantsev ( @shutnik ) wrote this plugin that runs JSHint inside vim for Javascript files. If you like using another IDE I’m sure there’s some integration. The goal really is not to have to think about it, but having the feedback at your fingertips. #5 Adequate Record Aaron Patterson ( @tenderlove ) worked on AdequateRecord last year. It is now part of Rails, but it was a lot of “common sense” applied to Active Record. We can use his ideas for other parts of our projects I’m sure. And the most important lesson about improving performance: measure, apply change, measure. Without measuring you don’t really know what you are doing in terms of performance. DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License . Photo by McPig", "date": "2015-09-04"},
{"website": "Moove-It", "title": "lexis-semiotics-taxonomy-efficient-architecture", "author": [" Regina Acosta "], "link": "https://blog.moove-it.com/lexis-semiotics-taxonomy-efficient-architecture/", "abstract": "DevSnack #18: Good design is not only the appearance but the functionality of a product. Words communicate meaning, so first we have to define the basics. An efficient architecture is a consistency between lexis, semiotics, and taxonomy. This is our curated summary of IA for being aware of the rules and avoiding most common mistakes. Also, we’ve included a bonus track of the great case study of CNN’s Architecture for their Responsive Website Project. #1 – Top 10 Information Architecture Mistakes Bad information architecture causes the majority of outright user failures and isn’t improving at the rate of other web usability issues. To determine why, the article identifies 10 long-term sore thumbs that together cost websites billions of dollars each year. Jakob Nielsen, Don Norman, Tog, and colleagues: usability advocates offering evidence-based user experience (UX) research, training, consulting. Twitter: @NNgroup #2 – On Taxonomy Understanding the medium you are working on and the amount you can push things taxonomically is a critical lens to bring early in any structural work. Abby Covert is an independent Information Architect. #3 – Eight Principles of Information Architecture This is a theoretical framework – a guideline based on universal truths that provides a sketch of what makes any information architecture good. Dan Brown is a digital Product Designer @eightshapes . #4 – A visual vocabulary for describing information architecture and interaction design Diagrams are an essential tool for communicating information architecture and interaction design. This document discusses a common symbology for diagramming information architecture and interaction design concepts. It also provides guidelines for the use of these elements. Jesse James Garrett is a Human experience designer. Co-founded Adaptive Path. Wrote The Elements of User Experience. #5 – Object-Oriented UX For the first time in history, CNN.com was releasing a responsive experience. This is a fantastic, expense and greatly well documented, case of study about reframing interface elements and hierarchy to prioritize a great UX on mobile devices. Sofia Vovchehovski was tasked with designing the user experience of election night, while working at CNN. DevSnack by Moove-it is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .", "date": "2015-12-04"},
{"website": "Moove-It", "title": "sponsors-rubyconf-keeprubyweird-texas", "author": [" Ariel Luduena Karhs "], "link": "https://blog.moove-it.com/sponsors-rubyconf-keeprubyweird-texas/", "abstract": "As our local community of Ruby developers keeps growing and getting more mature, we have decided to dive in and start sponsoring two major Ruby-focused events in North America: KeepRubyWeird, in Austin, and RubyConf, in San Antonio. During your business’ lifetime, you inevitably get irrationally, powerfully attracted to many ideas – you have to – but you truly fall in love only a couple of times. These love affairs change your previous notions and become turning points that can drive your business, from then, on. At some point during its lifetime, Moove-it fell in love with Ruby. Ruby’s philosophy totally transformed the way the company thought about and practiced software engineering. Ruby also opened doors to interesting, new challenges, like working with startups and innovative companies, mainly from the US. Moove-it’s first American client – and, still, one of the company’s biggest and oldest accounts – has its technical department in Dallas, Texas. Through word of mouth, Moove-it started working with other clients from different parts of the US, like New York and San Francisco, while continuing to build its core client base in The Lone Star State. At the same time as investing in exciting, new projects in North America, like the StartupHouse in San Francisco, Moove-it continued to view Texas as the most natural candidate to become its focal point location in the US. The idea of establishing Moove-it in Texas became truly captivating. Due to its central role in the company’s corporate strategy, the explosive growth of its tech sector, its relatively low cost of living and its entrepreneurial spirit – and, not least, because of the kindness of its people, its gastronomic delights, its warm climate and other similarities with Uruguay – Texas became Moove-it’s next, great love. As a result, we have decided to take a more proactive approach and increase our visibility in the second largest state in this great country. We’re diving straight in, by sponsoring two major Ruby-focused events in the next few weeks: KeepRubyWeird , in Austin, on October 23rd, and RubyConf 2015, in San Antonio, November 15th thru 17th. Some of our crew will be traveling to Texas to attend the RubyConf and then staying on, in Austin, for a couple of weeks. It’s our second visit to Texas, this year, and we’re looking forward to shaking hands with great, old friends, making plenty of new ones and, of course, exploring new business opportunities! We also want to generate awareness about the reasons why we love Ruby and our profession: with that in mind, we’ve built something special for the occasion – it’s a cool, little IoT wearable device to take to the conference. We’ve called it “the ShakeJake project”. Stay tuned for further updates on our progress ahead of, and during, our trip – and, to get to know more about ShakeJake!", "date": "2015-10-14"},
{"website": "Moove-It", "title": "mastering-css-bem-and-itcss", "author": [" Gabriel Fagundez "], "link": "https://blog.moove-it.com/mastering-css-bem-and-itcss/", "abstract": "A few days ago , Moove-it gave us the chance to meet Harry Roberts, an experienced Front End developer. Harry provides courses to companies such as Google, Etsy, and the BBC. The focus was on improving our Front End skills through a two-day workshop, but the experience gave us more than a bunch of technical skills and knowledge. Writing CSS is very simple. Learning the language is not a big deal. It takes no more than 2 minutes to write 3 lines of CSS to style an element of the DOM. But, let’s think beyond that. What if the application grows bigger than we had originally expected, at the beginning of the project? Is our code maintainable? What things should we consider in order to write more maintainable and scalable CSS code? As a javascript development company, Moove-It should concern about this things. Let’s look at just two technical elements we learned during the workshop. BEM. Block, Element, Modifier When building a typical website, the process is based on the assumption that the design and technical specifications for a website do not change over the course of its development. The designer and the developer will work with little or no interaction between them. Typically, we assume that the design is ready before working on the development. This approach has the following iteration assumptions: First, the designer prepares a series of mock-ups visualizing the design of the website. Then, based on the mock-ups, a static page is created. Last, the developer brings the site to life with the integration with the BackEnd and adding JS logic. This approach works well, if the assumption that we mentioned above is true. However, that is rarely the case, as normally the website begins to evolve and take on a life of its own. The design of the pages gradually changes and new pages or sections are added. One of the most important features of using BEM, is that it ensures that everyone participating in the development of a website is working with the same codebase and using the same terminology. The design of the website can be changed at any moment, as site requirements change, without creating any problems or conflicts. BEM Principles Considering programming methodologies, one of the most common examples is Object Oriented Programming (OOP). We’re very familiar with OOP, and in some ways, BEM is similar to OOP. BEM is a way of describing reality in code, with a range of patterns, and a way of thinking about program entities regardless of the programming languages being used. BEM Components Block : A block is an independent entity, a “building block” of an application. A block can be either simple or compound (containing other blocks). For example, a block can be a Search Box. Element : An element is a part of a block that performs a certain function. Elements are context-dependent: they only make sense in the context of the block that they belong to. In our example, elements are the input of the Search Box, or the Submit button. Modifier : If we want to add a particular attribute to an element, we should use modifiers. For example, to add two Search Boxes, but one with a bigger button, we can add the modifier –big to the Submit button. In the above case, we will have something like this: <block:search>\r\n  <element:input/>\r\n  <element:button modifier:big>Search</e:button>\r\n</block:search> 1 2 3 4 < block : search > < element : input / > < element : button modifier : big > Search < / e : button > < / block : search > And the CSS should be something like this: .b-search {\r\n}\r\n  .b-search__e-input {\r\n  }\r\n  .b-search__e-button {\r\n  }\r\n    .b-search__e-button--big {\r\n    } 1 2 3 4 5 6 7 8 . b - search { } . b - search__e - input { } . b - search__e - button { } . b - search__e - button -- big { } As you can see, this approach gives us the chance to modify the block, remove it, or add new components, without any unexpected changes across the application. The code will be maintainable, easy to learn, and simply awesome. ITCSS. A sane, scalable, managed CSS architecture from CSS Wizardry The title of the official page is very descriptive. The workshop made us think a lot about our own apps and highlighted the importance of having our own CSS architecture. We usually build large and complex applications, which change over time. Having a defined architecture is something we can’t avoid and gives us the ability to offer better code in less time. Managing CSS at scale is hard, and sometimes we make it a lot harder than it should be. The Inverted Triangle CSS is a simple effective and as yet unpublished methodology to help us manage, maintain and scale CSS projects of all sizes. ITCSS, is not a library, it’s a school of thought, a meta framework if you will. It helps us manage the problems associated with writing CSS at scale, and helps us maintain our sanity, at the same time. It’s also based on this statement: “each piece of CSS needs knowledge of what came before it, and what might come after it”. CSS is a giant dependency tree, and we need a way to manage this dependency at a very low level. ITCSS was one of the most valuable topics that we learned in the workshop, and something that we have already started to use in our web applications. Miscellaneous… Last, but not least, let’s talk about Harry. We met not only a CSS expert, but also a great person. We can talk a lot about the workshop, the ITCSS architecture, BEM and all this great technical stuff, but that’s not the whole story. The workshop provided us with the opportunity to meet a person who has worked with leading companies across the globe, and we were able to learn a lot from his experience. We also went out for a few beers to talk about culture, sports, technologies, etc, with a person we had met the day before. It was a great experience, which gave us a huge amount of technical knowledge, and the opportunity to make a new friend. Interested in other services? Read about our highly- experienced Java Consulting firm.", "date": "2015-05-08"},
{"website": "Moove-It", "title": "just-enough-linux-to-be-dangerous", "author": [" Andreas Fast "], "link": "https://blog.moove-it.com/just-enough-linux-to-be-dangerous/", "abstract": "This post is inspired on a talk about Ruby & Linux at the RubyConf Uruguay 2014. The Linux Process in Ruby Creating a new process In linux we use fork to create a new child process. In Ruby we have access to this by using Process :: fork It accepts an optional block. We can use either Process.fork or simply fork . When a block is given the child process runs the block and terminates with a status of zero. When no block is given the fork call returns twice, once in the parent process returning the child pid and once in the child process returning nil. When using fork only the thread calling fork will be running in the child process, no other thread will be copied. Since ruby 2 there’s been an improvement to copy on write with relation to the GC. Instead of duplicating all the data it just copies the data when the shared memory is modified by one of the processes. Files & Network Connections When using a fork the open files and network connections are shared between processes. This can be a good or a bad thing depending on what you are doing. Sometimes you are interested in writing to the same file from both processes at the same time. But in case of a database connection you could run into weird scenarios. So remember to re-connect and close/open files if you are not interested in using the same connection as the father process did. Fork Example fork.rb Ruby def puts_with_pid(string)\r\n  puts \"[#{Process.pid}]$ #{string}\"\r\nend\r\n\r\nputs_with_pid \"Fork Example\"\r\n\r\nchild_pid = Process.fork { puts_with_pid \"I'm the child process in a fork block\" }\r\nProcess.wait(child_pid)\r\n\r\nchild_pid = fork\r\n\r\nif child_pid.nil?\r\n  puts_with_pid \"This is the child process because child_pid.nil? => true and the PID is #{Process.pid}\"\r\nelse\r\n  puts_with_pid \"This is the parent process with pid #{Process.pid} and the child's pid is #{child_pid}\"\r\nend\r\n\r\nputs_with_pid \"Both processes print this and child_pid is the way to differentiate between them (child_pid=#{child_pid.inspect})\"\r\n\r\nProcess.wait(child_pid) if child_pid 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def puts_with_pid ( string ) puts \"[#{Process.pid}]$ #{string}\" end puts_with _ pid \"Fork Example\" child_pid = Process . fork { puts_with _ pid \"I'm the child process in a fork block\" } Process . wait ( child_pid ) child_pid = fork if child_pid . nil ? puts_with _ pid \"This is the child process because child_pid.nil? => true and the PID is #{Process.pid}\" else puts_with _ pid \"This is the parent process with pid #{Process.pid} and the child's pid is #{child_pid}\" end puts_with _ pid \"Both processes print this and child_pid is the way to differentiate between them (child_pid=#{child_pid.inspect})\" Process . wait ( child_pid ) if child_pid Example run $ ruby fork.rb \r\n[5109]$ Fork Example\r\n[5111]$ I'm the child process in a fork block\r\n[5109]$ This is the parent process with pid 5109 and the child's pid is 5114\r\n[5109]$ Both processes print this and child_pid is the way to differentiate between them (child_pid=5114)\r\n[5114]$ This is the child process because child_pid.nil? => true and the PID is 5114\r\n[5114]$ Both processes print this and child_pid is the way to differentiate between them (child_pid=nil) 1 2 3 4 5 6 7 $ ruby fork.rb [5109]$ Fork Example [5111]$ I'm the child process in a fork block [5109]$ This is the parent process with pid 5109 and the child's pid is 5114 [5109]$ Both processes print this and child_pid is the way to differentiate between them (child_pid=5114) [5114]$ This is the child process because child_pid.nil? => true and the PID is 5114 [5114]$ Both processes print this and child_pid is the way to differentiate between them (child_pid=nil) Process ID (PID) To get a process’ PID we can use Process::pid and to get the parent’s pid we use Process::ppid $ ruby -e'puts \"pid=#{Process.pid} and parent pid=#{Process.ppid}\"'\r\npid=5251 and parent pid=2220 1 2 $ ruby -e'puts \"pid=#{Process.pid} and parent pid=#{Process.ppid}\"' pid=5251 and parent pid=2220 Reaping a child’s status When using fork in linux the child’s exit status needs to be collected, otherwise the operating system will accumulate zombies. There are a number of ways to do this in ruby, here’s a list: Process . wait ( pid = - 1 , flags = 0 ) pid > 0 Waits for the child whose process ID equals pid. 0 Waits for any child whose process group ID equals that of the calling process. -1 Waits for any child process (the default if no pid is given). < -1 Waits for any child whose process group ID equals the absolute value of pid. Process . waitall Waits for all children, returning an array of pid/status pairs (where status is a Process::Status object). Process . detach Sets up a separate Ruby thread whose sole job is to reap the status of the process pid when it terminates. Use detach only when you do not intent to explicitly wait for the child to terminate. Spawn Spawn executes the specified command and return its pid. It does not wait for the command to finish and the parent process should wait for it to finish or use detach if they don’t care about the return status. Ruby pid = spawn(RbConfig.ruby, \"-eputs'Hello, world!'\")\r\nProcess.wait pid 1 2 pid = spawn ( RbConfig . ruby , \"-eputs'Hello, world!'\" ) Process . wait pid Daemonize Process :: daemon allows a ruby process to detach from the controlling terminal and run as a system daemon. Signals Trapping Signals Signals play a big part in IPC. To handle a signal in ruby you can use Ruby Signal::trap(signal, command)\r\nSignal::trap(signal) {||block} 1 2 Signal :: trap ( signal , command ) Signal :: trap ( signal ) { || block } It receives the signal and a command or block. There are special commands that you can look up in the Signal::trap documentation. The more interesting scenario here is to pass a block to the trap and execute whatever it is we want to do when receiving the signal. Note that signal handlers need to be reentrant and that signals are deferred. This means that your process might receive the signal only once if it was triggered many times in quick succession. There’s no way to tell beforehand. Sending Signals To send a signal in ruby use Ruby Process::kill(signal, pid) 1 Process :: kill ( signal , pid ) With kill you indicate the signal and the pid. This will send the signal to the process indicated by pid. Example signal.rb Ruby pid = fork do\r\n trap 'TERM' do\r\n puts 'OK, I\\'m done'\r\n exit\r\n end\r\n\r\n loop { sleep }\r\nend\r\n\r\nProcess.kill 'TERM', pid\r\nProcess.wait 1 2 3 4 5 6 7 8 9 10 11 pid = fork do trap 'TERM' do puts 'OK, I\\'m done' exit end loop { sleep } end Process . kill 'TERM' , pid Process . wait Execution $ ruby signal.rb \r\nOK, I'm done 1 2 $ ruby signal.rb OK, I'm done Reentrant When trapping signals you need to make sure your code is reentrant. This means that it has to be able to be executed many times and even if it’s being executed, because basically it may be interrupting the handling of an interruption. A good practice here is to store the signal in a queue and work through the queue in another thread or in the main process. Deferred Not all signals will make it to the process, because signals are not queued, they are pending. So if you send a signal and the same signal is still pending it won’t make it to the process. Pipes In Linux on the command line we like to combine commands using the pipe operator “|”. This is very useful to connect one process with the other and have small programs to specific stuff and use pipes to combine and get more complex stuff done. In ruby we can access the pipe functionality using IO::pipe pipe.rb Ruby reader, writer = IO.pipe\r\n\r\nwriter.puts 'Hello'\r\nwriter.puts 'World'\r\n\r\nputs reader.gets\r\nputs reader.gets\r\n# reader.gets # would block!\r\n\r\nrequire 'io/wait'\r\nputs reader.ready? # => false\r\n\r\nwriter.puts '!'\r\nputs reader.ready? # => true\r\nputs reader.gets 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 reader , writer = IO . pipe writer . puts 'Hello' writer . puts 'World' puts reader . gets puts reader . gets # reader.gets # would block! require 'io/wait' puts reader . ready ? # => false writer . puts '!' puts reader . ready ? # => true puts reader . gets Pipes are write atomic up until 512 bytes, but there are limits to how much data a pipe can buffer. The size is provided by the Kernel and is 64 kb, after that the pipe start losing data. IPC using pipes Now the interesting part is how to communicate processes using pipes. After a fork the pipes remain open and are shared. So it’s only a matter of closing the writer on the child’s end and the reader on the parent’s end and we have a communication channel from the parent to the child. pipe_ipc.rb Ruby reader, writer = IO.pipe\r\n\r\n3.times do\r\n  fork do\r\n    writer.puts \"I'm child with pid #{Process.pid}\"\r\n  end\r\nend\r\n\r\nwriter.close\r\n3.times { puts reader.gets }\r\nreader.close 1 2 3 4 5 6 7 8 9 10 11 reader , writer = IO . pipe 3.times do fork do writer . puts \"I'm child with pid #{Process.pid}\" end end writer . close 3.times { puts reader . gets } reader . close $ ruby pipe_ipc.rb \r\nI'm child with pid 13434\r\nI'm child with pid 13437\r\nI'm child with pid 13440 1 2 3 4 $ ruby pipe_ipc.rb I'm child with pid 13434 I'm child with pid 13437 I'm child with pid 13440 We can start opening more processes and keep communication back and forth through pipes. One per direction per process. Conclusion Linux has small but very powerful Inter Process Communication tools and they are usable through the ruby API. It’s up to us finding the right tool for the right job and leverage proven tools that work fast and have been around for years. They’re going nowhere so we can definitely rely on them.", "date": "2014-12-01"},
{"website": "Moove-It", "title": "agile-retrospectives", "author": [" Michel Golffed "], "link": "https://blog.moove-it.com/agile-retrospectives/", "abstract": "A few weeks ago, our team participated in RailsRumble , a 48-hour-non-stop programming competition. We named our app Kaigi – Japanese for “business meeting”. Kaigi helps remote teams running Agile retrospectives. It is completely free and awesome. Combining Kaigi with a video conference tool will simply do the job. Give it a try! This is our suggested guide for an efficient Retrospective meeting: Prepare the agenda taking into account the available time. In this case 50 min. There should always be a limited timespan for a meeting. 5 min – Introduction / go over the plan from previous meeting. 10 min – Data collection: Each member should write down their feelings using simple keywords on stickies. Use the Start-Stop-Continue technique. Consider: technical stuff, planning, estimates, design, communication, demos, deploys, etc. 3 min – Grouping: Group together related topics – this will allow to learn from each other’s comments. 2 min – Voting: Each member has 5 votes. Each member will vote for the topics they‘d like to discuss. This voting will determine the priority for each topic. 25 min – Go over the topics: Discuss each item. The number of topics that will be discussed depends on the available time. For each topic, the moderator will take some notes to discuss in the next phase. 5 min – Action plan: Create actionable items. Every item should have a due date. Define the responsible for each individual item. Some items could be for the entire team. The next meeting should start by analyzing whether the action items were completed or not. Each topic that wasn’t discussed during this meeting should be included in a backlog to be reviewed within the next session.", "date": "2014-11-14"},
{"website": "Moove-It", "title": "whats-new-on-rspec", "author": [" mig "], "link": "https://blog.moove-it.com/whats-new-on-rspec/", "abstract": "On June 2, 2014, RSpec 3 was released after months of announced features that were becoming available through new versions of the RSpec 2 branch. Here at Moove-IT, we decided to take a look at the new stuff, as well as say goodbye to some of the old things that have been removed. Just in case some of us who use RSpec haven’t paid attention to them lately, we put a little bit of Better Specs on top of all that. Here is the presentation we used at our latest Friday Talk.", "date": "2014-08-06"},
{"website": "Moove-It", "title": "understanding-web-components", "author": [" Rodrigo De León "], "link": "https://blog.moove-it.com/understanding-web-components/", "abstract": "Quoting Eric Bidelman , an engineer at Google especially involved in this technology: “Web components are a set of emerging standards that allow developers to expand HTML and its functionality” And these standards are: Templates Custom Elements Shadow DOM Imports I will try to give you a general sense of what these are. Templates – Reusing HTML Templates shouldn’t be a new concept to any developer out there. A lot of languages, libraries and frameworks use some kind of template engine . Let’s see how to use them: XHTML <template> \r\n  <h1>Hi there!</h1>\r\n</template> 1 2 3 <template> <h1> Hi there! </h1> </template> Here we are defining a new tag, called template and we could put any html markup we want inside it. But, what’s the new thing here? It’s just a tag… It is…but the content inside the template will not be rendered in the page until we explicitly activate it. Even better, no external content like images or videos will be loaded, nor javascript code inside a script tag will be executed. How do we activate a template? JavaScript // select the template\r\nvar template = document.querySelector('#mytemplate');\r\n\r\n// import a copy of the template’s content\r\nvar clone = document.importNode(template.content, true);\r\n\r\n// append the content to the body\r\ndocument.body.appendChild(clone); 1 2 3 4 5 6 7 8 // select the template var template = document . querySelector ( '#mytemplate' ) ; // import a copy of the template’s content var clone = document . importNode ( template . content , true ) ; // append the content to the body document . body . appendChild ( clone ) ; Custom Elements – Creating tags HTML provides us with a set of tags that we use to define the structure of our web. After using them you will realize that there are not too many and most of the time they don’t describe what you are building. This is how it looks if we inspect the markup of a site like Twitter: Twitter markup. It’s simply too hard to know what is going on, it lacks expressivity. Here is where custom elements fit well. We can create our own tags that the browser will treat as normal tags. And we can name it! It only requires to have a dash (-) in its name, to distinguish from the default tags. Also we can define our own public API with custom attributes. Let’s define our tag and register it in the DOM: XHTML <moove-it>\r\n  <template id=\"phrase\">\r\n   Software for Human needs.\r\n  </template>\r\n</moove-it>\r\n<script>\r\n  //register our element\r\n  var mooveIt = document.registerElement('moove-it');\r\n</script> 1 2 3 4 5 6 7 8 9 <moove-it> <template id = \"phrase\" > Software for Human needs. </template> </moove-it> <script> //register our element var mooveIt = document . registerElement ( 'moove-it' ) ; </script> We can see in Chrome DevTools: We are able to see the text “Software for human needs” but it’s not displayed in the browser. This is because we put it inside a template and like we said it will not be rendered until we activate it. XHTML <moove-it>\r\n  <template id=\"phrase\">\r\n   Software for Human needs.\r\n  </template>\r\n</moove-it>\r\n\r\n<script>\r\n  //register our element\r\n  var mooveIt = document.registerElement('moove-it');\r\n  \r\n  //get and clone the template\r\n  var mooveItTemplate = document.querySelector('#phrase');\r\n  var clone = document.importNode(mooveItTemplate.content, true);\r\n\r\n  // append the content of the template to our\r\n  document.body.appendChild(clone);\r\n</script> 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 <moove-it> <template id = \"phrase\" > Software for Human needs. </template> </moove-it> <script> //register our element var mooveIt = document . registerElement ( 'moove-it' ) ; //get and clone the template var mooveItTemplate = document . querySelector ( '#phrase' ) ; var clone = document . importNode ( mooveItTemplate . content , true ) ; // append the content of the template to our document . body . appendChild ( clone ) ; </script> And there it is! Our first custom element, using templates. Shadow DOM – Encapsulation If you want to build a small widget within your page you will face one big problem: encapsulation . The DOM tree lacks of it. Your special widget may have CSS classes, ids and javascript code that use this to implement some behavior. You have to pay special attention to this, since it could accidentally apply to other parts of the page. So Shadow DOM is the component we will use to separate the content, and it’s functionality, from the presentation of our element. To enable it in DevTools (Chrome 35+), just right click in Chrome, go to inspect element, and in the settings you can activate ‘Show user agent Shadow DOM‘ Now you will be able to see a node called “#shadow-root” inside our moove-it tag, and inside of it the content of the element. This separation will enable us, among other things, to style our element without affecting the rest of the page. Let’s see an example of how to use this feature. We will create our own custom tag, using templates and encapsulate its style and content, using Shadow DOM. XHTML <body>\r\n <p>Moove-it </p>\r\n \r\n <moove-it name=\"moove-it\" constructor=\"MooveIt\">\r\n </moove-it>\r\n\r\n <template id=\"phrase\" >\r\n <style>\r\n   p {\r\n    background: orange;\r\n    color: white;\r\n   }\r\n </style>\r\n\r\n   <p>Software for human needs.</p>\r\n</template>\r\n\r\n <script >\r\n  // register our custom element\r\n  var mooveIt = document.registerElement('moove-it');\r\n\r\n  // append to the body\r\n  document.body.appendChild(new mooveIt());\r\n\r\n  //select element\r\n  var mooveItElement = document.querySelector('moove-it');\r\n\r\n  //select template\r\n  var template = document.querySelector('#phrase');\r\n\r\n  // create shadow root and fill with the content of the template\r\n  mooveItElement.createShadowRoot().appendChild(template.content);\r\n </script>\r\n\r\n</body> 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 <body> <p> Moove-it </p> <moove-it name = \"moove-it\" constructor = \"MooveIt\" > </moove-it> <template id = \"phrase\" > <style> p { background : orange ; color : white ; } </style> <p> Software for human needs. </p> </template> <script > // register our custom element var mooveIt = document . registerElement ( 'moove-it' ) ; // append to the body document . body . appendChild ( new mooveIt ( ) ) ; //select element var mooveItElement = document . querySelector ( 'moove-it' ) ; //select template var template = document . querySelector ( '#phrase' ) ; // create shadow root and fill with the content of the template mooveItElement . createShadowRoot ( ) . appendChild ( template . content ) ; </script> </body> DevTools Browser We can see how the browser sets the paragraph text inside our element with the orange background but doesn’t do the same with the <p> tag outside of it, even though we didn’t declare anything special in our CSS. The end-user will only see a custom tag that will have all the style and content built-in. This is the encapsulation we want for the web! Imports – Including components HTML imports provide us a way to include HTML documents inside other HTML. If you are a web developer you know that if you want to include your CSS files you need to do something like this: XHTML <head>\r\n  <link rel=\"stylesheet\" type=\"text/css\" href=\"main.css\">\r\n</head> 1 2 3 <head> <link rel = \"stylesheet\" type = \"text/css\" href = \"main.css\" > </head> In a similar way we are going to import a document: XHTML <head>\r\n  <link rel=\"import\" href=\"/path/moove-it.html\">\r\n</head> 1 2 3 <head> <link rel = \"import\" href = \"/path/moove-it.html\" > </head> It is worth mentioning that importing an HTML file doesn’t mean to copy all its content . What we are doing is telling the parser to go and fetch the content so we can use it. The great thing about this, is that we get to decide whether to use the entire document or just a part of it. To access we use: var documentContent = document . querySelector ( 'link[rel=\"import\"]' ) . import ; and inside documentContent we have our new document to use it as needed. This gives us the opportunity to start creating reusable components that we can import when we need them without repeating code, which of course, is a great thing! Resources If you got really excited about Web Components and want to know more about it please check these talks: Web Components: A Tectonic Shift for Web Development – Google I/O 2013 Building modern apps with Polymer & Web Components And these articles too: A Detailed Introduction To Custom Elements Custom Elements defining new elements in HTML And that’s it… I hope you now have a better understanding of what this new technology is capable of doing and start creating your reusable components to share with the community 🙂", "date": "2014-07-07"},
{"website": "Moove-It", "title": "token-based-authentication-json-web-tokenjwt", "author": [" Matías Nieves "], "link": "https://blog.moove-it.com/token-based-authentication-json-web-tokenjwt/", "abstract": "What is it for? Token based authentication is a new security technique for authenticating a user who attempts to log in to a secure system (e.g. server), using a signed token provided by the server. The authentication is successful if the system can prove that the tokens belong to a valid user. It is important to note that Json Web Token (JWT) provides signed tokens but not encrypted ones, so passwords or any critical information must not be included in the token unless you encrypt the data (e.g. using JWE). Why use it? Here are some advantages of choosing JWT: Standard: JWT is becoming a standard, and there a multiple libraries for a lot of languages (Ruby, Java, Python, Node, Backbone). So the integration with your language or technology should be pretty easy. Cross-domain / CORS: Since the information is transmitted using an HTTP header, you are able to make AJAX requests to any server or domain. Protection from CSRF: The token must be included in every request made to the server, and will be validated by the server. The token is linked to the user’s current session. Server side Scalability: The token is self-contained(i.e. contains all the user info), so there’s no need to keep a session store. The rest of the state lives in the client’s local storage. The token might be generated anywhere, so you are not tied to any specific authentication scheme, decoupling this process from your application. Warning: since the information is transmitted in an HTTP header and its size is limited, the token size could be an issue. How does it work? Let’s see an example of how to use JWT to authenticate a user. In this example we will be using Ruby, Rails and AngularJS. Server Side Let’s suppose we have an authentication controller. When a post to create a session comes to the server, we validate the user and create a new JWT. Create Token Ruby class AuthController < ApplicationController\r\n\r\n  # session creation\r\n  def create\r\n    user = User.authenticate!(params[:user])\r\n    render json: { token: create_token(user) }\r\n  end\r\n \r\n  private\r\n \r\n  def create_token(user)\r\n    secret = ‘secret’ # must be an environment variable\r\n    JWT.encode(user, secret)\r\n  end\r\nend 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class AuthController < ApplicationController # session creation def create user = User . authenticate ! ( params [ : user ] ) render json : { token : create_token ( user ) } end private def create_token ( user ) secret = ‘ secret ’ # must be an environment variable JWT . encode ( user , secret ) end end Then, for every request we have to validate the token before processing it. Validate Token Ruby class Api::ExampleController < ApplicationController\r\n  before_action :validate_token\r\n \r\n  def index\r\n    render json: Example.all.to_json\r\n  end\r\n \r\n  private\r\n \r\n  def validate_token\r\n    secret = ‘secret’ # must be an environment variable \r\n    begin\r\n      token = request.headers['Authorization'].split(' ').last\r\n      JWT.decode(token, 'secret')\r\n    rescue JWT::DecodeError\r\n      head :unauthorized\r\n    end\r\n  end\r\nend 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class Api :: ExampleController < ApplicationController before_action : validate _ token def index render json : Example . all . to _ json end private def validate _ token secret = ‘ secret ’ # must be an environment variable begin token = request . headers [ 'Authorization' ] . split ( ' ' ) . last JWT . decode ( token , 'secret' ) rescue JWT :: DecodeError head : unauthorized end end end Let’s take a look at the token. When we encode a new token, we get this string: Encode Example Ruby > user = {username: 'example_user', email: 'mail@example.com', id: 123}\r\n => {:username=>\"example_user\", :email=>\"mail@example.com\", :id=>123} \r\n> token = JWT.encode(user, \"secret\")\r\n => \"eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJ1c2VybmFtZSI6ImV4YW1wbGVfdXNlciIsImVtYWlsIjoibWFpbEBleGFtcGxlLmNvbSIsImlkIjoxMjN9.BEtGLs7sZwLztKsitaFTCysmluXMM6yU2-373JGHKWk\" 1 2 3 4 > user = { username : 'example_user' , email : 'mail@example.com' , id : 123 } = > { : username = > \"example_user\" , : email = > \"mail@example.com\" , : id = > 123 } > token = JWT . encode ( user , \"secret\" ) = > \"eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJ1c2VybmFtZSI6ImV4YW1wbGVfdXNlciIsImVtYWlsIjoibWFpbEBleGFtcGxlLmNvbSIsImlkIjoxMjN9.BEtGLs7sZwLztKsitaFTCysmluXMM6yU2-373JGHKWk\" Then, when we decode the token, we retrieve this: Decode Example Ruby > JWT.decode(token, \"secret\")\r\n => [{\"username\"=>\"example_user\", \"email\"=>\"mail@example.com\", \"id\"=>123}, {\"typ\"=>\"JWT\", \"alg\"=>\"HS256\"}]\r\n\r\n[{\"username\"=>\"example_user\", \"email\"=>\"mail@example.com\", \"id\"=>123} # user information\r\n{\"typ\"=>\"JWT\", \"alg\"=>\"HS256\"}] # token information, typ is the content type and alg is the encode algorithm used 1 2 3 4 5 > JWT . decode ( token , \"secret\" ) = > [ { \"username\" = > \"example_user\" , \"email\" = > \"mail@example.com\" , \"id\" = > 123 } , { \"typ\" = > \"JWT\" , \"alg\" = > \"HS256\" } ] [ { \"username\" = > \"example_user\" , \"email\" = > \"mail@example.com\" , \"id\" = > 123 } # user information { \"typ\" = > \"JWT\" , \"alg\" = > \"HS256\" } ] # token information, typ is the content type and alg is the encode algorithm used If we want, we could add some claims to the token: exp : identifies the expiration time on or after the token MUST NOT  be accepted for processing. nbf : identifies the time before the token MUST NOT be accepted for processing. iat : identifies the time when the  JWT was issued. Client Side An example of a sign in request to the server from the Angular app. Sign In JavaScript myApp.controller('UserCtrl', function ($scope, $http, $window) {\r\n  $scope.user = {username: ''example_user'', password: 'pass'};\r\n  $scope.submit = function () {\r\n    $http\r\n      .post('/sessions', $scope.user)\r\n      .success(function (data, status, headers, config) {\r\n        $window.sessionStorage.token = data.token;\r\n      })\r\n  };\r\n}); 1 2 3 4 5 6 7 8 9 10 myApp . controller ( 'UserCtrl' , function ( $ scope , $ http , $ window ) { $ scope . user = { username : '' example _ user '' , password : 'pass' } ; $ scope . submit = function ( ) { $ http . post ( '/sessions' , $ scope . user ) . success ( function ( data , status , headers , config ) { $ window . sessionStorage . token = data . token ; } ) } ; } ) ; Once we have the token, we can make any request to the server using the token. Request with Token JavaScript angular.module('exampleApp')\r\n    .service('Example', function Leagues( ENV, $http, Session ) {\r\n      return {\r\n             get: function( callback ){\r\n                  $http.defaults.headers.common['Authorization'] = \"Bearer \" +   \r\n                       $window.sessionStorage.token;\r\n                  $http.get( ENV.apiEndPoint + '/example')\r\n                        .success(function(data, status, headers, config) {\r\n                            callback(data);\r\n                        })\r\n              }\r\n       }\r\n}); 1 2 3 4 5 6 7 8 9 10 11 12 13 angular . module ( 'exampleApp' ) . service ( 'Example' , function Leagues ( ENV , $ http , Session ) { return { get : function ( callback ) { $ http . defaults . headers . common [ 'Authorization' ] = \"Bearer \" + $ window . sessionStorage . token ; $ http . get ( ENV . apiEndPoint + '/example' ) . success ( function ( data , status , headers , config ) { callback ( data ) ; } ) } } } ) ; Reference Links http://self-issued.info/docs/draft-ietf-oauth-json-web-token.html https://github.com/hokaccha/node-jwt-simple https://github.com/jwt/ruby-jwt https://jwt.io/", "date": "2014-06-25"},
{"website": "Moove-It", "title": "lean-startup-machine-montevideo", "author": [" admin "], "link": "https://blog.moove-it.com/lean-startup-machine-montevideo/", "abstract": "Lean Startup Machine is taking place in Montevideo this year, on August 1st – 3rd, 2014. This three-day workshop focuses on teaching innovators and entrepreneurs how to start a business using Lean Startup. Ariel Ludueña (CEO) and Martin Cabrera (CTO) both at Moove-IT have been invited as mentors to be part of this great Lean Startup educational experience. For more information: https://www.leanstartupmachine.com/cities/montevideo/", "date": "2014-05-19"},
{"website": "Moove-It", "title": "asynchronous-planning-poker-estimations", "author": [" Michel Golffed "], "link": "https://blog.moove-it.com/asynchronous-planning-poker-estimations/", "abstract": "Have you ever worked in teams, where several and simultaneous projects take place and stories need to be estimated quickly and accurately so as to commit to a delivery date? If you ever encountered this challenge, you would probably find this article very useful. A different scenario from Sprint Planning Traditional Scrum Sprint planning definitely works for some of our clients. But, in this particular scenario we are talking about a team working on several projects for a single client. Therefore, we are constantly receiving tickets which need to be estimated. Since it does not make sense to gather the whole team too often in order to estimate, we came up with this idea of an asynchronous planning. We are positive you would find it extremely helpful. How it works? Once a new task arrives, the team members working for that particular project know they must post their personal estimates on a special board (asynchronously and not anonymously). This board would then contain several post-its with the right estimations facing the wall, so they are not visible for the rest of the team. After a minimum of estimates have been cast, one team member turns around the pieces of paper and gets an average. In cases where the estimates presented are too different (which is rare), and then the average could be just not that useful, the team members gather to discuss the estimates more in details and reach an agreement. Every two weeks the estimations are analyzed and compared with the actual execution time. Along with this, a retrospective meeting takes place to improve the overall functioning of the team. Pros Super fast. Average improves accuracy. Visibility (every team member sees all estimates). Time saving. Team player philosophy. Improves performance. Cons Fewer group discussions. Only works with highly organized teams. Taking it to the next level We started this process using only a whiteboard, a few markers and some post-its. As we are geeks, and love creating software to solve pretty much everything, we decided to take this to the next level and create a specific tool for it. We called it “Zenkai” – playing with the letters of “ Kaizen ”. Zenkai It works by simply automating the above described process. Dashboard with pending estimates. Dashboard. Tickets per project report. Hours and Team Velocity over Time. Zenkai on GitHub – Fork it and tweak it, or submit your Pull Request! We’ve been following this process (and using this tool) for almost a year now and are constantly trying to improve it. Naturally, this facilitates the estimation process. Beyond that, it truly helps our teams to stay aligned, deliver in time, and keep track of their improvement. If you want to implement this with your team, please contact us and we’ll be happy to discuss further!", "date": "2014-05-21"},
{"website": "Moove-It", "title": "when-and-why-clojure", "author": [" Enrique Rodriguez "], "link": "https://blog.moove-it.com/when-and-why-clojure/", "abstract": "Introduction When you have to solve a problem, in order to make an efficient and effective solution you must choose the right tools to help you in the process of building it. As programmers, the languages we choose determine a big part of the solution and they will influence the way we reason about it, so it’s really helpful to understand what are the kind of problems that a language aims to solve before starting to use it. Following that line, this post tries to summarize the key features of Clojure that make it preferable over other languages under some conditions, based on what Clojure designers and its community predicate about the language. Immutable data structures The Clojure libraries provide several data structures along with a handful of functions to manipulate them. The core ones are list , vector , map and set and they are all treated using the same abstraction: collection . Out of the box Clojure provides functions that operate on any collection, like = for testing equality, count to know the number of items in the collection, conj to add an element, empty to create a new empty collection and seq to obtain a sequence out of a collection, which is a sequential view of the latter. To work with sequences the core library offers the obvious first and rest but also the more powerful map , reduce , filter , take and drop , that is just to mention the fundamentals but you can find a lot more in the clojuredocs . It’s important to note that all of these data structures are immutable. For instance, conj adds an element to a collection without mutating it, the trick is just to return a new collection, one that contains exactly the same elements as the original plus the new one. There’s an unavoidable penalty cost of allocating new data for these operations, but it’s very optimized since the structures are persistent and Clojure is hosted in the JVM, which is known for a trustworthy garbage collector. Also, we’re trading that performance cost for correctness guarantees and other performance optimizations using parallel computing, as we’ll see next. Functional programming Clojure is mainly a functional programming language, meaning that it drives you to think in the mathematical sense of functions: a relation between a set of inputs and outputs. Given an input, a function always yields the same value, no matter how many times you call it, and most importantly, it doesn’t modify anything externally visible (AKA side-effects); let’s look at a trivial example. This is a common way to transform a list in an imperative manner: for elem in list\r\n    transform(elem); 1 2 for elem in list transform ( elem ) ; And this is how you would do the same in a functional (lisp) fashion: (map transform list) 1 ( map transform list ) You could argue about the succinctness of the syntax and you could even say that the first is clearer to read, but it is a fact that, if you want certain correctness guarantees , at least two implications arise from the imperative choice. First: you are bound to sequentially and synchronously iterate the list. Why? Because in imperative programming, a procedure ( transform in our example) may cause alterations visible to other procedures (yeah, side-effects!). How do you know that the transformation of one element isn’t affected by the transformation of the previous one? How do you know that the overall result is always the same? To convince yourself, change transform for print . Second: you are forbidden from sharing the list between threads. As you are modifying elements in place, you could be doing so while another thread is trying to access the same element; for those coming from the Java world, this exception may sound familiar. On the other hand, the limitations of programming with functions turn out to help in making safer constructions. Implementing transform with pure functional programming means that it won’t make any visible modification and will also return a new value instead of modifying the previous one. Now, that means you can asynchronously calculate the transformation of each element and safely share the list between threads, what is more, these optimizations can be automatically done by the compiler. Identity and state Here is where actual modifications take place: in order to model real world problems, we sometimes need entities that mutate over time. Clojure’s approach to achieve this, while maintaining immutable data structures and functional constructions, is to have a clearly defined concept of identity separated from the state . Here’s the definition for each concept: Identity: a stable logical entity associated with a series of different values over time. State: the value of an identity at a point in time. The relation between these two concepts is that an identity has exactly one state at any point in time, quoting clojure.org: an identity can be in different states at different times, but the state itself doesn’t change . Notice how the immutable structures perfectly fit as states. To better visualize these definitions you can think of many examples: an identity can be a counter of page visits, while the states of the counter may be 42 or 1000 ; another identity could be the list of current users in a chat group with states being (immutable) lists of users. Just as at any point in time the counter may change from having 42 to having 43 and neither 42 or 43 will change as a value, also the list of users may change from the state [‘user_1’, ‘user_2’] to the new state [‘user_2’, ‘user_3’] without any list being modified but creating a new one and updating the content of the identity. Clojure implements identities with three reference types : Atoms , Agents and Refs . The difference between them is basically their concurrency semantics, i.e. whether their value changes are synchronous or asynchronous and coordinated or uncoordinated. Atoms are a simple reference whose changes are synchronous and atomic but uncoordinated and independent from other references. Agents are also simple references but changes are made asynchronously, so the semantic is to send a change to the Agent, and the change will be applied in an unknown time by other thread. Finally, Refs are the only reference type that implement coordinated modifications. They are backed by Clojure’s implementation of software transactional memory which gives you the power to modify more than one reference within a transaction. Conclusion Clojure aims to be a language used in highly concurrent environments where a lot of work can be parallelized, scaling up by adding more processing units instead of increasing the speed of a single unit. The way to achieve this is by making you think in functional transformations and immutable values, with state changes being very controlled and limited. Although learning a lisp-based language and migrating from other tools can be really hard or expensive, it’s a good exercise to learn this way of reasoning about problems and apply these concepts with more or less difficulty in almost any language. References and further reading clojure.org clojure-doc.org clojuredocs.org robots.thoughtbot.com/tips-for-clojure-beginners clojurekoans.com 4clojure.com", "date": "2014-07-28"},
{"website": "Moove-It", "title": "story-about-gemified-engines", "author": [" mig "], "link": "https://blog.moove-it.com/story-about-gemified-engines/", "abstract": "Needless to say, Engines are a core aspect of the Ruby on Rails framework (versions ~>3.1 and ~>4.0, for now). Having a basic understanding of what they are and how they work is what we wanted to address in these slides. This is a short introduction we made for one of our “Friday Talks”, together with “empanadas” for lunch.", "date": "2013-08-21"},
{"website": "Moove-It", "title": "is-scala-worthwhile", "author": [" Enrique Rodriguez "], "link": "https://blog.moove-it.com/is-scala-worthwhile/", "abstract": "If you’re a Java developer and start digging into some Java blogs, forums, books, etc. you’ll  surely find some people arguing in favor of Scala, and read a lot of people talking about its functional approach and how cool it is. Here I won’t attempt to cover Scala’s features in depth, though I will point out why I think it’s worth giving it a try. Some history Scala was designed in 2001 and its first version was implemented in 2003 on the Java platform, by Ph D. Martin Odersky. He is a  professor of programming methods at EPFL, implemented Java generics and also the current generation of javac, so I am pretty sure this guy has a lot of Java Knowledge 🙂 Later, on January 2011, the Scala team won a five year research grant of over €2.3 million and with this funds, in May, Odersky and his collaborators founded Typesafe Inc. In 2012 Typesafe received a $3 million investment from Greylock Partners. Though you may wonder, is this just a theoretical research from some university? or,  Is it a platform to build enterprise software? Well, after looking at who’s using it, it turns out that many companies are building new services based on Scala, including Siemens, Novell, Sony, LinkedIn, Foursquare and Twitter. Let’s get into some code Syntactic sugar As a newer language it provides some syntactic constructions aiming at the developer efficiency. Some quick examples: Scala val p = new Person // Type inference: no need for declaring variable’s type\r\np incrementAge 1 // Equivalent to p.incrementAge(1)\r\ndef deleteAll(dir: File = File.ROOT) = dir.delete // Method default parameters\r\nobject Person {\r\n    val MIN_AGE = 18\r\n    def adults(people: List[Person]) = people.filter(_.age &gt;= MIN_AGE)\r\n} 1 2 3 4 5 6 7 val p = new Person // Type inference: no need for declaring variable’s type p incrementAge 1 // Equivalent to p.incrementAge(1) def deleteAll ( dir : File = File . ROOT ) = dir . delete // Method default parameters object Person { val MIN_AGE = 18 def adults ( people : List [ Person ] ) = people . filter ( _ . age & gt ; = MIN_AGE ) } Scala aims at not disrupting object oriented principles, that’s why it has an object construction which is exactly what it says: one instance of a class. Provided that, and bearing in mind that you can name an object the same name as a class, there’s no more static keyword, just call an object by its name. Functions As you can read everywhere, Scala is an hybrid, it’s both an imperative object oriented language as well as a turing complete functional one. There’s a lot of interesting theory you can read about functional programming written by excellent authors, so here we’ll just see some examples again: Java def filter[A](list: List[A])(predicate: A =&gt; Boolean) = … 1 def filter [ A ] ( list : List [ A ] ) ( predicate : A =& gt ; Boolean ) = … A function taking another function as a parameter. predicate is a function that takes one element of the same type A of the list and returns whether it should be filtered or not. Scala def filterList = filter(List(“A”, “B”, “AB”, “BA”)) _ 1 def filterList = filter ( List ( “ A ” , “ B ” , “ AB ” , “ BA ” ) ) _ Here we’re creating another function utilizing the previous one but fixing one of its parameters, this returns a new function, which I called filterList. Note that it could have been declared as a val since in functional languages, functions are so called first-class citizens . Scala val startWithA = filterList(_ startsWith “A”) 1 val startWithA = filterList ( _ startsWith “ A ” ) Finally,  we can call the filterList function with a predicate, which given an element of type String (inferred by the List of Strings) computes the standard java.lang.String.startsWith(String). Pattern matching Some people introduce pattern matching as a switch statement with some enhancements. In my opinion this is  like an entire new feature since it’s more powerful and less restrictive. Let’s see an example: Scala list match {\r\n    case List(“A”, “AB”) =&gt; “A -&gt; AB”\r\n    case “ABC” =&gt; “A”\r\n    case Person(age, gender) =&gt; age + “|” + gender\r\n    case i: Int =&gt; “Int: ” + i\r\n    case s: String if s startsWith “A” =&gt; “Starts with A!”\r\n    case _ =&gt; “Default value”\r\n} 1 2 3 4 5 6 7 8 list match { case List ( “ A ” , “ AB ” ) =& gt ; “ A - & gt ; AB ” case “ ABC ” =& gt ; “ A ” case Person ( age , gender ) =& gt ; age + “ | ” + gender case i : Int =& gt ; “ Int : ” + i case s : String if s startsWith “ A ” =& gt ; “ Starts with A ! ” case _ =& gt ; “ Default value ” } Here we can see a lot of things going on. The first one is that we’re pattern matching on list value, and Scala will compare its value with the left side of each case expression, from top to bottom. In the first case we compare against a literal list, there we ask if it’s a list and if it contains those exact two strings. Then we simply compare to a string literal, or we can ask for the value’s type like the Int case. We can also add a conditional guard like the string with the if s startsWith “A” condition. Everything is an expression Another great thing to add is that the whole match we just saw is an expression and it’s returning the value on the right side of the matched case,  meaning that we could write: Scala val myListString = list match { … } 1 val myListString = list match { … } And the compiler will infer the type of myListString, in this case being String,  because all right sides are Strings. Though they could be of other types,  and Scala would  take the base class that all of them extend. In fact, all sentences in Scala are expressions, meaning we can get the result, for example of an if expression: Scala val account =\r\n    if (user.registered) user.account\r\n    else defaultAccount 1 2 3 val account = if ( user . registered ) user . account else defaultAccount Traits You can think of traits either as Java interfaces where you can implement methods,  or as abstract classes with the advantage that you can extend as many as you want. Whatever way you see it, traits are a pretty way of encapsulating behavior and reutilizing code. Imagine you have a Bird class like this: Scala abstract class Bird {\r\n    def flyMessage: String\r\n    def fly() {\r\n        println(flyMessage)\r\n    }\r\n    def swim() {\r\n        println(“I’m swimming”)\r\n    }\r\n} 1 2 3 4 5 6 7 8 9 abstract class Bird { def flyMessage : String def fly ( ) { println ( flyMessage ) } def swim ( ) { println ( “ I ’ m swimming ” ) } } Then you could use it like this: Scala class Pigeon extends Bird {\r\n    val flyMessage = “Flying like a Pigeon” // Yes, Pigeons can swim\r\n}\r\nclass Hawk extends Bird {\r\n    val flyMessage = “Flying like a Hawk”\r\n}\r\nList(new Pigeon, new Hawk).foreach(_.fly()) 1 2 3 4 5 6 7 class Pigeon extends Bird { val flyMessage = “ Flying like a Pigeon ” // Yes, Pigeons can swim } class Hawk extends Bird { val flyMessage = “ Flying like a Hawk ” } List ( new Pigeon , new Hawk ) . foreach ( _ . fly ( ) ) That was easy. Now suppose you need a bird like the Penguin, who can swim but which can’t fly. Then you could make an abstraction like FlyingBird so Pigeon and Hawk would extend it while Penguin could directly extend Bird. Ok, now there comes the Frigatebird, a bird that can fly but cannot swim. This bird will surely drive you crazy because he can’t swim and, in a language like Java, you can’t create a class hierarchy modeling all these birds, without duplicating code. The problem we are facing here is the well known problem of multiple inheritance and how Java handles it, you can create two abstract classes one for flying and another one for swimming, but you can’t extend both. Here’s how you would likely solve it if you’re using Scala: Scala trait Bird\r\ntrait Swimming {\r\n    def swim() {\r\n        println(“I’m swimming”)\r\n    }\r\n}\r\ntrait Flying {\r\n    def flyMessage: String\r\n    def fly() {\r\n        println(flyMessage)\r\n    }\r\n} 1 2 3 4 5 6 7 8 9 10 11 12 trait Bird trait Swimming { def swim ( ) { println ( “ I ’ m swimming ” ) } } trait Flying { def flyMessage : String def fly ( ) { println ( flyMessage ) } } This is how birds would look like: Scala class Penguin extends Bird with Swimming\r\nclass Pigeon extends Bird with Swimming with Flying {\r\n    val flyMessage = “Flying like a Pigeon”\r\n}\r\nclass Frigatebird extends Bird with Flying {\r\n    val flyMessage = “Flying like a Frigatebird”\r\n} 1 2 3 4 5 6 7 class Penguin extends Bird with Swimming class Pigeon extends Bird with Swimming with Flying { val flyMessage = “ Flying like a Pigeon ” } class Frigatebird extends Bird with Flying { val flyMessage = “ Flying like a Frigatebird ” } And now you could execute things like this: List(new Pigeon, new Frigatebird).foreach(_.fly()) 1 List ( new Pigeon , new Frigatebird ) . foreach ( _ . fly ( ) ) But something like this won’t compile: Scala List(new Frigatebird, new Penguin).foreach(_.fly()) 1 List ( new Frigatebird , new Penguin ) . foreach ( _ . fly ( ) ) In this last case, Scala would complain that fly is not a member of Bird. That’s because it infers that Bird is the only class that Frigatebird and Penguin have in common, assigning Bird as the type parameter of the List. So, why Scala? Besides these features we’ve quickly looked at, Scala introduces implicits, structural types, extractors, and many more, taking the best from both worlds (Object Oriented and Functional Programming). Having that said, it could be stated that Scala is concise, flexible and Scalable. I haven’t mentioned performance, but you could be worried about some functional constructions that create new instances instead of writing values in-place. Well, we know that the garbage collector is usually pretty good with short-lived objects but also, thanks to the concise language, you can spend your time optimizing critical parts instead of dealing with tedious non-performance critical parts. There are a few benchmarks that compare Scala to Java and they confirm that it performs fast. As I mentioned earlier, Scala is accepted by the industry, so you can be sure it will last long and its community will be supported. Finally it’s good to know there are plenty of tools to empower Scala, from plugins for almost every IDE and a very cool building tool , to web frameworks and database access libraries . In the worst case scenario you could end up writing Java-style code in Scala and it would be compiled to almost the same binary code and it’d be run in the same JVM. The question we should ask ourselves is: why don’t we just give Scala a try? References scala-lang.org en.wikipedia.org/wiki/Scala_(programming_language) M. Odersky, L. Spoon and B. Venners – Programming in Scala Ed. artima Second edition. Dic 2010 joelabrahamsson.com/learning-scala-part-seven-traits", "date": "2013-09-23"},
{"website": "Moove-It", "title": "ruby-and-exception-notification", "author": [" Adrian Gomez "], "link": "https://blog.moove-it.com/ruby-and-exception-notification/", "abstract": "The problem In a project we are working on right now we needed to have a way to be notified when a crash on our application occurred, something that we though was going to we easily archived by a simple: gem install 'notify_me' After some research we came to the conclusion that it was not going to so easy. Existent solutions If we where on rails we would just have done: gem install 'exception_notification' And even tough i tried to run that gem outside of a rails application with some hacking i was unsuccessful and even i could have done it i felt like there where way too much dependencies for a simple notification email. [toggle title_open=”Close Me” title_closed=”Exception Notification Dependencies” hide=”yes” border=”yes” style=”default” excerpt_length=”0″ read_more_text=”Show dependencies” read_less_text=”Read Less” include_excerpt_html=”no”] i18n multi_json activesupport builder activemodel erubis journey rack rack-cache rack-test hike tilt sprockets actionpack mime-types polyglot treetop mail actionmailer eventmachine multipart-post faraday faraday_middleware hashie json http_parser.rb simple_oauth twitter-stream tinder [/toggle] So we started to take a look to other options like sinatra-enotify , it didn’t seem to cover our needs (you can’t even configure the email settings) and did not seem like a production ready gem. We also investigated bugsnag and airbrake again they seemed overkill for a simple email and even required an account (in some cases paid) on an external site to get them to work. Path Taken Having not found any existent solution and in the spirit to do some research and learning i embarked in the process of writing a simple exception notification gem for ruby. Thus [RU]by [S]imple [E]xception [N]otification ( rusen ) was born. The idea behind it was that it should we really easy to pickup and use but also provide more advanced options and control if it was needed. The simplest way to use it is the following: gem install 'rusen' then use it inside of your ruby script: require 'rusen'\r\n\r\n  Rusen.settings.email_prefix = '[ERROR] '\r\n  Rusen.settings.sender_address = 'some_email@example.com'\r\n  Rusen.settings.exception_recipients = %w(dev_team@example.com test_team@example.com)\r\n  Rusen.settings.smtp_settings = {\r\n    :address =&gt; 'smtp.gmail.com',\r\n    :port =&gt; 587,\r\n    :domain =&gt; 'example.org',\r\n    :authentication =&gt; :plain,\r\n    :user_name =&gt; 'dev_team@moove-it.com',\r\n    :password =&gt; 'xxxxxxx',\r\n    :enable_starttls_auto =&gt; true\r\n  }\r\n\r\n  begin\r\n    method.call\r\n  rescue Exception =&gt; exception\r\n    Rusen.notify(exception)\r\n  end 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 require 'rusen' Rusen . settings . email_prefix = '[ERROR] ' Rusen . settings . sender_address = 'some_email@example.com' Rusen . settings . exception_recipients = % w ( dev_team @ example . com test_team @ example . com ) Rusen . settings . smtp_settings = { : address =& gt ; 'smtp.gmail.com' , : port =& gt ; 587 , : domain =& gt ; 'example.org' , : authentication =& gt ; : plain , : user_name =& gt ; 'dev_team@moove-it.com' , : password =& gt ; 'xxxxxxx' , : enable_starttls_auto =& gt ; true } begin method . call rescue Exception =& gt ; exception Rusen . notify ( exception ) end It also comes with a rack middleware, how to setup that and the more complex ways to use it can be found in the documentation . Conclusion In the end it was an enjoyable experience in witch i learned a lot of things and the end result is something simple yet flexible that we can actually use in our production environment.", "date": "2013-02-01"},
{"website": "Moove-It", "title": "5-basics-to-master-scrum", "author": [" sebastian.suttner "], "link": "https://blog.moove-it.com/5-basics-to-master-scrum/", "abstract": "Agile has been part of our lives for a while now, and most of the time we use it in our own kind of way. Several agile frameworks have been a subject of study by our teams throughout the years and we have learned a great deal from each of them. As it turns out, in the last couple of months we felt like our agile mixin’ needed some kind of structure in order to really make us grow, so we decided to give Scrum a chance. But there is one thing about Scrum that didn’t quite make it for us. The software field is full of Scrum Fanatics who are always trying to evangelize you and make you “buy” this framework. The thing is that we normally question everything and like to draw our own conclusions; thus their fanaticism turned out to be kind of counterproductive. Well, some of us signed up for a ScrumMaster Certification to find out for ourselves. To our surprise, this experience was completely different from what we had expected. We all thought we would be sitting for two days, listening to a guy talk about Scrum and the whole framework (roles, techniques, etc), but the exact opposite happened. Alan (our trainer) had a much more philosophical way of teaching. He didn’t sell us Scrum; he explained to us how it worked and gave us the tools for us to apply it. He didn’t put thoughts in our heads; he guided us. We didn’t just listen about the framework; we learned about life itself. I am not just going to talk about Scrum right now, I want to do more than that. Scrum is something you can continue to learn and investigate by yourselves. I want to share with you all the other things that fascinated us, the ones that really got us thinking and realizing, that there is much more to Scrum than just a framework. I will try to be as concrete as possible; it is up to you to make these concepts grow and change your life. Perspective First of all, we have to be intellectually humble . And by humility we mean, the ability to evaluate ourselves correctly. It is really important that we are convinced that we don’t know everything, that we are probably wrong as many times as we are right, and that others have as much chances to be right as we do. It is most important that we learn to put ourselves in someone else’s shoes. We won’t achieve this until we gain enough humility. Thinking as if we were the team, the client and the product, will lead us to richer and better decisions. Yes, and… We could say there are three kinds of people in this world. There is the negative ones; who will always give “No” for an answer, no matter what we say. We need to avoid teaming up with them. Then there is the frightened ones; they will always find an excuse, a way to avoid challenge, and they are not committed. Their favourite line is “Yes, but…” , and we know they might still be saved if we all try very hard. Finally there is the positive ones; they not only answer “Yes” , they always manage to answer “Yes, and…” . These ones always look at the bright side of things, find ways to make ideas grow and they are committed 100%. These are the ones we want to have by our side in our teams, in our lives. These are the ones we want to be. Always Deliver Value Every customer seeks to get some kind of value in return, and software is not the exception. When someone comes to us asking for a product or service, we not only have to give them what they asked for, we need to make sure it will still be of value to them once we deliver. That is why Scrum tells us to apply the concept of organic growth . Organic growth defines a process in which we iteratively make our deliverables evolve from the previous one, with one little characteristic; we need to assure that every deliverable is functional and that it adds value to our customer. If it does not, it is no good. We need to understand that by having functional and frequent deliveries, we gain the ability to detect errors, bugs or enhancements much earlier, making errors cheap . Continuous Improvement We won’t grow unless we suffer. The only thing we will achieve by being comfortable is decay. We need to get out of our comfort zone to better ourselves, try to break the status quo. Alan defined the concept of Pain-Driven Facilitation , in which he stated that the best way to grow is to challenge ourselves with small, tolerable and constant changes. Once they stop being a challenge, we need to introduce new ones, and so on. There is a Japanese word called “Kaizen” that means “change for the better” which helped us make that concept sunk in. Perfection Some people say that perfection does not exist. I say it does. Whether we are able to achieve it or not is another story. People always try to get what they can’t have, and maybe perfection is one of those things. That is what makes it so unique and desirable. We will always try to get there, because it is hard, because we know we probably won’t. Our ambition is what makes us try harder, learn more and grow to be successful. Scrum is very similar. We know we won’t be able to make everything work as Scrum expects, but it challenges us to do so, and we like it. We will always be improving ourselves to get there. Keep Thinking I don’t feel there is much need to write a big conclusion. There is only one thing I would like to ask you: keep thinking about these concepts and sleep on them for a while. Then, if you think it is worth it, try to spread them out as much as possible, as our trainer did with us. Let’s help people be more humble, have a positive perspective, deliver value, change for the better and be ambitious. Stay tuned, there is more to come! Photo credit: graphistolage", "date": "2013-09-09"},
{"website": "Moove-It", "title": "performance-matters", "author": [" Andreas Fast "], "link": "https://blog.moove-it.com/performance-matters/", "abstract": "Here are the slides from my talk on Performance at http://rubyconfuruguay.org Below you can find the mentioned references and tools. Why does performance matter? Google Search uses site speed in search Ranking Marissa Mayer at web 2.0 The Google Gospel of Speed Nielsen Norman group on web response times Tools to measure: jMeter Rails guides for performance testing Gems that will help you detect problems Query Reviewer Rails Footnotes New Relic Perftools Test The xkcd comic that inspired the test example. The subscription link to Ben Orenstein’s emails with suggestions to speed up the test cycle. Remember! Measure, know where to look and most important write enough unit/functional tests to avoid breaking havoc on your app!", "date": "2013-03-22"},
{"website": "Moove-It", "title": "handsoap-testing-web-services", "author": [" Andreas Fast "], "link": "https://blog.moove-it.com/handsoap-testing-web-services/", "abstract": "If you’re using the Handsoap gem to implement a web service client you may be interested in knowing how to stub the call to the webservice and returning the xml you want. Here is a quick example showing how to do just that. Let’s dive in! I used Mocha to stub the methods. But you can use whatever library you like. When using Handsoap normally you have a class that inherits from Handsoap::Service where you configure the Web Service client. The aim is to use a xml string to mock the web service’s response. When we want to call a webservice we use the invoke method supplied by the Handsoap::Service class. Example: response = invoke('ns:ReturnAccountInfo') do |message|\r\n       message.add 'ns:UserName', username\r\n       message.add 'ns:Password', password\r\n     end 1 2 3 4 response = invoke ( 'ns:ReturnAccountInfo' ) do | message | message . add 'ns:UserName' , username message . add 'ns:Password' , password end So that method is exactly what we want to stub. That way we make sure we test all our code, not worrying about the gem’s code and of course without actually hitting the remote web service. The xml string we would like to get as a response might look like this. <?xml version=\"1.0\" encoding=\"utf-8\"?>\r\n    <soap:Envelope xmlns:soap=\"http://www.w3.org/2003/05/soap-envelope\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\">\r\n            <soap:Body><ReturnMethodResponse xmlns=\"https://www.myawesomeapp.com/webservices\">\r\n                <ReturnMethodResult>\r\n                    <string>hello</string>\r\n                    <tring>world</string>\r\n                </ReturnMethodResult>\r\n                <ErrorCode>0</ErrorCode>\r\n                <ErrorText />\r\n        </ReturnMethodResponse>\r\n    </soap:Body>\r\n</soap:Envelope> 1 2 3 4 5 6 7 8 9 10 11 12 <? xml version = \"1.0\" encoding = \"utf-8\" ?> < soap : Envelope xmlns : soap = \"http://www.w3.org/2003/05/soap-envelope\" xmlns : xsi = \"http://www.w3.org/2001/XMLSchema-instance\" xmlns : xsd = \"http://www.w3.org/2001/XMLSchema\" > < soap : Body > < ReturnMethodResponse xmlns = \"https://www.myawesomeapp.com/webservices\" > < ReturnMethodResult > < string > hello < / string > < tring > world < / string > < / ReturnMethodResult > < ErrorCode > 0 < / ErrorCode > < ErrorText / > < / ReturnMethodResponse > < / soap : Body > < / soap : Envelope > You can get this xml string if you print out the response of a webservice call. Looking for the request call Digging into the handsoap gem I found that the response object of the Service#invoke method is the one returned by: parse_http_response(response) 1 parse_http_response ( response ) How does parse_http_response work? Building the response object If you look at Service#parse_http_response it calls Service#parse_soap_response_document where you can see how it builds the xml object from the raw xml string. So this is exactly how we will build the response object for our stubbed method, namely: xml_string = ‘&lt;xml … &lt;/xml&gt;’ # Your webservice response\r\nxml_doc = Handsoap::XmlQueryFront.parse_string(xml_string, Handsoap.xml_query_driver) 1 2 xml_string = ‘ & lt ; xml … & lt ; / xml & gt ; ’ # Your webservice response xml_doc = Handsoap :: XmlQueryFront . parse_string ( xml_string , Handsoap . xml_query_driver ) This is safe to do since we are using Handsoap’s configuration to parse the string and it will work just as if you are using it in production. Now again in Service#parse_http_response you can see the final line doing: return SoapResponse.new(xml_document, response) 1 return SoapResponse . new ( xml_document , response ) So when stubbing we need to respond with: Handsoap::SoapResponse.new(xml_doc, nil) 1 Handsoap :: SoapResponse . new ( xml_doc , nil ) But before that, make sure that if you did override the on_response_document method, you will have to call it explicitly since the parse_http_response won’t be calling it for us: MyWebService.on_response_document(xml_doc) 1 MyWebService . on_response_document ( xml_doc ) Finally, we are ready to stub the web service with our xml document: MyWebService.any_instance.stubs(:invoke).returns(Handsoap::SoapResponse.new(xml_doc, nil)) 1 MyWebService . any_instance . stubs ( : invoke ) . returns ( Handsoap :: SoapResponse . new ( xml_doc , nil ) ) Bringing it all together xml_string = ‘&lt;xml … &lt;/xml&gt;’ # Your webservice response\r\nxml_doc = Handsoap::XmlQueryFront.parse_string(xml_string, Handsoap.xml_query_driver)\r\nMyWebService.on_response_document(xml_doc)\r\nMyWebService.any_instance.stubs(:invoke).returns(Handsoap::SoapResponse.new(xml_doc, nil) 1 2 3 4 xml_string = ‘ & lt ; xml … & lt ; / xml & gt ; ’ # Your webservice response xml_doc = Handsoap :: XmlQueryFront . parse_string ( xml_string , Handsoap . xml_query_driver ) MyWebService . on_response_document ( xml_doc ) MyWebService . any_instance . stubs ( : invoke ) . returns ( Handsoap :: SoapResponse . new ( xml_doc , nil ) Now, whenever you call invoke on your web service class, it will respond with the xml string you passed in to the XmlQueryFront#parse_string.", "date": "2012-09-03"},
{"website": "Moove-It", "title": "that-big-data-problem-thinking-the-hadoop-way", "author": [" Fernando Doglio "], "link": "https://blog.moove-it.com/that-big-data-problem-thinking-the-hadoop-way/", "abstract": "What is the “big data problem”? “On the night of July 9, 1958 an earthquake along the Fairweather Fault in the Alaska Panhandle loosened about 40 million cubic yards (30.6 million cubic meters) of rock high above the northeastern shore of Lituya Bay. This mass of rock plunged from an altitude of approximately 3000 feet (914 meters) down into the waters of Gilbert Inlet (see map below). The impact generated a local tsunami that crashed against the southwest shoreline of Gilbert Inlet. The wave hit with such power that it swept completely over the spur of land that separates Gilbert Inlet from the main body of Lituya Bay. The wave then continued down the entire length of Lituya Bay, over La Chaussee Spit and into the Gulf of Alaska. The force of the wave removed all trees and vegetation from elevations as high as 1720 feet (524 meters) above sea level. Millions of trees were uprooted and swept away by the wave. This is the highest wave that has ever been known.“ (quoted from https://geology.com/records/biggest-tsunami.shtml ) Now lets use our imagination a bit, and pretend we’re on a digital world, and that an even bigger wave can be seen on the horizon, only that the wave is made up of 1’s and 0’s. That’s the current status of information on the net right now. A huge wave of data is being generated every second, ranging from user generated information such as tweets, status updates, uploaded pictures, blog posts, comments, text messages, e-mails and so on to machine generated data, like server access logs, error logs, transaction logs, etc. And that’s not even the problem, the problem is that we need to start thinking in terms of TB or even PB of information, billions of rows instead of millions of them in order to be able to handle this big wave that’s coming. Normally, when you have to store information on your application you ask yourself one basic question: What do I need this information for? And from the answer you get, you plan your storage and you start saving that specific information. Lets look at an example, from two different perspectives: Traditional way of thinking: Say for example, you’re a web development company and you’re asked to create a basic web analytics  app for your company site.  So you ask yourself: What do I need the  information for? As an answer, you might get something like: To get number of visits to each page. To get a list of referrer sites. To get the number of unique visits. To get a list of web browsers used on the site. It’s a short list, I know, but this is a basic example. Back to the problem: You have your answer, all that information can be fetched from the server’s access log, so you configure your log files to store that information, great! You’re done! Yes, you’re done, you got your system ready, it shows the information you were asked to show, but you also closed the door to other potential analytics that could come out of the information stored on those access logs (like request method used, response code given, size of the object returned and so on) and other sources of information. Thinking in “big data” terms: Thinking in “big data” terms means (at least to me), saving all the information you’re working with on your project and then finding out new and exiting ways to interpret that information and get results out of it. Back to the problem, with the “big data” way of thinking this time: This time around, you think in “big data” terms, so you already have lots of data being saved for every visit, such as: Access log information. Error log information. User input (if there is any) User behavior data (such as clicking patterns and smiliar) and so on. That’s because when you created your website, you asked yourself a different question: What is all the information I can get from my website? And since you changed your question, you significantly change the answer to your problem. You now have a vast amount of information to analyse and get insight from. This is great, but where do we store all this log information? It could potentially become too much for a single machine and we don’t want to loose any information by rotating logs and using other techniques. So another valid question would be: What kind of hardware do I need to store and process all that information in a timely manner? What kind of hardware do we need then? We need some kind of setup that will allow us to: Store vasts amounts of data Process this data in a timely manner Be able to grow as much as we want (storage and processing power wise) Be fault tolerant (storage and processing power wise) Affordable That is a lot to ask (specially if we consider the last point) of a single computer, isn’t it? So the answer will probably come in the form of a distributed system. Enter Hadoop What is Hadoop? In a nutshell, Hadoop is the solution to our problems, one of them (mind you), but a pretty powerful one at that. In more detail, Hadoop is an Open Source Apache project, dedicated to solve two major problems related to big data: Where to store all of the information? How to process that information at an affordable cost and in an reasonable amount of time? To answer these questions, Hadoop provides the following solutions: HDFS This is the Hadoop Distributed File System, it allows us to store in a reliable way all the information we need. This works by interconnecting commodity machines (affordable) and using the resulting shared storage (Store vasts amounts of data). The HDFS takes whatever we throw at it and splits the files into evenly sized chunks of data, and then spreads them throughout the cluster. In this stage, it also replicates the files, providing data redundancy and fault tolerance. Thanks to the HDFS we can have  as much storage capacity as we need, by adding new machines to the cluster (Be able to grow as much as we want ). We also gain a very important asset, that is fault tolerance. Since we’re replicating  the information into several nodes of the cluster, our commodity machines are free to fail and the only place where that will affect us is performance (no data loss or incomplete information). MapReduce This is the other “leg” of Hadoop, an implementation of the MapReduce algorithm proposed by Google in 2004. The MapReduce algorithm allows us to process large amounts of information (terabytes of information) in a distributed (thouthend of nodes) and fault tolerant manner (Process this data in a timely manner) . And if we consider that we already have a cluster of computers working for us with the HDFS, MapReduce is the perfect match to take advantage of that computational power sitting there on every node of the cluster. This algorithm has two basic steps: Map step : In this stage, the input data will be split into smaller chunks to be analyzed and transformed by processes called “mappers”. Thanks to the integration with the HDFS, the main node will effectively schedule map jobs to use the data that’s already on the nodes they’re running on, allowing the system to utilize very little bandwidth. The output of these mapper jobs will be a set of (key, value) tuples. Reduce step : The output of the mappers will be sent into the reduce jobs. These jobs will process the information with an added benefit of knowing that it’s input will be given in a sorted manner by the system. They’re main purpose is to aggregate the information given by the mappers and output only that which is needed. There is an implicit step between 1 and 2, that is the shuffle & sort step , done by the system automatically. In this step, the system will sort the output of all mapper nodes by the key of each tuple and it’ll send these sorted results into the reduce nodes, assuring that all tuples with the same key will go to the same reducer. Thinking the Hadoop way So, we have our data, we have our questions to ask to that data, we have our needs and we have our solution. What now? Your following steps could include: Installing and configuring your Hadoop cluster :  For this step, the company Cloudera has a standarized distribution of hadoop, which they call Cloudera’s Distribution Including Apache Hadoop(CDH). You can download it for free and it comes with serveral other projects from the Hadoop ecosystem (such as Pig, Hive, Hbase, and so on). And for managing your cluster, you could use their Cloudera Manager, which allows you to manage up to 50 nodes for free. Upload the information to your HDFS . Transform your information using a MapReduce job : I consider this step to be optional. I would use a “hand-written” MapReduce job if I had transform my data set in a specific way in order to query it later on. Query your data set : There are several ways to do this, tools like Pig or Hive , allow you write MapReduce jobs (for data transformation) on a higher level language (PigLatin or SQL). Others like HBase and Cassandra work better for quick queries to that data, they work directly over the HDFS ignoring the MapReduce framework, but you’re a bit limited on what you can do with the information. And finally, a pretty common question: Is Hadoop the best solution for big data analysis out there? Probably not, since “the best” is always relative to your needs, but it’s a pretty darn good one, so give it a try. Besides, all the cool kids are doing it: Facebook – 15 PB of information last time they revealed the number. Ebay – 5.3 PB of information on their clusters. LinkedIn Twitter And many others, check out the complete list here.", "date": "2012-03-20"},
{"website": "Moove-It", "title": "howto-create-udfs-using-python", "author": [" Fernando Doglio "], "link": "https://blog.moove-it.com/howto-create-udfs-using-python/", "abstract": "When using Pig to query and transform the information stored on our HDFS, we might need functions that are not part of the default arsenal of Piglatin (the language used by Pig). But the cool thing about this tool, is that we can actually extend the language using UDFs (or User Defined Functions). So what’s a UDF? A UDF is a function that you write in order to extend the language you’re using (in this case: Piglatin). Even though Pig is written in Java, you can use different languages to create your own UDFs. For this specific example, we’ll discuss creating such functions using the Python programming language. By default, Pig allows you to define your UDFs on the following languages: JAVA, Python and Javascript. Currently JAVA UDFs are the ones that have the most extensive support, specially since they have access to additional interfaces (such as the Algebraic Interface and the Accumulator Interface) but you can achieve a lot of things using python and JS as well. If you want more details on how to write UDFs using one of these languages, you can visit the official documentation here The python code For our example, we’ll write a UDF that will transform a human readable date into a number that can be sorted easily. Something that will take: Sun Sep 13 00:00:00 UYT 2009 and transform it into 20090913 Lets get coding then! The code of a UDF is simply  the code of the function, with some optional annotations: #/usr/bin/jython\r\n\r\nimport re\r\n@outputSchema(\"date:int\")\r\ndef simplifyDate(date):\r\n  str_date = ''.join([chr(x) for x in date])\r\n  months = {'Jan':'01','Feb':'02','Mar':'03','Apr':'04','May':'05','Jun':'06','Jul':'07','Aug':'08','Sep':'09','Oct':'10', 'Nov':'11','Dec':'12'}\r\n  matches = re.findall('[a-zA-Z]+ ([a-zA-Z]{3}) ([0-9]{2}).*([0-9]{4})',str_date)\r\n\r\n  if len(matches) &gt; 0:\r\n    matches = matches[0]\r\n    return int(matches[2] + months[matches[0]] + matches[1])\r\n  else:\r\n    return 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #/usr/bin/jython import re @ outputSchema ( \"date:int\" ) def simplifyDate ( date ) : str_date = '' . join ( [ chr ( x ) for x in date ] ) months = { 'Jan' : '01' , 'Feb' : '02' , 'Mar' : '03' , 'Apr' : '04' , 'May' : '05' , 'Jun' : '06' , 'Jul' : '07' , 'Aug' : '08' , 'Sep' : '09' , 'Oct' : '10' , 'Nov' : '11' , 'Dec' : '12' } matches = re . findall ( '[a-zA-Z]+ ([a-zA-Z]{3}) ([0-9]{2}).*([0-9]{4})' , str_date ) if len ( matches ) & gt ; 0 : matches = matches [ 0 ] return int ( matches [ 2 ] + months [ matches [ 0 ] ] + matches [ 1 ] ) else : return 0 Lets go over important lines of the code above: #/usr/bin/jython 1 #/usr/bin/jython Pig uses Jython (a Python interpreter that runs on the JVM) to interpret our python code. Make sure you install the correct version of jython (should be the sameone that your installed version of pig uses). @outputSchema(\"date:int\") 1 @ outputSchema ( \"date:int\" ) An annotation that specifies the output of our function. In our case, we are returning an integer called “date”. There are other annotations that can be used (from the official documentation): outputSchema – Defines schema for a script UDF in a format that Pig understands and is able to parse. outputFunctionSchema – Defines a script delegate function that defines schema for this function depending upon the input type. This is needed for functions that can accept generic types and perform generic operations on these types. A simple example is square which can accept multiple types. SchemaFunction for this type is a simple identity function (same schema as input). schemaFunction – Defines delegate function and is not registered to Pig. If we don’t specify any decorator, Pig will assume that our output is a bytearray. str_date = ''.join([chr(x) for x in date]) 1 str_date = '' . join ( [ chr ( x ) for x in date ] ) In our case, our date to be changed is a string, and it’ll be passed as a bytearray to our function, that is why we need to convert it back to a string. Other than that, the code is pretty straightforward, we use a nice little regexp to get the parts of the date that we want, join them up and return them as an integer. Easy right? Registering your UDFs on piglatin So, we now have the UDF, how do we use it on our piglatin script? Simple! Add the following line to your script: Register  ‘yourpythonfile.py’ using jython as myfuncs; 1 Register ‘ yourpythonfile . py ’ using jython as myfuncs ; Some considerations: Your python file should be on the same folder as your pig script, otherwise, you should use the correct path to find it. All of the functions your define on your file, will be accesible insde the “myfuncs” namespace. So in our case, in order to access our simplifyDate function, we would have to call it like this: myfuncs.simplifyDate. That’s it! You have your UDF working on your piglatin script, congrats! Conclusion Pig is a very powerful data-flow language, and the ability it provides to extend it makes it even more powerful. There is also a place where users contribute their JAVA UDFs to the community, called PiggyBank, and is available to all. According to the oficial documentation, right now, only Java UDFs can be contributed to the bank, but support for Python and Javascript UDFs is on its way. If you want to access the bank, you can checkout the repo with the source code like so (from the oficial documentation page): To build a jar file that contains all available UDFs, follow these steps: Checkout UDF code: svn co http://svn.apache.org/repos/asf/pig/trunk/contrib/piggybank Add pig.jar to your ClassPath: export CLASSPATH=$CLASSPATH:/path/to/pig.jar Build the jar file: from directorytrunk/contrib/piggybank/java run ant. This will generate piggybank.jar in the same directory. To obtain javadoc description of the functions run ant javadoc from directory trunk/contrib/piggybank/java. The documentation is generate in directory trunk/contrib/piggybank/java/build/javadoc. To use a function, you need to determine which package it belongs to. The top level packages correspond to the function type and currently are: org.apache.pig.piggybank.comparison – for custom comparator used by ORDER operator org.apache.pig.piggybank.evaluation – for eval functions like aggregates and column transformations org.apache.pig.piggybank.filtering – for functions used in FILTER operator org.apache.pig.piggybank.grouping – for grouping functions org.apache.pig.piggybank.storage – for load/store functions (The exact package of the function can be seen in the javadocs or by navigating the source tree.) That’s it for now, but if you want to checkout the official docs, go to this url: http://pig.apache.org/docs/latest/udf.html", "date": "2012-05-24"},
{"website": "Moove-It", "title": "agile-development-on-rails", "author": [" Mariana De Carli "], "link": "https://blog.moove-it.com/agile-development-on-rails/", "abstract": "Do you want to learn how to make some cool Web developments using Ruby on Rails? Have you searched but you didn’t find a course that fills all your requirements? Moove-It is inviting you to be part of “Agile development on Rails”, our new two months e-learning course. Using the Uruguayan e-learning platform “Hacé un Click” (http://www.haceclick.com.uy/) we’ve developed a really dynamic course with all you need to know about servers like Ruby, Git, Deployment, Ruby on Rails, Unit Test and Rspec. The coolest thing about his new course is that it will be based in a real practice case, that is; the entire course designed as a real project so you can have a real time learning experience working directly on the code and see you final results on a real prototype.", "date": "2012-05-24"},
{"website": "Moove-It", "title": "running-selenium-on-a-headless-server", "author": [" matias.astor "], "link": "https://blog.moove-it.com/running-selenium-on-a-headless-server/", "abstract": "When it comes to functional testing, please have a look at Selenium, a testing framework that provides tools and asserts on a webpage running on a web browser. The aim of this article is to show you how to set up a Selenium server, with or without Xserver, and run PHP unit tests on a web page. Before starting you will need: Linux running machine (with or without Xserver) Java Selenium server jar file (can be found here ) You will learn how to install the following: Xvfb (check this ) [just if we dont have a Xserver] PHPUnit – PHP – Pear – Selenium TestCase extension (PHPUnit_Extensions_SeleniumTestCase) I am going to use Fedora for this example, but bear in mind that the only thing that might change in other Linux ditros could be the package manager. After downloading the Selenium server jar file, and installing Java, you will need to follow these steps: Xvfb: [box]➜ ~ sudo yum install xvfb[/box] Now that you’ve got it, you will need to start it up like this: [box]➜ ~ Xvfb :99 -screen 0 800x600x16[/box] Then, you have to export an environment variable called Display and set the value :99. [box]➜ ~ export DISPLAY=:99[/box] At this point, you can start the Selenium server. You can found it here . Please note that in the following example, I have renamed the Selenium server jar to selenium-server.jar. [box]➜ ~ java -jar selenium-server.jar[/box] Now that it’s launched, you might go ahead and develop some tests in PHPUnit. Remember that you will need to have PHP, Pear, PHPUnit and PHPUnit Selenium’s extension to get this working. [box]➜ ~ sudo yum install php php-pear[/box] Now you have to update pear. [box]➜  ~  sudo pear upgrade pear[/box] After that, you need to set auto_discovery to 1, and install PHPUnit extension: [box] ➜  ~  sudo pear config-set auto_discover 1 ➜  ~  sudo pear install pear.phpunit.de/PHPUnit [/box] Next, you can install the Selenium PHPUnit extension. [box]➜ ~ sudo pear install phpunit/PHPUnit_Selenium[/box] Alright, we should be fine to start writing some tests, let’s check an example (taken from here , and modified so it works for the example); &lt;?php\nrequire_once 'PHPUnit/Extensions/SeleniumTestCase.php';\n\nclass WebTest extends PHPUnit_Extensions_SeleniumTestCase {\n\n  protected function setUp() {\n    $this-&gt;setBrowser('*firefox');\n    $this-&gt;setBrowserUrl('https://blog.moove-it.com/');\n  }\n\n  public function testTitle() {\n    $this-&gt;open('/');\n    $this-&gt;assertTitle('Our Geek Space | Moove-it blog');\n  }\n}\n?&gt; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 & lt ; ? php require _ once 'PHPUnit/Extensions/SeleniumTestCase.php' ; class WebTest extends PHPUnit_Extensions_SeleniumTestCase { protected function setUp ( ) { $ this - & gt ; setBrowser ( '*firefox' ) ; $ this - & gt ; setBrowserUrl ( 'https://blog.moove-it.com/' ) ; } public function testTitle ( ) { $ this - & gt ; open ( '/' ) ; $ this - & gt ; assertTitle ( 'Our Geek Space | Moove-it blog' ) ; } } ? & gt ; Imagine you have a test that has failed, and you want to know where it has failed. Remember you are on a headless server, so you are not seeing what the explorer is actually doing. Selenium can show you where the test has failed, by taking a screenshot. In the next example I will show you how to take a screenshot when a test fails. You will see, it’s pretty simple. &lt;?php\nrequire_once 'PHPUnit/Extensions/SeleniumTestCase.php';\n\nclass WebTest extends PHPUnit_Extensions_SeleniumTestCase {\n  protected $captureScreenshotOnFailure = true;\n  protected $screenshotPath = '/home/maltray/';\n  protected $screenshotUrl = 'http://localhost/screenshots';\n\n  protected function setUp() {\n    $this-&gt;setBrowser('*firefox');\n    $this-&gt;setBrowserUrl('https://blog.moove-it.com/');\n  }\n\n  public function testTitle() {\n    $this-&gt;open('/');\n    $this-&gt;assertTitle('nope');\n  }\n}\n?&gt; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 & lt ; ? php require _ once 'PHPUnit/Extensions/SeleniumTestCase.php' ; class WebTest extends PHPUnit_Extensions_SeleniumTestCase { protected $ captureScreenshotOnFailure = true ; protected $ screenshotPath = '/home/maltray/' ; protected $ screenshotUrl = 'http://localhost/screenshots' ; protected function setUp ( ) { $ this - & gt ; setBrowser ( '*firefox' ) ; $ this - & gt ; setBrowserUrl ( 'https://blog.moove-it.com/' ) ; } public function testTitle ( ) { $ this - & gt ; open ( '/' ) ; $ this - & gt ; assertTitle ( 'nope' ) ; } } ? & gt ; Here are some more advantages of using Selenium: Run functional tests on a website, going either forward or backwards. Check through an API if the data introduced in a form is correct! (not the actual point of Selenium, but it could be useful sometimes…). Work with almost any language you need and use different drivers. For example you can work on PHP ( php-webdriver-bindings [using Selenium 2 – webdriver -], PHPUnit Extension [which I explained above, using Selenium 1], Facebook’s webdriver [Using Selenium 2 -webdriver-]), on Ruby (A LOT of integrations here!), and on Java, etc, etc. That’s all for now, please let me know if you have any questions!", "date": "2012-07-16"},
{"website": "Moove-It", "title": "digital-blackout-against-sopa-pipa", "author": [" Mariana De Carli "], "link": "https://blog.moove-it.com/digital-blackout-against-sopa-pipa/", "abstract": "The largest online protest in the history of Internet is taking place today, more than one hundred sites, including the popular Wikipedia, Google and WordPress confirmed their participation in this digital blackout against the new anti-piracy laws of the USA. Stop Online Piracy Act (SOPA) and Protect IP Act (PIPA) will be voted on Jan 24 th by the Congress in attempt to pass internet censorship in the Senate. These two laws are probably the most rejected ones by Americans citizen because some of them consider that they affect the most appreciated thing on internet, freedom. The SOPA law attempt to close any foreign site which sells or shares pirated content from the USA, including music, films, books and every product non authorized for free distribution on the internet. The PIPA law meanwhile has its focus directly on protect Intellectual Property Act, avoiding any economic threats and thefts to creativity. Anyway, these laws have great support from big industries like National Cable & Telecommunications Association, the National Association of Theatre Owners, Viacom, Copyright Alliance and NBC Universal, which argue that their businesses are dramatically affected by online piracy. We will have to wait until next January 24 to see if the public opinion will have a direct influence on the fate of these laws. Wikipedia.org – Home page Jan 18 th Google.com – Home page Jan 18 th WordPress.com – Home page Jan 18 th", "date": "2012-01-18"},
{"website": "Moove-It", "title": "we-proudly-announce-our-new-website", "author": [" Mariana De Carli "], "link": "https://blog.moove-it.com/we-proudly-announce-our-new-website/", "abstract": "We want to thank everyone who was involved in this challenging and creative process, and truly hope you like the results. We certainly enjoyed the creation. Part of the celebration was thanks to Mariana de Carli, who brought some delicious homemade cupcakes. Thanks Mariana! They were yummy! Please have a look at our new website, and leave us your comments if you have a chance. Keep on mooving!", "date": "2012-10-17"},
{"website": "Moove-It", "title": "ruby-websites-for-beginners-cool-initiatives", "author": [" Mariana De Carli "], "link": "https://blog.moove-it.com/ruby-websites-for-beginners-cool-initiatives/", "abstract": "The coolest thing about Ruby is that even though it’s a dynamic and reflective programming language it’s very easy to learn. More and more programmers around the world are interested in learning this new tool for making cool things. We’ve selected 3 interesting websites to help people (kids or adults) to know a little bit more of Ruby’s world. Kids Ruby (to kids) http://kidsruby.com : Kids Ruby is especially focused on kids, with a very easy interface which allows you to see the code, run it, and at the same time see what it outputs. Kids Ruby is also attractive for his Turtle graphics that allows you to draw pictures and have fun by mixing and trying colors. Kids Ruby includes a lot of useful resources and you don’t even need an internet connection to work. Developers also created a complete KidsRuby operating system based on Ubuntu Linux that makes program in Ruby a lot easier for kids. Rails for zombies https://www.pluralsight.com/courses/code-school-rails-for-zombies : Rails for Zombies offers an open-source web framework with all the power of the Ruby language and with no additional configuration needed. In this site you can see tutorial videos which allow you to learn more about Ruby on Rails in just five levels. After seeing each video you’ll be challenged with cool exercises to practice your new skills. So if you’re a zombie and you’re hungry for Ruby’s knowledge this is the perfect site for you. Try Ruby https://www.pluralsight.com/search?q=ruby : This website brings a very interactive Ruby tutorial; you can test new functions step by step and understand a little bit more about this language. In just 15 minutes and with a very interactive interface you can understand what Ruby is about. This site also allows you to save your progress by sign up for free at Code School . Now that you have these very easy options to learn Ruby why don’t you try it out and maybe we’ll see you soon as a new member of Moove-IT’s team 😉", "date": "2012-01-26"},
{"website": "Moove-It", "title": "moove-it-is-meeting-up", "author": [" Mariana De Carli "], "link": "https://blog.moove-it.com/moove-it-is-meeting-up/", "abstract": "We say YES to the Tech Meetup taking place this 3rd November, 2012 in Montevideo, Uruguay. This is an opportunity for you to share your IT experience and learn from others, grow as professionals and constantly improve your work. Through the story of seven speakers, we will learn from the experience of professionals from Uruguay and the region in the most different environments. Some of the topics are: Development Processes (teamwork, scrum, Agile, continuous integration, code review, continuous delivery, testing, QA), Infrastructure and Production (monitoring, scaling and server redundancy, track changes, versioning, deployment) and Others (remote teams, real experiences). If you want to share your own experience you still have time to sign up as a speaker (whether you work in a large company or from home). We want to hear from you! You just have to complete the form you will find at www.meetup.com.uy. The event is $ U400 and you can book your place through “Red Pagos” (collective number 34264). If you are a student, you can request a free pass for the event. Just apply through the event website. Do you like the idea? We will see you next November 3rd, 2012 at 9:00 am at the Telecommunications Tower (Rondeau Ave. and Guatemala St.). Please contact info@meetup.uy for more information. Afiche difusión meetup.uy", "date": "2012-10-16"},
{"website": "Moove-It", "title": "why-uruguay", "author": [" Mariana De Carli "], "link": "https://blog.moove-it.com/why-uruguay/", "abstract": "Outsourcing to Uruguay does make sense. According to CUTI, the Uruguayan Chamber of Information Technologies, Uruguay’s strategic location, cultural affinity and economic stability are some of the key factors that make this country one of the top technology producers in Latin America. Please have a look at the infographic below for further information.", "date": "2012-06-27"},
{"website": "Moove-It", "title": "meet-moove-it-ux-group", "author": [" sebastian.suttner "], "link": "https://blog.moove-it.com/meet-moove-it-ux-group/", "abstract": "What do users really need? That’s the question any software developer should ask themselves. Here at Moove It, we always put ourselves in our clients shoes to understand their needs and give them exactly what they are looking for. In order to do so we’ve created the UX Department. From the moment we started gathering up to discuss latest design patterns and the top UX tendencies, we knew something great would come out of it, and so it did. We managed to nurture the whole team with what we’ve learned, improve existing products and enhance new projects’s design from scratch. We’ll keep working as hard as possible on UX, not only because of how thrilled we’ve got with the results, but also because the way the users feel the product is what matters most. We present partial conclusions found by the UX group. We share the presentation (in spanish)", "date": "2011-11-24"},
{"website": "Moove-It", "title": "introduction-to-node-js", "author": [" cherta "], "link": "https://blog.moove-it.com/introduction-to-node-js/", "abstract": "I’m gonna talk about node.js and the benefits of using node.js on the server side. First lets clear some concepts. Regular web servers use threads (OS threads) for handling requests. 1 request = 1 OS threads 1000 requests = 1000 OS threads Every OS thread takes some memory from the system and does not return it till the thread execution is finished (i.e: servers the response). Most of the time when doing I/O the thread just sits there and waits for the I/O operation to finish. var result = query(“select * from users”)\r\n//do something with result 1 2 var result = query ( “ select * from users ” ) //do something with result The conclusion is simple, in order to serve applications with a lot of users making simultaneous requests or even holding requests from your web server, like chats or real time applications you can’t use regular web servers because it is impossible to scale. Entering the loop Ok, so there must be some other ways to handle simultaneous requests in an efficient way; yes there is a better way and is called the event loop. Instead of using OS threads, the event loop uses green threads or co-routines. Co-routines cost less than regular OS threads and can switch from one context to another, preventing an unnecessary memory allocation. Using green threads is much more efficient than using regular OS threads but still has some problems: Context switching still costs There is still machinery involved to create the illusion of holding execution on I/O var result = query(“select * from users”) 1 var result = query ( “ select * from users ” ) We are still using this way of I/O In order to code in an event loop you still need to know which functions will block and which ones won’t block. It is difficult to combine the event loop libraries with regular, I/O blocking libraries. Node js Node js is a server implemented on top of the V8 the chromium (and chrome) javascript engine and in several ways is better than using an event loop system. It uses one OS thread to handle multiple clients connections Programs are written in plain javascript, server side javascript so, basically with node js we are using one language for client and server. Does I/O better, because we are using javascript, for us javascript developers is easy to understand something like this: db.query(“select * fro users”, function(data){\r\n//Somethig with users\r\n}); 1 2 3 db . query ( “ select * fro users ” , function ( data ) { //Somethig with users } ) ; We use callbacks all the time and node behaves this way every time I/O is performed. File I/O is non blocking same as db queries. It performs really well serving large requests simultaneously, comparable with specialized web servers like nginx, take a look at the presentations. It will improve performance because Google will continue improving the V8 javascript engine. Node js example A node js program will be something like this: var http = require('http');\r\nhttp.createServer(function (req, res) {\r\n  res.writeHead(200, {'Content-Type': 'text/plain'});\r\n  res.end('Hello World\\n');\r\n}).listen(8124, \"127.0.0.1\"); 1 2 3 4 5 var http = require ( 'http' ) ; http . createServer ( function ( req , res ) { res . writeHead ( 200 , { 'Content-Type' : 'text/plain' } ) ; res . end ( 'Hello World\\n' ) ; } ) . listen ( 8124 , \"127.0.0.1\" ) ; console.log('Server running at http://127.0.0.1:8124/'); 1 console . log ( 'Server running at http://127.0.0.1:8124/' ) ; Taken from node js site: https://nodejs.org/en/ As you can see even the server creation is using the callback approach. Conclusion Web development has change 180° since first static web pages appear, so there is no surprise that regular web servers evolve as well. Regular web servers based on OS threads are great to deliver occasional content in a fast and efficient way but they fail when we try to develop large rich web application that use the wire not only request data. New “cool” developments like google talk on the browser, the facebook chat an many others are changing what we expect from the web to be, and we will need web servers capable of supporting this new features. So let’s start playing around with node, or nginx or anything you feel comfortable with, and see what happens. Resources This presentation I did it here at moove-it. I also showed and coded some examples. Reference https://nodejs.org/en/ http://code.google.com/p/v8/ https://github.com/ry/node/wiki http://howtonode.org/ https://www.nginx.com/resources/wiki/ http://jsapp.us/ (test your node js developments)", "date": "2011-06-23"},
{"website": "Moove-It", "title": "grasp-principles", "author": [" Gian Zas "], "link": "https://blog.moove-it.com/grasp-principles/", "abstract": "Slides for the presentation given here at Moove-it about GRASP Principles and Object Oriented Design GRASP Principles View more presentations from snmgian .", "date": "2011-06-10"},
{"website": "Moove-It", "title": "damian-arrillaga%e2%80%99s-robot-story", "author": [" Ariel Luduena Karhs "], "link": "https://blog.moove-it.com/damian-arrillaga%e2%80%99s-robot-story/", "abstract": "Writing about these things makes my day. Today:  Damian Arrillaga’s ROBOT story. Damian is a very young and clever System Engineering student from the State University of Uruguay; he is also an important member of our team. He is simply brilliant. I have no other words to describe him. His interest in Robotics started in 2009 when he and his friends heard about a contest in which everyone has to program a Robot which would have to compete one-on-one against an other Robot. This is called “Sumo.UY” (Sumo Fight Contest). Once Damian found out about these Robot’s contests, he couldn’t help but falling in love with them. This is why he has decided to study System Engineering (aka Computer Science) and to major in Robotics. In 2009, he and his friends Ismael, Nacho, Marcos and Anael decided to take part in the annual Sumo Fight Contest.  After long hours of programming and hard work, they enrolled in the competition and won the contest. It was a fantastic experience for the whole team. Last year, they did it again. Won again. This year they decided to take this contest to the next level. They built a Robot from scratch, using tape, paper, mouse pads’ pieces and basically anything they came across. “CachaBot” the Robot.  Look at these pictures: And they got the 4th place!  For some reason “CachaBot” consumed more energy than they had expected. The funniest part was when Damian had to run away in the middle of the competition in order to buy new batteries for their robot … so it would not get rid of energy to finish the fight. They didn’t win this time, but it was definitely worth it.  It was a wonderful experience. A new challenge, though an old passion…", "date": "2011-10-06"},
{"website": "Moove-It", "title": "html-css-best-practices", "author": [" Veronica Rebagliatte "], "link": "https://blog.moove-it.com/html-css-best-practices/", "abstract": "80% of the end-user response time is spent on the front-end. (YSlow Team) By following these best practices we can have a great impact over the performance of our sites and applications. In these slides we will go through some best practices related to performance, semantics & accessibility and patterns for better maintainability and readability which is gold when collaborating. In the second part of the slideshow we will share some tips on how to pick the best layout available, create the slices with optimization in mind, master the basics and stay organized form the beginning with your CSS code. Html & CSS – Best practices View more presentations from Veronica Rebagliatte", "date": "2011-08-24"},
{"website": "Moove-It", "title": "introducing-oranged-a-pokki-based-app-for-reddit", "author": [" Juan Pablo "], "link": "https://blog.moove-it.com/introducing-oranged-a-pokki-based-app-for-reddit/", "abstract": "About a month ago I found Pokki with a friend, Pokki is a framework that uses chromium and enables developers to make light desktop applications using only HTML5, Javascript and CSS3. Right now it’s supported on Windows XP, Vista and 7, but there are plans to support Mac before the end of this year. Pokki also provides an app store on your Windows taskbar that has benefits for users and developers, users are able to install or uninstall apps with only one click and developers gain a lot of visibility for their apps. In order to publish your app on the app store you need to have your app approved by the Pokki compliance team, they basically check that your app meets quality standards, design guidelines and security procedures. So, my friend and I thought Pokki was cool enough to give it a try and we decided to make an app there. We wanted to take advantage of desktop notifications pokki has, and I’m an avid reddit user, that’s why we decided to make an app to check your messages, comment replies and latest news from reddit. In the meantime we found out that there was a contest to win a Samsung Galaxy Tab for the best app submitted before August 1st, so that was our deadline! I also gave a talk at moove-it to present Oranged and share all the stuff I learned working with Pokki. It was really nice to see other people excited on what we were doing and people interested on working on their own pokkies. We made our best effort to make it on time and we submitted Oranged 4 minutes before the contest ended. Unfortunately our app was rejected because it had some bugs and didn’t have the expected quality. Nevertheless the people from pokki were very excited with our app and told us to keep working on it because they really liked the app. So we kept working on it and submitted Oranged a couple of days ago, this time it was accepted! We didn’t win a tablet but we are very happy for what we’ve accomplished together. Here it is… If you are a redditor give it a try and I’m sure you will like it. You can also check the source code here , we used jquery templates, css3, localStorage and other cool stuff.", "date": "2011-08-20"},
{"website": "Moove-It", "title": "alternatives-to-full-text-queries-part-ii", "author": [" Fernando Doglio "], "link": "https://blog.moove-it.com/alternatives-to-full-text-queries-part-ii/", "abstract": "For part one, click here … What do they have in common and what makes them different? Even though it’s hard to come up with a comparison table between all four alternatives, mainly because I can’t claim to have personal experience with all of them, the Internet has a lot of information on the subject, so I went ahead and did a bit of research on the matter. Another point of interest to consider is that though on the long run, all four solutions provide very similar services; they do it a bit differently, since they can be categorized into two places: Full text search servers : They provide a finished solution, ready for the developers to install and interact with. You don’t have to integrate them into your application; you only have to interact with them. In here we have Solr and Sphinx . Full text search APIs : They provide the functionalities needed by the developer, but at a lower level. You’ll need to integrate these APIs into your application, instead of just consuming it’s services through a standard interface (like what happens with the servers). In here, we have the Lucene Project and the Xapian project . Taking all of this into account, we can now proceed into a more in-depth discussion about our options: Full text search servers Like I’ve discussed before, search servers provide an end solution, ready to be installed, to be tweaked into your needs and to be interfaced with. The installing and tweaking process can be hard, depending on your specific needs, but the bright side is that all of it is done outside your application and your code. Once you’re done and the server is installed according to your needs, the only step left is interacting with it from your application using one of the methods provided by the server. So, what do they have in common? They’ll both satisfy your needs regarding searching and indexing speed, since they do it very efficiently. They both have a long list of high-traffic sites using them, as we’ve seen above. Both offer commercial support, which is great if you’re planning on developing a commercial application to use them. Both offer client API bindings for several platforms/languages. Both can be distributed to increase speed and capacity. They both have great support for advanced querying. This allows them to use proximity search, relevance sorting and so on. Some differences: Solr is an Apache Project built on top of the Lucene Project, allowing it to improve some of it’s features whenever the Lucene projects updates. Sphinx is an isolated project, which requires it to improve independently. Solr supports the use of wildcards for it’s searches, while the current release version of Sphinx (0.9) does not. The latest beta for Sphinx (2.0) though, appears to be supporting this feature. Solr comes with spell check for search terms out of the box, while Sphinx does not. Solr can parse rich text formats like Word documents, PDF files and so on; Sphinx does not provide this feature out of the box. Sphinx integrates more tightly with RDBMSs, especially MySQL since it was built with this functionality in mind. Sphinx can use the stopwords for a more relevant result ranking by default, whether Solr extracts them before doing the search. Sphinx supports SQL as a query language, when Solr forces you to learn it’s own language. Sphinx does not support multithreading on Windows machines, which slows down it’s performance on this OS. So, which one is better? As you might have guessed, there is straight answer to this question, since they both do a similar job, but have different strong and weak points. This allows them to be better than the other for some specific cases. On an out-of-the-box basis, for a generic search engine implementation, my opinion (which should be taken lightly, since it’s only the opinion of a single developer) would be to implement it with Solr. This resolution comes based on the features that Solr provides out of the box. Some of them, being quite important for a search engine, such as wildcard support, rich text format parsing, and so on. On the other hand, if your needs are as specific as indexing content from a database, then Sphinx would be the way to go, since it appears to have integration with that kind of content natively, which would assure a higher performance over other solutions. Full text search APIs In this case, we’ll be comparing APIs, which could be thought as “tools” to add to the “tool box” provided by the programming language you’re using. Our two contestants are the Lucene Project and the Xapian project . They both have quite a number of followers and applications that use them, so lets see what else they have in common: They’re both highly portable. Lucene is written in JAVA, and thus it works on any JAVA capable OS. Xapian on the other hand is written in C++ and has support for most of the operative systems on the market. They both support rich text formats, which is definitely a plus if you’re trying to index any type of information (no need to pre-process them yourself). They both support advance search mechanics, like wildcards, proximity search, stemming, and so on. They’re both able to add data into their index on a real-time basis, making information accessible immediately. Basically, Lucene and Xapian are both very much alike: they’re both very powerful and very customizable. And yet, they have some differences, as we’ll see next. Some of the differences Lucene does not support faceted search, whilst Xapian does. Lucene does not support spelling corrections out of the box, whilst Xapian does. Xapian has support for synonyms out of the box, Lucene does not. Lucene has been ported into other programming languages (like PHP, Ruby, C#, etc, etc) providing Lucene-like APIs for those languages. Xapian provides bindings for other languages, but maintains a core API that’s always the same, securing that no matter which binding it is that you’re using, you’re always working with the official distribution and not a poorly done copy. So, which one is better? In this case, the answer (according to my research) to this question is not, as one would expect, the same as the one between Sphinx and Solr. According to what we can see from our list of common and different features, one would be inclined to believe that Xapian is the way to go (at least I am!) since it is clearly superior (on an out-of-the-box basis of course). So, why would we ever pick Lucene? Well, for starters, Lucene’s ports tend to have a good integration with some of those languages’ most famous frameworks (on php, Zend framework implements the Lucene search API, Ruby has it’s own implementation as well, called Ferret, which in turn has a RoR plug-in, and so on), which would ease the development process considerably. This could actually be a major point in favor of Lucene, if what we’re already using (or are planning on using) one of those frameworks. Final thoughts All in all, there are many solutions out there worth the try, even though I’ve only covered those that would appear to be the four most known (or used) options. There are others like DataparkSearch , Ht -// Dig , mnoGoSearch , KinoSearch , and a very long etc, which might be the right pick for your needs. What is really important to remember is that database engines are not the only solution out there for data handling. And also that full text search solutions are not the silver bullet needed to kill the proverbial werewolf that represents our searching problems either. We need to think about our needs very carefully before choosing a technology or we can end up having that proverbial wolf biting our rear…", "date": "2011-10-12"},
{"website": "Moove-It", "title": "productivity-at-work-2", "author": [" sebastian.suttner "], "link": "https://blog.moove-it.com/productivity-at-work-2/", "abstract": "When it comes to work, being organized and having good habits always helps. The more we know our working habits, the better we exploit our virtues and manage our weaknesses. Increasing productivity is not about working harder and harder, but about using every available resource to achieve better results—with no extra effort. Many small decisions we take throughout the day can lead us to either a very productive day, or end up turning it into a complete failure. Now I’ll point out some concepts I think can be very useful to optimize our way of working: Tasks Planning Spending the last minutes of the day planning for the next one could end up being highly productive. We can go home knowing which tasks will be the important ones to accomplish, as well as our goals for the following day. It’s about knowing what to do, what may go wrong, and being prepared for unexpected problems, which can always surprise us whether we like it or not. Key tasks Always identify the most important tasks for the day. It’s highly recommended to start by one of them, also keeping in mind that those tasks will probably represent more than 50% of your productivity. Routine tasks Do not start with these ones. The key is to find the right moment for doing them. Maybe when our workflow is low, we are less tied up with things, or our productivity is not at our highest. Sooner or later, we’ll have to deal with them. Large tasks Don’t be afraid of them. Finishing those ones early will clear the rest of the day up. Divide & Conquer is a great strategy that can helps us out a lot. We can accomplish great things by breaking them up into little ones. Solving simpler and smaller stuff first, and then putting everything back together is a good practice. Although it seems pretty intuitive, most of the times we forget to do this. Microtasks It could be very productive to apply the “2 minute” rule. It states that whenever we come across tasks that can be finished in one or two minutes, we should do them right away. If we are not in the middle of something important, getting those sort of tasks done can speed things up a lot. The only thing we should ask ourselves is whether we think a task can be wrapped up quickly, and if so, go ahead and do it. If we put them off, they will slow us down later. Avoid Multitasking Multitasking generates stress and most important reduces our concentration and creativity. Let’s focus only on one thing at a time and do it right. Concentration Digital environment For those of us who work all day long with our computers, there is a great variety of sites and applications that are just one click away. As exciting as it may seem, it also ends up being a true source of distraction. It’s considered a good practice to think of them as if they were not there, that way we can focus entirely on our tasks until having them finished. Interruptions Automated notifications and cell phones are the two main concentration breakers. How many times do we check our phone just to see if anything new came up? How many mouse clicks do we spend on closing notification pop-ups? We should deactivate them and keep our phone away so we can work disconnected and check any of them when we are not in the middle of something. Intensity Doing work sprints, i.e. working non-stop for short periods of time, helps to stimulate concentration and to increase intensity. It also lets us rest from time to time and stay relaxed. Information We’ve never had that much information at the reach of our fingertips. Maybe too much. That’s why it’s important to know how to ration it out. Feeds Choose carefully which ones are interesting, meaningful or simply those ones that are worth reading. Ignore the other ones or spend the rest of the day reading them. Social networks A well known sickness for some of us. They always manage to take a lot of our precious time, therefore we should avoid them at work or use only the ones that have something to do with it. Don’t keep them open in the background. E-mail Knowing how to use it wisely is very important for those of us who employ it both as a working and as a communication tool with colleagues and clients. We have to make it wait. To check it obsessively generates too much distraction and gives us the wrong feeling of being more effective, but it’s not true. It’s better to read it periodically and answer only the important ones. It also helps a lot if you keep it organized, e.g. with the use of labels and filters or any other technique. What you shouldn’t do is to get lost inside of it searching for a message. Rest time Free time Always find time for hobbies. Sports, outdoor activities, or some kind of art, can all help to enhance our creativity and mental clarity. That’s exactly why here at Moove-IT we always arrange extra group activities we can enjoy together. Night’s sleep A good night’s sleep is imperative to make our day pay off. Between seven and eight hours should keep us far from being a zombie in front of a computer. Breaks It’s always good to take a break and stretch our legs a bit. Free our mind from work for a while. Here we simply go to the kitchen and have a cup of coffee with some of our colleagues. There’s always someone willing to have a nice short chat! Tools Paper & pencil A faithful friend by our side. Some of us can find very useful to have both paper and pencil handy and write everything down. Even when something looks extremely trivial, we might forget it and it’ll still be there when we need it. There’ll always be time to decide what to do with it later. Whiteboard and post-its A great way to keep every important tasks in sight. We use them to manage our TO-DOs, so that everyone can be aware of everyone else’s responsibilities. Software We all work with the software we find more comfortable. There are a lot of tools, some tend to be more productive, others have some other kind of advantages. “Different strokes for different folks”. Projects Fake limit date It helps a lot to set a fake previous date limit and work hard to finish in time. If so, we can have time to review everything. Otherwise we can still try to get it done. Phases As I said earlier, breaking things up always helps. Then we can base ourselves on more estimable parts of the project and make a more accurate planning. Daily and weekly evaluation It’s absolutely necessary to know if the project is going in the right direction. Making evaluations helps to keep track of everything and plan following phases even better. Proactivity Solving our problems and being successful in what we do is strongly related to be the ones who take the initiative. Take the first step, call for a meeting, be the one who investigates, make a phone call, stay always active and most important, be positive about it. Getting desperate is the first step into failure. Restrospective By keeping record of our achievements, we’ll always be aware of the things we can accomplish and also find motivation in times we need it the most. It’s very important to be self-critical, learn both from our mistakes and our successes. All these things I’ve mentioned are not meant for you to do by the book, I just think it can turn out to be quite useful to bring some of these good practices into our work routine. I’m not saying it’ll be easy, I also have a hard time trying to do so. Let’s always try to improve and better ourselves. Enjoy the jokes! References Productivity tips from thinkwasabi.com Click on images to see their source.", "date": "2011-07-15"},
{"website": "Moove-It", "title": "seba-borrazas-won-the-programming-contest", "author": [" Ariel Luduena Karhs "], "link": "https://blog.moove-it.com/seba-borrazas-won-the-programming-contest/", "abstract": "Today I want to write about Sebastian Borrazas and the programming contest he is taking part in. Sebastian Borrazas is a talented developer, member of an exclusive group of developers which makes Moove-IT a big company – I’m not talking about quantity, I’m talking about greatness. Last week, he together with his team, won the Programming Contest organized annually by the ORT University of Montevideo, Uruguay. The contest was basically about Programming Algorithms and Algorithm Design Techniques. Each group of contestants had to apply techniques like Backtracking, Dynamic programming, Divide & Conquer, Knapsack, Dijkstra, etc. The challenge was clear: to successfully do the greatest number of exercises in no more than 4 hours. Then the jury would evaluate the execution time of the Algorithm and have a winner. Next step will be to travel to Buenos Aires, Argentina on November 5 to take part in the regionals. This will be hard. More than 500 teams from all over Latin America will be there too. We are very proud to have Sebastian as our Geek Ambassador in this contest! Good luck Moovetiano!", "date": "2011-10-03"},
{"website": "Moove-It", "title": "its-about-timing-baby", "author": [" Andreas Fast "], "link": "https://blog.moove-it.com/its-about-timing-baby/", "abstract": "Yeah, it’s about timing. There was a problem in one of our projects at moove-it related to slow processing. There is a daemon spawning new threads to process certain new entries to the database. The entries come from a different system, that’s the reason for this program which processes each new entry. Sometimes at certain hours of a day there are peaks in the entries to the database and the process will fall behind by about 20.000 entries or more. So we started analyzing the code to understand what was happening and why it took so long. We noted that each new thread the daemon spawned took about 5 seconds to complete its task. As we narrowed the measurement we came up with some code that took 5 seconds to execute but it only involved access to the database. So thanks to Aaron Patterson’s ( @tenderlove ) talk at RubyConf Uruguay about “Who makes the best asado” where he talked about rails and how it manages threads and database connections, we knew where to look. What he explained is that each new thread requests its own database connection from the connection pool, and if there isn’t a free connection, rails waits for about 5 seconds and if after that there is no free connection it iterates over all the threads to take back the connections of dead threads(more info). See the correlation with the 5 seconds I talked about in the previous paragraph? We immediately suspected that this was the problem. So we started searching the Rails API for a way to release the connection at the end of each thread’s execution. Surprisingly, we didn’t find an easy and understandable explanation anywhere at first googling ;), so we digged deeper and came up with the following line: ActiveRecord::Base.connection_handler.clear_active_connections! The ActiveRecord::Base.connection_handler method returns the connection handler for the current thread and the clear_active_connections method does what it looks like, or from the Rails doc: “Returns any connections in use by the current thread back to the pool, and also returns connections to the pool cached by threads that are no longer alive.” So this line returns the connections in use by a thread to the pool and enables the new threads spawned by the daemon to use the freed connections. This way we avoid the 5 second wait for rails to free the connections for us. This one line picked up our performance from processing 1.000 entries in almost 2 hours to processing 10.000 in 5 minutes. Nice huh?! That’s it. I’m not sure if this is the best way of doing it since this method also “… returns connections to the pool cached by threads that are no longer alive.” I guess this means it does the iteration over all the threads Aaron mentioned, but as you can see I’m happy with the performance improvement. We are using Rails 3.0.5, Aaron said that he will change the behavior, read more about it here . Special thanks to @cheloeloelo who helped detecting the problems and digging through the Rails API finding the proper method to free the connections. Image: Suat Eman / FreeDigitalPhotos.net", "date": "2011-11-15"},
{"website": "Moove-It", "title": "we-run-montevideo-2011-10k-nike-competition", "author": [" Ariel Luduena Karhs "], "link": "https://blog.moove-it.com/we-run-montevideo-2011-10k-nike-competition/", "abstract": "Last Saturday a group of brave Moovetians decided to accept the challenge and run the Nike 10K competition. Nike 10K consist in running 10 kms along the coastline through some Montevideo’s neighborhoods enjoying the beautiful landscape. Take a look at the pictures. The whole team Silvana, Martin and Nicolas Ariel before breaking the finish ribbon  😛 Bird´s eye view", "date": "2011-11-08"},
{"website": "Moove-It", "title": "you-cant-help-but-mooving", "author": [" Gabriela Isnardi "], "link": "https://blog.moove-it.com/you-cant-help-but-mooving/", "abstract": "Our main tradition, heritage and passion: The beat of the “Uruguayan” drums. It doesn’t matter if you listen to the drums once a month, everyday, or once a year.  When it comes to local music, there is nothing more Uruguayan than the sound produced by rhythmically striking a drum, and especially when playing Candombe, a unique way of percussion, which will make your hair stand on its end. Team building activities can range from treasure hunts to Safari trips, though this time, we have decided to do one which people could easily identify with, and which does not require sophisticated skills, but the desire to unwind, switch off and connect. Drumming workshops come first, the bonding is just a consequence. Last week we had our first percussion workshop. Pablo Leites, an excellent musician and percussionist, also known as “Gancho” has been our instructor.  He has also been Martin Cabrera’s (Moove-IT cofounder) best friend for a long time. Please have a look at the following pictures… Why percussion Why is percussion so important to us? Being a country of immigrants, Uruguay was formed by people from all over the world. And like it always happens, music has played a tremendous positive role in bringing people together and creating stronger, more significant bonds. We connect with our basic instincts, we forget about language barriers, cultural differences, rank, and we just let ourselves feel and relax.  Just listen to a few strikes and you will feel multicolor, ageless and energized. You may have heard of Las Llamadas (The Callings), a popular annual event during Carnival here in Uruguay, which gathers thousands of people from all over the world. The drums are the main stars, and the African music roots brought by the people once made slaves in this country (and happily freed more than 150 years ago) are now our truly genuine and local music. I personally love this rhythm, and even though I am not a music expert I will recognize its pace wherever I go. I am not sure if it is the adrenaline than runs through your body, or the inseparable link to human nature, but percussion makes your body shake, like toddlers instinctively struggling to move their bodies to the rhythm of the music. Team Building !!", "date": "2011-10-17"},
{"website": "Moove-It", "title": "second-rubyconf-in-uruguay-11th-12th-november", "author": [" Gabriela Isnardi "], "link": "https://blog.moove-it.com/second-rubyconf-in-uruguay-11th-12th-november/", "abstract": "We are sponsoring one of the greatest technology events here in Uruguay. The Second RubyConf taking place within less than two weeks, the 11th and 12th November 2011, where many IT experts from all over the world get together in order to be immersed in this dynamic world and and up to date get with the latest trends of Ruby and Agile methodologies. RubyConf Uruguay 2011 We are hungry for knowledge and refreshment, and we all want to be on the same train. Please welcome all the new members to this awesome community. And help spreading the news, but even more important, do not miss the opportunity to meet the experts, discuss the future of RoR, and be Rail!", "date": "2011-11-01"},
{"website": "Moove-It", "title": "from-silicon-valley-to-moove-it", "author": [" Gabriela Isnardi "], "link": "https://blog.moove-it.com/from-silicon-valley-to-moove-it/", "abstract": "We are doing a good job, that’s right, but Michael Staton and Howard Kao from Inigral did not exactly decide to move to Uruguay and work from here now on, they just paid us a one week visit, and worked with us like any other member of the Moove-iT team. The Schools project is exciting itself, but having Howard and Michael here made us feel it even closer, and the boundaries between Inigral and Moove-iT were sort of a blur, for good. Correct me if I am wrong, but I am really positive we have made them feel at home. We worked for a week straight, but we also had time to have fun. No doubts about it, having visitors from around the globe is always refreshing and truly motivating… Let me tell you why. We conduct daily meeting with clients from different countries. Most of the times we use skype, and we are about 4 or 5 people in the same room, talking in English, though our primary language is Spanish. On the other end, there is a person whose native language is generally English, but who is normally thousands of miles away, many Fahrenheits apart and only a few hours ahead or behind us. We rarely use a camera, so we do not even know what the room he or she is in looks like. I believe that the third dimension is definitely more important than we might think. And being in the same room with someone whose voice is extremely familiar to us, and has been part of our work lives for over an year, but whose looks, posture or gestures we used to have little or no idea about is just fantastic. We shared this sort of energy and connection you could only generate when meeting in person. The truth is we all love to put a name to a face. And this time we had two new faces to look at, to talk to and to laugh with. In the flesh we met Michael and Howard. THANK YOU!", "date": "2011-02-17"},
{"website": "Moove-It", "title": "communication-clients-kp-process", "author": [" Martin Cabrera "], "link": "https://blog.moove-it.com/communication-clients-kp-process/", "abstract": "At “Moove-it Tech Talks cycle” … This is the presentation that Gabriela and I made in moove-it today … The Topic are: Conference call Intro Role Play (to understand and ilustrate problems) Process Language Showing disagreement Conclusion Here is the presentation … https://docs.google.com/present/edit?id=0AdbREprcml_EZGY4OXZ2N3pfMzBoODR4cmJkcg&hl=es", "date": "2011-06-03"},
{"website": "Moove-It", "title": "dart-structured-web-programming", "author": [" Andreas Fast "], "link": "https://blog.moove-it.com/dart-structured-web-programming/", "abstract": "On October 10th Lars Bak & Gilad Bracha presented a technology preview on Dart. Lars Bak & Gilad Bracha are Google employees leading the development of Dart. Dart is open source, so anyone can use and change it. It’s still in the early stages but the design goals are very clear. It aims to be a structured yet flexible programming language for the web. To feel familiar and be easy to learn, focus on high performance and fast startup. To be appropriate for all devices from phones and tablets to notebooks and servers. There is also a lot of work being done on tools for Dart to run fast on all major modern browsers. It runs on a Virtual Machine on the server and there is a tool to compile the code to javascript to run it on a browser. It also provides a DOM api. The following presentation shows the basics of the language including some examples. In addition, here are some photos of the presentation at moove-iT! Structured web programming", "date": "2011-11-02"},
{"website": "Moove-It", "title": "rally-on-rails-made-buscando-a-tanatos", "author": [" Martin Cabrera "], "link": "https://blog.moove-it.com/rally-on-rails-made-buscando-a-tanatos/", "abstract": "Last weekend four members of moove-it took part in the rally on rails ( www.rallyonrails.com ). Pablo Ifran Gonzalo Arreche Juan Pablo Blanco Ivan Etchart Rally on Rails is a competition to make a Web app from scratch in 48 hours ! Obviously this project/game/etc had to be made in rails ! Our team created www.buscandoatanatos.com a game about the search for “Tánatos” (the leader of Sotirias … a secret organization). Based on Google street view , rails, and a lot of gems. Here go some pictures of this weekend … Cast your vote from 18 to 20 of August … www.rallyonrails.com", "date": "2011-08-15"},
{"website": "Moove-It", "title": "alternatives-to-full-text-queries-part-i", "author": [" Fernando Doglio "], "link": "https://blog.moove-it.com/alternatives-to-full-text-queries-part-i/", "abstract": "When it comes to data storage and data handling, we developers, are quite used to working with database engines (MySQL, Oracle, SQLite, etc) and database query languages. Depending on the application being developed, one of these solutions can be more than enough to meet our needs and that is what we usually end up using. But there are times, when the amount of information to be handled is so big (we’re talking about millions of rows of information) and the needed response times are so low (we’re talking about a few milliseconds time) that we need solutions designed specifically for searching large amount of information instead of generic data handling ones. What’s so good about them? While most database engines work great with very dynamic loads of information (i.e.: where new information needs to be shown in real-time), and managing highly complex relational data, sometimes what we need is actually a solution that can search through a great amount of data in a very short time, giving the user a quick response. The solutions we’ll be talking about here, do just that, they give the developers a way of accessing information at a striking speed, but at a price: the information used is not always the most updated one 1 . Another lose to consider here, is the fact that these searching solutions do not manage relational information like relational databases do. So you need to be careful when choosing one over the other, and consider exactly what your needs are. How do they work? The way they work, is, roughly, by indexing all the information, using some of several indexing mechanics (like the use of stop words 2 , stemming 3 , etc). Once the information has been indexed, it is there (on the index) where the search takes place, instead of searching directly on the unprocessed data. Some of the most interesting features presented by these APIs are: Ranked search : The results can be returned ordered by the relevance of the match. Proximity search : Because sometimes, the searched terms are not written correctly, these APIs can return results that are close to the original ones. The use of wildcards : When searching, not always do we know the exact words, that’s when wildcards come into place, allowing us to specify a basic structure for our search and letting the search engine find the matching results. Comparing some of them Considering this scenario, today we’ll be discussing some alternative options for data handling and querying. I’ve chosen these projects based on general opinion on internet, according to most places, these guys are the most known/used alternatives out there (there are many others of course). Our options are: Lucene: An Apache developed project, written entirely in Java. Xapian: An Open Source Search Engine Library, released under the GPL and written in C++. Sphinx: An open source full text search server with a high level of integration with database engines. Solr: Solr is a standalone enterprise search server with a REST-like API that uses Lucene as its base full-text search engine. What am I looking for? Before even comparing products, it is important to understand that there is rarely a single solution to our problems. That almost every time, there is more than one product that will meet our needs. Knowing this, I’ll be comparing products based on an “ out-of-the-box ” basis. This means, that no addons or plugins or anything will be taken into account for comparison purposes (we have to draw the line somewhere). My chosen criteria I won’t be benchmarking product’s speed against each others, if that’s what you’re looking for then stop reading here. Even though it might be a good measure of how good a product is (in some cases) I feel that in this particular case, where we don’t really have a specific problem to solve, those numbers would not bring anything of value to the table. What I will be taking into account is the amount of features (search related features actually) that come with a product out of the box. I fell that this approach could give a basic understanding of which product stands over others for any generic given problem. In the end, based solely on this information, my resolution can be wrong (or misleading) for some specific cases, but then again, that’s not what I’m trying to solve, since in order to tackle specific scenarios this article would have to be extended into a small book. Another point of notice, is the fact that all of the presented solutions (as we’ll see in a minute), are long term projects, with active support and under constant development (and improvement). The projects Now that we know what we’ll be comparing and what to look for on our projects, lets take a look at them: Lucene Apache Lucene(TM) is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform. It has support for indexing several industry standards, like Word documents, Excel, PowerPoint, PDF, HTML files and many more. Basic features High performance indexing Over 95GB/hour on modern hardware Incremental indexing as fast as batch indexing Index size roughly 20-30% the size of text indexed Powerful, Accurate and Efficient Search Algorithms Ranked searching — best results are returned first Many powerful query types: Phrase queries: allows for more than one word to be used on the search. Use of wildcards: The API supports the use of “*” and “?” as wildcards on queries. Proximity queries: Use of ~ to find look-a-like words. Range queries: Used for range of numbers (1 to 10), sorted words (Amanda to Sabrina), date range and more. Fielded searching (e.g., title, author, contents) Sorting by any field Multiple-index searching with merged results Allows simultaneous update and searching Cross-Platform Solution Available as Open Source software under the Apache License which lets you use Lucene in both commercial and Open Source programs 100%-pure Java Implementations in other programming languages available that are index-compatible PHP: ZendSearch Ruby: Ferret C++: CLucene Many others! Who uses it? This is a widely used API, and some of the major software companies that use it are: Eclipse: The Eclipse IDE uses Lucene for searching it’s documentation. CNET Reviews – Product Listing pages with Filter drill downs. IBMOmniFindPersonale-mailSearch – A semantic search engine for your e-mail. Others like Linkedin, AOL, etc (full list here ) Xapian Xapian, like Lucene, is another open source, widely used full text search API, but unlike the first one, this one is written in C++. Some basic features: Cross-platform Highlyportable – runs on Linux, Mac OS X, FreeBSD, NetBSD, OpenBSD, Solaris, HP-UX, Tru64, IRIX, and probably other Unix platforms; as well as Microsoft Windows and OS/2. Written in C++. Perl bindings are available in the module Search::XapianonCPAN . Java JNI bindings are included in the xapian-bindings module. It supports SWIG which generates bindings for many languages like Python, PHP, Tcl, C#, Ruby and Lua. Great algorithms: Ranked probabilistic search – important words get more weight than unimportant words, so the most relevant documents are more likely to come near the top of the results list. Relevance feedback – given one or more documents, Xapian can suggest the most relevant index terms to expand a query, suggest related documents, categorize documents, etc. Phrase and proximity searching – users can search for words occurring in an exact phrase or within a specified number of words, either in a specified order, or in any order. Full range of structured boolean search operators (“stock NOT market”, etc). The results of the boolean search are ranked by the probabilistic weights. Boolean filters can also be applied to restrict a probabilistic search. Supports stemming of search terms (e.g. a search for “football” would match documents which mention “footballs” or “footballer”). This helps to find relevant documents which might otherwise be missed. Stemmers are currently included for many languages (not only English), like Danish, Dutch, English, Finnish, French, German and more. Wildcard search is supported (e.g. “xap*”). Synonyms are supported, both explicitly (e.g. “~cash”) and as an automatic form of query expansion. Xapian can suggest spelling corrections for user supplied queries. This is based on words which occur in the data being indexed, so works even for words which wouldn’t be found in a dictionary (e.g. “xapian” would be suggested as a correction for “xapain”). Faceted Search Xapian supports this added feature, called faceted search. What this basically means, is that search queries can have categories of interest and so can the terms and information indexed. This allows Xapian to narrow your search, to a particular sub-set of categories. Performance: Supports database files > 2GB – essential for scaling to large document collections. Platform independent data formats – you can build a database on one machine and search it on another. Allows simultaneous update and searching. New documents become searchable right away. Who uses it? Debian GNU/Linux Debian use Xapian for: Debian Mailing List Archive Search Languages: Danish, Dutch, English, Finnish, French, German, Hungarian, Italian, Portuguese, Romanian, Russian, Spanish, Swedish, and Turkish Database size: 3.9 million messages (2009-05-18) DebianWikiSearch Database size: over 6700 pages (2009-05-18) Searching their archive of software packages Database size: over 30000 packages (2009-03-16) Library of the University of Cologne Xapian is used with OpenBib to search the library’s OPAC (Online Public Access Catalogue) spin . de Application: Free online community with profiles, chat, boards, blogs, games, and more Sphinx Our third item on the list is Sphinx, an open source full text search server. This one was designed for indexing database content. It currently supports MySQL, PostgreSQL, and ODBC-compliant databases as data sources natively. The integration with RDBMSs (Relational Data Base Management Systems) allows us to use Sphinx as if it were a database itself. As it is with the rest of these solutions, this might be the thing that you need for your application or it can be completely irrelevant. Some basic features: Performance and scalability Indexing performance. Sphinx indexes up to 10-15 MB of text per second per single CPU core, that is 60+ MB/sec per server (on a dedicated indexing machine). Searching performance. According to their statistics, searching through 1,000,000-document (a 1.2GB text collection) runs at around 500 queries/sec on a 2-core desktop machine, with 2GB or RAM. Scalability. Biggest known Sphinx cluster indexes almost 5 billion documents , resulting in over 6 TB of data . Key features Batch and Real-Time full-text indexes. Two index backends that support both offline index construction and incremental on-the-fly index updates are available. Non-text attributes support . An arbitrary number of attributes (product IDs, company names, prices, etc) can be stored in the index and used either just for retrieval (to avoid hitting the DB), or for efficient Sphinx-side search result set post-processing. SQL database indexing. Sphinx can directly access and index data stored in MySQL (all storage engines are supported), PostgreSQL, Oracle, Microsoft SQL Server, SQLite, Drizzle, and anything else that supports ODBC. Non-SQL storage indexing. Data can also be streamed to batch indexer in a simple XML format called XMLpipe, or inserted directly into an incremental RT index. Easy application integration. Sphinx comes with three different APIs: SphinxAPI is a native library available for Java, PHP, Python, Perl, C, and other languages. SphinxSE, a pluggable storage engine for MySQL, enables huge result sets to be shipped directly to MySQL server for post-processing. SphinxQL lets the application query Sphinx using standard MySQL client library and query syntax. Advanced full-text searching syntax . It’s querying engine supports arbitrarily complex queries combining boolean operators, phrase, proximity, strict order, and quorum matching, field and position limits, exact keyword form matching, substring searches, etc. Rich database-like querying features. Sphinx does not limit you to just keyword searching. On top of full-text search result set, you can compute arbitrary arithmetic expressions, add WHERE conditions, do ORDER BY, GROUP BY, use MIN/MAX/AVG/SUM, aggregates etc. Essentially, full-blown SQL SELECT is supported. Relevance ranking. Sphinx claims to have a better-than-average ranking algorithm, analyzing keyword proximity, and ranking closer phrase matches higher, with perfect matches ranked on top. Also, ranking is flexible: you can choose from a number of built-in relevance functions, tweak their weights by using expressions, or develop new ones. Flexible text processing. Sphinx indexing features include full support for SBCS and UTF-8 encodings (meaning that effectively all world’s languages are supported); stopword removal and optional hit position removal (hitless indexing); morphology and synonym processing through word forms dictionaries and stemmers; exceptions and blended characters; and many more. Distributed searching. Searches can be distributed across multiple machines, enabling horizontal scale-out and high availability. Some things to notice that are not provided by Sphinx: Wildcards: Sphinx (on it’s release version) does not allow the use of wildcards on it’s searches, although it’s latest beta version does. Sphinx cannot parse rich text format documents (like Word documents, PDF files, etc). Who uses it? Craigslist : A classifieds page, with more than 20 billion page views per month. DailyMotion : Widely used video site. NetLog : Social network with more than 77million users. Solr Solr is an open source enterprise search platform from the Apache Lucene project. This means that it uses (and extends) the Lucene full-text search API we talked before, for all the full-text searches. This particular feature is of note, since this allows Solr to improve whenever the Lucene project integrates an improvement into its product. This is especially true as of May 2011, since both projects decided to merge. It also differentiates from the other solutions presented here, because of it’s unique REST-like interface. You can put documents in it (called “indexing”) via XML, JSON or binary over HTTP and then you query them via HTTP GET and receive XML, JSON, or binary results. This makes Solr completely language independent, allowing any developer using an HTTP capable programming language to interact with this platform. Some basic features: Searching: Advanced Full-Text Search Capabilities External file-based configuration of stopword lists, synonym lists, and protected word lists Many additional text analysis components including word splitting, regex and sounds-like filters Sort by any number of fields, and by complex functions of numeric fields Spelling suggestions for user queries “ More Like This” suggestions for given document Caching Configurable Query Result, Filter, and Document cache instances Pluggable Cache implementations, including a lock free, high concurrency implementation Cache warming in background When a new searcher is opened, configurable searches are run against it in order to warm it up to avoid slow first hits. During warming, the current searcher handles live requests. Auto warming in background The most recently accessed items in the caches of the current searcher are re-populated in the new searcher, enabling high cache hit rates across index/searcher changes. Fast/small filter implementation Admin Interface Full logging control Text analysis debugger, showing result of every stage in an analyzer Web Query Interface w/ debugging output parsed query output Lucene explain() document score detailing explain score for documents outside of the requested range to debug why a given document wasn’t ranked higher. Amongst others… Who uses It? http :// www . whitehouse . gov / – Uses Solr via Drupal for site search w/highlighting & faceting (More details: 1 , 2 )a AOL is using Solr to power its channels Yellow Pages Music NFL Sports AOL Recipes Reference Center etc. digg uses Solr for search NASA Planetary Data System ( PDS ) uses Solr to search for dataset, mission, instrument, target, and host information reddit uses Solr for search And many more (http://wiki.apache.org/solr/PublicServers) This is not always the case, some of the latest engines implement incremental indexing (or some other strategy) in order to solve this problem. Words that carry too little meaning or are too common to be of use during the search process. The process of relating words that are written different but have the same stem, allowing for cases like “drive” being equal to “drove” or “driven”. Stay tunned for part II, where we’ll actually match them against each other….", "date": "2011-10-03"},
{"website": "Moove-It", "title": "san-francisco-wear-moove-it-shirt", "author": [" Gabriela Isnardi "], "link": "https://blog.moove-it.com/san-francisco-wear-moove-it-shirt/", "abstract": "We have been to the top 10 places to visit in the US. From a business perspective, Silicon Valley is the place to go. We share the same passion as the area of the world’s largest technology corporations and high-tech firms: Great Ruby on Rails design & development work. Outsourcing has never been so much fun here in Uruguay. And now our Schools friends are wearing Moove-IT shirts, so we are virtually present—though also planning to re- meet in the flesh soon… Thanks to Howard Kao, Lane Lillquist, Michael Staton, Nick Punt, and Daniel Jabbour from Inigral! From South America with love, working offshore", "date": "2011-08-31"},
{"website": "Moove-It", "title": "agile-software-development", "author": [" Gian Zas "], "link": "https://blog.moove-it.com/agile-software-development/", "abstract": "At “Moove-it Tech Talks cycle” we made a presentation about agile software development. Agile Software Development View more presentations from gian-zas .", "date": "2011-06-03"},
{"website": "Moove-It", "title": "ruby-on-rails-for-mobiles", "author": [" sebastian.sassi "], "link": "https://blog.moove-it.com/ruby-on-rails-for-mobiles/", "abstract": "Some time ago, one of our customers asked us to implement a new web project with support for smartphones. The objective was to support the most common operating systems: iOS, android, blackberry, symbian, with the possibility of using the phone capabilities (gps, sound and vibration). After researching on different frameworks and technologies we have arrived to some conclusions that I would like to share with people who have had the same dilemma of choosing a good framework to use. I must say that this research was based on data taken from different sources (both official and unofficial), and also having taken some particular decisions while evaluating the alternatives, that do not have to match with your own criteria. First of all, we classified the available options into two different categories: ‘Native’ Development frameworks: Rhodes, NimbleKit, Titanium (this is new to the list), PhoneGap Javascript libraries: Sencha, JQTouch, JQuery mobile Rhodes According to the official description, The Rhodes application framework allows developers to create native mobile applications with portability of editing HTML templates and the power of the Ruby programming language. Applications written in Rhodes exhibit the performance and richness of apps written to the native device operating systems with local data but enable developers to have the productivity of web interfaces in HTML. Developers write their applications once and they then run on all major mobile device operating systems: iPhone, Windows Mobile, Blackberry and more. Advantages: Programming in Ruby Full access to the Device API (Application looks different in each device) Several devices supported: IPhone, BlackBerry, Android, Windows mobile RhoSync: RhoSync retrieves data via web services (REST or SOAP) from backend enterprise applications for distribution to downstream mobile devices. Disadvantages: Licensing: It is a commercial product. Costs arround U$S 500 per project. NimbleKit NimbleKit says that it is a framework to create applications for iPhone and iPod touch fast. Programmers do not need to know Objective-C or iPhone SDK. All they need is to know how to write an HTML page with Javascript code. Advantages: HTML, CSS & Javascript Disadvantages It only works for iphone, ipods & ipads. Titanium & PhoneGap From architectural standpoint, these two frameworks are very similar. Titanium and PhoneGap expose the smartphone features through a set of Javascript APIs, while the application’s logic (html, css, javascript) runs inside a native WebView control. Through the javascript APIs, the “web app” has access to the mobile phone functions such as Geolocation, Accelerometer Camera, Contacts, Database, File system, and so on. The differences: PhoneGap does not expose the native UI components to javascript. Titanium, on the other hand, has a comprehensive UI API that can be called in javascript to create and control all kinds of native UI controls. Utilizing these UI APIs, a Titanium app can look more “native” than a PhoneGap app. Second, PhoneGap supports more mobile phone platforms than Titanium does. PhoneGap APIs are more generic and can be used on different platforms such as iPhone, Android and Blackberry. Titanium is primarily targeting iPhone and Android. Some of its APIs are platform specific (like the iPhone UI APIs). The use of these APIs will reduce the cross-platform capability of your application. If your concern for your app is to make it more “native” looking, Titanium is a better choice. If you want to be able to “port” your app to another platform more easily, PhoneGap will be better. Advantages: Multi device Programming in HTML, CSS & Javascript MIT License Disadvantages: Some devices native features are not exposed in the API Sencha According to the official documentation, Sencha Touch is the first app framework built specifically to leverage HTML5, CSS3, and Javascript for the highest level of power, flexibility, and optimization. They make specific use of HTML5 to deliver components like audio and video, as well as a localStorage proxy for saving data offline. They have made extensive use of CSS3 in our stylesheets to provide the most robust styling layer possible. Altogether, the entire library is under 80kb (gzipped and minified), and it is trivial to make that number even smaller by disabling unused components or styles. Advantages Great UI Controls Multi device Disadvantages Commercial license (U$S 1.800 for 5 developers) JQTouch It is “A jQuery plugin for mobile web development on the iPhone, iPod Touch, and other forward-thinking devices.” [1] Advantages Great UI Controls Multi device MIT License Disadvantages Is focused on small screen devices It’s a jquery plugin It was created in 2009 by David Kaneda and has been relatively quiet until release of beta 3 (April 24th 2011). JQuery Mobile It is a unified user interface system across all popular mobile device platforms, built on jQuery and jQuery UI foundation. Its lightweight code is built with progressive enhancement, and has a flexible, easily themeable design. Advantages Multi device (See https://jquerymobile.com/browser-support/ ) (supports much more devices than JQTouch) Very simple to use John Resig, JQuery’s founder is one of the main developers and it is a high activity project It is not a jquery plugin, it is a new framework Dual licensed under MIT and GPL Version 2 Disadvantages It is still in Alpha version CONCLUSION After evaluating the different frameworks, we arrived at the next conclusions: We should use a framework that generates html + css + js, because this technology guarantees a long term support on different devices We should use a framework that supports access to devices specific capabilities (i.e. sound, gps, camera, vibration, data, etc.) in case we need them in a near future Because we prefer unrestrictive licenses (MIT, BSD, LGPL and others) our options are reduced to: PhoneGap, Titanium & JQTouch. Finally, we choose PhoneGap + JQuery mobile because they are open technologies and support multiple platforms. Also, resulting interfaces are similar across different devices and that is a desired feature for our users. JQTouch and JQuery mobile are awesome frameworks to develop mobile applications, but we prefer to choose JQuery Mobile since it support much more devices (altough it is in alpha version), has high activity and is not just optimized for WebKit as jQtouch does. We also think that JQuery Mobile is pretty easy to use, the resulting interfaces are usable and has many components to use. The rest of the story is pretty easy ( just because we use Ruby on Rails 😛 ), you should create some helpers that adds the JQuery Mobile features to the pages, or use some existent plugin, you can find a starting guide here: http://goo.gl/5Soyq . I hope you find this post useful, have a nice coding! [1]: forward-thinking: Someone who is forward-thinking is thinking progressively and possesses the ability to look beyond the “now” and formulate strategies for future success. They are constantly asking “what’s next?”. Sources: https://www.phonegap.com/ http://www.nimblekit.com/ https://rms.rhomobile.com/ http://jqtjs.com/ https://jquerymobile.com/ https://www.sencha.com/products/touch/ http://fuelyourcoding.com/getting-started-with-jquery-mobile-rails-3/", "date": "2011-05-06"},
{"website": "Moove-It", "title": "workshop-ruby-on-rails-scrum", "author": [" Martin Cabrera "], "link": "https://blog.moove-it.com/workshop-ruby-on-rails-scrum/", "abstract": "Moove-IT is now part of the Ruby on Rails Workshops and Conferences around the world. Even when Ruby seems to be the most popular modern programming language in America, there was nowhere in Uruguay you could actually learn it. Moove-IT will give you a heads up on this exciting language and Rails framework on July 6, 2011 (please see ad below). This will be the first in a series of workshops focusing on Ruby, RoR, Scrum, and hands-on training exercises. We are so proud to have made so much progress working with this technology and methodology as a Ruby on Rails Development Company , that we feel the need to contribute to the local Ruby community. Join us! Be part.", "date": "2011-06-28"},
{"website": "Moove-It", "title": "sass-why-is-useful", "author": [" Pablo Ifran "], "link": "https://blog.moove-it.com/sass-why-is-useful/", "abstract": "Sass is used for generating css files based on his own syntax, it’s very useful because allows you to avoid repeating code across the different scss files using import (allows you to import code from other scss file), or mixins(scss functions). Also, allows you to write varibles, for using in different scss, this makes easier to do different templates based on colors or images for example. _sprites.scss @mixin sprite_for($image_class, $x, $y, $image: \"/images/sprite_image.png\") {\r\n  .sprite_image.#{$image_class} {\r\n    background: transparent url($image) no-repeat scroll $x $y;\r\n    height:16px;\r\n    width:16px;\r\n  }\r\n}\r\n_images.scss 1 2 3 4 5 6 7 8 @ mixin sprite_for ( $ image_class , $ x , $ y , $ image : \"/images/sprite_image.png\" ) { . sprite_image . #{$image_class} { background : transparent url ( $ image ) no - repeat scroll $ x $ y ; height : 16px ; width : 16px ; } } _images . scss $my_sprite_image_path: \"/images/my_sprite_image_path.png\"; 1 $ my_sprite_image_path : \"/images/my_sprite_image_path.png\" ; default.scss @import \"sprites\", \"images\";\r\n\r\n@include sprite_for(\"my_image\", 0, 0, $my_sprite_image_path); 1 2 3 @ import \"sprites\" , \"images\" ; @ include sprite_for ( \"my_image\" , 0 , 0 , $ my_sprite_image_path ) ; That generates default.css .sprite_image.my_image {\r\n    background: transparent url(/images/my_sprite_image_path.png) no-repeat scroll 0 0;\r\n    height:16px;\r\n    width:16px;\r\n  } 1 2 3 4 5 . sprite_image . my _ image { background : transparent url ( / images / my_sprite_image_path . png ) no - repeat scroll 0 0 ; height : 16px ; width : 16px ; } So, this is best sorted using this method, because you have the image path in a single file and you use it in other files only by importing the file with the images path, it’s also simpler to remove an image because you only need to find the references to that variable and delete them. The sass framework also allows you to use the DRY principle because you can write nested selector, I’ll show you with an example: default.scss .some_class {\r\n  width: 100px;\r\n  .some_other_class {\r\n    display: block;\r\n  }\r\n} 1 2 3 4 5 6 . some _ class { width : 100px ; . some_other _ class { display : block ; } } default.css .some_class {width:100px;}\r\n.some_class .some_other_class {display:block;} 1 2 . some _ class { width : 100px ; } . some _ class . some_other _ class { display : block ; } Doesn’t affect the performance because it generates a css file at development time. It also allows you to benefit from some of the advantages of css 3 right now.", "date": "2010-11-26"},
{"website": "Moove-It", "title": "rails-security", "author": [" Pablo Ifran "], "link": "https://blog.moove-it.com/rails-security/", "abstract": "When you are working with svn (pulling your project from the svn to the web server) and you want to deploy a system into production with apache (mod_rails), you must filter the svn folders (to prevent that other users view your svn files). To do that task, you must add the following lines to the apache configuration. <DirectoryMatch \"^/.*/\\.svn/\"> ErrorDocument 403 /404.html Order allow,deny Deny from all Satisfy All </DirectoryMatch>", "date": "2010-10-14"},
{"website": "Moove-It", "title": "jpa-onetomany-i-need-fk-child-table", "author": [" Ariel Luduena Karhs "], "link": "https://blog.moove-it.com/jpa-onetomany-i-need-fk-child-table/", "abstract": "Suppose you have this two entities: PhoneList and Phone. PhoneList have a @OneToMany to Phone. (I need only a unidirectional relationship) Something like: @OneToMany(cascade=CascadeType.ALmappedBy=”phoneList”) public List<Phone> getPhones() { return phones; @OneToMany(cascade=CascadeType.ALL)\npublic List&lt;Phone&gt; getPhones() {\n   return phones;\n} 1 2 3 4 @ OneToMany ( cascade = CascadeType . ALL ) public List & lt ; Phone & gt ; getPhones ( ) { return phones ; } The phone entity doesn’t have nothing special. If you deploy those entities your JPA provider will create 3 tables. One table for each entity and also a “JoinTable”. But, why? This kind of relationship can be done by 2 tables only and I don’t want a third table. I want a PHONE table with an FK column to PHONELIST table. How can I do that? Two options: Make your relationship bidirectional Add an @JoinColumn in the @OneToMany attribute Option number 1 – Make your relationship bidirectional The problem is that “I don’t need a bidirectional relationship”! … Ok, but, suppose that you are more interested in solve this and move forward to a “real issue”. To make this relationship bidirectional, you must add in the Phone entity the reference to the PhoneList entity with the @ManyToOne annotation. Something like: @Entity\npublic class Phone {\n\nprivate Long id;\nprivate String phoneNumber;\nprivate PhoneList phoneList;\n\n@ManyToOne\npublic PhoneList getPhoneList() {\n   return phoneList;\n}\n... 1 2 3 4 5 6 7 8 9 10 11 12 @ Entity public class Phone { private Long id ; private String phoneNumber ; private PhoneList phoneList ; @ ManyToOne public PhoneList getPhoneList ( ) { return phoneList ; } . . . Also you have to add to the relationship @OneToMany in the PhoneList entity the mapped by attribute. Something like this: @Entity\npublic class PhoneList {\n\n\tprivate Long id;\n\tprivate String name;\n\tprivate String description;\n\tprivate List&lt;Phone&gt; phones;\n\n\t@Id @GeneratedValue\n\tpublic Long getId() {\n\t\treturn id;\n\t}\n\n\tpublic void setId(Long id) {\n\t\tthis.id = id;\n\t}\n\n\t@OneToMany(cascade=CascadeType.ALL, mappedBy=\"phoneList\")\n\tpublic List&lt;Phone&gt; getPhones() {\n\t\treturn phones;\n\t} 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 @ Entity public class PhoneList { private Long id ; private String name ; private String description ; private List & lt ; Phone & gt ; phones ; @ Id @ GeneratedValue public Long getId ( ) { return id ; } public void setId ( Long id ) { this . id = id ; } @ OneToMany ( cascade = CascadeType . ALL , mappedBy = \"phoneList\" ) public List & lt ; Phone & gt ; getPhones ( ) { return phones ; } Option number 2 – Add @JoinColumn in the @OneToMany attribute If you don’t want to have a bidirectional relationship, you have to use @JoinColumn like this: @Entity\npublic class PhoneList {\n\n\tprivate Long id;\n\tprivate String name;\n\tprivate String description;\n\tprivate List phones;\n\n\t@Id @GeneratedValue\n\tpublic Long getId() {\n\t  return id;\n\t}\n\n\tpublic void setId(Long id) {\n          this.id = id;\n\t}\n\n\t@OneToMany(cascade=CascadeType.ALL)\n\t@JoinColumn(name=\"phone_list_id\", referencedColumnName=\"id\")\n        public List getPhones() {\n\t  return phones;\n\t} 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 @ Entity public class PhoneList { private Long id ; private String name ; private String description ; private List phones ; @ Id @ GeneratedValue public Long getId ( ) { return id ; } public void setId ( Long id ) { this . id = id ; } @ OneToMany ( cascade = CascadeType . ALL ) @ JoinColumn ( name = \"phone_list_id\" , referencedColumnName = \"id\" ) public List getPhones ( ) { return phones ; } and the Phone entity remain without references @Entity\npublic class Phone {\n\nprivate Long id;\nprivate String phoneNumber;\n\n... 1 2 3 4 5 6 7 @ Entity public class Phone { private Long id ; private String phoneNumber ; . . .", "date": "2010-10-18"},
{"website": "Moove-It", "title": "rails-3-new-features-changes", "author": [" Michel Golffed "], "link": "https://blog.moove-it.com/rails-3-new-features-changes/", "abstract": "In this post I’ll show you some basic examples about the new features and changes of the web framework Rails 3, for this I’ll use two models: User and Task, having a relationship between them where a User has_many :tasks. ActiveRecord finder methods Let’s start by finding the five most recently created users: Rails 2 User.find(:all, : order =&gt; \"created_at desc\", :limit =&gt; 5) 1 User . find ( : all , : order =& gt ; \"created_at desc\" , : limit =& gt ; 5 ) What’s new about this method call in rails 3? Rails 3 looks at the hash of options that is being passed to find(:all, : order, and :limit) and replace each item in the hash with an equivalent method. New API methods: where, having, select, group, order, limit, offset, joins, includes, … So, in Rails 3 this becomes: User.order(\"created_at desc\").limit(5) 1 User . order ( \"created_at desc\" ) . limit ( 5 ) Let’s continue with this second example: Rails 2: User.find(:all, :conditions =&gt; [\"created_at &lt;= ?\", Time.now], :include =&gt; :tasks) 1 User . find ( : all , : conditions =& gt ; [ \"created_at &lt;= ?\" , Time . now ] , : include =& gt ; : tasks ) Rails 3 : User.where(\"created_at &lt;= ?\", Time.now).includes(:tasks) 1 User . where ( \"created_at &lt;= ?\" , Time . now ) . includes ( : tasks ) In this example the “where” method substitutes the :conditions parameter. Named Scopes Changes to named_scopes in Rails 3: User model in rails 2 with two named scopes: class User &lt; ActiveRecord::Base\r\nnamed_scope :active, :conditions =&gt; '!is_deleted'\r\nnamed_scope :south_american,\r\n:conditions =&gt; [\"nationality IN (?)\", Nationality::south_american_countries]\r\nend 1 2 3 4 5 class User & lt ; ActiveRecord :: Base named_scope : active , : conditions =& gt ; '!is_deleted' named_scope : south_american , : conditions =& gt ; [ \"nationality IN (?)\" , Nationality :: south_american_countries ] end The same in rails 3 would be: class User &lt; ActiveRecord::Base\r\nscope :active, where('!is_deleted')\r\nscope :south_american,\r\nwhere(\"nationality IN (?)\", Nationality::south_american_countries)\r\nend 1 2 3 4 5 class User & lt ; ActiveRecord :: Base scope : active , where ( '!is_deleted' ) scope : south_american , where ( \"nationality IN (?)\" , Nationality :: south_american_countries ) end Note that the method that we use to define as named_scope has become to just “scope”. Also, we no longer pass the conditions as a hash but, as with find, we use methods. Chainability Another new feature is the ability to build up scopes. If we want to create a scope called “recent”, that will return the most recently created active south american users ordered by their creation date, we can do so by reusing the two scopes we already have. Chaining together the two scopes we already have and add an order method to create the new scope, is all we need to do. Here is the result: scope :recent, active.south_american.order(\"created_at desc\") 1 scope : recent , active . south_american . order ( \"created_at desc\" ) ActiveRecord Validation Separate validations in Rails 2 validates_presence_of :email\r\nvalidates_uniqueness_of :email\r\nvalidates_length_of :email, :maximum =&gt; 30 1 2 3 validates_presence_of : email validates_uniqueness_of : email validates_length_of : email , : maximum =& gt ; 30 All validations together for a field in Rails 3 validates :email, :presence =&gt; true,\r\n:uniqueness =&gt; true,\r\n:length =&gt; {:maximum =&gt; 30} 1 2 3 validates : email , : presence =& gt ; true , : uniqueness =& gt ; true , : length =& gt ; { : maximum =& gt ; 30 } Hope you enjoyed it, some other features of Rails 3 will be published on upcoming posts.", "date": "2010-11-18"},
{"website": "Moove-It", "title": "fondation_forge_and_moove-it_play_it_real", "author": [" Gabriela Isnardi "], "link": "https://blog.moove-it.com/fondation_forge_and_moove-it_play_it_real/", "abstract": "We are proud to announce we have 2 new members in our team. Damian and Nicolas are part of Fondation Forge programs for teenagers. These two bright young programmers are joining our staff after working as interns in our premises. Please welcome them! Moove-it and Forge have been working together for 2 years. Among other things Forge provides an opportunity for teenagers to get their first job, and Moove-it supports its values and contributes to make them possible. The FORGE FOUNDATION is a Swiss established non-profit foundation dedicated to developing and implementing youth work and life skills training programs in nations throughout Latin America.  The foundation began operations in 2006 and now has three training program centers: two in Argentina (Buenos Aires and Pilar) and one in Montevideo, Uruguay web page of FF: www.fondationforge.org", "date": "2010-10-18"},
{"website": "Moove-It", "title": "juggernaut-chat-on-rails", "author": [" ivan.etchart "], "link": "https://blog.moove-it.com/juggernaut-chat-on-rails/", "abstract": "To implement chat on Rails we need the Juggernaut gem. The newest version of Juggernaut is build upon nodeJS. 1. Install Juggernaut: First, install nodejs (http://nodejs.org), redis (http:/code.google.com/p/redis/) and Juggernaut gem (gem install juggernaut). Then download server git clone … 2. Integrate with Rails to implement chat! After Juggernaut gem is installed, add it to environment.rb: config.gem \"juggernaut\" 1 config . gem \"juggernaut\" Suppose you have a system with user authentication, the idea is to generate an environment where you’re able to chat with other users and see to what extent features can be added in order to create an awesome chat! Note: Providing a step by step guide to create a working chat isn’t the idea of this tutorial. It’s to show different possibilites to develop on Juggernaut. Connecting to Juggernaut from Rails It’s necessary to add the file WebSocketMain.swf to the public folder. This hasn’t to be exactly this way, you can change its location, but if you do, then you’ll have to change the WEB_SOCKET_SWF_LOCATION, due to an issue with Firefox (… for further information visit juggernaut git page). Juggernaut serves every file necessary for running by default, so you can leave them on the server where Jaggernaut is installed and configure everything in your client (mainly due to Firefox) in order to avoid adding extra files to your project. Add to your view this line: &lt;script src=\"http://localhost:8080/application.js\" type=\"text/javascript\" charset=\"utf-8\"&gt;&lt;/script&gt; 1 & lt ; script src = \"http://localhost:8080/application.js\" type = \"text/javascript\" charset = \"utf-8\" & gt ; & lt ; / script & gt ; Note : you can add this file in another way or eventually add it to your project if you want. Inside another script tag: var jugger = new Juggernaut; 1 var jugger = new Juggernaut ; ** Connection events provided by Juggernaut Juggernaut provides you with three different events : Connect, Disconnect o Reconnect. jugger.on(\"connect - disconnect - reconnect\", callback) 1 jugger . on ( \"connect - disconnect - reconnect\" , callback ) You can use this events to implement whatever function you want to execute, i.e. alerting when a client has connected: jug.on(\"connect\", function() {alert(\"I'm connected!!\");}); 1 jug . on ( \"connect\" , function ( ) { alert ( \"I'm connected!!\" ) ; } ) ; At this point you have the connection of your client ready, you can test it, you have to start Juggernaut. * First run redis : ./redis-server redis.conf 1 . / redis - server redis . conf (whereever you have installed it) * Then go to the folder where you’ve downloaded the server to and run juggernaut: node server.js 1 node server . js Let’s chat! Now that we have everything going, would be a good time to start chatting. First you need to subscribe to a channe. Inside the channel you’ll be able to connect and user messages scope is restricted to channel. Then : jug.subscribe(\"name_or_channel_id\", function(data) { .. here you'll handle all messages coming from channel ... }) 1 jug . subscribe ( \"name_or_channel_id\" , function ( data ) { . . here you ' ll handle all messages coming from channel . . . } ) Clarifying a bit… with this function we subscribe to the channel and can pass a function as a parameter to handle data coming from it, i.e. any user’s messages, let’s give an example: First you need to have a div tag with an id (id_div), what you want is to append a list element containing message content to the div element everytime someone sends a message. jug.subscribe(\"name_or_channel_id\", function(data){\r\nvar li = $(\"&lt;li /&gt;\");\r\nli.text(data);\r\n$(\"#id_div\").append(li);\r\n}); 1 2 3 4 5 jug . subscribe ( \"name_or_channel_id\" , function ( data ) { var li = $ ( \"&lt;li /&gt;\" ) ; li . text ( data ) ; $ ( \"#id_div\" ) . append ( li ) ; } ) ; You can have a div or textearea element, adding text to a textarea element is relatively simpler, it’s enough to do textarea.value += (data + \"\\n\"); 1 textarea . value += ( data + \"\\n\" ) ; but you’ll lose customizing power. li elements can be enriched with style, making the chat more attractive. You’ve received data and added to a div element, but you’re still unable to send data. Let’s setup a form: &lt;% form_remote_tag(:url =&gt; path_to_send_message_method, :success =&gt; \"$('#msg_body').value=''\" do &gt;\r\n&lt;= text_field_tag 'msg_body', '', :size =&gt; '50' &gt;\r\n&lt;= submit_tag 'Send Message' &gt;\r\n&lt;% end %&gt; 1 2 3 4 & lt ; % form_remote_tag ( : url =& gt ; path_to_send_message_method , : success =& gt ; \"$('#msg_body').value=''\" do & gt ; & lt ; = text_field _ tag 'msg_body' , '' , : size =& gt ; '50' & gt ; & lt ; = submit _ tag 'Send Message' & gt ; & lt ; % end % & gt ; And the method : def send_message\r\nJuggernaut.publish(\"name_or_channel_id\", parse_chat_message(params[:msg_body], current_user))\r\nend 1 2 3 def send_message Juggernaut . publish ( \"name_or_channel_id\" , parse_chat_message ( params [ : msg_body ] , current_user ) ) end parse_chat_message? … Yes, before publishing data, you can process it and why not modify it a bit. def parse_chat_message(msg, user)\r\nreturn \"#{user.login} says: #{msg}\"\r\nend 1 2 3 def parse_chat_message ( msg , user ) return \"#{user.login} says: #{msg}\" end We are sending information to Juggernaut with this format : ‘usuario says: something’ Done!, you have your server up and running, and know how to send and receive data, there’s nothing else to try. Extras: Private chat case :* You can’t call a chat a chat if you can’t have a private conversation with someone. A good way of implementing this, is subscribing your users to a particular chatroom identified by user’s id. Simple add another subscribe: jug.subscribe(\"name_or_channel_id\", function(data){ .. handle private message .. } 1 jug . subscribe ( \"name_or_channel_id\" , function ( data ) { . . handle private message . . } The receiving user must be able to subscribe to your chat and send messages to you. To achive that, you should write a function that when you click on the user you want to send a private message to, subscribes to its chatroom and allows you to send messages to the user. This can be implemented in many different ways! You’ll have to simply make your choice according to your needs! <script src=\"&lt;a href=\"><!--mce:0--></script> 1 <script src = \"&lt;a href=\" > < ! -- mce : 0 -- > </script>", "date": "2010-11-19"},
{"website": "Moove-It", "title": "bullet-a-gem-to-help-reduce-n1-queries", "author": [" Lucía Escanellas "], "link": "https://blog.moove-it.com/bullet-a-gem-to-help-reduce-n1-queries/", "abstract": "Bullet is a plugin written by Richard Huang, that helps reducing the number of queries an application makes. It was first posted on 2009, but it is still a pretty useful gem to monitor your application for performance improvements. It has several ways of notifying problems: by Growl notifications, JavaScript alerts by default, and even using XMPP too. Additionally, it saves on its own bullet.log the exact line and stack trace of what caused the alert, and if you want to, it can also write to the application log. The project is on GitHub: http://github.com/flyerhzm/bullet You are probably familiar with the N+1 query and cache counter problems, so let’s look how Bullet will detect these by monitoring the database queries. The N+1 query problem Suppose we have our application with accounts having many rate plans: class RatePlan &lt; ActiveRecord::Base\n  belongs_to :account\nend 1 2 3 class RatePlan & lt ; ActiveRecord :: Base belongs_to : account end If we iterate over the rate plans, accessing rate plans attributes, like: RatePlan.all.each {\n  |post| puts \"#{rate_plan.name}, by #{rate_plan.account.name}\"\n} 1 2 3 RatePlan . all . each { | post | puts \"#{rate_plan.name}, by #{rate_plan.account.name}\" } Will be using N+1 queries: one query to get all the rate plans then, for each rate plan, one query to get the corresponding account. (This means that if we have 1000 posts, we’ll be doing 1000+1 queries) By using the :include option, we can retrieve the rate plans and their corresponding accounts on the same query: RatePlan.find(:all, :include =&gt; :account).each {\n   |post| puts \"#{rate_plan.name}, by #{rate_plan.account.name}\"\n} 1 2 3 RatePlan . find ( : all , : include =& gt ; : account ) . each { | post | puts \"#{rate_plan.name}, by #{rate_plan.account.name}\" } By detecting if we forgot to use eager loading or a counter cache, Bullet can save time looking for problems on the fly. For example, Bullet will show this message: We have to be careful not to abuse the :include option. If the rate_plans table is big enough, it could take too much server memory. Also, we could end up with big, slow requests, or getting from the server a lot of data that won’t be used. Bullet will show a message if it detects an unused eager loading. Cache counters Cache counters is another improvement in a one-to-many relationship. Because the _has_many_ relationship defines an attribute that is a collection, if we use the size property on this attribute, it will trigger a select count(*) on the child table. This is generally acceptable, except when we are using frequently count and we end up going to the database unnecessarily. In order to avoid that, we can use counter caching: Active Record will maintain for a parent table the number of child references. Bullet also shows a message if it finds a cache counter is needed. Configuration You have to first install the gem, by adding it to the Gemfile, or by installing manually: sudo gem install bullet --pre 1 sudo gem install bullet -- pre The next thing is to add some configuration to the development environment. Bullet won’t do anything unless you configure it explicitly, and also note that it’s not a good idea to configure it on the production environment, because on that case your users would also receive those N+1 query alerts. Configuration allows Growl notifications, and sending messages by XMPP (Jabber), but since the default JavaScript alerts are good enough for me, I configured it like this: config.after_initialize do\n  Bullet.enable = true\n  Bullet.alert = true\n  Bullet.bullet_logger = true\n  Bullet.console = true\n  Bullet.growl = false\n  Bullet.rails_logger = true\n  Bullet.disable_browser_cache = true\nend 1 2 3 4 5 6 7 8 9 config . after_initialize do Bullet . enable = true Bullet . alert = true Bullet . bullet_logger = true Bullet . console = true Bullet . growl = false Bullet . rails_logger = true Bullet . disable_browser_cache = true end Note the last option, because disabling browser cache can save you some trouble on some configurations. In any case, if Bullet is not working, try first disabling browser cache. With this configuration, it will show a JavaScript alert, the same message on the console, and the detail and stack trace on the bullet.log.", "date": "2010-11-11"},
{"website": "Moove-It", "title": "all-you-shold-know-about-web-development", "author": [" Juan Pablo "], "link": "https://blog.moove-it.com/all-you-shold-know-about-web-development/", "abstract": "I want to share with you a compilation of topics you should know if you are planning on developing a successful web application. This list is just a collection of the best suggestions I read from this discussion on Stack Overflow . All credit goes to all the people who contributed on that thread, I’m just picking the best topics from there. All the practices mentioned fall into the following categories: Security, Performance, Interface, SEO, Maintenance and Productivity. Security This is my favorite topic, if you ask me what is my best advice about security this is it: Never trust user input (that means cookies too!) Other advices mentioned were… Avoid cross site scripting Avoid cross site request forgeries Know about SQL injection and how to prevent it Make sure your database connection information is secured. Keep yourself informed about the latest attack techniques and vulnerabilities affecting your platform. Use SSL / HTTPS for login and any pages where sensitive data is entered (like credit card info) Hash and salt passwords rather than storing them plain-text. Read The Google Browser Security Handbook Performance Techniques on making our site lighter and faster. Optimize images – don’t use a 20 KB image for a repeating background Learn how to gzip/deflate content Combine/concatenate multiple stylesheets or multiple script files to reduce number of browser connections and improve gzip ability to compress duplications between files Take a look at the Yahoo Exceptional Performance site, lots of great guidelines including improving front-end performance and their YSlow tool. Google page speed is another tool for performance profiling. Both require Firebug installed. Use CSS Image Sprites for small related images like toolbars (see the “minimize http requests” point) Busy web sites should consider splitting components across domains. Static content (ie, images, CSS, JavaScript, and generally content that doesn’t need access to cookies) should go in a separate domain that does not use cookies, because all cookies for a domain and it’s subdomains are sent with every request to the domain and its subdomains. One good option here is to use a Content Delivery Network (CDN). Utilize Google Closure Compiler for JavaScript and other minification tools Interface Be aware that browsers implement standards inconsistently and make sure your site works reasonably well across all major browsers. At a minimum test against a recent Gecko engine (Firefox), a Webkit engine (Safari, Chrome, and some mobile browsers), your supported IE browsers (take advantage of the Application Compatibility VPC Images), and Opera. Also consider how browsers render your site in different operating systems. Staging: How to deploy updates without affecting your users. Don’t display unfriendly errors directly to the user Don’t put users’ email addresses in plain text as they will get spammed. Build well-considered limits into your site Learn how to do progressive enhancement Always redirect after a POST. SEO Consider URLs, a URL design with REST in mind could make exposing APIs easier in the future. Definitely much easier to get your URLs right the first time then to change them in the future and deal with the SEO consequences. Avoid links that say “click here”. Use “search engine friendly” URL’s, i.e. use example.com/pages/45-article-title instead of example.com/index.php?page=45 Have an XML sitemap Use <link rel=”canonical” … /> when you have multiple URLs that point to the same content Use Google Webmaster Tools and Yahoo Site Explorer Install Google Analytics right at the start Know how robots.txt and search engine spiders work Redirect requests (using 301 Moved Permanently) asking for www.example.com to example.com (or the other way round) to prevent splitting the google ranking between both sites Know that there can be bad behaving spiders out there Manteinance and Productivity Understand you’ll spend 20% of the time coding and 80% of it maintaining Set up a good error reporting solution Have some system for people to contact you with suggestions and criticism. Document how the application works for future support staff and people performing maintenance Make frequent backups! (And make sure those backups are functional) Don’t forget to do your Unit Testing . Get it looking correct in Firefox first, then Internet Explorer. Code from the beginning with maintainability in mind I hope you learn something new from this list the same way I did. Thanks to all the people from Stack Overflow community that contributed in such a rich discussion of web development practices. We will focus on some of those topics in upcoming posts.", "date": "2010-10-15"},
{"website": "Moove-It", "title": "shareplus-ipad-iphone-sharepoint", "author": [" Martin Cabrera "], "link": "https://blog.moove-it.com/shareplus-ipad-iphone-sharepoint/", "abstract": "One of our business partners, SouthLabs, has just published the following video on their flagship product, SharePlus . It’s iPad/iPhone client app for Microsoft’s SharePoint. They tell me they are about to release version 2.0 anyday now. Gabriel (Gabo) and Daniel are the owners of this new company. They are a young startup not very different from ourselves, Gabo is a great friend of mine and an excellent professional. Best of lucks guys !", "date": "2010-07-23"},
{"website": "Moove-It", "title": "need-help-make-a-donation", "author": [" Gabriela Isnardi "], "link": "https://blog.moove-it.com/need-help-make-a-donation/", "abstract": "Stop for a sec. We all want to start the New Year full of energy. But the truth is that when this time of the year comes we hardly ever have any energy left to even think of ways to make the imminent year look better, brighter or more successful. Besides, during 2010 we all may have had pretty different moments and facts to deal with, laugh at or cry for. But whatever the case is, in 2011 we all want to improve, feel better and move on. Here goes my idea: Get rid of the things you don’t need and make a donation before the end the year. The Children Center needs tons of things, from a sauce pan to color pencils. Start the New Year fresh and free from the things you no longer use… clothes, children books, a boardgame, towels, toys, folders, a blender you name it. Please contact us to make your donation. We are ready to go and pick them up for you.", "date": "2010-12-23"},
{"website": "Moove-It", "title": "history-modifications-hibernate-envers", "author": [" Martin Cabrera "], "link": "https://blog.moove-it.com/history-modifications-hibernate-envers/", "abstract": "Envers is one of the Hibernate’s project sponsored by Redhat. Works with Hibernate and Hibernate Entity Manager. Aims to enable easy auditing of persistent classes. For each audited entity you must annotate your persistent clases with @Audited, a table will be created, which will hold the history of changes made to the entity. The idea is very similar to CVS or Subersion. Each transaction with updates, deletes or inserts made in the database with hibernate generate a new revision number. One transaction is one revision. Envers provide a simple way to retrive your data of a revision using: revision number, date, queries with min and max function between others. You can use Envers wherever Hibernate works: standalone, inside JBoss AS, with JBoss Seam or Spring. It’s very easy … @Entity @Audited public class Usuario { public enum Rol{ EMPRESA, ADMINISTRADOR,MESA_ENTRADA; } private Long id; private String usuario; private String contrasenia; private Rol rol; @Id @GeneratedValue public Long getId() { return id; } public void setId(Long id) { this.id = id; } @Entity @Audited\r\npublic class User {\r\nprivate Long id;\r\n\r\n        @Id @GeneratedValue\r\n        public Long getId() { return id; }\r\n        public void setId(Long id) { this.id = id; }\r\n\r\n        // other fields and methods\r\n        ...\r\n} 1 2 3 4 5 6 7 8 9 10 11 @ Entity @ Audited public class User { private Long id ; @ Id @ GeneratedValue public Long getId ( ) { return id ; } public void setId ( Long id ) { this . id = id ; } // other fields and methods . . . } Envers configuration When configuring your Hibernate ( persistence.xml if you are using JPA, hibernate.cfg.xml or other if you are using Hibernate directly), add the following event listeners: &lt;persistence-unit ...&gt;\r\n&lt;provider&gt;org.hibernate.ejb.HibernatePersistence&lt;/provider&gt;\r\n&lt;class&gt;...&lt;/class&gt;\r\n&lt;properties&gt;\r\n   &lt;property name=\"hibernate.dialect\" ... /&gt;\r\n   &lt;!-- other hibernate properties --&gt;\r\n\r\n   &lt;property name=\"hibernate.ejb.event.post-insert\"\r\n             value=\"org.hibernate.ejb.event.EJB3PostInsertEventListener,org.hibernate.envers.event.AuditEventListener\" /&gt;\r\n   &lt;property name=\"hibernate.ejb.event.post-update\"\r\n             value=\"org.hibernate.ejb.event.EJB3PostUpdateEventListener,org.hibernate.envers.event.AuditEventListener\" /&gt;\r\n   &lt;property name=\"hibernate.ejb.event.post-delete\"\r\n             value=\"org.hibernate.ejb.event.EJB3PostDeleteEventListener,org.hibernate.envers.event.AuditEventListener\" /&gt;\r\n   &lt;property name=\"hibernate.ejb.event.pre-collection-update\"\r\n             value=\"org.hibernate.envers.event.AuditEventListener\" /&gt;\r\n   &lt;property name=\"hibernate.ejb.event.pre-collection-remove\"\r\n             value=\"org.hibernate.envers.event.AuditEventListener\" /&gt;\r\n   &lt;property name=\"hibernate.ejb.event.post-collection-recreate\"\r\n             value=\"org.hibernate.envers.event.AuditEventListener\" /&gt;\r\n&lt;/properties&gt;\r\n&lt;/persistence-unit&gt; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 & lt ; persistence - unit . . . & gt ; & lt ; provider & gt ; org . hibernate . ejb . HibernatePersistence & lt ; / provider & gt ; & lt ; class & gt ; . . . & lt ; / class & gt ; & lt ; properties & gt ; & lt ; property name = \"hibernate.dialect\" . . . / & gt ; & lt ; ! -- other hibernate properties -- & gt ; & lt ; property name = \"hibernate.ejb.event.post-insert\" value = \"org.hibernate.ejb.event.EJB3PostInsertEventListener,org.hibernate.envers.event.AuditEventListener\" / & gt ; & lt ; property name = \"hibernate.ejb.event.post-update\" value = \"org.hibernate.ejb.event.EJB3PostUpdateEventListener,org.hibernate.envers.event.AuditEventListener\" / & gt ; & lt ; property name = \"hibernate.ejb.event.post-delete\" value = \"org.hibernate.ejb.event.EJB3PostDeleteEventListener,org.hibernate.envers.event.AuditEventListener\" / & gt ; & lt ; property name = \"hibernate.ejb.event.pre-collection-update\" value = \"org.hibernate.envers.event.AuditEventListener\" / & gt ; & lt ; property name = \"hibernate.ejb.event.pre-collection-remove\" value = \"org.hibernate.envers.event.AuditEventListener\" / & gt ; & lt ; property name = \"hibernate.ejb.event.post-collection-recreate\" value = \"org.hibernate.envers.event.AuditEventListener\" / & gt ; & lt ; / properties & gt ; & lt ; / persistence - unit & gt ; Then, annotate your persistent class with @Audited (like the User example) And that’s it! Queries Find audited data You can access the audit (history) of an entity using the AuditReader interface, which you can obtain when having an open EntityManager. AuditReader reader = AuditReaderFactory.get(entityManager); \r\nUser oldUser = reader.find(User.class, userId, revision) 1 2 AuditReader reader = AuditReaderFactory . get ( entityManager ) ; User oldUser = reader . find ( User . class , userId , revision ) Search a revision number the whole documentation is in the Web site: https://docs.jboss.org/envers/docs/index.html#queries To ilustrate one example I show the way to search the max number revision of one Entity audited. Number revision = (Number) reader.createQuery().forRevisionsOfEntity(User.class, false, true)\r\n.addProjection(AuditEntity.revisionNumber().max())\r\n.add(AuditEntity.id().eq(userId))\r\n.getSingleResult(); 1 2 3 4 Number revision = ( Number ) reader . createQuery ( ) . forRevisionsOfEntity ( User . class , false , true ) . addProjection ( AuditEntity . revisionNumber ( ) . max ( ) ) . add ( AuditEntity . id ( ) . eq ( userId ) ) . getSingleResult ( ) ; We use Envers in a few projects. Works fine and very quickly, we recomended … no dubt ! Envers Web site: www.jboss.org/envers Envers documentation: https://docs.jboss.org/envers/docs/index.html", "date": "2010-10-27"},
{"website": "Moove-It", "title": "make-a-donation-contact-moove-it", "author": [" Gabriela Isnardi "], "link": "https://blog.moove-it.com/make-a-donation-contact-moove-it/", "abstract": "Moove-it plans to turn a block into a big apple This year we have chosen to help children and families from a modest neighborhood situated in Suárez- CANELONES, just about 20 minutes from the Capital. For this purpose we chose a block where 2 different organizations work together to enhance the life styles, social and health conditions of some underprivileged children and families. What do they need? Think of everything a child may need or want. From toys to diapers, everything will be welcome. We will provide a list of materials to help you with ideas soon. These Centers are: Arapí Center (Centro CAIF Arapí ) This center will open its doors to 84 underprivileged children under the age of 4  in February 2011, taking care of young children and their families. In the same premises there is a school which works with more than 50 children, some of them with learning difficulties,  who will also benefit from your donations. Centro CAIF Arapí . Address: Camino Andaluz KM. 1.200, ruta 84, Localidad de Suárez- Departamento de  CANELONES. Tel: 22964584. Clinic Cassarino This Medical Clinic provides health care to underprivileged local families in Suarez. Pediatricians, nurses, psychologists, elementary school children and other professionals work together to make a dream come true. You can help. Clinica Cassarino . Address: Camino Andaluz KM. 1.200, ruta 84, Localidad de Suárez- Departamento de  CANELONES. Tel: 22965793 How can you help? You can contact Moove-It 2706 60 71,  and we will take care of your donations. Otherwise you can contact the Centers directly. If you want to help do not hesitate to call us.", "date": "2010-12-06"},
{"website": "Moove-It", "title": "new-game-xo-computers-trojan-chicken", "author": [" mn.burghi "], "link": "https://blog.moove-it.com/new-game-xo-computers-trojan-chicken/", "abstract": "Trojan Chicken , a young company aimed at the creation and development of high quality, content-rich videogames. As business partners we are planning several developments together. We want to share their new development for XO Computers; the game called D.E.D. Uruguay. Best of lucks guys ! The game. Is an adventure in which the player takes the role of a detective, and has to travel around the country capturing thieves who are stealing part of the national heritage. Players will have to solve puzzles using their knowledge of mathematics and language in order to obtain clues. Afterwards, they will have to associate the obtained clues using their knowledge of history and geography, in order to find the place where the thief is hidden. After finding the hideout, the player will chase after the thief until he is finally caught and imprisoned. This game was inspired by a successful game of the 80s named Carmen San Diego. Their purpose is to revitalize this kind of videogames in our community, providing schoolchildren with entertaining activities in which simultaneously help them in their learning process of local culture, history and other knowledge that will contribute to their personal development. See this video … a tour by the game !", "date": "2010-07-23"},
{"website": "Moove-It", "title": "best-place-to-live-and-work-urugua", "author": [" Gabriela Isnardi "], "link": "https://blog.moove-it.com/best-place-to-live-and-work-urugua/", "abstract": "According to the Legatum Prosperity Index, Uruguay ranks first in Latin America. This index produces rankings based on the foundations of prosperity, which means that Moove-it is located in a country with factors that will produce economic growth and happy citizens over the long run. If you have a look at the ranking in the Americas, Canada is on top of the list and Uruguay is in the third place, just after the United States. For more information please visit: https://www.prosperity.com/ You can donwload the pdf file with this information too.", "date": "2010-11-23"},
{"website": "Moove-It", "title": "rules-engine-java-drools-ruby-ruleby", "author": [" Martin Cabrera "], "link": "https://blog.moove-it.com/rules-engine-java-drools-ruby-ruleby/", "abstract": "Today we made one presentation with the whole team of moove-it about rules engine in Java and Ruby. We talk about Drools and ruleby. We share the presentation … Rules Engine – java(Drools) & ruby(ruleby) View more presentations from martincabrera .", "date": "2010-12-03"},
{"website": "Moove-It", "title": "mooveit-social-network-ceibal-project", "author": [" Martin Cabrera "], "link": "https://blog.moove-it.com/mooveit-social-network-ceibal-project/", "abstract": "This past june 24 Moove-it won the contest “Best ideas for Ceibal 2010” organized by Rayuela and the Ceibal Plan (One laptop per child), presenting its idea “Together with Ceibal”. Follow this link to read the official information (spanish) Rayuela Web site of Best ideas Moove-it was selected by the Ceibal Plan as the company who will develop the first social network for more than 400.000 students of primary and high school. The idea is to build a web-based tool which will allow students and teachers to interact inside and also outside the classroom. The platform will provide virtual classrooms where teachers can work directly with their groups using all the benefits of the Web, Internet and XOs. Students will also be able to use the tool to meet each other, share their interests or even play together. This video shows a brief idea of what the “Ceibal Plan” is. It feels really good to be part of something like this!", "date": "2010-06-25"},
{"website": "Moove-It", "title": "usability-part-1", "author": [" admin "], "link": "https://blog.moove-it.com/usability-part-1/", "abstract": "“ Usability is making your website easy for your visitors to find the information they need when they need it .” When you are building a web page one of the first things that come to your mind is where to put the most important features so the user find them easily. So, doing a bit of research I found that most users have a particulary way of looking the screen that is called the  “F-Shaped Pattern “. This pattern says that users tend to first read in a horizontal movement, usually across the upper part of the content area. This initial element forms the F’s top bar. Next, users move down the page a bit and then read across in a second horizontal movement that typically covers a shorter area than the previous movement. This additional element forms the F’s lower bar. Finally, users scan the content’s left side in a vertical movement. Sometimes this is a fairly slow and systematic scan that appears as a solid stripe on an eyetracking heatmap. Other times users move faster, creating a spottier heatmap. This last element forms the F’s stem. For more information about the F-Shaped Pattern visit this link Other usability tip that I found is that users tend to focus on people’s faces and eyes, this means that if there is a face in your page users focus on it, next they are going to look what the eyes of the face are watching. So if you are going to place people in your page make sure they watch the content you want to show. Talking a bit about scrolling, recent studies prove that users are quite comfortable with scrolling and in some situations they are willing to scroll to the bottom of the page. So it is a good idea to divide your layout into sections for easy scanning, separating them with a lot of white space. There are many of usability advices but I think that the most important is to keep it simple. If you want that users navegate your page easily and they don’t get lost, your page must follow conventions of web design.  For example:  use blue for hiperlinks, if you use another color the user will have to learn the new color to associate it with a link.", "date": "2009-10-13"},
{"website": "Moove-It", "title": "designing-a-restful-api-specification-part-ii", "author": [" Gian Zas "], "link": "https://blog.moove-it.com/designing-a-restful-api-specification-part-ii/", "abstract": "– Every resource is identified by an URI The URI <base_uri>/car/<car_id> identifies the car whose ID is <car_id>. If you are designing an API to interact with a software system, the ID probably will be the primary key of the car in the database – Use the HTTP Verbs. GET <base_uri>/car/<car_id> -> This operation retrieves information about the car whose id is car_id. No side effects here, please! GET <base_uri>/cars -> Retrieves the set of cars, parameters containing the page_size and page_number are highly recommended. No side effects. POST <base_uri>/car -> Create a new car, the car attributes are passed as HTTP parameters. This operation must retrieve the ID of the new object created. PUT <base_uri>/car/<car_id> -> Modify the data associated with the car, the attributes are passed as HTTP parameters DELETE <base_uri>/car/<car_id> -> You want to delete an object? Just it! Don’t use action names, like get_car, or browse_for_id in the URI, the verbs indicate the desired action A principle-design discussion… singular or plural nouns at the URI? Personally I prefer nouns in singular for operations involving ONE resource, and plural for MORE than one resource. – Use of HTTP Status codes Use it to indicate the result of the operation. I recommend that the API return appropriate HTTP status codes for each request. 200 OK: The request has been successfully completed. 304 Not Modified: Operation ok. There was no new data to return 400 Bad Request: The request is invalid. Error messages are returned for explanation purposes. 401 Not Authorized: The credentials provided are invalid or are needed for the operation. 403 Forbidden: The request is refused by the API implementation. Error messages are returned. 404 Not Found: The resource requested doesn’t exist. Ex: The provided car_id in the url doesn’t exist. 500 Internal Server Error: An unrecoverable error has occurred at server side. 502 Bad Gateway: The API service is down or being upgraded 503 Service Unavailable: The service is overloaded with a lot requests and can’t manage the incoming request. – Error Messages When the API returns error messages it must comply with a structure. Here I present a simple structure in JSON format: {\r\n    error_messages: [\r\n        \"error_message_1\",\r\n        \"error_message_2\",\r\n        ...,\r\n        \"error_message_n\"\r\n    ]\r\n} 1 2 3 4 5 6 7 8 { error_messages : [ \"error_message_1\" , \"error_message_2\" , . . . , \"error_message_n\" ] } – Data Format The operations return information about the resource requested or information about error messages. The format of that information could be preferably at the moment JSON or XML. If you will be supporting more than one format, or think that in future you’ll be, a possible manner is to include the format at the URI, for example: PUT <base_uri>/car.json/<car_id> PUT <base_uri>/car.xml/<car_id> Conclusion: The key base-aspects of designing a RESTful API are – Identify every resource (or object in OOP idiom) with an URI – Use HTTP verbs – Use HTTP status code – Don’t use action names in the URI An important implementation detail: Use UTF-8, unless your data is only on languages with high code points (digging more in the cloud about this topic is recommended) Another important aspect is about security access to the operation, but is an subject for a future post, stay tuned!", "date": "2009-02-27"},
{"website": "Moove-It", "title": "my-personal-information-belongs-to-mark-s", "author": [" Ariel Luduena Karhs "], "link": "https://blog.moove-it.com/my-personal-information-belongs-to-mark-s/", "abstract": "Today I logged to my facebook and my home was like always.. nothing new… nothing told me that the ownership and proprietary rights had changed, and nobody told me that my personal information now belongs to Mark (including all my beautiful pictures!) What happened with the “Accept” and “Not accept” buttons? (maybe Mark forgot about this…) I was thinking about why this happened… Maybe Mark needs money! How much does the data of the largest social network in the world cost? Surely a lot of money. Imagine if the data belongs to the social network rather than the users themselves. Maybe Mark has development problems! If a user is deleted from facebook, what happens with all the information linked to his friends? (delete in cascade? :P) I don’t know what happens here and I don’t care. Obviously this is annoying because you don’t have control of anything and this change was not clear at all. This is the paragraph of the controversy: You hereby grant Facebook an irrevocable, perpetual, non-exclusive, transferable, fully paid, worldwide license (with the right to sublicense) to (a) use, copy, publish, stream, store, retain, publicly perform or display, transmit, scan, reformat, modify, edit, frame, translate, excerpt, adapt, create derivative works and distribute (through multiple tiers), any User Content you (i) Post on or in connection with the Facebook Service or the promotion thereof subject only to your privacy settings or (ii) enable a user to Post, including by offering a Share Link on your website and (b) to use your name, likeness and image for any purpose, including commercial or advertising, each of (a) and (b) on or in connection with the Facebook Service or the promotion thereof UPDATE 17/02 : Facebook has rolled back to it’s previous TOS, s urely because of all the negative reactions this caused in the online community . This happens when this type of changes, which are inherently controversial, are poorly communicated and explained AFTER being implemented. This is a sensitive issue, and before I am stripped apart of the ownership of MY data I would like to be at least told about it . (source: http://www.techcrunch.com/2009/02/17/facebook-backtracks-under-community-pressure-goes-back-to-old-tos-for-now/)", "date": "2009-02-17"},
{"website": "Moove-It", "title": "206", "author": [" Gian Zas "], "link": "https://blog.moove-it.com/206/", "abstract": "One of the most popular rants against ruby are based on its not so good performance. Actually has been made improvements on this topic, specially in the new Ruby implementation ( Ruby 1.9 ) based on the YARV virtual machine. JRuby (Ruby over JVM implementation) has been focused in its performance from its lastest releases. Here at moove-it we are exploring the posibility of use Rails over JRuby in some JEE application servers, so we need some facts about JRuby performance (and other topics like gems compatibility, etc) The Ruby community has put a set of benchmarks at ruby1.9 trunk: http://svn.ruby-lang.org/repos/ruby/trunk/benchmark/ So, we test with these benchmarks and here are the results!. The code that run the benchmarks is like that: block_to_benchmark = lambda { load BENCHMARKS_DIR + '/' + filename}\r\nBenchmark.measure &amp;block_to_benchmark 1 2 block_to_benchmark = lambda { load BENCHMARKS_DIR + '/' + filename } Benchmark . measure & amp ; block_to_benchmark (yes, we are using the benchmark module bundled with the ruby standard lib) Benchmark Ruby1.8 Ruby1.9 JRuby1.3.0 Ruby1.8 / Ruby1.9 Ruby1.8 / JRuby1.3.0 bm_app_fib.rb 9.02 4.00 3.83 2.25 2.36 bm_app_mandelbrot.rb 3.36 0.81 1.49 4.14 2.26 bm_app_pentomino.rb 144.24 91.97 105.64 1.57 1.37 bm_app_raise.rb 6.94 6.98 1.63 -1.01 4.27 bm_app_strconcat.rb 2.94 1.63 1.30 1.81 2.27 bm_app_tak.rb 12.27 5.66 4.06 2.17 3.02 bm_app_tarai.rb 9.81 4.83 3.27 2.03 3.00 bm_app_uri.rb 6.59 3.77 3.69 1.75 1.79 bm_io_file_create.rb 7.17 3.12 7.70 2.30 -1.07 bm_io_file_read.rb 2.27 0.61 0.64 3.71 3.54 bm_io_file_write.rb 1.77 10.98 0.31 -6.22 5.64 bm_loop_for.rb 2.97 7.81 7.50 -2.63 -2.53 bm_loop_generator.rb 149.64 3.00 10.95 49.88 13.66 bm_loop_times.rb 4.88 6.88 7.95 -1.41 -1.63 bm_loop_whileloop.rb 11.23 3.67 9.42 3.06 1.19 bm_loop_whileloop2.rb 2.33 0.72 1.89 3.24 1.23 bm_so_array.rb 8.41 7.33 16.17 1.15 -1.92 bm_so_binary_trees.rb 4.50 2.14 2.84 2.10 1.58 bm_so_concatenate.rb 2.49 1.95 3.03 1.27 -1.22 bm_so_exception.rb 7.62 10.42 2.63 -1.37 2.90 bm_so_fasta.rb 13.59 11.77 16.03 1.16 -1.18 bm_so_lists.rb 2.27 1.38 1.48 1.65 1.53 bm_so_mandelbrot.rb 44.55 32.49 49.28 1.37 -1.11 bm_so_matrix.rb 2.69 2.13 1.84 1.26 1.46 bm_so_meteor_contest.rb 52.55 25.17 22.02 2.09 2.39 bm_so_nbody.rb 35.80 26.61 16.78 1.35 2.13 bm_so_nested_loop.rb 6.09 6.88 8.64 -1.13 -1.42 bm_so_nsieve.rb 26.89 13.11 24.69 2.05 1.09 bm_so_nsieve_bits.rb 62.50 46.05 42.55 1.36 1.47 bm_so_object.rb 11.56 11.52 3.11 1.00 3.72 bm_so_partial_sums.rb 80.13 228.91 31.22 -2.86 2.57 bm_so_pidigits.rb 10.33 10.44 7.03 -1.01 1.47 bm_so_random.rb 4.59 12.88 1.86 -2.80 2.47 bm_so_sieve.rb 0.84 0.34 0.63 2.45 1.35 bm_so_spectralnorm.rb 41.86 92.88 20.03 -2.22 2.09 bm_vm1_block.rb 26.22 13.44 27.44 1.95 -1.05 bm_vm1_const.rb 19.02 6.27 17.64 3.03 1.08 bm_vm1_ensure.rb 20.06 5.11 16.19 3.93 1.24 bm_vm1_ivar.rb 17.95 9.50 18.78 1.89 -1.05 bm_vm1_ivar_set.rb 19.22 9.83 21.67 1.96 -1.13 bm_vm1_length.rb 22.95 7.44 18.16 3.09 1.26 bm_vm1_neq.rb 20.81 6.58 14.98 3.16 1.39 bm_vm1_not.rb 14.91 5.58 12.64 2.67 1.18 bm_vm1_rescue.rb 15.72 4.64 21.38 3.39 -1.36 bm_vm1_simplereturn.rb 23.84 9.66 16.05 2.47 1.49 bm_vm1_swap.rb 50.25 5.73 24.38 8.76 2.06 bm_vm2_array.rb 10.72 19.27 5.42 -1.80 1.98 bm_vm2_case.rb 5.06 1.66 4.00 3.06 1.27 bm_vm2_eval.rb 32.20 200.98 69.41 -6.24 -2.16 bm_vm2_method.rb 15.45 9.45 11.36 1.63 1.36 bm_vm2_mutex.rb 5.47 6.34 7.30 -1.16 -1.33 bm_vm2_poly_method.rb 20.61 12.16 21.05 1.70 -1.02 bm_vm2_poly_method_ov.rb 5.00 1.66 4.53 3.02 1.10 bm_vm2_proc.rb 12.00 3.86 6.75 3.11 1.78 bm_vm2_regexp.rb 5.89 19.25 5.63 -3.27 1.05 bm_vm2_send.rb 5.05 2.11 4.38 2.39 1.15 bm_vm2_super.rb 5.75 3.17 4.44 1.81 1.30 bm_vm2_unif1.rb 5.20 1.99 3.61 2.62 1.44 bm_vm2_zsuper.rb 6.87 3.48 5.49 1.97 1.25 bm_vm3_thread_create_join.rb 1.95 7.70 19.13 -3.94 -9.79 bm_vm3_gc.rb 292.30 266.14 0.36 1.10 814.20 . A looser conclusion may be that Ruby 1.9 is 95% faster than Ruby1.8, and JRuby 1.3.0 is 10% faster than 1.8, in general the new implementations are faster than Ruby1.8,  especially 1.9 (twice as faster). The benchmarks were under WindowsXP  SP3 , 4GB RAM , and a Intel Core 2 duo 2.0GHz. Happy hacking!", "date": "2009-06-18"},
{"website": "Moove-It", "title": "transferring-directory-trees-over-ftp", "author": [" Gian Zas "], "link": "https://blog.moove-it.com/transferring-directory-trees-over-ftp/", "abstract": "Imagine all the people having to live without ssh and an assigned job related to hosting migrations with low bandwidth access, what a painfull world… Ok, get up! it was only a nightmare, but if a strange reason causes you must accomplish that task you may wish get/put directories recursively from/to a ftp server . The bad news are that the FTP protocol doesn’t supports this operation, you can get only an individual file or a group of files that expands some wildcard expression, but you can’t get/put recursively an entire directory. Luckily ncftp saves the day, it’s a free ftp client (free as in beer and free as non-private), that supports many features like background processing and directory tree copy. So, to GET the contents a whole directory tree just invoke ncftpget command: $&gt; ncftpget -R -u &lt;user_account&gt; ftp.moove-it.com /home/gian/migration_h /remote_directory 1 $ & gt ; ncftpget - R - u & lt ; user_account & gt ; ftp . moove - it . com / home / gian / migration_h / remote_directory * -R copy a whole directory * ftp.xxx.x is the remote ftp server * /home/yyy is the local destination * and /remote_xx is the ftp directory to be transfered PUT a directory is trivial too, just invoke the ncftpput command: $&gt; ncftput -R -u &lt;user_account&gt; ftp2.moove-it.com /home/gian/migration_h /remote_directory 1 $ & gt ; ncftput - R - u & lt ; user_account & gt ; ftp2 . moove - it . com / home / gian / migration_h / remote_directory be happy!", "date": "2009-04-19"},
{"website": "Moove-It", "title": "facebooker-rails-site-facebook-connect-3", "author": [" Augusto Guido "], "link": "https://blog.moove-it.com/facebooker-rails-site-facebook-connect-3/", "abstract": "Welcome to part 3 of the facebooker trilogy I’m kind of tired of writing the same title over and over again and changing just the last number, problem is there are so many things we can do with this stuff we could write tons of this. I could put in the title what we will talk about specifically in this post, but the idea is to follow this post/tutorials in order. This will probably be the last part of this set of posts, the following ones will refer in it’s specifically to what will be done. So far we explained how to configure facebooker, sign up, invite friends and bit of the theory. So today we’ll publish something in the users dashboard, so him and all his friends can see it his wall. So go to developers.facebook.com/tools.php and select “Feed Template Console”, select the application you want and click next. Give me a minute to explain you what we are doing. We are going to register our feeds in facebook, and then we are going to call them by a number they will give us. You can do this using facebooker, or you can register them in the link I gave you above. I think that using the facebook tools is way easier and also facebooker may be a bit out of date regarding this area due to all the legal and business changes in facebook. Also not event facebook has yet clear what they want: “At this time, the policy on automatically publishing one line stories has not been finalized”. Anyway, complete the step guide for feeds in facebook and keep the number of the template you created. BTW while completing it you will find this kind of stuff “{*actor*}” without quotes. These are tokens that allow to put variable stuff (like names, links, etc.), you can read about them while creating the templates since they are well explained over there. Now, you have the story. You need the users approval to publish it, so what facebook does is shows the story to the user and gives him otions to skip or publish. If we are in luck the users hits “publish”. So we now obviously want to show it to the user so he can choose. Here’s the code FB.ensureInit(function() {\r\nvar body_general = \"Join them in my facebook connect site\"\r\nvar template_data =\r\n{\r\n\"actor\":  \"&lt;%= @_logged_user.name %&gt;\",\r\n\"friends\":\"&lt;%= @event.users.collect{|u| u.name_or_alias}.join(', ') %&gt;\",\r\n\"event\":  \"&lt;%= @event.description %&gt;\",\r\n\"place\":  \"&lt;%= @event.place %&gt;\",\r\n\"time\" :  \"&lt;%= @event.start.strftime('%m/%d/%Y %I:%M %p') %&gt;\"\r\n};\r\nvar user_message_prompt = \"&lt;%= @match.comment %&gt;\";\r\nFB.Connect.showFeedDialog('the_number_of_your_template', template_data, [], body_general, null, FB.RequireConnect.require, FB.RequireConnect.promptConnect, user_message_prompt);\r\n}); 1 2 3 4 5 6 7 8 9 10 11 12 13 FB . ensureInit ( function ( ) { var body_general = \"Join them in my facebook connect site\" var template_data = { \"actor\" : \"&lt;%= @_logged_user.name %&gt;\" , \"friends\" : \"&lt;%= @event.users.collect{|u| u.name_or_alias}.join(', ') %&gt;\" , \"event\" : \"&lt;%= @event.description %&gt;\" , \"place\" : \"&lt;%= @event.place %&gt;\" , \"time\" : \"&lt;%= @event.start.strftime('%m/%d/%Y %I:%M %p') %&gt;\" } ; var user_message_prompt = \"&lt;%= @match.comment %&gt;\" ; FB . Connect . showFeedDialog ( 'the_number_of_your_template' , template_data , [ ] , body_general , null , FB . RequireConnect . require , FB . RequireConnect . promptConnect , user_message_prompt ) ; } ) ; In the template data you have to complete the tokens you created when publishing your story. You can everything pretty well explained in this couple of links: http://wiki.developers.facebook.com/index.php/JS_API_M_FB.Connect.ShowFeedDialog http://wiki.developers.facebook.com/index.php/Feed.publishUserAction http://wiki.developers.facebook.com/index.php/Publishing_Feed_Stories_to_Facebook Pay attention to the “FB.ensureInit(function(){“ , I don’t know why I couldn’t find anywhere in facebook that mention this should be there. If you don’t add this nothing will happen, maybe it’s obvios for some people, but it wasn’t for me. I’m sory I can’t remember where I find this, but you can read here what it does. Well, that’s kind of the big picture of what you have to do to publish stories, remember this isn’t supposed to be a complete tutorial of everything you can do, just a guide based on my personal experience to get started. Your facebook connect site should be quite complete now that you can signup, invite your friends and write stuff that will appear directly in facebook. Not to mention if you have used all the other resources facebook give us and facebooker facilitate us. Thanks for reading!", "date": "2009-05-25"},
{"website": "Moove-It", "title": "lightweight-cache-java-applications-whirlycache", "author": [" Martin Cabrera "], "link": "https://blog.moove-it.com/lightweight-cache-java-applications-whirlycache/", "abstract": "Data caching is a very important consideration for JEE applications. A classic problem for any application is to detect and solve the recurrent calls for optimizing the applications. Some cases can be: database calls business logic calls remote method invocations Data caching limits the number of remote invocations in distributed applications and improves performance of web applications by reducing the number of calls. Several solutions exist to include a framework for implementing a cache. OsCache SwarmCache jofti jboss cache WhirlyCache EhCache Cache4J Check out this interesting comparative table. WhirlyCache ( https://whirlycache.dev.java.net ) I use whirlycache in two projects and works really good. I recommend it to incorporate in a simple lightweight Web application Whirlycache is a fast, configurable in-memory object cache for Java. It can be used, for example, to speed up a website or an application by caching objects that would otherwise have to be created by querying a database or by another expensive procedure. From the testing that we have done, it appears to be faster than any other Java cache that we have been able to inspect. Steps to include WhirlyCache (Step I) Donwload whirlycache and include it in your project -> https://whirlycache.dev.java.net/files/documents/1995/34601/whirlycache-1.0.1.zip (Step II) Create a cache instance CacheConfiguration cc = new CacheConfiguration();\r\n\r\ncc.setName(\"KBCache\");\r\n\r\ncc.setBackend(\"com.whirlycott.cache.impl.ConcurrentHashMapImpl\");\r\n\r\ncc.setTunerSleepTime(60);\r\n\r\ncc.setPolicy(\"com.whirlycott.cache.policy.LFUMaintenancePolicy\");\r\n\r\ncc.setMaxSize(10000);\r\n\r\nCache c = CacheManager.getInstance().createCache(cc); 1 2 3 4 5 6 7 8 9 10 11 12 13 CacheConfiguration cc = new CacheConfiguration ( ) ; cc . setName ( \"KBCache\" ) ; cc . setBackend ( \"com.whirlycott.cache.impl.ConcurrentHashMapImpl\" ) ; cc . setTunerSleepTime ( 60 ) ; cc . setPolicy ( \"com.whirlycott.cache.policy.LFUMaintenancePolicy\" ) ; cc . setMaxSize ( 10000 ) ; Cache c = CacheManager . getInstance ( ) . createCache ( cc ) ; (Step III) Use cache instance c.store(key, object);\r\nc.remove(key);\r\nc.retrieve(key); 1 2 3 c . store ( key , object ) ; c . remove ( key ) ; c . retrieve ( key ) ; Example of java class (is a jboss seam entity class) – StockCache.java", "date": "2009-05-28"},
{"website": "Moove-It", "title": "improving-performance-requests-in-rails", "author": [" Pablo Ifran "], "link": "https://blog.moove-it.com/improving-performance-requests-in-rails/", "abstract": "Reducing the number of request made to the server improves the performance of a web application in about 80% . There are many techniques that allow us to reduce the amount of requests that are made on a page, among them are: the sprites, put the stylesheets on top of the page, javascripts compress, among others. But what’s offered by Rails to improve the performance of our web application? It offers a great plugin called bundle_fu (http://code.google.com/p/bundle-fu/) It allows us with a single request obtain all the javascripts and with another request all the stylesheets  (it also offers the possibility of compress javascripts). Using this plugin is really easy but it’s very powerfull &lt;% bundle do -%&gt;\r\n  &lt;%= javascript_include_tag :default -%&gt;\r\n  &lt;%= javascript_include_tag \"javascript1\" -%&gt;\r\n  &lt;%= javascript_include_tag \"javascript2\" -%&gt;\r\n  &lt;%= javascript_include_tag \"javascript3\" -%&gt;\r\n  &lt;%= stylesheet_link_tag \"style1\" -%&gt;\r\n  &lt;%= stylesheet_link_tag \"style2\" -%&gt;\r\n  &lt;%= stylesheet_link_tag \"style3\" -%&gt;\r\n  ...\r\n&lt;% end %&gt; 1 2 3 4 5 6 7 8 9 10 & lt ; % bundle do - % & gt ; & lt ; %= javascript_include_tag : default - % & gt ; & lt ; %= javascript_include _ tag \"javascript1\" - % & gt ; & lt ; %= javascript_include _ tag \"javascript2\" - % & gt ; & lt ; %= javascript_include _ tag \"javascript3\" - % & gt ; & lt ; %= stylesheet_link _ tag \"style1\" - % & gt ; & lt ; %= stylesheet_link _ tag \"style2\" - % & gt ; & lt ; %= stylesheet_link _ tag \"style3\" - % & gt ; . . . & lt ; % end % & gt ; All these javascripts and stylesheets are converted in only two files when the request is processed.", "date": "2009-04-08"},
{"website": "Moove-It", "title": "designing-a-restful-api-specification-part-i", "author": [" Gian Zas "], "link": "https://blog.moove-it.com/designing-a-restful-api-specification-part-i/", "abstract": "When your business is about pushing and pulling data from an external system, or exposing your application domain to external clients, you often have two options: SOAP Webservices or a RESTful service. If you’re here there are great chances that you are ridding this new hype-age of web2.0 startups as if it was a new religion (just like us). But these preferences are not enough to achieve a successful job done, you must learn new concepts and “stay in the zone”. Ok, so you need to design a RESTful API (and implement it later?), you must be asking “first what the hell is REST?” “Why you write RESTful?” “Or is it a mistake?”. Here we go. If you feel comfortable with the basic concepts please jump to the second part. What is REST? REST (REpresentational State Transfer) is a style or a set of principles for software architecture for distributed systems, nothing else, is not a specification neither a standard and neither an implementation. Actually REST has a niche in applications over HTTP, but technically this is not a barrier. – HTTP is a stateless protocol – It defines a set of operations: POST, GET, PUT, DELETE – Every resource has an universal address, the URI – Hypermedia documents for modeling the information of a resource What is a Resource? A resource is nothing else than the objects that you expose, like an invoice, a paycheck or a blog entry. HTTP Verbs: The POST, GET, PUT and DELETE verbs map with the basic CRUD operations POST -> Create GET -> Read PUT -> Update DELETE -> Delete REST Operations A REST operation consists of: – an URI identifying the resource – a set of additional parameters – a HTTP verb – the response status code – the information retrieved by the operation continues…", "date": "2009-02-27"},
{"website": "Moove-It", "title": "algorithm-generation-soccer-fixture", "author": [" Pablo Ifran "], "link": "https://blog.moove-it.com/algorithm-generation-soccer-fixture/", "abstract": "What’s the problem? We wish to create a fixture in which every team will face every other team one single time. It is also required that each team will only play one single match in each of the defined rounds. It’s a classic tournament or league scenario. Algorithm description The first thing we must do is to see if the amount of teams is odd or even. In case it is odd, an additional team needs to be created which will be used as a “free day”, the team that has a match against it will not play in that round. Once we have all the teams we must arrange them in the following way: First round The first with the last The second with the penultimate And so on until every team is assigned To generate the following rounds, we must take the last element of the previous one (the last team of the first round order) and the last team (team number 6 if there are 6 teams) and we arrange them in the following way: If the round we are generating is even (First one is 1, so it is always odd) we must take the first team of the previous round and then the last team. On the contrary, if it’s even, the last team goes first and then we put the last team of the previous round. Note: last team = 6 if there are 6 teams. Last team of previous round = 3 if the round was 6 vs 5, 1 vs 4 and 2 vs 3 Then we must remove the teams that were first in the previous round, keeping the same order. Once we have this list, we put the eliminated teams at the beginning of it. Beginning from the end, we take two teams from this list and we put them at the front. We do this with every pair until we go through the entire list . We keep the process until every round is generated. Example We have 6 teams, so we will generate 5 rounds with 3 matches each. Teams – 1, 2, 3, 4, 5 y 6 First round 1 vs 6, 2 vs 5 and 3 vs 4 Second round We take the last team of the previous round (team 4) and the last team (team 6), since the round is par we must put first 6 and then 4, the result is 6 vs 4. We remove 6 and 4 from the list of the first round (1, 6, 2, 5, 3 and 4 ) and we sep the resulting order, which results in 1, 2, 5 and 3. We take the last two elements of this list (5 and 3) and we put them after 6 and 4. Finally, we add 1 and 2 to the end of the list: 6 vs 4, 5 vs 3 and 1 vs 2 Third round Applying the same as in the previous round: 2 vs 6, 3 vs 1 and 4 vs 5 Fourth round 6 vs 5, 1 vs 4 and 2 vs 3 Fifth round 3 vs 6, 4 vs 2 and 5 vs 1", "date": "2009-03-27"},
{"website": "Moove-It", "title": "facebooker-rails-site-facebook-connect-1", "author": [" Augusto Guido "], "link": "https://blog.moove-it.com/facebooker-rails-site-facebook-connect-1/", "abstract": "I think that from the title you can pretty much guess what this post will be about, specially if you are familiar with these magic words: Rails, Facebooker, Facebook Connect. Ok they are not that magic, but you can do lots of fun stuff with them. In case you don’t live in the facebook planet I will briefly explain them, since there is a lot of info out there about them I wont get into detail. I won’t explain Rails for obvios reaons. Facebook Connect It’s something (who knows what and who cares anyway?) from Facebook that allows you to use your facebook login to login into other web sites. Facebooker It’s a gem for Ruby, and also a plugin for Rails that converts the results from the facebook API into ruby objects so you can interact with the API using just ruby. Let the magic begin… We want to have a site that handles users like we would normally do, except we won’t handle nor model them. We will get them from facebook. The first thing we want to do is join the developers group in facebook, go to https://www.facebook.com/developers/ and create a new application. You will then have to configure some couple of things, the main one is “Connect URL” in which you should put the url of your site like: “https://my_new_fb_app.com/”. Do not forgett the last slash, it won’t work if it’s not there (and you can spend hours trying to figure out what’s wrong). You have many other setting but we won’t get to them here. As an advice, you may want to create two apps so that you can have one for development and one for production. the development one for example can point to http://localhost:3000/ Other thing you may see is that facebook gives you some strange numbers after you create your app, now is when we start with facebooker . Follow this tutorial until point 5, we won’t care too much for the other stuff, but you can read it. I want you to know how to install facebooker (and to actually do it),  and then generate and complete the facebooker.yml file. Now what? After you have that completed the first thing we wan’t is to show this little pretty blue button . This is the login button, and when you click it a pop up from facebook asking your password and email should appear. So, let’s make that happen. Add this to your application controller before_filter :set_facebook_session\r\nhelper_method :facebook_session 1 2 before_filter : set_facebook_session helper_method : facebook_session Add the followng lines to one of your views (typically a login page) &lt;%= fb_connect_javascript_tag %&gt;\r\n&lt;%= init_fb_connect \"XFBML\"%&gt;\r\n&lt;%= fb_login_and_redirect(facebook_login_users_path) %&gt; 1 2 3 & lt ; %= fb_connect_javascript_tag % & gt ; & lt ; %= init_fb _ connect \"XFBML\" % & gt ; & lt ; %= fb_login_and_redirect ( facebook_login_users_path ) % & gt ; Now you should be seeing the button after you refresh. The facebook_login_users_path is the url you want to redirect your users after they login. As usual in Rails facebook_login is the action and users the controller. After the user logges in you will have a facebook_session variable abailable to do almost whatever you want. For example you can: facebook_session.user , this will return you the facebook user and all of it’s methods. Everything you can do with it it’s here https://facebooker.rubyforge.org/classes/Facebooker/User.html . The typicall things you may want are facebook_session.user.first_name, facebook_session.user.last_name, facebook_session.user.first_name.id (this will return an id facebook provides. You can access with it the user any other time). Other interesting thins you can do is facebook_session.friends to get all the user friends or facebook_session.user.friends_with_this_app will return all the users that use this application and are already your friends in facebook. Well, that’s it for now. In part 2 we will discuss a bit about XFBML (facebook markup language) and show you how to invite friends to your app, show stories in their dashboard, etc. Also a little example of using javascript to call the Facebook API.", "date": "2009-04-06"},
{"website": "Moove-It", "title": "how-to-resolve-mystic-problems", "author": [" Ariel Luduena Karhs "], "link": "https://blog.moove-it.com/how-to-resolve-mystic-problems/", "abstract": "A few days ago, my friend Gian sent me a very interesting link that I’d like to share with you. The post describes a technique to solve complex problems or what I usually call “mystical problems”. This technique is called by the author: “Rubber Duck method of debugging” and seems to work very well. I have tested this with a fellow programmer instead of a Rubber Duck and works! Here you have the technique and the reference link: There is an entire development methodology (whose name escapes me at the moment) that makes use of that very phenomenon. We called it the Rubber Duck method of debugging.  It goes like this: 1) Beg, borrow, steal, buy, fabricate or otherwise obtain a rubber duck (bathtub variety) 2) Place rubber duck on desk and inform it you are just going to go over some code with it, if that’s all right. 3) Explain to the duck what you code is supposed to do, and then go into detail and explain things line by line 4) At some point you will tell the duck what you are doing next and then realize that is not in fact what you are actually doing.  The duck will sit there serenely, happy in the knowledge that it has helped you on your way. Works every time.  Actually, if you don’t have a rubber duck you could at a pinch ask a fellow programmer or engineer to sit in. Andy Reference link 🙂", "date": "2009-03-15"},
{"website": "Moove-It", "title": "facebooker-rails-site-facebook-connect-2", "author": [" Augusto Guido "], "link": "https://blog.moove-it.com/facebooker-rails-site-facebook-connect-2/", "abstract": "Hey, thanks for coming back for part 2. I know it took some time to start writing this second part, it’s just that facebook keeps getting better and keeps taking my time away (don’t tell Conrado). If I remember correctly in Part 1 we ended up with facebooker installed, configured and running. We even added the facebook connect button and explained how to use some of the great facebooker helpers. As promised in my last post we are going to explain a bit how the magic happens with XFBML, invite friends and publish feed items. XFBML Facebook uses XFBML as a way for you to incorporate FBML (Facebook Markup Language, an extension to HTML) into an HTML page on a Facebook Connect site or an iframe application. read more here . This is a typical XFBML tag, it brings up the profile picture of the user with the uid=”12345″. &lt;fb:profile-pic uid=\"12345\" facebook-logo=\"true\" linked=\"false\" width=\"300\" height=\"400\"&gt;&lt;/fb:profile-pic&gt; 1 & lt ; fb : profile - pic uid = \"12345\" facebook - logo = \"true\" linked = \"false\" width = \"300\" height = \"400\" & gt ; & lt ; / fb : profile - pic & gt ; What’s happening here? Facebook is turning this into a typicall HTML <img> tag. They do this using a Javascript cross-domain communications library. You can read more here if you are interested. This is all done for you when using Facebooker. Boring right? Let’s invite some friends to our connect site to keep things more interesting. Inviting Friends If you went through the facebooker helpers you are probably thinking about using fb_multi_friend_selector to select friends. Well you are right! We’ll be using that helper, but we will need it inside another helper that is fb_request_form . There’s also another helper that could help us that is fb_multi_friend_request, which is basically the first two together, but we are going to use the first option. Here’s the resulting code: &lt;% fb_serverfbml do %&gt;\r\n&lt;script type=\"text/fbml\"&gt;\r\n&lt;fb:fbml&gt;\r\n&lt;% content_for(\"invite_user\") do %&gt;\r\n&lt;%= \"Check out my brand new FB Connect site.  Lots of good stuff in there! #{fb_req_choice('Check it out!', login_users_url)}\" %&gt;\r\n&lt;% end %&gt;\r\n&lt;% fb_request_form(\"GetUnbored\",\"invite_user\", login_users_url) do %&gt;\r\n&lt;%= fb_multi_friend_selector(\"Invite your friends to check out this site\", :showborder =&gt; true,\r\n:exclude_ids =&gt; facebook_session.user.friends_with_this_app.map(&amp;:id).join(\",\"), :condensed =&gt; false) %&gt;\r\n&lt;% end %&gt;\r\n&lt;/fb:fbml&gt;\r\n&lt;/script&gt;\r\n&lt;% end %&gt; 1 2 3 4 5 6 7 8 9 10 11 12 13 & lt ; % fb_serverfbml do % & gt ; & lt ; script type = \"text/fbml\" & gt ; & lt ; fb : fbml & gt ; & lt ; % content_for ( \"invite_user\" ) do % & gt ; & lt ; %= \"Check out my brand new FB Connect site.  Lots of good stuff in there! #{fb_req_choice('Check it out!', login_users_url)}\" % & gt ; & lt ; % end % & gt ; & lt ; % fb_request_form ( \"GetUnbored\" , \"invite_user\" , login_users_url ) do % & gt ; & lt ; %= fb_multi_friend_selector ( \"Invite your friends to check out this site\" , : showborder =& gt ; true , : exclude_ids =& gt ; facebook_session . user . friends_with_this_app . map ( & amp ; : id ) . join ( \",\" ) , : condensed =& gt ; false ) % & gt ; & lt ; % end % & gt ; & lt ; / fb : fbml & gt ; & lt ; / script & gt ; & lt ; % end % & gt ; I don’t want to explain something that’s already out there in many places, I will just describe a bit what’s going on. We put things inside a fb_serverfbml , because we are in a facebook connect site and need users interaction with facebook directly (that is when selecting their friends). The content_for(“invite_user”) is the content that will be show in the fb_request_form . The fb_request_form is a facebook form used when we need to submit information to facebook. And the fb_multi_friend_selector is the nice facebook like friend selector. You can choose condensed => true to show an ugly smaller one. The exclude_ids => facebook_session.user.friends_with_this_app.map(&:id).join(“,”) is pretty great, it makes the friend selector not to show the friends who are already using our facebook connect site. Anyway, you can get much more things done, here are some of the facebooker helpers to do anything you like with them ;). And of course the facebook developers wiki is a great place for starting and becoming a guru. I will add in other post the publishing feeds part. Enjoy!", "date": "2009-04-29"},
{"website": "Moove-It", "title": "configuring-a-mail-server-how-to-inbox", "author": [" Ariel Luduena Karhs "], "link": "https://blog.moove-it.com/configuring-a-mail-server-how-to-inbox/", "abstract": "Many people think that configuring a mail server is a trivial task … you think the same? don’t be so sure… I remember my thoughts… “I want to inbox !! why are the mails arriving to the junk folder?? I’m not a spammer” The truth regarding this, is that we do not have a unique technique or way to inbox in the different mail servers (hotmail, yahoo, gmail, aol, etc), because these mail servers have different security protocols (SPF, Domain Keys, Sender ID?…) and each has it’s particular tricks. The first step is to make sure that  all the DNS stuff is configured properly. Some mail servers can send mail from multiple IP addresses at the same time, so, you must determine the IP or IPs that you want to use to send emails.  Once you have identified this IPs, you must view their rDNS configuration (Reverse DNS) .  You can do this executing the following command: host -t PTR 206.189.123.25 After executing the command you will look at somthing like this: 25.123.189.206.in-addr.arpa domain name pointer anyplace.pepetrueno.com . The sentence in bold above must point to the mail server! (Obviously the first time it will point to some place that you don’t know, commonly a temporary hosting domain. Every mail server has a name (machine name) and the rDNS must point to that name. ( xeonmachine02.myhostingprovider.com for example) To configure the rDNS in a correct way, you must submit a ticket to the hosting that manages your server (the IP owner) and ask for this change. Once this change is done, you must wait 12-24-48 hours (propagation time) and test again. The rDNS setting is a very important thing, almost all mail servers have this validation. For example, if this configuration is wrong when you are trying to send mails to hotmail,  your mail is discarded and doesn’t even go to junk folder. How to inbox in hotmail? To inbox in hotmail, you must implement the SPF validation( more details ). The truth is that hotmail, has a security framework called Sender ID that is based in SPF and the configuration is the “same” (Sender ID has other features that you can use). The first step is create a SPF record and with this wizard it is really easy. The SPF record is a string line like this:  v=spf1 ip4:128.121.145.168 mx mx:one.textdrive.com include:cmail1.com include:aspmx.googlemail.com a ~all  ( Twitter SPF configuration) Basically in the record you have to specify wich IP or IPs can send emails using your domain. Once you have your SPF record, you must publish this record in your DNS server. If you use a DNS server provided by Godaddy.com for example (or another hosting/server provider), you will have a tool inside your console that lets you enter your SPF record. Anyway, if you don’t find the tool, submit a ticket to your provider! If you have a DNS server, you must put the SPF record inside the configuration file of your domain. (mydomain.com.db for example).  (If you have a DNS server, I’m sure that you know what I mean.) In this configuration file you must add a line like this:  mydomain.com IN TXT ” v=spf1 ip4:128.121.145.168 mx mx:one.textdrive.com include:cmail1.com include:aspmx.googlemail.com a ~all”… that’s it. To test if your SPF record is published, you can go the wizard that I use above and put your domain name. In the step 2, if you have a SPF record, the wizard will display it to you. Another way to test your SPF record, is use this tool that I found .. it’s really good. How to inbox in yahoo? To inbox in yahoo you must configure Domain Keys . I will write about this configuration (or maybe my friend Gian) in a few days. I hope this helps you with your problems 🙂", "date": "2009-02-16"},
{"website": "Moove-It", "title": "hotmail-sends-my-mails-to-junk-why", "author": [" Ariel Luduena Karhs "], "link": "https://blog.moove-it.com/hotmail-sends-my-mails-to-junk-why/", "abstract": "Ok, first of all i’m a good guy and I’m not a spammer. I’m simply developing a SN and I want to send mails between its members. I have everything nicely configurated (SPF, DomainKeys, reverse DNS for the mail server, all the stuff). Why do my mails go to the junk folder? Hotmail has as “smart” guy called the SmartScreen filter. This guy is responsible for filtering the mails based on different patterns: email content, IP reputation, … if this guy marks you as a possible spammer you’re in trouble. The only way that you have is to talk with the hotmail guys. To do that, submit a form with your mail server information => https://support.msn.com/eform.aspx?productKey=senderid&ct=eformts Hotmail guys will contact you for further information regarding your site and your mail policy. After 4 or 5 mails between you and hotmail support, if it’s all okay, they will  help you with a mitigation to your IP 🙂 “I am pleased to inform you that we have taken steps to implement a temporary mitigation to your mail delivery problem on your IP (206.212.246.210). The mitigation will take 24-48 hours to reflect fully within our system.” Congrats! I also suggest to enroll to the Sender Score program at http://www.senderscorecertified.com (the only white list that Hotmail uses). Good luck!", "date": "2009-04-03"},
{"website": "Moove-It", "title": "rails-iphone-applications-simple-mix", "author": [" Augusto Guido "], "link": "https://blog.moove-it.com/rails-iphone-applications-simple-mix/", "abstract": "As Rails developers we are all in love with keeping things simple. As Einstein said: “Make everything as simple as possible, but not simpler”. You may not agree with the theory of relativity, but you should really agree on this one. A couple of months ago I started my final major project: an iPhone based application. It’s been a really great journey so far and I’ve learned so many amazing things about iPhone development. I found it very similar in some ways with Rails, being the biggest one the way they both keep things as simple as they should be. Well they both use the MVC architecture, but right now I’m talking beyond that stuff. It’s more about an ideology on how to build stuff, web and mobile applications in this case. As an idea here at moove-it we thought of having an iPhone application for our faltauno.com project, kinda like facebook does. So the research began on how this could be done, the first (and probably definitely) answer appeared quickly. The guys at iphoneonrails.com have developed ObjectiveResource. “ObjectiveResource is an Objective-C port of Ruby on Rails’ ActiveResource. It provides a way to serialize objects to and from Rails’ standard RESTful web-services (via XML or JSON) and handles much of the complexity involved with invoking web-services of any language from the iPhone.”. What’s not to love in that sentence? I won’t get into it since I haven’t used that much, and who could explain better than themselves?. In case you are thinking it will be too complex to get started, you can download the whole package with an example application that does all the basic stuff you are probably thinking on trying to do right now. The example is a typical Rails application that can be handled using an iPhone application, which is also inside the example, you should of course have installed XCode. You then start the rails app and the iPhone simulator running the other one, and something kind of magic starts happening. What amaze me the most is the simplicity of the code you’ll need to write (of course 🙂 ). Really try it out it’s worth it. Don’t forget to tell us about your experience!", "date": "2009-10-09"},
{"website": "Moove-It", "title": "moove-it-in-locosxrails", "author": [" Martin Cabrera "], "link": "https://blog.moove-it.com/moove-it-in-locosxrails/", "abstract": "Locos x Rails is the first conference in the Southern Cone dedicated to the ground-breaking Ruby on Rails framework. Locos por Rails Conference 2009 will be held on April 3rd and 4th in Buenos Aires, Argentina. South America’s most popular travel destination is the perfect backdrop for two days of local and international presentations, networking, and fun. Part of the moove-iT development team attended to this event. Bellow you will find some nice pictures! See more photos on facebook group and keep the thread news at locosxrails twitter .", "date": "2009-04-07"},
{"website": "Moove-It", "title": "interfacing-with-twitter", "author": [" Gian Zas "], "link": "https://blog.moove-it.com/interfacing-with-twitter/", "abstract": "Tweet tweet tweet! All the farm is chating about Twitter , all the farm is using twitter and all the farm is building applications on top of it. Luckily, Twitter seems to be a programmer’s best friend, why? you may be asking… because of its wonderful API . Well designed and documented, what else could you wish? The Twitter API is RESTful , but it doesn’t only mean that you can access to it over HTTP , sending parameters in GET or POST requests, it also means that it is designed complying with the principles of REST . A plus to this API is that it supports JSON , XML , RSS and Atom data formats and user authentication is reached by Basic Auth . Due to the popularity of the service, a bunch of libraries have been developed and are heavily used to interact with Twitter, but if you like to explore the other end of the telephone line you are reading the right post ;). As we’ve said the API is accessible through HTTP, so let’s go and fire up your browser and point to: http://twitter.com/statuses/user_timeline.xml?screen_name=twitterapi Ok, what do you see? the last tweets of the user whose screen name is twitterapi formatted in xml. Another popular operation is: http://twitter.com/statuses/friends_timeline.json? That url returns (after successful authentication) the latest tweets posted by the authenticated user and it’s friends. Because requesting a URL and satisfying basic authentication mechanisms are easy tasks in the day-to-day programming languages, invoking the API is a piece of cake. As Python has been one of our favorite languages, we’ll show you how to retrieve user statuses and post a new one. Here we go! <b><span style=\"color: #339966;\">import simplejson as json\r\nimport urllib\r\nimport urllib2</span></b> 1 2 3 < b > < span style = \"color: #339966;\" > import simplejson as json import urllib import urllib2 < / span > < / b > <span style=\"color: #339966;\"><b>GET_STATUS_URL = 'http://twitter.com/statuses/user_timeline.json?'\r\nUPDATE_STATUS_URL = 'http://twitter.com/statuses/update.json?'</b></span> 1 2 < span style = \"color: #339966;\" > < b > GET_STATUS_URL = 'http://twitter.com/statuses/user_timeline.json?' UPDATE_STATUS_URL = 'http://twitter.com/statuses/update.json?' < / b > < / span > First we must declare the necessary imports (simplejson must be downloaded and installed before) and declare the URLs to be accessed. <b><span style=\"color: #339966;\">def get_statuses(screen_name):\r\n    data = {'screen_name' : screen_name}</span><span style=\"color: #339966;\">\r\n    url = GET_STATUS_URL + urllib.urlencode(data)</span><span style=\"color: #339966;\">\r\n    f = urllib2.urlopen(url)\r\n</span><span style=\"color: #339966;\">    response = ''.join(f.readlines())\r\n    return json.loads(response)</span></b> 1 2 3 4 5 6 < b > < span style = \"color: #339966;\" > def get_statuses ( screen_name ) : data = { 'screen_name' : screen_name } < / span > < span style = \"color: #339966;\" > url = GET_STATUS_URL + urllib . urlencode ( data ) < / span > < span style = \"color: #339966;\" > f = urllib2 . urlopen ( url ) < / span > < span style = \"color: #339966;\" > response = '' . join ( f . readlines ( ) ) return json . loads ( response ) < / span > < / b > get_statuses retrieves the last tweets of #{screen_name} , it simply adds the screen_name parameter to the url, then requests it and finally parse the response using the json library. To print the tweets retrieved in a simple manner: <b><span style=\"color: #339966;\">    statuses = get_statuses(username)\r\n    for status in statuses:\r\n        print \"# \" + status['user']['screen_name'] + \": \" + status['text']</span></b> 1 2 3 < b > < span style = \"color: #339966;\" > statuses = get_statuses ( username ) for status in statuses : print \"# \" + status [ 'user' ] [ 'screen_name' ] + \": \" + status [ 'text' ] < / span > < / b > <b></b> 1 < b > < / b > To post a tweet we simply do a POST request to the UPDATE_STATUS_URL passing the status (the tweet’s text) parameter. <b><span style=\"color: #339966;\">def update_status(username, password, status):\r\n    __authenticate(username, password)\r\n    data = {'status' : status}\r\n    f = urllib2.urlopen(UPDATE_STATUS_URL, urllib.urlencode(data))\r\n    response = ''.join(f.readlines())</span></b>\r\n    <span style=\"color: #339966;\"><b>return json.loads(response)</b></span> 1 2 3 4 5 6 < b > < span style = \"color: #339966;\" > def update_status ( username , password , status ) : __authenticate ( username , password ) data = { 'status' : status } f = urllib2 . urlopen ( UPDATE_STATUS_URL , urllib . urlencode ( data ) ) response = '' . join ( f . readlines ( ) ) < / span > < / b > < span style = \"color: #339966;\" > < b > return json . loads ( response ) < / b > < / span > Username and password corresponds to the twitter user (you for example) whose status will be updated, obviously the user (ex: you again) must have a twitter account. Because the API uses basic authentication we call to the __authenticate function: <b><span style=\"color: #339966;\">def __authenticate(username, password):\r\n    passman = urllib2.HTTPPasswordMgrWithDefaultRealm()\r\n    passman.add_password(None, TWITTER_HOST, username, password)\r\n    auth_handler = urllib2.HTTPBasicAuthHandler(passman)\r\n    opener = urllib2.build_opener(auth_handler)\r\n    urllib2.install_opener(opener)\r\n</span></b> 1 2 3 4 5 6 7 < b > < span style = \"color: #339966;\" > def __authenticate ( username , password ) : passman = urllib2 . HTTPPasswordMgrWithDefaultRealm ( ) passman . add_password ( None , TWITTER_HOST , username , password ) auth_handler = urllib2 . HTTPBasicAuthHandler ( passman ) opener = urllib2 . build_opener ( auth_handler ) urllib2 . install_opener ( opener ) < / span > < / b > For a good explanation on how this chunk of code works, please refer to  this great tutorial As you’ve seen, in a small amount of code we are interacting with Twitter, where to use it, how to display the results and what ideas bring to life is up to you!", "date": "2009-03-31"},
{"website": "Moove-It", "title": "moodle-wiziq-bridge-problem-with-gmt13-timezone", "author": [" Martin Cabrera "], "link": "https://blog.moove-it.com/moodle-wiziq-bridge-problem-with-gmt13-timezone/", "abstract": "brief introduction to this tools … Moodle (www.moodle.org): Moodle is a Course Management System (CMS), also known as a Learning Management System (LMS) or a Virtual Learning Environment (VLE). It is a Free web application that educators can use to create effective online learning sites (we recommend moodle for elearning sites … no doubt) Wiziq (www.wiziq.com): WiZiQ is a free alternative to expensive conferencing tools selling as online teaching/learning software (very good tool for virtual class) For this issue try this two solutions: *First of all try with this information from a wiziq support center… WiZiQ Live Class form picks up time zone as you set in your profile on Moodle. Please set your time zone in your Moodle profile. All the following steps need to be done in your Moodle account and not in your WiZiQ account. Sign-in to your Moodle account. Visit your profile that you have filled on Moodle. There you have time zone set as GMT +13. Go to ‘Edit Profile’ tab. In the Timezone field select ‘GMT+1’ and save the information. Now, select WiZiQ Live Class as an activity. You should be able to add WiZiQ Live Classes for GMT which I’m assuming is your actual time zone. **if it does not work, we will have to change the default value of the timezone. To do this … Follow these steps: In the module of wizip “domain/mod/wiziq”, change one line in two files: “mode1.html” and “session.php” find this peace of code: default: { $timezone=”GMT+13″; } change for:(for example my timezone is GMT-3) default: { $timezone=”GMT-3″; } enjoy", "date": "2009-02-27"},
{"website": "Moove-It", "title": "building-a-social-network-on-rails", "author": [" Augusto Guido "], "link": "https://blog.moove-it.com/building-a-social-network-on-rails/", "abstract": "So you want to build your own social network site on rails? As you probably know, there are many plugins around to add social functionality to your site, and there are also some open source platforms that provide you with complete social network functionality. Some of them are Lovd by Less , comunity engine, tog , etc. These platforms, as rails premises, are based in the idea of not reinventing the wheel. In this post we’ll be referring about tog our experience installing it and trying it out. Why are we using tog? Actually we are not using it, we are just doing a proof of concept trying out the different platforms and getting to know each of them. The idea behind this is to have enough knowledge to, according to our clients requirements, select the best solution for them and for us in order to save our clients some money by reducing our development time. What is tog? Tog is a non-intrusive, extensible open source platform that helps you to add social network features to your Ruby on Rails apps. We aim to convert the tedious and long process of creating a community site on a straightforward process that will give your site social capabilities in minutes. (from www.toghq.com/) Installing tog may not be a simple procedure for a Rails beginner, but the install process is very well explained in their wiki. After following the procedure you will have installed a very complete and expandable social network. It is composed by many plugins developed by tog and some by 3rd parties. After installing the basic plugins you can install more plugins that will easily adapt to the default site. Tog is today in version 0.4, which means you will probably find some bugs and things to fix. Here are some we ran into: After running the togify command the following error may appear: “error  There has been a problem trying to unpack the tog_core tarball downloaded from github. Remove the changes made on your app by togify and try again. Sorry for the inconveniences. We found the solution in the project assembla here In the migration create_posts from plugin tog_conversation the line t.datetime :published_at is missing Another thing you shouldn’t forget is to set up your email conf, since some plugins may depend on the user activation like the message ones (I tried to manually activate some users, and then had to manually fix some things than an observer was supposed to do after activation) The greatest thing about tog is the ease of extending it. You can read about it here . Basically you can override any behavior you want just by creating models, controllers or views like you would normally do in your apps. On the cons side, it may be a bit complex finding out what functionality is where, to override with your own. The idea is to not touch the plugins and use new controllers, models, etc to extend. This will allow plugins to be updated when they need to, without affecting your site’s working functionality. In conclusion, I think tog is a very promising platform that may be a great solution for building social networks on the go, allowing us to extend them to meet our needs. This may be a good solution for a user who just wants his site running. If you want to customize too much about it, you should consider other platform or starting from scratch. We are right now working on adding Facebook Connect to a demo site we are building with tog, please come back to know how that went 😉", "date": "2009-02-09"},
{"website": "Moove-It", "title": "improving-ruby-array-class", "author": [" Pablo Ifran "], "link": "https://blog.moove-it.com/improving-ruby-array-class/", "abstract": "Improving the ruby Array class Many times, when you are iterating over an array you want to know the last or the first element to do something. Unfortunately ruby don’t provide any method to help you doing this task. So, if you want a trick to do this, you can add this few lines in a rails project inside an initalizer. Put this code in initializers/array.rb class Array\r\n\r\n  #     array.each_with_first {|item, first| block }   -&gt;   array\r\n  #\r\n  #\r\n  # Call a <em>block</em> ones per element, passing\r\n  # the element and the first element of the array as parameters.\r\n  #\r\n  #    a = [ \"a\", \"b\", \"c\" ]\r\n  #    a.each_with_first {|x, f| print \"#{x} -- #{f} | \" }\r\n  #\r\n  # produces:\r\n  #\r\n  #    a -- a | b -- a | c -- a |\r\n  #\r\n  def each_with_first(&amp;block)\r\n    first = self.first\r\n\r\n    self.each do |element|\r\n      yield element, first\r\n    end\r\n  end\r\n\r\n  #     array.each_with_last {|item, last| block }   -&gt;   array\r\n  #\r\n  #\r\n  # Call a <em>block</em> ones per element, passing\r\n  # the element and the last element of the array as parameters.\r\n  #\r\n  #    a = [ \"a\", \"b\", \"c\" ]\r\n  #    a.each_with_last {|x, f| print \"#{x} -- #{f} | \" }\r\n  #\r\n  # produces:\r\n  #\r\n  #    a -- c | b -- c | c -- c |\r\n  #\r\n  def each_with_last(&amp;block)\r\n    last = self.last\r\n\r\n    self.each do |element|\r\n      yield element, last\r\n    end\r\n  end\r\n\r\n  #     array.each_with_first_and_last {|item, first, last| block }   -&gt;   array\r\n  #\r\n  #\r\n  # Call a <em>block</em> ones per element, passing\r\n  # the element, the first and the last element of the array as parameters.\r\n  #\r\n  #    a = [ \"a\", \"b\", \"c\" ]\r\n  #    a.each_with_last {|x, f, l| print \"#{x} -- #{f} -- #{l} | \" }\r\n  #\r\n  # produces:\r\n  #\r\n  #    a -- a -- c | b -- a -- c | c -- a -- c |\r\n  #\r\n  def each_with_first_and_last(&amp;block)\r\n    first = self.first\r\n    last = self.last\r\n\r\n    self.each do |element|\r\n      yield element, first, last\r\n    end\r\n  end\r\n\r\nend 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 class Array #     array.each_with_first {|item, first| block }   -&gt;   array # # # Call a <em>block</em> ones per element, passing # the element and the first element of the array as parameters. # #    a = [ \"a\", \"b\", \"c\" ] #    a.each_with_first {|x, f| print \"#{x} -- #{f} | \" } # # produces: # #    a -- a | b -- a | c -- a | # def each_with_first ( & amp ; block ) first = self . first self . each do | element | yield element , first end end #     array.each_with_last {|item, last| block }   -&gt;   array # # # Call a <em>block</em> ones per element, passing # the element and the last element of the array as parameters. # #    a = [ \"a\", \"b\", \"c\" ] #    a.each_with_last {|x, f| print \"#{x} -- #{f} | \" } # # produces: # #    a -- c | b -- c | c -- c | # def each_with_last ( & amp ; block ) last = self . last self . each do | element | yield element , last end end #     array.each_with_first_and_last {|item, first, last| block }   -&gt;   array # # # Call a <em>block</em> ones per element, passing # the element, the first and the last element of the array as parameters. # #    a = [ \"a\", \"b\", \"c\" ] #    a.each_with_last {|x, f, l| print \"#{x} -- #{f} -- #{l} | \" } # # produces: # #    a -- a -- c | b -- a -- c | c -- a -- c | # def each_with_first_and_last ( & amp ; block ) first = self . first last = self . last self . each do | element | yield element , first , last end end end", "date": "2009-04-06"}
]