[
{"website": "Single-Store", "title": "backup-with-split-partitions-robust-partition-split-via-backup", "author": ["Amy Qiu"], "link": "https://www.singlestore.com/blog/backup-with-split-partitions-robust-partition-split-via-backup/", "abstract": "You run a database. You try to anticipate the future. You try to provide sufficient, even generous, resources to accommodate future growth in your service. But sometimes, growth far exceeds even your most optimistic expectations, and now you have to figure out how to accommodate that. One obvious way of increasing your database’s ability to handle and service more data is to simply add more machines to your cluster. That should increase the number of cores, and the amounts of memory and disk space available in your cluster. But with a limited number of partitions, at some point, adding the number of cores and memory and disk won’t be able to increase the  parallelism and breadth of your database. Since partitions cannot span leaves, you cannot share data storage and processing between N leaves unless you have at least N partitions. Additionally, parallel query performance will be best when we have one or two cores per partition. Therefore, it would seem that being able to increase the number of partitions amongst which the data is spread out is critical for the cluster’s scalability. Until now, however, there has not been an easy and convenient way to do this in SingleStore. Introducing Backup with Split Partitions In order to split the number of partitions in your database, we’ve come up with a clever strategy that adds partition splitting to the work of a backup operation. Normally, a full backup will create one backup for each partition database, with a .backup file for the snapshots of the rowstore portion of data, and a .backup _ columns _ num _ tar file, or files, for the columnstore blobs of a partition. There is also rowstore backup for the reference database and a BACKUP_COMPLETE file for a completed backup. Figure 1. Normal backup – no split partitions Figure 1. Normal backup – no split partitions But for splitting partitions, each partition will actually generate backup files for two partitions – with each of these split partitions having the data required to create half of the original partition upon a restore. A multi-step hash function determines which of the new split partitions any row in the original partition should belong to, and this function splits all rows between the new split partitions in the backup. Figure 2. Backup with split partitions Figure 2. Backup with split partitions We chose to piggyback on backup, because it was already a very sturdy and reliable operation that succeeds or fails very transparently and because backup already makes sure that we have a consistent image of the whole distributed database. When we restore our split backup, the split partitions in the backup will restore as split partitions in the cluster. Figure 3. Restore of backup with split partitions Figure 3. Restore of backup with split partitions As you can see, splitting partitions is actually a three-stage process consisting of taking a split backup, dropping the database, and restoring the split backup. Now let’s now see this in practice. Example Workflow Pre-Split-Partitions Work (Optional) Add Leaves If you’re expanding your database’s resources by adding new leaves, you will want to add the leaves first before taking a split backup. This could save you an extra step of rebalance partitions at the end of restore, since for all backups except for local filesystem backups, restore will start with creating partitions in a balanced way to begin with. An extra bonus is that recovery will be faster if recovery is spread out among more leaves and resources. The command would look something like this: ADD LEAF user[:'password']@'host'[:port] [INTO GROUP {1|2}] The reason I recommend doing it before backup is because although you can add leaves anytime, if you were going to pause your workload for backup split, you would want to avoid any action like add leaves that would prolong your workload downtime. If you do end up adding leaves after restoring the split backup, or you did a backup with split partitions to a local filesystem location on each leaf, you will need to remember to run REBALANCE PARTITIONS after the restore completes. (Optional, But Recommended) Pause Workload It is true for all backups that backups only contain writes up to the point of that backup. If you restore an old backup, you will not have new rows added after the old backup was taken. This holds true for our split backups as well. If you do not wish to lose any writes written after the split backup was taken, you should make sure that writes are blocked when the backup starts until the database is restored. Split Partitions Work (Step 1) Backup with split partitions command: BACKUP [DATABASE] db_name WITH SPLIT PARTITIONS \nTO [S3|AZURE] ‘backup_location’ [S3 or Azure configs and credentials] Rowstore data will be split between the split backups, so the total size of rowstore split backups should be roughly the same as rowstore split backups. In your backup location, these will have the .backup postfix. Columnstore blobs of a partition will be duplicated between the split backups of that partition, and so columnstore blobs will take twice the disk space in a backup with split partitions. I recommend leaving enough space for two full backups in order to attempt one backup with split partitions. Once the split backups are restored and columnstore merger has run, the blobs will no longer be duplicated (more details in the optimize table portion below). (Step 2) Normal drop database command: DROP DATABASE db_name (Step 3) Normal restore database command: RESTORE [DATABASE] db_name \nFROM [S3|AZURE] \"backup_location\" [S3 or Azure configs and credentials] Post-Split-Partitions Work (Optional, but Recommended) Unpause Workload If you paused your write workload earlier, your workload can be unpaused as soon as restore runs. (Recommended) Explain Rebalance Partitions If you backed up to NFS, S3 or Azure, and you did not add any leaves after you restored, there should be no rebalance work suggested by the EXPLAIN REBALANCE PARTITIONS command. If you backed up to the local filesystem, which creates individual backups on every preexisting leaf and you have more leaves now, EXPLAIN REBALANCE PARTITIONS will most likely suggest a rebalance, which you should execute to optimize future query performance. (Optional, but Sometimes Recommended) Optimize Table After the split database backups are restored, the background columnstore merger will automatically be triggered. So although backup with split partitions duplicated the columnstore blobs, the columnstore merger will eventually compact (and likely merge) the columnstore blobs so that they use less space on disk. The columnstore merger will eventually compact (and likely merge) the columnstore blobs so that they use less space on disk. After the merger is done, the database should get back to using roughly the same amount of disk space for the same amount of data as before the backup split, and all future normal backups will take a normal amount of space. Expect to see more CPU usage from this background process while it is reducing the disk usage of the blobs. During our testing, we found that overly sparse columnstore segments incurred a temporary performance hit for queries like select and range queries before the columnstore merger or optimize table finished running. A performance-sensitive user may want to manually run optimize table on all the tables of the newly split database to explicitly trigger columnstore merger rather than waiting around for the background columnstore merger to eventually get to tidying up a particular table.", "date": "2020-09-03"},
{"website": "Single-Store", "title": "what-is-the-database-of-now", "author": ["Domenic Ravita"], "link": "https://www.singlestore.com/blog/what-is-the-database-of-now/", "abstract": "The Database of Now delivers operational insights and advantages by providing the current state of the business. It is a modern, efficient approach to cloud data management which broadens, accelerates, and simplifies access to all the relevant in-the-moment and historical data while unifying data access styles and patterns. Digital transformation projects have accelerated to meet the increased demand for digital products and services providing answers and solutions with immediacy. With the onset of the ‘always-on’ culture, the pervasive use of smartphones and ubiquitous devices has driven a global shift in customer experience and consumer expectations. Business is now won or lost in a moment. To put this in perspective, in the world of institutional and high-frequency derivatives and commodities trading, every nanosecond of added latency costs financial traders millions of dollars as losses to competing traders. Exchanges such as the Investors Exchange (IEX) and NASDAQ reward maximum order execution performance. For web experience, Akamai found in 2017 that added latency of just 100ms in website load time drops conversion rates by 7%. Seconds can save lives too. Thorn’s mission is to save children from sex trafficking and they do that by continuously processing massive amounts of web data to identify children and decrease law enforcement investigation time by as much as 63%. And True Digital seeks to proactively reduce the likelihood of new viral hotspots by monitoring mass population movement trends and the rates of population density changes through anonymized cellphone location data. Each of these scenarios define a make-or-break moment in time. “Now scenarios” are time-critical, but the length of time available for effective action varies by situation, as does the variety, volume, and velocity of data required. But, what is essential for “Now scenarios” is to leverage all the relevant data to establish the most accurate, complete, and timely context to drive proactive responses. For business operations, with each passing moment, real revenue is lost or gained in a split second. For customer experience, latency adversely affects the interactivity and responsiveness customers expect. For law enforcement and public health, lives are at stake. The Database of Now delivers the operational insights and advantages by providing the current state of the business so that business can proactively identify, capture and capitalize on the most crucial moments for their own endeavors and their customers’ success. It achieves this by simplifying the data infrastructure required to execute diverse workloads across various data access styles, patterns and types. Data professionals, application stakeholders and end users gain the advantages of speed, scale and simplicity.", "date": "2020-09-10"},
{"website": "Single-Store", "title": "infosys-memsql-working-together", "author": ["Domenic Ravita and Amit Kumar Srivastava"], "link": "https://www.singlestore.com/blog/infosys-memsql-working-together/", "abstract": "Infosys and SingleStore are now working together, identifying opportunities to implement SingleStore – a fast, scalable, relational database, renowned for fast analytics, streaming support, and solutions in areas such as financial services, digital media, telecom, and IoT – with the speed and effectiveness for which Infosys is rightly renowned, worldwide. Infosys is a global leader in next-generation digital services and consulting, with clients in 46 countries. With over three decades of experience in managing the systems and workings of global enterprises, Infosys expertly steers clients through their digital journey. Key to their work is enabling the enterprise with an AI-powered core which helps prioritize the execution of change at each company. They seek to enable an agile digital strategy across the business. They then deliver a learning agenda, driving continuous improvement through building and transferring digital skills, expertise, and ideas from the Infosys innovation ecosystem. SingleStore is an up-and-coming database provider gaining increasing recognition for fast, scalable relational database offerings. The SingleStore database delivers lightning-fast analytics and compatibility with ANSI SQL, standing out from the NoSQL crowd . SingleStore is also gaining attention with customers that include many of the Fortune 500, including half of the Top 10 North American banks. SingleStore and Infosys have now partnered to help clients across verticals. The key to the partnership is the ability of SingleStore to offer uniquely powerful solutions, with outstanding price-performance, to a variety of database-related issues, especially in analytics. Infosys contributes a gimlet eye for opportunities which can best be realized by the use of SingleStore. Infosys can also lead the systems integration work needed to quickly stand up SingleStore, and put it to work, within a complex operating environment. The solutions jointly offered by the SingleStore and Infosys blur the boundaries of two categories which are commonly used to describe technology offerings: painkillers and vitamins. A painkiller solves problems – reports that take days to run, and dashboards that take hours to update. Slow-running ecommerce sites that inadvertently send customers to the competition. Applications that refuse to scale, requiring doublings and redoublings of back-end investment for meager performance gains. Vitamins help companies take advantage of opportunities – the new report, new dashboard, new ecommerce site, or new applications that helps a large company step ahead of smaller competitors, or helps a government agency offer services faster, more effectively, and at less cost. And, when technology is used in just the right way, it can do both at once: fix things that are broken, and open up competitive advantage. SingleStore provides raw power for such solutions; Infosys offers finesse in identifying opportunities and implementing solutions, helping to fix problems the right way, the first time, maximizing opportunities, and minimizing time to market. With the description given here, you’ll be able to identify opportunities in which Infosys and SingleStore may be able to help in your organization. What Infosys Offers Infosys is a global organization, nearly 40 years old, with nearly a quarter of a million employees and with a market capitalization, at this writing, of more than $40 billion. Infosys offers digital services and consulting, with Infosys Consulting operating as a business within the overall Infosys organization. Slightly more than half their business is in the US, with the rest spread around the globe. Infosys partners closely with its clients, and offers a broad range of services that adapt to their needs through its vast suites of services, with platforms spread across different industry verticals. Infosys acts as a trusted advisor to its clients. Part of their value proposition is their ability to identify valuable emerging technologies, use these technologies in a few situations for which they may be particularly well suited, then share their findings across the Infosys organization worldwide. This creates a portfolio of proven technologies that all Infosys clients can adopt with confidence. What SingleStore Offers SingleStore offers a fast, scalable, relational database. SingleStore combines transaction and analytics processing in a single database, like other databases in the emerging NewSQL category, also referred to as hybrid transactional and analytical processing, or HTAP – a term coined by Gartner nearly a decade ago, shortly after SingleStore was founded. SingleStore is a private, venture capital-funded company based in San Francisco. SingleStore has grown to more than 200 employees, including a large proportion of engineers who continue to develop the platform. At its current stage of development, SingleStore is especially well suited for powering fast analytics that blend historical and real-time data, with fast ingest, high concurrency, and with breakthrough responsiveness to SQL queries, whether simple or complex. This field is known as operational analytics, and is used heavily in financial services, digital media, telecom, IoT, and other areas. SingleStore usage is sometimes held as something of a secret among its customers, who gain tremendous competitive advantage in applications including credit card fraud detection, predictive maintenance, ad marketplace sales, and other applications. With an increasing range of business partnerships, however, and with the Infosys partnership as the most prominent, the secret is increasingly out. How Infosys and SingleStore Work Together Infosys and SingleStore work together with both new and existing customers. Infosys looks for opportunities where the SingleStore database adds unique value, due to its speed, scalability, SQL compatibility, and other features. SingleStore looks for opportunities where advising is needed at a broader level than whether the SingleStore database is the right fit for a specific use case – where an organization is looking at significant change within IT, such as moving a large proportion of their application portfolio from on-premises servers to the cloud. Infosys sees SingleStore as a key enabler, and well-suited for use cases such as: Real-time business intelligence Offloading OLTP databases Performing operational analytics Building real-time apps for highly-concurrent read access (such as for dashboards and internal enterprise apps) Performing real-time analytics, such as anomaly detection to spot fraud and customer segmentation Here are a few examples of recent implementations of SingleStore in Infosys-led projects: A large consumer electronics company is moving internal data warehousing workloads from Vertica to SingleStore. Infosys is working closely with SingleStore to set up and manage SingleStore infrastructure to support the former Vertica workload, migrate applications out of Vertica, and point dashboards to run on SingleStore. A global music company is using SingleStore to power real-time business intelligence (BI) solutions. Tableau is being used as a visualization layer for data extract from a range of sources and displayed in dashboard. Formerly, the dashboard was having inconsistency and latency issues. Infosys introduced SingleStore as an intermediate, low-latency layer, moving data sets from Hadoop to SingleStore. A leading bank is offloading a transaction processing workload from the leading “old SQL” database to SingleStore. Offloading the transactional database to SingleStore resulted in cost savings and better customer experience. Learn More This blog post may have left you curious about using the SingleStore database; about working with Infosys; or about working with Infosys and SingleStore as partners. You can contact Infosys askus@infosys.com ; contact SingleStore ; or try SingleStore for free .", "date": "2020-09-29"},
{"website": "Single-Store", "title": "singlestore-for-fastboards", "author": ["Sarung Tripathi"], "link": "https://www.singlestore.com/blog/singlestore-for-fastboards/", "abstract": "Introduction Over the last many years, analytical dashboards have proliferated enterprises from the boardroom to the manufacturing line. As businesses have become increasingly more reliant on analytics, end users have started to demand snappier performance from their dashboards. Gone are the days that visualizations are used simply for historical data. Data professionals are now using them for predictive and prescriptive analytics on petabytes of information, expecting the most real-time insights to tell their data stories. Many organizations across consumer products, healthcare, fintech and other verticals have been met with the challenge of slow moving BI applications in recent years, due to the growth of data and many new types of users. Backend platforms are often retooled for each use case and still are unable to handle the growing concurrency demands of the business. This leads to applications that take too long to render and users that are faced with the dreaded “loading” pinwheel. The Pain The challenge of slow-running dashboards is not limited to legacy systems. Users experience slow dashboards when their applications are backed by legacy architectures as well as by more modern database systems. The pain points that enterprises face with legacy architectures start with the original intent of those systems: to support historical reporting and batch-oriented dashboards. Many databases from vendors like Oracle were built around a different era of business demands — when operational decisions were not made through data visualization. Over time, other (now legacy) systems like SQL Server emerged to help with transactional processing of data. This spread of new database systems purpose-built by workload introduced put even more stress on ETL technologies. The Hadoop era aimed to solve this, but ultimately consumers were left at the mercy of too many different systems for data to reside in and excessive data duplication across those silos. This made data storytelling very difficult. The Challenge of Legacy Architectures The struggle with legacy data systems has been even further exposed with the emergence of streaming data. Data is now being unlocked from chatty systems like e-commerce systems, financial trades, and oil and gas machinery. Real-time data pipelines from messaging infrastructures like Kafka, Kinesis, Pulsar and others have put an even greater burden on old, slow databases. Users expect the data from these applications to be readily available in real-time and operational dashboards, often combined with historical reference data. Instead, they end up stuck with dashboards struggling to load data for critical business decisioning. Introducing Streaming Data The challenges that legacy data platforms faced in the face of the growing base of analytics users has been met with the advent of modern data platforms. These platforms tout a new focus toward analytics, AI and machine learning. While modern data platforms still tend to specialize in OLTP or OLAP workloads, data platform vendors are adding even further delineation on the lines of data type — MongoDB specializes in document data, Redis for key-value data, etc. This approach works very well for application data that can often be unstructured and transactional. However, tying together multiple single-purpose databases for a unified visual experience remains extremely slow, and thus data stories end up very disjointed. Cloud data warehouses have started to solve this problem, but scaling to satisfy an enterprise-scale analytics audience and handling operational data with platforms like Snowflake has proven to be extremely costly. This concern is primarily rooted in unpredictable compute costs as more and more users start accessing these dashboards. As more users come onto the platform, response times also become far less consistent. Ultimately, consumers end up losing the low latency response they were promised and organizations still end up spending more than expected. Finally, organizations also try to solve their dashboard needs with a data federation approach. Federation vendors often tout the ability to have a single point of data access to all data sources. Unfortunately, this typically gets customers in more trouble due to 1) the high costs of procuring this technology and hosting it on very large servers, 2) a single point of failure, and 3) a newly introduced bottleneck which slows down dashboards even more. Introducing Modern Data Platforms, and their challenges The pains faced by dashboard users and the engineers behind them can very often be linked back to the monolithic legacy systems that they are architected upon, or often the overindulgence in new, single-purpose data platform technology. Accelerating those dashboards, and tuning your systems to be ready for fastboards requires a scalable, general purpose database built for fast data ingestion and limitless amounts of users. SingleStore Managed Service is built for fastboards. SingleStore Managed Service SingleStore ’s database-as-a-service offering is called SingleStore Managed Service. Our platform is a distributed, highly-scalable, cloud-native SQL database built for fastboards. SingleStore is designed for highly performant ingest of any operational data, whether it is batch or streaming in nature. We make ingesting your data incredibly easy through SingleStore Pipelines, your way to get data from anywhere with five lines of SQL. Data can be brought from many different source systems like S3, Kafka, Azure Blob, etc. and streamed in parallel to make your data instantaneously queryable upon ingest. SingleStore offers a singular table type for all of your data storytelling needs. SingleStore’s architecture allows you to support large-scale Online Transaction Processing (OLTP) and Hybrid Transactional and Analytical Processing (HTAP) at a lower total cost of ownership. It is a continuing evolution of the columnstore , supporting transactional workloads that would have traditionally used the rowstore . With the ability to seek millions of rows at a time, scan billions, and compress your data 10X — SingleStore is the ultimate solution for fastboards. Most importantly, SingleStore Managed Service is a converged data platform that can store data of any type, JSON, time-series, key-value, etc — all with SQL. Here at SingleStore, we believe many modern data platforms can coexist. Many of our customers leverage in-memory, NoSQL technologies for their mobile applications and relational, cloud EDWs for long-term storage of their data. However, when it comes to accelerating the most critical business insights driven by analytics, AI and machine learning, they turn to SingleStore. Our Managed Service can help you extend your legacy platforms like Oracle and Teradata with fast ingest and querying, complement Snowflake and BigQuery without unpredictable costs, and onboard any new type of data without the constraints of NoSQL databases. SingleStore Managed Service: The Data Platform for Fastboards What kinds of dashboards? As discussed, SingleStore is a fantastic choice for your most important visualization and analytics needs. The following section goes a bit deeper into exactly what types of dashboards SingleStore is powering today, and some of the important concepts. SingleStore is the core data platform for many analytical applications: Real-Time Analytics Operational BI Historical Reporting ML-Driven Analytics Ad-Hoc Data Discovery, and many more Real-time dashboards are used to make critical business decisions in the moment. They often require sub-minute or sub-second latency, and are highly relevant in preventative maintenance, inventory and IoT applications. These types of workloads benefit greatly from SingleStore’s streaming ingestion, built-in predictive capabilities, and scalable analytics. SingleStore’s ability to quickly ingest high volumes of data, rapidly run predictive models for scoring, and store for fast retrieval makes it best in class for real-time dashboards. Medaxion leverages SingleStore to achieve hospital event to insight within 30 seconds. Historical Reporting dashboards encompass both the most recent data as well as long-term insights. They are often found supporting financial reporting and historical sales analytics use cases. SingleStore offers a number of different features that accelerate historical dashboards — drop-in SQL compatibility, scalable analytics functions, and built-in machine learning to name a few. SingleStore makes it extremely simple for dashboard developers to deliver high-quality, fast historical analytics for end consumers. Kellogg uses the SingleStore platform to accelerate the speed of their Tableau dashboards by 80X. Over time, we have seen machine learning and AI emerge to the forefront of analytics ecosystems and thus, visualizing ML through visualization has become a common way to share the performance of models. These dashboards are meant to provide a beautiful visual interface for otherwise complex data science workflows. They are often used to help executives and business people align with an organization’s predictive capabilities. At SingleStore, we see many of our customers leveraging our ML-enabled pipelines to score and visualize data in real-time. Users also leverage built-in predictive functions and our SingleStore capabilities to perform training and testing at large scale. Tools Having discussed many different variations of fastboard use cases, it is also important to address the vast landscape of tools and technologies that enable these. Furthermore, there is no feature more important for a database than seamless, performant connectivity to every dashboarding tool. SingleStore is wire protocol compatible with MySQL making it instantly accessible from any BI tool such as Tableau, PowerBI or Looker and with widely-available bindings to popular programming languages such as R and Python. We also have native connectors for many dashboarding tools, purpose-built for accelerating the speed of your dashboards. Many of our customers have also found success custom building their dashboards using frameworks such as React, in architectures where SingleStore acts as a performant API-backend. Summary Here at SingleStore, we understand that fastboards may come in all shapes and sizes, and that each use case is unique. As discussed above, our ability to approach more fastboard use cases than any other database is rooted in our native SQL compatibility, SingleStore engine, streaming ingest and vast predictive capabilities. SingleStore Managed Service can empower modern data platforms with fastboards in order to achieve faster, more informed decisions, and improved customer experiences. We invite you to explore how some of our customers are powering their fastboards to tell rich data stories here .", "date": "2020-11-02"},
{"website": "Single-Store", "title": "nucleus-security-singlestore-partner-to-manage-vulnerabilities-at-scale", "author": ["Domenic Ravita"], "link": "https://www.singlestore.com/blog/nucleus-security-singlestore-partner-to-manage-vulnerabilities-at-scale/", "abstract": "Cybercrime damages in 2021 are expected to reach $6 trillion , with organizations of all sizes and industries exploring ways to protect themselves better. Potential security vulnerabilities come in many forms, from cross site scripting to improper privilege management. Over 17,000 new security vulnerabilities were discovered in 2020 alone. IT security teams are responsible for securing infrastructure that’s continually increasing in complexity, so it’s challenging to respond quickly to newly discovered exploits or confirm that a known vulnerability is addressed. It doesn’t take long for an organization to end up with a substantial vulnerability backlog. On average, over six months, organizations end up with a backlog exceeding 57,000 and fail to mitigate 28 percent of these vulnerabilities . A pre-emptive, holistic, and scalable vulnerability management approach is needed to keep systems safe. Addressing a Critical Vulnerability Management Process Gap The founders of Nucleus Security , Stephen Carter, CEO, and Scott Kuffer, COO, had decades of experience providing vulnerability management services to United States government agencies. They used a variety of vulnerability scanning tools for this purpose. Whenever they found a vulnerability, they had to manually enter a remediation request into a ticketing system, such as Jira or ServiceNow. The gap between the identification of vulnerabilities and the creation of tickets created an efficiency bottleneck. All of the vulnerability reports needed to be normalized across a range of formats used by different tools and prioritized by the user. The response then needs to be automated for greater speed, manageability, and scalability. Carter and Kuffer created the Nucleus vulnerability management platform to make it faster and easier to provide services to their clients. Their original intent was to have a custom tool for their own operation, but they quickly discovered a major need for this type of platform. The users most interested in Nucleus Security were: Large enterprises and government agencies needing to manage security vulnerabilities at scale. Managed Service Security Providers (MSSPs) looking for a platform that supports their vulnerability management processes with multiple clients, offers client access, and provides a way to expand it with their proprietary functionality. Handling Unified Vulnerability Management at Scale Since Nucleus was created by and for vulnerability management experts, the platform solved many problems that stood in the way of discovering and remediating vulnerabilities before they became exploits. Smart automation and workflow optimization eliminated many tedious and time-consuming parts of vulnerability management through: Centralizing vulnerability aggregation, normalization, and prioritization in a single platform. Delivering the right information to the right people in the right format by connecting vulnerability, ticketing, alerting, and issue tracking tools together. Increasing vulnerability management speed and capacity from end-to-end. Reducing costs for vulnerability management. Improving the visibility of vulnerability management data and remediation efforts. How Nucleus Works The Nucleus application was delivered as a software-as-a-service (SaaS) offering and was designed with a traditional three-tier architecture. “There’s a web application, which is customer-facing, and a job queue that processes data. Then there’s a database on the backend, serving both,” says Kuffer. Users log in to the customer-facing Nucleus web application. They set up integrations with the tools they’re using from a list of more than 70 supported applications . The users create rules to ingest data and trigger alerts, reports, and tickets. They can access a centralized vulnerability management dashboard, perform searches, and analyze their scan results. A separate backend job queue ingests and processes the data on the user-specific schedule from the selected data sources’ APIs. A database powers the frontend and backend operations. The product licensing is based on the number of assets that an organization is working with. In Nucleus, an asset is defined as an item that the user scans. Each asset can have multiple vulnerabilities. The Challenges of Pivoting to a Commercially Viable Vulnerability Management Platform The decision to launch a separate company, Nucleus Security, had not originally been the founders’ plan. Kuffer explains, “We actually just went to a conference called Hacker Halted in Atlanta, Georgia, and we had Nucleus as basically a line item in a data sheet for our old company. We just got basically hammered with leads at that point.” “We were not prepared at all for any leads whatsoever, and so none of us had any sales experience, we didn’t have a company, we had nothing. We didn’t really have much of anything, other than a product that didn’t scale for all of these needs.” Nucleus had many leads coming in that were bigger names and companies, which helped to expedite the decision to fork it off into a separate business at the end of 2018. However, the founders needed a way to scale the platform for these Fortune 500 companies, and they needed it fast. Finding the Right Database Solution When Nucleus Security began architecting the Nucleus platform, they explored several database options, including document-oriented databases, graph databases, and traditional relational databases. Carter says, “All of the options had their strengths and weaknesses, and we felt that several database technologies could work well for the initial proof of concept. However, it quickly became apparent that a high-performance relational database system was a hard requirement to support our data model and the features we were planning to bring to market.” They started out developing the solution they would have needed when they were working directly with government agencies. These clients are highly focused on compliance and run scans weekly or monthly, based on the regulatory requirements. The database solutions that Nucleus tried often took a long time to process vulnerabilities, but it wasn’t a big issue at a weekly or monthly cadence. Struggles with MariaDB The Nucleus prototype used MariaDB, which is on many federal government-approved software lists. “MariaDB comes bundled with the government-approved operating system we were using, which is Red Hat Linux,” explains Carter. “For the prototype, this worked just fine. But when we started to onboard larger customers, we hit some ceilings performance-wise, and some pretty major performance issues.” “As a workaround, we were trying to precalculate a bunch of stuff, to show it in the UI. But if you’re scanning every hour, and it takes an hour and a half to do the calculations, then you get a huge backlog,” says Carter. The team spent a lot of time tuning queries to squeeze out every bit of performance that they could to keep up with the capacity needed for their beta customers. However, even with the clustering and load-balanced configurations available, it was clear that a different database solution would be needed for Nucleus to support very large enterprises, which have hundreds of thousands of devices and tens of millions of vulnerabilities. Commercial clients scan for vulnerabilities multiple times daily or weekly. They wanted to see the results of a scan in minutes, not hours or longer. Over time, the backlog built up and hit a ceiling where the Nucleus application couldn’t process jobs fast enough. Batch processing worked with federal agencies, but enterprises demand real-time vulnerability management. “It was the database that was the bottleneck all along,” says Kuffer. “We looked at some NoSQL solutions. We looked at Percona for clustering, but we would have had to rewrite a lot of our code – and all of our queries.” Nucleus Security also investigated other SQL solutions based on PostgreSQL core, such as Greenplum. The relational database for a commercially viable version of Nucleus needed: Real-time processing Support for 10s of millions of vulnerabilities and 100s of thousands of devices High scalability The Benefits of SingleStore DB for Vulnerability Management Platforms Nucleus Security started looking into alternatives to MariaDB that were better suited for its vulnerability management platform. They found Percona first, but the initial tests indicated that it wouldn’t help with their use case. The scaling focused more on being a high-availability cluster. While they could add to the cluster and use load balancing schemes with different nodes, it was an extremely manual process. It also required a minimum of three servers. SingleStore came up during Nucleus Security’s search for a database and impressed them from the start. “SingleStore was a great option, because SingleStore is not only relational; it also supports the MySQL wire protocol, which of course is inherent in MariaDB,” explains Carter. “It was almost a drop-in replacement for MariaDB, and it’s way less complex, and also much easier to maintain than the Percona cluster solution that we were looking at.” SingleStore is The Database of Now™ powering modern applications and analytical systems with a cloud-native, scalable architecture for maximum ingest and query performance at the highest concurrency. It delivered many powerful capabilities, such as: Optimization to run anywhere from bare metal to the hybrid cloud with commodity hardware, including multi-cloud Simple to deploy Better performance than legacy RDBMS and NoSQL databases for high-velocity big data workloads Memory-optimized Low total cost of ownership by integrating with existing systems Ingest millions of events per second with ACID transactions while simultaneously analyzing billions of rows of data in relational SQL, JSON, geospatial, and full-text search formats Data sharding Latency-free analytics Seamless horizontal and vertical scaling Workload management Compressed on-disk tables Switching to SingleStore DB from MariaDB in Nucleus Nucleus Security started with the free tier of SingleStore DB for the proof of concept early in 2019. The founders wanted to determine how hard it would be to migrate the application to the new database. They imported 80-100 gigs of data from MariaDB that included several tables with several 100 million rows to test their slowest queries. It only took an afternoon to migrate the development database to SingleStore DB and get Nucleus working without any architectural changes. Carter says, “It was dead simple to get set up. Whereas, I’ve got experience getting Oracle database clusters set up, and those things can be nightmares. And our experience with SingleStore was very good. We do not spend a lot of time maintaining it, troubleshooting it.” Following the successful proof of concept, the Nucleus application moved to a three-node cluster along with several single server instances for customers in Australia, the United States, and the European Union. It took 3-4 weeks to get Nucleus tested, deployed, and entirely in production on SingleStore DB. Nucleus got its first keystone client, the Australian Post Office, in March 2019, shortly after the migration. They had 2,000 code repositories and required approximately 50,000 scans per day. Kuffer says, “They paid us a lot of money upfront on a multi-year deal, and plus they had the brand name and it allowed us to transition that into a lot of later success. We definitely wouldn’t have been able to support the scale that they had without SingleStore.” No Architectural Changes The Nucleus app brings in data directly from the APIs of vulnerability scanning tools used by customers, and interacts directly with their job scheduling systems, such as Jira or ServiceNow, directly. There’s no need, at this time, to use Kafka or other streaming technologies. Nucleus did not need to make any architectural changes to their application to move to SingleStore; it has served as a drop-in replacement for MariaDB. Since MySQL wire compatibility is shared by both, making the move was easy. By replacing MariaDB with SingleStore, Nucleus customers can now support MSSPs with full scalability. Reference Architecture: Nucleus replacing MariaDB with SingleStore The Nucleus team did not need to make any architectural changes to move to SingleStore, and they describe using SingleStore as “dead simple.” Stephen said, “It was dead simple to get set up. Whereas, I’ve got experience getting Oracle database clusters set up, and those things can be nightmares. And our experience with SingleStore was very good. We do not spend a lot of time maintaining it, troubleshooting it. Anyone that’s using MariaDB, we would recommend it to. No question.” Why Nucleus Security Deployed SingleStore DB on AWS Nucleus Security opted to use SingleStore DB on AWS rather than SingleStore Managed Service for several reasons: Enterprise clients have strict security and compliance requirements for their sensitive data. Some Nucleus clients require their data to be hosted on servers within their country or region. Enterprises may be uncomfortable with smaller third-party providers. What SingleStore DB Helped Nucleus Security Accomplish By leveraging SingleStore DB for its vulnerability management platform, Nucleus Security has: Improved query speeds from 2 – 20x faster without any schema or query changes. Increased customer size capacity by 5 – 10x on the same hardware through SingleStore’s data compression, which reduced 90 gigs down to 5 gigs. Enabled quick and easy horizontal and vertical database scaling to support large enterprise clients. Achieved a 50X improvement on asset scan ingestions, going from 5,000 assets in 3 hours to 100,000 assets in 45 minutes. Kept AWS costs at one-third of what Percona would have required. Carter explains: “Several vendors have tried to build vulnerability management products that scale, but nothing in the market actually did. Then we found SingleStore, which allows us to handle customers 10 times the size, and to be ready for future growth as well. For years, large enterprises and managed service security providers have been looking for a vulnerability management solution that can scale; many had tried to build such a platform themselves. Now we have it, thanks to SingleStore, and we are growing rapidly. ” Learn more from this youtube video: Launch Codes for SaaS Success – The Know Show What’s Next for Nucleus Security? As a company with a quickly expanding customer base, Nucleus Security is just getting started. The team is constantly working to keep up with increasing volumes of data generated by enterprise vulnerability scanning tools and continually expanding its growing list of 70+ integrations with scanners and external tools. Nucleus Security will continue to work closely with SingleStore and looks forward to the new features of SingleStore DB to power the growth of its platform. Get started with SingleStore DB for free , or contact us to learn more about our distributed SQL database built for operational analytics.", "date": "2020-11-04"},
{"website": "Single-Store", "title": "memsql-available-red-hat-open-shift-marketplace", "author": ["Gina von Esmarch"], "link": "https://www.singlestore.com/blog/memsql-available-red-hat-open-shift-marketplace/", "abstract": "Red Hat Marketplace provides a one-stop shop to purchase enterprise applications and deploy across any cloud or on-premise SAN FRANCISCO – SEPTEMBER 8, 2020 – SingleStore, The Database of NowTM for operational analytics and cloud-native applications, today announced that the SingleStore scalable SQL database is now available through Red Hat Marketplace. Red Hat Marketplace is an open cloud marketplace for enterprise customers to discover, try, purchase, deploy, and manage certified container-based software across environments – public and private, cloud and on premises. Through the marketplace, customers can take advantage of responsive support, streamlined billing and contracting, simplified governance, and single-dashboard visibility across clouds. SingleStore is a cloud-native distributed, highly-scalable, relational SQL database that can handle both OLTP and OLAP workloads in a single system, which fits with the direction of new applications to combine transactional and analytical (HTAP – Hybrid Transaction/Analytical Processing) requirements. Enterprises are standardizing container-based approaches to ensure that applications can consistently run and be managed identically, regardless of the underlying cloud infrastructure. SingleStore is an ideal converged database for operational analytics that require fast data ingestion, low-latency queries, and elastic scaling with familiar relational SQL. Global enterprises use the SingleStore distributed database to easily capture, process, analyze, and act on data to thrive in today’s insight-driven economy. “Change is a constant, so organizations need scalable, high-performance solutions that enable them to make decisions and act in the moment,” said SingleStore co-CEO Raj Verma. “Businesses can now do that with the SingleStore database powering real-time insights, augmented analytics, and artificial intelligence and machine learning models. With these capabilities, enterprises can gain the necessary insights to meet current and future business requirements, deliver differentiated customer experiences and stay ahead of the competition.” Built in collaboration with Red Hat and IBM, Red Hat Marketplace delivers a hybrid multicloud trifecta for organizations moving into the next era of computing: a robust ecosystem of partners, an industry-leading Kubernetes container platform, and award-winning commercial support – all on a highly scalable backend powered by IBM. A private, personalized marketplace is also available through Red Hat Marketplace Select, enabling clients to provide their teams with easier access to curated software their organizations have pre-approved. “We believe Red Hat Marketplace is an essential destination to unlock the value of cloud investments,” said Lars Herrmann, senior director of technology partnerships, Red Hat. “With the marketplace, we are making it as fast and easy as possible for companies to implement the tools and technologies that can help them succeed in this hybrid multicloud world. We’ve simplified the steps to find and purchase tools like the SingleStore database that are tested, certified and supported on Red Hat OpenShift, and we’ve removed operational barriers to deploying and managing these technologies on Kubernetes-native infrastructure.” “Through Red Hat Marketplace, we’re expanding our ecosystem together with partners like SingleStore and helping our customers thrive in a hybrid multicloud world,” said Sandesh Bhat, IBM General Manager, Open Cloud Technology & Applications. “Container-based environments are the future of enterprise technology, and Red Hat OpenShift is the industry’s most comprehensive enterprise Kubernetes platform. We’re excited to simplify software purchase and adoption for our clients through a curated private Marketplace experience.” Red Hat Marketplace is designed to meet the unique needs of developers, procurement teams and IT leaders through simplified and streamlined access to popular enterprise software. All solutions available through the marketplace have been tested and certified for Red Hat OpenShift, allowing them to run anywhere OpenShift runs. A containers-based approach helps ensure that applications can be run and managed the exact same way, regardless of the underlying cloud infrastructure. This gives companies the flexibility to run their workloads on premises or in any public or private cloud with improved portability and confidence that their applications and data are protected against vendor lock-in. About SingleStore SingleStore is The Database of NowTM, powering modern applications and analytical systems with a cloud-native, massively scalable architecture. SingleStore delivers maximum ingest, accelerated transaction processing and blisteringly fast query performance, including AI integration and machine learning models, all at the highest concurrency. Global enterprises use the SingleStore distributed database to easily ingest, process, analyze and act on data, to thrive in today’s insight-driven economy. SingleStore is optimized to run on any public cloud or on premises with commodity hardware. Visit www.singlestore.com or follow us @SingleStoreDB. Contact Gina von Esmarch 415-203-4660 gvon@singlestore.com Red Hat, Red Hat Enterprise Linux and OpenShift are trademarks or registered trademarks of Red Hat, Inc. or its subsidiaries in the U.S. and other countries. Linux® is the registered trademark of Linus Torvalds in the U.S. and other countries.", "date": "2020-09-10"},
{"website": "Single-Store", "title": "revolution", "author": ["Domenic Ravita"], "link": "https://www.singlestore.com/blog/revolution/", "abstract": "Today is an important milestone for our company. We have rebranded the company to reflect that we offer much more than an in-memory database. We also have an expanded vision to share. SingleStore, formerly MemSQL, provides one platform to actualize all of your enterprise’s data. “What’s in a name?” The MemSQL name has stood for unrivaled speed, scale, and simplicity for operational data since its founding in 2011. It is a well-recognized name among the data architect and performance engineer experts. Our Co-Founder and Chief Strategy Officer, Nikita and Adam, our CTO, founded the company on the vision of building a massively scalable, elastic transactional system. As our product has expanded to fulfill the growing developer community’s needs, the name no longer reflected the breadth and depth of our current capabilities and product vision. To our faithful communities of users, contributors, customers and advocates, we will “retain that dear perfection” for which you’ve known and loved MemSQL while broadening our capabilities. The Early Years The initial version of the product was designed to meet the low-latency requirements of real-time analytics workloads and leveraged the newly affordable large memory hardware available. At the time, the cost of RAM was going down dramatically which made it cost-effective for the first time to build a totally in-memory transactional database where the entire dataset fit in memory. The advantage was blazing speed for concurrent reads and writes, but without the need to manage disk I/O, it also opened the door to using a more efficient indexing approach, namely the skip-list index . Even now, almost a decade later, most databases leverage a less efficient index approach of a bygone disk-based era, the Btree. And so, with an initial version which provided an in-memory transactional rowstore, the company was named “MemSQL” with “mem” signifying in-memory and “SQL” making it clear that you could indeed achieve speed, scale, and SQL without giving up on the expressive power and advantages of relational algebra executing for your application using a simple, widely-understood declarative statement. NewSQL Pioneers and ACID Guarantees Within a couple of years, MemSQL became a leader in a new area called “NewSQL” delivering the scalability, speed and flexibility promised by non-relational systems while retaining the support for SQL queries and ACID (atomicity, consistency, isolation and durability) guarantees. As a scalable distributed SQL database, MemSQL was among the first systems to provide these NewSQL capabilities along with lock-free data structures, code generation, MVCC, replication, and k-safety. The next several releases added an in-memory compressed columnstore backed by disk, JSON as a native datatype, times-series and geospatial types and functions, SIMD vectorization, a resource governor, and so many more than we can list here. This brought us into a category Gartner called hybrid transactional analytical processing, HTAP. With the addition of data science and machine learning model integration several years ago, we expanded into an area we called Operational Analytics . Fast forward to today The product evolved beyond in-memory several years ago to use a sophisticated tiered storage approach leveraging modern cloud and hardware innovations giving customers 10x the performance at ⅓ the cost of database incumbents like Oracle. Our new name signifies the company’s goals of helping businesses adapt more quickly, embrace diverse data and accelerate digital innovation by operationalizing all data through one platform for all the moments that matter . What’s Changed – We are now SingleStore The renaming of the company to SingleStore also brings new product and service names. SingleStore DB, formerly MemSQL Software It continues to provide a converged data platform for transactions, analytics, and operationalizing ML for time-critical scenarios. It continues to handle structured, semi-structured, and unstructured data and is available in the public clouds, on-premises, and in hybrid deployments. SingleStore Managed Service, formerly MemSQL Helios The MemSQL Helios launch over a year ago introduced the world’s first cloud-native HTAP and translytical database with converged transactional and analytical capabilities provided in a single offering without the need to replicate or link data from one cloud database to another cloud database. This convergence means better cloud cost management through lower monthly bills for the same cloud workloads, fewer moving parts, and fewer skill sets needed. Uber , John L. Scott , Medaxion , SSIMWave and others are enjoying the database simplicity made possible by the converged cloud database we now call SingleStore Managed Service. The new name is not so new. Our community of customers and developers will recognize “SingleStore”. It has been the name of the multi-year initiative to move from our dual table type approach in a single database to a solution which both OLTP and OLAP workloads efficiently and with high-speed performance using a single table type. That journey began with the 7.0 release , continues, and will be mature with the upcoming 7.5 release . This capability of a single table type and the initiative is now known as “universal storage” indicating our intent to expand that diversity of data types, data models, built-in functions and data access patterns beyond the current multi-model set we currently support. Expanded Vision Takes Hold With today’s rebranding announcement also comes a preview of the expanded vision for the product. Our Chief Product Officer, Jordan Tigani, announced today our intent to provide access to data located anywhere, even beyond SingleStore. The new capability will provide a global namespace for data located across a multi-cloud landscape allowing SingleStore to provide an API from which to operationalize your data no matter where it lives. SingleStore databases will be accessible from anywhere you have a SingleStore compute cluster, while honoring access permissions and sovereignty restrictions. You will be able to join data in AWS against data in GCP and on-premises, for instance. We will manage replication, consistency, and security according to policies. This global federated access to all your data will provide greater flexibility and will further simplify cloud data management for organizations. It’s a game changer. We’re very excited to share this news with you today during our (R)Evolution 2020 event hosted by our CEO, Raj Verma, and CPO, Jordan Tigani, in collaboration with our customers and partners. Thank you to all of our customers, partners, and employees for helping to build this company and joining us as we continue towards our expanded mission, now as SingleStore, to operationalize all data through one platform for all the moments that matter .", "date": "2020-10-21"},
{"website": "Single-Store", "title": "veterans-day-singlestore", "author": ["Gina von Esmarch"], "link": "https://www.singlestore.com/blog/veterans-day-singlestore/", "abstract": "Today is Veterans Day, a time to recognize and thank our military servicemen and servicewomen. As a civilian, my frame of reference for the kind of sacrifice these individuals make is limited. In my efforts to better understand and appreciate their experiences – and enormous contributions – I have relied on a few movies that I find extremely inspirational. Amid the pandemic, people are subscribing to streaming services and watching movies more than ever . Movies bring history to life, inspiring generations across the globe. Classics like “The Longest Day” with John Wayne and “Saving Private Ryan” with Tom Hanks certainly leave an impression. Yet the movie scene I find most unforgettable features Jeremy Renner in “The Hurt Locker”. For me, this scene hits harder than all of the rest – and it takes place in a cereal aisle. I have never served my country in any military capacity. But I can try to imagine the struggle of finding meaning in his everyday choices following experience in combat. The aftermath that is seldom uttered, and seemingly quickly forgotten, is a battle that confronts the bigger themes of purpose and family. I imagine this brings to mind your own favorite movies in this genre. Such movies illustrate that the veterans in our lives are extremely special. And Veterans Day affords Americans the opportunity to pay our respects to all who have served our country during war and peacetime. How we pay tribute to these brave individuals is personal. For some, it involves making a phone call. Others honor veterans with a tag on social media. Listening, learning about, witnessing and sharing the stories of our veterans helps remind us of what they have sacrificed, how they continue to contribute, and the many benefits we all enjoy as a result. Last week, I had the honor of speaking in one of the SingleStore community’s recent tributes.  On Nov. 7, Mitchell Wright helped his father, Col. Robert Wright, who is a retired Marine from Oakland, Calif., to celebrate the 245th birthday of the United States Marine Corps. With the help from fellow Marines and their families, the Wrights acquired and placed a small flag on every one of the 2,600 Marine’s graves at the Sacramento National Cemetery. This exemplified the Marine Corp motto Semper Fidelis, which means always faithful in Latin. And it symbolizes the lifelong commitment held by every Marine for the Corps and America.", "date": "2020-11-11"},
{"website": "Single-Store", "title": "a-forrester-singlestore-qa-using-real-time-analytics-to-prevent-fight-the-covid-19-pandemic", "author": ["Gina von Esmarch"], "link": "https://www.singlestore.com/blog/a-forrester-singlestore-qa-using-real-time-analytics-to-prevent-fight-the-covid-19-pandemic/", "abstract": "As a follow-up to our recent webinar with SingleStore Field CTO Domenic Ravita and our guest Forrester VP Principal Analyst Noel Yuhanna, we take a deeper dive into market-facing pandemic-relevant questions. Noel shares some of his key learnings over the past eight months. He talks about how businesses are learning to leverage data and analytics during COVID-19, the suddenly urgent need for real-time data, the benefits of a multi-cloud data and analytics strategy, and why companies must focus on customers as digital transformation races forward. What are the top questions organizations are asking about data and analytics during COVID-19? We have been getting quite a few inquiries about data and analytics, especially around cloud, open source, real time, automation, and modernization. Cloud is helping organizations to become more agile to improve customer experience, supply chain, and operational efficiency. Due to budget cuts, many organizations — including billion-dollar companies — are now seriously considering enterprise-wide cloud strategies including mission-critical applications. Open source has come up for discussion time and again. During the 2008 recession, organizations started to leverage various open source solutions for transactional and operational use cases. This year, open source is back on the forefront; even Global 2000 companies are now looking to leverage it for their tier-2 and tier-3 applications. We also see organizations asking more about real time, focusing on customer personalization and demand analysis for new products and services in various regions to control inventory. Besides, some organizations are looking to modernize their legacy platforms, especially as they realize their traditional platforms are failing to keep up with the new demands of self-service, real time, and connected data to support customer 360, risk analytics, and industry-specific analytics. Automation has also become a hot topic. Organizations want to do more with less when it comes to resources and process optimization. Businesses are seeking solutions that can help them automate data management and analytics functions to accelerate new insights. Why are real-time data and insights even more critical during this pandemic? Real-time data and insights have been growing in importance in general. As more organizations make progress in their digital transformations, more data is available about internal processes, customers, partners, supply chains, etc. Forrester has solid evidence that firms that are more advanced in using data analytics to make decisions are significantly more successful than those that are not. For example, firms that are rated ’advanced‘ in our insights-driven business maturity model are almost three times more likely to report double-digit year-on-year revenue growth than ’beginner‘ firms. They are also 1.4 times more likely to report that using analytics has reduced IT costs. So, to start with, it’s critical for businesses to advance their data and analytics competencies and capabilities. And real-time data and analytics are simply more effective, allowing businesses to sense and respond in real time to changes in their customers’ needs and behaviors, as well as to understand how their business is operating. With more digital activity moving to edge devices, computing has to be real time to take advantage of sensors and data that reflect what people and machines are doing. The pandemic amplified this need to a significant degree. Customer behavior changed drastically, making existing predictive models obsolete. Firms that had advanced competencies could find out in real time how product demand shifted, what customers turned to as supply problems emerged, and more. Real-time data supported contact tracing, showed where people were traveling to, and helped public sector organizations manage the impact of the pandemic. Firms that are advanced in being insights driven are 1.6 times more likely to report that big data has increased their business agility. Are more organizations embracing a multi-cloud data and analytics strategy? Yes. Our primary data and analytics survey data did not ask about multi-cloud strategies, but it did ask organizations about the most important components of their data strategies. The top two components were ‘big data integration’ followed by ‘public cloud big data services.’ Also, when asked how their spending was changing for their primary cloud deployments, about 65% of respondents reported increases in spending in all categories of BI and analytics projects for the next year. So, cloud remains a very big trend, and anecdotally from speaking with our clients, multi-cloud is seen as the way to avoid lock-in with a particular cloud vendor and to be able to deal with issues related to cloud vendors’ presence in different geographies, and to provide the best tools for different organizational cohorts. For some enterprises, having cloud implementations with multiple vendors occurs because of autonomous decision-making in different parts of the organization; for others, it is a conscious strategy to avoid lock-in and take advantage of various capabilities and offerings. Organizations that plan their architectures carefully are typically hybrid by design and multi-cloud by design. Why must companies be customer centric in the age of digital transformation? One of the main lines of research that Forrester has published is about how companies that are customer-obsessed will significantly outperform companies that are simply customer-aware — or worse. For 10 years Forrester has been telling clients that we are in the age of the customer. And, after a decade of the age of the customer, we know that businesses catalyze profitable transformation when they put the customer front and center in every decision. No matter how sophisticated your segmentation models, how rich your customer feedback platform, or how confident you are that you intuitively know what consumers want, you must continuously challenge your thinking about consumers for your business to thrive. To be successful, firms must deliver great customer experiences that resonate with each and every customer, and they must provide a consistent brand experience across digital and physical channels. They must maximize revenue and returns while constantly creating new consumer value. And they must compete in a world of disruption while deep-seated consumer needs remain the same. As digital transformations progress, the possibility of disruption increases and only an intense focus on the customer will enable a firm to thrive. This post was coauthored by Noel Yuhanna, Forrester VP Principal Analyst.", "date": "2020-11-06"},
{"website": "Single-Store", "title": "if-your-business-is-not-working-in-real-time-youre-out-of-time-to-capture-the-business-moment", "author": ["Domenic Ravita"], "link": "https://www.singlestore.com/blog/if-your-business-is-not-working-in-real-time-youre-out-of-time-to-capture-the-business-moment/", "abstract": "Business is about serving the needs of customers. But customer expectations are changing quickly, and most organizations are not truly aware of how fast that’s happening. Most businesses are moving in slow motion relative to their customers. That means they miss out on opportunities to make decisions about and act on the moments that matter. In the past, lag time was accepted. Nielsen called people on the phone to understand their TV viewing habits. Broadcast TV networks set advertising rates and advertisers gauged viewership based on Nielsen ratings. It took a long time for a legion of people to collect this data, and once they got the data, it was typically a small and outdated sample size. But this was the best available method given the technology of the time. Today these types of approaches simply don’t work — and they don’t have to. Organizations can use modern technology to move quickly and benefit from in-the-moment opportunities. That enables them to act in real time to deliver better experiences to retain and add customers — and optimize solutions for their clients and business partners. What Is Real Time? The definition of “real time” depends upon the context. In the context of a video streaming service, “now” means instantaneously. If you’re serving up pixelated videos or you can’t deliver an advertisement, you can lose consumer users or advertising sponsors. Latency is also a conversion killer for websites and a costly problem for financial traders. Akamai, one of my company’s clients, reports that conversion rates drop 7% for every 100 milliseconds of added latency . Real time can mean seconds or minutes. Thorn, another client of my company, which works to prevent child sex trafficking, processes massive amounts of web data quickly. This improves child identification and investigation time by up to 63%. Each passing minute matters and determines the likelihood of saving a child. Speed is also key in fighting the pandemic. True Digital, also one of my company’s clients, is using real-time data to monitor human movement using anonymized cellular location information. This can help authorities prevent large gatherings that can become coronavirus hot spots. In each of these scenarios, what is considered “real time” is dependent upon the context and the goal. But all of these scenarios define crucial moments in which having the relevant current and historical data immediately available for processing is essential. In-The-Moment Decision-Making Requires Infrastructure Simplicity You have to simplify to accelerate business in this way. To go faster and get finer-grained, real-time metrics, you can’t have 15 different steps in the process and 15 different types of databases and storage. That adds up to too much latency and exorbitant maintenance fees. Instead, you need to be able to do the same things and add new business functions, with less infrastructure. This requires technology convergence. As Andrew Pavlo of Carnegie Mellon University and Matthew Aslett of 451 Research wrote , NewSQL database management systems now converge the capabilities that in the past were implemented one at a time in separate systems. This a byproduct “of a new era where distributed computing resources are plentiful and affordable, but at the same time the demands of applications [ are ] much greater.” Now you can go faster. You can make decisions and act on them in real time. You’re in the game rather than sitting on the sidelines waiting for information while competitors are acting. Modern Businesses And Their Customers Benefit From Real-Time Data Today FedEx founder and CEO Fred Smith said in 1979 that “the information about the package is as important as the package itself .” This highlights the power of data. Companies like FedEx now use this power to dynamically reroute packages based on customer interactions and to optimize their routes . Real-time data allows customers to employ digital interfaces to see when and where their packages will be delivered, request that a package be sent to an alternate location and have that request honored. It’s not just FedEx that’s doing this ; other companies like DHL and UPS have done dynamic rerouting for years. This is important because people are a lot more mobile these days; customers expect businesses to be more responsive to their needs and tend to give businesses that cater to them higher customer satisfaction and Net Promoter Scores. On-time delivery helps logistics companies avoid missing service level agreements and then paying penalties. You can’t do route optimization and dynamic rerouting if your information about the package and other relevant details is hours behind where the package actually exists. You need your digital environment to mirror what’s happening in the real world. When you create a digital mirror of your environment, you get what is called a digital twin. As our co-founder recently explained , digital twins are often associated with industrial and heavy machinery. But organizations in many sectors are now exploring and implementing digital twins to get a 360-degree view of how their businesses operate. This requires organizations to have converged, cloud-native, massively scalable and fast-performing infrastructure that supports artificial intelligence and machine learning models. Organizations that don’t have these capabilities will be outmaneuvered by faster companies that do have the intelligence and agility to make decisions and act in the now. Embracing Intelligence And Agility Understand that delivering faster data isn’t the objective. The objective is to deliver the optimal customer experience and improved operational insights. Let these two objectives be your guide — and seek ways to leverage all relevant data in the moments that matter. Dreaming big is important. But to start, identify a small project combining current, live and real-time data with historical data for in-the-moment views and automated decision-making and trendspotting to address customer experience or operational opportunities or challenges. Polyglot persistence provides real development advantages. But it’s not necessary to assemble multiple types of data stores to get those advantages. Choose simplicity with flexibility by searching for solutions that provide support for a spectrum of workloads, reducing cloud data infrastructure complexity. This was previously posted on Forbes.", "date": "2020-11-23"},
{"website": "Single-Store", "title": "dashboards-to-fastboards", "author": ["Mike Pickett"], "link": "https://www.singlestore.com/blog/dashboards-to-fastboards/", "abstract": "When I joined an integration company in late ‘99, I didn’t think data and integration would be my focus and passion. But here I am coming up to 2021, more than 20 years later, in a market that has only gotten more exciting and more relevant to the business. In the early 2000s, it was all about the “connected enterprise”, getting your apps working together. Then it was the “intelligent enterprise” and we saw the rise of the enterprise data warehouse and analytics. In the early 2010s, Big Data became the rage with a game-changing filesystem, Hadoop, that could hold unlimited data and distributed processing frameworks, MapReduce & Spark, that could work through all the data. It was in this era that we really started talking about data democratization and digital transformation. The ability to empower everyone with tools and all the data they could want to make data-driven decisions. With this in place, businesses could be on the path to digital transformation – create new revenue streams, improve operational efficiency, spot & stop churn well before it happens, etc. As companies went down this path, many found that the promises of the data-driven enterprise, data democracy and digital transformation were actually very hard to attain. Sure, some companies were showing some amazing results with their data insights but for most, it was still elusive (almost like a snipe hunt). Big Data environments were still too complex even with the new class of ingestion, data preparation and data exploration tools. The primary participants were still highly technical. The common knowledge worker or business analyst continued working with their traditional BI tools. They were following the common process of going back and forth with a data gatekeeper asking for data sets in the hope the new one or additional data set would answer their questions, complete their reports, or meet their dashboard needs. The process was slow, data was not truly democratized, individual data exploration and insights was in the hands of the few. The cloud changed everything. Cloud-native data warehouses like Snowflake and data science platforms like Databricks make the analysis of historical, static big data more accessible. These tools do not require specialized resources to provision and maintain the hardware and software. Users who have access rights to their data (e.g. Salesforce, Google AdWords, Files, etc.) can use tools like Fivetran and Stitch to easily load the data into the environments and they can start exploring and reporting. Their drawback is that they are not suitable for real-time applications and, for BI, are not capable of providing consistent low-latency, interactive query responses. Business intelligence has also followed the trend of moving to the cloud and becoming more accessible. Take a cloud-native tool like Chartio which business users can easily pick and connect to their data in minutes. They’re off and running exploring and reporting on their data, the way they want to see it. Data democratization finally arrived. The foundation for digital transformation is solidly in place. Cloud-native tools are the fuel for this engine. But we are now seeing the next level problem driven in part by accessibility and in part by the greater digital demands of the COVID-19 era . Everyone, both internal and external, is being empowered with self-service dashboards. Users want data to refresh immediately. Waiting 30 seconds is very painful. They want to explore the data dynamically. All of it, not a subset. And, they want the most current data, e.g. operational data and streaming data. With this increased demand for self-service analytics, businesses are finding their data infrastructure isn’t able to keep up. Cloud data warehouses are very good at storing large amounts of historical data for exploration and deep analytics, however, when they are faced with a high number of concurrent queries (e.g. people refreshing their dashboards) or lots of people sending different queries (e.g. individuals drilling down into the dashboard data), they are not able to to return the individual answers quickly. The latency of each underlying query serving a dashboard compounds to seconds and even minutes for the user experience. Applying more compute resources doesn’t really solve the latency or concurrency problems, but certainly does increase expenses. The dashboards are still too slow and flaky in the eyes of the user. Legacy technologies (on-premises or in the cloud) simply weren’t designed for this type of data interaction. Expanding a legacy data warehouse appliance like Teradata, Netezza or Exadata is  astronomically cost prohibitive. These systems were not designed for modern data workloads that emphasize real-time insights to changing customer or machine conditions. Expecting legacy systems to do this is analogous to expecting a car to suddenly adapt and begin flying like a jet. Putting wings on the car and making modifications might work temporarily, but it’s unlikely to work long term. In essence, it is a bad idea to modify a car to fly. This is the “flying car dilemma” . Oftentimes people think they don’t have the data scale problem and this is where they try to use a database like MySQL for their analytical database. However, they quickly find out the pains of trying to scale MySQL or other single-node OLTP databases to meet the growing analytics demands. Enter SingleStore, a modern distributed SQL database that easily handles both the data volumes and query loads – be it a high number of concurrent queries, individuals doing dynamic or interactive queries or both! SingleStore is also uniquely able to make data available for queries (or in dashboards) as it is being ingested into the environment. This can be operational data from a CDC tool like HVR or Qlik/Attunity or streaming data from a tool like Confluent or StreamSets. Cloud data warehouses simply can’t do this. The data needs to land and then be curated into the various tables and data warehouses before it’s available for use. And this is why SingleStore is the right database to complement existing data stores and power your BI tools and make your dashboards into fastboards. Chartio, with it’s Visual SQL interface writing queries directly on-top of the database takes full advantage of the speed provided in SingleStore.  It is incredibly powerful in how easy it is to use both for business users (executives) and data professionals. With SingleStore underneath, dashboards are refreshed quickly while drill downs and data explorations are able to happen at the speed of thought. We recently did a live demonstration of this in action with Dave Fowler, Chartio’s CEO and Sarung Tripathi our Technical Lead for SingleStore Managed Service. You can see for yourself how fast you can get answers to your questions, even with a dataset that’s 10x as large as the one that was being used prior (PostgreSQL). No smoke and mirrors. So time for all of us to accelerate our digital transformation efforts. The tools are ready and the speed is there to explore your data at the speed of your thoughts. Get started with Chartio and SingleStore today!", "date": "2020-11-25"},
{"website": "Single-Store", "title": "toward-more-efficient-and-inclusive-work-with-asynchronous-communication", "author": ["David Gomes"], "link": "https://www.singlestore.com/blog/toward-more-efficient-and-inclusive-work-with-asynchronous-communication/", "abstract": "I once heard that a “book worth reading is worth reading twice”, and I’ve found this to be true about other things as well. There’s a documentation page that’s been with me for the majority of my professional career; a page that I keep coming back to and that I’ve read from start to finish, without skipping a word, more than a few times in the last few years. This document is GitLab’s Embracing Asynchronous Communication guide , a section of GitLab’s public handbook that goes over their approach to asynchronous communication as a business. To provide you with some context, as of writing this blog post, my role at SingleStore is Engineering Manager and I work with teams that are geographically distributed across a few countries (mainly the US, UK and Portugal). I have direct reports across these 3 countries as well, and prior to the COVID-19 pandemic, I was traveling to San Francisco a few times a year. I started at SingleStore around four and a half years ago in San Francisco, but ended up deciding to relocate to Portugal and work there for the last three years. Being based eight hours away from the California time zone, I’ve learnt a lot about working across time zones, but mainly about how to work asynchronously, and all the principles behind async work are things that I apply and will apply in the future whether working across time zones or not. Efficiency and inclusiveness I fundamentally believe asynchronous communication leads to a more inclusive working environment because it allows individuals to work more flexible hours. Whether it’s because folks have to take their kids to school in the morning, or because a health condition prevents them from working continuously for 8 hours, working async allows us to more easily choose our working hours (and these may even change day-to-day). Moreover, working asynchronously also allows an organization to more easily hire people in different countries and regions, which is key for diversity and inclusion. By not limiting hiring to a specific city or even country, it’s much easier to hire people with different backgrounds, races and nationalities. My colleague, Carl Sverre, discusses our own successful approach to building a team for remote work and additional considerations regarding asynchronous communication in this talk on geo-diverse team-building . The added efficiency from asynchronous communication comes from various aspects. Perhaps the first one that comes to mind is allowing knowledge workers to block out “focus time” on their calendars to do things like preparing slides, writing and reviewing code, going through applicant resumes, etc. A great book on this subject, which I also recommend, is Cal Newport’s Deep Work . Focused work time is an invaluable tool toward getting things done and moving faster. Another added benefit of asynchronous work is easier access to information (which also brings further transparency to the workplace), since more things should be written down in public as opposed to communicated 1:1 between 2 individuals. Applying this in my day-to-day The main principle of asynchronous communication that I apply in my day-to-day is not making assumptions about coworkers’ working hours. I may do a 9 AM to 5 PM in my time zone, but somebody else might prefer a very different schedule, even if they’re on the same time zone as I am. To help me not make any assumptions about the working hours of others, I do all of the following (and assume others do as well): Disabling any type of email or Slack/IM notifications Not having Slack or work email on my phone Not having an “online” status in your IM tool (i.e., setting yourself as permanently “Away” or permanently “Online”) Avoiding communication that feels synchronous in Slack such as “Busy right now, I’ll check this later”. In most situations, I prefer to just reply whenever I can properly reply. These mechanisms encourage us to prioritize asynchronous communication and also to plan ahead to avoid needing quick or immediate unplanned feedback/help. More and better planning means less urgent things coming up throughout the week. Whenever I file tasks or write documents/emails, I put an emphasis on making sure that people don’t need to ask follow-up questions by including all needed information, and making communication as clear as possible. Writing well is one of the most important skills that one needs to develop when working in a mostly asynchronous environment. Notice, however, that I do check Slack regularly (more often than email), especially due to the nature of my role and size of the team, but I check it when I want to , thus allowing me to focus on my deep work. Keep in mind that for certain positions (software engineering, site reliability engineering, etc.), an on-call rotation should be set-up with a tool such as PagerDuty to address urgent issues. Another principle that I apply in my day-to-day is making sure that meetings are very well organized (which also helps with the aforementioned “better planning”): Never holding meetings without an agenda (with some exceptions) and sticking to it Taking notes alongside the agenda usually works great Having participants go through required reading beforehand, if there’s any Scheduling meetings only when necessary and not when they can be replaced with an asynchronous workflow Recording as many meetings as possible (Google Meet and Zoom make this quite easy) Moreover, it’s very important to be able to fallback to synchronous mechanisms when it makes sense (and once again, this is really explained in the linked documentation page ). For example, when new hires join our team, I shift towards a more synchronous working style with them until they’re more independent. We’re also working on trying to make our onboarding process a bit more self-service, but we haven’t gotten there just yet. Finally, this style of work can lead to less human interaction, which can be addressed by holding informal events (such as games sessions, “remote coffees” and other things of the sort) to make sure team members are bonding. Last words This was an article that I’ve been meaning to write for a long time, as it’s a topic that I’m quite passionate about. I’d love to hear your feedback (both positive and negative), so reach out on Twitter or LinkedIn ! Also, if this sounds like an environment in which you’d thrive as an engineer, we are hiring. You can find our open positions on our Careers page .", "date": "2020-12-03"},
{"website": "Single-Store", "title": "aws-sagemaker-and-singlestore-operationalizing-machine-learning-at-scale", "author": ["Sarung Tripathi"], "link": "https://www.singlestore.com/blog/aws-sagemaker-and-singlestore-operationalizing-machine-learning-at-scale/", "abstract": "Abstract Many organizations today are looking for ways to run Machine Learning (ML) models in operational systems at scale. Data Scientists can now take models that they build in SageMaker and deploy them as user-defined functions within our general purpose, relational database. Read more about how you can accelerate the execution of your models against real time data. You can also find in-depth notebooks detailing how to implement this on GitHub . Setting the Stage AWS SageMaker has quickly become one of the most widely used data science platforms in the market today. Evidence suggests that even though there has been a proliferation of models in the enterprise, only 17% of companies have actually deployed a machine learning model into production. The following tutorial shares how we at SingleStore are helping our customers do just that. Today, many enterprises are seeking to complete that machine learning lifecycle by deploying models in-database. There are many reasons why AI Engineers are taking this approach, as opposed to operationalizing their models using SageMaker: First, training and deploying models where the data lives means your models are always based on the entirety of your data, even the freshest records Keeping models in the same place as the data also reduces latency and cost (i.e., EC2) that the separation of the two may present In the case of real-time data, users can even run their models against every new live data point that is streamed into the database with amazing performance (while continuing to reference historical data) Here at SingleStore, we are enabling AI developers to deploy models within our converged data platform. Our ability to ingest data in real-time from anywhere, perform sub-second petabyte-scale analytics with our Universal Storage engine and support for high concurrency makes us the perfect home for your models. The next section details how SageMaker and SingleStore Managed Service, our cloud database-as-a-service offering, coexist to help accelerate your machine learning models. Reference Architecture There are many different ways that machine learning engineers can use SageMaker and our Managed Service together. The reference architecture below states just one approach, but these components can be variably interchanged based on your preferred architecture. Below is an example of how SingleStore is able to easily leverage your existing data and modeling ecosystem of S3 and SageMaker working in-concert to make models run faster. In this architecture, models are built in SageMaker using data from S3. The models are then converted into UDFs using SingleStore’s “SageMaker to Python” library. At that point, the models live in the database and can be executed against real-time data streams coming from Kafka using native SQL. SingleStore also supports many other real-time and batch ingest methods, listed here . Getting Started To start leveraging AWS SageMaker and SingleStore, launch a Managed Service Cluster here. This is our fully managed service in AWS that will allow you to try out all of the possibilities of SingleStore without the burden of infrastructure. First, we will discuss how to collect your data and bring it into the platform. Then, we will train a machine learning model using that dataset. We will then deploy that model into the database and finally, use that model to score new data points that are coming from a real-time feed. Our entire example is run out of Jupyter notebooks, but you could also use our visual SQL client, SingleStore Studio, for many of the operations. Data Collection In this example, we will load the SageMaker sample bank churn data set into S3. As demonstrated in the GitHub repository, we start this example by instantiating our AWS credentials and creating an S3 bucket: **s3_input_train = sagemaker.s3_input(s3_data=f's3://{BUCKET}/train', content_type='csv')\n** Model Training We will be using a gradient boosting framework for modeling. Gradient boosting is a method of supervised learning used for classification and regression problems. In this example, that means taking a set of variables and producing a prediction that represents a percentage likelihood of churn. Gradient boosting can also be used to predict categories rather than values. Today, we will use XGBoost, which is a library used for gradient boosting. In order to train the model, we will call AWS SageMaker via Python. SageMaker allows us to select instance type, a number of instances, and a number of hyperparameters about the model when training it. These values will depend on the size of your dataset, the complexity of the model, etc. **sess = sagemaker.Session(session)**\n**xgb = sagemaker.estimator.Estimator(**\n**    CONTAINER,**\n**    ROLE,**\n**    train_instance_count=1,**\n**    train_instance_type='ml.m4.xlarge',**\n**    output_path=f's3://{BUCKET}/models',**\n**    sagemaker_session=sess)**\n**xgb.set_hyperparameters(max_depth=5,eta=0.2,gamma=4,min_child_weight=6,subsample=0.8,silent=0,**\n**                        objective='binary:logistic',num_round=100)**\n**xgb.fit({'train': s3_input_train})\n** Model Deployment To deploy the model to SingleStore, we first establish the database connection using the SingleStore Python Connector . Next, we will use the SingleStore/SageMaker Python library to convert the XGBoost model into a user-defined function in the database. UDFs are how databases define a process to accept parameters, perform a calculation, and return a result. **memsql_sagemaker.xgb_model_path_to_memsql(**\n**    'predict_yes', xgb.model_data, memsql_conn, session,**\n**    feature_names=features,  allow_overwrite=True)\n** Once this step completes, we can run a simple SHOW FUNCTIONS command either from Python or from SingleStore Studio and we will see the SageMaker model deployed as a user-defined function in SingleStore Managed Service! We can even quickly run our model against some sample rows in our dataset: memsql_conn.query(f”SELECT predict_yes({‘,’.join(features)}) as res FROM bank ORDER BY id LIMIT 10″) Model Inferencing in Real-Time The final step in the process of running our machine learning model in-database is to execute it against real-time data. In this example, we will use Kafka. Kafka is a great distributed event streaming platform, for which we have native connectivity in SingleStore using Pipelines . Data consumed from Pipelines can land either directly into a table or into a stored procedure. In order to execute this model continually against live data, we will also need to use stored procedures. The procedure we define will run the model every time SingleStore consumes a new message from Kafka and land the prediction in a table directly. **memsql_conn.query(**\n**    f'''**\n**    CREATE OR REPLACE PROCEDURE process_kafka_data(pipe query({\", \".join([f\"{f} DOUBLE NOT NULL\" for f in [\"id\"] + list(all_data.columns)])})) AS**\n**    BEGIN**\n**        INSERT INTO res(expected, predicted) **\n**        SELECT y_yes, predict_yes({\", \".join(booster.feature_names)})**\n**        FROM pipe;**\n**    END**\n**    '''** **)\n** Next, we will create the pipeline itself. As you will see, it is just a simple three lines of SQL to start consuming live data from Kafka. Pipelines enable us to ingest in a highly parallel fashion data in Kafka partitions. **memsql_conn.query(**\n**    f'''**\n**    CREATE PIPELINE process_kafka_data **\n**    AS LOAD DATA KAFKA '{kafka_topic_endpoint}'**\n**    INTO PROCEDURE `process_kafka_data`**\n**   '''** **)**\n**memsql_conn.query(\"START PIPELINE process_kafka_data\")\n** As data starts to flow into SingleStore, we can make a simple query to the table directly to see our predictions: **rows = memsql_conn.query(\"SELECT * FROM res LIMIT 5\")**\n**pd.DataFrame([dict(r) for r in rows]).head()\n** Summary With SingleStore Managed Service and AWS SageMaker, we are able to both build and deploy supervised learning models against operational data. This enables our user base to be wise about their AWS investments, reduce latency in their data science environments, and have a unified store for all mission critical machine learning features. You can try our Managed Service today with $500 in Free Credits here .", "date": "2020-12-04"},
{"website": "Single-Store", "title": "singlestore-universal-storage-episode-3-revenge-of-the-upsert", "author": ["Eric Hanson"], "link": "https://www.singlestore.com/blog/singlestore-universal-storage-episode-3-revenge-of-the-upsert/", "abstract": "SingleStore Universal Storage technology is a dramatic evolution of our columnstore data format that allows it to support both operational and analytical workloads with low total cost of ownership (TCO). People love our rowstores for OLTP-style access patterns because of our great performance based on memory-optimized data structures and compilation of queries to machine code. But for users with large data sets, the cost of providing servers with enough RAM to hold all the data started to be significant. Universal Storage solves this cost problem, while also providing even better analytical performance via query execution that takes advantage of columnar storage formats, vectorized execution, and single-instruction multiple-data (SIMD) instructions. Our 7.0 release introduced universal storage and the 7.1 release enhanced it . Now, we’re delivering Episode 3 of universal storage in our 7.3 release. In this installment, we’ve added: Columnstore can be made the default table type by setting a new engine variable default _ table _ type to ‘columnstore’ UPSERT support for columnstore tables with single-column unique keys , including INSERT ON DUPLICATE KEY UPDATE INSERT IGNORE REPLACE Analogous capabilities for Pipelines and LOAD DATA Support for unique key enforcement for very large INSERTs and UPDATEs Now, arbitrary INSERTs and UPDATEs, regardless of the number of rows updated, will succeed. In 7.1, if more than, say, 20-30 million rows were updated in a single statement on a small node, the operation could fail, due to an out-of-memory condition. Please see this video to learn more about our universal storage update in SingleStore 7.3: Columnstore as Default In SingleStore Managed Service, the default table type will be set to columnstore for new clusters when 7.3 is deployed. In SingleStore DB, the default table type is rowstore, but can be set to columnstore by running: set global default_table_type = 'columnstore'; Now, just about any CREATE TABLE statement you might have run to create an application on MySQL, MariaDB, or for SingleStore rowstore tables will execute properly and create a columnstore. The biggest benefit of this for new users is that they won’t run out of RAM when they add a lot of data. Veteran users who use columnstore most of the time will appreciate the convenience of it. As an example, the following CREATE TABLE statement will run and create a columnstore in this mode: create table fact_sales(ts datetime(6), qty int, prod_id int,\n unit_price decimal(18,2), store_id int); This statement is equivalent to the following, which is output by show create table fact _ sales: CREATE TABLE `fact_sales` ( `ts` datetime(6) DEFAULT NULL, \n`qty` int(11) DEFAULT NULL,\n`prod_id` int(11) DEFAULT NULL, \n`unit_price` decimal(18,2) DEFAULT NULL,\n`store_id` int(11) DEFAULT NULL, \nKEY `__UNORDERED` () USING CLUSTERED COLUMNSTORE , SHARD KEY () ); Notice that the default shard key is empty, resulting in round-robin data distribution across leaf nodes. Also, the default columnstore sort key is empty, which means the system doesn’t actively maintain the data in sorted order. For many use cases, and for developers just getting started, these choices are fine. You can easily specify a columnstore sort key with the new shorthand SORT KEY that’s equivalent to KEY(…) USING CLUSTERED COLUMNSTORE. In addition, KEY(…) designations in a CREATE TABLE statement automatically cause creation of hash keys. See our 7.3 documentation for full details. If you’re in columnstore-as-default mode and you want a rowstore, you can just say CREATE ROWSTORE TABLE tableName(...) Bottom line, if you’re new to SingleStore and you don’t want to worry about running out of RAM, you have a mostly-analytical workload, or both, set default _ table _ type = ‘columnstore’. And, on our managed service, you’ll get this by default. Upserts on Universal Storage When we designed the roadmap for universal storage, we saw that many of our users were using rowstores to accept new data, detect duplicates, and do upserts. Then after the data settled down, they’d move it to a columnstore. And then they’d have to do some special query logic to query the colder columnstore part and the hotter rowstore part together to combine the data in both of them. Universal storage eliminates the need to do this. And 7.3 delivers a major part of this: upsert capability columnstores. The most typical kind of upsert does a conditional insert if a key is not present, and otherwise updates the existing record with that key somehow. E.g. the raw input data might have “network events” which record device _ id — a unique identifier of a network device bytes _ transmitted — bytes transmitted for this even ts — the time the transmission even occurred Perhaps this is in the following table: create table network_events_new(\n  device_id_n int,\n  bytes_transmitted_n int,\n  ts_n datetime(6),\n  sort key(ts_n),\n  shard key (device_id_n)\n); Remember, this is a columnstore; the SORT KEY makes it so. Now, suppose our ultimate goal is to keep a running total of bytes transferred by a device, as well as the latest time of an event affecting that device, in this table: create table network_events_summary(\n  device_id int,\n  bytes_transmitted int,\n  ts datetime(6),\n  unique key(device_id) using hash,\n  sort key(ts),\n  shard key (device_id)\n); Now, load some initial data into the “new” networks events table: load data infile \"/data/network_events.csv\" \ninto table network_events_new\nfields terminated by ','\nlines terminated by '\\n'; Where the .csv file contains: 1,1024,2020-12-09 14:22:06.765384\n2,4096,2020-12-09 14:22:43.613131\n2,2048,2020-12-09 14:23:06.786447\n1,512,2020-12-09 14:23:27.964939\n3,128,2020-12-09 15:55:48.209948 Suppose the initial data in network _ events _ summary is created as follows: insert into network_events_summary values(1,256,\"2020-12-09 12:57:46.244642\"); So it contains just: +-----------+-------------------+----------------------------+\n| device_id | bytes_transmitted | ts                         |\n+-----------+-------------------+----------------------------+\n|         1 |               256 | 2020-12-09 12:57:46.244642 |\n+-----------+-------------------+----------------------------+ Now, we can do an upsert to tally up total bytes transmitted, plus add rows for new devices seen for the first time: insert into network_events_summary (device_id, bytes_transmitted, ts)\nselect * from network_events_new\non duplicate key update\nbytes_transmitted = bytes_transmitted + values(bytes_transmitted),\nts = values(ts); Now, the summary will have all three devices and their totals and last-updated time, as follows: sdb> select * from network_events_summary order by device_id;\n+-----------+-------------------+----------------------------+\n| device_id | bytes_transmitted | ts                         |\n+-----------+-------------------+----------------------------+\n|         1 |              1792 | 2020-12-09 14:23:27.964939 |\n\n|         2 |              6144 | 2020-12-09 14:23:06.786447 |\n\n|         3 |               128 | 2020-12-09 15:55:48.209948 |\n+-----------+-------------------+----------------------------+ This kind of upsert pattern previously worked on SingleStore rowstore tables with a unique key. What’s new is that it now works on columnstore tables with single-column unique keys. UPSERT-style logic features (by which I mean REPLACE, IGNORE, and SKIP DUPLICATE KEY) of LOAD DATA and pipelines also now can be used with columnstores, also with the single-column unique key restriction. Universal storage is getting very near functionally complete, and can already cover most use cases. Look for the full functionality of universal storage to be finished, with introduction of multi-column key support, in Episode 4! Support for Large INSERTs/UPDATEs With Unique Keys As a simple example of a large batch INSERT with a unique key, consider this script: drop database if exists db1;\ncreate database db1;\nuse db1;\n\ncreate table t(a int not null, unique key(a) using hash, shard(a), key(a) using clustered columnstore); insert into t values(1); delimiter //\n/* Fill table t with n or more rows, doubling the size until\n   the goal is reached. */\ncreate procedure inflate(n bigint) as \ndeclare\n  c bigint;\nbegin\n  select count(*) from t into c;\n  while (c < n) loop\n    insert into t \n    select a + (select max(a) from t)\n    from t; select count(*) from t into c;\n  end loop;\nend //\ndelimiter ; In a VM on a laptop with 8GB RAM, we get these results: call inflate(32*1000*1000); /* success on 7.1.13 and 7.3.1 */ call inflate(64*1000*1000); /* OOM on 7.1.13, success on 7.3.1 */ That is, when inflating the table to over 67 million rows, it fails on 7.1.13 with an out-of-memory error, but it succeeds on 7.3.1. The algorithms for enforcing unique keys have changed so they will work even if the data is larger than will fit in RAM. This is helpful in a number of real situations, such as when you are copying a large table with an in INSERT INTO … SELECT FROM … operation and the new target table has a unique key on it. In 7.1, you might have had to break the data into chunks and run several commands to do that. Now, you can do it in one step. Summary Universal storage now is maturing and has almost all the functionality needed to support applications that use to require rowstores. This is a big TCO improvement. Many shops may be able to set the default table type to columnstore now given the functional surface area supported, and this will be the default on SingleStore Managed Service. Finally, support for unique key enforcement regardless of data size improves reliability and simplifies development.", "date": "2020-12-15"},
{"website": "Single-Store", "title": "singlestore-db-7-3-is-now-generally-available", "author": ["Rick Negrin"], "link": "https://www.singlestore.com/blog/singlestore-db-7-3-is-now-generally-available/", "abstract": "SingleStore is proud to announce the general availability of SingleStore DB 7.3 for immediate download. SingleStore DB 7.3 is also available today on SingleStore Managed Service, the company’s elastic cloud database, available on public cloud providers around the world. At SingleStore, we are building the next great database platform. SingleStore DB is a distributed operational database that delivers speed and scale. As the leading NewSQL platform for real-time analytics and augmented transaction processing [ Ron19 ] , SingleStore DB can handle all your workloads in the cloud or in your data center. This release provides improved ingestion and multi-way join efficiency to further strengthen SingleStore for mission-critical applications providing transaction processing, operational analytics, and more. This blog post describes the major features of SingleStore DB 7.3 self-managed software and SingleStore Managed Service in some detail. For a quick 5-minute rundown, checkout the release overview video here: For even more detail, see the 7.3 release notes . Why the Cloud World Needs to Simplify to Accelerate The proliferation of special-purpose datastores is complicating cloud infrastructure, inflating cloud bills, and adding unnecessary latency in copying and moving data from multiple operational stores to analytical stores. To tame the growing cloud complexity and data infrastructure sprawl, organizations should look to solutions like SingleStore that unify workloads across operational and analytical use cases without sacrificing the speed, scale, and flexibility which makes special-purpose datastores attractive. With fewer types of datastores and less data movement among them, data infrastructure is simplified and there are fewer skill sets to maintain allowing you to focus on accelerating your business. Major Features in SingleStore DB 7.3 Along with improvements across the product, SingleStore DB 7.3 provides notable features in the areas below: Programmability SingleStore supports the MPSQL language for programming internal extensions, including stored procedures and user-defined functions. The 7.3 release enhances this capability by providing additional flexibility for developers in these aspects: Query plans are smaller and thus use less memory than before. The new JSON _ AGG function takes a rowset with an arbitrary set of relationships and returns a single JSON object. The new APPROX _ PERCENTILE function does fast and accurate percentile calculations over large data sets and can be up to 10X faster than standard percentile functions like PERCENTILE _ CONT. User-defined variables allow you to write statements that look like SELECT INTO @varname. User-defined variables have session scope, and you can use them across queries. Our customers frequently asked for UDVs since they are convenient when building complex SQL scripts. Now you can EXPLAIN and PROFILE queries within stored procedures for easier query tuning. Flexibility for Developers SingleStore DB now has DDL forwarding, which allows you to send both DDL and DML statements to any aggregator. This simplifies things for your developers so they don’t have to remember which endpoint to use based on the type of operation. Cloud-Native Ingestion Enhancements This release has enhancements for data ingestion that are helpful for cloud deployments: SELECT … INTO GCS allows you to load data from SingleStore DB into a Google Cloud Store bucket. The syntax is similar to the existing SELECT … INTO S3 command. A new configuration option multipart _ chunk _ size _ mb allows you to load up to 5 TB of data per partition when running SELECT … INTO S3. SingleStore DB pipelines now integrate with Confluent schema registry, making it easy to add and remove fields from Avro schemas. Multi-way Join Efficiency Improvements For queries that join more than 18 tables, the query optimizer generates plans more quickly and produces query plans that run faster. This enhancement is especially helpful for our large enterprise customers. Universal Storage SingleStore Universal Storage technology is a suite of features that we are delivering over several releases. The aim of this technology is to support real-time analytics and online transaction processing (OLTP) on SingleStore, with great performance and low total cost of ownership (TCO). Universal Storage is an extension of our columnstore technology that excels at analytics and which also improves OLTP, including support for indexes, unique keys, seeks, and fast, highly selective, nested-loop-style joins. Because the data is highly compressed and doesn’t all have to fit in RAM, Universal Storage gives excellent TCO. In this release, SingleStore DB extends Universal Storage with these features: INSERT … ON DUPLICATE KEY UPDATE allows you to update an old row when trying to insert a unique key that already exists. This operation is commonly known as UPSERT. You can now set the default table type to columnstore by setting the default _ table _ type global variable appropriately. Simplified syntax for creating columnstore tables where you can write SORT KEY (…) instead of KEY (…) USING CLUSTERED COLUMNSTORE. Universal storage is now the default table for SingleStore Managed Service. For a detailed explanation of what’s new in Universal Storage, read Eric Hanson’s blog post here . Greater Operations Visibility & Resilience Release 7.3 of SingleStoreDB has improvements that make the product easier and more robust to operate and monitor: The new PROMOTE AGGREGATOR command lets you switch the master aggregator of your cluster without stopping the old master aggregator. This helps you keep your cluster online during maintenance. The SHOW REPLICATION STATUS command now has columns that give a clear picture of how far secondary partitions are from primary partitions. In particular, the ESTIMATED _ CATCHUP _ TIME _ S column shows how long it will take the secondary partition to catch up to the primary one. A new management view LMV _ BACKUP _ STATUS permits you to track the progress of a running backup along with the estimated time remaining. SingleStore DB lets you create resource pools for specifying resource limits so that non-critical workloads do not overwhelm your cluster. You can now run the BACKUP and RESTORE commands in resource pools to manage how much CPU they use. These are just a few of the new features you’ll get when you upgrade to SingleStore DB 7.3. For the full list of features, see the release notes at docs.singlestore.com . Conclusion The SingleStore DB 7.3 release delivers a big advance in our ability to support system-of-record applications on SingleStore, including improvements to Universal Storage (upserts, default table type); replication status monitoring; easier Avro schema migrations; faster large joins; and more. Are you interested in building a data platform with unlimited scale and power? Come join our growing team at http://www.singlestore.com/careers . References [ Ron19 ] Adam Ronthal, There is Only One DBMS Market!, https://blogs.gartner.com/adam-ronthal/2019/07/17/one-dbms-market/ , Gartner, 2019.", "date": "2020-12-15"},
{"website": "Single-Store", "title": "fast-and-accurate-percentiles-with-approx_percentile-in-singlestore", "author": ["Roxanna Pourzand and Andrii Khoma"], "link": "https://www.singlestore.com/blog/fast-and-accurate-percentiles-with-approx_percentile-in-singlestore/", "abstract": "SingleStore introduced an APPROX_PERCENTILE function to estimate percentiles for a distribution of values. This function is faster and near-exact as compared to the analogous functions that calculate exact percentiles. Read more to learn about this feature and see the performance and accuracy comparisons between approximations and precise values on a sample dataset. Please check this demo video to learn more about the approx_percentile function in SingleStore: If you have a sizable numeric dataset that you are analyzing, you likely require the ability to do some standard statistical analysis on it (and maybe more advanced analytics, too!) Perhaps your dataset represents the frequency of trades for stocks in a portfolio, clicks per user from your mobile application, or the average consumption from smart meters in a given day. For any of these types of datasets, you may need to pinpoint outliers, determine the median, or in general, better understand the distribution of your data ( it’s always a normal distribution, right? ). One analysis you might do to understand data distribution better is to use functions to calculate percentiles. Notable examples of these functions may include PERCENTILE_DISC or PERCENTILE_CONT (which we have in SingleStore, and they are also industry-standard). PERCENTILE_DISC calculates the observed percentile, while PERCENTILE_CONT estimates the quantile depending on the input distribution. If accuracy is more important in this calculation, you should use PERCENTILE_CONT . Regardless, using either of these functions to calculate percentiles on a vast dataset can be painfully slow; precise percentile calculations are expensive operations because the data must be ordered before a result can be returned. With large datasets, this ordering operation can use many resources and ultimately is a significant bottleneck. We recently introduced an APPROX_PERCENTILE built-in function ( documentation ) to allow users to efficiently approximate the values at a given percentile for a distribution of values without giving up accuracy in the computation. SingleStore is built for speed and scale, and so our implementation of APPROX_PERCENTILE leverages all of the SingleStore database’s performance benefits. Now, let’s get into a real-world example of using APPROX_PERCENTILE and comparing the performance to analogous percentile functions. The Data and Hardware We will demonstrate to you the performance benefits and efficiency using APPROX_PERCENTILE versus PERCENTILE_CONT . We use a dataset that consists of metrics measured from operating systems and WebLogic Server monitoring beans provided by the Kaggle Datasets . Here are the characteristics of the data and system we are running this on: The dataset contains 60M rows. We are running on i3.xlarge Amazon EC2 instances. The SingleStore DB cluster size is 4 Leaf Nodes. The cluster has 16 leaf cores total (4 per leaf). Problem Let’s say you’re analyzing network data for all your applications. You want to understand the connection delays across applications to pinpoint the ones that need improvement. Here’s the table definition of the characteristics of your table that’s monitoring the connection info for your applications: =>CREATE TABLE cluster_management(\n host CHAR(16) NOT NULL,\n process CHAR(16) NOT NULL,\n process_started DATETIME NOT NULL,\n is_anomaly BOOL NOT NULL,\n connection_delay DOUBLE NOT NULL,\n process_cpu DOUBLE NOT NULL,\n thread_user_time BIGINT NOT NULL,\n thread_cpu_time BIGINT NOT NULL,\n KEY(process) USING CLUSTERED COLUMNSTORE,\n SHARD KEY(host, process)\n); To identify the slowest processes, say you want to calculate the percentiles of the connection delays in the 95th percentile, grouped by process and host. First, we run this using PERCENTILE_CONT, which calculates exact percentile distributions. Below is the query output using PERCENTILE_CONT. It takes over 30 seconds to execute. =>select PERCENTILE_CONT(0.95) WITHIN GROUP(ORDER BY connection_delay) as conn_delay_95, process, host from cluster_management\ngroup by process, host order by host, process;\n+--------------------+---------+----------+\n| conn_delay_95      | process | host     |\n+--------------------+---------+----------+\n|  60.33306084414562 | wls1    | lphost06 |\n|  74.22222222222223 | wls2    | lphost06 |\n|   90.1111189787824 | wls1    | lphost07 |\n|  97.16666666666667 | wls2    | lphost07 |\n|  48.22222222222222 | wls1    | lphost08 |\n|  81.22222222222223 | wls2    | lphost08 |\n|  504.6111111111111 | wls1    | lphost09 |\n|                114 | wls2    | lphost09 |\n| 257.94444404835895 | wls1    | lphost10 |\n| 398.77777777777777 | wls2    | lphost10 |\n| 244.72222222222223 | wls1    | lphost11 |\n| 141.49996132984242 | wls2    | lphost11 |\n| 158.60986670805664 | wls1    | lphost14 |\n|  53.91070471779503 | wls2    | lphost14 |\n|  95.72222222222223 | wls1    | lphost15 |\n|  330.3888921935945 | wls2    | lphost15 |\n| 337.77777777777777 | wls1    | lphost17 |\n|  300.2221403142701 | wls2    | lphost17 |\n|  323.6111111111111 | wls1    | lphost18 |\n| 388.27793939374266 | wls2    | lphost18 |\n+--------------------+---------+----------+\n20 rows in set (31.82 sec) The Reveal Now, we use APPROX_PERCENTILE to get approximate percentiles for the same percentage value. As you can see, it is 10X faster than the above and completes in a little over 3 seconds! select approx_percentile(connection_delay, 0.95) as conn_delay_95, process, host from cluster_management\ngroup by process, host order by host, process;\n+--------------------+---------+----------+\n| conn_delay_95      | process | host     |\n+--------------------+---------+----------+\n| 60.331255432672975 | wls1    | lphost06 |\n|  74.22222222222223 | wls2    | lphost06 |\n|  90.11111898411292 | wls1    | lphost07 |\n|  97.16666666666667 | wls2    | lphost07 |\n|  48.22222222222222 | wls1    | lphost08 |\n|  81.22222222222223 | wls2    | lphost08 |\n| 504.58873982961217 | wls1    | lphost09 |\n|                114 | wls2    | lphost09 |\n|   257.927873719216 | wls1    | lphost10 |\n|  398.8204351566121 | wls2    | lphost10 |\n| 244.72222578013086 | wls1    | lphost11 |\n| 141.49996122540992 | wls2    | lphost11 |\n| 158.59711333367832 | wls1    | lphost14 |\n|  53.91070683423085 | wls2    | lphost14 |\n|  95.72222222222223 | wls1    | lphost15 |\n| 330.42222794891194 | wls2    | lphost15 |\n| 337.77666235665123 | wls1    | lphost17 |\n|  300.2182932586068 | wls2    | lphost17 |\n|  323.6093433110946 | wls1    | lphost18 |\n| 388.30877260280937 | wls2    | lphost18 |\n+--------------------+---------+----------+\n20 rows in set (3.37 sec) The Margin of Error Let’s compare the difference between the exact calculation from PERCENTILE_CONT and APPROX_PERCENTILE. We gather the differences in the below table. +--------------------------------------------------+\n| abs(actual.conn_delay_95 - approx.conn_delay_95) |\n+--------------------------------------------------+\n|                       0.000000005330520025381702 |\n|                              0.00111542112654206 |\n|                             0.012753374378320359 |\n|                            0.0017678000164664809 |\n|                                                0 |\n|                              0.02237128149891987 |\n|                                                0 |\n|                             0.030833209066713607 |\n|                         0.0000021164358230407743 |\n|                             0.033335755317466464 |\n|                                                0 |\n|                            0.0018054114726453463 |\n|                                                0 |\n|                              0.04265737883434895 |\n|                        0.00000010443250175740104 |\n|                             0.016570329142950868 |\n|                                                0 |\n|                             0.003847055663300125 |\n|                         0.0000035579086272718996 |\n|                                                0 |\n+--------------------------------------------------+ The average difference between all the values in the distribution is 0.008. This represents a percent error of about 0.002%, which is very accurate! +-------------------------------------------------------+\n| avg(abs(actual.conn_delay_95 - approx.conn_delay_95)) |\n+-------------------------------------------------------+\n|                                  0.008353140031257311 |\n+-------------------------------------------------------+ Overall, you can use APPROX_PERCENTILE to speed up your percentile calculations without losing a lot of accuracy. More on APPROX_PERCENTILE Accuracy and Error Note that for some workloads, it is imperative to calculate exact percentiles. If that is the case, we would recommend continuing to use functions like PERCENTILE_CONT for this computation. You should use APPROX_PERCENTILE if your accuracy can accept some error tolerance. Our example above shows that our error is, on average, roughly 0.002 percent off from the exact numbers in our example calculations, which is pretty close. Note that the margin of error will differ across datasets. Specifically, the error threshold of APPROX_PERCENTILE depends directly on the size of data and the requested quantile. In general, we expect the approximate estimation to differ from the exact estimate within a limited range of the number itself [ -0.01,0.01 ] for a dataset of more than 100k rows. For example, if you have a uniform distribution from 0 to the 200, then the 50th percentile will be 100, 49th – 98, 51th – 102 percentiles respectively. In other words, when you run APPROX_PERCENTILE(column_x, 0.5) on this distribution, you should expect that the return value will be in the range between 98 to 102 from the actual percentile (100). Additionally, accuracy improves with more massive datasets and also as the percentage approaches the lower or upper bounds of the distribution (i.e., 1st and 99th percentile). Accuracy Argument By default, in SingleStore, the accuracy parameter for APPROX_PERCENTILE is 0.01. You can also vary this parameter from 0.01 to 0.5. Note that the larger this number, the less accurate the calculation is. However, reducing accuracy will improve performance. If the function expects lower accuracy, we store much fewer data points during processing, leading to a decrease in query execution time. We recommend you keep the default unless you require better performance. T-Digest Implementation SingleStore uses the remarkable T-digest algorithm to calculate APPROX_PERCENTILEs. T-digest is generated by clustering real-valued samples and retaining the mean and number of samples for each cluster, also known as a centroid. T-digest keeps a buffer of incoming samples. When the buffer fills, the contents are sorted and merged with the centroids computed from previous samples. The size bound for centroids is imposed using a scale function that forces clusters near the beginning or end of the digest to be small, possibly containing only a single sample. The scale function is chosen to provide an appropriate trade-off between a very accurate quantile estimate in the tails of a distribution, reasonable accuracy near the median while keeping the number of clusters as small as possible for performance. Conclusion APPROX_PERCENTILE allows us to do fast and accurate percentile calculations. The T-digest implementation in this function sorts only a small amount of data at a given time, whereas analogous percentile functions like PERCENTILE_CONT order all data on every partition, which leads to slower performance overall. You may leverage the APPROX_PERCENTILE function for more efficient calculations of percentiles across your dataset, mostly if you are operating on massive datasets, and you can accept a small margin of error for this calculation. Download SingleStoreDB here and try APPROX_PERCENTILE today!", "date": "2021-01-05"},
{"website": "Single-Store", "title": "where-should-top-coders-work", "author": ["Nikita Shamgunov"], "link": "https://www.singlestore.com/blog/where-should-top-coders-work/", "abstract": "My career as a software engineer really began when I won a medal at the ACM ICPC programming contest in 2001. To place in the tournament, I had spent 24 hours traveling from Russia to Vancouver and back, just to spend 5 hours on the actual competition. The rules are simple: you have 5 hours to solve up to 12 problems. For each problem you need to implement a small program in Java or C++ and send it to the jury. They compile it and run it through an extremely intensive set of tests. Only if it passes every test will the jury count your submission. Even though it only marked the beginning of my career, it had taken 4 years of intense preparation to place at the tournament – 4 years of training, learning to think fast, practicing on weekends, and other sorts of mental gymnastics. ACM was a good school for me. I loved it, but I wasn’t alone. Is it really hard to become a Top Coder? Becoming an ACP ICPC medalist in 2011 is about 20 times harder than it was in 2001. People who can do it now are coding machines and algorithm junkies. They practice every day on TopCoder, Google Code Jam, and Code Force to stay sharp. They memorize whole books filled with algorithms and equations in case they need a tool to solve a problem quicker than their opponent. It should be no surprise that this programming subculture has caught the attention of many great companies. So where do programming contest winners go and work? It used to be Microsoft (where I started my career), then Google. Now it’s Facebook. These companies know how valuable top coders are. Microsoft, Google and Facebook wouldn’t be where they are without exceptional engineers. How do Top Coders fare after graduation? They fare very well and tend to build their careers at big companies. Many are attracted to the high salary right out of school. However, according to topcoder.com , many ultimately find work at large companies to be tedious and less challenging than the mental jujitsu of programming contests. Large teams have the luxury of compartmentalizing a problem to reduce complexity. This creates an unfortunate side effect: these smaller problems just aren’t that interesting. In addition, many ICPC champions soon miss the dynamic of small teams, the kind they experienced while training and competing during university. As it so happens, there is a time in a company’s history when it’s perfect for a top coder to join – when the company’s just getting started. Why do some Top Coders found/join an early-stage startup? For the challenge, of course. Just take a look at these real-life examples: Adam D’Angelo was a finalist in the international Topcoder Collegiate Challenge in 2005. Later he was VP and CTO of Facebook and then left to found Quora. Nikolai Durov is Employee #1 at the biggest Russian social network vkontakte . It beats Facebook on the Russian market. He could’ve gone to Google or stayed in academia, but he made a small bet that had lots of upside. One of my team members Leonid Volkov went to an early stage company and built the “TurboTax of Russia”. He enjoyed an incredibly successful exit, and he’s since gone into politics. Prasanna Sankaranarayanan is the founder of LikeALittle and was the highest ranked Top Coder in India. It took him one year at Microsoft to realize that the perfect job for a Top Coder is at an early stage startup. Small startups offer Top Coders lots of responsibility and with it the trust and autonomy to solve tough problems. Top Coders at SingleStore Today, SingleStore has four Top Coders. I won a bronze medal in ACM ICPC in 2001, and Alex Skidanov was a Top Coder #13 in 2008 and ACM ICPC #3 Champion. A Top Coder would feel right at home at SingleStore. Here, we work on hard algorithmic and systems-level problems, distributed systems, and cloud infrastructure. We also now have Top Coders who ranked #4 and #8 in algorithms. Sometimes it’s scary to leave a big corporation, but the truth is that apart from fun, market pay, and the potential of a huge upside, a good early stage startup gives the kind of experience that makes a Top Coder extremely relevant in today’s tech industry. If you’d like to learn more, shoot us an email at topcoders@singlestore.com .", "date": "2012-06-18"},
{"website": "Single-Store", "title": "welcome-to-the-memsql-developer-blog", "author": ["Adam Prout"], "link": "https://www.singlestore.com/blog/welcome-to-the-memsql-developer-blog/", "abstract": "We’ve been hard at work building the world’s fastest database, and now that we’re shipping SingleStore, we’re looking forward to having a bit more time to blog about some of the fundamentals around the SingleStore technology. In the coming weeks, we’ll be publishing posts that cover an array of topics, including benchmarking, stress testing, database theories, algorithms, and more. In the mean time, we encourage you to download SingleStore and have some fun doing your own benchmarking. We’ve also published a workload simulator on Github to help get you started in the right direction. Thanks for dropping by and we look forward to a lot of great posts and discussions.", "date": "2012-06-18"},
{"website": "Single-Store", "title": "loading-half-a-billion-records-into-memsql", "author": ["Nikita Shamgunov"], "link": "https://www.singlestore.com/blog/loading-half-a-billion-records-into-memsql/", "abstract": "Disclaimer. This will not be an apples to apples comparison with derwik, since we obviously don’t have the same dataset, and we need a much bigger machine to load everything into memory. But I believe this experiment will get the point across. So without further ado, let’s go through the steps. Adam Derewecki wrote a cool post about his experience loading half a billion records into MySQL. SingleStore is a MySQL-compatible drop-in replacement that is built from the ground up to run really fast in memory. Disclamer: this isn’t an apples-to-apples comparison with derwik since we don’t have his dataset and need a much beefier machine to load everything into memory. Schema Here is the schema for the table. Note that the table has three indexes. On top of it memsql will automatically generate a primary index under the covers. We won’t be disabling keys. drop table if exists store_sales_fact;\ncreate table store_sales_fact( \n       date_key                smallint not null,\n       pos_transaction_number  integer not null,\n       sales_quantity          smallint not null,\n       sales_dollar_amount     smallint not null,\n       cost_dollar_amount      smallint not null,\n       gross_profit_dollar_amount smallint not null,\n       transaction_type        varchar(16) not null,\n       transaction_time        time not null,\n       tender_type             char(6) not null,\n       product_description     varchar(128) not null,\n       sku_number              char(12) not null,\n       store_name              char(10) not null,\n       store_number            smallint not null,\n       store_city              varchar(64) not null,\n       store_state             char(2) not null,\n       store_region            varchar(64) not null,\n       key date_key(date_key),\n       key store_region(store_region),\n       key store_state(store_state)\n); Hardware We have a very cool machine, with 64 cores and 512 GB of RAM, from Peak Hosting . You can rent one for yourself for a little under two grand a month. They were kind enough to give it to us to use for free. Here is a spec of one core. vendor_id       : AuthenticAMD\ncpu family      : 21\nmodel           : 1\nmodel name      : AMD Opteron(TM) Processor 6276\nstepping        : 2\ncpu MHz         : 2300.254\ncache size      : 2048 KB You read that correctly, this machine has sixty-four 2.3 GHz cores and 512 GB of RAM or almost 8 times the largest memory footprint available in the cloud today, all on dedicated hardware with no virtualization overhead or resource contention with other unknown third parties. Loading efficiently Loading data efficiently is actually not that trivial. The best way of doing it with SingleStore is to use as much CPU as you can get. Here are a few tricks that can be applied. 1 . Multi-inserts We can batch inserts into 100 row multi-inserts. This will reduce the number of network roundtrips. Each roundtrip now accounts for 100 rows instead of one. Here is what multi-inserts look like. insert into store_sales_fact values('1','1719','4','280','97','183','purchase','14:09:10','Cash','Brand #6 chicken noodle soup','SKU-#6','Store71','71','Lancaster','CA','West'),\n('1','1719','4','280','97','183','purchase','14:09:10','Cash','Brand #5 golf clubs','SKU-#5','Store71','71','Lancaster','CA','West'),\n('1','1719','4','280','97','183','purchase','14:09:10','Cash','Brand #4 brandy','SKU-#4','Store71','71','Lancaster','CA','West'), 2 . Load in parallel. Our customer has a sample file of  510,593,334 records. We can use the command line mysql client to pipe this file into SingleStore, but this would not leverage all the cores available in the system. So ideally we should spit the file to at least as many chunks as there are CPUs in the system. 3 . Increase granularity Splitting the file into 64 big chunks will introduce a data skew. The total data load time will be the time the slowest thread loads the data. To address this problem we will split the file into thousands of chunks. And every time a thread frees up we will start loading another chunk. So we split the file into 2000 chunks. 1 -rw-r--r--  1  35662844 2012-06-06 11:42 data10.sql\n 2 -rw-r--r--  1  35651723 2012-06-06 11:42 data11.sql\n 3 -rw-r--r--  1  35658433 2012-06-06 11:42 data12.sql\n 4 -rw-r--r--  1  35663665 2012-06-06 11:42 data13.sql\n 5 -rw-r--r--  1  35667480 2012-06-06 11:42 data14.sql\n 6 -rw-r--r--  1  35659549 2012-06-06 11:42 data15.sql\n 7 -rw-r--r--  1  35661617 2012-06-06 11:42 data16.sql\n 8 -rw-r--r--  1  35650414 2012-06-06 11:42 data17.sql\n 9 -rw-r--r--  1  35661625 2012-06-06 11:42 data18.sql\n10 -rw-r--r--  1  35667634 2012-06-06 11:42 data19.sql\n11 -rw-r--r--  1  35662989 2012-06-06 11:42 data1.sql 4 . Load script. The load script uses python multiprocessing library to load data efficiently. 1 import re\n 2 import sys\n 3 import os\n 4 import multiprocessing\n 5 import optparse\n 6\n 7 parser = optparse.OptionParser()\n 8 parser.add_option(\"-D\", \"--database\", help=\"database name\")\n 9 parser.add_option(\"-P\", \"--port\", help=\"port to connect. use 3306 to connect to memsql and 3307 to connect to mysql\", type=\"int\")\n 10 (options, args) = parser.parse_args()\n 11\n 12 if not options.database or not options.port:\n 13     parser.print_help()\n 14     exit(1)\n 15\n 16 total_files = 2000\n 17\n 18 def load_file(filename):\n 19     try:\n 20         print \"loading from cpu: %d\" % os.getpid()\n 21         query = 'mysql -h 127.0.0.1 -D %s -u root -P %d < %s' % (options.database, options.port, filename)\n 22         print query\n 23         os.system(query)\n 24         print \"done loading from cpu: %d\" % os.getpid()\n 25     except e as Exception:\n 26         print e\n 27         pass\n 28\n 29 os.system('echo \"delete from store_sales_fact\" | mysql -h 127.0.0.1 -u root -P %d' % options.port)\n 30 p = multiprocessing.Pool(processes = 2*multiprocessing.cpu_count())\n 31 for j in range(0, total_files):\n 32     p.apply_async(load_file, ['data/data%d.sql' % j])\n 33\n 34 p.close()\n 35 p.join() 5 . Running it I started loading by issuing the following command. time python load.py -D test -P 3306 After we do this let’s start htop to check the processor saturation. It looks pretty busy. SingleStore uses lockfree data structures that eliminate a lot of contention. It took a bit of time, but the data has been loaded. real    39m21.465s\n user    33m53.210s\n sys     5m24.470s The result is almost 200K inserts a second for a table with 4 indexes. The memory footprint of the memsql process is 267 Gb. Running the same test against mysql I would not be fair to skip comparison with mysql, particularly that it’s so easy to do since memsql uses mysql wire protocol and support mysql syntax. I used the my.cnf settings from http://derwiki.tumblr.com/post/24490758395/loading-half-a-billion-rows-into-mysql and fired up the same command time python load.py -D test -P 3307 real    536m0.376s\nuser    27m17.130s\nsys     5m36.780s I did not disable indexes for this run to make it fair compared to SingleStore. Conclusion With the Peak Hosting offering of 512Gb of RAM and memory optimized software like SingleStore you can save full business days in data loading and get immediate, valuable insight into your data.", "date": "2012-08-13"},
{"website": "Single-Store", "title": "excellent-post-on-memsql-architecture", "author": ["Nikita Shamgunov"], "link": "https://www.singlestore.com/blog/excellent-post-on-memsql-architecture/", "abstract": "http://highscalability.com/blog/2012/8/14/memsql-architecture-the-fast-mvcc-inmem-lockfree-codegen-and.html ", "date": "2012-08-15"},
{"website": "Single-Store", "title": "my-1st-real-startup-experience-as-an-intern", "author": ["Nikita Shamgunov"], "link": "https://www.singlestore.com/blog/my-1st-real-startup-experience-as-an-intern/", "abstract": "This is reposted from Momchil Tomov’s blog . Momchil was part of the first summer batch of SingleStore Interns. After seemingly stumbling into their office by accident, getting interviewed on a lark, and receiving an offer as my Christmas gift, I kept an open mind for what to expect from SingleStore . The one-year-old YC alum was set to build the world’s fastest database, leaving competitors like MySQL and MongoDB in the dust. Very ambitious indeed. From day one, I was thrown in the fire pit. Someone had to finish the Workload Simulator , an important developer tool, before the launch. Everyone was super busy fixing bugs and polishing the product, so I had to do a quick Javascript/Flask/Python crash course and jump on it. After eight long nights, a thousand lines of code, and a huge amount of support from Ankur, my mentor, the Workload Simulator was finished to coincide with launch. I spent the next month working on code generation: writing code that compiles SQL to C++, the core strength of SingleStore. Once I got acquainted with the codebase, Ankur decided to step up my game with data compression. I had to find a way to compress 10 TB of random strings down to at most 5 TB without compromising the speed of the transactions, and I had two weeks to do so. After some research, experimentation, and lots of dumb luck, I managed to bring it down to 2 TB. It required all the SingleStore knowledge I had obtained thus far, including things I learned during my interview. Just as I finished the first prototype, I joined the team working on sharding. Sharding is what enables SingleStore to get distributed across multiple machines, an important feature of every scalable database and also an important feature for our biggest clients. I was onboarded on Sunday, began work on Monday, and shipped my first code review on Tuesday. One of the dev leads, Ankur, gauged my progress, and kept assigning me more and more sharding tasks, slowly walking me through the implementation of Distributed SingleStore. Just as a father silently lets go of the back seat of his kid’s bicycle, he slowly backed away and moved on to testing and bug fixing. Before realizing it, I found myself working on the team with no safety wheels, full speed ahead, pushing the forefront of SingleStore. I was no longer an intern. I was a full-time SingleStore engineer. They say Princetonians work hard and play really really hard. If that’s true, then SingleStore felt even more like home. From the extraordinarily delicious meals prepared by our in-house chef Daniel, to the post-launch Las Vegas Celebration Trip, to the relaxing Saturdays on Vasili’s boat underneath the Golden Gate, the SingleStore team knows how to have a good time. As SingleStore is growing, I sincerely hope they maintain their great culture of ridiculous engineering and chill free time. I had an amazing summer many thanks to the incredible people at 380 10th Street. I wish Eric, Nikita, Marko, Vasili, Alex, Masha, Nika, Pieguy, Adam, Ankur, Daniel, et al. all the best and hope they build a company 300x better than the rest! Go SingleStore!", "date": "2012-09-11"},
{"website": "Single-Store", "title": "where-should-topcoders-work-one-year-later", "author": ["Nikita Shamgunov"], "link": "https://www.singlestore.com/blog/where-should-topcoders-work-one-year-later/", "abstract": "A little over a year ago we published a blog post “ Where Should Top Coders Work? ”. In a few weeks we were approached by topcoder.com and offered to sponsor TopCoder Open. We hired two amazing folks after that event, and two more joined as interns. With a handful of red TopCoders working on the engine, we’ve had a great relationship with the TopCoder community. When I published the original blog post, I had absolute faith into the TopCoder brand, but we had nothing to really back it up. Since a year has passed, I thought it’d be worthwhile to share what our Top Coders have been working on. What have TopCoders built at SingleStore? Here at SingleStore, engineers have a lot of freedom in deciding what to work on. Instead of saying “we have a great team solving hard problems,” I’d like to share what each TopCoder has worked on since joining the company. Pieguy . David fell in love with concurrency and lock-free data structures. He has built/improved our entire lock-free story.  David is also great at finding tough race conditions by studying the code and investigating various concurrency scenarios including finding a bug in a well-known, lock-free paper ( http://www.research.ibm.com/people/m/michael/podc-1996.pdf ). TopCoder trains people to read and understand code very well. Nika . Nika tackled the challenges in implementing joins and subqueries, which included building an updated optimizer, an artificial intelligence component that decides what indexes and what join order to use in a complex query. Momchil . As an intern, Momchil worked on various components of the codebase, but he really got hooked working on distributed query execution. By the end of his internship SingleStore was able to run queries like the one below in a distributed environment over a 100 node cluster (actual use case): select x.i, x.a, x.b, o.c, o.x from (select * from t) as x left join (select c, count(*) as x from ob group by c) o on x.a = o.c Check out Momchil’s blog in which he writes about his experience at SingleStore. Decowboy . While interning at SingleStore, Jelle worked on replication. He helped with the robust design and implemented major components of the feature. This is a highly technical project that involved transaction log shipping and recovery on the replica, failover, and resiliency to all kinds of DDL (data definition language) such as create/drop table and create drop database, etc. SkidanovAlex . Alex is the first SingleStore TopCoder and he has contributed to all the major components of SingleStore. He has been a crucial contributor to SingleStore’s distributed product. The list can go on and it’s just the highlights of what the TopCoder crew has built. What can TopCoders learn at SingleStore? Databases are complex at every level: storage engines, optimizers, parsers, distributed systems – things most regular developers don’t work with on a day-to-day basis. In a famous blog post ( http://herbsutter.com/welcome-to-the-jungle/ ), Herb Sutter claims: Applications will need to be at least massively parallel, and ideally able to use non-local cores and heterogeneous cores. Efficiency and performance optimization will get more, not less, important. Programming languages and systems will increasingly be forced to deal with heterogeneous distributed parallelism. SingleStore is at the forefront of building massively parallel software. For engineers who possess a solid foundation in problem solving with algorithms and data structures, SingleStore is a great place to build these skills. Why do TopCoders succeed at SingleStore? Apart from the obvious such as an incredibly high IQ, excellent problem solving skills, and proficiency in algorithms, there is one trait that we find very attractive: TopCoders love to code. If you code a lot every day it generates a compound effect. You are getting much better and more productive. Startup vs Big Company Growth and opportunity are crucial for a successful engineering culture. Unfortunately, big company environments often hinder growth potential because of bureaucracy and a lack of emphasis on engineering culture. We are lucky to have very hard problems in spades and have no bureaucratic roadblocks for a TopCoder to grow and advance his or her career. Are we going to hire more TopCoders? We are happy to announce that we are sponsoring TopCoder Open once again and to keep our doors open for more amazing engineers coming out of this unique community. Being a TopCoder is not a prerequisite for working at SingleStore, but being a great engineer is. Please find us at careers@singlestore.com", "date": "2012-09-23"},
{"website": "Single-Store", "title": "common-pitfalls-in-writing-lock-free-algorithms", "author": ["David Stolp"], "link": "https://www.singlestore.com/blog/common-pitfalls-in-writing-lock-free-algorithms/", "abstract": "Formally, a multi-threaded algorithm is considered to be lock-free if there is an upper bound on the total number of steps it must perform between successive completions of operations. The statement is simple, but its implications are deep – at every stage, a lock-free algorithm guarantees forward progress in some finite number of operations. (That’s why lock-free algorithms are used in SingleStore skiplists and, more recently, in sync replication – Ed.) Deadlock is impossible. The promise of a lock-free algorithm seems remarkable in theory. Concurrent threads can modify the same object, and even if some thread or set of threads stalls or stops completely in the middle of an operation, the remaining threads will carry on as if nothing were the matter. Any interleaving of operations still guarantees of forward progress. It seems like the holy grail of multi-threaded programming. This stands in contrast to the traditional method of placing locks around “critical sections” of code. Locking prevents multiple threads from entering these critical sections of code at the same time. For highly concurrent applications, locking can constitute a serious bottleneck. Lock-free programming aims to solve concurrency problems without locks. Instead, lock-free algorithms rely on atomic primitives such as the classic “compare-and-swap” which atomically performs the following: bool CompareAndSwap(Value* addr, Value oldVal, Value newVal){\nif(*addr == oldVal){\n*addr = newVal;\nreturn true;\n}else{\nreturn false;\n}\n} The biggest drawbacks to using lock-free approaches are: Lock-free algorithms are not always practical. Writing lock-free code is difficult. Writing correct lock-free code is extremely difficult. To illustrate the third point above, let’s break down a typical first attempt at writing a lock-free stack. The idea is to use a linked-list to store nodes, and use CompareAndSwap to modify the head of the list, in order to prevent multiple threads from interfering with each other. To push an element, we first create a new node to hold our data. We point this node at the current head of the stack, then use CompareAndSwap to point the head of the stack to the new element. The CompareAndSwap operation ensures that we only change the head of the stack if our new node correctly points to the old head (since an interleaving thread could have changed it). To pop an element, we snapshot the current head, then replace the head with the head’s next node. We again use CompareAndSwap to make sure we only change the head if its value is equal to our snapshot. The code, in C++ template\nclass LockFreeStack{\nstruct Node{\nEntry* entry;\nNode* next;\n};\n\nNode* m_head;\n\nvoid Push(Entry* e){\nNode* n = new Node;\nn->entry = e;\ndo{\nn->next = m_head;\n}while(!CompareAndSwap(&m_head, n->next, n));\n}\n\nEntry* Pop(){\nNode* old_head;\nEntry* result;\ndo{\nold_head = m_head;\nif(old_head == NULL){\nreturn NULL;\n}\n}while(!CompareAndSwap(&m_head, old_head, old_head->next));\n\nresult = old_head->entry;\ndelete old_head;\nreturn result;\n}\n} Unfortunately, this stack is riddled with errors. Segfault The Push() method allocates memory to store linked-list information, and the Pop() method deallocates the memory. However, between the time a thread obtains a pointer to a node on line 22, and subsequently accesses the node on line 26, another thread could have removed and deleted the node, and the thread will crash. A safe way to reclaim the memory is needed. Corruption The CompareAndSwap method makes no guarantees about whether the value has changed, only that the new value is the same as the old value. If the value is snapshotted on line 22, then changes, then changes back to the original value, the CompareAndSwap will succeed. This is known as the ABA problem. Suppose the top two nodes on the stack are A and C . Consider the following sequence of operations: Thread 1 calls pop, and on line 22 reads m _ head ( A ), on line 26 reads old _ head->next ( C ), and then blocks just before calling CompareAndSwap. Thread 2 calls pop, and removes node A . Thread 2 calls push, and pushes a new node B . Thread 2 calls push again, and this time pushes a new node occupying the reclaimed memory from A . Thread 1 wakes up, and calls CompareAndSwap. The CompareAndSwap on line 26 succeeds even though m _ head has changed 3 times, because all it checks is that old _ head is equal to m _ head. This is bad because now m _ head will point to C , even though B should be the new head. Not lock-free The C++ standard makes no guarantee that new and delete will be lock-free. What good is a lock-free data structure that makes calls to non-lock-free library calls? For our stack to be truly lock-free, we need a lock-free allocator. Data races When one thread writes a value to a location in memory, and another thread simultaneously reads from the same location, the result is undefined in C++ unless std::atomic is used. The reads and writes both need to be atomic, and C++ makes no such guarantee for primitive types. Prior to C++11, it was a common practice to use volatile for atomic variables, however this approach is fundamentally flawed . In this case, threads reading the head pointer constitute data races, both in Push and Pop, because another thread could be simultaneously modifying it. Memory reordering It is convenient to think of computers as being sequentially consistent machines, where every load or store can be placed into a single total order that is consistent with every thread. At the very least, we would expect the “happens before” relationship to be transitive. Unfortunately this is not the case, and in both theory and practice the following sequence of events is possible, with x and y initialized to 0: thread 1 thread 2 print x x.store(1) print y print y y.store(1) print x output: thread 1 thread 2 x -> 0 y - > 0 y - > 0 x - > 0 This would seem to imply that the printing of x happened before the store of x, which happened before the printing of y, which happened before the store of y, which in turn happened before the printing of x. This is just one example of how different threads may see changes to memory occur in different orders. Furthermore, it is possible for loads and stores to be reordered within a single thread, both by the compiler and by the hardware. In the above lock-free stack a reordered store could result in another thread seeing a stale version of a node’s next pointer and subsequently dereferencing an invalid memory location. How to write a correct lock-free stack Most of the above problems have many different solutions. Here I’ll describe the approaches I use at work for our lock-free stack. Segfault Before dereferencing a node, we need to make sure it cannot be deleted. Each thread has a special globally visible pointer called a “hazard pointer”. Before accessing the head node, a thread will set its hazard pointer to point to that node. Then, after setting the hazard pointer it makes sure the node is still the head of the stack. If it is still the head, then it is now safe to access. Meanwhile, other threads that may have removed the node from the stack cannot free the associated memory until no hazard pointer points to this node. For efficiency, the deletion process is deferred to a garbage collection thread that amortizes the cost of checking hazard pointers. Note that the ABA problem does not have any consequences here. It does not matter if the head changed between the two reads. As long as the hazard pointer has been continuously pointing to a node for some interval of time that includes a moment in which the node belonged to the stack, then it is safe to access the node. Corruption One way to avoid the ABA problem is to ensure that the head of the stack never has the same value twice. We use “tagged pointers” to ensure uniqueness of head values. A tagged pointer contains a pointer and a 64-bit counter. Every time the head changes, the counter is incremented. This requires a double-width CompareAndSwap operation, which is available on most processors. Not lock-free Instead of allocating a separate node to store each entry, the “next” pointer is embedded in the entry itself, thus no memory allocation is needed in order to push it onto the stack. Furthermore, we may reuse the same node multiple times, recycling rather than deleting. Requiring an extra 8 bytes per entry is an unattractive property if you want a stack for small data structures, but we only use the stack for large items where the 8 bytes is insignificant. Data races We currently use boost::atomic for this. Our compiler is gcc 4.6, which currently supports std::atomic, but its implementation is much less efficient than boost’s. In gcc 4.6, all operations on atomic variables apply expensive memory barriers, even when a memory barrier is not requested. Memory reordering C++11 provides a new memory model and memory ordering semantics for atomic operations that control reordering. For our lock-free stack to work correctly, CompareAndSwap needs to be performed with sequentially consistent memory semantics (the strongest kind). Sequential consistency means that all such operations can be put into a total order in which all threads observe the operations. There is a very easy to miss race condition (that went unnoticed in our code for months) involving the aforementioned hazard pointers. It turns out that hazard pointers also need to be assigned using sequentially consistent memory semantics. Here’s the case that broke our stack: Thread 1, in preparation for a Pop operation, reads the head of the stack. Thread 1 writes the current head to its hazard pointer (using only release semantics, which are weaker than sequentially consistent semantics). Thread 1 reads the head of the stack again. Thread 2 removes head from the stack, and passes it to the garbage collector thread (using sequentially consistent memory semantics). The garbage collector scans the hazard pointers, and (because the assignment was not done with sequentially consistent memory semantics) is not guaranteed to see thread 1’s hazard pointer pointing to the node. The garbage collector deletes the node Thread 1 dereferences the node, and segfaults. With sequentially consistent memory semantics applied to both the hazard pointer assignment and head modification, the race condition cannot happen. This is because for any two sequentially consistent operations, all threads must see them happen in the same order. So if thread 2 removing the pointer happens first, thread 1 will see a different value on its second read and not attempt to dereference it. If thread 1 writes to its hazard pointer first, the garbage collector is guaranteed to see that value and not delete the node. Performance Now that we’ve solved the issues with our stack, let’s take a look at a benchmark. This test was performed on an 8 core Intel(R) Xeon(R) machine. The workload consisted of random operations, with Push and Pop equally likely. The threads were not throttled in any way – all threads performed as many operations as the machine could handle. Well, that’s not very impressive. The stack has a single point of contention, and is limited by the rate at which we can modify the head of the stack. As contention increases, so does the number of failed CompareAndSwap operations. A simple yet highly effective way to reduce the number of failed operations is simply to sleep after each failed operation. This naturally throttles the stack to a rate it can effectively handle. Below is the result of adding “usleep(250)” after any failed operation. That’s right, the addition of a sleep improved throughput by roughly 7x! Furthermore, the addition of usleep(250) reduced processor utilization. Take a look at the output from HTOP while running each benchmark for 16 threads. Locked stack: Lock-free stack, no sleep: Lock-free stack with usleep(250): So lock-free is better, right? Well, the lock can be improved as well. Instead of using std::mutex, we can use a spinlock that performs the same usleep(250) each time it fails to acquire the lock. Here are the results: Results Under high contention, additional threads dramatically reduced overall throughput. The addition of a sleep reduced contention, and consequently both increased throughput while decreasing processor utilization. Beyond 3 threads all stacks showed a relatively level performance. None of the multi-threaded stacks performed better than a single-threaded stack. Conclusion Lock-free guarantees progress, but it does not guarantee efficiency. If you want to use lock-free in your applications, make sure that it’s going to be worth it – both in actual performance and in added complexity. The lock-free stack presented at the beginning of this article contained a multitude of errors, and even once the errors were fixed it performed no better than the much simpler locked version. Avoid trying to write your own lock-free algorithms, and look for existing implementations instead. Code Lock-based. class LockFreeStack{\n#include\n#include\n\ntemplate\nclass LockedStack{\npublic:\nvoid Push(T* entry){\nstd::lock_guard lock(m_mutex);\nm_stack.push(entry);\n}\n\n// For compatability with the LockFreeStack interface,\n// add an unused int parameter.\n//\nT* Pop(int){\nstd::lock_guard lock(m_mutex);\nif(m_stack.empty()){\nreturn nullptr;\n}\nT* ret = m_stack.top();\nm_stack.pop();\nreturn ret;\n}\n\nprivate:\nstd::stack<T*> m_stack;\nstd::mutex m_mutex;\n}; Lock-free. The garbage collection interface is omitted, but in real applications you would need to scan the hazard pointers before deleting a node. The cost of the scan can be amortized by waiting until you have many nodes to delete, and snapshotting the hazard pointers in some data structure with sublinear search times. class LockFreeStack{\npublic:\n// The elements we wish to store should inherit Node\n//\nstruct Node{\nboost::atomic<Node*> next;\n};\n\n// Unfortunately, there is no platform independent way to\n// define this class. The following definition works in\n// gcc on x86_64 architectures\n//\nclass TaggedPointer{\npublic:\nTaggedPointer(): m_node(nullptr), m_counter(0) {}\n\nNode* GetNode(){\nreturn m_node.load(boost::memory_order_acquire);\n}\n\nuint64_t GetCounter(){\nreturn m_counter.load(boost::memory_order_acquire);\n}\n\nbool CompareAndSwap(Node* oldNode, uint64_t oldCounter, Node* newNode, uint64_t newCounter){\nbool cas_result;\n__asm__ __volatile__\n(\n\"lock;\" // This makes the following instruction atomic (it is non-blocking)\n\"cmpxchg16b %0;\" // cmpxchg16b sets ZF on success\n\"setz %3;\" // if ZF set, set cas_result to 1\n\n: \"+m\" (*this), \"+a\" (oldNode), \"+d\" (oldCounter), \"=q\" (cas_result)\n: \"b\" (newNode), \"c\" (newCounter)\n: \"cc\", \"memory\"\n);\nreturn cas_result;\n}\nprivate:\nboost::atomic<Node*> m_node;\nboost::atomic m_counter;\n}\n// 16-byte alignment is required for double-width\n// compare and swap\n//\n__attribute__((aligned(16)));\n\nbool TryPushStack(Node* entry){\nNode* oldHead;\nuint64_t oldCounter;\n\noldHead = m_head.GetNode();\noldCounter = m_head.GetCounter();\nentry->next.store(oldHead, boost::memory_order_relaxed);\nreturn m_head.CompareAndSwap(oldHead, oldCounter, entry, oldCounter + 1);\n}\n\nbool TryPopStack(Node*& oldHead, int threadId){\noldHead = m_head.GetNode();\nuint64_t oldCounter = m_head.GetCounter();\nif(oldHead == nullptr){\nreturn true;\n}\nm_hazard[threadId*8].store(oldHead, boost::memory_order_seq_cst);\nif(m_head.GetNode() != oldHead){\nreturn false;\n}\nreturn m_head.CompareAndSwap(oldHead, oldCounter, oldHead->next.load(boost::memory_order_acquire), oldCounter + 1);\n}\n\nvoid Push(Node* entry){\nwhile(true){\nif(TryPushStack(entry)){\nreturn;\n}\nusleep(250);\n}\n}\n\nNode* Pop(int threadId){\nNode* res;\nwhile(true){\nif(TryPopStack(res, threadId)){\nreturn res;\n}\nusleep(250);\n}\n}\n\nprivate:\nTaggedPointer m_head;\n// Hazard pointers are separated into different cache lines to avoid contention\n//\nboost::atomic<Node*> m_hazard[MAX_THREADS*8];\n}; The code is also available on github (if you want to test it locally): https://github.com/memsql/lockfree-bench", "date": "2013-03-27"},
{"website": "Single-Store", "title": "test-coverage-manage-long-tail-surfaced-bugs-using-force-multipliers", "author": ["Nikita Shamgunov"], "link": "https://www.singlestore.com/blog/test-coverage-manage-long-tail-surfaced-bugs-using-force-multipliers/", "abstract": "High quality is hard to achieve and very expensive, but it’s worth every penny and must be taken extremely seriously. There is no silver bullet – just lines of defense. The good news is with the proper lines of defense, quality becomes incremental. It only goes up with every release. With enough test coverage and quality tools you can substantially increase the quality of your product and protect it from regressions. When you embark upon a large software project you need to figure out how to think about quality. And you should start with testing. Not all tests are created equal. Here are the most common types: Unit tests Invariant tests (asserts, etc) Functional tests Stress tests Performance tests Scalability tests Customer workload based testing Scenario based testing In this blog post we will discuss test coverage as well as how to manage the long tail of rarely surfaced bugs using force multipliers – a slew of engineering techniques that transform our existing test suite to produce new ways of testing the product. Each of these has been a small “aha” moment and yielded good results. Put Your Test Plan on Paper: The Test Matrix With testing, one of the hardest things is deciding exactly what to test. Every test you create or run has a cost, and this is where a tester’s intuition shines. When your product has a lot of features it’s easy to poke around, but the real question is how can you do it in a systematic way. It’s important to track which areas of the product are tested well and which are not, and this is made all the more difficult by the many moving parts of the codebase. Here are a few SingleStore examples: Correctness of read queries in the presence of write queries Correctness of replication in the presence of alter table It helps tremendously to test combinations of features. It’s especially hard because different people or even teams could work on those features and the problem may only arise when both features are involved at the same time. The number of possible combinations explodes very quickly. However, we’ve noticed that in practice the majority of bugs surface in the combination of just two features. So, one way to tackle the combinatorial explosion is to only consider pairs of features and write tests for every pair. You can also employ n-wise testing and use a different combination every time. On top of that you should also think about dimensions. A classical test dimension for a SQL database is “all data types” or “all possible positions within a SQL query such as the where clause, select clause, group by, etc.” When testing a specific feature you want to test it with every dimension. We wrote down all feature pairs and dimensions into a giant spreadsheet. After that we fanned out the test work to the engineering team to color the matrix green. Invest Into A Stellar Test Execution System You can write a lot of tests, but it is continuous integration testing that allows developers to make far-reaching changes without a paralyzing fear of undetected breaks [ 1 ] . Such integration testing should be fast and scalable. You need stellar test infrastructure to frictionlessly add, manage, and run these tests. With a well-built execution system, the number of functional tests can grow safely and quickly. Building good test infrastructure is similar to writing a consumer product for your engineering team. The easier it is to produce tests, the more the engineering team is engaged and more tests are running against your system every day. In the ideal world, when you modify a single line of code you should instantly know what tests you just broke. Practically it’s not always possible because of the computational complexity of test runs, but you should push towards getting instant feedback as much as possible. A perfect test execution system would run the entire test suite after every push. We used to run every test before commits on each developer’s machine. When that started taking too long, we ran only a subset of tests locally and performed full test runs every night on a dedicated box. When the dedicated box couldn’t handle the load, we built a cloud-based test execution platform called “Psyduck.” The Psyduck platform can run any subset of our tests against any version of our codebase.  Furthermore, if you want to run tests against local code changes, Psyduck accepts patches, and so we enforce running Psyduck before pushing code. For example: memcompute1:~/memsql/memsqltest nikita(arcpatch-D2497)$ ./psy test --filter=.nikita\nCompressing patch...done.\nSending 5340 bytes to S3.....done.\npatch id = 901be82c33ee4b07b142d102660e3206Psyduck uses a combination of in-house hardware along with hundreds of Amazon spot instances to run tests as fast as possible in parallel. Last summer a SingleStore intern built a real-time visualization layer called Liveduck and it became an instant hit.  This interface displays all ongoing and recently completed tests along with metadata such as the associated engineer and the pass/fail count. Psyduck provides us with a quick metric on where we stand now and where we need to be at release. As we approach a release, the team pushes towards what we call “green Psyduck”. This means that all tests are passing. Another property of an excellent test system is that every test must run the same on a developer’s machine as it does on the platform.  Furthermore, each test should be self-contained so that it requires zero setup.  This will greatly assist in test suite maintenance and accelerate failure investigation. Force Multipliers Every one of these tricks took SingleStore quality to the next level. Here is the list. Transforms Transforms allow to put a twist on all existing tests. By implementing a few functions in Python you can munge both the incoming query and the expected output. Then, you can run your entire functional test suite via a transform. SingleStore has many transforms. Here are few: Replication transform. All the read queries are directed to a replication replica. Subquery transform. Every read query is wrapped into subquery Backup restore transform. As queries run, the transform performs backups and restore and makes sure the results of the queries stay the same We have had internal transform competitions, where the goal is to write the transform which finds the most bugs. When you write transforms you feel like you are standing of the shoulders of giants. With a few lines of code, you can stress the system with thousands of new tests. Random Query Generation Random query generators produce SQL statements using a given grammar. The cool part is that it can read the bison grammar of the actual product and use it to generate random queries. There are a bunch of force multipliers applicable for random query generation: Produce many different queries that give the same result; for example, identical queries over tables with different indexes and cross verify the results. Build intelligence into the generator to gradually increase the complexity of the generated tests. This ensures that the system is robust against simple cases before you proceed into more complex cases. If you start with complex cases you won’t be able to converge to a quality system fast enough. Once a bug is identified, the generator automatically reduces the complexity of the query to come up with the smallest possible reproducible test case. Random query generators can produce extremely intimidating queries. If the repro is left “as is” it would take a lot more time for a developer to investigate the root cause of the failure. Functional Stress Now that you have created a lot of functional tests, you can leverage them to run functional stress . The idea is that you take all these tests and randomly throw them at the system in parallel. When you do that you can’t really verify the test results, but all you care about is crashes and internal consistency checks. Failed Code Path Enumerator This is a very neat trick. Every time there is a condition that is true 99.9% of the time (like memory allocations), you inject a failure once per unique code path . So if you run a query “select * from t” the first time the server will throw an error with the first memory allocation. Then when you run this command again, the first memory allocation succeeds because this call code path has already been failed once, but the next allocation fails, and so forth. If you can keep re-running a query until it succeeds, you can deterministically test each possible resource failure. We have a transform which does exactly this with every test in our code base, and it has proven invaluable. Production failures Even though you test relentlessly, you still need pathways for your customers to surface issues back to your engineering team for investigation. You want to introduce minidump and full core dump functionality, in addition to logging to correctly diagnose bugs. Promote Transparency and Quality Culture Via Code Review One of the first steps towards quality that we did was to have every change go through a code review. To assist with this process a good tool is a must. We use Phabricator because it integrates into our workflow and is fun and easy to use. Code review helps promote a quality-oriented culture in the office.  Engineers will trend towards producing better code and more tests to increase the number of positive comments they receive.  Furthermore, having such a system in place helps new engineers learn the ropes faster. Conclusion It’s entirely worthwhile to invest into testing infrastructure from Day One. By doing so, you can achieve the following goals: Using stellar tools and force multipliers to do more with fewer engineers and really push the long tail of bugs Always be aware of the current state of the quality of the product Quickly stabilize and ship the product! [ 1 ] Steven Hemingray. Google", "date": "2013-04-10"},
{"website": "Single-Store", "title": "real-time-analytics-platform", "author": ["Eric Frenkiel"], "link": "https://www.singlestore.com/blog/real-time-analytics-platform/", "abstract": "When we started SingleStore a little over two years ago, our goal was to deliver the fastest OLTP database ever. Inspired by the scale and architectures we saw at Facebook, we hoped to help every enterprise leverage in-memory technologies similar to those that leading web companies use. As we worked with our early customers, we saw that an in-memory solution could provide the greatest value by enabling users to analyze real-time and recent historical data together. Customers like Zynga and Morgan Stanley not only wanted to quickly commit transactions to the database, they also wanted instant answers to questions about how their real-time data compared to historical data. This inspired us to build something new – a solution that supports highly concurrent transactional and analytical workloads at Big Data scale. That brings us to today. We’re proud to announce that SingleStore’s real-time analytics platform is available for download . This is the first generally available version of SingleStore that scales horizontally on commodity hardware. It provides the blazing fast performance for which SingleStore is known, and now does it at Big Data scale. Customers have deployed SingleStore across hundreds of nodes and dozens of terabytes of data, and we’ve tested at even greater volumes and velocities. (Check out our calculator to get an idea of the number of reads and writes you can perform depending on the size of your cluster.) This is also the first version to include SingleStore Watch , a visual web-based interface for monitoring and managing your cluster. We expect this to be the beginning of our foray into real-time visualizations as many of our customers look to operationalize their analytics. Deploying a database can be difficult, so we’ve made it as simple as possible. Download SingleStore for free on our site and take it for a spin. You’ll definitely be impressed by the performance, but you’ll also be impressed by what’s missing: Batched loading – Don’t wait until the middle of the night to refresh your reports. Complicated programming languages (and a limited talent pool) – Use SQL for real-time analytics. An expensive, proprietary box (and a plan to rip and replace it in a few years) – Scale incrementally on commodity hardware. A lengthy implementation cycle – Launch your first SingleStore instance in minutes in the cloud. We’re proud of the progress our engineering team has made building out an enterprise-grade software solution. Stay tuned to this blog to learn more about the real-time analytics challenges we are helping customers conquer. More to come.", "date": "2013-04-23"},
{"website": "Single-Store", "title": "carl-wright-as-evp-worldwide-sales-of-memsql", "author": ["Eric Frenkiel"], "link": "https://www.singlestore.com/blog/carl-wright-as-evp-worldwide-sales-of-memsql/", "abstract": "I’m excited to announce today that SingleStore has expanded its executive team with the hiring of Carl Wright as EVP of Worldwide Sales. Carl is exceptional both as a technologist and as a sales leader. We are delighted he has joined SingleStore to help us capture all the demand we’re seeing since we announced the release of our distributed memory-centric real-time analytics platform last month. Carl is already active in the field with customers and prospects while building a top-notch team for SingleStore. Carl joins us from Coraid, where he increased their revenue 12x in three years and brought on 1,500 new customers. He’s also held executive positions at Kidaro, acquired by Microsoft, and Decru, acquired by NetApp. Before this, he was Chief Information Security Officer for the U.S. Marine Corp, where he was responsible for a network that supported 120,000 users worldwide, so he’ll ensure excellent technical depth on our sales team. You can read more in our press release .", "date": "2013-05-08"},
{"website": "Single-Store", "title": "announcing-startcup", "author": ["Nikita Shamgunov"], "link": "https://www.singlestore.com/blog/announcing-startcup/", "abstract": "SingleStore is excited to announce start [ c ] up – a programming competition, hosted by Codeforces with an onsite at SingleStore HQ in San Francisco, California. Start [ c ] up consists of two rounds. Both rounds will be prepared by SingleStore engineers: pieguy , nika , exod40 , SkidanovAlex and dolphinigle . Round 1 is online and takes place on July 13. Round 1 follows regular Codeforces rules and consists of 5 problems. For this round, the complexity of the problems will be comparable to a regular Codeforces round. There are no eligibility restrictions to participate in the round. Round 2 takes place on August 3, consists of 5 problems, and uses regular Codeforces rules. The complexity of the problems is higher than a regular Codeforces round. Only people who finished in the top 500 in Round 1 can participate. The top 100 in round 2 will receive a start [ c ] up T-shirt. For Silicon Valley residents, SingleStore will be hosting up to 25 people on-site during the second round. The winner of the on-site round will be awarded a special prize.", "date": "2013-06-20"},
{"website": "Single-Store", "title": "googles-code-jam-world-finals-two-senior-memsql-engineers-make-top-25", "author": ["Nikita Shamgunov"], "link": "https://www.singlestore.com/blog/googles-code-jam-world-finals-two-senior-memsql-engineers-make-top-25/", "abstract": "SingleStore is proud to announce that David Stolp (Pieguy) and Nika Jimsheleishvili (nika) are on their way to Google’s London office to compete in the 2013 Code Jam World Finals. Pieguy and Nika will compete in the final competition this August for a chance to win $15,000 and the prestigious title of 2013 Code Jam Champion. To qualify for the chance to become Code Jam Champion, Pieguy and Nika have competed in four online qualifying competitions and finished in the top 25 out of more than 35,000 worldwide contenders. In its 10th year, Google’s Code Jam has been uniting student and professional programmers  from around the world to solve hard algorithmic puzzles. Learn more about the competition and how to get involved next year by visiting the official Code Jam site. Show your support for Pieguy and Nika, who represent Silicon Valley startup engineers at Google Code Jam, in the comments below!", "date": "2013-07-02"},
{"website": "Single-Store", "title": "memsql-releases-2-1-increasing-the-value-of-all-data-in-the-enterprise", "author": ["Mark Horton"], "link": "https://www.singlestore.com/blog/memsql-releases-2-1-increasing-the-value-of-all-data-in-the-enterprise/", "abstract": "Today, SingleStore released version 2.1 of its real-time analytics platform, which is available for immediate download . This release includes new features and enhancements to give customers the ability to access, explore and increase the value of data, regardless of the size, age, or format. The release also includes additional SQL surface area and a custom AWS CloudFormation template for fast and easy deployment in the cloud. Here are the new features customers will see in the release: Multi­threaded Load of CSV files: CSV is one of the most commonly used file formats for exchanging data between applications. You can set up on-­demand uploads of CSV files to SingleStore, so you can harness the power of an in­-memory distributed database for sub­-second query response times on hundreds of millions of rows of data. Distributed Joins: The ability to perform distributed joins on tables with primary key to foreign key relationships is critical to accelerating many types of analysis. SingleStore DB 2.1 unlocks this functionality to accelerate analytics use cases by allowing you to choose the column(s) on which tables shard, making SingleStore smart and flexible on schema design. Linux-Based Package Manager: With SingleStore DB 2.1, you have the ability to download, install, and upgrade SingleStore via a package manager on major Linux distributions. You can now easily install, upgrade, and manage SingleStore using common Linux workflows. Node Management: The ability to manage nodes efficiently is essential in database cluster management. With SingleStore DB 2.1, failed nodes are more easily tracked in the SingleStore Watch dashboard and can be re­attached to the cluster without re-provisioning. The entire experience of backing up, restoring, shutting down, and managing failures is optimized for DBAs. SingleStore built these features based on extensive collaboration with DBAs to ensure its solution is the easiest platform to manage. Data volumes are growing exponentially, and understanding the full value of all that data can be difficult. Individual data sets bring valuable analytical insights into operations, sales and marketing campaigns, product development, and customer support, but their value only represents a small fraction of the total value of an enterprise’s data. Organizations need their data to play a cross-functional role for their analytics and business intelligence to show the bigger picture. This means that analytic platforms need to quickly ingest, query, and combine data from anywhere inside the organization. As the speed at which data is analyzed increases, so does the quality of insight. By connecting the dots across all the data inside an organization into a single view, your enterprise can surface valuable competitive advantages, provide deeper customer engagement, and enable better data-driven decisions. Achieving this brings organizations even closer to “data nirvana.” With a real-time database, you can access and explore data instantly, greatly appreciating the value of all data. SingleStore is the only database that allows companies to manage the velocity of Big Data transactions without sacrificing the ability to interact with and analyze all data in the system. With SingleStore’s 2.1 release, we are unifying all forms of data, taking size, age, and format out of the discussion and giving you peace of mind that you can find all the value in your data. Available for immediate download , visit our website for a free 30-day trial of the world’s fastest and most scalable in-memory database. If you’re already a SingleStore customer, please contact your account manager for more information on upgrading or with any additional questions.", "date": "2013-07-10"},
{"website": "Single-Store", "title": "learn-how-to-build-your-own-analytic-platform", "author": ["Mark Horton"], "link": "https://www.singlestore.com/blog/learn-how-to-build-your-own-analytic-platform/", "abstract": "Analytic platforms can be difficult to build, even more so when you need to factor in ingestion and querying of real-time data. With so many vendors making competing and overlapping claims, it’s easy to be overwhelmed. That’s why it’s important to take a step back and consider a third-party perspective. SingleStore is hosting a webinar next week with well-respected analyst Matt Aslett of 451 Research on key criteria that enterprises should keep in mind as they build analytic platforms to derive value from terabytes of data. After speaking with Matt on many occasions, it’s clear he has great insight on how a real-time analytics platform is different from a traditional data warehouse or a Hadoop cluster – and how all three can work in concert in an enterprise data environment. Matt frequently discusses how analytic platforms must be able to scale incrementally; integrate with existing data technologies; and enable you to respond to rapidly changing business environments. Are you currently working on an analytics or real-time data project? Register for the webinar to hear Matt’s advice. If there are criteria you’d like to understand, please mention it in the comments below so Matt and SingleStore can incorporate them into the webinar.", "date": "2013-07-18"},
{"website": "Single-Store", "title": "how-federal-agencies-can-tackle-data-challenges", "author": ["Mark Horton"], "link": "https://www.singlestore.com/blog/how-federal-agencies-can-tackle-data-challenges/", "abstract": "Government agencies are struggling to extend the lifecycle of applications that were not built for today’s data velocity and volume. CIOs and CTOs from many agencies tell us that they are struggling to scale with growing data volumes and can not deliver on the response times that their end users expect. That’s why we’re excited about our presentation next week at IDGA’s Data Analytics for Government Conference. Carl Wright, EVP Sales for SingleStore, will explain how federal agencies can upgrade their infrastructure so yesterday’s applications can handle today’s Big Data. Aside from our industry-leading ingestion and query response rates, our government partners recognize significant benefits from their ability to horizontally scale SingleStore’s real-time analytics platform on commodity hardware. They can manage growing data volumes very inexpensively and without the rip-and-replace distractions and downtime of a traditional database upgrade. Of course, speed is also critical. When the results of queries can save lives or help determine where to deliver precious resources in emergency situations, sub-second responses take on a whole new level of importance. With a steady flow of new information being loaded into the system, SingleStore will continue providing incredible response times on both real-time and historical data. If you’re attending Data Analytics for Government, please join us for the presentation to learn how SingleStore can help you with your real-time Big Data analytics initiatives. Be sure to stop by our booth and see a demo of SingleStore in action with our partner TVAR Solutions.", "date": "2013-07-23"},
{"website": "Single-Store", "title": "celebrating-the-memsql-startcup-finals", "author": ["Nikita Shamgunov"], "link": "https://www.singlestore.com/blog/celebrating-the-memsql-startcup-finals/", "abstract": "This weekend SingleStore challenged some of the world’s best computer programmers in the finals of the inaugural SingleStore start [ c ] up competition. Similar to computer programming competitions held by Google and Facebook, SingleStore’s start [ c ] up showcased the best from around the world in a grueling 3 hour competition. [ slideshow _ deploy id=’664′ ] SingleStore Engineers pieguy , nika , exod40 , SkidanovAlex and dolphinigle created a complex problem set for contestants. Made up of binary search, pigeonhole principle, Grundy numbers, dynamic programming, and probabilities, these problems were meant to stump most engineers cold.  However, almost all contestants solved at least one problem, though no one was able to solve the final problem – a pieguy special that stumped everyone. To see the post-competition editorial, click here . SingleStore’s executive chef, Daniel Norton, prepared steak, crab mashed potatoes, asparagus, and side salad with béarnaise sauce for lunch. It was a great time for contestants to relax and and review the solutions while anxiously awaiting the results. And the winner is…. We wanted to change it up and make the prizes fun – so we picked money leis for the winners. Third place takes home $300, second place $600, and first place $1,200. Here are the results from the competition: First place: Petr Mitrichev from Russia Second place: Lovro Puzar from Croatia Third place: Anton Lunyov from Ukraine Petr Mitrichev had an impressive win by being the only contestant to finish with five problems. He has also won numerous events in the past, and has been a long-time veteran of competitive programming. [ slideshow _ deploy id=’676′ ] Following the competition, SingleStore provided an afternoon of fun and games, with one lucky contestant taking home an 11” MacBook Air as the grand prize for the poker tournament. As part of SingleStore’s ongoing tradition, we served pie (made by pieguy himself) toward the end of the event to celebrate the fun and challenging start [ c ] up competition. It was a great event overall,  and we want to thank all the contestants for participating from around the world. If you would like to participate next year, be sure to check back for the latest updates.", "date": "2013-08-06"},
{"website": "Single-Store", "title": "esg-lab-validates-memsqls-real-time-big-data-analytics-solution", "author": ["Mark Horton"], "link": "https://www.singlestore.com/blog/esg-lab-validates-memsqls-real-time-big-data-analytics-solution/", "abstract": "Enterprise Strategy Group (ESG) recently performed a benchmark that established SingleStore’s distributed in-memory database as the standard for real-time, Big Data analytics. ESG validated that SingleStore delivers outstanding performance and linear scalability, superior reliability and durability, and the ability to support rapid growth. Download the FREE report . Tony Palmer, a senior ESG Lab analyst who performed the benchmark said, “Organizations today increasingly need a fast and easily scalable database to query Big Data in order to meet the real-time demands of their business. Throughout our tests, SingleStore’s distributed in-memory database demonstrated outstanding ingest and query performance, with ingest and queries occurring concurrently. As a result, we would recommend businesses take a serious look at SingleStore as a solution that provides highly available, high-performance, real-time, Big Data analytics at scale.” SingleStore was designed for today’s Big Data analytical needs, and strives to solve the problems that plague traditional databases that simply cannot compete in the high velocity, high volume data environments. SingleStore delivers performance, complex analytical capabilities, and a highly reliable and durable real-time database for Big Data analytics.", "date": "2013-08-28"},
{"website": "Single-Store", "title": "memsql-to-present-at-xldb-conference-on-sept-10", "author": ["Kate Borger"], "link": "https://www.singlestore.com/blog/memsql-to-present-at-xldb-conference-on-sept-10/", "abstract": "SingleStore is proud to be a part of this year’s XLDB conference, hosted by Stanford University next week on September 11. The 7th Extremely Large Database Conference is designed to help answer the growing need for systems and tools capable of managing and analyzing extremely large data sets, and will focus on practical solutions to capture and leverage big data. More than 98 percent of all the world’s stored information is digital, a drastic increase from 25% only thirteen years ago. Newer data types such as RFID, clickstream, biometric, GPS, still images, audio, video and stock ticker data are captured at constantly accelerating speeds, and modern businesses must analyze this massive stream of incoming data to produce real insights and provide true business value. Join Nikita Shamgunov , CTO and co-founder of SingleStore, in room 107 of the Gunn Building during the conference on Wednesday September 10th, where he will discuss current database trends and the future of real-time Big Data analytics. In addition to answering questions from participants, Nikita will give a live demo of SingleStore’s in-memory database solution. We hope to see you there!", "date": "2013-09-06"},
{"website": "Single-Store", "title": "oracle-announces-an-in-memory-technology-at-an-ungodly-speed-and-cost", "author": ["Nikita Shamgunov"], "link": "https://www.singlestore.com/blog/oracle-announces-an-in-memory-technology-at-an-ungodly-speed-and-cost/", "abstract": "Oracle OpenWorld started yesterday, and Larry Ellison announced new in-memory options in Oracle 12c. According to PCWeek “The Big Memory Machine costs $3 million, a sum Ellison termed as ‘a fraction’ of what competitors charge.” The core of the newly announced Oracle technology is a columnar store . It is in fact a fascinating technology that leverages compression, vectorized execution using SIMD , and parallel query processing. However, this technology has been on the market for a long time and has started to commoditize. Oracle is probably the last big vendor to come up with columnar technology. But even if you pay big dollars, you don’t solve your analytics problem for good. There is one common property among all analytics workloads: they grow. Suddenly 32 TB is not all that big. And the price! Nearly $100,000 per TB. By stark contrast, the cost of commodity DRAM is now about $5,000 per TB, and FusionIO, which is almost as fast as DRAM, is about $2,000 per TB. With a vertically-scaled architecture 32 TB is shockingly expensive. You either need to switch to a bigger appliance or move to Exadata or Oracle Rac. Oracle is pushing expensive hardware for a reason. It allows Oracle to justify charging much higher licensing fees. It’s a win-win for Oracle on the hardware and software, and a lose-lose for the customer that needs to balance their IT budget. Columnar technology is a very important checkbox for Oracle, but by focusing on it, they divert attention from the more important trend: the ability to run on commodity hardware. If you look at Google and Facebook data centers you’ll find that everything is engineered around commodity hardware. It applies even more so for in-memory technology. The reason is that in-memory stores eliminate I/O costs, which enables incredible performance on commodity servers where DRAM is just as fast. As we look at some current real-world use cases for in-memory databases, we see an absolute explosion of requirements in many sectors. For example, in ad-tech, it’s not just dealing with Big Data – in reality they are dealing with “ungodly” data volumes and velocities. Customers require the ability to ingest 500,000,000 events/hour while concurrently processing real-time analytical workloads and storing petabytes of historical data. This kind of data very quickly exceeds 32 TB, yet analyzing it in real-time is still incredibly important. SingleStore routinely runs on hundreds of nodes providing incredible storage and computing power on commodity virtualized cloud infrastructure with high availability. With technologies like Amazon Web Services (AWS), as well as private cloud offerings from VMware and OpenStack, the world is moving towards elastic computing. Just like mainframes, appliances don’t fit in the new world where customers think in terms of nodes in data centers, availability zones, and geographically distributed storage and compute. The new world is being built on distributed systems.", "date": "2013-09-23"},
{"website": "Single-Store", "title": "exploring-in-memory-databases-at-memsql-meetups", "author": ["Kate Borger"], "link": "https://www.singlestore.com/blog/exploring-in-memory-databases-at-memsql-meetups/", "abstract": "Many technologies are not adapting fast enough to solve modern data problems, and SingleStore is one emerging technology that seeks to bridge the gap. We recognize that these problems cannot be solved in a vacuum, which is why SingleStore seeks to foster a collaborative environment in which developers can work together to solve today’s Big Data challenges. We hosted our first Developer Meetup last month, and we’re looking forward to hosting our next Meetup on November 12 at our headquarters in San Francisco. The developer community is very important to us — as we are developers ourselves — and we are excited to have the opportunity to collaboratively explore modern business issues and build evolving theories around database technology and the future of real-time analytics. Be sure to join our group on meetup.com to connect with other SingleStore developers and receive the latest news on upcoming events. We also invite you to “like” the SingleStore Facebook page where we often share useful technical articles on the continuing development of the platform.", "date": "2013-10-10"},
{"website": "Single-Store", "title": "real-time-data-fusion-the-final-frontier", "author": ["SingleStore"], "link": "https://www.singlestore.com/blog/real-time-data-fusion-the-final-frontier/", "abstract": "In a recent discussion with an engineer from Facebook regarding the time value of data, it was continuously emphasized that the relevance of “hot” data vastly outweighed the importance of “cold” data when considering the daily/weekly/monthly monetization of their business. In order to exploit the hot data, complex data processing factories were created to facilitate the “conditioning” of raw JSON and structured data in order to ingest it into Scuba … a system they developed for doing real-time, ad-hoc analysis of arbitrary datasets. While this is not a new story by any means, enterprise companies and government organizations may be surprised to hear that over 50% of Facebook engineers now leverage Scuba’s real-time capabilities for decision making. Data fusion is generally defined as the process of synthesizing raw data from several sources to generate more meaningful information that can be of greater value than single sourced data. For over 20 years enterprises have struggled to migrate from stove-piped legacy database applications to centralized data warehouses in the hope that through consolidation they would achieve cost savings and greater access to their data. But as long as SQL processing was trapped in “the box,” enterprises would always be at the performance mercy of single box solutions. The only people that benefited from this approach were the large institutional database purveyors. As the market is quickly finding out, SingleStore solved this longstanding and painful performance bottleneck through the introduction of its distributed in-memory scale-out cluster computing model predicated on cost effective commodity servers. When combined and powered by a massively parallel and distributed query optimizer, the SingleStore solution effectively allows SQL to escape “the box” by leveraging hundreds or thousands of computing cores for each and every in-memory database transaction. This innovation was the first step on a journey to unleash the power of your data enabling true real-time analytics. WebOps/DevOps based companies like Facebook have developed in-house solutions to provide real-time data fusion in order to take advantage of Hot data. Facebook has half the number of employees as Yahoo and one-tenth the number of employees of Google. Could enabling real-time ad-hoc analytics be part of their growing competitive advantage and profitability within their market? The good news is that SingleStore has commercialized an enterprise ready capability that can help your organization quickly find out. Waiting Sucks! Call SingleStore.", "date": "2013-10-16"},
{"website": "Single-Store", "title": "hadoop-real-time-batch-processing", "author": ["SingleStore"], "link": "https://www.singlestore.com/blog/hadoop-real-time-batch-processing/", "abstract": "Waiting sucks, as I’m sure you’d agree, and when it comes to analytics, the gravity of increasing data volumes has become a primary concern for both industry and IT professionals.  In order to deal with the volume and velocity of disparate data types, organizations have had to take an “assembly line”-like approach that requires significant batch processing. These factories are cobbled together using different software packages to ETL and transform data, which unfortunately creates systemic reporting delays. Real-time operational requirements cannot be serviced by processes built to support all time historical volumes. This “supply chain management” approach of data transforms and batch processing has become too unwieldy and, as can be seen below, requires complex architectures and programming languages to facilitate. Instead of building another “old fashioned factory” by layering many systems on top of Hadoop, you can simplify and streamline your data processing by utilizing in-memory database technologies that are directly accessed via standard ANSI-SQL. Hot data resides in-memory while cold data resides in Hadoop or a columnar store.  To address the complexity and volume of incoming disparate data, SingleStore now supports native JSON objects and standards based ODBC/JDBC/Hadoop connectors, allowing organizations to process both structured and semi-structured data together in a common database. Combined with commodity X86 scale-out architectures, the SingleStore in-memory solution provides the capabilities to service both today’s and tomorrow’s Big Data workloads economically. Using the advertising sector as an example, ad-tech professionals are not just dealing with Big Data… these teams are faced with “ungodly” data volumes and velocities. Customers require the ability to ingest 500,000,000 events/hour while concurrently processing real-time analytical workloads in support of monetization and bidding engines.  SingleStore has deployed several reference architectures of our scale-out in-memory database that can significantly help reduce the complexity of managing multi vendor solutions for real-time analytics or processing. While the Hadoop ecosystem changes gradually with tweaks atop its foundation,  evolution progresses with “punctuated equilibrium.” Think dinosaurs and mammals, for example. When it comes to database systems, in-memory technology is exactly the disruptive catalyst that takes a field by storm and can revolutionize your own business overnight. If you agree that waiting sucks, it’s time to join the real-time revolution with SingleStore.", "date": "2013-10-28"},
{"website": "Single-Store", "title": "unsexy-database-features-that-matter-part-2", "author": ["Nikita Shamgunov"], "link": "https://www.singlestore.com/blog/unsexy-database-features-that-matter-part-2/", "abstract": "In the previous post , we talked about how you should focus on how people derive value from your product. Today we will focus on database management — your goal should always be 24/7 100% uptime. How People Manage A Database Product Deployment (time to get the system up and running) How much time does it take to get your product up and running? People have talked about barriers to entry for a long long time. The worst one is, “I couldn’t install your product so I gave up.” This is simply unforgivable. Modern platforms offer wonderful turn-key deployment tools: rpm/deb for linux, virtual machine images and AMIs, Amazon Marketplace, cloud formation, docker containers. Pick your poison and do it right. Security Enterprise demands high security products. Kerberos authentication, password management policies. It’s unsexy, it boring, but it matters. Without it you can’t get into some of the most important enterprise accounts. Online Operations You would think that every database has a backup story, until you add one word to it: online. Online means that the backup doesn’t takes read or write locks for the duration of the backup and that the database state is consistent with regard to a specific point in time. Of course commercial databases have had this feature for a long time and some of the open source counterparts (Postgres) have it as well, but even MySQL and MongoDB don’t have a turn-key solution for this. It’s relatively easy to have offline alter table that blocks all the reads and writes to the database. Online alter table is very hard to build, but it allows production systems to have zero downtime in the face of schema changes. We bit the bullet and built it. Needless to say, it’s a tremendous customer delight. High Availability Commodity hardware tends to fail. Non-commodity hardware tends to fail just as much. Your system must survive that with minimum downtime. As usual, it’s easier said than done, but you must have it, otherwise no one would want to run it in production. Conclusion When you build a database server, it’s easy to get carried away with sexy things like exotic data structures, trendy new distributed designs, new query languages, etc. But you have to always invest in fundamentals just as much as you are investing in what makes your product unique and special to your customers.", "date": "2013-11-12"},
{"website": "Single-Store", "title": "unsexy-database-features-that-matter-part-1", "author": ["Nikita Shamgunov"], "link": "https://www.singlestore.com/blog/unsexy-database-features-that-matter-part-1/", "abstract": "As enterprise customers start to use your product in many different ways, new feature requests will start to flow into the product team. These features aren’t sexy features by any means, but every enterprise customer cares about them deeply, and we try to include as many customer feature requests as possible into every release, sexy or not. To really understand these unsexy features, we must examine two areas: How users derive value out of your product. How users manage a database product. In this two-part blog series, we will explore these topics and why they matter in today’s database market. Part 1: How People Derive Value Tools Compatibility At the end of the day, people connect to a database via a suite of tools and apps such as SequalPro, Tableau, Looker, ORMs, database explorers, etc. Ideally, database developers are able to use existing drivers/network protocols that people already understand. Query Surface Area and Query Optimizer Super successful products “just work.” A major component of “just working” is having SQL query surface area that supports everything an unsophisticated user can throw at it (e.g. full set of data types, builtin function, subqueries, etc). This can be a hard pill to swallow, but it’s also what makes your business defendable from the barrier to entry standpoint. This also applies to a query optimizer (QO). The “just works” idea goes out the window if the QO chooses a disastrously slow execution plan. In a way, the job of a QO is to avoid disastrously slow plans rather than finding perfect execution plans. Building a great QO is a hard problem, and SingleStore is constantly improving our QO in every release to ensure that queries run as efficiently as possible. And by the way, from the engineering perspective building a QO is quite a sexy project. Data Loading I guarantee that anyone who deals with a lot of data has one major pain: ETL . ETL sucks! You are moving data across different systems and it’s never smooth or fast. Date formats are different, data types don’t perfectly match, NULLs, character escaping, etc. It is all the little things that start to get really annoying. SingleStore supports a lot of ways of getting data into the database. Obviously we support CSV load via LOAD DATA, as well as a variety of other ways to push data into SingleStore, including out-of-the-box streaming of data from collectd, and statsd to enable machine data performance analytics. Our goal is to help customers avoid ETL as much as possible. The Little Things Working on error messaging, language support, character encodings, data type conversions is brutal, as it requires tremendous attention to detail. We write a ton of tests and churn a lot of code that is not groundbreaking, but it has to be done. This improves the lives of our customers and lets them iterate on their applications faster. Precise error messages make application development easier. Character encoding support broadens the reach of the product. The little things provide what we call a “delight” moment when working with SingleStore is easy and intuitive. Stay tuned for part two as we talk more about how people manage their databases.", "date": "2013-10-31"},
{"website": "Single-Store", "title": "exploring-big-data-technology-with-memsql", "author": ["Kate Borger"], "link": "https://www.singlestore.com/blog/exploring-big-data-technology-with-memsql/", "abstract": "We are taking our message of blazing speed and real-time Big Data analytics on the road. Will you join us? We’ve hosted developer meetups at our headquarters in San Francisco, and we are now an official sponsors of Data Science DC. The developer community is very important to us — as we are developers ourselves — and we are excited to have the opportunity to collaboratively explore modern business issues and build evolving theories around database technology and the future of real-time analytics. Most technologies aren’t adapting fast enough to solve modern Big Data problems. SingleStore seeks to bridge that gap, as the platform has already been adopted by some of the most recognizable brands in the world. We realize that today’s Big Data challenges cannot be solved in a vacuum, and that we need to work together to find the optimal solution. Data Science DC is a non-profit professional group that meets monthly to discuss diverse topics in predictive analytics, applied machine learning, statistical modeling, open data, and data visualization. Members are professionals, students, and others with a deep interest in these fields and related technologies. Join us this Thursday to learn about local outlier detection in big geospatial data. Be sure to join our group on meetup.com to connect with other SingleStore developers and receive the latest news on upcoming events. We also invite you to “like” the SingleStore Facebook page where we often share useful technical articles on the continuing development of the platform. See you in DC!", "date": "2013-11-20"},
{"website": "Single-Store", "title": "memsql-2-5-ships-today-with-json-datatype-online-alter-table-and-more", "author": ["Eric Frenkiel"], "link": "https://www.singlestore.com/blog/memsql-2-5-ships-today-with-json-datatype-online-alter-table-and-more/", "abstract": "We are excited to announce that SingleStore DB 2.5 is now available for download.  Of the many new features and performance improvements, one of the most exciting is support for JSON analytics. With native support for the JSON datatype, SingleStore delivers a consolidated view across structured and semi-structured data in real-time. Using SingleStore DB 2.5, developers can use SQL to easily analyze JSON objects while quickly ingesting JSON data feeds. In addition, SingleStore DB 2.5 ships with support for ONLINE ALTER TABLE, which means that you can make changes to your production schemas without any downtime. SingleStore DB 2.5 enables rapid application prototyping,  updating data models on the fly, and otherwise eliminates downtime. The list of upgrades in SingleStore DB 2.5 includes: JSON Data Type SingleStore DB 2.5 supports JSON as a column type, which enables developers to store and query structured and semi-structured data together in one database. SingleStore JSON integration provides the following features: Use standard SQL functions with JSON, including built-ins, GROUP BY, JOINs, and more. Create JSON indexes online. Client drivers require no changes to support JSON. JSON properties are updatable. JSON columns can contain ‘null’, ‘true’, ‘false’, double-precision floating-point values, quoted strings, heterogeneous arrays, and/or nested JSON objects. Improved Index and Scan Performance SingleStore DB 2.5 performs 3x faster over large datasets, enabling scans of over 30 million rows per second per core. Online “ALTER TABLE” SingleStore DB 2.5 supports online “ALTER TABLE” operations (“ADD [ COLUMN ] ”, “DROP [ COLUMN ] ”, and “ADD {INDEX|KEY}”) on distributed tables. The table is available for reads and writes while “ALTER TABLE” executes. The memory overhead of online ALTER TABLE is only a few megabytes, regardless of the table size. Online Backup and Restore SingleStore DB 2.5 offers single commands to backup and restore a cluster. “BACKUP DATABASE” takes a point-in-time snapshot of data across the cluster while your read and write workloads continue to run. “RESTORE DATABASE” restores an entire distributed database. The new cluster can have a different number of nodes or different redundancy level from the original. SQL Support SingleStore DB 2.5 supports the following operations on both standalone and distributed tables: Scalar subselects, subselects with EXISTS and IN Constants are parameterized in projections, as well as ORDER BY and GROUP BY clauses UNION ALL, GROUP _ CONCAT, various built-in functions, and expression performance improvements Persisted computed columns on arbitrary SQL expressions Please visit www.singlestore.com to learn more about SingleStore DB 2.5 and to download a free trial.", "date": "2013-11-25"},
{"website": "Single-Store", "title": "big-data-holds-big-benefits-for-national-security-programs", "author": ["SingleStore"], "link": "https://www.singlestore.com/blog/big-data-holds-big-benefits-for-national-security-programs/", "abstract": "The Intelligence Community Information Technology Enterprise (ICITE) initiative, one of the federal government’s most complex and sensitive technology projects, aims to transform the current 17-member U.S. Intelligence Community from agency-specific IT silos to an enterprise environment of shared services and systems. The goal of the project is to cut federal costs and to improve information sharing and data security across intelligence agencies. Join us for an in-depth discussion. Register Now. With the rapid growth of sensor platforms across the Department of Defense and Intelligence Community, we expect there to be a strong desire to centrally store and share this data. The ability to not only consolidate infrastructure costs through these efforts, but to also provide real-time centralized analytics from big data fusion factories would allow for seamless sharing and cross-platform situational awareness. Implementing an updated real-time data system can, for example, give war-fighters instantaneous access to high-value information from several agencies at any location at any time, even across disparate sensor platforms. This ability to fuse disparate big data elements in real-time, in memory, is the latest force multiplier to be added to the intelligence community’s IT arsenal. By integrating existing internal decision making processes such as OODA (Observe, Orient, Decide, Act), organizations can capitalize on modern technologies to significantly enhance execution. Using this example, the organization that can process the OODA loop faster than the competition will ultimately prevail. Technologies such as SingleStore’s distributed in-memory database can help intelligence agencies rapidly build next-generation applications that ensure information dominance while decreasing total cost of ownership. SingleStore can be the glue that will help five disparate intelligence agencies extract real-time information – and better yet – make sense of it all very quickly by providing the needed situational awareness and visibility.", "date": "2013-11-26"},
{"website": "Single-Store", "title": "the-need-for-speed-webinar-take-big-data-real-time", "author": ["Mark Horton"], "link": "https://www.singlestore.com/blog/the-need-for-speed-webinar-take-big-data-real-time/", "abstract": "“The trickiest part of speeding up a program is not doing it, but deciding whether it’s worth doing at all,” says Carlos Bueno, a former Facebook engineer and award-winning author of the Mature Optimization Handbook . He’s now a senior engineer at SingleStore, helping to create in-memory database solutions that create significant value by leveraging Big Data analytics. Register Now . Carl Wright, a former CSO/CTO of the United States Marine Corps, will join Carlos as they discuss how in-memory technologies are changing the Big Data landscape and processing data faster than ever before. In-memory solutions like SingleStore allow companies to manage Big Data in real-time, something that out-of-date disk-based storage systems simply cannot provide. Join us on Thursday, December 12 at 10am PST as these two experts explore the following topics: Flexibility and incremental scale-out that allow you to grow your platform according to volume and performance needs. Agile responsiveness to act on changes to your business environment faster than traditional data warehouses allow. SQL + JSON to enable real-time analytics on data feeds of variable structure. Learn how you can unleash the full potential of Big Data for your business by consuming and returning data instantly. Join us on the 12th! Register Now .", "date": "2013-12-06"},
{"website": "Single-Store", "title": "notes-on-sizing-a-memsql-cluster", "author": ["Carlos Bueno"], "link": "https://www.singlestore.com/blog/notes-on-sizing-a-memsql-cluster/", "abstract": "One of the most valuable features a distributed system can have is predictability. When a system is predictable, you can reason about how it will respond to changes in workload. You can run small-scale experiments, and trust that the results will scale linearly. It’s much more pleasant and efficient to tailor the hardware to the task than the other way around. No one likes to waste money on more compute than you need because your RAM is full, or be unable to use the full power of your systems because of an I/O bottleneck. In this article we will walk through a few scenarios and rules of thumb for sizing a SingleStore database cluster. “The biscuits and the syrup never come out even.” — Robert Heinlein < p dir=”ltr”>All reads and writes in SingleStore flow through “aggregator” servers to many “leaf” servers. Each CPU core on each leaf can be thought of as its own instance of SingleStore, talking only to aggregators. Under normal operation, each aggregator is also acting independently. As inserts are sent to a randomly chosen aggregator, their shard keys are hashed and the data is sent to the relevant leaves. Select queries for particular keys are sent only to the relevant leaves, and selects or joins across entire tables are executed in parallel across the cluster. A shared-nothing architecture has many nice properties. The network behavior is easy to reason about because there is no cross-talk between leaves; a badly-written query can’t suddenly cause congestion on the network. You can use inexpensive in-server storage for durability instead of an expensive SAN. The bandwidth and parallelism of disks directly attached to your leaf servers scales linearly with the size of your cluster — no I/O bottlenecks from many servers fighting over the same shared drives. Latency is also easy to reason about. Let’s say you have a small database: 120 million records spread over 4 leaves. If it takes 700 milliseconds to do a full table scan, then adding 4 more leaves will almost certainly make your queries twice as fast. Durability is handled with replication across leaves, disk journaling, and snapshots. All of that is separate from the business of answering your queries. It’s actually pretty difficult to push your SingleStore cluster into a state in which query times exhibit high variability due to I/O contention or locking; the CPU maxes out first. An easy fix for a highly-parallel CPU-bound system, of course, is to add more CPU. The only question is where. As a rule of thumb, writes tend to be bottlenecked on the aggregators and reads on the leaves. The mix of reads to writes determines the optimal mix of aggregators to leaves. If you expect a fairly high input volume, e.g. over 50,000 inserts / 50MB per second per leaf, a ratio of 2-3 aggregators to 8 leaves is a good starting point. If you have a very high sustained rate of inserts, consider using most of your aggregators exclusively for write traffic and keep some apart to handle reads. This reduces contention at the aggregator layer and helps keep read and write latencies predictable. And that is the key to capacity planning for your SingleStore cluster. There’s no need to be psychic. You can measure and find out. If you are not sure how many servers you need for a given workload, try a test on a few servers with 1/10 or 1/20 of the total dataset. Have to choose between server types with different amounts of CPU and RAM? Run a quick test on similar hardware in the cloud, and see what happens. Unlike most databases, especially systems with excessive coupling between nodes, scaling up your SingleStore performance testing is straightforward. [ 1 ] The only exceptions are DDL and cluster management commands, which can only be run the master aggregator. [ 2 ] One exception is a query that sends a lot of data to the aggregator. Running “select distinct user_id” on a very large table, for example. This is a problem no matter what database you use because it tries to send a cluster-worth of data down a single narrow pipe. In general you want to bring the computation to the data, not the data to the computation.", "date": "2013-12-18"},
{"website": "Single-Store", "title": "through-a-table-sparsely", "author": ["Carlos Bueno"], "link": "https://www.singlestore.com/blog/through-a-table-sparsely/", "abstract": "Take a look at your database. What do you see? An alphabetic list of table names. Is that really the most important thing to know? Surely, other facts about your database are more interesting. Maybe you want to know how big the tables are. Database table sizes, like mountain heights and city populations and book sales, tend to follow a pattern casually called a power-law distribution . A minority of tables hold the majority of data. The rest are a long tail of necessary but small things, a kind of embroidery around the edges. Below is a more meaningful list of tables, sorted by the space they consume. Does the pattern stop there? Look at your tables. Not just the columns, but what’s in them. The schema tells you that the “title” column is in the 5th position. It’s a varchar that can go up to 64 characters… or it might be blank. Or NULL. In a just world, blank values would be a rare corner case, a placeholder for the occasional missing datum. Tables would be an endless stream of rows of identical structure, with a place for every bit and every bit in its place. But it just ain’t so. Blank values crop up a lot more often than we’d like because the data we shove into tables is not really tabular. The attributes of real-world things, the columns in your tables, exhibit the same lopsided power-law distribution. There will always be more first names than middle names, and more middle names than suffixes like “Junior”. “Reality is that which, when you stop believing in it, doesn’t go away.” — Philip K Dick Over time this “persons” table evolves into the usual 100-column string soup. There will be dozens of long-tail attributes that are important but rare: previous _ nationality, maiden _ name, or passport _ number. Adding support for those attributes means reserving a blank spot for every row where it does not and will never apply. After years of attempting to encompass messy reality with a rigid grid, the table becomes mostly air. Dealing with some missing values is one thing. Dealing with so many thinly-populated dimensions is something else entirely. Plot the Pain It’s straightforward to plot sparseness using standard SQL. A handy feature of the count() function is that it ignores NULLs. To count columns that have a default value, you can sum over a boolean expression. I’ll bet money that your biggest tables have a large number of mostly-empty columns and small set of core columns, just like this example. memsql> select\n  count(id)/count(1) as id,\n  sum(first_name != '')/count(1) as first,\n  count(maiden_name)/count(1) as maiden,\n  ...\n  sum(passport_num != 0)/count(1) as passport\nfrom persons;\n+--------+--------+--------+       +------------+\n| id     | first  | maiden |  ...  | passport   |\n+--------+--------+--------+       +------------+\n| 1.0000 | 0.9984 | 0.1270 |  ...  |     0.0942 |\n+--------+--------+--------+       +------------+ You can also plot the sparseness horizontally to get a sense of how many columns are actually used on average. There may be 100 possible columns in this table but only about 20 are needed at a given time. memsql> select\n  used_column_count,\n  format(count(1),0) as cnt\nfrom (\n  select\n    (id is not null) +\n    (first_name != '') +\n    (maiden_name is not null) +\n    ...\n    (passport_num != 0) as used_column_count\n   from persons\n ) as tmp\n group by 1\n order by 2 desc;\n\n+-------------------+------------+\n| used_column_count | cnt        |\n+-------------------+------------+\n|                21 | 11,090,871 |\n|                20 |  2,430,207 |\n|                14 |    319,891 |\n|                19 |     11,766 |\n|                25 |      2,725 |\n|                17 |        499 |\n|                18 |        303 |\n... The most obvious problem is managing the storage for huge sparse tables. An “empty” cell still consumes space. Repeated values compress nicely, but you’re not always dealing with compressed data. And high compressibility only underscores the fact that there’s not much information there to begin with. A bigger problem is that the harder it is to add / move / delete columns, the more conservative you are about doing it. The overhead and operational pain of adding columns to support the features you write is only the visible cost. What about all the long-tail features you don’t write, or even let yourself think about, because altering a big table in traditional SQL databases is so traumatic? Consider Your Options There are several ways to deal with the sparseness of the world around us. You can rearrange your schema, maybe factor out that address table, but essentially continue to treat sparseness as one of the miseries of life. You can read up on the Fourth Normal Form and slice your tables into lots of thin vertical strips, but that presents obvious practical problems. You can switch to a “ NoSQL ” system that promises more flexibility, but blaming the syntax has never made sense to me. SQL is not the problem, nor is your schema. The implementation underneath is what matters. There is nothing inherent in SQL that prevents it from adding columns and indexes without locking the table, or otherwise handling sparse dimensions gracefully . “There are only two kinds of languages: the ones people complain about and the ones nobody uses.” — Bjarne Stroustrup Say what you want about SQL, but it’s a pretty good language for manipulating data. It’s suspiciously difficult to design a new relational query language that doesn’t come to resemble it. In the ruthless, amnesiac world of technology, anything that’s been thriving for 40 years is probably doing something right. A pragmatic option for sparse data is to use a SQL database with a native JSON column type . That buys you sets, maps, lists, trees, and any combination. Adding a new attribute is as simple as writing new keys and values, no schema changes needed. But you can still use them in queries as you would normal columns. More to the point, those optional keys and values can live right next to the dense ones. It doesn’t have to be either/or. A few relational databases support JSON natively now, including SingleStore. You can store JSON as plain strings in any database, of course. Or serialized Python objects for that matter. But native support helps you find a middle ground between the constraints of fixed table schemas and turning your database into a glorified key/value store. The relational set operations at the core of your application have to happen somewhere, so why not let the database do what it was designed to do? For a table like this you’ll want to keep the dense attributes (say, anything over 30% full) as first-class typed columns. The sparse stuff goes into a JSON blob in the same table. create table persons (\n  id bigint unsigned primary key,\n  first_name varchar(64) not null default '',\n  last_name varchar(64) not null default '',\n  --- 10 more dense columns...\n  opt JSON not null\n); The structure of your queries hardly changes at all. Instead of filtering on sparse columns like this: WHERE maiden_name like '%jingleheimer-schmidt%' You do it like this: WHERE opt::$maiden_name like '%jingleheimer-schmidt%' If later on an optional column becomes heavily used, you can simply hoist it up to a first-class column and add an index: memsql> alter online table persons add column middle_name\n  as opt::$middle_name persisted varchar(64);\n\nmemsql> alter online table persons add key (middle_name); …and start using it right away. Your production SingleStore database won’t miss a beat. This hybrid table model is an effective way to handle the sparseness you can’t avoid and shouldn’t let get in your way. It saves space, time, and energy you can apply to more important things.", "date": "2014-02-26"},
{"website": "Single-Store", "title": "size-and-shape-of-tables", "author": ["Carlos Bueno"], "link": "https://www.singlestore.com/blog/size-and-shape-of-tables/", "abstract": "If you ever catch yourself doing mental arithmetic, or writing complex queries while trying to debug a capacity or performance problem, it’s a sign that your database is missing a feature. The information you need may exist, but it’s scattered. One of the most frequent feature requests from our users was more detailed statistics about memory use. In the current version of SingleStore tables are stored in memory (hence the name), so memory management and storage capacity planning are essentially the same activity. In SingleStore version 2.6 there is a new table in information_schema that exposes those statistics: Each row describes a “partition” of a distributed table, or a copy of a reference table . You can roll this table up in lots of ways to generate things like the total size of a table in the cluster. One thing to watch out for in any distributed database is “skew.” The rows in a database can be distributed around the machines in the cluster randomly, which ensures that every node has more or less the same amount of data. But they can also be sharded according to the values in one or more columns. For example, a “hits” table in a web traffic log could be sharded by the IP address of the client. This has many nice properties, but can also cause the data to be spread unevenly. An ISP might employ a proxy, for example, so a large amount of traffic might appear to come from a single IP address, and send a lot of rows to the same partition. To measure skew you count up the number of rows in and the space consumed by each partition and see how much they vary from the average. A handy statistical function for this is the “relative standard deviation”, expressed as stddev()/avg(). As a general rule of thumb, anything below 0.05 is an acceptable amount of skew. It’s nice to have the data available, but typing these queries from memory is tiresome. SingleStore Ops is a performance and monitoring application built on top of the database, and runs inside the browser. Building visualizations that combined table_statistics with the table schema was a joy. How new features get made There’s always too much to do. The set of features you could write is nearly infinite, so having a way to choose the right ones to do next is crucial. These new table stats features developed side-by-side in the interplay between our “engine” team, which builds the database, and the “ops” team, which builds monitoring and performance apps on top. Ops folks generally ask for better data, and Engine folks generally ask for better visualizations. Having a demanding customer on the inside shortens the feedback loop immensely and makes for better software. For example, the ops team wanted more detailed information about the size of values in each column in a table. They wrote a query that scanned every field in every row in every table in every database in the cluster, and set it to run every 60 seconds. This accomplished two things simultaneously. It allowed table statistics UI to be prototyped quickly using real data. It also horrified the engine team so much that they built a way to get at those statistics much more cheaply. “The best way to get the right answer on the Internet is not to ask a question, it’s to post the wrong answer.” — Cunningham’s Law All of these engine features are available today with SingleStore DB 2.6. The new UI features will ship with SingleStore Ops 3.0 in a few weeks. Enjoy!", "date": "2014-03-11"},
{"website": "Single-Store", "title": "memsql-tip-tuesday-how-to-use-the-memsql-cluster-installer", "author": ["Conor Doherty"], "link": "https://www.singlestore.com/blog/memsql-tip-tuesday-how-to-use-the-memsql-cluster-installer/", "abstract": "Whether installing SingleStore on premises or in the cloud, the Cluster Installer is a fast way to get started. The Cluster Installer is a simple command line tool that installs and configures an entire SingleStore Cluster to your specifications. Watch this short instructional video to see how easy it is. There are several advantages to using the Cluster Installer. It is faster and more error-proof than installing manually. Then, once you’ve installed SingleStore with the Cluster Installer, making configuration changes to your cluster is as simple as changing a value in the configuration file and running a single command. In addition, the Cluster Installer comes with a demo application that will have you running queries in minutes. For instructions on using the SingleStore Cluster Installer, click here .", "date": "2014-03-11"},
{"website": "Single-Store", "title": "how-memsql-distributes-data", "author": ["Carlos Bueno"], "link": "https://www.singlestore.com/blog/how-memsql-distributes-data/", "abstract": "The world is moving toward highly-parallel, distributed systems up and down the stack. It started with the app servers on the front lines. Now it’s happening at the data layer. This is generally a good thing, but these kinds of systems can sometimes feel like a foreign country. In this article we’ll talk about how SingleStore spreads the work around a cluster of servers without sacrificing ease of use. There are two kinds of machines in a SingleStore cluster: aggregators and leaves. The same software runs on both kinds of machines, and they all communicate using the same MySQL-compliant protocol. The only difference is in what role each one is told to play. Aggregators aggregate An aggregator is like a SQL proxy sitting in front of your cluster. Under normal circumstances, you only ever talk to the cluster through an aggregator. You connect to it with your favorite SQL client and insert rows, run selects, and manipulate data any way you want. From your perspective you are simply talking to “the” database server. The work of coordinating the parallel operations and combining everything into a single set of results is what an aggregator does. There could be any number of leaves acting in concert to answer your query, or only one. To you it’s completely transparent. Leaves… are where you leave data A “leaf” server is where your data gets stored. There’s nothing very complicated about it. Each leaf is an instance of SingleStore that acts independently. In fact, a leaf is barely aware that it’s part of a larger cluster of machines. All it knows is that some user named “root” logs into it from time to time and tells it to store or retrieve some data, and that it sometimes has to replicate data to and from its buddies. If you were to connect to a leaf with a SQL client and poke around, you’d see something like this: Each of those memsql_demo_N databases is a partition , created and managed by the aggregator. When you query “select count(1) from lineitem” from an aggregator, in the background it gets translated into parallel queries across the cluster. So where does the data go? As much as the aggregators try to maintain the illusion of a single consistent database, it helps to know a bit about how things work under the hood. A distributed system is fundamentally different from a single-box database. There are a few ways you can arrange your cluster, each with its tradeoffs. The simplest SingleStore cluster has one aggregator and one leaf. Let’s ignore the aggregator and focus on the leaf. When you create a database, by default it’s split into 8 partitions. Think of a donkey loaded up with a bunch of packages. This works just fine. Modern multi-core servers, running an in-memory database like SingleStore, can handle an enormous number of concurrent reads and writes very quickly. Because the data is partitioned even on a single leaf it’s easy to achieve parallelism and use all those expensive CPU cores to their fullest. But there’s only so much load a single donkey can carry. To expand the size of your cluster, you add another one and rebalance. This also works fine. As data gets inserted through an aggregator, the aggregator will make sure that both donkeys get equal amounts of data . In non-donkey terms, this configuration is like RAID-0. There is only one copy of your data, split up and distributed over multiple devices. As with RAID, you should use donkeys of similar sizes to achieve good balance. This configuration can take you pretty far: However, there is a potential problem. The more donkeys you add, the more likely it is that any one donkey will stumble or get sick, making part of the dataset unavailable. Since there is only one copy this increases the chances of the entire cluster failing. For best results you want two copies of each partition. This configuration operates more like RAID 1+0 . You can expand your cluster’s capacity more or less infinitely by adding more pairs of donkeys. In the case of a failure, the backup donkey automatically takes up the slack until full capacity is restored. This doesn’t mean that the backup donkeys are doing nothing. SingleStore high-availability is more fine-grained than that. Remember that each database is split into lots of partitions which are spread across the leaves. The aggregator makes sure each leaf machine gets some of the active partitions and some of the backups, so you can use all of your cluster’s CPUs during normal operation. In other words, your data is striped . Building a SQL-compliant database that can scale over thousands of machines is bigger deal that it might sound. Most of the hard scaling and performance problems I’ve observed with traditional single-box SQL databases come down to two causes: I/O bottlenecks on the disk, and the fact that it runs on a single box. There’s only so much RAM and CPU power you can cram into a single machine. If the success of Google, Facebook, and Amazon have taught us anything, it’s the importance of scaling horizontally over lots of commodity computers. But it’s been too easy to throw the baby out with the bath water, to give up a powerful query language in exchange for scale. A distributed “No SQL” system like MongoDB is a relatively easier programming problem than the one we’ve tackled. MongoDB, for instance, doesn’t yet support very high rates of concurrent reads and writes. For years Mongo ignored the hard bits of supporting relational set operations. Even now it has only partial support for joins and aggregations, and of course you have to learn a new query language to use it… a language that seems to be edging closer and closer to SQL itself. Our approach is to cut to the chase and give you a highly-concurrent, distributed SQL database, built out of parts that are straightforward to understand. A couple of details There are two kinds of tables in SingleStore : reference tables and distributed. Distributed tables are laid out as described here. Reference tables are intended for smaller data that tends to get joined frequently along many dimensions. To make this more efficient, reference tables are replicated on each leaf and aggregator. In data warehouse terms, the tiny dimension tables are reference tables, and gigantic fact tables are distributed. There are two kinds of aggregators: the master and the children. There is no real difference between the software running on them. Master is just a role assigned to one (and only one) aggregator at a time. The master is responsible for DDL commands like adding or dropping tables, rearranging partitions, and making sure all the other aggregators have consistent metadata about the cluster. All the normal stuff like inserts and deletes can be handled by any aggregator, even when the master is down. In the event of a failure, you can easily promote any aggregator to be the master, and the cluster marches on.", "date": "2014-03-18"},
{"website": "Single-Store", "title": "announcing-memsql-startcup-2-0", "author": ["Nikita Shamgunov"], "link": "https://www.singlestore.com/blog/announcing-memsql-startcup-2-0/", "abstract": "SingleStore is excited to announce Start [ c ] up 2.0 – the second annual programming competition hosted by Codeforces with an onsite at SingleStore HQ in San Francisco, California. Start [ c ] up 2.0 consists of two rounds. Round 1 is online and takes place on July 27th at 10:00 AM PST. Round 1 follows regular Codeforces rules and consists of 5 problems. For this round, the complexity of the problems will be comparable to a regular Codeforces round. There are no eligibility restrictions to participate in the round. Round 2 takes place on August 10th at 10:00 AM PST, consists of 6 problems, and uses regular Codeforces rules. The complexity of the problems is higher than a regular Codeforces round. Only people who finished in the top 500 in Round 1 can participate. The top 100 in round 2 will receive a Start [ c ] up 2.0 T-shirt. For Silicon Valley residents, SingleStore will be hosting up to 25 people on-site during the second round. The winner of the on-site round will be awarded a special prize.", "date": "2014-07-21"},
{"website": "Single-Store", "title": "get-in-the-delorean-its-time-for-the-database-news-roundup", "author": ["Conor Doherty"], "link": "https://www.singlestore.com/blog/get-in-the-delorean-its-time-for-the-database-news-roundup/", "abstract": "Stop it database industry, we’re blushing. Imitation is the sincerest form of flattery and, as the spring conference season gives way to summer vacations, we’ve noticed a flood of announcements from database vendors doing their best SingleStore impressions. Here are a few stories that caught our attention. Oracle and SQL Server Go In-Memory The new Oracle 12c and SQL Server 2014 both feature in-memory storage engines on top of their existing disk-based storage. Seek latency and disk contention are dirty little (open) secrets among legacy database vendors, so the move makes sense as the granddaddies of the industry try to update their technology for real-time applications. Oracle even went so far as to title their announcement webcast, “The Future of the Database Begins Soon.” SingleStore agrees that the Future of Databases is in-memory. It’s kind of our thing. In fact, we’ve had an in-memory database on the market for a while, so I guess the Future is now, or the recent past…we tore a hole in the space-time continuum! Postgres is now also Pregres and cloud computing means using a laptop from your hoverboard. Anyway, it’s worth noting that these recently announced in-memory storage engines are built on top of legacy technology. Oracle 12c and SQL Server were both originally designed to run on a single machine, not in a distributed environment. You can shard your database, but you’re going to end up doing extra computations client-side and find severe performance degradation when you scale beyond a few nodes. Conversely, SingleStore was originally designed as a distributed database. You get linear performance improvement as you add nodes, sharding is automatic, and the distributed query optimizer makes use of all system resources. This is a Future Oracle and Microsoft have yet to realize. Spark 1.0 Includes Spark SQL Alpha If Oracle and Microsoft are the granddaddies of the database industry, then Apache Spark is kind of like a bright but moody teenager. While there has been some confusion recently as to whether Spark is in fact a “speedy Swiss Army Knife 1 ,” the point remains that Spark is an interesting technology and will probably see wider adoption in the future. If you’ve been paying attention, you probably know that the biz is a-buzz about Spark. Spark…so hot right now…Spark. An interesting point about the Spark 1.0 release is that it contains the alpha version of Spark SQL. Why would they go and sully this Hot New Technology with boring old SQL? For good reason, actually: because people already know how to use it and because it’s a simple, powerful query language. Spark isn’t alone in adding SQL support. Usability by anyone other than experienced developers has been an impediment to wide(er) spread Hadoop adoption. In response, SQL on Hadoop engines (Hive, Impala, et al.) have grown in popularity as a way to make data accessible to analysts and other business types. Even NoSQL query languages are starting to look more SQL-y as they try to appease developers who are getting nostalgic for the days when they could run JOINs. At SingleStore, we also like SQL. We like it so much that we put it in our name and built a database around it. SQL may not be the new hotness, but it enables organizations to realize the full value of (i.e. actually use) their new Big Data solution. SAP Offers HANA Cloud Platform-as-a-Service I’ll leave it to the SAP marketing folks to decide whether Platform-as-a-Service still counts as an “appliance,” but HANA Cloud PaaS is now available. The move makes sense given the restrictive hardware requirements on traditional HANA deployments and the fact that if it ain’t cloud, it ain’t current. It also ain’t cheap. It costs $83,295 per month for 1TB of RAM 2 , or about a million dollars a year. SingleStore has always run in cloud and virtualized environments on commodity hardware. https://www.singlestore.com/ lets you deploy to AWS with a few clicks and the time it takes to make a fried egg. SingleStore doesn’t publicly discuss pricing information (you’ll have to contact sales), but even this marketing guy knows that a million dollars is going to get you a lot more than a single terabyte. We also think the transparency and flexibility of an easy deploy-it-yourself approach outweigh the perceived convenience of PaaS. Predicting Future News Cycles I have been to the Future and, before the memory loss effects of time travel set in, I’m going to tell you what’s coming next. At SingleStore, we see a future where customers are using Hybrid Transaction and Analytical Processing (HTAP). Gartner published a whitepaper 3 a few months ago explaining that HTAP, or the ability to carry out OLTP (or “operational”) and OLAP workloads simultaneously in a single database of record, will drive innovation and open up new channels of revenue. Applications like anomaly detection, real-time ad serving, personalized consumer recommendations, and real-time operational analytics will enable organizations to extract more value from Big Data and turn new opportunities for customer engagement and operational efficiency into cash. Thanks to some novel technology 4 like lock-free data structures and compiled query plans, HTAP is already possible with SingleStore. Try it today! Download a SingleStore trial today before the rip in the space-time continuum closes and you just have to sit around waiting for the Future (that’s already here). 1 [http://www.infoworld.com/d/application-development/straight-talk-apache-spark-and-why-you-should-care-243737](http://www.infoworld.com/d/application-development/straight-talk-apache-spark-and-why-you-should-care-243737) 2 [http://www.pcworld.com/article/2105000/sap-adds-subscription-pricing-for-hana-cloud-services.html](http://www.pcworld.com/article/2105000/sap-adds-subscription-pricing-for-hana-cloud-services.html) 3 [https://www.gartner.com/doc/2657815/hybrid-transactionanalytical-processing-foster-opportunities](https://www.gartner.com/doc/2657815/hybrid-transactionanalytical-processing-foster-opportunities) 4 [https://www.singlestore.com/managed-service/](/managed-service/)", "date": "2014-06-17"},
{"website": "Single-Store", "title": "debunking-myths-surrounding-the-modern-database-landscape", "author": ["Kate Borger"], "link": "https://www.singlestore.com/blog/debunking-myths-surrounding-the-modern-database-landscape/", "abstract": "Is your organization experiencing common NoSQL pain points around performance, scale, and flexibility? New database technologies can help eliminate ETL and provide instant access to your data to allow for data-driven decisions that happen in real-time. Join SingleStore on Tuesday, September 9 at 10 am PT to learn more about the realities of NoSQL, misconceptions about MongoDB, and where NoSQL, NewSQL, and SingleStore fit into the modern database landscape.", "date": "2014-09-04"},
{"website": "Single-Store", "title": "running-memsqls-107-node-test-infrastructure-on-coreos", "author": ["Carl Sverre"], "link": "https://www.singlestore.com/blog/running-memsqls-107-node-test-infrastructure-on-coreos/", "abstract": "At a recent CoreOS meetup, I gave a presentation on the design and implementation of Psyduck, which is the name for SingleStore’s test infrastructure.  In this talk, I’ll explain how SingleStore runs over 60,000 tests every day on our 107 machine cluster running CoreOS.", "date": "2014-09-10"},
{"website": "Single-Store", "title": "c-error-handling-with-auto", "author": ["Marko Tintor"], "link": "https://www.singlestore.com/blog/c-error-handling-with-auto/", "abstract": "Every professional C++ engineer sooner or later asks herself a question: “How do I write exception safe code?” The problem is far from trivial: http://www.stroustrup.com/except.pdf http://www.gotw.ca/gotw/059.htm … Consider the following example: bool Mutate(Logger* logger)\n{\n    logger->DisableLogging();\n    AttemptOperation();\n    AttemptDifferentOperation();\n    logger->EnableLogging();\n    return true;\n} If AttemptOperation() throws an exception we will not call EnableLogging() on the logger object. If you don’t use exceptions and use return codes instead to propagate errors the problem doesn’t go away. You use error codes, but you’ll realize that now you need to call EnableLogging() in multiple places. bool Mutate(Logger* logger)\n{\n    logger->DisableLogging();\n    if (!AttemptOperation())\n    {\n        logger->EnableLogging();\n        return false;\n    }\n    if (!AttemptDifferentOperation())\n    {\n        logger->EnableLogging();\n        return false;\n    }\n\n    logger->EnableLogging();\n    return true;\n} The remedy is well researched: Use a try catch block or a so called Auto object, whose sole purpose is to call EnableLogging() in it’s destructor. Auto object is better because it’s exception agnostic, it will work regardless of the fact if you use exceptions or not. class AutoDisableLogging\n{\npublic:\n    AutoDisableLogging(Logger* logger): m_logger(logger)\n    {\n        m_logger->DisableLogging();\n    }\n\n    ~AutoDisableLogging()\n    {\n        m_logger->EnableLogging();\n    }\n\nprivate:\n    Logger* m_logger;\n}; (Class above should also have assignment operator and copy constructor defined as it has custom destructor, but I’ve omitted them for clarity). Now you can safely put AutoDisableLogging autoDisableLogging; in the beginning of your function and be safe. However the number of such operations quickly grows and it becomes impractical to create auto classes for all of such cases. Ideally you could separate Auto part from Enable/DisableLogging() part and write something like this: bool Mutate(Logger* logger)\n{\n    logger->DisableLogging();\n    Auto(logger->EnableLogging());\n    AttemptOperation();\n    AttemptDifferentOperation();\n    logger->EnableLogging();\n    return true;\n} which ensures that the statement passed in as a parameter is called at the end of the scope. If it could capture variables by reference you could write things like: Transaction xact;\nbool isCommited = false;\nAuto(if(!isCommited) xact.Rollback());\n\nif (!replicaNetworkThread.InitReplicateDatabase())     \n    return false;\nif (!InsertReplicaRecord(xact))\n    return false;\nif (!dbMgr->PrepareReplicationThreads())\n    return false;\n\nxact.Commit();\n// No failures beyond this point\nisCommited = true;\n\n....\n\nreturn true; Notice that the value of isCommited is not saved at the time Auto() statement is seen the first time, but is read at the time when statement inside Auto is executed. If you use Auto() multiple times in the same scope, order of execution of Auto’s at the end of the scope must be reversed: dbMgr.StartChangingReplayState();\nAuto(dbMgr.StopChangingReplayState());\n\ndbMgr.AcquireTablesLock();\nAuto(dbMgr.ReleaseTablesLock());\n\ndbMgr.AcquireBeginSnapshotLock();\nAuto(dbMgr.ReleaseBeginSnapshotLock()); How would you implement Auto() described above? You can use a macro and construct C++11 lambda around the statement passed in to macro and do something like: class AutoOutOfScope\n{\npublic:\n    AutoOutOfScope(std::function<void()> destructor)\n        : m_destructor(destructor) { }\n    ~AutoOutOfScope() { m_destructor(); }\nprivate:\n    std::function<void()> m_destructor;\n}\n\n#define Auto(Destructor) AutoOutOfScope auto_oos([&]() { Destructor; }) There are couple of problems with this implementation. First problem is the duplicate name conflict on “auto _ oos” if Auto() is used more than once in the same scope. We can solve this with concatenating special macro COUNTER at the end of our name (Why don’t we use LINE instead?). #define TOKEN_PASTEx(x, y) x ## y\n#define TOKEN_PASTE(x, y) TOKEN_PASTEx(x, y)\n\nclass AutoOutOfScope\n{\npublic:\n    AutoOutOfScope(std::function<void()> destructor)\n        : m_destructor(destructor) { }\n    ~AutoOutOfScope() { m_destructor(); }\nprivate:\n    std::function<void()> m_destructor;\n}\n\n#define Auto(Destructor) AutoOutOfScope TOKEN_PASTE(auto_, __COUNTER__)([&]() { Destructor; }) Second problem is that conversion to std::function will allocate memory internally which we would like to avoid. We can avoid it by templatizing AutoOutOfScope class and passing lambda without converting it to std::function<void() > . With both of these issues fixed code will look like this: template \nclass AutoOutOfScope\n{\npublic:\n   AutoOutOfScope(T& destructor) : m_destructor(destructor) { }\n   ~AutoOutOfScope() { m_destructor(); }\nprivate:\n   T& m_destructor;\n};\n\n#define Auto_INTERNAL(Destructor, counter) \\\n    auto TOKEN_PASTE(auto_func_, counter) = [&]() { Destructor; };\n    AutoOutOfScope<decltype(TOKEN_PASTE(auto_func_, counter))> TOKEN_PASTE(auto_, counter)(TOKEN_PASTE(auto_func_, counter));\n\n#define Auto(Destructor) Auto_INTERNAL(Destructor, __COUNTER__) That is it! There are close to 500 usages of Auto on the SingleStore codebase. It has become one of our most favourite abstractions.", "date": "2014-09-15"},
{"website": "Single-Store", "title": "seven-databases-in-seven-weeks-memsql-at-cmu", "author": ["Ankur Goyal"], "link": "https://www.singlestore.com/blog/seven-databases-in-seven-weeks-memsql-at-cmu/", "abstract": "Last week I gave a presentation as part of the “Seven Databases in Seven Weeks” seminar series designed to showcase new technologies available for front-end application developers. Hosted by Carnegie Mellon University, the series features leading developers of NoSQL and NewSQL database management systems. Watch the presentation below to learn about Hybrid Transactional and Analytical Processing (HTAP) and SingleStore’s key innovations that enable us to deliver on being a powerful, in-memory, HTAP-driven database. View in a new window", "date": "2014-09-16"},
{"website": "Single-Store", "title": "memsql-announces-strategic-investment-from-in-q-tel", "author": ["Kate Borger"], "link": "https://www.singlestore.com/blog/memsql-announces-strategic-investment-from-in-q-tel/", "abstract": "Today SingleStore is excited to announce an investment from In-Q-Tel (IQT), the strategic investment firm that identifies innovative technology solutions to support the missions of the U.S. Intelligence Community. SingleStore is used by enterprise companies such as Comcast, Shutterstock, and Zynga, and IQT’s recent investment will allow us to generate market opportunities within the government space. The investment from IQT further validates SingleStore’s leadership in the distributed in-memory database market, and we are pleased to be able to work with IQT to help bring deep insights to the U.S. Intelligence Community. Read the full press release on SingleStore’s website.", "date": "2014-09-23"},
{"website": "Single-Store", "title": "the-internet-of-furbies", "author": ["Neil Dahlke"], "link": "https://www.singlestore.com/blog/the-internet-of-furbies/", "abstract": "At SingleStore engineering there are few things we love as much as building great products. One, however, is a good laugh. This past weekend a team from SingleStore set out to make our mothers proud at Cultivated Wit ’s Comedy Hack Day , an event for comedians and hackers to get together in attempt to make the funniest hack. And drink whiskey. I’m very excited to share with you our product today, the grand prize winner, and the future of wearable technology. Engineering at SingleStore goes to great lengths to ensure that our technology enhances your life, rather than detracting from your special moments. Our engineering and design teams have determined that the best place for strapping technology to your body is not to your face or to your wrist, but rather, to your shoulder. The wait is over, the next furry thing is here. We present to you: AwwCog – the future of wearable technology. It comes in the perfect size for everyone . Isn’t it beautiful? Wait a second, then look again. It’s still beautiful, isn’t it? It’s even programmable in python! Since the original Furby was invented, nothing has been this revolutionary. We’re awesome. We know. If you’re an engineer that likes to laugh as much as you like to code, we want to talk to you. Reach out to us at recruiting@singlestore.com , and let us know who you are, and what you like to build. For the technical folk that want to build their own AwwCogs, or contribute to the code, the repos can be found at: Furback open-furby-platform", "date": "2014-10-01"},
{"website": "Single-Store", "title": "memsql-does-oracles-own-demo-ten-times-as-fast-sixty-times-cheaper", "author": ["Nikita Shamgunov"], "link": "https://www.singlestore.com/blog/memsql-does-oracles-own-demo-ten-times-as-fast-sixty-times-cheaper/", "abstract": "In preparation for Open World, I asked some of our engineers to recreate a demo that Oracle has been using over the last year to show off their “in-memory option.” It’s impressive to look at: the demo shows the database searching through billions of records from Wikipedia search trend data for popular terms in less than a second. The thing about Oracle’s demo is it runs on a gigantically expensive server. In fact it is the biggest one they have at 32TB of RAM and hundreds of CPU cores. They are proud to say that it costs “only” three million dollars. Here is a picture of the beast, which is bigger than your refrigerator. Distributed databases that scale horizontally on commodity hardware are the way of the future. To prove that, we created a modest cluster on Amazon and built the same demo using SingleStore. The cluster consists of just 6 Amazon nodes. We’ve been presenting our new demo at Oracle OpenWorld this week, and the responses have been incredible. People can search the Wikipedia data set for popular keywords and trends over time. The ideas they have come up with to search are amazing! And all of it runs on a 6-node cluster that costs about $5/hr on Amazon or less than $20k if you wanted to buy 6 servers outright. All in all it cost less than the t-shirts we gave away at the show. One of the reasons that SingleStore’s version goes much faster for these queries is that it supports sorted columnar tables.  Oracle In-Memory Option is a columnar compressed dataset with no ordering information. SingleStore’s compression format allows it to perform operations on compressed data, and take advantage of the sort order. Ultimately this demonstrates how elastic, cloud friendly databases provide a much lower cost of ownership compared to monolithic (literally) appliances. Try SingleStore for 90 days: http://www.singlestore.com/free/ SingleStore is an in memory database that enables businesses to process transactions and run analytics in real-time with SQL.", "date": "2014-10-01"},
{"website": "Single-Store", "title": "apache-spark-predictions", "author": ["Kevin White"], "link": "https://www.singlestore.com/blog/apache-spark-predictions/", "abstract": "While at Strata+Hadoop World, SingleStore CEO, Eric Frenkiel , sat down with Mike Hendrickson of O’Reilly Media for a conversation about the evolution of the database ecosystem, and how Hadoop and in-memory computing fit into the picture. Watch this 8 minute interview to learn more about: Developments throughout the in-memory data technology market, and where it will be 12 months from now. How Apache Spark fits into the Hadoop Ecosystem and why it will replace MapReduce. The importance of real-time analytics and how it can be applied to business. Benefits that data-driven organizations are gaining from taking a hybrid approach to transactional and analytical data processing. Get The SingleStore Spark Connector Guide The 79 page guide covers how to design, build, and deploy Spark applications using the SingleStore Spark Connector. Inside, you will find code samples to help you get started and performance recommendations for your production-ready Apache Spark and SingleStore implementations. Download Here", "date": "2014-10-20"},
{"website": "Single-Store", "title": "memsql-and-cisco-work-together-to-make-real-time-performance-on-hadoop-a-reality", "author": ["Conor Doherty"], "link": "https://www.singlestore.com/blog/memsql-and-cisco-work-together-to-make-real-time-performance-on-hadoop-a-reality/", "abstract": "While Hadoop is great for storing large volumes of data, it’s too slow for building real-time applications. However, our recent collaboration with Cisco provides a solution for Hadoop users who want a better way of processing real-time data. Using Cisco’s Application Centric Infrastructure including APIC and Nexus switch technology, we’ve been able to demonstrate exceptional throughput on concurrent SingleStore and Hadoop 2.0 workloads. Here’s How It Works Cisco’s new networking technology automatically prioritizes smaller packet streams generated by real-time workloads over the larger packet streams typically generated by Hadoop. This enables impressive throughput on clusters running simultaneous SingleStore and Hadoop workloads. At the Strata + Hadoop conference last week in New York, Cisco demonstrated the solution on an 80 node cluster running both SingleStore and Hadoop. Without additional network traffic, the cluster can serve 2.4 million reads per second from SingleStore’s in-memory database. Without packet-prioritization, the database’s performance drops to under 600 thousand reads per second when a simulated Hadoop workload is added to saturate the cluster’s network. With packet-prioritization, the performance recovers to 1.4 million reads per second, more than doubling the throughput. Why Does It Matter? This advance provides the ability to collocate SingleStore, for real-time, mission critical data ingest and analysis, with Hadoop workloads that are less time-sensitive and executed as large batch jobs on historical data. By combining Hadoop’s storage infrastructure with SingleStore’s real-time data processing ability, businesses get the best of both worlds: real-time analytics with Hadoop scale workloads. As an added bonus, the solution allows businesses to save on hardware costs by running SingleStore and Hadoop together on the same cluster. Cisco/SingleStore Demo from SingleStore on Vimeo . If you want to learn more, contact a SingleStore representative at sales@singlestore.com or at (855) 463-7660.", "date": "2014-10-21"},
{"website": "Single-Store", "title": "cache-is-the-new-ram", "author": ["Carlos Bueno"], "link": "https://www.singlestore.com/blog/cache-is-the-new-ram/", "abstract": "One of the (few) advantages of being in technology for a long time is that you get to see multiple tech cycles beginning to end. You get to see how breakthroughs actually propagate. If all you have seen is a part of the curve, it’s hard to extrapolate correctly. You either overshoot the short-term progress or undershoot the long. What’s surprising is not how quickly the facts on the ground change, but how slowly engineering practice changes in response. This is a Strowger switch, an automated way to connect phone circuits. It was invented in 1891. In 1951, right on the cusp of digital switching, the typical central switching office was basically a super-sized version of the Victorian technology. There was a strowger switch for every digit of every phone call in progress. From the perspective of the time, this was the highest of high technology . Of course from our perspective, it was the world’s largest Steampunk art installation. It’s probably a mistake to feel superior about that. It’s been 65 years since the invention of the integrated circuit, but we still have billions of these guys around, whirring and clicking and breaking. It’s only now that we are on the cusp of the switch to fully solid-state computing. The most exciting kinds of technological shifts are when a new model finally becomes feasible, or when an old restriction falls away. Both kinds are happening right now in our industry. Distributed computing is becoming the dominant programming model throughout the entire software stack. The so-called “Central Processing Unit” is no longer central, or even a unit. It’s only one of many bugs crawling over a mountain of data. The database is the last holdout. At the same time, the latency gap between RAM and hard drive storage is becoming irrelevant. For 30 years the central fact of database performance was the gigantic difference in the time it takes to access a random piece of data in RAM versus on a hard drive. It’s now feasible to skip all that heartache by placing your data entirely in RAM. It’s not as simple as that, of course. You can’t just take a btree, mmap it, and call it a day. There are a lot of implications to a truly memory-native design that have yet to be unwound. These two trends are producing an entirely new way to think about, design, and build applications. So let’s talk about how we got here, how we’re doing, and hints about where the future will take us. Back in the day, every component in the architecture diagram had a definite article attached to it. Each thing was a separate function: “the” database and “the” web server, characters in a one-room drama. Incidentally, this is where the term “the cloud” came from. A fluffy cloud was the standard symbol for an external WAN whose details you didn’t have to worry about. Distributed computing hit the mainstream with the lowest-hanging fruit. Multiple identical application servers were hidden behind a “load balancer” which spread the work more or less evenly. Load-balancing only the stateless bits of the architecture sidestepped a lot of philosophical problems. As the system scaled up, those components outflanked and eventually surrounded “the” database. We told ourselves that it was normal to spend more on special database hardware with fast disks and a faster CPU, and it was only one machine anyway. The hardware vendors were happy to take our money. Eventually, database replication became reasonable and we salved our consciences by adding a hot spare database. We then told ourselves there were no longer any single points of failure. It was even true — for a few minutes. That hot spare was too tempting to leave sitting idle, of course. Once the business analysts realized they could run gigantic queries on live production data without touching production, the so-called “hot spare” became nearly as busy and mission-critical as the production copy. We told ourselves it would be fine because if the spare is ever needed we can just take it from them for the duration of the emergency. But that’s like saying you don’t really need to carry a spare tire because you can always steal one from another car. Then Brad Fitzpatrick released memcached, a daemon that caches data in memory. (Hence the name.) It was amazingly pragmatic software, a simplified version of the distributed hash tables then in vogue in academia. It had lots of features: a form of replication, sharding, load balancing, simple math operators. We told ourselves that most of our load was reads, so why make the database thrash the disk running the same queries over and over again? All you needed was a bunch of small-calibre servers with tons of RAM, and of course the hardware vendors were happy to take our money. And… maybe you have to write some cache invalidation code. That doesn’t sound too hard. Right? To its credit, the memcached design took things a pretty long way. It replaced the random IO performance of a hard drive with the random IO performance of multiple banks of RAM. Even so, the database machine kept getting bigger and busier. We realized that caching cost at least as much RAM as the working set (otherwise it was ineffective), plus the nearly unbearable headache of cache consistency. But we told ourselves that was the cost of “web scale”. More worrisome was that applications were getting more sophisticated and chattier. Multiple database writes were being performed on almost every hit. Writes, not reads, became the bottleneck. This is when we finally got serious about sharding the database. Facebook initially sharded its user data by university and got away with concepts like “The Harvard Database” for a surprisingly long time. Flickr is another good example. They hand-built a sharding system in PHP that split the database up by a hash of the user ID, in much the same way that memcached shards on the key. In their tech talks there are jolly hints about having to denormalize their tables and double-write objects such as comments, messages, and favorites. But that’s a small price to pay for infinite scaling that solves everything ever. Right? The problem with sharding a relational database by hand is that you no longer have a relational database. The API that orchestrates the sharding has in effect become your query language. Your operational headaches didn’t get better either; the pain of altering schemas across the fleet was actually worse. This was the point at which a lot of people took a deep breath, catalogued all the limitations and warts of their chosen implementation of SQL … and for some reason decided to blame SQL. A flood of hipster NoSQL and refugee XML databases appeared, all promising the moon. They offered automatic sharding, flexible schemas, some replication… and not much else at first. But it was less painful than writing it yourself. You know things are really desperate when “less painful than writing it yourself” is the main selling point. Moving to NoSQL wasn’t worse than hand-sharding because we’d already given up any hope of using the usual client tools to manipulate and analyze our data. But it wasn’t much better either. What used to be a SQL query written by the business folks turned into hand-written reporting code maintained by the developers. Remember that “hot spare” database we used to use for backups and analytics? It came back with a vengeance in the form of Hadoop filestores and Hive querying on top. Now this worked, and largely got the business folks off our backs. The biggest problem is the operational complexity of these systems. Like the Space Shuttle, they were sold as reliable and nearly maintenance-free but turn out to need a ton of hands-on attention. The second biggest problem is getting the data in and out; a lag time of one day (!) was considered pretty good. The third problem is that it manages to be I/O-bound on both network and disk at the same time. We told ourselves that was the price of graduating to BIG DATA. Anyway, that’s how Google does it. Right? As various NoSQL databases matured, a curious thing happened to their APIs: they started looking more like SQL. This is because SQL is a pretty direct implementation of relational set theory, and math is hard to fool. To paraphrase Paul Graham’s unbearably smug comment about Lisp: once you add group by, filter, & join, you can no longer claim to have invented a new query language, only a new dialect of SQL. With worse syntax and no optimizer. Because we had taken this strange detour away from SQL, crucial bits missing from most of the systems are a storage engine and query optimizer designed around relational set theory. Bolting that on later led to severe performance hits. For the ones that got it right (or papered it over by being resident in RAM) there were other bits missing like proper replication. I know of one extremely successful web startup you’ve definitely heard of that uses four, count ‘em, FOUR separate NoSQL systems to cover the gaps. It’s pretty clear that there’s no going back to “the” database and 10-million-nanosecond random seek times. Underneath the endless hype cycles in search of the One True Thing To Solve Everything Ever is an interesting pattern: a pain point relieved by a clever approach that comes with an new pain point. So what’s the next complex gadget to add to this dog’s breakfast? Maybe the real trick is to make things simpler. For instance, RAM: You have lots of RAM in the “database” machines, for caching and calculation. You also have lots of RAM in the Memcached machines. The sum of RAM in those systems should be at least equal to the size of your working data set. If it isn’t then you’ve under-bought. Also, I very much doubt that your caching layers are 100% efficient. I’ll bet money you have plenty of data that are cached and never read again before eviction. I’ll bet more money you don’t even track that. That doesn’t mean you’re a bad person. It means that caching is often more trouble than it’s worth. A lot of the features each of these components provide seem to be composable and complementary to each other. If only they could be arranged better. Once you take it as axioms that the system will be distributed and the data will always be solid-state, a curious thing happens: it all gets much simpler. The “temporary” memory data structures you’d normally only use during query invocation becomes the only structure there is. Random access is no longer a cardinal sin; it’s the normal course of business. You don’t have to worry about splitting pages, or rebalancing, or data locality. This is a nice, simple architecture. Just as load balancers abstract away the application servers, SQL “aggregators” abstract away the greasy details of orchestrating the reading and writing of data.  This keeps the guts of the data placement strategies behind a stable API, which allows both sides to make changes with less disruption. So it’s all good now, right? We’re finally arrived at the happy place at the end of history. Right? It’s a mistake to feel complacent about the state of the art of computing, no matter when you live. There’s always another bottleneck. This is the AMD “Barcelona” chip, a relatively modern design. It has four cores but the majority of the surface is taken by the cache and I/O areas surrounding cores, like a giant parking lot around a WalMart. In the Pentium era cache was only about 15% of the die. The third, quieter, revolution in computing is how much faster the CPU has gotten relative to memory. There’s a reason all this expensive real estate is now reserved for cache. The central fact of database performance used to be the latency gap between RAM and disk. At the moment we’re kidding ourselves that the latency gap between CPU cache and RAM isn’t exactly the same kind of problem. But it is. And as much as we like to pretend that shared memory actually exists, it doesn’t. With lots of cores and lots of RAM, inevitably some cores will be closer to some parts of RAM. When you get right down to it, a computer really does only two things: read symbols and write symbols. Performance is a function of how much data the computer must move around, and where it goes. The happiest possible case is a endless sequential stream of data that’s read once and dealt with quickly, never to be needed again. GPUs are a good example of this. But most interesting workloads aren’t like that. Every random pointer that’s chased almost guarantees a cache miss. Every contention for the same area of memory (eg a write lock) causes huge coordination delay. Even if your CPU cache-hit rate was 99%, which it isn’t, time spent waiting on RAM would still dominate. Or put it this way: if disk is the new tape, and RAM is the new disk, then the CPU cache is the new RAM. Locality still matters. So what will solve this problem? It seems that there’s the same old fundamental conflicts: do we optimize for random access, or serial? Do we take the performance penalty on writes, or reads? Can we just sit tight and let the hardware catch up? Maybe memristors or  other technology will make all of this irrelevant. Well, I want a pony, too. The good news is that the gross physical architecture of distributed databases seems to be settling down. Data clients no longer need to deal with the guts and entrails of 4 or 5 separate subsystems. It’s not perfect yet; it’s not even mainstream yet. Breakthroughs take a while to propagate. But if the next bottleneck really is memory locality, that means the rest of it has become mature. New innovations will tend to be in data structures and algorithms. There will be fewer sweeping architectural convulsions that promise to fix everything ever. If we’re lucky, the next 15 years will be about SQL databases quietly getting faster and more efficient while exposing the same API. But then again, our industry has never been quiet. To download SingleStore please visit: http://www.singlestore.com/free/ SingleStore is an in memory database that enables businesses to process transactions and run analytics simultaneously with SQL.", "date": "2014-11-19"},
{"website": "Single-Store", "title": "chris-fry-joins-memsql-advisory-board", "author": ["Conor Doherty"], "link": "https://www.singlestore.com/blog/chris-fry-joins-memsql-advisory-board/", "abstract": "We are excited to announce that web scaling expert and technology growth executive Chris Fry has joined the SingleStore advisory board. Chris will provide technical and organizational expertise to SingleStore as we expand the capabilities and market adoption of our distributed in-memory database. Scaling expert @chfry former executive at @twitter and @salesforce joins @SingleStoreDB advisory board – Click to Tweet Chris has broad experience in scaling both technology infrastructure and engineering organizations through high growth phases. Most recently he served as Senior Vice President of Engineering at Twitter where he killed the “Fail Whale” and created a more reliable service, led the rapid growth of the engineering team, drove the strategy for storage and compute, and controlled the technology cost structure for the company’s IPO. Prior to Twitter, Fry was the Vice President of Software Development at Salesforce.com covering all areas including applications, platform, core, and Chatter. “When people work well together, their output is unimaginably better.” — Chris Fry, First Round Capital CTO Summit Chris brings a wealth of knowledge about scaling both software and businesses, and has developed a management philosophy for promoting stability in an engineering organization without sacrificing speed or the ability to innovate. We are looking forward to working with him! You can read the full news release here .", "date": "2014-12-11"},
{"website": "Single-Store", "title": "gary-orenstein-joins-memsql-as-chief-marketing-officer", "author": ["Conor Doherty"], "link": "https://www.singlestore.com/blog/gary-orenstein-joins-memsql-as-chief-marketing-officer/", "abstract": "News release: SingleStore Welcomes Gary Orenstein as Chief Marketing Officer SAN FRANCISCO, December 18, 2014 – SingleStore, the leader in real-time and historical Big Data analytics, today announced that Gary Orenstein has joined the company as Chief Marketing Officer. In this role, Orenstein will lead the marketing strategy, growth, communications, and customer engagement as SingleStore expands adoption of its distributed, in-memory database, enabling companies to capture, store, and query terabytes of data in real-time. Orenstein brings 20 years of experience in high technology marketing to SingleStore most recently at Fusion-io where he was SVP of Products and then Chief Marketing Officer. Orenstein joined Fusion-io six months before its initial public offering and over the next several years Fusion-io grew to achieve over $400 million in annual sales and was ultimately acquired by SanDisk for $1.1 billion. At Fusion-io, Orenstein grew the product organization to three product lines, simplified the product position for flash solutions, and led experiential initiatives such as the Fusion-io Performance Cafe. Prior to Fusion-io, Orenstein worked at infrastructure companies across file systems, caching, and high speed networking. He also served as the vice president of marketing at Compellent, which went public in 2007 and was acquired by Dell in 2010. “Our interconnected and interactive lives require a new approach to recording actions and making decisions,” said Orenstein. “Data is at the heart of it all and SingleStore has seized the opportunity with a distributed in-memory database that provides a unique approach to big data operational analytics. Entire industries have the opportunity to rethink instantaneous data and SingleStore has tapped the forefront of this wave.” “We are thrilled to have Gary join the SingleStore team,” added Eric Frenkiel, SingleStore co-founder and CEO. “He brings a detailed understanding of the need for memory-based solutions in today’s data-driven world. As we continue to expand our business, Gary and the marketing team will be instrumental in sharing our story.” About SingleStore SingleStore is the leader in real-time and historical Big Data analytics based on a distributed in-memory database. SingleStore is purpose built for instant access to real-time and historical data through a familiar SQL interface and uses a horizontally scalable distributed architecture that runs on commodity hardware. Innovative enterprises use SingleStore to accelerate time-to-value by extracting previously untapped value in their data that results in new revenue. SingleStore is proven in production environments across hundreds of nodes in high velocity Big Data environments. Based in San Francisco, SingleStore is a Y Combinator company funded by prominent venture capitalists and angel investors including Accel Partners, Khosla Ventures, First Round Capital and Data Collective. You can also view the news release on our website .", "date": "2014-12-18"},
{"website": "Single-Store", "title": "full-speed-ahead", "author": ["Gary Orenstein"], "link": "https://www.singlestore.com/blog/full-speed-ahead/", "abstract": "Nearly ten years ago I received a phone call about a startup in Silicon Valley solving application performance problems with memory. From my data center infrastructure experience, I knew the days of mechanical disk drives were limited. I had to get on the memory train, so I went. That experience led me to meet the co-founders of Fusion-io and ultimately join them in 2010. When Fusion-io went public in 2011 revenue was on its way from $36 million to $197 million annually. The time was right for flash memory and Fusion-io had the products to deliver. Companies like Facebook and others jumped at the opportunity to supercharge their databases and infrastructure, going so far as to deploy all solid-state data centers to meet the needs of a globally connected population interacting with data and images around the clock. During the next several years I watched customers deploy solutions for Oracle, SAP HANA, Microsoft SQL Server, and MySQL to achieve great results. Ultimately however, these solutions remained available to only a portion of the population. They were driven by adding remarkable hardware to good software. What if we could change the equation so customers could work with remarkable software and  hardware that didn’t break the bank? My excitement for SingleStore comes from this very premise. Far beyond making the databases of yesteryear look good, SingleStore has rethought the database itself. With a ground-up focus on memory and DRAM, distributed systems, and the ability to deploy anywhere from bare metal to a cloud container or VM, SingleStore has designed a product for today’s interconnected and interactive world. It scales out, handles the most torturous workloads without breaking a sweat, delivers analytics in the midst of massive data capture, and preserves the SQL goodness that has served as the enterprise analytics lingua franca for decades. The SingleStore team has made phenomenal progress in the last few years, delivering a solid product with incredible market opportunity. A real-time world awaits as we experience the growth of data, applications, and touch points in our daily lives. The SingleStore path is flanked by customers generating new revenue, driving down solution costs, and innovating with data-driven solutions in ways that had not been possible before. I’m thrilled to be a part of it!", "date": "2014-12-18"},
{"website": "Single-Store", "title": "memsql-loader", "author": ["Wayne Song"], "link": "https://www.singlestore.com/blog/memsql-loader/", "abstract": "One of the most common tasks with any database is loading large amounts of data into it from an external data store. Both SingleStore and MySQL provide the LOAD DATA command for this task; this command is very powerful, but by itself, it has a number of restrictions: It can only read from the local filesystem, so loading data from a remote store like Amazon S3 requires first downloading the files you need. Since it can only read from a single file at a time, loading from multiple files requires multiple LOAD DATA commands. If you want to perform this work in parallel, you have to write your own scripts. If you are loading multiple files, it’s up to you to make sure that you’ve deduplicated the files and their contents. Why We Built the SingleStore Loader At SingleStore, we’ve acutely felt all of these limitations. That’s why we developed SingleStore Loader, which solves all of the above problems and more. SingleStore Loader lets you load files from Amazon S3, the Hadoop Distributed File System (HDFS), and the local filesystem. You can specify all of the files you want to load with one command, and SingleStore Loader will take care of deduplicating files, parallelizing the workload, retrying files if they fail to load, and more. Use a load command to load a set of files View the progress of a job using the ps command We have been using SingleStore Loader here at SingleStore for quite a while now, and have provided a binary version on our website for anyone to use. However, we are proud of the code we produced (or at least proud enough), and have decided to open source the SingleStore Loader project. Give SingleStore Loader a Try – Download Now on GitHub The project uses several open source libraries, such as the Voluptuous data validation library and our own SingleStore Python connector. You can find the project here . Check it out, and let us know what you think!", "date": "2014-12-30"},
{"website": "Single-Store", "title": "iot-a-sensor-in-it-a-database-behind-it", "author": ["Gary Orenstein"], "link": "https://www.singlestore.com/blog/iot-a-sensor-in-it-a-database-behind-it/", "abstract": "On opening day, Molly Wood at The New York Times recounted the official CES theme emerging as “Put a sensor in it.” The unofficial theme seemed to be: Put a sensor in it . Later that day Samsung declared its entire product portfolio will be connected within 5 years. “In five years,” every single one of Samsung’s products will be a connected “Internet of Things” device, Samsung chief executive Boo-Keun Yoon said today during the opening keynote at the 2015 Consumer Electronics Show in Las Vegas. – Venturebeat The Consumer Electronics Show reigns as the tech industry’s annual device bonanza, and a large part of this year’s euphoria relates to connected devices that form the Internet of Things. The theme and message remain clear. Sensors, and interconnected devices will stampede ahead. Less frequently discussed is what happens behind those devices, and in particular, the expectations users have about their daily device interactions and demand for data. Fulfilling Data for The Internet of Things Both companies and users have a lot at stake. Device and application providers aim to serve users with a rich engaging set of functionality. They also seek to instrument service delivery to monitor and react to different situations. Users invest time and money in connected devices and applications to fill needs and desires. But along with that comes a set of expectations. Examples include: Fitness device users want their information up to the last step Video watchers expect streams to be delivered efficiently Drivers expect hassle free connected services Ultimately, all of the edge data drives demand and application requirements at the other end of the network, in the data center. Specifically, that infrastructure must include a database that can: Support massive data ingest across millions of devices and connections Database systems must keep up with the incoming flood of data to ensure no loss, and that every user or device has a complete picture of its history. Serve as the system of record while simultaneously providing real-time analytics In a real-time world, there is no luxury or pain of transferring data between systems, commonly referred to as Extract, Transform and Load (ETL). Systems of record for the Internet of Things need to mix transactions and analytics seamlessly and simultaneously. Respond to and integrate well with familiar ecosystems With sensor data touching everything from business intelligence to ad networks, connecting to multiple systems must be painless and simple. Allow for online scaling and online operations The world stops for no one, and successful services will be judged by their ability to grow and provide enterprise level service quality. It will be fun to see the possibilities of devices, drones, automated equipment and the emerging services they will power as part of the Internet of Things. As that happens, mountains of data will need to be captured and applied quickly to provide the richest user experience. We built SingleStore to give organizations behind the Internet of Things a head start. Every day, we work with development and operations teams to make sure they can ingest large amounts of sensor data with ease, make sound decisions in real-time, and manage complex, real-world data models with the familiarity of SQL. If that sounds like something that might help, give us a ring , or download SingleStore and try it for 30 days .", "date": "2015-01-06"},
{"website": "Single-Store", "title": "gartner-in-memory-guide", "author": ["Kevin White"], "link": "https://www.singlestore.com/blog/gartner-in-memory-guide/", "abstract": "From the inception of SingleStore, we’ve seen the ability to merge transactions and analytics into a single database system as a core function for any data centric organization. Gartner’s recent report, “Market Guide for In-Memory DBMS” mirrors that belief, and is chock full of key findings and recommendations for businesses looking to take advantage of in-memory computing. Skip this article and download a complimentary copy of Gartner’s Market Guide for In-Memory DBMS In the report, Gartner found that “rapid technological advances in in-memory computing (IMC) have led to the emergence of hybrid transactional/analytical processing (HTAP) architectures that allow concurrent analytical and transactional processing on the same IMDBMS or data store.” HTAP Solves for Real-Time Data Processing HTAP promises to open a green field of opportunities for businesses that are not possible with legacy database management systems. Gartner highlights that with HTAP, “large volumes of complex business data can be analyzed in real time using intuitive data exploration and analysis without the latency of offloading the data to a data mart or data warehouse. This will allow business users to make more informed operational and tactical decisions.” HTAP Use Cases We are in the early days of HTAP, and it is not always clear how it can be applied in the real world. As a rule of thumb, any organization that handles large volumes of data will benefit from HTAP. To provide a bit more context, we’ve compiled the following applications of HTAP in use today. Application Monitoring When millions of users reach mobile or web-based applications simultaneously, it’s critical that systems run without any hiccups. HTAP allows teams of system administrators and analysts to monitor the health of applications in real-time to spot anomalies and save on costs incurred from poor performance. Internet of Things Applications built for the internet of things (IoT) run on huge amounts of sensor data. HTAP easily processes IoT scale data workloads, as it is designed to handle extreme data ingestion while concurrently making analytics available in real-time. Real-Time Bidding Ad Tech companies struggle to implement complex real-time bidding features due of the sheer volume of data processing required. HTAP delivers the processing power that’s necessary to serve display, social, mobile and video advertising at scale. Market Conditions Financial organizations must be able to respond to market volatility in an instant. Any delay is money out of their pocket. HTAP makes it possible for financial institutions to respond to fluctuating market conditions as they happen. In each of these use cases, the ability to react to large data sets in a short amount of time provides incredible value and, with HTAP, is entirely possible. Finding the Right In-Memory DBMS Before diving into a proof of concept, we highly suggest reading Gartner’s “Market Guide for In-Memory DBMS.” By giving it a quick read, you’ll come away with a better understanding of the in-memory computing landscape, new business opportunities, applicable use cases for your organization, and an action plan for getting started. For a limited time, we’re offering a complimentary download of the report. Download it now to learn: Why In-memory computing is growing in popularity and adoption How IMDBMSs are categorized and the three major use cases they support New business opportunities emerging from hybrid transactional and analytical processing (HTAP) How to jump ahead of the competition with recommendations for effective use of IMDBMS Get a better understanding of the in-memory computing landscape. Download the Gartner Market Guide here. – Click to Tweet Required Disclaimer: Gartner does not endorse any vendor, product or service depicted in its research publications, and does not advise technology users to select only those vendors with the highest ratings or other designation. Gartner research publications consist of the opinions of Gartner’s research organization and should not be construed as statements of fact. Gartner disclaims all warranties, expressed or implied, with respect to this research, including any warranties of merchantability or fitness for a particular purpose.", "date": "2015-01-12"},
{"website": "Single-Store", "title": "welcome-jerry-held-as-memsql-executive-chairman", "author": ["Eric Frenkiel"], "link": "https://www.singlestore.com/blog/welcome-jerry-held-as-memsql-executive-chairman/", "abstract": "At SingleStore, we have always respected the challenges the computing industry has tackled in the past. It is no easy feat to build formidable technology companies, and when it happens, we look on with admiration. It is in that spirit that we are thrilled to welcome Jerry Held as Executive Chairman of SingleStore. Jerry’s experience in technology across computing and database technology is unparalleled. A consummate technology innovator and entrepreneur, he has helped create new database technologies and new companies, beginning with pioneering work to build the INGRES database during his time at U.C. Berkeley, and then Tandem, where he built it from a startup to reaching over $2 billion in annual revenues. He ran Oracle’s server products division during a growth period from $1.5 billion to $6 billion in annual revenues. He served as “CEO-in-residence” at Kleiner Perkins Caufield & Byers. He served as lead director of Business Objects until its sale to SAP, and was executive chairman of Vertica until its sale to HP. Today he serves as the chairman of Tamr and on the boards of Informatica, NetApp, Kalio and Copia Global. SingleStore Welcomes Database Visionary Jerry Held as Executive Chairman – Click to Tweet In short, Jerry is an expert in high-growth technology companies and knows the opportunities they create. He has seen the full picture and enjoys sharing his expertise. SingleStore began with a simple idea to build a new database that would change the way people think about business: every company should be a real-time company, using SingleStore to make their businesses more data-driven, responsive, and scalable. To achieve our objectives, we have surrounded ourselves with the best and brightest in the industry, in turn delivering our customers easy access to database innovation. Jerry epitomizes the talent and experience we continue to seek at SingleStore. As we continue the expansion of our business and world-class product, we look forward to Jerry’s participation in reshaping the real-time database landscape. Welcome, Jerry!", "date": "2015-01-14"},
{"website": "Single-Store", "title": "the-rise-of-the-cloud-memory-price-drop", "author": ["Gary Orenstein"], "link": "https://www.singlestore.com/blog/the-rise-of-the-cloud-memory-price-drop/", "abstract": "Last week, Data Center Knowledge published a piece on Microsoft’s ‘Monster’ Azure instances, with RAM capacities approaching half a terabyte at 448 GB. Microsoft Azure has launched its most powerful cloud instances to date. The new G-series instances go up to 32 cores, 448 GiB of RAM, and 6,596 GB of local SSD storage. Microsoft Azure Launches Monster Cloud Instances , Data Center Knowledge, 8 January 2015 The article continues to detail The highest-memory instance available on Google Compute Engine is 104 GB and The Azure announcement comes before the expected roll-out of new high-octane cloud instances by AWS. …the upcoming C4 instances, which will go up to…60 GB of RAM. However, the R3 instances from Amazon, optimized for memory, reach capacities of up to 244 GB. This week, Business Insider published a post outlining “ The Vicious Price War Going On In Cloud Computing ,” that details in finer granularity the precipitous drop of average monthly cost per GB of RAM. The chart comes from RBC Capital’s Mark Mahaney. All of this bodes well for customers who are tackling the most pressing data workloads that require more and more memory. While the increase availability of flash and solid state media have helped alleviate workload pressure, memory in the form of DRAM, with its drastically faster performance capabilities, continues to dominate the discussion. Of course, if you wanted to go full bore, you could pick up a SuperServer Solution from Supermicro and put a whopping 6TB of DDR3 memory inside. And if you’d like to see the power of putting all of that memory to use, feel free to try SingleStore for free . Or just give us a ring .", "date": "2015-01-16"},
{"website": "Single-Store", "title": "dbms-disadvantages", "author": ["Kevin White"], "link": "https://www.singlestore.com/blog/dbms-disadvantages/", "abstract": "Our data is changing faster than our data centers, making it harder and harder to keep up with the influx of incoming information, let alone make use of it. IT teams still tolerate overnight batch processing. The cost of scaling legacy solutions remains cost prohibitive. And many promised solutions force a complete departure from the past. If this sounds familiar, you are not alone. Far too many innovative companies struggle to build applications for the future on infrastructure of the past. It’s time for a new approach. In their report, “ Hybrid Transaction/Analytical Processing Will Foster Opportunities for Dramatic Business Innovation ,” Gartner identifies four major drawbacks of traditional database management systems, and how a new approach of hybrid transactional and analytical processing can solve these issues. A Brief Overview of HTAP Hybrid transactional/analytical processing (HTAP) merges two formerly distinct categories of data management: operational databases that processed transactions, and data warehouses that processed analytics. Combining these functions into a single system inherently eliminates many challenges faced by database administrators today. How HTAP Remedies the Four Drawbacks of Traditional Systems ETL In HTAP, data doesn’t need to move from operational databases to separated data warehouses/data marts to support analytics. Rather, data is processed in a single system of record, effectively eliminating the need to extract, transform, and load (ETL) data. This benefit provides much welcomed relief to data analysts and administrators, as ETL often takes hours (sometimes days) to complete. Analytic Latency In HTAP, transactional data of applications is readily available for analytics when created. As a result, HTAP provides an accurate representation of data as it’s being created, allowing businesses to power applications and monitor infrastructure in real-time. Synchronization In HTAP, drill-down from analytic aggregates always points to the “fresh” HTAP application data. Contrast that with a traditional architecture, where analytical and transactional data is stored in silos, and building a system to synchronize data stores quickly and accurately is cumbersome. On top of that, it’s likely that the “analytics copy” of data will be stale and provide a false representation of data. Copies of Data In HTAP, the need to create multiple copies of the same data is eliminated (or at least reduced). Compared to a traditional architectures, where copies of data must managed and monitored for consistency, HTAP reduces inaccuracies and timing differences associated with the duplication of data. The result is a simplified system architecture that mitigates the complexity of managing data and hardware costs. Why HTAP and Why Now? One of the reasons we segmented workloads in the past was to optimize for specific hardware, especially disk drives. In order to meet performance needs, systems designed for transactions were best optimized one way, and systems designed for queries another. Merging systems on top of the same set of disk drives would have been impossible from a performance perspective. With the advent of low cost, memory-rich servers , in your data center or in the cloud, new in-memory databases can transcend prior restrictions and foster simplified deployments for existing use cases while simultaneously opening doors to new data centric applications. Want to learn more about in-memory databases and opportunities with HTAP? – Take a look at the recent Gartner report here . If you’re interested in test driving an in-memory database that offers the full benefits of HTAP, give SingleStore a try for 30 days , or give us a ring .", "date": "2015-01-23"},
{"website": "Single-Store", "title": "shutterstock-case-study", "author": ["Gary Orenstein"], "link": "https://www.singlestore.com/blog/shutterstock-case-study/", "abstract": "In our connected world, some businesses adapted to the Internet, and some businesses blossomed with the Internet. Shutterstock embodies defining characteristics of the latter. For more than ten years, Shutterstock has grown into one of the largest and most vibrant two-sided marketplace for creative professionals to license and contribute content, including images, videos and music. As an Internet-based company, IT infrastructure is a critical component to Shutterstock’s success, with customers searching from a collection of over 47 million royalty-free photos, vector graphics and illustrations, as well as 2 million video and music clips for license. With this massive system in place, the ability to have an in-depth understanding of website performance and user engagement in real-time promised Shutterstock a transformative benefit. Solving for Hyperscale Shutterstock did not have any systems that supported real-time data analysis at this large volume. Instead, an alert would fire when a tracked metric changed significantly, however, such abnormalities were not able to be viewed in real-time. It became clear that a distributed system with high availability was absolutely necessary to making the Shutterstock website more resilient and faster, while providing clear insights to the engineering team. Looking for a real-time remedy, Shutterstock sought to find a database engine that could simultaneously collect and query tens of thousands of data points per second. After a thorough review of various solutions, Shutterstock deployed SingleStore as the preferred database management system that met its stringent high availability and analytics requirements. Adding to the value, analysts on the Shutterstock team were familiar with SQL, so SingleStore was a drop-in, easy-to-use solution. “SingleStore is the platform to handle our scale and provide real-time insights, allowing us to effectively manage our infrastructure and detect anomalies.” “At Shutterstock, we have thousands of nodes storing millions of metrics every minute. We chose SingleStore to be the platform on which we perform crucial systems analytics because this data is about real-time site performance, and a delay even as short as five minutes isn’t acceptable,” said Chris Fisher, VP Technology Operations, Shutterstock. “SingleStore is the platform to handle our scale and provide real-time insights, allowing us to effectively manage our infrastructure and detect anomalies.” Pushing the Boundaries of Real-Time For businesses like Shutterstock, a comprehensive understanding of their customer base can provide a huge marketplace advantage that allows them to understand and instantly react to user preferences. Working with SingleStore, Shutterstock achieves this in real-time. “The combination of rapid data capture and analytics is one of the true values of Big Data, and Shutterstock is a perfect example of a company pushing the boundaries of what that can do,” said Eric Frenkiel, CEO at SingleStore. “As Big Data deployments become increasingly widespread, companies like Shutterstock who are able to gain insight from their data will help their customers with better solutions.” “The combination of rapid data capture and analytics is one of the true values of Big Data, and Shutterstock is a perfect example of a company pushing the boundaries of what that can do.” More and more businesses are adopting distributed, in-memory systems to get answers to questions in real-time . In addition to helping them maintain a healthy IT infrastructure, these companies are able to accomplish incredible feats such as adapt to market conditions, deliver contextual user experiences, and power applications built for the Internet of Things . To hear from the details directly from Shutterstock, watch the short case study video here: 2014-07-XX-Shutterstock-Case-Study-Content-All Download the full Shutterstock Case Study here – Download Now", "date": "2015-02-05"},
{"website": "Single-Store", "title": "closing-the-batch-gap", "author": ["Gary Orenstein"], "link": "https://www.singlestore.com/blog/closing-the-batch-gap/", "abstract": "Sometimes we become so accustomed to certain things, we stop asking questions and accept the status quo. That can last a long time. Such is the case with batch processing in the enterprise. And it was made abundantly clear in a survey titled “ Enterprise Big Data, Business Intelligence, and Analytics Trends ” by the Enterprise Strategy Group, that there remains an ongoing complacency with the time it takes to complete batch processing. This is in stark contrast to the recognition that real-time processing still mandates measurement in microseconds. The survey involved 375 IT and business professionals familiar with their organization’s current database, business intelligence, and analytics solutions. Respondents are primarily large mid-market organizations (500 to 999 employees) and enterprise organizations (1,000 employees or more) in North America. Industry verticals including financial, business services, manufacturing and retail. A Dichotomy of Opinions When surveying users about real-time operations, 45% of respondents indicated data is updated in seconds to support real-time databases, with 29% citing milliseconds, and 12% citing microseconds. That is 86% of users working with second or sub-second updates . Figure 1: Frequency of Updating a Data Set for Teal-Time Analytics. Source ESG However, the picture on the batch processing side is at the opposite end of the spectrum, with most respondents indicating they update daily. Figure 2: Frequency of Updating a Data Set for Batch Analytics Source ESG Why Such Discrepancy? For too long, the database world has been split into silos, where individual datastores each have a portion of data. In order to get systems to work together, companies rely on time consuming ETL (extract, transform, and load) to move data into place for batch processing. Ultimately, that leads to lengthy update cycles. Real-time Transactions and Analytics in a Single Database Today, customers have a choice. And one of those is to close the batch gap by adopting databases that can handle real-time transactions and analytics concurrently . In doing so, customers can eliminate many of the delays with ETL to achieve complete visibility to their data, even as it continues to change. As industry expert Curt Monash recently noted, Internet interaction applications increasingly require data freshness to the last click… Curt Monash “Up to the last click,” is now possible with modern approaches that merge transactions and analytics, eliminating the delays of ETL and help close the batch gap. These are core tenets of SingleStore and why we built our real-time database for transactions and analytics. To learn more visit www.singlestore.com give it a try for free, visit www.singlestore.com/free .", "date": "2015-02-06"},
{"website": "Single-Store", "title": "memsql-spark-connector", "author": ["Wayne Song"], "link": "https://www.singlestore.com/blog/memsql-spark-connector/", "abstract": "Apache Spark is one of the most powerful distributed computing frameworks available today. Its combination of fast, in-memory computing with an architecture that’s easy to understand has made it popular for users working with huge amounts of data. While Spark shines at operating on large datasets, it still requires a solution for data persistence. HDFS is a common choice, but while it integrates well with Spark, its disk-based nature can impact performance in real-time applications (e.g. applications built with the Spark Streaming libraries). Also, Spark does not have a native capability to commit transactions. Making Spark Even Better That’s why SingleStore is releasing the SingleStore Spark connector , which gives users the ability to read and write data between SingleStore and Spark. SingleStore is a natural fit for Spark because it can easily handle the high rate of inserts and reads that Spark often requires, while also having enough space for all of the data that Spark can create. Operationalize and streamline Spark deployments with the SingleStore Spark Connector – Click to Tweet The SingleStore Spark Connector provides everything you need to start using Spark and SingleStore together. It comes with a number of optimizations, such as reading data out of SingleStore in parallel and making sure that Spark colocates the data in its cluster with SingleStore nodes when SingleStore and Spark are running on the same physical machines. It also provides two main components: a SingleStoreRDD class for loading data from a SingleStore query and a saveToSingleStore function for persisting results to a SingleStore table. We’ve made our connector open source; you can find the project here . Check it out and let us know how it works. Get The SingleStore Spark Connector Guide The 79 page guide covers how to design, build, and deploy Spark applications using the SingleStore Spark Connector. Inside, you will find code samples to help you get started and performance recommendations for your production-ready Apache Spark and SingleStore implementations. Download Here", "date": "2015-02-10"},
{"website": "Single-Store", "title": "operationalizing-spark-with-memsql", "author": ["Gary Orenstein"], "link": "https://www.singlestore.com/blog/operationalizing-spark-with-memsql/", "abstract": "Combining the data processing prowess of Spark with a real-time database for transactions and analytics, where both are memory-optimized and distributed, leads to powerful new business use cases. SingleStore Spark Connector links at end of this post. Data Appetite and Evolution Our generation of, and appetite for, data continues unabated. This drives a critical need for tools to quickly process and transform data. Apache Spark, the new memory-optimized data processing framework, fills this gap by combining performance, a concise programming interface, and easy Hadoop integration, all leading to its rapid popularity. However, Spark itself does not store data outside of processing operations. That explains that while a recent survey of over 2000 developers chose Spark to replace MapReduce, 62% still load data to Spark with the Hadoop Distributed File System and there is a forthcoming Tachyon memory-centric distributed file system that can be used as storage for Spark. But what if we could tie Spark’s intuitive, concise, expressive programming capabilities closer to the databases that power our businesses? That opportunity lies in operationalizing Spark deployments, combining the rich advanced analytics of Spark with transactional systems-of-record. Introducing the SingleStore Spark Connector Meeting enterprise needs to deploy and make use of Spark, SingleStore introduced the SingleStore Spark Connector for high-throughput, bi-directional data transfer between a Spark cluster and a SingleStore cluster. Since Spark and SingleStore are both memory-optimized, distributed systems, the SingleStore Spark Connector benefits from cluster-wide parallelization for maximum performance and minimal transfer time. The SingleStore Spark Connector is available as open source on Github . SingleStore Spark Connector Architecture There are two main components of the SingleStore Spark Connector that allow Spark to query from and write to SingleStore. A SingleStoreRDD class for loading data from a SingleStore query A saveToSingleStore function for persisting results to a SingleStore table Figure 1: SingleStore Spark Connector Architecture This high performance connection between SingleStore and Spark enables several relevant use cases for today’s Big Data, high-velocity environments. Spark Use Cases: Operationalize models built in Spark Stream and event processing Extend SingleStore Analytics Live dashboards and automated reports Understanding that operationalizing Spark often involves another system, SingleStore—with its performance, scale, and enterprise fit—provides significant consolidation, incorporating several types of Spark deployments. Operationalize Models Built in Spark In this use case, data flows into Spark from a specified source, such as Apache Kafka, and models are created in Spark. The results set of those models can be immediately persisted in SingleStore as one or multiple tables, whereby an entire ecosystem of SQL-based business intelligence tools can consume the results. This rapid process allows data teams to go to production and iterate faster. Figure 2: Operationalize models built in Spark Stream and Event Processing In the same survey of Spark developers, 67% of users need Spark for event stream processing, and streaming can also benefit from a persistent database. A typical use might be to capture and structure event data on the fly, such as that from a high-traffic website. While the event stream may include a bevy of information about overall site and system health, as well as user behavior, it makes sense to structure and classify that data before passing to a database like SingleStore in a persistent queryable format. Processing the stream in Spark, and passing to SingleStore, enables developers to Use Spark to segment event types Send each event type to a separate SingleStore table Immediately query real-time data across one or multiple tables in SingleStore Figure 3: Stream and event processing Extend SingleStore Analytics Another use case brings extended functionality to SingleStore users who need capabilities beyond what can be offered natively with SQL or JSON. As SingleStore is typically the system-of-record for primary applications, it holds the freshest data for analysis. To extend SingleStore analytics, users can Set up a replicated cluster providing clear demarcation between operations and analytics teams Give Spark access to live production data for the most recent and relevant results Allow Spark to write results set back to the primary SingleStore cluster to put new analyses into production Figure 4: Extend SingleStore Analytics Live dashboards and automated reports Many companies run dashboards using SQL analytics, and the opportunity to do that in real-time with the most recent data provides a differentiating advantage. There are, of course, advanced reports that cannot be easily accomplished with SQL. In these cases, Spark can easy access live production on the primary operational datastore to deliver custom real-time reports using the most relevant data. Figure 5: Live Dashboards and automated reports For more information on the SingleStore Spark Connector please visit: Github Site for SingleStore Spark Connector SingleStore Technical Blog Post SingleStore free 30 day trial Get The SingleStore Spark Connector Guide The 79 page guide covers how to design, build, and deploy Spark applications using the SingleStore Spark Connector. Inside, you will find code samples to help you get started and performance recommendations for your production-ready Apache Spark and SingleStore implementations. Download Here", "date": "2015-02-11"},
{"website": "Single-Store", "title": "pinterest-apache-spark-use-case", "author": ["Neil Dahlke"], "link": "https://www.singlestore.com/blog/pinterest-apache-spark-use-case/", "abstract": "Setting the Stage for Spark With Spark on track to replace MapReduce , enterprises are flocking to the open source framework in effort to take advantage of its superior distributed data processing power. IT leads that manage infrastructure and data pipelines of high-traffic websites are running Spark–in particular, Spark Streaming which is ideal for structuring real-time data on the fly–to reliably capture and process event data, and write it in a format that can immediately be queried by analysts. As the world’s premiere visual bookmarking tool, Pinterest is one of the innovative organizations taking advantage of Spark. Pinterest found a natural fit in SingleStore’s in-memory database and Spark Streaming, and is using these tools to find patterns in high-value user engagement data. Pinterest’s Spark Streaming Setup Here’s how it works: Pinterest pushes event data, such as pins and repins, to Apache Kafka. Spark Streaming ingests event data from Apache Kafka, then filters by event type and enriches each event with full pin and geo-location data. Using the SingleStore Spark Connector , data is then written to SingleStore with each event type flowing into a separate table. SingleStore handles record deduplication (Kafka’s “at least once” semantics guarantee fault tolerance but not uniqueness). As data is streaming in, Pinterest is able to run queries in SingleStore to generate engagement metrics and report on various event data like pins, repins, comments and logins. Visualizing the Data We built a demo with Pinterest to showcase the locations of repins as they happen. When an image is repinned, circles on the globe expand, providing a visual representation of the concentration of repins by location. The demo also leverages Spark to enrich streaming data with geolocation information between the time that it arrives and when it hits the database. SingleStore adds to this enrichment process by serving as a key/value cache for data that has already been processed and can be reused for future repins. Additionally, as part of the enrichment process, any repin that enters the system is looked up against SingleStore, and is saved to SingleStore if the full pin is missing. All full pins that come in through the stream are saved automatically to avoid this lookup. Pinterest showcases real-time user engagement across the globe using @SingleStoreDB and @apachespark – Click to Tweet So, What’s the Point? This infrastructure gives Pinterest the ability to identify (and react to) developing trends as they happen. In turn, Pinterest and their partners can get a better understanding of user behavior and provide more value to the Pinner community. Because everything SQL based, access to data is more widespread. Engineers and analyst can work with familiar tools to run queries and track high-value user activity such as repins. It also should be noted that this Spark initiative is just the beginning for Pinterest. As the Spark framework evolves and the community continues to grow, Pinterest expects to expand use cases for SingleStore and Spark. Initial Results After integrating Spark Streaming and SingleStore, running on AWS, into their data stack, Pinterest now has a source of record for sharing relevant user engagement data and metrics their data analyst and with key brands. With SingleStore and Spark, Pinterest has found a method for repeatable, production-ready streaming and is able to go from pipe dump to graph in real time. Get The SingleStore Spark Connector Guide The 79 page guide covers how to design, build, and deploy Spark applications using the SingleStore Spark Connector. Inside, you will find code samples to help you get started and performance recommendations for your production-ready Apache Spark and SingleStore implementations. Download Here", "date": "2015-02-18"},
{"website": "Single-Store", "title": "memsql-at-strata-hadoop-world-booth-1015", "author": ["Gary Orenstein"], "link": "https://www.singlestore.com/blog/memsql-at-strata-hadoop-world-booth-1015/", "abstract": "The fun begins at Strata + Hadoop World this week in San Jose. Be sure to check out SingleStore at Booth 1015 for the latest product details, demonstrations, games, and prizes. Here is a quick rundown of our activity at the show. SingleStore Introduces Seamless Spark Connectivity for Enterprise Deployments Last week, SingleStore announced a high-performance parallel connector for SingleStore and Spark. You can read all the details here and see it live at the show. SingleStore and Pinterest Showcase Operationalizing Spark at Strata + Hadoop World 2015 We partnered with our friends at Pinterest to share the latest and greatest with Spark and SingleStore. Read all of the details here . Keynote: Close Encounters with the Third Kind of Database 9:10am-9:15am Thursday, February 19th, Grand Ballroom 220 Join us for this engaging presentation by our CEO and co-founder Eric Frenkiel. Tutorial Session: Bringing OLAP Fully Online: Analyze Changing Datasets in SingleStore and Spark with Pinterest Demo 10:40am-11:20am Thursday, February 19th, Room LL20D This session includes appearances from Robert Stepeck, CTO, Novus and Yu Yang, Software Engineering, Pinterest. Test Your Skills with Query Kong Win an Estes Proto X Drone after proving your low-latency skills with Query Kong, the breakout game sensation of Strata + Hadoop World! Visit the SingleStore Booth 1015 We have cool t-shirts for all visitors during the show expo hours: Wednesday, February 18, 5:00pm – 6:30pm Thursday, February 19, 10:00am – 4:30pm and 5:30pm – 7:00pm Friday, February 20, 10:00am- 4:00pm See you there! We look forward to sharing great technical insights and fun times at booth 1015! http://www.singlestore.com/events", "date": "2015-02-18"},
{"website": "Single-Store", "title": "data-stores-for-the-internet-of-things", "author": ["Gary Orenstein"], "link": "https://www.singlestore.com/blog/data-stores-for-the-internet-of-things/", "abstract": "Like the world wide web, the Internet of Things is personal. It represents a near complete connectedness, including the industrial world, and never ending possibilities of new applications and services. The Internet of Things also represents a need to examine conventional assumptions on databases and data stores to support real-time data pipelines. In an article on Silicon Angle, Designing data stores for the Internet of Things , SingleStore CEO and co-founder Erik Frenkiel shares his insight on the critical requirements to support new interconnected devices, interactive applications, and the analytics to understand their use. Principles of data store design for the Internet of Things Capture Everything Save Data While Serving Data Fit the Ecosystem Online All the Time Be sure to read the entire a article at Designing data stores for the Internet of Things .", "date": "2015-02-24"},
{"website": "Single-Store", "title": "distributed-in-memory-sql-database-memsql-at-amp-lab", "author": ["Gary Orenstein"], "link": "https://www.singlestore.com/blog/distributed-in-memory-sql-database-memsql-at-amp-lab/", "abstract": "Please join us next week as two members of the SingleStore engineering team present at the AMPLab at Berkeley on Wednesday March 11th from 12:00pm to 1:00pm. AMP SEMINAR Ankur Goyal and Anders Papitto, SingleStore, A Distributed In-Memory SQL Database Wednesday 3/11, Noon, 405 Soda Hall, Berkeley Talk Abstract This talk will cover the major architectural design decisions with discussion on specific technical details as well as the motivation behind the big decisions. We will cover lockfree, code generation, durability/replication, distributed query execution, and clustering in SingleStore. We will then discuss some of the new directions for the product, including some ideas on leveraging Spark. Speakers Ankur Goyal is the Director of Engineering at SingleStore. At SingleStore he has focused on distributed query execution and clustering, but has touched most of the engine. His areas of interest are distributed systems, compilers, and operating systems. Ankur studied computer science at Carnegie Mellon University and worked on distributed data processing at Microsoft before SingleStore. Anders Papitto is an engineer at SingleStore, where he has worked on distributed query execution, column store storage and query execution, and various other components. He joined SingleStore shortly before completing his undergraduate studies at UC Berkeley. About the AMPLab AMP: ALGORITHMS MACHINES PEOPLE TURNING UP THE VOLUME ON BIG DATA Working at the intersection of three massive trends: powerful machine learning, cloud computing, and crowdsourcing, the AMPLab is integrating Algorithms, Machines, and People to make sense of Big Data. We are creating a new generation of analytics tools to answer deep questions over dirty and heterogeneous data by extending and fusing machine learning, warehouse-scale computing and human computation. We validate these ideas on real-world problems including participatory sensing, urban planning, and personalized medicine with our application and industrial partners.", "date": "2015-03-06"},
{"website": "Single-Store", "title": "video-the-state-of-in-memory-and-apache-spark", "author": ["Kevin White"], "link": "https://www.singlestore.com/blog/video-the-state-of-in-memory-and-apache-spark/", "abstract": "Strata+Hadoop World was full of activity for SingleStore. Our keynote explained why real-time is the next phase for big data . We showcased a live application with Pinterest where they combine Spark and SingleStore to ingest and analyze real-time data. And we gave away dozens of prizes to Strata+Hadoop attendees who proved their latency crushing skills in our Query Kong game. During the event, Mike Hendrickson of O’Reilly Media sat down with SingleStore CEO Eric Frenkiel to discuss: The state of in-memory computing and where it will be in a year What Spark brings to in-memory computing Industries and use cases that are best suited for Spark Get The SingleStore Spark Connector Guide The 79 page guide covers how to design, build, and deploy Spark applications using the SingleStore Spark Connector. Inside, you will find code samples to help you get started and performance recommendations for your production-ready Apache Spark and SingleStore implementations. Download Here Watch the video in full here: Get started with SingleStore in Minutes – Start your 30-day free trial .", "date": "2015-02-25"},
{"website": "Single-Store", "title": "geospatial-intelligence", "author": ["Carlos Bueno"], "link": "https://www.singlestore.com/blog/geospatial-intelligence/", "abstract": "This week at the Esri Developers Summit in Palm Springs, our friends at Esri are previewing upcoming features for the next release of SingleStore, using a huge real-world geospatial dataset. Esri develops geographic information systems (GIS) that function as an integral component in nearly every type of organization. In a recent report by the ARC Advisory Group, the Geographic Information System Global Market Research Study, the authors stated, “Esri is, without a doubt, the dominant player in the GIS market.” SingleStore showcases Geospatial features at Esri Developers Summit – Click to Tweet Everything happens somewhere. But, traditionally, spatial data has been locked away in specialized software that either lacked general database features, or didn’t scale out. With SingleStore we are making geospatial data a first-class citizen: just as easy to use, at scale, at great speed and high throughput, as any other kind of data. The demonstration uses the “Taxistats” dataset: a compilation of 170 million real-world NYC taxi rides . It includes GPS coordinates of the pickup and dropoff, distance, and travel time. SingleStore is coupled with the new version of Esri’s ArcGIS Server, which has a new feature to translate ArcGIS queries into external database queries. From there we generate heatmaps from the raw data in sub-second time. Heatmaps are a great way to visualize aggregate geospatial data. The X and Y are the longitude and latitude of “cells” or “pixels” on the map, and the color shows the intensity of the values. From there you can explore the dataset across any number of dimensions: zoom in on an area, filter by time, length of ride, and more. How it Works To prepare the data, we simply loaded it into a SingleStore table running on a modest Amazon AWS cluster. At first we thought about pre-processing the geospatial points to calculate hexgrid values for faster querying, but that turned out to be unnecessary. SingleStore is fast enough to aggregate the point data into heatmap “pixels” on the fly. Alternatively, we could have generated a table of hexagons and performed a spatial join between the point table and the pixel table. The queries are almost absurdly simple. We filter the table to an area on the map using GEOGRAPHY_INTERSECTS, filter further by time and date, round off the GPS coordinates to gather them into rough “pixels” about 100 meters wide, and group them. Spatial data is just another type of data in SingleStore, complete with lock-free indexes and powerful functions for manipulating it. The demo zips through nearly 200 million complex records in under 150 milliseconds: select\n  count(1) as cnt,\n  round(GEOGRAPHY_LONGITUDE(pickup), 3) as lon,\n  round(GEOGRAPHY_LATITUDE(pickup), 3) as lat\nfrom taxistats\nwhere\n  day_of_week = ‘Monday’\n  and hour_of_day = 2\n  and month_of_year = ‘January’\n  and GEOGRAPHY_INTERSECTS(pickup, 'POLYGON((-74.96857503 40.79939298,...))')\ngroup by 2, 3\nhaving count(1) > 10\nlimit 10;\n\n+-----+---------+--------+\n| cnt | lat     | lon    |\n+-----+---------+--------+\n|  19 | -73.997 | 40.723 |\n|  19 | -74.003 | 40.730 |\n|  16 | -74.000 | 40.732 |\n|  21 | -73.998 | 40.735 |\n|  13 | -74.007 | 40.735 |\n|  51 | -73.998 | 40.729 |\n|  18 | -74.000 | 40.754 |\n|  20 | -74.004 | 40.740 |\n|  17 | -73.981 | 40.765 |\n|  21 | -74.000 | 40.761 |\n+-----+---------+--------+\n10 rows in set (0.14 sec) Real-world Use Cases for Geospatial Slicing by hour of the day, we can calculate the average speed of a taxi ride (distance / time) and find the best and worst places for traffic jams. Slicing by day of week, we can see the ebb and flow of traffic during workdays and weekends. For a city planner, this kind of data can be used to redirect traffic at specific times in effort to unclog traffic congestion. For the taxi business, this data can improve efficiency with supply and demand of cabs during times of high or low traffic for any given region. Welcoming Geospatial to the World of Relational In-Memory Databases For too long, geospatial data has been relegated to specific databases and datastores. With the emergence of mobile phones and the coming wave of wearables and sensor data related to the Internet of Things, everything is now logged in both time and space. Now enterprises have an opportunity to combine geospatial data with the rest of their organizational data in a single database that is in-memory, linearly scalable, and supports a full range of relational SQL and geospatial functions. We look forward to a whole world of applications and opportunities.", "date": "2015-03-12"},
{"website": "Single-Store", "title": "memsql-at-spark-summit-east", "author": ["Gary Orenstein"], "link": "https://www.singlestore.com/blog/memsql-at-spark-summit-east/", "abstract": "We are happy to be in New York City this week for Spark Summit East . We will be sharing more about our new geospatial capabilities, as well as the work with Esri to showcase the power of SingleStore geospatial features in conjunction with Apache Spark. Last week we shared the preliminary release of SingleStore geospatial features introduced at the Esri Developer Summit in Palm Springs. You can read more about the live demonstration showcased at the summit here . The demonstration uses the “Taxistats” dataset: a compilation of 170 million real-world NYC taxi rides. It includes GPS coordinates of the pickup and dropoff, distance, and travel time. SingleStore is coupled with the new version of Esri’s ArcGIS Server, which has a new feature to translate ArcGIS queries into external database queries. From there we generate heatmaps from the raw data in sub-second time. This week we launched the official news release of SingleStore geospatial capabilities. By integrating geospatial functions, SingleStore enables enterprises to achieve greater database efficiency with a single database that is in-memory, linearly scalable and supports the full rage of relational SQL and geospatial functions. With SingleStore, geospatial data no longer remains separate and becomes just another data type with lock-free capabilities and powerful manipulation functions. Meanwhile at Spark Summit East , you can catch the Esri-Spark-SingleStore combination in action at the talk by Mansour Raad from Esri. Geospatial and Temporal Analysis and Visualization Esri-Spark-SingleStore talk and demonstration Mansour Raad (Esri) 4:30pm – 05:00pm, Track B, Applications You can read more details in the media alert : Esri and SingleStore Use Open Transportation Data to Showcase the Power of Real-Time Geospatial Intelligence at Spark Summit East. Be sure to catch SingleStore at Spark Summit East Wednesday March 18th – Thursday March 19th Booth 13 Sheraton New York Times Square Hotel We will be giving away t-shirts and Estes flying drones! If you would like to schedule a meeting or demonstration with SingleStore while at the show you can do so here . Hope to see you at Spark Summit East! Get The SingleStore Spark Connector Guide The 79 page guide covers how to design, build, and deploy Spark applications using the SingleStore Spark Connector. Inside, you will find code samples to help you get started and performance recommendations for your production-ready Apache Spark and SingleStore implementations. Download Here", "date": "2015-03-17"},
{"website": "Single-Store", "title": "importance-of-high-speed-database-counters", "author": ["Nikita Shamgunov"], "link": "https://www.singlestore.com/blog/importance-of-high-speed-database-counters/", "abstract": "Scaling tends to make even simple things, like counting, seem difficult. In the past, businesses used specialized databases for particular tasks, including high-speed, high-throughput event counters. Due to the constraints of legacy systems, some people still assume that relational databases cannot  handle high-throughput tasks at scale. However, due to advances like in-memory storage, high-throughput counting no longer requires a specialized, single-purpose database. Why do we even need counters? Before we get into the implementation, you might be asking why we need counters at all. Why not just collect event logs and compute counts as needed? In short, querying a counter is much faster than counting log records, and many applications require instant access to this kind of data. Counting logs requires a large table scan and aggregation to produce a count. If you have an updatable counter, it is a single record lookup. The challenge with high-throughput counters is that building a stateful, fault tolerant distributed system can be challenging. Fortunately, SingleStore solves those hard problems for you, so you can focus on building your application. In the rest of this article we’ll design a simple robust counter database running on a modest SingleStore cluster, and benchmark how it performs. Counters are records Let’s start by creating the following schema: create database test;\n\nuse test;\n\ncreate table counters_60 (\n    time_bucket int unsigned not null,\n    event_type int unsigned not null,\n    counter int unsigned not null,\n    primary key (time_bucket, event_type)\n);\n\ncreate table event_types (\n    event_type int unsigned not null primary key,\n    event_name varchar(128),\n    owner varchar(64),\n    status enum ('active', 'inactive')\n); The column time_bucket is the timestamp on the event rounded to the nearest minute. Making the time_bucket and event_type the primary key allows us to easily index events by time and type. insert into counters_60 select unix_timestamp() / 60, 1234, 1\non duplicate key update counter = counter + 1; If a primary key value does not exist, this query will insert a new record into SingleStore. If the primary key value exists, the counter will be incremented. This is informally called an “upsert.” The management of event_types is outside the scope of this article, but it’s trivial (and fast) to join the counter table to a table containing event metadata such as its human-friendly name. Let’s also insert some data into the event_types table: insert into event_types values (1234, 'party', 'memsql', 'active'); Querying Counters Now you have the counts of each event type bucketed by minute. This counter data can easily be aggregated and summarized with simple SQL queries: -- all-time historical counts of various event types\nselect e.event_type, e.event_name, sum(c.counter)\nfrom counters_60 c, event_types e\nwhere c.event_type=e.event_type\n    and e.event_type in (1234, 4567, 7890)\ngroup by 1, 2;\n\n-- total number of events in the last hour\nselect sum(counter), sum(counter)/60 as 'avg per min' from counters_60\nwhere event_type = 1234\n    and time_bucket >= unix_timestamp() / 60 - 60;\n\n-- total number of events in time series, bucketed in 10-minute intervals\nselect floor((unix_timestamp()/60 - time_bucket)/10) as interval, sum(counter)\nfrom counters_60\nwhere event_type = 1234\n    and time_bucket >= unix_timestamp() / 60 - 60\ngroup by 1; 1.6 Million increments per second Inserting naively into the counters table, one record at a time, actually gets you pretty far. In our testing this resulted in a throughput of 200,000 increments per second. It’s nice to get impressive performance by default. Then we tried to see how much farther we could go. In this simulation we processed 1,000 different event types. We created a threaded python script to push as many increments a second as possible. We made three changes to the naive version: multi-insert batches, disabling cluster-wide transactions, and sorting the records in each batch to avoid deadlocking. insert into counters_60 values\n(23768675, 1234, 1),\n(23768675, 4567, 1),\n(23768675, 7890, 1),\n...\non duplicate key update counter = counter + 1; We used a 6 node AWS cluster with 2 aggregators and 4 leaves to simulate the workload. Each node was m3.2xlarge consisting of 8 cores and 15GB of RAM, with an hourly cost of $2.61 for the entire cluster. When starting this script on both aggregator nodes, we achieved a throughput of 1.6M upserts a second. Data Collection In this simulation we use a Python script to simulate the data ingest. In the real world, we see our customers use technologies like Storm, Kafka and Spark Streaming to collect events in a distributed system for higher throughput. For more information on SingleStore integration with stream processing engines, see this blog post on how Pinterest uses SingleStore and Spark streaming to track real-time event data. Want to build your own high throughput counter? Download SingleStore today!", "date": "2015-03-24"},
{"website": "Single-Store", "title": "memsql-at-gartner-bi", "author": ["Kevin White"], "link": "https://www.singlestore.com/blog/memsql-at-gartner-bi/", "abstract": "We are thrilled to be in Las Vegas this week for the Gartner Business Analytics and Intelligence Summit. We will be at booth #119 and we have a ton in store for the event, including games and giveaways, happy hour for attendees, and a featured session from SingleStore CEO, Eric Frenkiel. We will also be showcasing our new geospatial capabilities, and a demo of how Pinterest is using SingleStore and Spark for real-time analytics. Free Gartner Report: Market Guide for In-Memory DBMS See the latest developments and use cases for in-memory databases. Download the Report Here → From the report… “The growing number of high performance, response-time critical and low-latency use cases (such as real-time repricing, power grid rerouting, logistics optimization), which are fast becoming vital for better business insight, require faster database querying, concurrency of access and faster transactional and analytical processing. IMDBMSs provide a potential solution to all these challenging use cases, thereby accelerating its adoption.” Don’t Miss the SingleStore Featured Session From Spark to Ignition: Fueling Your Business on Real-Time Analytics SingleStore CEO and Founder, Eric Frenkiel, will discuss how moving from batch-oriented data silos to real-time pipelines means replacing batch processes with online datasets that can be modified and queried concurrently. This session will cover use cases and customer deployments of Hybrid Transaction/Analytic Processing (HTAP) using SingleStore and Spark. Session Details Speaker: Eric Frenkiel, SingleStore CEO and Founder Data and Time: 12:30pm–12:50pm Monday, 3/30/2015 Location: Theater A, Forum Ballroom Join SingleStore on Monday Night for Happy Hour We will be hosting a happy hour at Carmine’s in The Form Shops at Caesars on Monday night at 8:00PM. ALTER TABLE TINIs and heavy hors d’oeuvres will be served. Stop by and meet with SingleStore CEO, Eric Frenkiel and CMO, Gary Orenstein. More details here . Suggested Sessions We have handpicked a few sessions that you don’t want to miss. Do We Still Need a Data Warehouse? Speaker: Donald Feinberg VP Distinguished Analyst 30 March 2015 2:00 PM to 2:45 PM For more than a decade, the data warehouse has been the architectural foundation of most BI and analytic activity. However, various trends (in-memory, Hadoop, big data and the Internet of Things) have compelled many to ask whether the data warehouse is still needed. This session provides guidance on how to craft a more modern strategy for data warehousing. Will Hadoop Jump the Spark? Speaker: Merv Adrian Research VP 31 March 2015 2:00 PM to 2:45 PM The Hadoop stack continues its dramatic transformation. The emergence of Apache Spark, suitable for many parts of your analytic portfolio, will rewrite the rules, but its readiness and maturity are in question. The DBMS Dilemma: Choosing the Right DBMS For The Digital Business Speaker: Donald Feinberg VP Distinguished Analyst 31 March 2015 2:00 PM to 2:45 PM As your organization moves into the digital business era, the DBMS needs to support not only new information types but also the new transactions and analytics required for the future. The DBMS as we know it is changing. This session will explore the new information types, new transaction types and the technology necessary to support this. Games and Giveaways Be sure to stop by Booth #119 at the event. We will be giving away t-shirts and Estes flying drones! If you would like to schedule a meeting or demonstration with SingleStore while at the show you can do so here . Hope to see you at Gartner Business Intelligence and Analytics Summit!", "date": "2015-03-27"},
{"website": "Single-Store", "title": "real-time-geospatial-intelligence-with-supercar", "author": ["Gary Orenstein"], "link": "https://www.singlestore.com/blog/real-time-geospatial-intelligence-with-supercar/", "abstract": "Today, SingleStore is showcasing a brand new demonstration of real-time geospatial location intelligence at the Gartner Business Intelligence and Analytics Summit in Las Vegas. The demonstration, titled Supercar, makes use of a dataset containing the details of 170 million real world taxi rides. By sampling this dataset and creating real-time records while simultaneously querying the data, Supercar simulates the ability to monitor and derive insights across hundreds of thousands of objects on the go. You can read further details in the full news release here . By natively integrating geospatial datatypes in its relational database, SingleStore enables simple queries to derive informative results. The queries available in Supercar include How many riders did we serve? What was the average rider wait time? What was the average trip distance? What was the average trip time? What was the average price/fare? Simple queries with native geospatial intelligence The demonstration uses the developer-focused mapping platform from Mapbox and combines simple SQL queries generated on the fly. For example, users can pan across the map and zoom in to specific sections which creates an area in which they can then run the query. One query example for passenger count is shown below. The coordinates of the polygon were removed for simplicity sake, but in practice represent the latitude and longitude of the four corners of the visible map area. SELECT SUM(passenger_count) as result\nFROM trips\nWHERE GEOGRAPHY_INTERSECTS(pickup_location, \"POLYGON((...))\")\nOR GEOGRAPHY_INTERSECTS(dropoff_location, \"POLYGON((...))\") To watch the demonstration in full, check out the following animated short For more information, visit the SingleStore Booth #119 at the Gartner Business Intelligence and Analytics Summit or visit us at SingleStore.com .", "date": "2015-03-30"},
{"website": "Single-Store", "title": "in-the-end-we-seek-structure", "author": ["Gary Orenstein"], "link": "https://www.singlestore.com/blog/in-the-end-we-seek-structure/", "abstract": "In Short: A range of assumptions led to a boom in NoSQL solutions, but in the end, SQL and relational models find their way back as a critical part of data management. In the End We Seek Structure. Why SQL and relational models are back as a critical part of data management – Click to Tweet Background By the mid 2000s, 10 years into the Netscape-inspired mainstream Internet, webscale workloads were pushing the limits of conventional databases. Traditional solutions could not keep up with a myriad of Internet users simultaneously accessing the same application and database. At the time, many websites used relational databases like MySQL, SQL Server from Microsoft, or Oracle. Each of these databases relied on a relational model using SQL, the Structured Query Language, which emerged nearly 40 years ago and remains the lingua franca of data management. Genesis of NoSQL Scaling solutions is hard, and in particular scaling a relational, SQL database proved particularly challenging, in part leading to the emergence of the NoSQL movement. FIGURE 1: Interest in NoSQL 2009 – 2015 Source: Google Trends While there are numerous reasons to explain this interest graph, a few include prior solutions being hard to scale hard to achieve new performance needs hard to build An Explosion of Database Options As developers sought alternatives, an explosion of database options emerged. Document Datastores Enabled scheme-less design which made a building new applications a breeze, but running concurrent reads and writes a significant challenge Key-value Stores Offered simple lookups and scale based on an eventual consistency model suitable to some, but not all, workloads Unstructured File Systems Delivered nearly infinite distributed storage making it easy to store everything, but nearly impossible to quickly make use of it Graph Databases Provided a superior data model for graph-specific datasets but not enough to cover a full spectrum of data management FIGURE 2: An Explosion of Database Options Some SQL With Your NoSQL As reality hit, many approaches edged back towards a relational and SQL focused model. Document datastore companies incorporated 3rd party storage engines to solve some of the most complex parts of operational, and relational, databases like multi-version concurrency control and record-level locking, as opposed to database locking. Key-value companies developed entirely new custom query languages that while kind of like SQL, are not. A query language per datastore became the norm. Unstructured file systems, holding troves of untapped data, quickly spurred an entire market of SQL on Hadoop solutions so customers could make use of everything they had been storing. And some smaller graph database companies merged with larger NoSQL companies perhaps because the graph-only market did not represent a large enough independent opportunity. Strength of the Relational Model Fortunately, the relational model has kept pace with modern workloads from webscale Internet applications, to the Internet of Things, to real-time analytics. Two critical inventions have catalyzed the strength of the relational model: In-Memory Computing With DRAM footprints increasing, and memory prices dropping, it becomes economically advantageous to keep high value data in memory Distributed Systems Advances in distributed programming deliver near unlimited scale to foundational infrastructure Coupling these technical advancements with a relational model delivers a solution to tackle large data workloads with ease, and with structure built in. FIGURE 3: A Relational Database Model All for One, One for All In the first round of the big data explosion, infrastructure tools became so abundant that far too quickly data practitioners were working more on data plumbing than data science. A cascading flow of infrastructure tools became as common as the data flow itself. Now companies can store data in-memory, scale with distributed systems, and maintain a relational model from the outset. This provides the operational model and required performance, with the structure to immediately understand.", "date": "2015-03-27"},
{"website": "Single-Store", "title": "hana-and-hadoop", "author": ["Gary Orenstein"], "link": "https://www.singlestore.com/blog/hana-and-hadoop/", "abstract": "Takeaways from the Gartner Business Intelligence and Analytics Summit Last week, SingleStore had the opportunity to participate in the Gartner Business Intelligence and Analytics Summit in Las Vegas. It was a fun chance to talk to hundreds of analytics users about their current challenges and future plans. As an in-memory database company , we fielded questions on both sides of the analytics spectrum. Some attendees were curious about how we compared with SAP HANA, an in-memory offering at the high-end of the solution spectrum. Others wanted to know how we integrated with Hadoop, the scale-out approach to storing and batch processing large data sets. And in the span of a few days and many conversations, the gap between these offerings became clear. What also became clear is the market appetite for a solution. Hardly Accessible Not Affordable While HANA does offer a set of in-memory analytical capabilities primarily optimized for the emerging SAP S4/HANA Suite, it remains at such upper echelons of the enterprise IT pyramid that it is rarely accessible across an organization. Part of this stems from the length and complexity of HANA implementations and deployments. Its top of the line price and mandated hardware configurations also mean that in-memory capabilities via HANA are simply not affordable for a broader set of needs in a company. Hanging with Hadoop On the other side of the spectrum lies Hadoop, a foundational big data engine, but often akin to a large repository of log and event data. Part of Hadoop’s rise has been the Hadoop Distributed File System (HDFS) which allowed for cheap and deep storage on commodity hardware. MapReduce, the processing framework atop HDFS, powered the first wave of big data, but as the world moves towards real-time, batch processing remains helpful but rarely sufficient for a modern enterprise. In-Memory Speeds and Distributed Scale Between these ends of the spectrum lies an opportunity to deliver in-memory capabilities with an architecture on distributed, commodity hardware accessible to all. The computing theme of this century is piles of smaller servers or cloud instances, directed by clever new software, relentlessly overtaking use-cases that were previously the domain of big iron. Hadoop proved that “big data” doesn’t mean “big iron.” The trend now continues with in-memory. Moving To Converged Transactions and Analytics At the heart of the in-memory shift is the convergence of both transactions and analytics into a single system, something Gartner refers to as Hybrid transactional/analytical processing (HTAP). In-memory capabilities make HTAP possible. But data growth means the need to scale. Easily adding servers or cloud instances to a distributed solution lets companies meet capacity increases and store their highest value, most active data in memory. But an all-memory, all-the-time solution might not be right for everyone. That is where combining all-memory and disk-based stores within a single system fits. A tiered architecture provides infrastructure consolidation and low cost expansion high value, less active data. Finally, ecosystem integration makes data pipelines simple, whether that includes loading directly from HDFS or Amazon S3, running a high-performance connector to Apache Spark, or just building upon a foundational programming language like SQL. SQL-based solutions can provide immediate utility across large parts of enterprise organizations. The familiarity and ubiquity of the programming language means access to real-time data via SQL becomes a fast path to real-time dashboards, real-time applications, and an immediate impact. Related Links: To learn more about How HTAP Remedies the Four Drawbacks of Traditional Systems here Want to learn more about in-memory databases and opportunities with HTAP? – Take a look at the recent Gartner report here . If you’re interested in test driving an in-memory database that offers the full benefits of HTAP, give SingleStore a try for free , or give us a ring at (855) 463-6775.", "date": "2015-04-09"},
{"website": "Single-Store", "title": "overlap-ad-targeting", "author": ["Conor Doherty"], "link": "https://www.singlestore.com/blog/overlap-ad-targeting/", "abstract": "Digital advertising is a numbers game played out over billions of interactions. Advertisers and publishers build predictive models for buying and selling traffic, then apply those models over and over again. Even small changes to a model, changes that alter conversion rates by fractions of a percent, can have a profound impact on revenue over the course of a billion transactions. Serving targeted ads requires a database of users segmented by interests and demographic information. Granular segmentation allows for more effective targeting. For example, you can choose more relevant ads if you have a list of users who like rock and roll, jazz, and classical music than if you just have a generic list of music fans. Knowing the overlap between multiple user segments opens up new opportunities for targeting. For example, knowing that a user is both a fan of classical music and lives in the San Francisco Bay Area allows you to display an ad for tickets to the San Francisco Symphony. This ad will not be relevant to the vast majority of your audience, but may convert at a high rate for this particular “composite” segment. Similarly, you can offer LA Philharmonic tickets to classical fans in Southern California, Outside Lands tickets to rock and roll fans in the Bay Area, and so on. Until recently, technical challenges made overlap targeting difficult. Legacy OLTP databases, the original backend for ad exchanges, could not deliver the analytic query performance to do overlap analysis in real time. On the other hand, precomputing segment overlap requires storing inordinately many segment combinations (e.g. classical and SF, rock and SF, jazz and SF, … , classical and LA, rock and LA, …), and the amount of computation and storage grows even faster as you begin combining more segment types. SingleStore enables a simpler approach by converging real-time data ingest and segment analysis in a single database. This post explains how to use SingleStore to build a system that computes audience segment overlap in real time. Size and shape of the data Suppose there are 2 billion users (all the users on the internet), and those users can be members of any number of 100 thousand defined categories, or “segments.” Clicks are recorded as triples: (user_id, segment_id, timestamp). We use the following DDL for the table definition: CREATE TABLE user_groups\n(\n    user_id BIGINT,\n    segment_id INT,\n    ts TIMESTAMP,\n    shard key(user_id),\n    key segment (segment_id) using clustered columnstore\n) Note that this data is not unique – different users and groups can occur multiple times. Now suppose we collect around 100 billion records (clicks) every month. Computing unions and intersections over two or more segments in subsecond time, while constantly loading new data, was not feasible with legacy technology. SingleStore offers a column store index to allow efficient storage and scanning for large tables. SingleStore column store tables persist to disk in a highly compressed format. Those 200 billion click records consume less than one TB of compressed data. Data loading and maintaining the sort order 100 billion records a month works out to around 3 billion records a day. In order to deliver this level of ingest, SingleStore introduced an open source tool called SingleStore Loader that enables bulk data loading at high speeds from AWS, S3, HDFS or a local file system. As data is loaded, SingleStore sorts and merges the new data with existing data in the background. Instead of maintaining a perfect sort order, which is very expensive, SingleStore keeps the data set “almost sorted” as described in this documentation page . Maintaining some degree of sort order during data load is key to delivering sub-second query performance for unions and intersections. Data loading is fully transactional and doesn’t disrupt concurrent queries. You don’t need to “vacuum” data like in a legacy data warehouse. SingleStore sorts data automatically and you can monitor progress using SHOW COLUMNAR MERGE STATUS command. Finding and targeting composite segments With all user data in a single table, SQL makes it easy to slice and dice to analyze the relationship between user segments. The following query computes group union of segment 1 (s1) and segment 2 (s2): SELECT\n    distinct segment_id\nFROM\n    user_groups\nWHERE\n    segment_id in (s1, s2) There are a number of ways to compute segment intersections. While you may intuitively jump to writing the query as a self join, it can actually be written more simply with clever use of built-in functions. The following query computes the intersection of s1 and s2 (i.e. all users in both s1 AND s2): SELECT\n    user_id,\n    (max(segment_id = s2) and max(segment_id = s1)) both_segments\nFROM\n    user_groups\nWHERE\n    segment_id IN(s1, s2)\nGROUP BY\n    user_id\nHAVING\n    both_segments = true In this query, the expression (max(segment_id = s2) and max(segment_id = s1)) tests whether a given user, identified by user_id, falls under both segment s1 and segment s2. segment_id = s2 returns 1 (true) when segment_id matches s2 and 0 (false) otherwise. max(segment_id = s2) in the projection of the query returns 1 if any segment_id matches s2 and returns 0 otherwise. The expression as a whole is summarized as the variable both_segments, as in: “this user_id falls under both segments.” With minor tweaks, you can use queries like this one to answer other segment overlap questions. For example, you can find all the users in s1 but not s2 using the following query: SELECT\n    user_id,\n    (max(segment_id = s1) and min(segment_id != s2)) s1_only\nFROM\n    user_groups\nWHERE\n    segment_id IN(s1, s2)\nGROUP BY\n    user_id\nHAVING\n    s1_only = true Similar to both_segments in the previous query, the expression (max(segment_id = s1) and min(segment_id != s2)) returns 1 (true) only if there is some user_id uid such that the table user_segments some record (uid, s1, …) but zero records matching (uid, s2, …). Sub-second query performance The SingleStore query optimizer understands that, since the dataset is sharded by user_id, it can push down the computation of distinct user_ids to each individual partition and then just sum up the results. Further, each partition is sorted on segment_id, which allows SingleStore to skip over the majority of the data without having to actually read it. Only the parts of the table relevant to the query need to be decompressed and used. Due to this efficient scanning, each partition processes hundreds of thousands of records in parallel, which is why SingleStore can compute overlap queries in under a second. Data retention By its nature, ad targeting data becomes less relevant as it ages. Data retention is achieved by simply running a delete query. DELETE\nfrom\n    user_groups\nwhere\n    ts < date_sub(now(), interval 2 month); After the statement is issued, SingleStore compacts column segments in the background. Delete operations don’t disrupt the ongoing queries. This allows you to keep your ad platform online and running queries on fresh data 24 x 7. Hardware footprint The workload described can be achieved on just three nodes: one aggregator and two leaves, all of them Amazon cr3x.large instances. Due to the high compression of sorted lists of integers, 700GB of SSD per node is enough to store 200 billion data points. Go deeper with your audience Combining and intersecting user segments allows advertisers to better understand their audience using only pre-collected data. Conventional database technology limitations have complicated this type of analysis in the past. However, SingleStore makes computing composite segments simple in both concept and execution. These techniques apply outside of digital advertising as well. Any business problem that requires analyzing potentially overlapping categories can benefit from this big data Venn diagram. With SingleStore, you can produce composite segments on live data using only SQL. Want to build your own overlap targeting platform? Download SingleStore today .", "date": "2015-04-14"},
{"website": "Single-Store", "title": "real-time-stream-processing-with-hadoop", "author": ["Lesia Myroshnichenko"], "link": "https://www.singlestore.com/blog/real-time-stream-processing-with-hadoop/", "abstract": "While SingleStore and Hadoop are both data stores, they fill different roles in the data processing and analytics stack. The Hadoop Distributed File System (HDFS) enables businesses to store large volumes of immutable data, but by design, it is used almost exclusively for batch processing. Moreover, newer execution frameworks, that are faster and storage agonistic, are challenging MapReduce as businesses’ batch processing interface of choice. Lambda Architecture A number of SingleStore customers have implemented systems using the Lambda Architecture (LA). LA is a common design pattern for stream-based workloads where the hot, recent data requires fast updates and analytics, while also maintaining long-term history on cheaper storage. Using SingleStore as the real-time path and HDFS as the historical path has been a winning combination for many companies. SingleStore serves as a real-time analytics serving layer, ingesting and processing millions of streaming data points a second. SingleStore gives analysts immediate access to operational data via SQL. Long-term analytics and longer running, batch-oriented workflows are pushed to Hadoop. Use Case: Real-Time Analytics at Comcast As an example, SingleStore customer Comcast focuses on real-time operational analytics. By using SingleStore and Hadoop together, Comcast can proactively diagnose potential issues from real-time intelligence and deliver the best possible video experience. Their Lambda architecture writes one copy of data to a SingleStore instance and another one to Hadoop. SingleStore enables Comcast to run lightning fast real-time analytics on large, changing datasets and makes their analytics infrastructure more performant overall. Instead of just logging all Xfinity data and analyzing it hours or days later, SingleStore gives Comcast the power to get both viewership and infrastructure monitoring metrics in real time. HDFS provides a quasi-infinite data store where they can run machine learning jobs and other “offline” analytics. Watch the recorded session from Strata+Hadoop World to find out more on how SingleStore helps Comcast improve their Xfinity platform to work with millions of users, process enormous volumes of data and, at the same time, perform advanced real-time analytics. Review more customer stories . If you’re interested in test driving SingleStore, download it now .", "date": "2015-04-15"},
{"website": "Single-Store", "title": "harnessing-enterprise-capabilities-memsql-spark", "author": ["Conor Doherty"], "link": "https://www.singlestore.com/blog/harnessing-enterprise-capabilities-memsql-spark/", "abstract": "As more developers and data scientists try Apache Spark, they ask questions about persistence, transactions and mutable data, and how to deploy statistical models in production. To address some of these questions, our CEO Eric Frenkiel recently wrote an article for Data Informed explaining key use cases integrating SingleStore and Spark together to drive concrete business value. The article explains how you can combine SingleStore and Spark for applications like stream processing, advanced analytics, and feeding the results of analytics back into operational systems to increase efficiency and revenue. As distributed systems with speedy in-memory processing, SingleStore and Spark naturally complement one another and form the backbone of a flexible, versatile real-time data pipeline. Read the full article here . Get The SingleStore Spark Connector Guide The 79 page guide covers how to design, build, and deploy Spark applications using the SingleStore Spark Connector. Inside, you will find code samples to help you get started and performance recommendations for your production-ready Apache Spark and SingleStore implementations. Download Here", "date": "2015-04-16"},
{"website": "Single-Store", "title": "celebrating-memsql-availability-two-years-in", "author": ["Eric Frenkiel"], "link": "https://www.singlestore.com/blog/celebrating-memsql-availability-two-years-in/", "abstract": "Today, I couldn’t be more excited to mark the two year anniversary of SingleStore general availability! SingleStore began with a simple idea to build a new database that would give any company the ability to operate in real-time and make their business more data-driven, responsive, and scalable. Since releasing SingleStore , it’s been an amazingly fun journey as the company has grown by leaps and bounds every quarter. To celebrate our second birthday, I wanted to take a brief moment to reflect on what we’ve been able to accomplish in the two years since releasing SingleStore. People SingleStore started in the Y-Combinator winter class of 2011 with just two people – co-founder and CTO Nikita Shamgunov, and myself. Since then, we’ve grown the company to more than 50 people who bring great experience, energy, and passion to the company. We’ve also added database visionaries like Jerry Held and Chris Fry to our executive team to help us see our vision come to fruition. Customers We’ve added 40+ enterprise customers over the past 2 years, including top brands like Comcast, Samsung and Shutterstock . It’s been incredibly rewarding to see our customers use SingleStore in ways we never imagined, truly pushing the boundaries of what is possible in Big Data. Product Since launching with general availability in 2013, we’ve expanded the SingleStore platform to scale with growing market demand. Major additions to the platform include: Going beyond memory by including a flash-optimized column store that is closely integrated with the in-memory row store to provide a single database for real-time and historical analytics Working with Apache Spark by shipping a bi-directional connector to operationalize Spark models and results Incorporating real-time geospatial intelligence to help customers build location-aware applications and analytics What’s Next? The most exciting times are still ahead! Big data has been traditionally thought of as a mechanism for extracting insights from yesterday’s data. We seek to change that way of thinking, empowering businesses to be more responsive by operating with real-time data in the here and now. As demand for real-time and in-memory databases increases, we plan to be there helping customers achieve phenomenal results.", "date": "2015-04-23"},
{"website": "Single-Store", "title": "driving-relevance-with-real-time-and-historical-data", "author": ["Gary Orenstein"], "link": "https://www.singlestore.com/blog/driving-relevance-with-real-time-and-historical-data/", "abstract": "As technology weaves into our daily lives, our expectations of it continue to increase. Consider mobile devices and location information. Recently 451 Research released data that 47% of consumers would like to receive personalized information based on immediate location. Source: 451 Research Addressing this requires the ability to track real-time and historical data and to put both in context. Let’s examine that spectrum. Incoming High Value Content With a focus on ‘immediate,’ the highest value content typically arrives via real-time streaming . This series of events must be capture and logged in a persistent datastore such as an operational database. It will always involve some type of transaction whether that is recording a data point and location, or completing a purchase. Real-time activity gets us closer to the moment of truth, when a series of events trigger an offer that converts. Online advertising is the epitome of this moment. In a fraction of a second, online advertisers seek to understand who is viewing a page, segment and calculate what that visitor might be interested in, and subsequently present the most compelling (and perhaps top paying) offer. Merging Real-Time and Historical Data The moment at hand rarely completes the picture. Past activity helps determine the right information and offer. For example, did you visit a particular restaurant previously, did you rate it, and did your friends comment on it? Or what articles have you read in the past few weeks that would help refine an understanding of your most pressing interests? These questions can be answered when real-time and historical data can be easily merged into a single question, or query, giving businesses the power to operationalize analytics. Historical Data Retention No one likes to throw data away. The upside of potential value tends to outweigh the challenge and cost of retention. But with incoming data volumes and accumulation, there needs to be an easy way to retain historical data, and to do so affordably. Covering the Spectrum At SingleStore, we work to help our customers cover the spectrum from high value to high volume data, as outlined in the following diagram: A complete real-time data pipeline includes in-memory capabilities for streaming and transactions combined with disk-optimized solutions for historical data. It further includes the ability to independently address and simultaneously work with both types of data in simple operations, such as a query across different types of tables in a database. A Simple Software Approach Today, many big data architects have to wrestle with numerous solutions to cover a spectrum like the one outlined above. Fortunately, with an in-memory database that also includes a disk-based column engine, a simplified approach is just a download away. Give SingleStore a try for free or give us a call with any questions you have: (855) 463-6775", "date": "2015-04-30"},
{"website": "Single-Store", "title": "tech-data-field-day-reception-may-13th", "author": ["Gary Orenstein"], "link": "https://www.singlestore.com/blog/tech-data-field-day-reception-may-13th/", "abstract": "Tech Field Day is coming to  San Francisco next week with a focus on data, and that means time for a party! On Wednesday, May 13th, at 6:00pm in San Francisco , we will gather with industry participants and expert delegates from Tech Field day for an evening of food, drinks, and engaging conversation about All Things Data! Please RSVP for the evening reception .\\\nWednesday, May 13th, 6:00pm, 534 4th St, San Francisco . About Tech Field Day From big data to analytics to hyperscale architecture and cloud security, a new wave of innovation is transforming IT. The old way of doing things can’t keep up with the proliferation of data and micro-services essential to deliver services to mobile devices and the Internet-connected world. These “new stack” companies and technologies have a different audience from the traditional infrastructure. That’s why we created Data Field Day! Agenda 6:00 – 7:00 : Welcome reception, food and drinks (non-alcoholic too!) 7:00 – 8:00 : Ignite presentations 8:00 – 9:00 : More networking, food and drinks The event will include all of the delegates from Tech Field day listed below. Please RSVP for this free event and we hope to see you there. Delegates Andrew Mauro @Andrea_Mauro IT Architect with focus on virtualization, cloud and storage Joey D’Antoni @JDAnton Native New Orleanian, Database Professional, Lover of food and wine, cyclist. Microsoft SQL Server MVP John Obeto @JohnObeto I like SMBs and Windows John Troyer @JTroyer Leader of the TechReckoning community, podcaster, blogger, analyst Josh Luedeman @JoshLuedeman Josh Luedeman is a Consultant for Pragmatic Works, that works on projects involving Big Data, Business Intelligence, and Cloud Architecture. Justin Warren @JPWarren Justin is a consultant and freelance journalist who enjoys coding in Python and words that are fun to say, like ‘llama’ and ‘shenanigans’. Karen Lopez @DataChick Data Evangelist and Architect Paul Miller @PaulMiller Cloud Computing/ Big Data/ Open Data Analyst & Consultant. Writer, Speaker & Moderator. Gigaom Research Analyst. Based in UK, clients world-wide. Theo Priestley @ITredux Technology Evangelist, Analyst, Startup Advisor, Geek Yves Mulkers @YvesMulkers BI Professional enjoying family, business intelligence, data, music, and DJ-ing. BBBT member. See you then! Please RSVP .", "date": "2015-05-05"},
{"website": "Single-Store", "title": "join-memsql-at-adtech-san-francisco", "author": ["Lesia Myroshnichenko"], "link": "https://www.singlestore.com/blog/join-memsql-at-adtech-san-francisco/", "abstract": "We look forward to ad:tech San Francisco next week, May 20-21, and hope to see you there. Visit us at booth #1442 for games and giveaways, and learn how SingleStore can help you boost conversions through overlap ad targeting. We will be showcasing our latest demo with Pinterest that features building real-time data pipelines with Kafka, Spark, and SingleStore. We also will be demonstrating our new real-time geospatial intelligence capabilities though our Supercar demo. Meet with us at ad:tech Schedule an in-person meeting or demo at the event. Reserve a Time → Learn How to Boost Conversions with Overlap Ad Targeting Combining and intersecting user attributes allows you to better understand your audience and serve ads with greater relevance. However, conventional database limitations have made such functionality difficult, adding complexity and latency. SingleStore gives you the ability to drive conversions by serving tailored ads to users based on their interests and demographics. Learn more about how to use overlap ad targeting in our technical blog post . Games and Giveaways Drop by the SingleStore booth to get your free, super-soft t-shirt and play our reaction test game to win an Estes ProtoX Mini Drone. Suggested Sessions Reaching Creative Addressability at Scale Thursday, May 21, 2015 3:00 PM – 3:50 PM As Advertising technology platforms accelerate from planning and buying to content marketing and measurement, creative automation is the next frontier. Some database companies and marketers are claiming ‘Addressable Messaging at One-to-One’ is not far off. As we move from one-to-many, to one-to-some targeting, to the holy grail of one-to-one messaging, what will the creative look like? Will headlines be written by algorithms? Or will copywriters evolve from campaign writing to serving ad copy in real time? More details here Watches, Bracelets and Bras: Where Smart Wearables are Going Thursday, May 21, 2015 10:30 AM – 11:20 AM Right now, most wearable technology goes unworn. But with practical items like Victoria Secret’s sports bra with built-in heart rate monitor, and fashionable accessories that do everything from track your steps to alert you when you forget your phone, that is undoubtedly about to change. This session will explore the practicality of wearable technology, where it is going, and how marketers can leverage it. More details here", "date": "2015-05-15"},
{"website": "Single-Store", "title": "memsql-4-market-and-strategy-ceo-qa", "author": ["Eric Frenkiel"], "link": "https://www.singlestore.com/blog/memsql-4-market-and-strategy-ceo-qa/", "abstract": "SingleStore DB 4 represents a leap forward in relational database innovation with new capabilities such as real-time geospatial intelligence and high-throughput connectivity to Apache Spark. It also includes a free forever, unlimited capacity Community Edition , enhancements to the Optimizer for distributed joins, and a new version of the SingleStore Ops management framework. Start Using SingleStore Community Edition Now Unlimited scale and capacity. Free forever. Download Community Edition → As Big Data abounds, understanding every company and product can be tricky. To make easier, here are a few questions and answers about SingleStore. The database landscape is big. Where does SingleStore fit? SingleStore is the leading database for real-time transactions and analytics. That means we’re operational by nature, much more akin to SQL Server, Oracle, or SAP HANA than Hadoop. SingleStore is also: In-Memory Providing the utmost performance for today’s demanding workloads Distributed Enabling cost-effective, horizontal scale-out on-premises or in the cloud Relational and multi-model Allowing companies to use in-house SQL tools, applications, and knowledge With JSON and Geospatial data formats supported Software Designed to run on commodity hardware for costs savings You talk about transactions and analytics. Can you explain in more detail? To meet real-time demands, companies must be able to capture information across millions to hundreds of millions of sensors or mobile applications. They also want to analyze that data up to the last transaction. So with real-time operations you don’t have the luxury or the pain of ETL. You need to bypass ETL by transacting and analyzing in a single database designed to support these concurrent workloads. It boils down to analytics on changing datasets, today’s critical capability. SingleStore DB 4 includes the SingleStore Spark Connector and the SingleStore Loader for HDFS and S3. How should we think about SingleStore with Spark and Hadoop? SingleStore and Spark work well together as they both have memory-optimized, distributed frameworks. Spark is a processing framework that enables real-time transformation and advanced analytics, but Spark itself does not have a storage or persistence ability. By storing data permanently in SingleStore, customers get an easy way to build operational applications, and the ability to take operational data and ‘round-trip’ it to Spark for advanced analytics. With Hadoop, customers frequently build simplified Lambda architectures using SingleStore. All data can go directly to HDFS for long term archiving. Simultaneously, data can go directly into SingleStore, bypassing Hadoop for the real-time path. Should historical data be needed for analysis, SingleStore can import that data from HDFS using the SingleStore Loader. Many folks say not everything needs to be in-memory. How do you respond? We agree! While in-memory computing remains critical for many applications, the pace of data growth still eclipses memory-only solutions. This is exactly why SingleStore ships with an integrated column-store optimized for disk and flash. Now customers can create tables entirely in-memory, or across a combination of memory and disk and flash. And starting with SingleStore DB 4, we license the software based on DRAM capacity so the use of disk or flash storage in the column store is unlimited at no additional cost. We are big believers that customers will see the benefit of placing data in a structured format from the beginning, and the SingleStore column store will let them do that affordably. You have always focused on SQL. All the while NoSQL has received a lot of discussion. Where is SingleStore in this? SQL is the lingua franca of the data processing world, having been well adopted over its decades-long history. Companies can build SQL applications quickly, and then immediately use in house analytics tools and practices to derive insights. Prevailing wisdom used to be that SQL could not scale. That is untrue and companies are now discovering the powerful combination SingleStore delivers of a relational, in-memory, distributed database with speed, scale, and simplicity. The future is multi-model and SingleStore also includes JSON and Geospatial datatypes to fulfill this promise. The Community Edition is freely available but not open-source. What strategy is SingleStore pursuing? Our strategy is to build a scalable software business. This means running on all the platforms customers want to use, including public and private clouds, in the way they want to use them. Community Edition is a way to allow more people to use SingleStore without limitations on time or capacity. There is a wide spectrum of commercial database offerings. On one extreme there are proprietary hardware / software combinations. Our belief is that proprietary hardware is a down elevator. On the other end of the spectrum there are open-source projects with businesses built around them. But that is essentially selling consulting hours, not software. There are many successful businesses in the middle of the spectrum, such as MySQL’s dual commercial / open-source licensing, and we maintain open-source projects, including SingleStore Loader for Hadoop and S3, a large number of general-purpose Python packages, and the SingleStore Spark Connector. At SingleStore, we want to offer performance that trumps legacy vendors, deliver it a fraction of the cost, and provide complete deployment choice across hardware, data centers, and clouds. In doing so we hope to help more companies achieve their goals to become real-time enterprises.", "date": "2015-05-20"},
{"website": "Single-Store", "title": "memsql-community-edition", "author": ["Eric Frenkiel"], "link": "https://www.singlestore.com/blog/memsql-community-edition/", "abstract": "We started SingleStore with a vision to enable every company to be a real-time enterprise, and to do that by delivering the leading real-time database for transactions and analytics. Since then, the forces shaping our digital economy have only added wind to our sails. The world is more mobile, interconnected, interactive, and on the cusp of several industry transformations such as the Internet of Things. Real-time processing is the secret to keeping up, and in-memory solutions are the foundation. Yet existing options have been too expensive or too complex for companies to adopt. That changes today with the release of SingleStore DB 4 and our new Community Edition , a free unlimited capacity, unlimited scale offering of SingleStore that includes all transactional and analytical features. By sharing SingleStore capabilities with the world, for free, we expect many developers and companies will have a chance to explore what is possible with in-memory computing. As the pace of business advances, SingleStore will be there. Start Using SingleStore Community Edition Now Unlimited scale and capacity. Free forever. Download Community Edition → We hope you enjoy working with our Community Edition. Please feel free to share feedback at our Community Edition Help page . Eric Frenkiel, CEO and co-founder, SingleStore Nikita Shamgunov, CTO and co-founder, SingleStore Community FAQ What is Community Edition? Community Edition is a way to use SingleStore at unlimited scale and unlimited capacity at no cost. You can download it and immediately start building an app with SingleStore. Is it Open Source? Ecosystem integrations like the SingleStore Spark Connector and the SingleStore Loader for HDFS and Amazon S3 are open source, and on GitHub . The Community Edition is distributed as an executable binary and is a free edition of the commercial SingleStore Enterprise Edition. You are free to download and use SingleStore Community Edition within your organization. How much does it cost? SingleStore Community Edition is free forever with unlimited scale and database capacity. What is the difference between Community and Enterprise Editions? Enterprise Edition comes with direct support from SingleStore, plus production features such as high availability, cross data center replication, granular user permissions, and SSL. More information can be found on at singlestore.com/free/ How do I move from Community to Enterprise? You can sign up for a free trial of Enterprise at any time and apply the license key. Or, talk to us at [info@singlestore.com] (mailto: info@singlestore.com ?Subject=Community Edition Inquiry) or (855) 463-6775. Does SingleStore support Community Edition in production? While you are free to use Community for your projects, SingleStore does not support or endorse using it in production. SingleStore Enterprise Edition comes with the features needed to support critical deployments. What level of support comes with Community? SingleStore Community Edition does not come with phone or email support. We and other SingleStore experts are active in the StackOverflow community, our documentation is public, and we open real-time chat during California business hours. More info here .", "date": "2015-05-20"},
{"website": "Single-Store", "title": "finding-bottlenecks-in-your-computing-system", "author": ["Carlos Bueno"], "link": "https://www.singlestore.com/blog/finding-bottlenecks-in-your-computing-system/", "abstract": "Read data in, write data out. In their purest form, this is what computers accomplish. Building a high performance data processing system requires accounting for how much data must move, to where, and the computational tasks needed. The trick is to establish the size and heft of your data, and focus on its flow. Identifying and correcting bottlenecks in the flow will help you build a low latency system that scales over time. Characterizing your system Before taking action, characterize your system using the following 8 factors: Working set size </ br>Set of data a system needs to address during normal operation. A complex system will have many distinct working sets, but one or two of them usually dominate.</ br> Average transaction size </ br>Working set of a single transaction performed by the system.</ br> Request size </ br>Expected throughput. The combination of throughput and transaction size governs most of the total data flow of the system. Update rate </ br>Measure of how often data is added, deleted, and edited.</ br> Consistency </ br>Time required for an update to spread through the system.</ br> Locality </ br>Portion of a working set a request needs access to.</ br> Computation </ br>Amount of math needed to run on the data.</ br> Latency </ br>Expected time for transactions to return a success or failure.</ br> Download the Capacity Planning Cheat Sheet 8 system factors to define before capacity planning Identifying bottlenecks After pinpointing these characteristics, it should be possible to determine the dominant operation responsible for data congestion. Your answer might be obvious, but identifying the true bottleneck will provide a core factor to focus on. The pizzeria example Let’s say you own a pizza shop and want to make more money. If there are long lines to order, you can double the number of registers. If the pizzas arrive late, you can work on developing a better rhythm. You might even try raising the oven temperature a bit. But fundamentally, a pizza shop’s bottleneck is the size of its oven. Even if you get everything else right, you won’t be able to move more pizzas per day without expanding your oven’s capacity or buying a second one. If you can’t clearly see a fundamental bottleneck, change a constraint and see what shifts in response. What happens if you had to reduce the latency requirement by 10x? Halved the number of computers? What tricks could you get away with if you relax the constraint on consistency? It’s common to take the initial constraints as true and unmoving, but they rarely are. Creativity in the questions has more leverage than creativity in the answers. If you’re looking to build a well-designed computing system, I contributed an in-depth article on Infoq , that provides use cases and real-world examples.", "date": "2015-05-29"},
{"website": "Single-Store", "title": "memsql-and-docker", "author": ["Carl Sverre"], "link": "https://www.singlestore.com/blog/memsql-and-docker/", "abstract": "Evaluating software infrastructure is important, but it should not be difficult. You should be able to try and see quickly whether a piece of core software suits your needs. This is one of the many helpful use cases for Docker. Of course Docker has many more uses, including helping run a 107 node cluster with CoreOS , but this post focuses on the quick start scenario. With an install of boot2docker.io for Mac or Windows, and a pre-configured ‘cluster-in-a-box’ Docker container, you can be on your way to interacting with a distributed system like SingleStore in a few minutes. If you are ready to jump in, head to our Quick Start with Docker documentation . In a nutshell, we have built a ‘quickstart’ container that comes installed with SingleStore Ops for management, a single-node SingleStore cluster, and some sample programs referenced in tutorials. Obviously, this is not the configuration to test drive maximum performance. If that were the case, you would want to take advantage of the distributed architecture of SingleStore across several nodes. But it is a great way to get a sense of working with SingleStore, connecting with a MySQL client, and experiencing the database first hand. If you already have Docker installed, you can jump right in with a few simple commands. Spin up a cluster $ docker pull memsql/quickstart\n$ docker run --rm --net=host memsql/quickstart check-system\n$ docker -d -p 3306:3306 -p 9000:9000 --name=memsql memsql/quickstart At this point you can create a database, and interact with the ‘cluster-in-a-box’ install of SingleStore. For example you can run a quick benchmark against SingleStore with the following Docker command $ docker run --rm -it --link=memsql:memsql memsql/quickstart simple-benchmark For more information on working with SingleStore and the ‘cluster-in-a-box’ Docker container, visit our documentation at docs.singlestore.com/latest/setup/docker . We’ve also made the Dockerfile available on Github at github.com/memsql/memsql-docker-quickstart . And if you would like to try SingleStore in full, please visit singlestore.com/free . There we have a free unlimited scale and capacity Community Edition and a free 30-day Enterprise Trial.", "date": "2015-06-04"},
{"website": "Single-Store", "title": "what-is-real-time", "author": ["Conor Doherty"], "link": "https://www.singlestore.com/blog/what-is-real-time/", "abstract": "The phrase “real-time,” like love, means different things to different people. At its most basic, the term implies near simultaneity. However, the amount of time that constitutes the “real-time window” differs across industries, professions, and even organizations. Definitions vary and the term is so often (ab)used by marketers and analysts, that some dismiss “real-time” as a meaningless buzzword. However, there is an important distinction between “real-time” and “what we have now but faster.” A real-time system is not just faster, but fast enough to cross a performance threshold such that your business can reproducibly gain net new value. This abstract definition is likely too general to assuage the real-time absolutists. However, there is no way to select a single number value definition of “real-time” that works for all use cases. Rather, it’s better to talk about “real-time” as a heuristic and allow stakeholders to establish conventions tailored to their own idiosyncratic business problems. Instead of claiming real-time means X seconds, this article will describe two classes of real-time applications and their performance requirements. Machines Acting in Real-Time One class of real-time applications is where machines programmatically make data-driven decisions. The ability to automate data-driven decisions is especially valuable for applications where the volumes of data or demanding service level agreements (SLAs) make it impossible for the decision to hinge on human input. Example: Real-Time Bidding Take the example of digital advertising, where real-time user targeting and real-time bidding on web traffic have revolutionized the industry. Selecting a display ad or choosing whether to buy traffic based on the viewer’s demographic and browsing information can boost click-through and conversion rates. Clearly, the process of choosing ads and whether to buy traffic must be done programmatically – the volume of traffic on a busy site is too large and the decisions must be made too quickly for it to be done by humans. For this application, “real-time” means roughly “before the web page loads in the browser window.” This brief lag period is essentially free computation time while the viewer waits a fraction of a second for the page to load. This definition of real-time may not be numerically absolute, but it is well-defined. While businesses implementing real-time advertising platforms will often impose particular SLAs (“this database call must return in x milliseconds”), these time values are just heuristics representing an acceptable execution time. In practice, there may not be a hard and fast cutoff time beyond which it “doesn’t work.” The business may determine that clicks tail off at some rate as page load time lengthens, and shrinking average load time causes an increase in clickthrough rate. Example load times and clickthrough rate Time to load (s) Clickthrough rate .2 3 .4 2 .6 1 .8 .5 1.0 .1 This real-time window is not a discreet interval that guarantees uniform outcomes—rather, it’s defined probabilistically. Every time a user views a web page with a display ad, we know they will click on an ad with some probability (i.e. clickthrough rate). If the page or display ad loads slowly, the viewer is more likely to overlook the ad, or navigate to a different page entirely, decreasing the average clickthrough rate. If the page and ad load quickly, the viewer is more likely to click on the ad, increasing the average clickthrough rate. While this definition of real-time allows a range of response times, in practice the range tails off quickly. For instance, the clickthrough rate at 2 seconds load time is likely near 0%. This is what I mean when I say a real-time application is one that is “fast enough” to capture some untapped value. The “real-time” approach of dynamically choosing display ads or bidding on traffic based on user profile information is fundamentally different from the legacy approach of statically serving ads regardless of viewer profile. However, real-time digital advertising is only worth implementing if it can be done fast enough to lift intended user behavior. There are many applications for machines programmatically making decisions in real time, not just digital advertising. Applications include fraud detection, geo-fencing, and load-balancing a datacenter or CDN, to name a few. Humans Acting on Real-Time Data The other class of real-time applications is where humans respond to events and make data-driven decisions in real time. Despite strides in artificial intelligence and predictive analytics, many business problems still require a human touch. Often, solutions require synthesizing information about a complex system or responding to anomalous events. While these problems require the critical thinking of a human, they are still data-driven. Providing better information sooner lets humans reach a solution faster. Example: Data Center Management A good example of this type of problem is managing complex systems like data centers. Some of the management can be automated, but ultimately humans need to respond to unexpected failure scenarios. For online retailers and service providers, uptime directly correlates with revenue. With or without a real-time monitoring system in place, data center administrators can access live server data through remote access and systems monitoring utilities. But standard systems monitoring tools can only provide so much information. The challenge lies in collecting, summarizing, and understanding the flood of machine-generated data flowing from hundreds or thousands of severs. Doing this in real-time has some demanding requirements: Granular logging of network traffic, memory usage, and other important system metric Interactive query access to both recent and historical log and performance data so administrators can spot anomalies and act on them The ability to generate statistics reporting on recent machine data without tying up the database and blocking new data from being written The third requirement is arguably the hardest, and the one on which the definition of real-time hinges. It entails processing and recording all machine data (an operational or OLTP workload) and aggregating the data into useful performance statistics (a reporting or OLAP workload). The reporting queries must execute quickly without blocking the inflow of new data. Once again, there is no hard and fast rule for what constitutes a real-time window. It could be a second or a few seconds. Rather, the distinguishing feature of a real-time monitoring system is the ability to converge live data with historical data, and to interactively analyze and report on them together. The technical challenge is not simply collecting data, but how quickly can you extract actionable information from it. There are many applications for real-time monitoring beyond data center administration. It can be applied to understand and optimize complex dynamic systems such as an airline or shipping network. It can also be used for financial applications like position tracking and risk management. Moving to Real-Time Data Pipelines While the specific numerical values associated with “real-time” may vary between organizations, many enterprises are deploying similar data processing architectures to power data-driven applications. In particular, enterprises are replacing legacy architectures, that separate operational data processing from analytical data processing, with real-time data pipelines that can ingest, serve, and query data simultaneously. SingleStore forms the core of many such pipelines, often used in conjunction with Apache Kafka and Spark Streaming, for distributed, fault-tolerant, and high throughput data processing.", "date": "2015-06-06"},
{"website": "Single-Store", "title": "memsql-at-spark-summit", "author": ["Kevin White"], "link": "https://www.singlestore.com/blog/memsql-at-spark-summit/", "abstract": "We’re excited to be at Spark Summit next week in our hometown of San Francisco. If you’re attending, stop by booth K6 for games and giveaways, and checkout our latest demo that showcases how organizations are using SingleStore and Spark for real-time analytics. Meet with us at Spark Summit Schedule an in-person meeting or demo at the event. Reserve a Time → SingleStore and Spark Highlights Over the past year, we’ve been working closely with our customers and the Spark community to build real-time applications powered by Spark and SingleStore. Highlights include: Real-Time Analytics at Pinterest with Spark and SingleStore Learn how Pinterest built a real-time data pipeline with SingleStore and Spark Streaming to achieve higher performance event logging, reliable log transport, and faster query execution on real-time data. Read the full post on the Pinterest Engineering Blog SingleStore Spark Connector The SingleStore Spark Connector provides everything you need to start using Spark and SingleStore together. It comes with a number of optimizations, such as reading data out of SingleStore in parallel and making sure that Spark colocates data in its cluster. Download now on GitHub Building Real-Time Platforms with Apache Spark Watch our session from Strata+Hadoop World to learn how hybrid transactional and analytical data processing capabilities, integrated with Apache Spark, enable businesses to build real-time platforms for applications. Extending SingleStore Analytics with Spark Learn how the SingleStore Spark Connector enables data-driven businesses to couple operational data with advanced analytics. Read our post on the Databricks Blog Games and Giveaways Be sure to stop by booth K6 for a cool t-shirt and chance to win an Estes Drone! If you would like to schedule a meeting or demonstration with SingleStore while at the show, you can do so here . Hope to see you at Spark Summit!", "date": "2015-06-11"},
{"website": "Single-Store", "title": "modeling-the-city-of-the-future-with-kafka-and-spark", "author": ["Emily Friedman"], "link": "https://www.singlestore.com/blog/modeling-the-city-of-the-future-with-kafka-and-spark/", "abstract": "Today at Spark Summit in San Francisco, we are showcasing MemCity, a simulation that measures and maps the energy consumption across 1.4 million households in a futuristic, metropolitan city, approximately the size of Chicago. MemCity tracks, processes, and analyzes data from various energy devices that can be found in homes, measured by the minute in real-time. We define real-time as up to the last click, meaning all of the data being processed, up until the moment you hit enter on your query, is included. Each device is measured according to timestamp, device identification, and watts consumed. Appliances measured include laundry and water devices, kitchen appliances, entertainment, lighting, HVAC, and small devices.  The MemCity dashboard showcases the total Megawatts per hour by device, hour of the day, and zip code. MemCity is built with the “ real-time trinity ” of Apache Kafka, Apache Spark, and SingleStore. This is the same architecture that companies, like Pinterest, have implemented to unlock new business opportunities with real-time data processing and analytics. Kafka, Spark, and SingleStore pair well as all three technologies are distributed and memory-optimized. Together, they solve the problem of how to ingest, process, and serve real-time data across an organization. Kafka, a message queue, condenses the data from individual houses and devices into consumable streams. Next, Spark reads from Kafka, then transforms and enriches the data with geolocation and device type information. Finally, SingleStore provides persistence and real-time SQL queries, ingesting data from Spark while simultaneously powering live power consumption dashboards. Now for some of the technical specifications. Runs on four Amazon EC2 nodes, which cost roughly $2.35 an hour, equating to under $21,000 per year. Pipeline processes 186.67k transactions/sec – timestamp, device ID, watts consumed, geolocation info Runs simultaneous real-time SQL queries MemCity highlights an application of data that will guide our future – smart cities are no longer nice-to-have; they are becoming a necessity.  By capturing energy readings by device in real time over geographical distances and over periods of time, we can enable urban planners, energy companies, and other like organizations to identify trends in consumption and figure out the right solutions for smarter energy systems and devices. We can gather deep and meaningful insights from sensors, and realize the possibility of doing so with low-cost, commodity hardware. Putting data to good use makes long-term planning viable. For more information on MemCity, check out the official news release: /media-hub/releases/memcity/ . Come see us at Booth #K6 at Spark Summit West showcasing MemCity today through Wednesday. More information about our presence at the Summit: singlestore.com .", "date": "2015-06-15"},
{"website": "Single-Store", "title": "apache-spark-resources", "author": ["Lesia Myroshnichenko"], "link": "https://www.singlestore.com/blog/apache-spark-resources/", "abstract": "There’s no doubt about it. Apache Spark is well on its way to becoming a ubiquitous technology. Over the past year, we’ve created resources to help our users understand the real-world use cases for Spark as well as showcase how our technologies compliment one another. Now, we’ve organized and consolidated those materials into this very post. Videos Pinterest Measures Real-Time User Engagement with Spark Demo of real-time data pipeline processing and analyzing re-pins across the United States. The State of In-Memory and Apache Spark Interview with SingleStore CEO and Co-Founder, Eric Frenkiel, describing how a memory-optimized solution made the difference for Pinterest. Keynote: Close Encounters with the Third Kind of Database Strata+Hadoop World keynote presentation from SingleStore CEO, Eric Frenkiel, outlining the business case for Spark and SingleStore. Strata+Hadoop Session: Bringing OLAP Fully Online How to analyze changing datasets in SingleStore and Spark with a demo from Pinterest. Blog Posts and Articles Real-Time Analytics at Pinterest More about real-time analytics at Pinterest from the official Pinterest engineering blog . How Pinterest Measures Real-Time User Engagement with Spark Learn how Pinterest is using Spark Streaming and SingleStore to find patterns in high-value user engagement data in this blog post . Apache Kafka + Spark + Database = Real-Time Trinity Read this article from The New Stack to learn how to build real-time data pipelines with Kafka, Spark, and SingleStore. Harnessing the Enterprise Capabilities of Spark Learn how to combine SingleStore and Spark for applications like stream processing and advanced analytics to increase business efficiency and revenue in this data-informed article . Using Apache Spark in the Enterprise In this Inside big data article , Eric Frenkiel champions the use of Spark to achieve the promise of real-time analytics. Extending SingleStore Analytics with Spark Learn how the SingleStore Spark Connector integrates operational data with advanced analytics in this Databricks blog post . Operationalizing Spark with SingleStore This blog post highlights the shared characteristics of SingleStore and Spark and provides real-world use cases for using the two technologies in concert. Presentations From Spark to Ignition: Fueling Your Business on Real-Time Analytics View slides from our Strata+Hadoop World session on the enterprise application of Spark for real-time analytics. Demos MemCity At Spark Summit, we launched a simulation built on the “ the real-time trinity ” of Kafka,  Spark, and SingleStore. We call it MemCity, and use it to model energy consumption across 1.4 million homes to isolate trends in usage by volume, hour of the day, and location in a futuristic city. Read more about this application of Spark technology on the SingleStore blog. ESRI with SingleStore Geospatial Intelligence Working with SingleStore and Spark, Esri, the leading provider of geographic information systems, analyzed data from millions of taxi rides in New York to identify trends to further urban planning. Learn more about the project here: NYC taxi data can drive smarter urban planning . Tools SingleStore Spark Connector The SingleStore Spark Connector provides everything you need to start using Spark and SingleStore together. Download now on GitHub . Get started building your own real-time applications with Spark and SingleStore, download SingleStore Community Edition today. Get The SingleStore Spark Connector Guide The 79 page guide covers how to design, build, and deploy Spark applications using the SingleStore Spark Connector. Inside, you will find code samples to help you get started and performance recommendations for your production-ready Apache Spark and SingleStore implementations. Download Here", "date": "2015-06-19"},
{"website": "Single-Store", "title": "top-5-questions-answered-at-spark-summit", "author": ["Conor Doherty"], "link": "https://www.singlestore.com/blog/top-5-questions-answered-at-spark-summit/", "abstract": "The SingleStore team enjoyed sponsoring and attending Spark Summit last week, where we spoke with hundreds of developers, data scientists, and architects all getting a better handle on modern data processing technologies like Spark and SingleStore. After a couple of days on the expo floor, I noticed several common questions. Below are some of the most frequent questions and answers exchanged in the SingleStore booth. 1 . When should I use SingleStore? SingleStore shines in use cases requiring analytics on a changing data set. The legacy data processing model, which creates separate siloes for transactions and analytics, prevents updated data from propagating to reports and dashboards until the nightly or weekly ETL job begins. Serving analytics from a real-time operational database means reports and dashboards are accurate up to the last event, not last week. That said, SingleStore is a relational database and you can use it to build whatever application you want! In practice, many customers choose SingleStore because it is the only solution able to handle concurrent ingest and query execution for analyzing changing datasets in real-time. 2 . What does SingleStore have to do with Spark? Short answer: you need to persist Spark data somewhere, whether in SingleStore or in another data store. Choosing SingleStore provides several benefits including: In-memory storage and data serving for maximum performance Structured database schema and indexes for fast lookups and query execution A connector that parallelizes data transfer and processing for high throughput Longer answer: There are two main use cases for Spark and SingleStore: Load data through Spark into SingleStore, transforming and enriching data on the fly in SparkIn this scenario, data is structured and ready to be queried as soon as it lands in SingleStore, enabling applications like dashboards and interactive analytics on real-time data. We demonstrated this “real-time pipeline” at Spark Summit, processing and analyzing real-time energy consumption data from tens of millions of devices and appliances. Leverage the Spark DataFrame API for analytics beyond SQL using data from SingleStoreOne of the best features of Spark is the expressive but concise programming interface. In addition to enabling SingleStore users to express iterative computations, it gives them access to the many libraries that run on the Spark execution engine. The SingleStore Spark connector is optimized to push computation into SingleStore to minimize data transfer and to take advantage of the SingleStore optimizer and indexing. 3 . What’s the difference between SingleStore and Spark SQL? There are several differences: Spark is a data processing framework, not a database, and does not natively support persistent storage. SingleStore is a database that stores data in memory and writes logs and full database snapshots to disk for durability. Spark treats datasets (RDDs) as immutable – there is currently no concept of an INSERT, UPDATE, or DELETE. You could express these concepts as a transformation, but this operation returns a new RDD rather than updating the dataset in place. In contrast, SingleStore is an operational database with full transactional semantics. SingleStore supports updatable relational database indexes. The closest analogue in Spark is IndexRDD, which is currently under development, and provides updateable key/value indexes within a single thread. In addition to providing a SQL server, the Spark DataFrame library is a general purpose library for manipulating structured data. 4 . How do SingleStore and Spark interact with one another? The SingleStore Spark Connector is an open source tool available on the SingleStore GitHub page. Under the hood, the connector creates a mapping between SingleStore database partitions and Spark RDD partitions. It also takes advantage of both systems’ distributed architectures to load data in parallel. The connector comes with a small library that includes the SingleStoreRDD class, allowing the user to create an RDD from the result of a SQL query in SingleStore. SingleStoreRDD also comes with a method called saveToSingleStore(), which makes it easy to write data to SingleStore after processing. 5 . Can I have one of those cool t-shirts? (Of course!) What does the design mean? The design is a graphical representation of Hybrid Transactional/Analytical Processing (HTAP), a term coined by Gartner. It refers to the convergence of transactional and analytical processing in a single database, usually for real-time analytics. Circling back to the first question, SingleStore excels at this kind of hybrid workload. In addition to reducing latency and consolidating hardware, HTAP powers tight operational feedback loops that can create opportunities for net new revenue and bottom line cost savings. For more information on HTAP, read the Gartner Market Guide for In-Memory Databases .", "date": "2015-06-23"},
{"website": "Single-Store", "title": "in-memory-computing-summit", "author": ["Kevin White"], "link": "https://www.singlestore.com/blog/in-memory-computing-summit/", "abstract": "The inaugural In-Memory Computing Summit begins next week, June 29-30, and we are thrilled to be a part of it. From speaking sessions on Spark and customer use cases, to games and giveaways, you will not want to miss the action. Visit us at booth #4 to pick up our brand new t-shirt, and learn how in-memory computing can bring peak performance to new or existing applications. SingleStore Speaking Sessions From Spark to Ignition: Fueling Your Business on Real-Time Analytics Monday, June 29 at 10:40am – Eric Frenkiel, SingleStore CEO and Co-Founder Real-time is the next phase of big data. For the modern enterprise, processing and analyzing large volumes of data quickly is integral to success. SingleStore and Spark share design philosophies like memory-optimized data processing and scale-out on commodity hardware that enable enterprises to build real-time time data pipelines with operational data always online. This session shares hands-on ‘how-to’ recipes to build workflows around Apache Spark, with detailed production examples covered. A Hitchhiker’s Guide to the Startup Data Science Platform Monday, June 29 at 4:40pm – David Abercrombie, Principal Data Analytics Engineer at Tapjoy Join David Abercrombie for a session chronicling the growth of the Tapjoy data science team as a lens for examining the infrastructure and technology most critical to their success. This includes implementations and integrations of Hadoop, Spark, NoSQL and SingleStore, which enable Tapjoy to turn sophisticated algorithms into serviceable, data-driven products. Resources to Gear Up for the Event Gartner Market Guide for In-Memory DBMS This complimentary guide from Gartner provides a comprehensive overview of the in-memory database landscape. Download it to learn about three major use cases for in-memory databases and Gartner’s recommendations for evaluation and effective use. Download the Guide The Modern Database Landscape Download this white paper to learn how in-memory computing enables transactions and analytics to be processed in a single system and how to leverage converged processing to save time and cut costs, while providing real-time insights that facilitate data-driven decision-making. Download the White Paper Games and Giveaways Drop by the SingleStore booth #4 to get your free, super-soft t-shirt and play our reaction test game for a chance to win an Estes ProtoX Mini Drone. Hope to see you there!", "date": "2015-06-25"},
{"website": "Single-Store", "title": "tapjoy-is-powering-its-mobile-ad-platform-with-memsql", "author": ["Emily Friedman"], "link": "https://www.singlestore.com/blog/tapjoy-is-powering-its-mobile-ad-platform-with-memsql/", "abstract": "Over the past several months, we worked closely with the Tapjoy data science and engineering team to implement SingleStore as the database to power their Mobile Marketing Automation and Monetization Platform. In order to deliver optimized ads to over 500 million global users and support over one million transactions per minute, Tapjoy needed a database that could enable HTAP, a Gartner term we refer to frequently at SingleStore, which stands for Hybrid Transactional and Analytical Processing. Two Use Cases Real-Time Ad Optimization \\\nWhat does ad optimization look like to a user? Essentially, the Tapjoy Mobile Marketing Platform serves relevant ads from active ad campaigns based on how the ad has performed recently and how well the user fits the target profile of the ad campaign. Users are able to engage with ads and earn reward points, and advertisers generate revenue. For ad optimization to occur in real-time, data needs to be made usable very quickly. The combination of high performance and low latency makes SingleStore a good fit for this need. Across the cluster, Tapjoy is able to process 60,000 queries per second at a response time of less than ten milliseconds. Overlap Analysis \\\nThe second use case for Tapjoy and SingleStore is overlap analysis, or ad targeting based on a user falling into multiple market segments. For example, it is easier to target a user if you can categorize them in several ways, like a sports fan from Texas with kids. SingleStore provides a real-time stream of data, and familiar SQL interface, enabling Tapjoy to analyze user interest data quickly and efficiently to serve the right ad to the right person at the right time. In short, if you know SQL, you can do big data. We detailed more of this use case in a previous blog post: Boost Conversions with Overlap Ad Targeting . Realizing HTAP SingleStore provides several important benefits when it comes to enabling HTAP, beyond the SQL interface. Unlike HBase, Tapjoy’s prior solution, SingleStore allows for updates and deletes. In addition, SingleStore can be easily integrated with standard tools and APIs. Interoperability with hardware is something that we see as a boon to our customers – they can build their own white box solution, leverage commodity hardware and deploy in public or private clouds. Finally, SingleStore embodies HTAP by combining transactions and analytics in one system, which achieves high throughput and low latency. With a slew of competitors in the digital advertising space, having instantaneous access to data up to the last click and having the capability to concurrently analyze that data is a key differentiator for Tapjoy. For more information on Tapjoy and SingleStore at the In-Memory Computing Conference, visit: singlestore.com/events . Read the official announcement here .", "date": "2015-06-29"},
{"website": "Single-Store", "title": "database-speed-test", "author": ["Steven Camina"], "link": "https://www.singlestore.com/blog/database-speed-test/", "abstract": "SingleStore Ops 4.0.31 is now available for download! For this release, we have maintained a focus on providing the smoothest possible on-ramp for developers to quickly get productive with SingleStore. We believe enterprise-class software should be easy to install or use. Here are the new features of the latest SingleStore Ops release, available today. Note that aside from features, this release includes bug fixes, and is a free upgrade for existing SingleStore Ops users. Run a 30-second Database Speed Test A frequent question we receive is “how fast can SingleStore run on my machine(s)?”. SingleStore Ops answers this question by running a basic performance benchmark on your SingleStore cluster. A new Speed Test page in Ops creates a lightweight database table, inserts records into it as quickly as possible, and performs basic analysis on the changing dataset in real-time. Understandably, SingleStore is capable of far more relational operations with SQL but this provides a quick start. Sign up for an Enterprise Edition trial directly in Ops We have received feedback that more users want to try SingleStore Enterprise without needing to go to singlestore.com/download. It is now possible to sign up for an Enterprise trial without leaving the SingleStore Ops User Interface (UI). Enable High Availability with One Click With the latest release of Ops, users can seamlessly upgrade High Availability by clicking one button in the web interface. After the upgrade operation completes, all data in your database cluster will have a replica in another SingleStore node. HTTPS in web UI Accessing SingleStore Ops through an HTTPS:// URL is now possible by adding SSL certificates in SingleStore Ops. Additional Command Line Functionality Backup and Restore Databases SingleStore Ops now has ‘database-backup’ and ‘database-restore’ commands to create database backups and restore existing backups into SingleStore clusters. A UI view for this is planned for an upcoming SingleStore Ops release. Change a SingleStore node root password with Ops SingleStore Ops now allows users to change the root password of a SingleStore node with the ‘memsql-update-root-password’ command. This changes the root password on a SingleStore node and ensures the password is recognized by other SingleStore nodes in the cluster. Download and Try SingleStore Today SingleStore Ops is available for both Community and Enterprise Editions of SingleStore. Download SingleStore Community Edition for free, or try SingleStore now for 30-days by visiting singlestore.com/free .", "date": "2015-06-30"},
{"website": "Single-Store", "title": "how-we-hire-remarkable-engineers", "author": ["Ankur Goyal"], "link": "https://www.singlestore.com/blog/how-we-hire-remarkable-engineers/", "abstract": "To achieve our vision of building the next great database company, we seek the highest caliber engineers. Each team member brings expertise in areas like storage systems, code generation, query optimization, infrastructure testing, and more to SingleStore, with previous roles at companies like Facebook, Microsoft, Oracle, and Google. Our engineering team includes graduates of top university programs, including MIT, Stanford, and Carnegie Mellon University, and highly-rated competition programmers from TopCoder, Codeforces, and ACM-ICPC. This experience and background are just part of what make our team remarkable. As a growing company, we are constantly searching for best-in-class engineering talent. Here are three reasons to join our engineering team: 1 . A concrete roadmap for our company and product We believe that every enterprise should have access to database innovation, like distributed systems and in-memory performance. This mission rallies our team and is a common bond we look for with potential new hires. 2 . Our challenging and stimulating environment No two days are the same, and that is the way we like it. Small teams within the company are assigned projects, like complex programming puzzles, and have the freedom to tackle them creatively. We have team bonding events like Query Hour (trivia nights), Tahoe retreats, happy hours, ping pong competitions and board game nights. All of this complements our drive to tackle big engineering problems together. 3 . Our engineering team members are friends Similar interests and passions connect the team, whose members spend time together outside of the office on the weekends, travel together and more. Plus, our engineers are often our best recruiters, bringing talented friends from college or previous jobs to work at SingleStore. In their own words, here are some of the reasons why our engineers joined SingleStore: “The engineering team is brilliant. Whenever I have a question of any type, there is someone there who can answer it” – Pieguy, Senior Software Engineer “I immediately saw the spirit of collaboration that I felt in college that I was a little bit afraid of losing after university” – Michael Andrews, Software Engineer “It’s all about the impact here, what can we do to deliver value to our customers, and how can we do it quickly” – Doug Doan, Director of Infrastructure Quality Engineering If you want to join our team of inventors and hard-problem solvers, visit our careers page .", "date": "2015-07-08"},
{"website": "Single-Store", "title": "how-to-make-a-believable-benchmark", "author": ["Carlos Bueno"], "link": "https://www.singlestore.com/blog/how-to-make-a-believable-benchmark/", "abstract": "A benchmark asks a specific question, makes a guess about the expected result, and confirms or denies it with experiment. If it compares anything, it compares like to like and discloses enough details so that others can plausibly repeat it. If your benchmark does not do all of these things, it is not a benchmark. Today’s question comes from one of our engineers, who was talking to a customer about new features in SingleStore DB 4. We added support for SSL network encryption between clients and the cluster, and also between nodes in the cluster. The customer wanted to know how performance would be impacted. The original question was about replication, but in SingleStore, replication happens over the same protocol as everything else. So we’ll generalize it a little to “What is the performance hit when SSL is turned on?” and then make it more specific, i.e. “what is the difference in sustained query throughput and latency between SSL on and SSL off for a given workload?” Meet Herp and Derp These are servers that used to be part of our continuous testing suite . Now they sit around doing benchmarks and party tricks. They are both Dell T610 mid-tower servers with two sockets, eight 2.5 GHz CPU cores, and 48GB of RAM. Not shown is burp, which is a 4-core box I scavenged out of a mess of parts at the local computer shop. We will set up the cluster with SingleStore DB 4, one aggregator / traffic generator (burp), and two leaves (herp and derp) and high availability replication turned on between the leaves. I’m being a little cavalier about mixing the server and client workload, but it is unlikely that the bottleneck will be on the aggregator. The dataset is a table with 200 million records. Each record has a geospatial point , a few numbers, and an index. They will be loaded with 8 write threads in batches of 10,000 each. Then we will start 16 read threads, which will continuously fire queries that select all points that lay inside randomly-generated geospatial polygons drawn on the surface of the Earth. The polygons will be sized such that about 80 points are retrieved from the index, filtering down to 40-50 points returned to the aggregator. The read query load will be increased until the leaves hit 95% CPU utilization. Then we will measure sustained throughput and median latency. Finally, we will do it all over again with SSL and measure the difference. Loading a batch of records into SingleStore involves three kinds of network connections: client to aggregator, aggregator to leaves, and (asynchronous) replication between leaves for high availability. All of these happen over the same protocol; replication clients simply connect like regular SQL clients then run a command that starts the firehose of data. The read queries require two network hops: client to aggregator, and aggregator to the leaves. When SSL is on, we should expect higher latencies and higher demand for CPU. We should also see a reduction in overall throughput for the same number of threads. That is the bet, anyway. Let’s watch what happens. Non-SSL Benchmark Loading data without SSL moved at a good clip; about 550,000 to 600,000 rows per second. As the table grew in size, this throughput dropped off gently. SingleStore in-memory tables are based on skiplists , which have log(N) insertion behavior. As N grows larger and larger, a little more work is needed per insert. Graphing insert volume over long periods of time would show a pretty catenary curve. CPU load was at decent levels during loading, but not at the saturation point. $ time ./benchmark write-spatial\n[...snip...]\n200000000 total,  488849 per sec\n\n**real    6m49.066s**\nuser    14m39.200s\nsys     0m21.392s (NB: though the Ops screenshots imply “600GB of Memory”, that is physical RAM plus swap.) Reading was also straightforward. 16 threads was about right to pin the CPUs on the leaves, but left plenty of headroom on the aggregator/client machine. Read throughput on these geospatial queries was a steady 76,000 rows per second, or about 1,700 queries per second. One of the nice things about an in-memory database is no “warm up” or caching time on benchmarks; you reach cruising altitude immediately. The median latency was 7.7 milliseconds, p95 was 14 msec. Not bad for a pile of junk computers. SSL Benchmark I dropped the database, then configured SSL certificates so that all intra-cluster communication was secured, and restarted. Just to check whether SSL was on, I connected with a MySQL client and the certificate I’d generated. $ mysql -uroot -h burp --ssl-ca=ca-cert.pem -e \"show variables like '%ssl%'\"\n+---------------+--------------------------+\n| Variable_name | Value                    |\n+---------------+--------------------------+\n| have_openssl  | ON                       |\n| have_ssl      | ON                       |\n| ssl_ca        | ../certs/ca-cert.pem     |\n| ssl_capath    |                          |\n| ssl_cert      | ../certs/server-cert.pem |\n| ssl_cipher    |                          |\n| ssl_key       | ../certs/server-key.pem  |\n+---------------+--------------------------+ Now, on to the benchmark. I modified the benchmark script, which uses the singlestore.common Python convenience library , to connect via SSL. pool = connection_pool.ConnectionPool()\ndb = pool.connect('127.0.0.1', '3306', 'root', '', '', {'ssl': '../certs/ca-cert.pem'}) And off we go! Writes initially clocked in at 425,000 rows per second, trending down to 350K or so by the end. The aggregator’s CPU was higher with SSL on, since it was handling three separate encrypted links (client to aggregator, aggregator to client, aggregator to leaves). Note that in both write tests, we did not achieve CPU saturation on the leaves, so these are lower bounds. $ time ./benchmark write-spatial\n[...snip...]\n200000000 total,  387458 per sec\n\n**real    8m36.118s**\nuser    14m29.685s\nsys     0m14.852s The overall insert time for 200 million rows was 8:36, compared to 6:49 for the non-SSL config. Overall, 25% less write throughput. Now, on to reads. Read query throughput clocked in at 64,000 rows and just under 1,500 queries per second under SSL. Compared to the 76K from the non-SSL config, that’s a 16% drop in read throughput. Median latency was 10 msec with a p95 of 18 msec. Both reads tests reached full CPU saturation. And that is a decent benchmark. We asked a specific question and answered it with an experiment, comparing the same metrics against one and only one difference. Even though I lost the bet that the aggregator + client load would not bottleneck burp, the skew was not too large. Lastly, there is enough detail that you can repeat the test and be reasonably sure you will get similar results. But do not generalize everything you read. You cannot flatly say that “SSL is a 16% latency hit on SingleStore”. Larger polygons would probably give different results. Other kinds of queries, like distributed joins , would perform extra communication between leaves. We can make any number of guesses at what the result may be, but only a good benchmark can answer it. Try this at home The script I used is up on Github (it is ugly; be kind). Below is the schema, some sample data, and queries. CREATE TABLE terrain_points (\n  location geographypoint DEFAULT 'Point(0 0)',\n  elevation int(10) unsigned NOT NULL,\n  ent_id int(10) unsigned NOT NULL,\n  time_sec int(10) unsigned NOT NULL,\n  SHARD KEY location (location, ent_id, time_sec)\n); memsql> select * from perf.terrain_points limit 5;\n+----------------------------------+-----------+----------+------------+\n| location                         | elevation | ent_id   | time_sec   |\n+----------------------------------+-----------+----------+------------+\n| POINT(-89.00991728 7.95282675)   |     28822 | 20333901 | 1432681250 |\n| POINT(177.53195551 -32.35287545) |     64465 | 25248309 | 1432681250 |\n| POINT(-150.17150895 10.02000130) |     43688 | 66406185 | 1432681250 |\n| POINT(77.94888837 19.91037133)   |     57739 | 26693242 | 1432681250 |\n| POINT(-125.70522605 45.54260644) |     41195 | 24162850 | 1432681250 |\n+----------------------------------+-----------+----------+------------+\n5 rows in set (0.02 sec) memsql> SELECT * FROM perf.terrain_points with (index=location, resolution=6) WHERE geography_intersects(location, 'POLYGON((166.07059780 -13.90684205, 166.06868866 -13.90694224, 166.02333020 -13.93129089, 166.01865319 -13.99425334, 166.02876188 -14.00779541, 166.03506682 -14.01331439, 166.03979604 -14.01657349, 166.10317282 -14.01829710, 166.10531622 -14.01706579, 166.11815640 -14.00676434, 166.11893834 -14.00592391, 166.13045244 -13.98737615, 166.07059780 -13.90684205))');\n+----------------------------------+-----------+----------+------------+\n| location                         | elevation | ent_id   | time_sec   |\n+----------------------------------+-----------+----------+------------+\n| POINT(166.09984350 -14.00177767) |     36606 | 22886130 | 1432768116 |\n| POINT(166.06090626 -13.95221591) |     70244 | 19588890 | 1432767950 |\n| POINT(166.11221990 -13.97041329) |     32433 | 70087923 | 1432768053 |\n\n   [...snip...]\n\n| POINT(166.05442264 -13.96318868) |     84910 | 74432027 | 1432767899 |\n+----------------------------------+-----------+----------+------------+\n38 rows in set (0.01 sec) You can download SingleStore Community Edition for free and try this for yourself.", "date": "2015-07-13"},
{"website": "Single-Store", "title": "first-memsql-community-meetup-in-san-francisco", "author": ["Emily Friedman"], "link": "https://www.singlestore.com/blog/first-memsql-community-meetup-in-san-francisco/", "abstract": "Join us this Wednesday, July 15th at 6pm for the first official SingleStore Meetup ! Our headquarters are located in the South of Market (SOMA) neighborhood of San Francisco, a veritable hotbed of technology infrastructure startups. We want to foster a community of collaboration and create a space for our peers to both learn and share. This meetup represents a gathering of database architects, programmers, cloud enthusiasts and other technology professionals in the Bay Area. What’s in store for our meetup: 6:00-7:00pm – Happy Hour with heavy hors d’oeuvres 7:00-7:30pm – Carlos to present on Community Edition and performance on Amazon’s new M4 instance 7:30-8:00pm – Q&A and more Happy Hour Carlos Bueno, Principal Product Manager at SingleStore, will demonstrate how to spin up your very own cluster with SingleStore Community Edition, free forever with unlimited scale and capacity. He will also demo the recently launched database speed test to evaluate cluster performance, while exploring deployments on M4 instances, the latest offering from Amazon Web Services. Feel free to bring your laptop and participate in the hands-on portion of the event. Save any questions for our Q&A session directly following the presentation. RSVP: http://www.meetup.com/SingleStore . We look forward to seeing you there!", "date": "2015-07-14"},
{"website": "Single-Store", "title": "data-persistence-dilemmas-facing-cios", "author": ["Freja Mickos"], "link": "https://www.singlestore.com/blog/data-persistence-dilemmas-facing-cios/", "abstract": "At SingleStore, we see an in-memory, distributed approach to big data as the path forward to cost-effective deployments. Recently, Gartner released a report titled “ Five Data Persistence Dilemmas That Will Keep CIOs Up at Night ”, which reinforces this approach to data management. The report outlines three key impacts of utilizing new technologies across HTAP, or Hybrid Transaction/Analytical Processing, for in-memory processing, the compromises of NoSQL DBMSs, and the growing importance of agile cloud computing approaches. Download a complimentary copy of the Gartner Report: Five Data Persistence Dilemmas That Will Keep CIOs Up at Night Key Impacts from the Gartner Report The convergence of transaction and analytic database systems in hybrid transaction/analytical processing (HTAP) systems that use in-memory processing reduces the need for separate dedicated environments and shortens the time to value for new data, but it requires IT Leaders to make process compromises and changes to applications to maximize ROI. NoSQL DBMSs compromise a priori data models and strong levels of consistency to offer IT leaders high-throughput operations and scale-out architectures. Agile deployment approaches like cloud computing will present new opportunities that IT leaders and line-of-business heads must seize. The CIO Dilemmas The five dilemmas covered in the report generate a number of questions that CIOs must ask with any new database technology. We receive these questions daily from customers seeking to maximize opportunities with HTAP, scalable SQL databases, and flexible cloud deployments. The Single-Database Dilemma For decades, data processing has been split into databases for Online Transaction Processing and data warehouses for Online Analytical Processing. HTAP, largely enabled by in-memory computing, collapses the single database model and allows for the definition of new classes of applications, like those that fuse real-time and historical analysis. The HTAP Adoption Dilemma Moving from split OLTP and OLAP to converged HTAP requires thorough cost and capacity planning that does not happen overnight. Fully realizing the benefits of HTAP means transactions and analytics are easily integrated with existing or net new applications. HTAP results in less precomputation and more real-time queries. The Consistency Dilemma NoSQL databases gave up traditional consistency, and abandoned SQL, to achieve scalability. Fortunately, you can have scalability, performance, simplicity and SQL with an in-memory database like SingleStore. The Schema Dilemma You can define your structure up front, or define it later. However, multi-model databases like SingleStore support structured SQL and semi-structured data types like JSON, so you get the best of both worlds. The Cloud Dilemma While some database offerings restrict deployment choice, SingleStore can be deployed on-premises or in the cloud. Try HTAP, Scalable SQL, and Cloud Databases Today If you would like a hands-on look at HTAP, scalable SQL, and cloud deployments with in-memory databases, try SingleStore Community Editio n, available for free with unlimited capacity and scale. If you would like support or high availability features, try SingleStore Enterprise Edition free for 30 days . Required Disclaimer: © 2015 Gartner, Inc. and/or its affiliates. All rights reserved. Gartner does not endorse any vendor, product or service depicted in its research publications, and does not advise technology users to select only those vendors with the highest ratings or other designation. Gartner research publications consist of the opinions of Gartner’s research organization and should not be construed as statements of fact. Gartner disclaims all warranties, expressed or implied, with respect to this research, including any warranties of merchantability or fitness for a particular purpose.", "date": "2015-07-15"},
{"website": "Single-Store", "title": "data-science-summit", "author": ["Kevin White"], "link": "https://www.singlestore.com/blog/data-science-summit/", "abstract": "We are excited to exhibit at the Data Science Summit on Monday, July 20, in San Francisco. Stop by the SingleStore booth to learn about our MemCity demo , pickup a cool t-shirt, and play our reaction test game to win an Estes ProtoX Mini Drone. About the Data Science Summit The Data Science Summit is a non-profit event that connects researchers and data scientists from academia and industry to discuss the art of data science, machine learning, and predictive applications. What We Have in Store for the Event Visit the SingleStore booth to learn about: Our latest demo, MemCity , that leverages Kafka, Spark, and SingleStore to process and analyze data from various energy devices found in homes, all measured in real time. How in-memory computing can combat latencies in the enterprise, such as batch loading and query execution latency. How SingleStore enables data analyst to get real-time insights using SQL. SingleStore Community Edition – a free downloadable version of SingleStore that comes without limitations on capacity, cluster size, or time. Recommended Sessions How Comcast uses Data Science to Improve the Customer Experience Monday, July 20, 10:50am – Salon 9 Comcast Labs manager, Dr. Jan Neumann, will discuss how Comcast improves the visible parts of the user experience by powering the personalized content discovery algorithms and voice interface on the X1 set top boxes. Bonus: Learn how the Comcast VIPER team is using SingleStore for real-time stream processing . What’s New in the Berkeley Data Analytics Stack Monday, July 20, 1:20pm – Salon 9 In this talk, Prof. Mike Franklin of the Berkeley AMPLab will give a quick overview of BDAS (pronounced “badass”) and then describe several newer BDAS components including: the KeystoneML machine learning pipeline framework, the Velox model serving layer, and the SampleClean/AMPCrowd components for human-in-the-loop data cleaning and machine learning.", "date": "2015-07-17"},
{"website": "Single-Store", "title": "json-column-type", "author": ["Noah Zucker"], "link": "https://www.singlestore.com/blog/json-column-type/", "abstract": "This post originally appeared on the Novus Tech Blog More and more, companies are migrating from NoSQL databases back to relational, ACID-compliant databases that also offer high availability and horizontal scalability – aka “NewSQL.” Novus Partners is no exception, having recently completed our migration from MongoDB to SingleStore. Of course, NoSQL developers often come to depend on rapid development cycle and “schema-less” data models, which can make the transition back to relational – including the prospect of devising a relational model for your schemaless data – daunting. It turns out, however, that SingleStore offers a feature that enales you to keep many of those benefits and ease the NoSQL-to-NewSQL transition: the JSON column type. In this article, we’ll get up and running quickly with SingleStore, and then immediately learn about its native JSON support and what it can do. Getting Started with SingleStore Although SingleStore is a proprietary technology (binary download only), they recently released a free Community Edition that is perfect for developer-testing. All you need are a 64-bit Linux environment and the MySQL client (SingleStore made a strategic decision to implement the MySQL client protocol – “bug-for-bug”). After downloading and installing the Community Edition , and the MySQL Client, you will want to alias memsql to the MySQL client command as below (I have this alias in my ~/.local.bash: $ alias memsql\nalias memsql='mysql -u root -h 127.0.0.1 -P 3306 --prompt=\"memsql> \"'\n\n$ memsql\nWelcome to the MySQL monitor.  Commands end with ; or \\g.\nYour MySQL connection id is 15\nServer version: 5.5.8 SingleStore source distribution (compatible; MySQL Enterprise & MySQL Commercial)\nCopyright (c) 2000, 2014, Oracle and/or its affiliates. All rights reserved.\nOracle is a registered trademark of Oracle Corporation and/or its\naffiliates. Other names may be trademarks of their respective\nowners.\nType 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. If you already know MySQL, you already know most of SingleStore commands and syntax. SingleStore adds some proprietary commands and syntax, mostly related to its replication and sharding functionality, which we won’t discuss here. If you want to learn more, you can find all their documentation online . Let’s start out by inspecting the default databases, and then define our own “test” database where we can explore SingleStore’s JSON support: memsql? show databases;\n+--------------------+\n| Database           |\n+--------------------+\n| information_schema |\n| memsql             |\n| sharding           |\n+--------------------+\n3 rows in set (0.00 sec)\n\nmemsql> create database test;\nQuery OK, 1 row affected (2.21 sec)\n\nmemsql> use test;\nDatabase changed Prototyping a Schema Perhaps in our existing NoSQL database, we had a document collection named tasks that contained, among other things, unstructured data related to our distributed compute application. Our developers added and removed fields from their Task objects as needed for different compute jobs, which was great from a development perspective because it meant rapid development cycle without the need for frequent database schema changes and migrations. Fortunately, we don’t have to leave that agility behind when we transition from NoSQL back to SingleStore’s relational model. The simplest version of our new tasks table has two columns: a bigint primary key and the column for JSON data, which has the type… JSON memsql> create table tasks ( task_id bigint not null primary key auto_increment, task json not null );\nQuery OK, 0 rows affected (15.53 sec)\n\nmemsql> describe tasks;\n+---------+------------+------+------+---------+----------------+\n| Field   | Type       | Null | Key  | Default | Extra          |\n+---------+------------+------+------+---------+----------------+\n| task_id | bigint(20) | NO   | PRI  | NULL    | auto_increment |\n| task    | JSON       | NO   |      | NULL    |                |\n+---------+------------+------+------+---------+----------------+\n2 rows in set (0.00 sec) Now, let’s insert some data into this table. The task documents themselves are just JSON literals inside single-quotes, like any other data type: memsql> insert into tasks (task) values ('{\"user\" : \"nzucker\", \"uid\" : {\"clientId\" : 1, \"which\" : \"P\", \"id\" : 205}}');\nQuery OK, 1 row affected (0.00 sec)\nmemsql> insert into tasks (task) values ('{\"user\" : \"nzucker\", \"uid\" : {\"clientId\" : 7, \"which\" : \"P\", \"id\" : 1009}}');\nQuery OK, 1 row affected (0.00 sec)\nmemsql> insert into tasks (task) values ('{\"user\" : \"bqunibi\", \"uid\" : {\"clientId\" : 9, \"which\" : \"P\", \"id\" : 327 }}');\nQuery OK, 1 row affected (0.00 sec)\n\nmemsql> select * from tasks;\n+---------+---------------------------------------------------------------+\n| task_id | task                                                          |\n+---------+---------------------------------------------------------------+\n|       2 | {\"uid\":{\"clientId\":1,\"id\":205,\"which\":\"P\"},\"user\":\"nzucker\"}  |\n|       4 | {\"uid\":{\"clientId\":9,\"id\":327,\"which\":\"P\"},\"user\":\"bqunibi\"}  |\n|       3 | {\"uid\":{\"clientId\":7,\"id\":1009,\"which\":\"P\"},\"user\":\"nzucker\"} |\n+---------+---------------------------------------------------------------+\n3 rows in set (0.00 sec) An important note: SingleStore will not let you insert invalid JSON documents. Attempting to do so yields an error: memsql> insert into tasks (task) values ('{\"user\" : ');\nERROR 1844 (HY000): Leaf Error (10.20.79.111:3307): Invalid JSON value for column 'task' JSON Field Predicates Suppose we want to select rows based on a specific JSON document field. Even though the SingleStore JSON column type is something like a LONGTEXT column in terms of data format and limitations, the database query engine understands the JSON specification and how to navigate the document tree. For example, we can select all the tasks that I previously inserted having user of ‘nzucker’ memsql> select * from tasks t where t.task::$user = 'nzucker';\n+---------+---------------------------------------------------------------+\n| task_id | task                                                          |\n+---------+---------------------------------------------------------------+\n|       2 | {\"uid\":{\"clientId\":1,\"id\":205,\"which\":\"P\"},\"user\":\"nzucker\"}  |\n|       3 | {\"uid\":{\"clientId\":7,\"id\":1009,\"which\":\"P\"},\"user\":\"nzucker\"} |\n+---------+---------------------------------------------------------------+\n2 rows in set (2.71 sec) The clause t.task::$user navigates to the user field of the JSON document, with the $ prefix ensuring that the value is evaluated as a STRING data type (it is actually short-hand for the JSON_EXTRACT_STRING function). Naturally, SingleStore supports predicates that use nested JSON fields. For example, we can also query by the uid.clientId field: memsql> select * from tasks t where t.task::uid::%clientId = 7;\n+---------+---------------------------------------------------------------+\n| task_id | task                                                          |\n+---------+---------------------------------------------------------------+\n|       3 | {\"uid\":{\"clientId\":7,\"id\":1009,\"which\":\"P\"},\"user\":\"nzucker\"} |\n+---------+---------------------------------------------------------------+\n1 row in set (0.00 sec) The % prefix on ::%clientId ensures that the field is interpreted as a double (the only numeric data type supported by the JSON standard), which is important if you are doing numeric comparisons in your queries. memsql> select * from tasks t where t.task::uid::%clientId < 9;\n+---------+---------------------------------------------------------------+\n| task_id | task                                                          |\n+---------+---------------------------------------------------------------+\n|       3 | {\"uid\":{\"clientId\":7,\"id\":1009,\"which\":\"P\"},\"user\":\"nzucker\"} |\n|       2 | {\"uid\":{\"clientId\":1,\"id\":205,\"which\":\"P\"},\"user\":\"nzucker\"}  |\n+---------+---------------------------------------------------------------+\n2 rows in set (0.00 sec) You can also select “sub-documents” from JSON columns using a similar syntax: memsql> select task::uid from tasks;\n+--------------------------------------+\n| task::uid                            |\n+--------------------------------------+\n| {\"clientId\":7,\"id\":1009,\"which\":\"P\"} |\n| {\"clientId\":1,\"id\":205,\"which\":\"P\"}  |\n| {\"clientId\":9,\"id\":327,\"which\":\"P\"}  |\n+--------------------------------------+\n3 rows in set (2.73 sec) Because we’re extracting JSON documents from our JSON column no $ or % prefix is required in the task::uid clause. Persisted Columns SingleStore also supports persisted columns (also known as “computed columns”) extracted from JSON documents fields. This feature is very convenient if you find yourself repeatedly querying the same deeply-nested JSON document fields. memsql> alter table tasks add column client_id as task::uid::clientId persisted bigint;\nQuery OK, 0 rows affected (8.90 sec)\nRecords: 0  Duplicates: 0  Warnings: 0\n\nmemsql> select * from tasks;\n+---------+---------------------------------------------------------------+-----------+\n| task_id | task                                                          | client_id |\n+---------+---------------------------------------------------------------+-----------+\n|       3 | {\"uid\":{\"clientId\":7,\"id\":1009,\"which\":\"P\"},\"user\":\"nzucker\"} |         7 |\n|       2 | {\"uid\":{\"clientId\":1,\"id\":205,\"which\":\"P\"},\"user\":\"nzucker\"}  |         1 |\n|       4 | {\"uid\":{\"clientId\":9,\"id\":327,\"which\":\"P\"},\"user\":\"bqunibi\"}  |         9 |\n+---------+---------------------------------------------------------------+-----------+\n3 rows in set (2.75 sec) Here we persisted the client_id field, extracted from the task object. Note that if our task documents have a suitable primary key field (say, a field named _ id) we could extract that field to populate task_id field. Now let’s select using this persisted column: memsql> select count(*) from tasks where client_id = 9;\n+----------+\n| count(*) |\n+----------+\n|        1 |\n+----------+\n1 row in set (2.27 sec) If you plan your persisted columns in advance, you will make life easier for developers or analysts who may not be familiar with JSON structure. Also, you can create indexes using these JSON-derived persisted columns, which obviously has a huge benefit. Updating JSON Document Fields “In-Place” If you have used JSON persistence from an object-oriented language, you might have written code like the following: 1 . Fetch the entire JSON document out of the store. 2 . Deserialize the JSON document into an object. 3 . Update a single field on the object. 4 . Serialize the entire object back to the store as JSON. That’s quite a bit of data transfer just to manipulate a single field. Well, since we can select database rows by JSON fields, why not update individual fields of JSON documents as well? This too is possible: memsql> update tasks set task::uid::$which = 'F' where task_id = 3;\nQuery OK, 1 row affected (0.00 sec)\n\nmemsql> select * from tasks where task_id = 3;\n+---------+---------------------------------------------------------------+-----------+\n| task_id | task                                                          | client_id |\n+---------+---------------------------------------------------------------+-----------+\n|       3 | {\"uid\":{\"clientId\":7,\"id\":1009,\"which\":\"F\"},\"user\":\"nzucker\"} |         7 |\n+---------+---------------------------------------------------------------+-----------+\n1 row in set (0.00 sec) It’s worth mentioning that SingleStore doesn’t support JSON field-level validation, so “in-place” updates such as this run the risk generating data that violates your domain model. For example, if the task::uid::which field is required, but you set task::uid::$which = NULL, your application may encounter errors due to the missing field. So, use this feature with caution (and perhaps a robust set of integration tests). Manipulating JSON Arrays Another great feature of SingleStore’s JSON support is the ability to manipulate JSON arrays in-place, using the JSON_ARRAY_PUSH _ expression. Let’s continue with our “task” documents example by defining their domain model in Scala as follows: case class UID(clientId: Int, which: String, id: Int)\ncase class Task(user: String, uid: UID, history: List[String]) As with any other JSON field, SingleStore array manipulation functions enable us to add entries to a JSON array “in-place.” Using our example data set, we first update one of our documents to have a field called “history,” initialized with an empty JSON array: memsql> update tasks set task::history = '[]' where task_id = 2;\nQuery OK, 1 row affected (2.96 sec) Then can then we insert into the array and observe the results: memsql> update tasks set task::history = JSON_ARRAY_PUSH_STRING(task::history, \"New\") where task_id = 2;\nQuery OK, 1 row affected (0.00 sec)\n\nmemsql> select * from tasks;\n+---------+--------------------------------------------------------------------------------+-----------+\n| task_id | task                                                                           | client_id |\n+---------+--------------------------------------------------------------------------------+-----------+\n|       3 | {\"uid\":{\"clientId\":7,\"id\":1009,\"which\":\"F\"},\"user\":\"nzucker\"}                  |         7 |\n|       2 | {\"history\":[\"New\"],\"uid\":{\"clientId\":1,\"id\":205,\"which\":\"P\"},\"user\":\"nzucker\"} |         1 |\n|       4 | {\"uid\":{\"clientId\":9,\"id\":327,\"which\":\"P\"},\"user\":\"bqunibi\"}                   |         9 |\n+---------+--------------------------------------------------------------------------------+-----------+\n3 rows in set (0.00 sec)\n\nmemsql> update tasks set task::history = JSON_ARRAY_PUSH_STRING(task::history, \"InProgress\") where task_id = 2;\nQuery OK, 1 row affected (0.00 sec)\n\nmemsql> select * from tasks where task_id = 2;\n+---------+---------------------------------------------------------------------------------------------+-----------+\n| task_id | task                                                                                        | client_id |\n+---------+---------------------------------------------------------------------------------------------+-----------+\n|       2 | {\"history\":[\"New\",\"InProgress\"],\"uid\":{\"clientId\":1,\"id\":205,\"which\":\"P\"},\"user\":\"nzucker\"} |         1 |\n+---------+---------------------------------------------------------------------------------------------+-----------+\n1 row in set (0.00 sec) Finding the Last Element of the Array Perhaps we want to find the most recent update in our task document’s history array. One (inefficient) approach is to extract the entire array, and serialize it into an array object, and find the last element: memsql> select task::history from tasks where task_id = 2;\n+----------------------+\n| task::history        |\n+----------------------+\n| [\"New\",\"InProgress\"] |\n+----------------------+\n1 row in set (2.18 sec) This certainly works in a pinch, but a better solution is to obtain the last element – representing the most recent status – directly in the SQL. Thank you to Justin Fu at SingleStore Support, who provided this solution: memsql> select task_id, JSON_EXTRACT_JSON(task::history, JSON_LENGTH(task::history) - 1) as latest_status\n    -> from tasks\n    -> where task_id = 2;\n+---------+---------------+\n| task_id | latest_status |\n+---------+---------------+\n|       2 | \"InProgress\"  |\n+---------+---------------+\n1 row in set (1.24 sec) Performance Considerations A quick word about performance: JSON is a plain-text-based data format and therefore will always be slower than an optimized, binary format (see Thrift and ProtocolBuffers ). At Novus, our migration path was to continue using Salat for Scala case class-to-JSON serialization, which in turn uses json4s for JSON AST operations. For most cases, this was painless and Just Worked. However, in some cases, we encountered human-perceptible performance degradation after moving from MongoDB to SingleStore. Typically, this occurred when attempting to deserialize thousands of JSON documents into Scala objects while processing a SELECT result. Although many developers know Mongo as “just JavaScript,” it’s actually not. The storage and wire format are the carefully planned and optimized BSON format. Given our application was now transferring and parsing JSON text rather than BSON, this slow-down was completely understandable. So, the trade off was to sacrifice human-readability for performance, reverting to a binary format for the performance-sensitive functionality. We successfully used Twitter Chill for this purpose, storing the objects in a BLOB database column. Another option is to bite the bullet and devise a relational model for your object model, particularly if the design is stable. Either way, be sure to focus on functionality and domain modeling first, before turning to performance optimizations. Thank you to Carlos Bueno ( @archivd ) and the rest of the SingleStore team for feedback on earlier drafts of this article.", "date": "2015-07-20"},
{"website": "Single-Store", "title": "deploy-memsql-with-mesosphere", "author": ["Steven Camina"], "link": "https://www.singlestore.com/blog/deploy-memsql-with-mesosphere/", "abstract": "SingleStore is now certified for deployment on the Mesosphere DCOS (Datacenter Operating System). With a few simple commands, users can launch a SingleStore cluster on DCOS. The Mesosphere DCOS (Datacenter Operating System) is a commercially supported product built on top of Apache Mesos that serves as a datacenter abstraction layer. DCOS deploys distributed applications with a few simple command-line interface (CLI) steps, and handles application provisioning, resource management, and fault tolerance seamlessly. It also centralizes management of a distributed system in a single interface for easy deployment. SingleStore is a relational, in-memory database optimized for multi-machine deployment. Its distributed nature makes it a good match for DCOS. The package needed to deploy SingleStore on DCOS can be found in the Mesosphere Universe repository , which allows all DCOS users to install SingleStore anytime simply by running dcos package install memsql from a DCOS console. SingleStore leverages the Marathon scheduler to pull a SingleStore Docker image to deploy the SingleStore software, which is tightly integrated with DCOS. SingleStore supports a variety of deployment topologies, and the Mesosphere DCOS adds another option to this growing list. SingleStore can be deployed on: bare metal machines in your datacenter virtual machines in your datacenter Docker containers Amazon EC2, Microsoft Azure and other public cloud platform providers Mesosphere DCOS Additional Links Read the full press release here: Mesosphere Launches Developer Program, VIP Partner Program and SDK for Building Distributed Datacenter-Scale Services on Mesosphere DCOS Try SingleStore today! Run `dcos package install memsql` on your DCOS system, or download the free SingleStore Community Edition with unlimited scale and capacity.", "date": "2015-07-22"},
{"website": "Single-Store", "title": "scaling-a-sales-team-during-hypergrowth-meetup-with-former-evp-of-sales-at-box", "author": ["Emily Friedman"], "link": "https://www.singlestore.com/blog/scaling-a-sales-team-during-hypergrowth-meetup-with-former-evp-of-sales-at-box/", "abstract": "The second official SingleStore Meetup takes place next Tuesday, July 28th at 6pm ! This time, we will focus on the art and science of building a successful sales team through hypergrowth. Come join us for pizza and libations, as well as an opportunity to glean insights about scaling a sales team at a startup. Our featured guest next week is Jim Herbold, the first sales hire at Box and former Executive Vice President of Global Sales. Leveraging homegrown strategies, Jim grew the Box sales team to 400 people and took revenue from $1 million to $174 million. Jim will share go-to-market strategies, the importance of self-disruption, and sales-driven processes that are key to sales success during hypergrowth. We are thrilled to have Jim join us for a valuable learning experience. The Meetup Agenda: 6:00-7:00pm Happy Hour with heavy hors d’oeuvres 6:30-7:00pm Optional contest: Database Speed Test – Win a Drone! 7:00-7:30pm Main Presentation: Jim Herbold on Scaling Hypergrowth Sales 7:30-8:00pm Q&A, continued Happy Hour Feel free to bring your laptop and participate in the optional Database Speed Test for your chance to win an Estes ProtoX drone!  Save any questions for our Q&A session directly following the presentation. RSVP: http://www.meetup.com/SingleStore . Located in the heart of San Francisco’s bustling South of Market (SOMA) neighborhood, SingleStore Meetups are a fun way to meet and interact with neighboring technology startups and enthusiasts in Silicon Valley.", "date": "2015-07-24"},
{"website": "Single-Store", "title": "adoption-of-in-memory-computing", "author": ["Gary Orenstein"], "link": "https://www.singlestore.com/blog/adoption-of-in-memory-computing/", "abstract": "There is no question that data is infiltrating our world. Recently 451 Research predicted that the Total Data Market is expected to double in size from $60 billion in 2014 to $115 billion in 2019. IDC suggested that Internet of Things (IoT) spending will reach $1.7 trillion in 2020. and noted, “the real opportunity remains in the enterprise…” And as stated in a recent Gartner blog post , while the three leading independent Hadoop distribution players measure their revenue in 10s of millions, commercial database vendors like Oracle, Microsoft, IBM, SAP and Teradata measure revenues in billions or 10s of billions of dollars in a $33 billion dollar market. The data market is hot, and in-memory delivers the capabilities companies need to keep up. In the report Market Guide for In-Memory DMBS, published December 2014, analysts Roxane Edjlali, Ehtisham Zaidi, and Donald Feinberg outline the growing importance of in-memory. Four Reasons for the popularity and adoption of In-Memory Declining costs in memory and infrastructure Server main memory (now called server-class memory) is expanding to sizes as high as 32TB and 64TB at an increasingly lower cost, thereby enabling new in-memory technologies such as IMDBMSs, because many applications’ working sets fit entirely into this larger memory. This rapid decline in the infrastructure and memory costs results in significantly better price/performance, making IMDBMS technology very attractive to organizations. Growing importance of high-performance use cases The growing number of high performance, response-time critical and low-latency use cases (such as real-time repricing, power grid rerouting, logistics optimization), which are fast becoming vital for better business insight, require faster database querying, concurrency of access and faster transactional and analytical processing. IMDBMSs provide a potential solution to all these challenging use cases, thereby accelerating its adoption. Improved ROI promise A cluster of small servers running an IMDBMS can support most or all of an organization’s applications, drastically reducing operating costs for cooling, power, floor space and resources for support and maintenance. This will drive a lower total cost of ownership (TCO) over a three- to five-year period and offset the higher total cost of acquisition from more expensive servers. Improved data persistence options Most IMDBMSs now offer features for supporting “data persistence,” that is the ability to survive disruption of their hardware or software environment. Techniques like high availability/disaster recovery (HA/DR) provide durability by replicating data changes from a source database, called the primary database, to a target database, called the standby database. This means that organizations can continue to leverage IMDBMS-enabled analytical and transactional use cases without worrying about prolonged system downtime or losing their critical data to power failures. From Market Guide for In-Memory DBMS, Roxane Edjlali, Ehtisham Zaidi, Donald Feinberg, 9 December 2014 Download the Complete Report If you’d like to read more on the state of the In-Memory DBMS market, download the entire report here .", "date": "2015-07-27"},
{"website": "Single-Store", "title": "whats-hot-and-whats-not-in-working-with-data", "author": ["Lesia Myroshnichenko"], "link": "https://www.singlestore.com/blog/whats-hot-and-whats-not-in-working-with-data/", "abstract": "Data is often considered to be the foundation of many global businesses. Data fuels daily operations, from customer transactions to analytics, from operations to communications. So we decided to answer the question: what’s hot and what’s not in working with data today? HOT: Letting your database be a database Databases were constructed to store data. However, sometimes applications are used to store data itself, a result of legacy database limitations. Storing data in an application makes it hard to update that application or to extract value from that data easily. By using a database for its intended function, developers can easily make changes to an  application, without affecting the data. This can save time and money in the long run. NOT: Adding SQL to your NoSQL database, but not calling it SQL SQL, or Structured Query Language, is the lingua franca for working with data and therefore a convenient tool for managing or analyzing data in a relational database. As a result, SQL is experiencing a renaissance. Many NoSQL databases now realize the value of SQL and SQL-like features such as JOINs. They are making hasty attempts to integrate SQL into their offerings, without acknowledging the gaps. HOT: Giving a dash of structure to your data Rather than spending your days wrangling unstructured data, providing some structure to your data upfront improves your ability to put that data to use down the road. Time is of the essence when it comes to most applications, and a little structure goes a long way for enabling real-time applications. Real-time stream processing frameworks like Apache Spark make it possible to add structure to data on the fly, so it is ready to be queried as soon as it lands in the database. HOT: Putting your data in RAM If data is made easily accessible, data locality will increase. Hoping your dataset fits in RAM is not a strategy – a strategic decision to ensure  data is in RAM improves the efficiency of applications that sit on the top of a database. NOT: Calling your database representative for scaling support Instead of calling your traditional database representative for scaling support, just add nodes with more flexible databases to achieve scale out. Adding nodes increases the speed of data processing. For example, with SingleStore you can add additional nodes while the cluster remains online. HOT/NOT: Knowing what is/knowing what was People are interested in staying up to date with the latest data processing techniques. Knowing what works for the present reality is more important that sticking with trends of the past. Real-time analytics will pave the way forward for business – it will reveal the path forward and ensure data does not remain trapped in dark corners. If you work with with databases or data, understanding the hot topics at present will save you from having to do battle with your data as you build applications, scale and innovate for your companies and yourself.", "date": "2015-07-28"},
{"website": "Single-Store", "title": "creator-of-hhvm-joins-memsql", "author": ["Emily Friedman"], "link": "https://www.singlestore.com/blog/creator-of-hhvm-joins-memsql/", "abstract": "Drew Paroski, co-creator of the HipHop Virtual Machine (HHVM) and the Hack programming language at Facebook, is bringing his expertise in compilers and code generation to SingleStore. While at Facebook, Drew was part of a three-person team appointed to rebuild the software layer of the company’s web-tier to support over one billion new users. Drew and team built HHVM to improve the speed and efficiency of programs written in PHP, the programming language that predominantly powers Facebook’s web-tier. HHVM is an important part of Facebook’s infrastructure optimization efforts which have produced a total of $1.2 billion in savings as of January 2014. Facebook published a paper that tracks the efficiency gains of HHVM since its launch; see the image below. Drew also co-authored several patents in just-in-time compilation, dynamic programming languages, and runtime data structures. Prior to his tenure at Facebook, Drew worked as an engineer at Microsoft and IBM. Source: “The HipHop Virtual Machine”, OOPSLA ‘14, published October 15 2014 We sat down with Drew to get the story, in his own words, about joining SingleStore and becoming a champion of in-memory computing. Q: Tell us about your time at Facebook and seeking the next big thing. Drew: I’m a systems engineer by trade and I’m passionate about the technical aspects of solving hard problems close to the boundary where hardware and software meet. But I also have an entrepreneurial streak, and I like to seek out challenges that can have big business impact. When I joined Facebook almost six years ago, they had about 300 million active users and were facing an enormous challenge: how to scale their PHP-based web-tier to handle four times as many users without increasing operating costs. I wanted to be a part of architecting that solution. Over the course of a few years, I designed and built HHVM with several talented engineers to improve the performance of PHP at Facebook. We also built the Hack programming language to address other issues with PHP language that hinder developer productivity when working with large codebases. With HHVM now powering 3 of the top 10 web properties in the world and with Hack well on its way to improving the state of web development, I was ready to seek the next big challenge. Q: Why did you join SingleStore? Drew: I knew a few engineers at SingleStore from a previous life. It occurred to me that SingleStore was a small but growing company of exceptional engineers solving problems on the frontiers of computing infrastructure, and I found that very enticing. The company is bringing real utility to customers across fast-paced industries like financial services and advertising technology. Since its inception in 2011, SingleStore has focused on building a world-class SQL query compiler, which attracted me to the company. For those who aren’t familiar with the term, compilers are these extremely versatile tools that power an incredibly broad range of applications. A computer’s CPU doesn’t read programming languages natively – these languages have to be translated by a compiler into what’s called machine code. What I love about building an efficient compiler is that if you produce machine code that executes twice as fast for a popular programming language, that means tons of programs written in that language will now be able to execute 2x faster than was previously possible, often without having to change a line of code. Now that’s a powerful impact – to be able to speed up all kinds of different applications with this one piece of software. Q: What opportunities do you see at SingleStore? Drew: SingleStore brings real-time big data analytics capabilities to companies large and small. We recently launched a free Community Edition so developers can have easy access, no strings attached – that reflects the commitment the company has to making the best technology available to everyone. The company solves challenges for companies in real-time which empowers them to act on what is happening now, not yesterday, and I think that’s really compelling. SingleStore is holding its own with the largest companies in the industry like Oracle and SAP. I think there is an opportunity here for the company to really take off. Plus SingleStore is one of the best places in the San Francisco area for people who want to work on hardcore engineering challenges. Q:What are you currently working on? Drew: I want to continue to innovate with the SingleStore query compiler. Making distributed SQL fast is a really juicy problem. Some might say SQL is retro, there is newer, hotter stuff out there. But so many developers and business professionals know SQL, and there is a ton of SQL code already in existence; all different kinds of businesses rely on SQL for their current operations. Making SQL better and faster helps our customers build businesses that run better and faster. One of the my main professional interests is computer performance. I have been interested in performance since I first started writing code. Performance can, and definitely should, be measured quantitatively, but if you think about it, performance can have qualitative effects as well. Imagine that it took two minutes after pressing the remote to change the channel on TV. How would that impact the experience? What you think of now as “channel surfing” would not be possible. It’s not just slower, it is qualitatively not the same experience. This is what makes me so excited about working on performance. Of course, I’m also interested in performance for the traditional reason that comes to mind: making it possible to execute applications 2x faster or more can substantially reduce operating costs for companies that have a non-trivial datacenter footprint. SingleStore Meetup in August to Feature Drew Paroski Join Drew for a discussion on compilers and C++ at SingleStore headquarters in San Francisco on Wednesday, August 19th. More information: www.meetup.com/memsql/", "date": "2015-07-31"},
{"website": "Single-Store", "title": "new-memsql-4-ops", "author": ["Steven Camina"], "link": "https://www.singlestore.com/blog/new-memsql-4-ops/", "abstract": "The latest release of SingleStore Ops – version 4.0.34 – is now available for download ! In this release, we are offering SingleStore users new features to accelerate productivity. Download SingleStore Ops to get up and running on SingleStore Community Edition or SingleStore Enterprise Edition today. SingleStore Ops downloads and upgrades are available for free to all SingleStore Community and Enterprise users. Here are some of the features in the new SingleStore Ops release: Ops Superusers The new SingleStore Ops comes with an enhanced superuser account that locks down read and write access Superusers can be created with a single command: memsql-ops superuser-add --password <password\\> <username\\> All users will log-in through a screen that looks like this: Figure 1: SingleStore Ops Superuser Login Page Ops user accounts are distinct from SingleStore or Linux users: they only allow access to the SingleStore Ops user interface. Installation Directory Improvements In this release of SingleStore Ops, we made it more intuitive for users to navigate to SingleStore installation directories and access their content. SingleStore installation directories were previously stored in /var/lib/memsql-ops/installs . In this release, we moved it to /var/lib/memsql . SingleStore node installation directories were also previously unique IDs used by Ops for tracking. We changed the naming scheme to be human readable. SingleStore node install directories are now named after their role and port, for example, master-3306 represents a master aggregator running on port 3306. SingleStore node installation directories can now be discovered more easily, for example: $ cd /var/lib/memsql/master-3306 More Metadata about Hosts This release introduces several additional pieces of metadata on the SingleStore Ops cluster page. When a cluster is configured with High Availability, which SingleStore recommends for all mission-critical deployments, individual host pages now contain more metadata about the High Availability (HA) pairings, including their HA group and paired SingleStore node. Figure 2: HA Metadata for SingleStore Leaf Nodes All internal and external addresses known about a given host (i.e. public/private IP, public/private hostname, or other address) are now displayed on the cluster host page for each machine in the cluster. Furthermore, users can change the address displayed on the cluster page through a new option in the Settings > Config page. Figure 3: Host Display Preference Customizable SingleStore Upgrade Files Users can now specify an installer file (tar.gz) for SingleStore cluster and SingleStore Ops upgrades through a new --file-path argument in cluster-upgrade and agent-upgrade commands. This enhancement gives users control over which version of SingleStore they want to install. Without this argument specified, the default behavior is to install the latest version of SingleStore from www.singlestore.com/download. Thanks for reading! Full release notes for current and previous SingleStore Ops releases are available on SingleStore Documentation .", "date": "2015-08-03"},
{"website": "Single-Store", "title": "in-memory-database-platform-forrester-wave-report", "author": ["Freja Mickos"], "link": "https://www.singlestore.com/blog/in-memory-database-platform-forrester-wave-report/", "abstract": "As adoption of in-memory databases grows at a faster and faster pace, IT leaders turn to research firms to find valuable use cases and guidance for purchasing options. We are thrilled to share that SingleStore was among the select companies that Forrester Research invited to participate in its 2015 Forrester Wave™ evaluation. In this evaluation, SingleStore was cited as a strong performer for in-memory database platforms. The report, The Forrester Wave™: In-Memory Database Platforms, Q3 2015, evaluates in-memory databases across three categories: current offering, strategy and market presence. SingleStore received some of its highest scores in the subcategories of scale-out architecture, performance and scale, and product road map. Much of our company’s recent growth and success can be attributed to our strong leadership team and constant iteration from engineering on the product, as we work closely with our customers to solve their big data and analytics challenges. Authors of the Forrester Wave™ write, “today’s in-memory database platforms are changing the way we build and deliver systems of engagement and are transforming the practice of analytics, predictive modeling, and business transaction management.”  At SingleStore, we have championed in-memory computing since day one. When Eric Frenkiel and Nikita Shamgunov left Facebook to start SingleStore, they knew that a real-time, in-memory approach to data processing and analytics was the answer to closing gaps for enterprises using big data. The major benefit of in-memory platforms is the great performance they provide when working with massive volumes of data. We believe the Forrester Wave™ report validates this approach, stating that “the in-memory database market is new but growing fast as more enterprise architecture professionals see in-memory as a way to address their top data management challenges.” There’s another reason why in-memory technology is going to become even more critical in the next several years: predictive applications. Consumers desire personalization from every single application they use across numerous devices. Data is at the crux of predictive analytics, which transcends “context-aware” technology by enabling seamless interaction between customer and app. Companies need instantaneous access to hot data to power these kinds of seamless interactions. Many of our customers are in the throes of building predictive applications, and we get to provide fast, scalable infrastructure to support them. Overall, we are very excited that SingleStore has been recognized by Forrester as a strong performer. The Forrester Wave™ concludes its section on SingleStore with the following line: “customers that are building new transactional and analytical applications that need extreme performance and low-latency access and want a single database platform should look at SingleStore.” We agree.", "date": "2015-08-03"},
{"website": "Single-Store", "title": "memsql-on-aws-and-azure", "author": ["Carlos Bueno"], "link": "https://www.singlestore.com/blog/memsql-on-aws-and-azure/", "abstract": "Good news, everyone! Today we’re releasing SingleStore Community Edition on the Amazon AWS and Microsoft Azure Marketplaces. Many of our customers run SingleStore in the cloud, and often their entire infrastructure. An even larger number try SingleStore first on the cloud before pulling it into their production systems. A great example is VCare, currently using SingleStore deployed on AWS cloud instances to power its online, real-time charging platform. VCare supports Mobile Virtual Network Operators (MVNOs), whose customers pay for a certain number of minutes. With its end-to-end software solution, VCare can verify that there is sufficient balance available for users to make phone calls or send text messages. The entire process takes place in milliseconds to ensure there is no lapse in service, supported by the cloud. Now it is easier than ever to get started. With just a few clicks you can launch a “cluster in a box” on a single virtual machine, and run our DB Speed Test within minutes. The DB Speed Test is a 30-second performance benchmark that comes with SingleStore Ops, and current frequently push over 1 million inserts per second on a single virtual machine. SingleStore Database Speed Test The quick start guides for launching SingleStore on AWS and Azure are live in our documentation. You can also launch a multi-vm SingleStore cluster on AWS using our Cloud Formation Template generator . AWS and Azure are two of the largest and most broadly adopted cloud platforms. Azure currently offers the largest instance available to support in-memory databases, like SingleStore, with 448 GB of RAM. Cloud platforms provide the agility that many businesses need to scale growing volumes of data, which can be put to use powering applications, monitoring datacenters for abnormalities, and more. We are excited to make SingleStore in-memory technology available to everyone with access to Community Edition on AWS and Azure marketplaces. For Community Edition on AWS, click here: https://aws.amazon.com/marketplace/pp/B011MEZL88/ For Community Edition on Azure, click here: http://azure.microsoft.com/en-us/marketplace/partners/memsql/memsql-community-single-vm/", "date": "2015-08-04"},
{"website": "Single-Store", "title": "gartner-catalyst-san-diego", "author": ["Kevin White"], "link": "https://www.singlestore.com/blog/gartner-catalyst-san-diego/", "abstract": "Gartner Catalyst Conference kicks off next week, Aug 10-13 in San Diego, and we are thrilled to speak and exhibit at the event. Stop by the SingleStore booth #518 to see our latest demos: MemCity , Supercar , and Pinterest .  SingleStore CEO, Eric Frenkiel, and the SingleStore product team will be available at the booth to answer any questions. Book a 1:1 ahead of time with a SingleStore expert here . On top of that, we have a speaking session, happy hour, games and giveaways planned. Here’s what you can expect: Speaking Session: Real-Time Data Pipelines with Kafka, Spark, and Operational Databases 12:45 – 1:05 PM Harbor Ballroom – Tuesday, August 11. What happens when trillions of sensors go online? By 2020, this could be a reality and real-time mobile applications will become integral to capturing, processing, analyzing and serving massive amounts of data from these sensors to millions of users. In this session, Eric Frenkiel, CEO and Co-Founder of SingleStore, will share how-to recipes for building your own real-time data pipeline and applications today with Apache Kafka, memory-optimized Apache Spark, and SingleStore. Mixed Workload Happy Hour – Monday, August 10 at 8p.m. Join the SingleStore leadership team and product specialists at the Lion’s Share for an evening of handcrafted drinks, New American plates, and great conversation. Complimentary Gartner Reports Gear up for Gartner Catalyst with two complimentary Gartner reports, courtesy of SingleStore. Five Data Persistence Dilemmas That Will Keep CIOs Up at Night Learn how converging transactions and analytics in a single database shortens the time to value for new data, shortcomings of NoSQL, and the importance of agile deployment approaches like cloud computing. Download Now >> Gartner Market Guide for In-Memory DBMS Learn why in-memory computing is growing in popularity and adoption, top use cases for in-memory databases, and Gartner’s recommendations for technology leaders. Download Now >> Games and Giveaways Be sure to stop by the SingleStore Booth #518 to get our newest t-shirt and play our reaction test game for a chance to win an Estes Drone! If you would like to schedule a meeting or demonstration with a SingleStore team member at Gartner Catalyst, you can do so here . We hope to see you there!", "date": "2015-08-06"},
{"website": "Single-Store", "title": "memsql-training-videos", "author": ["Lesia Myroshnichenko"], "link": "https://www.singlestore.com/blog/memsql-training-videos/", "abstract": "We are thrilled to have Carlos Bueno, Product Manager at SingleStore and an expert in performance tuning, as the leader of many of our training efforts. Prior to working at SingleStore, Carlos held critical engineering roles at Facebook, Yahoo, and several startups. He is the author of “Lauren Ipsum”, a popular children’s novel that introduces kids to computer science, as well as “Mature Optimization”, Facebook’s manual on performance measurement and optimization. We sat down with Carlos to talk about his two major passions: finding new ways to make use of data, and building easy-to-use technical training modules. Q: Tell us about your experience working with big data across various roles. Carlos: Some industries stumble upon future realities sooner than others. In the early 2000s, I was in charge of operations for a large (for that time) search advertising platform. 21st century advertising evolved very quickly into a cutthroat game of who could ingest, process, and act on relevant data faster than anyone else. This opened my eyes early on to what Peter Norvig calls the “ unreasonable effectiveness of data “. Since then, I have gone where the data is, which is what led me to Facebook, to help the company build a sustainable business with the most efficient code and datacenter operations possible. Now I’m basically an arms dealer for data scientists. Q: How did you develop an interest in technical training? Carlos: Years ago I started writing longform technical articles for the engineering blog at Yahoo, and then I did the same for Facebook, A List Apart, Hacker Monthly, and others. At Facebook I was “volunteered” to teach a regular bootcamp class on performance profiling, and discovered that I really liked the energy of a live audience. Later I helped design their week-long “Datacamp” program. I wrote a book form of my class called “Mature Optimization”, mostly as an excuse to give talks outside of the company. Q: What makes teaching people how to use SingleStore fun? Carlos: SingleStore represents a new kind of database, one that allows you to do things that had been so difficult for so long that people think they are impossible. The design of the system is a mix of new algorithms (e.g. skiplists) and almost retro ideas like SQL that turn out to be superior. It’s fun to dig into the mechanisms and implications of all that, how it all fits together. Q: What do you think is the most interesting aspect of SingleStore? Carlos: I love teaching people about small, powerful ideas that are not widely understood. Currently that’s geospatial intelligence . I didn’t know anything about how geospatial worked a couple of years ago. I’m still amazed at how just a few simple operators can enable things like our Supercar demo, which shows real-time cab rides and fun statistics around those rides. Q: What is your favorite question to receive in a training session? Carlos: Every class is different, but usually there’s a question about performance. Too often people treat performance problems like code bugs. They try to reason them away with a little mental model of what the system is doing. This can lead you astray if you don’t test your intuitions. So if someone asks “what is the performance of such-and-such kind of disk?” the correct answer is “I don’t know. But if you try this or that and measure these things, you’ll quickly find out the answer for your hardware and your particular data”. I like putting the science back into computer science. SingleStore DB 4 Training Video Series Now it is time to introduce the first SingleStore video training series, led by Carlos! The SingleStore DB 201 playlist has 6 modules. Watch the full playlist or choose the modules that most appeal to you. Module 1: SingleStore Architecture Overview In the first module, Carlos describes SingleStore and its main database characteristics, including technical principles, architecture, design and implementation. Module 2: Durability, Backups, Replication, and High Availability What happens when an in-memory database reboots? How is data replicated? In module 2, Carlos walks through what happens when SingleStore is deployed on machines, and how SingleStore demonstrates the four essential qualities of any leading database: Atomicity, Consistency, Isolation and Durability. Module 3: SingleStore Geospatial SingleStore supports common datatypes like strings, dates, numbers. SingleStore DB 4 also supports geospatial data types. In module 3, Carlos explains how geospatial data is stored in SingleStore, relational functionality, and spatial indexing for fast queries. Module 4: Columnstore In Module 4, Carlos explains the structure of the SingleStore columnstore, which compresses and processes data on disk. The SingleStore columnstore is ideal for storing data that exceeds your memory capacity. Infinite disk storage on SingleStore is provided for free. Module 5: Moving Data In Module 5, Carlos describes ways to move data in and out of a SingleStore cluster. He covers Hadoop/HDFS, Amazon S3, MySQL, flat files, and ODBC data sources. Module 6: Schema Design and Capacity Planning In Module 6, Carlos dives into the SingleStore in-memory rowstore, explaining data types, indexes, sparnesses and JSON, online ALTER TABLEs, and sharding strategies. He concludes with advice for planning a cluster based on various data workloads.", "date": "2015-08-10"},
{"website": "Single-Store", "title": "big-data-scala", "author": ["Emily Friedman"], "link": "https://www.singlestore.com/blog/big-data-scala/", "abstract": "Big Data Scala by the Bay, Aug 16-18, is shaping up to be an engaging event, and will bring together top data engineers, data scientists, developers, and data managers who use the Scala language to build big data pipelines. At the SingleStore booth, we will showcase how enterprises can streamline this process by building their own real-time data pipelines using Apache Kafka, Apache Spark and operational databases. Many of our customers are moving to this real-time data pipeline: a simplified Lambda Architecture that minimizes overhead while delivering remarkably fast analytics on changing datasets. Learn more here: http://bigdatascala.bythebay.io/ . To provide more perspective on the intersection of Scala and in-memory databases, we sat down with, Ben Campbell, our in-house Scala expert. Q: Describe the technical underpinnings of Scala. Scala is notable, and has achieved widespread use, largely because of the way it combines two distinct programming paradigms: object-oriented and functional. Object-oriented programming is, of course, familiar to most with C++ or Java — nearly all programmers have some familiarity with one or both of those languages. Functional programming, on the other hand, is less well-known, having historically been consigned largely to academic theory and niche applications. By combining the two approaches, Scala has been able to do what its functional predecessors have not: achieve widespread adoption by a community largely reared on the object-oriented paradigm. There’s an interesting analogy between Scala and C++, which was the breakout object-oriented  language.  C++ was not the first object-oriented language, nor was it a pure object-oriented language. However, C++ became widely adopted because it bridged the gap between C, a non-object-oriented language in widespread use at the time, and the object-oriented approach. Scala has done something similar: based on Java — it makes use of Java libraries and compiles to Java bytecode through the Java virtual machine — it has been relatively easy to adopt for a generation raised on the object-oriented paradigm. But Scala can also be used in a highly functional manner. So programmers coming from a Java background tend to increasingly embrace Scala’s functional features with time. Q: What is the functional programming paradigm, and how does it differ from alternatives? Functional programming treats computation as a problem of evaluating mathematical functions. On the other hand, object-oriented programming treats computation as a series of changes in state. Functional programming avoids such state changes, and hence there is no requirement for mutable data. Scala is an interesting hybrid of these two approaches — it can be written in a functional style, or in a more traditional Java-like style, with mutable state. Q: Why is functional programming, and hence Scala, important for Big Data? As background, object-oriented programming is useful for projects that involve creating increasingly elaborate objects from simpler primitives. Similarly, functional programming is well-suited for applications that compose increasingly elaborate functions from simpler functional primitives. This is often the case in data science, explaining the growing interest in functional programming approaches. As for Big Data, the term implies a set of problems that are too large to handle with conventional approaches — which generally entails a certain amount of parallelism. However, parallel processing is plagued by changes in state: if two parallel processes are attempting to change the same data, the result might be delayed (at best) or unpredictable (at worst). By reducing or eliminating mutability, functional approaches tend to lead to programs that naturally and simply handle concurrency and scalability. Q: What are some of the important use cases for Scala? Scala gained a lot of publicity in 2009, when Twitter announced it would be adopting the language for much of its backend. Since then, a number of other large enterprises have followed suit. But perhaps the biggest development in Scala has been Apache Spark, the big data processing framework. As somewhat of a successor to Apache Hadoop (whose MapReduce model was itself loosely based on functional processing), Spark is seeing enormous growth in adoption and interest — and the fact that it is written in Scala is drawing many to the language. Other notable implementations of Scala include the messaging queue Kafka, and several mathematical and machine learning libraries (e.g. ScalaNLP and BIDMach). Q: What does the success of Scala bode for the future of the programming landscape? With its hybrid object-oriented / functional approach, Scala will serve as somewhat of a gateway drug, helping to gradually transform the landscape towards more functional approaches. While Scala is fully object-oriented, tools like Slick allow it to interface with relational databases to implement more of a functional-relational approach to data. The increasing interest in scalable functional programming thus dovetails with a resurgence of interest in scalable relational database technologies, such as SingleStore. We hope to see you at Big Data Scala!  For conference details, click here: http://bigdatascala.bythebay.io/ .", "date": "2015-08-13"},
{"website": "Single-Store", "title": "how-to-deploy-memsql-on-the-mesosphere-dcos-2", "author": ["Wayne Song"], "link": "https://www.singlestore.com/blog/how-to-deploy-memsql-on-the-mesosphere-dcos-2/", "abstract": "The Mesosphere Datacenter Operating System (DCOS) is a distributed operating system designed to span all machines in a datacenter. It provides mechanisms for deploying applications across the entire system with a few simple commands. SingleStore is a great fit for deployment on DCOS because of its distributed, memory-optimized design. For example, users can scale computation and storage capacity by simply adding nodes. SingleStore deploys across commodity hardware and cloud, giving users the flexibility to operate with existing infrastructure or build custom hardware solutions. SingleStore and DCOS can optimize and simplify your test and development projects, however it is not a supported configuration and is not recommended for production deployments. In this blog post, we will illustrate an example of how to deploy SingleStore for your development or test environment on a cluster of DCOS-configured machines. Deploying SingleStore on DCOS Users can quickly get started with DCOS by deploying a cluster on Amazon AWS. Mesosphere provides a Mesosphere DCOS template specifically for this purpose, which leverages the AWS CloudFormation infrastructure. Follow the steps on mesosphere.com/amazon to set up DCOS on AWS. Deploying SingleStore on DCOS is simple with the DCOS command line. Once you have deployed a DCOS cluster and installed the DCOS command-line interface (check out the Mesosphere documentation for more information on this step), simply run the following command on the DCOS command line: $ dcos package install memsql At that point, if you check the DCOS web interface, you should see the SingleStore service running: If you click on the SingleStore link under “Services Health”, you’ll see the SingleStore Mesos UI, which allows you to start, monitor, and delete SingleStore clusters: This dialog allows you to change the cluster size and node type, and choose the SingleStore Community Edition or provide a SingleStore Enterprise license key. An Enterprise license enables features like high availability to ensure your data remains safe in the event a node fails. Once you have selected all of your cluster options, you can create your cluster and the SingleStore framework will provision it based on resources within the Mesos cluster. Once cluster creation is complete, the SingleStore connection string will be shown, and can connect directly to your cluster from within the Mesos cluster. If your Mesos machines have publicly-accessible IPs, you will be able to view them in the SingleStore Ops web interface. SingleStore has always offered flexible deployments across physical machines, VMs, private clouds, public clouds, and Docker containers. We are excited to introduce Mesosphere DCOS as a new infrastructure platform for your SingleStore development or test deployment. Install SingleStore today on the Mesosphere DCOS , or download the SingleStore standalone installer at singlestore.com/free . For more information on the SingleStore partnership with Mesosphere, read our earlier blog post: http://blog.memsql.com/deploy-memsql-with-mesosphere/. This week on August 20-21, the Apache Mesos community, from users to developers, gather to celebrate the widespread adoption of Mesos at MesosCon. The conference will feature two days of sessions on the Mesos core, an ecosystem developed around the project, related technologies, and hands-on workshops. For more information, visit http://events.linuxfoundation.org/events/mesoscon .", "date": "2015-08-17"},
{"website": "Single-Store", "title": "how-to-write-compilers-meetup", "author": ["Emily Friedman"], "link": "https://www.singlestore.com/blog/how-to-write-compilers-meetup/", "abstract": "Visit our SoMa headquarters this Wednesday, August 19th for our third official meetup, from 6pm-8pm! This is an exclusive opportunity to learn the art of building compilers from Drew Paroski. Before joining SingleStore, Drew co-created the HipHop Virtual Machine (HHVM) and Hack programming language to support Facebook’s web scale across a growing user base in the billions. Read more about Drew here: http://blog.memsql.com/creator-of-hhvm-joins-memsql/ . We will have a delicious Mexican feast complete with appetizers, south of the border brews, and wine. Compilers maximize application performance by translating a given programming language, like C++, into machine code. The ideal compiler produces very efficient machine code for popular programming languages, which means that programs written in the source language (e-commerce websites, games, social networking sites, you name it) will be able to execute 2x, 5x, 10x faster. Compilers represent a single piece of software that can speed up all kinds of applications. Drew’s expertise includes computer performance, programming with big data, and the advancement of compilers over the past 20 years. At the meetup, he will outline key considerations for building the best possible compiler, including: identifying your performance goals evaluating full-custom approach versus alternatives developing measurement benchmarks Bring your laptop if you would like to participate in our database speed test contest at 6:30pm – we will be giving away an Estes ProtoX Drone for the best performance! The meetup agenda: 6:00-7:00pm Happy hour with heavy hors d’oeuvres 6:30-7:00pm Optional contest: Database Speed Test – win a drone! 7:00-7:30pm Main Presentation: Compilers and C++ 7:30-8:00pm Q&A with Drew, continued happy hour RSVP here: http://www.meetup.com/SingleStore/events/224294793/ We look forward to seeing you there!", "date": "2015-08-18"},
{"website": "Single-Store", "title": "painless-schema-changes", "author": ["Adam Prout"], "link": "https://www.singlestore.com/blog/painless-schema-changes/", "abstract": "The ability to change a table’s schema without downtime in production is a critical feature of any database system. In spite of this, many traditional relational databases have poor support for it. Quick and easy schema changes was a key advantage of early distributed NoSQL systems, but of course, those systems jettison relational capabilities. Though conventional wisdom may indicate otherwise, easy schema changes are possible with the relational model. At SingleStore we put careful thought and effort into making sure that ALTER TABLE operations have minimal impact to running workloads. This feature is commonly called an “online” ALTER TABLE. Most relational databases support the notion of an “online” ALTER TABLE, but every vendor has a different definition of what that means. In SingleStore we define a true online ALTER as one that: 1) Does not require doubling the disk or memory use of the table while executing (creating a 2nd copy of the table without destroying the original table is not allowed) 2) Does not lock the table or prevent querying it for long periods of time (read or write) while running (under a second of blocking queries is OK) 3) Does not use excessive system resources while running (CPU, Disk, Network) no matter the size of the table or the workload running against the table SingleStore is the only distributed relational database able to achieve all three. For example, MySQL Cluster fails to do (1) – it copies the table in many cases. VoltDB, Vertica, and Redshift fail to do (2) – they lock the table throughout the entire ALTER operation, effectively taking down your production system, or requiring tedious juggling of replicas. Explaining how our ALTER TABLE works it best done by stepping through an example. Let say we wanted to add a column to a table as follows: CREATE TABLE example(c1 int primary key);\nALTER TABLE example ADD COLUMN c2 VARCHAR(100) DEFAULT NULL; Consider this diagram while we outline how ALTER runs through four phases of execution in the SingleStore rowstore. The first thing SingleStore does is change the structure (also called the metadata) of the table on every node in the cluster to include the new column, c2 as shown above “New row memory”. A short-lived table lock that blocks queries from executing against the table is held for the duration of the metadata change, which takes less than a second. The distributed lock is required to synchronize all nodes in the cluster so they will start displaying the new column at the same time. No row data has been modified at this point, only the table’s metadata. Now, SELECT queries against example will see the newly added column, but the column won’t actually exist in the table’s rows just yet. During query execution SingleStore generates the default value for c2 on the fly (NULL in our example) wherever it needs to. INSERTs (or any write query) will now need to provide data for c2, or c2 will default to NULL. These new rows will be inserted into the table alongside the old rows that don’t yet have c2. We are able to do this because, in SingleStore, indexes are allocated in a separate memory space from the memory for row data (see diagram). Whenever an index needs access to a row it follows a memory pointer. This design allows memory for rows inserted with c2 (inserted after step 1 of the ALTER) to co-exist with rows allocated before the metadata switch that added c2. A new memory space is set up at the time of the metadata change (“New Row Memory” in the diagram) to allocate rows for the new table schema. Newly inserted rows are allocated in this memory region. The original table rows remain in the “Old Row Memory” region. The ALTER now begins transferring the row data from the old row format to the new format. We can do this fairly straightforwardly using the memory space for the old rows (“Old Row Memory” above). The transfer process is essentially the same as a series of table updates. The alter thread commits every few megabytes of rows to avoid duplicating the entire table in a single transaction. It will eventually shrink the old row memory to nothing. Production workloads are happening all throughout this process. They simply check each row to determine if it has the new column or not and patches appropriately. The transfer process can take a while for larger tables, but it doesn’t impact running queries beyond using some CPU. Our ALTER implementation does have some caveats as a result of our requirement to make the operation a low-impact as possible. An ALTER TABLE can’t be cancelled after phase 1 above has completed. The row transfer process (phase 3) is not transactional (this would require doubling memory use) so it can’t be rolled back or stopped once it starts. The blocking at phase 1 forces the ALTER to wait for any long running query to finish executing before it starts. Some ALTER operations cannot be performed online. For example, you can’t add UNIQUE keys online in the current version SingleStore because the ALTER may run into a duplicate key error in the middle of the operation. Consequently, commands of that sort are locked out. Only one ALTER can be run against a table at a time. Since an ALTER is essentially a long-running query, a second ALTER waits until the first one completes. However, you can ALTER different tables at the same time. In summary, our online ALTER TABLE support novel capability you get when using SingleStore. Adding new indexes or columns without downtime is something our larger enterprise users rely on. Having both the flexibility of painless schema changes and the high powered querying capabilities of a distributed in-memory database is a unique combination.", "date": "2015-08-20"},
{"website": "Single-Store", "title": "understanding-memsql-in-5-easy-questions", "author": ["Conor Doherty"], "link": "https://www.singlestore.com/blog/understanding-memsql-in-5-easy-questions/", "abstract": "The funny thing about SingleStore is that it is simultaneously familiar and leading edge. On one hand, it is a relational database management system with a SQL interface – you probably already know how to use one of those! On the other hand, it stores data in memory and runs in a cluster – you may or may not have experience with a modern system that leverages these features. This post addresses common questions that people ask the first time they encounter SingleStore. By reading these five questions and answers, you’ll understand most of the fundamental characteristics of SingleStore, what makes it special, and how you are already armed with most of what you need to get started. 1 . What is SingleStore? SingleStore is a relational database management system with a SQL interface. It has the following additional properties: Distributed architecture: SingleStore typically runs on a “cluster” of servers. However, users address a single interface to get data in or out of SingleStore. SingleStore handles details like which servers store certain volumes of data, and distributed query execution logic. Users benefit from the resources of many servers without needing distributed systems expertise. Memory-optimized: SingleStore gives users the option to read and write data directly to and from main memory. Accessing data in DRAM is orders of magnitude faster than disk or even flash/SSD, and this provides extreme performance benefits for high-throughput transaction processing and real-time analytics. However, SingleStore does not require that all data fit in memory, and includes the option to store data, in a compressed format, on flash/SSD/disk. Software only: SingleStore is distributed as software, and can run in variety of Linux environments including on physical hardware (“bare metal”), in virtual machines, or in software containers. It can be run on premises, in your data center, or in a public cloud. Unlike legacy database vendors that require the use of proprietary hardware, SingleStore can run on commodity servers. 2 . Where does SingleStore store its data? When creating a table in SingleStore, you specify whether the data for that table will reside in memory or on disk. SingleStore has two table types: rowstore tables and columnstore tables. Rowstore tables are primarily used for operational or transactional workloads, particularly workloads that require rapid updates. SingleStore rowstore tables store data entirely in memory, with logs and full database snapshots written to disk for durability. Columnstore tables, on the other hand, are typically used for analytical workloads and data warehousing, as the format naturally lends itself to compression, efficient scanning, and rapidly appending data. SingleStore columnstore tables store data primarily on disk, but cache data in memory when possible. 3 . How does SingleStore ensure durability for data stored in memory? SingleStore writes logs and full database snapshots to disk, which can be used to recover state in the event that a machine turns off. SingleStore writes logs as data changes, and periodically triggers a full backup of the data in memory (snapshot). Users can configure the frequency of full database snapshots. SingleStore supports transaction processing, and exposes parameters for the user to tune performance. The most common question we get is how do we ensure durability when processing transactions in memory. In SingleStore, a transaction is “committed” when it has been written to the in-memory transaction buffer. SingleStore keeps a thread running in the background that is constantly writing blocks of data from the in-memory transaction buffer to disk as logs. The size of the transaction buffer is configurable. For instance, setting the size of the transaction buffer to 0 MB means that a transaction will not be committed until it has been logged to disk. In practice, it is not necessary to set the transaction buffer to 0 MB since using a buffer allows SingleStore to write large chunks of data to disk all at once, and because doing so requires only sequential I/O. 4 . How is SingleStore architected? A SingleStore cluster consists of two types of nodes: aggregator and leaf nodes. Client applications connect to an aggregator, which serves as the query router. Aggregators are aware of the entire cluster and know where specific data reside. Leaf nodes handle data storage and most of the computation during query execution. When the client sends a query, the aggregator splits it into several queries which are sent to each leaf node. The leaf computes its query and sends the result back to the aggregator. The aggregator consolidates the results from each leaf and sends the final result back to the client. Every cluster has a special aggregator called the Master Aggregator (non-Master aggregators are called child aggregators). Any aggregator, master or child, can process data manipulation language (DML) queries including SELECT, INSERT, UPDATE, and DELETE. Data definition language (DDL) commands, such as CREATE, DROP, or ALTER TABLE, must run on the Master Aggregator. In the event that the Master Aggregator machine fails, an administrator can “promote” a child aggregator to Master. Database administrators can add (and remove) nodes to the cluster at any time while keeping the cluster online, even while running a workload. Simply provision additional nodes, then add them to the SingleStore cluster. The easiest way to do this is through SingleStore Ops, but it can also be accomplished by sending commands to the Master Aggregator. 5 . How does SingleStore licensing work? How do I know how many machines I will need in my cluster? SingleStore is licensed based on the total RAM capacity of a cluster (the sum of the RAM in each server/VM/container in the cluster). Unlike some vendors in the database space, we don’t license based on number of CPU cores. This allows our customers to maximize CPU resources without licensing overhead. When planning, note that the cluster must have enough RAM for the following: Rowstore data storage in memory Rowstore and columnstore query execution The operating system (kernel) SingleStore enables several types of workloads ranging from stream processing and real-time analytics, to transaction processing, to data warehousing. Each of these workloads has different performance characteristics and latency requirements. If you need help designing your cluster, we are happy to assist! For general inquiries, you can reach us at info@singlestore.com . Get Started with SingleStore Now, you have the tools and fundamental concepts needed to start using SingleStore. Visit www.singlestore.com/free to download SingleStore today. Want more training, check out our new video training series with Carlos Bueno, former performance engineer at Facebook and now Principal Product Manager at SingleStore.", "date": "2015-08-31"},
{"website": "Single-Store", "title": "battle-for-app-specific-maps", "author": ["Gary Orenstein"], "link": "https://www.singlestore.com/blog/battle-for-app-specific-maps/", "abstract": "In early August, a consortium of the largest German automakers including Audi, BMW, and Daimler (Mercedes) purchased Nokia’s Here mapping unit, the largest competitor to Google Maps, for $3 billion. It is no longer easy to get lost. Quite the opposite, we expect and rely on maps for our most common Internet tasks from basic directions to on-demand transportation, discovering a new restaurant or finding a new friend. And the battle is on between the biggest public and private companies in the world to shore up mapping data and geo-savvy engineering talent. From there, the race continues to deliver the best mapping apps. Recently a story on the talent war among unicorn private companies noted Amid a general scramble for talent, Google, the Internet search company, has undergone specific raids from unicorns for engineers who specialize in crucial technologies like mapping . Wrapping our planet in mobile devices gave birth to a new geographic landscape, one where location meets commerce and maps play a critical role. In addition to automakers like the German consortium having a stake in owning and controlling mapping data and driver user experiences, the largest private companies like Uber and Airbnb depend on maps as an integral part of their applications. That is part of the reason purveyors of custom maps like Mapbox have emerged to handle mapping applications for companies like Foursquare, Pinterest, and Mapquest. Mapbox raised $52.6 million earlier this summer to continue its quest. Mapbox and many others in the industry have benefitted from the data provided by Open Street Maps , a collection of mapping data free to use under an open license. Of course some of the largest technology companies in the world besides Google maintain their own mapping units including Microsoft (Bing Maps) and Apple Maps. Investment in the Internet of Things combined with mobile device proliferation are creating a perfect storm of geolocation information to be captured and put to use. Much of this will require a analytics infrastructure with geospatial intelligence to realize its value. In a post titled, Add Location to Your Analytics, Gartner notes The Internet of Things (IoT) and digital business will produce an unprecedented amount of location-referenced data, particularly as 25 billion devices become connected by 2020, according to Gartner estimates. and more specifically Dynamic use cases require a significantly different technology that is able to handle the spatial processing and analytics in (near) real time. Of course geospatial solutions have been around for some time, and database providers often partner with the largest private geospatial company, Esri , to bring them to market. In particular, companies developing in-memory databases like SAP and SingleStore have showcased work with Esri. By combing the best in geospatial functions with real-time, in-memory performance, application makers can deliver app-specific maps with unprecedented level of consumer interaction. Google’s balloons and Facebook’s solar powered drones may soon eliminate the dead zones from our planet, perhaps removing the word “lost” from our vocabulary entirely. Similarly, improvements in interior mapping technology guarantee location specific details down to meters. As we head to this near-certain future, maps, and the rich, contextual information they provide, appear to be a secret weapon to delivering breakout application experiences. Download SingleStore today to try a real-time database with native geospatial intelligence at: singlestore.com/free .", "date": "2015-09-03"},
{"website": "Single-Store", "title": "in-memory-database-survey-real-time-analytics", "author": ["Freja Mickos"], "link": "https://www.singlestore.com/blog/in-memory-database-survey-real-time-analytics/", "abstract": "To shed light on the state of the in-memory database market, we conducted a survey on the prevalent use cases for in-memory databases. Respondents included software architects, developers, enterprise executives and data scientists 1 . The results revealed a high demand for real-time capabilities, such as analytics and data capture, as well as a high level of interest in Spark Streaming. Real-Time Needs for In-Memory Databases It is no surprise that our survey results highlight real-time analytics as the top use case for in-memory databases. For years, big data was heralded as the future of technology – today, it is a reality for companies big and small. Going real-time is the next phase for big data, and people seek technologies that address real-time data needs above all else. Those who can successfully converge transactional and analytical data processing, see greater efficiency in data management and have an invaluable advantage over their competitors. The Rise of Spark Looking at in-memory market trends, we asked our respondents which technologies they plan to evaluate in the upcoming year. Apache Spark takes the cake, with more than 50% of respondents planning to evaluate the data processing framework in the next year. According to our survey, Spark Streaming is the most widely adopted Spark library. This  shift in popularity from Spark SQL to Spark Streaming is good news for companies like Pinterest, who utilize Spark capabilities to structure real-time data up to the last click. By combining Spark Streaming with other memory-optimized technologies, like SingleStore, Pinterest can measure user engagement and developing trends in real-time . Challenges and Solutions In many ways, the purpose of an in-memory database is to provide a fast, persistent solution to big data challenges. The final, open-ended, portion of our survey asked our respondents what kinds of big data challenges are currently causing them the greatest strife. Popular answers include log processing bottlenecks, subpar performance, high costs, data migration from legacy systems, and complex queries for analytics. The utility and value of a database engine depends on its ability to provide solutions these common challenges. At SingleStore, we have spent the past four years iterating to offer an in-memory solution that gives users flexibility, agility, and security, with features like: JSON Support As Chris Preimesberger notes in an eWeek article, the SingleStore real-time, big data analytics platform is the first to combine structured and semi-structured data in a single database with JSON analytics. This integration saves users time and dollars spent on other middleware solutions. SingleStore Loader The SingleStore Loader is a productivity tool that allows for more efficient and streamlined data ingest, like from Amazon Web Service (AWS) or Hadoop Distributed File System  (HDFS). Data ingestion is often a cumbersome and complex. The SingleStore Loader addresses this challenge by allowing direct streaming from the datastore in just one transfer, and is capable of supporting multiple parallel input streams. The result: increased performance and minimization of repetitive operations. Spark Connector The SingleStore Spark Connector , which unites Spark’s in-memory data processing framework with SingleStore, maximizes operational data with highly advanced analytics. Taking advantage of Spark’s capabilities means top notch analytics and high-performance parallel throughput. Converged Processing HTAP, or hybrid transaction and analytical processing , enables SingleStore customers to merge transactions and analytics into a single database system. HTAP facilitates analyzing large volumes of data without the need for separate data marts or warehouses. Because transactions and analytics are combined in a single database, users are able to aggregate and report on real-time data in a more advanced manner than with traditional architecture To download SingleStore Community Edition for free, visit www.singlestore.com/free . Market Outlook for In-Memory Databaeses – 67 survey respondents", "date": "2015-08-27"},
{"website": "Single-Store", "title": "big-data-innovation-boston", "author": ["Kevin White"], "link": "https://www.singlestore.com/blog/big-data-innovation-boston/", "abstract": "The Big Data Innovation Summit kicks off in Boston today, uniting some of the biggest data-driven brands, like Nike, Uber, and Airbnb. The conference is an opportunity for industry leaders to share diverse big data initiatives and learn how to approach prominent data challenges. We are exhibiting at booth #23 and will showcase several demos: MemCity, Supercar, and Real-time Analytics for Pinterest. On top of that, we will have games and giveaways at the booth, as well as complimentary download of the latest Forrester Wave report on in-memory database platforms . More on what to expect: Demos MemCity – a simulation that measures and maps the energy consumption across 1.4 million households in a futuristic city, approximately the size of Chicago. MemCity is made possible through a real-time data pipeline built from Apache Kafka, Apache Spark, and SingleStore . Supercar – showcases real-time geospatial intelligence features of SingleStore. The demo is built off a dataset containing the details of 170 million real world taxi rides. Supercar allows users to select a variety of queries to run on the ride data, such as the average trip length during a determined set of time. The real-world application of this is business or traffic analysts can monitor activity across hundreds of thousands of vehicles, and identify critical metrics, like how many rides were served and average trip time. Pinterest – In this demo , Pinterest maps the location of repins as they happen in real-time across the United States. Each pin is filtered and then enriched with geolocation and pin category information. The enriched data is persisted to SingleStore using the SingleStore Spark Connector and is made available for query serving. Forrester Wave: In-Memory Database Platforms Recently, SingleStore was featured as a Strong Performer in The Forrester Wave: In-Memory Database Platforms . Stop by the SingleStore booth at Big Data Innovation Summit to find out why SingleStore received high scores in the subcategories of scale-out architecture, performance and scale, and product road map. You can also download a complimentary copy of the report here: singlestore.com Games and Giveaways As always, we will be giving away our latest t-shirts at the event. Stop by to get yours, and play our reaction test game for a chance to win an Estes ProtoX drone. Hope to see you there!", "date": "2015-09-09"},
{"website": "Single-Store", "title": "incumbents-and-contenders-in-the-33b-database-market", "author": ["Bruce Armstrong"], "link": "https://www.singlestore.com/blog/incumbents-and-contenders-in-the-33b-database-market/", "abstract": "The database market continues to surprise those of us who have been in it for a while. After the initial wave of consolidation in the late 1990s and early 2000s, the market has exploded with new entrants: column-stores, document databases, NoSQL, in-memory, graph databases, and more. But who will truly challenge the incumbents for a position in the Top 5 rankings? Oracle, IBM, Microsoft, SAP, and Teradata dominate the $33B database market. Will it be a NoSQL database? Will it be an open source business model? Ripping and replacing existing databases has been described as heart and brain surgery – at the same time. As such, new entrants must find new use cases to gain traction in the market. In addition, the new use cases must be of enough value to warrant adding a new database to the list of approved vendors . Splitting the world roughly into analytic use cases and operational use cases, we have seen a number of different vendors come and go without seriously disrupting the status quo. Part of the problem appears to be the strategy of using open source as a way to unseat the established vendors. While people seem willing to at least try free software (especially for new use cases), is it a sustainable business model? The open-source market is growing rapidly. However, it is still less than 2% of the total commercial database market. Gartner’s latest numbers show the open-source database market at only $562M, and the total commercial database market at $33B, in 2014. Furthermore, databases are complex, carrying decades of history behind them. To match, and ultimately exceed incumbent offerings, the key is not to have armies of contributors working in individual lanes, but rather to have a focused effort on the features that matter most for today’s critical workloads. This is especially true with the increasing number of mixed analytical and transactional use cases driven by the new real-time, digital economy. In the case of MySQL, the most successful open source database product, less than 1% of the installed base pays anything. Monty Widenius, the creator of MySQL, himself pointed this out in a famous post a couple of years ago. The business model needs to make sense too. The open source world almost never subtracts, it adds: more components, more configurations, more scratches for individual itches. Witness the explosion of projects in the Hadoop ecosystem, and the amount of associated services revenue. A commercial model embeds features into the primary product, efficiently generating value. Today customers seek to consolidate the plethora of extensive data processing tools into fewer multi-model databases. So, it is likely that the next vendor to win a spot in database history will do so by winning on features and workload applicability, and a proven business model with a primary product roadmap. However, there are many compelling aspects of the open source model, with three core value propositions: (1) a functional, free version; (2) open-source at the “edges” of the product; and (3) a vibrant community around the product. How can a commercial vendor balance both worlds? Companies pursuing these strategies include MapR in the Hadoop space. With announcements earlier this summer, SingleStore appears to be heading there too, for operational and analytical databases. They now have a SingleStore Community Edition with unlimited size and scale, and full access to core database features. While the production version of the product requires a paid license, this seems to be a reasonable way to balance the need to support a growing, focused engineering team with core value propositions of an open-source model. So, the question remains: as the database wars heat up and the market gets crowded, who will prevail to lead the industry? With open-source becoming more mainstream, the true contenders will be the vendors that can offer a symmetry between open-source models and new critical workload features.", "date": "2015-09-15"},
{"website": "Single-Store", "title": "big-data-themes", "author": ["Gary Orenstein"], "link": "https://www.singlestore.com/blog/big-data-themes/", "abstract": "We spent last week at the Big Data Innovation Summit in Boston. Big data trade shows, particularly those mixed with sophisticated practitioners and people seeking new solutions, are always a perfect opportunity to take a market pulse. Here are the big 5 big data themes we encountered over the course of two days. Real-Time Over Resuscitated Data The action is in real time, and trade show discussions often gravitate to deriving immediate value from real-time data. All of the megatrends apply… social, mobile, IoT, cloud, pushing startups and global companies to operate instantly in a digital,connected world. While there has been some interest in resuscitating data from Hadoop with MapReduce or SQL on Hadoop, those directions are changing. For example, Cloudera recently announced the One Data Platform Initiative , indicating a shift from MapReduce this initiative will enable [ Spark ] to become the successor to Hadoop’s original MapReduce framework for general Hadoop data processing With Spark’s capabilities for streaming and in-memory processing, we are likely to see a focus on those real-time workflows. This is not to say that Spark won’t be used to explore expansive historical data throughout Hadoop clusters. But judge your own predilection for real-time and historical data. Yes, both are important, but human beings tend to have an insatiable desire for the now. Data Warehousing is Poised for Refresh When the last wave of data warehousing innovation hit mainstream, there was a data M&A spree that started with SAP’s acquisition of Sybase in May 2010. Within 10 months, Greenplum was acquired by EMC, Netezza by IBM, Vertica by HP, and Aster by Teradata. Today, customers are suffering economically with these systems which have become expensive to maintain and do not deliver the instant results companies now expect. Applications like real-time dashboards push conventional data warehousing systems beyond their comfort zone, and companies are seeking alternatives. Getting to ETL Zero If there is a common enemy in the data market, it is ETL, or the Extract, Transform, and Load process. We were reminded of this when Riley Newman from Airbnb mentioned that ETL was like extracting teeth…no one wanted to do it. Ultimately, Riley did find a way to get it done by shifting ETL from a data science to a data engineering function (see final theme below), but I have yet to meet a person who is happy with ETL in their data pipeline. ETL pain is driving new solution categories like Hybrid Transactional and Analytical Processing, or HTAP for short. In HTAP solutions, transactions and analytics converge on a single data set, often enabled by in-memory computing. HTAP capabilities are the forefront of new digital applications with situational awareness and real-time interaction. The Matrix Dashboard is Coming Of course, all of these real-time solutions need dashboards, and dashboards need to be seen. Hiperwall makes a helpful solution to tie multiple monitors together in a single, highly-configurable screen. The dashboards of the future are here! Emerging Data Science Organizational Structures Organizational structures for data science are still emerging. Riley Newman from Airbnb shared his view of the data stack, Visualization | Sustained narrative around execution Experimentation | Disentangling causality Data Products | Machine learning, recommendation algorithms Analysis | Understanding user behaviors and business drivers ETL | Curation of client data for analysis / reporting Infrastructure | Stability of warehouse systems and tools Then he identified where the data science team should be spending the bulk of its time, and defined an organizational structure to fill in additional areas with complementary resources. Infrastructure is largely handled in the cloud, and ETL was assigned to a dedicated data engineering team. Next Up Strata + Hadoop World New York 2015 We’ll be continuing our fall show circuit next with Strata + Hadoop World in New York, September 29th to October 1st. Visit singlestore.com to catch the latest SingleStore details. Hope to see you there!", "date": "2015-09-17"},
{"website": "Single-Store", "title": "rapid-scaling-for-startups-lessons-from-salesforce-and-twitter", "author": ["Chris Fry"], "link": "https://www.singlestore.com/blog/rapid-scaling-for-startups-lessons-from-salesforce-and-twitter/", "abstract": "RSVP for the SingleStore Meetup: 10 Tips to Rapidly Scale Your Startup with Chris Fry, former SVP of Engineering at Twitter There is nothing more challenging and exciting than experiencing hyper growth at a technology company. As users adopt a technology platform you have to rebuild the technology plane while flying it which can be a harrowing process. I found several approaches to scaling that held true across Salesforce, Twitter, and the startups I now work with on a daily basis. Every company is different but these common problems and solutions should help you on your journey. Team Structure The first problem most early stage companies face is how to grow and structure the team. There are common breaking points around 20 people and 150 where what you were doing ceases to function. What should your teams look like while you are simultaneously tackling growth, structure, and scale? Small teams are the most effective teams, with an ideal size between two and ten people (with smaller being better). Large teams don’t stay in sync while small teams can organically communicate, solve problems and fill in for teammates. You can decompose large teams into autonomous small teams. The best teams can work autonomously. Make sure that teams have all the resources needed to deliver on their goals and know the boundaries of what they should take on. It’s important to create teams that span technology horizontally to create consistency and vertically to attack user focused problems. Teams need structure so they can deliver on their mission without other teams getting in the way. Fast Iteration How do you keep delivering as your company scales? Early in a technology companies startup life many naturally iterate quickly. Unfortunately as companies scale communication and technology issues slow iteration speed. The best thing to focus on is to keep delivering work on a regular quick pace. Creating software is a learning process and each iteration is a chance for the team to learn. Automation also plays a critical role in maintaining a high quality product and should be developed while you are building features. Remember, quality is free – the better software you build, the more you test it, the faster you can change it. Retention and Culture How do you build and maintain a unique engineering culture? To scale an engineering culture you must have one. Discuss it. Set principles. Teach the team to easily remember and articulate key cultural tenets. Put these tenets in writing to bring on new employees and serve as a reference point. Finally, live the culture you set. Culture is a soft topic and if its not lived from the top it is just words on paper. To steal from Dan Pink I would always focus on delivering autonomy, mastery and purpose to each engineer and the engineering team as a whole and build out the cultural practices from there. For example hackweek or letting people pick what team they would work on every quarter. For example, at both Salesforce and Twitter we stressed a culture of experimentation and learning. This helped us focus on product and technology innovation and led directly to better product features for our primary platforms. It’s important to invest in the technical infrastructure to support iteration. At Twitter we used Mesos to scale computation and built out distributed storage to make data available anywhere it was needed. Your infrastructure should allow any engineer to put an idea into production in a day. Learn More Scaling Tips Chris will be presenting “10 Tips to Rapidly Scale Your Startup” on Thursday evening September 24th at SingleStore headquarters in San Francisco. Visit http://www.meetup.com/memsql to register. About Chris Fry Chris Fry was Senior Vice President of Engineering at Twitter, Inc. and before that Senior Vice President, Development, at Salesforce. He is currently an advisor to SingleStore and other startups.", "date": "2015-09-21"},
{"website": "Single-Store", "title": "spark-streamliner", "author": ["Ankur Goyal"], "link": "https://www.singlestore.com/blog/spark-streamliner/", "abstract": "SingleStore Streamliner is now generally available! Streamliner is an integrated SingleStore and Apache Spark solution for streaming data from real-time data sources, such as sensors, IoT devices, transactions, application data and logs. The SingleStore database pairs perfectly with Apache Spark out-of-the-box. Apache Spark is a distributed, in-memory data processing framework that provides programmatic libraries for users to work with data across a broad set of use cases, including streaming, machine learning, and graph data processing. SingleStore and Spark share many design principles: they are in-memory, distributed, and data-centric. Spark provides an amazing interface to the unique functionality in SingleStore: fast and durable transactions, a real-time hybrid row/column-oriented analytics engine, and a highly concurrent environment for serving complex SQL queries. The SingleStore Spark Connector , released earlier this year, allows Spark and SingleStore integration, facilitating bi-directional data movement between Spark and SingleStore. The connector generated a lot of interest from users who saw the benefits of using Spark for data transformation and SingleStore for data persistence. A consistent theme in the use cases we saw was the desire to use Spark to stream data into SingleStore with Spark Streaming. SingleStore Streamliner is the result of our work to productize this workflow into an easy, UI-driven tool that makes this process dead simple. Let’s review the thinking behind some of the decisions we made as we were developing Streamliner. Early work with Pinterest Pinterest showcased a Kafka+Spark+SingleStore solutio n in Strata+Hadoop World last February, which was a collaborative effort with SingleStore. See the Pinterest blog post and the Pinterest demo to learn more. The Pinterest solution leveraged Spark Streaming to quickly ingest and enrich data from Kafka, and then store it in SingleStore for analysis. The Kafka, Spark, and SingleStore solution, spurred by the Pinterest showcase, identified the market’s need for a simple solution to capture data streams from Kafka and other real-time inputs into a persistent, data serving endpoint. In other words, there was a need for a streamlined (pun intended) experience that captured the power of Spark Streaming behind an intuitive UI. The Streamliner concept was born. Streamliner Engineering Streamliner was built to make this process of building a real-time data pipeline seamless and intuitive. The underlying technologies – SingleStore and Spark – already existed, but combining them to build real-time data pipelines was still a nontrivial exercise. Users just wanted to “paste” a Kafka URL into the browser and get started with querying their data, but could not easily do so prior to Streamliner. Earlier experiences required writing code to build a data pipeline from Kafka, into Spark for processing, and out of Spark for persistence. Key features of Streamliner A Simple, Pipeline-oriented UI In the SingleStore Ops dashboard, the Streamliner page allows users to easily add new streaming pipelines to the system, do it online, and for the most part, do it without writing any code! Resource Sharing Based on our experience working with Pinterest, static resource allocation per stream (the standard paradigm in Spark) was not practical at scale. A robust and cost-effective solution requires multiple pipelines to overlap on and share the same set of resources, which is now handled out of the box in Streamliner. Very Simple Points of Extensibility We noticed clear patterns with how our users were loading data and developed two abstractions in Streamliner – Extractors and Transformers – which allow users to extend the system. Extractors define new sources (discretized streams) of data, and Transformers transform raw data (bytes) into structured or semi-structured data (data frames). Extractors and Transformers are configurable and composable, making it easy for users to create and reuse common elements across pipelines without writing additional code. Tracing Visibility into each of the pipeline phases is necessary for debugging. Streamliner allows users to see results inline for a few batches of data. Streamliner also collects and formats pipeline exceptions and exposes them in the UI, so users don’t have to dig through logs to find where things went wrong. Advanced Columnstore Functionality Optimized for Data Streaming SingleStore DB 4.1 allows streaming data directly into the columnstore (no need for batched writes) and columnar compression on JSON data (backed by Parquet). These are new features in SingleStore DB 4.1, whose release coincides with Streamliner. Spark Installation Another hurdle to productivity with Spark has been installation. Spark is complex to install, as it is a sophisticated data processing framework and requires knowledge of Java and Scala. Streamliner was designed with easy installation as a top priority, and can be used right away without writing any code or installing a development environment. SingleStore Ops already has the ability to install the SingleStore database with a click of a button. With Streamliner, Spark deployment is also just one click away. Streamliner and a library of example Extractors and Transformers are open source and available immediately . You can get started right away by downloading the latest version of SingleStore Ops and following this tutorial . We are looking forward to seeing what pipelines people build and to participating in the open, vibrant community around Apache Spark.", "date": "2015-09-24"},
{"website": "Single-Store", "title": "building-real-time-data-pipelines", "author": ["Kevin White"], "link": "https://www.singlestore.com/blog/building-real-time-data-pipelines/", "abstract": "In the era of high-volume and high-velocity data, the faster you can process information and make decisions from it, the better. Accelerating data feedback loops gives you the agility to test many ideas, sort the good from the bad, and react to changes faster than the competition. Getting to Real-Time Building faster feedback loops requires ingesting large volumes of streaming data and also having tools to explore that data as it is captured. This is easier said than done, as legacy RDBMSs rely on high-latency batch loading, and NoSQL stores limit analytic functionality. Transitioning to simultaneous data processing and analysis calls for adoption of a hybrid database model, where transactions and analytics converge in a single system. This means embracing evolving technologies like in-memory computing and distributed architectures that are optimized for rapid ingestion and exploration across large, changing datasets. Free O’Reilly Ebook: Building Real-Time Data Pipelines We teamed up with O’Reilly Media to bring you a complimentary ebook: Building Real-Time Data Pipelines – Unifying Applications and Analytics with In-Memory Architectures . This ebook will serve as your guide to achieving real-time business operations, providing examples of proven models, real-world use cases, and recommendations for implementation along the way.", "date": "2015-09-29"},
{"website": "Single-Store", "title": "digital-ocean-memsql-tutorial", "author": ["Carlos Bueno"], "link": "https://www.singlestore.com/blog/digital-ocean-memsql-tutorial/", "abstract": "As fun as it is to squirrel around inside the guts of some new technology, it’s sometimes nice to follow a recipe and end up with something that Just Works. For years, Digital Ocean, an up and coming cloud provider, has been producing quality tutorials on how to set up cool software on their virtual machines. Today Ian Hansen published an in-depth tutorial on setting up a three-node SingleStore cluster. Check it out here . Go to the Digital Ocean tutorial and learn how to install SingleStore in minutes Once the cluster is running, Ian walks through our DB Speed Test. He then dives into interacting with SingleStore using the stock MySQL client and handling structured and instructed data with our JSON datatype. The next tutorials in the series will deal with sharding strategies, replication, and security. We’re also lucky to have Ian here at Strata / Hadoop World in NYC to give a talk called “Big Data for Small Teams”, about how Digital Ocean uses SingleStore to unify and analyze their clickstream data with a minimum of fuss.", "date": "2015-09-30"},
{"website": "Single-Store", "title": "qa-with-dan-mccaffrey-vp-analytics-teespring", "author": ["Emily Friedman"], "link": "https://www.singlestore.com/blog/qa-with-dan-mccaffrey-vp-analytics-teespring/", "abstract": "One of the best parts of being a startup is getting to work with other startups – companies experiencing rapid growth get to share ideas and technology, and fortify the startup ecosystem. Yesterday we announced a new customer who exemplifies this: Teespring! Teespring is an on-demand ecommerce company that gives budding entrepreneurs the opportunity to build and grow their apparel businesses via its platform. Teespring uses SingleStore to power its sales analytics platform, which analyzes data in real-time to educate sellers about the buyer community, and facilitate meaningful purchasing transactions for both parties. We sat down with Teespring VP of Analytics, Dan McCaffrey to talk about the company’s platform, and how SingleStore fits into the picture. What are some of the trends you are seeing in analytics for ecommerce? One trend stands out above the rest for me, and is really driving on-demand social commerce right now: Personalization. End-users want to spend less time discovering and searching for what they want – they seek instant gratification, which means personalization is critical to the success of ecommerce platforms. How is Teespring tapping into real-time data streams to connect top-performing buyers and sellers? Right now, we are focused on providing our sellers with top-notch analytics to help them target buyers who would be a good fit for their products – ultimately, this benefits both sellers and buyers who can connect with custom products that reflect their interests.  Real-time analytics enables us to share metrics with sellers on how their business is doing, allowing them to tune and optimize faster. “Real-time” data is the key here – allowing for even better optimization and ROI for sellers that otherwise would not be possible. What led Teespring to choose SingleStore over other solutions? What I really liked about SingleStore was the ANSI SQL support for dynamic querying needs at scale, in a reliable, robust, easy-to-use database. As an in-memory database, SingleStore provides speed for real-time processing and analytics, and allowed us to achieve web scale. SingleStore was by far the best of all the choices we considered. How rapid was the business transformation once you made the move to SingleStore? It was easy to install SingleStore. Capturing real-time business insights is an ongoing process, but the positive effects have already been felt, demonstrated by our ability to personalize and target our website and marketing efforts, as well as serve the analytics data our sellers need to run their businesses more effectively. It took us just a few weeks to build our sales analytics database and feel the first impacts. How many internal groups/teams are working with the sales analytics database? Currently, several teams have access to the database: Product, Analytics, Finance and Marketing are a few of the major examples and we plan to add more departments soon. What other opportunities has this opened for Teespring? To operate an efficient on-demand ecommerce platform where end-users make purchases based on their virtual experience, we need to know that our back-end is strong and is reinforcing our business goals. The solution we have implemented with SingleStore has improved our operational health reporting for faster response times to issues. On top of that, we have been able to offload critical reports, which supports refreshing dashboards.  Teespring is growing quickly, and we will continue to use SingleStore as much as we can to propel our business forward. Check out the news release here: https://www.singlestore.com/media-hub/releases/teespring-powers-commerce-with-in-memory-database/ For more information on Teespring, visit www.teespring.com/about To download SingleStore, visit: www.singlestore.com/free", "date": "2015-10-09"},
{"website": "Single-Store", "title": "building-an-infinitely-scalable-testing-system", "author": ["Doug Doan"], "link": "https://www.singlestore.com/blog/building-an-infinitely-scalable-testing-system/", "abstract": "Quality needs to be architected like any other feature in enterprise software. At SingleStore, we build test systems so we can ship new releases as often as possible. In the software world, continuous testing allows you to make tiny changes along the way and keep innovating quickly. Such continuous testing is an essential task—and on top of that, we compete with large companies and their armies of manual testers. Instead of hiring hordes of testers, we decided to build infinitely scalable test software. This test system is called Psyduck, and it is extremely powerful. We currently run over 100,000 tests a day on Psyduck, almost double the number of tests from the last release of SingleStore. In order to achieve this, we had to architect Psyduck to scale as we grew. In this blog post, we will share how we utilize Psyduck to maximize product quality, as well as build an efficient developer workflow. Any engineering team, regardless of size, needs an infinitely scalable testing system of its own. Make Testing Easy The first step in building your test system is to ensure your entire team is on board. You can make testing mandatory, but the best way to develop extraordinary testing, is to make the process easy. This also helps foster an engineering culture where developers are passionate about testing. The SingleStore developer workflow for writing a new feature, including testing it, is engineered to be deliberately easy. See the image below. Make Testing Asynchronous, Scalable, and Optimal for Latency Building an infinitely scalable test system becomes especially critical as you hire more engineers. More engineers means more machines and more tests. Optimizing this infrastructure for latency is even harder—if your test system is easy to use but takes a full day to produce any results, then your engineers will approach testing conservatively. On the other hand, if your test system runs tasks in the background and produces results quickly, your engineers can aggressively test as they are coding. Conservative testing is tantamount to not testing at all, so it is important to be vigilant. You want your test system to be so scalable that test runs can happen instantaneously. Psyduck is incredibly scalable, so our engineers can reproduce and fix sporadic bugs that only occur once every 1000 test runs. They schedule a test run with 1000 executions of the same test and continue coding while Psyduck quickly distributes and executes tests across all available machines. When one of the 1000 tests fails, the engineer can immediately switch back to debugging that failed test. In a serial environment, it is not possible to optimize for such latency. Make Debugging a Failed Test Easy A test system can be easy-to-use and fast, but when an engineer investigates a test failure, they need access to disparate information. Typical test systems show PASS or FAIL. Psyduck goes beyond that—it presents comprehensive information in an actionable format. Data is immediately available for use by any engineer via a web UI or a one-liner in a terminal. Test Outputs If a test failed due to correctness, Psyduck provides the expected output and actual output, as well as shows useful tracing. Stack Traces If SingleStore experienced an error during testing and generated a core dump, you typically only need to see the stack trace. Psyduck presents the information readily with the click of a button. Test Environment Psyduck lets you spin up and SSH into a Docker container with the exact environment in which the test was running. This container makes the first step in your investigation extremely simple. On the other hand, if you like your local debugging tools better, Psyduck provides access to core dumps and debug symbols with just one command on your terminal. Performance Metrics and Graphs You can view performance metrics right on the web page. You can also download performance reports to create custom visualizations. Screenshots for Failed UI Tests If a UI test failed, Psyduck takes an automatic screenshot of a browser window so you know exactly what to investigate. Make Gathering Analytics Easy Assuming you can build a test system that meets the above criteria, the next step is to use the system’s data to view a snapshot of the current state of quality, influencing decisions about your release. To support this, your test system must have visual analytics tools. These tools, however, do not necessarily need to be built in-house. For example, we simply attach Tableau to the SingleStore database containing all our test runs and can instantly visualize the level of quality. Below is a sample use case during our development cycle. Make Testing Pervasive Once you have built a test system like Psyduck, you need to make yourself fully dependent on it. That is to say, you cannot check in code if your test system is down. This dependency may seem like a big commitment, but the alternative where you allow developers to check in untested code is much more expensive in the long term. To encourage full dependency on Psyduck at SingleStore, we put it everywhere: It is in everyone’s browser – our engineers can look at any test run across the office, and share results by copying/pasting links. It is in everyone’s terminal – our engineers kick off test runs with one command. For example, this will run all the geospatial “transforms”: $ psy test --filter=.spatial_transforms More about transforms here . It is in our code review system – Psyduck results show up automatically on code reviews, so other engineers feel confident that the code was rigorously tested. Make Testing Mandatory Earlier, we stated that you should not make testing mandatory. That is because the steps in the plan we have just outlined lead to an engineering culture where stellar testing is de facto mandatory. For us, every time a developer builds a new class of tests—such as large scale tests, integration with streaming systems, tests with complex external dependencies—it is tempting for them not to integrate these custom tests with Psyduck. However, if those tests are not running every night and marked as PASS or FAIL, they will simply die on the side of the road. Every single test runs on Psyduck. Make the Journey to the Infinite Future Exciting What we have built so far in Psyduck is only the beginning. We have many more ideas to implement, many other tools to build, and many different tests to write. If you share our passion for testing the right way, the infinitely scalable way, check out our careers page for job openings. Let’s build something great together.", "date": "2015-10-14"},
{"website": "Single-Store", "title": "the-benefits-of-an-in-memory-database", "author": ["Lesia Myroshnichenko"], "link": "https://www.singlestore.com/blog/the-benefits-of-an-in-memory-database/", "abstract": "Our CTO and co-founder Nikita Shamgunov recently sat down with Software Engineering Daily . In the interview, Nikita focused on the ideal use cases for an in-memory database, compared to a disk-based store, and clarified how SingleStore compares to MySQL. In this post, we will dig deeper into how we define the ‘in-memory database’ and summarize its benefits. What is SingleStore? SingleStore is a high-performance in-memory database that combines the horizontal scalability of distributed systems with the familiarity of SQL. How do you define an ‘in-memory database’? An in-memory database, also known as a main memory database, can simply be defined as a database management system that depends on main memory (RAM) for computer data storage. This is in contrast to traditional database systems, which employ disk-based storage engines. The term ‘in-memory’ is popular now, but it does not tell the whole story of SingleStore. Our preferred description is ‘memory first’, which means RAM is used as a first class storage layer – you can read and write directly to and from memory without touching the disk. This is opposed to “memory only,” which does not incorporate disk as a complimentary storage mechanism. What type of data lends itself well to an in-memory database? If you need ultra fast access to your data, store it in an in-memory database. Many real-time applications in the modern world need the power of in-memory database structures. There are several critical features that set in-memory databases apart. First, all data is stored in main memory. Therefore, you will not have to wait for disk I/O in order to update or query data. Plus, data is loaded into main memory with the help of specialized indexing data structures. Second, data is always available in memory, but is also persisted to disk with logs and database snapshots. Finally, the ability to read and write data so quickly in an in-memory database enables mixed transaction/analytical and read/write workloads. As you can see, in-memory databases provide important advantages. They have the potential to save you a significant amount of time and money in the long run. For more information, listen to Nikita’s complete podcast: http://traffic.libsyn.com/sedaily/memsql_nikita_2.mp3 Test the magic of in-memory for yourself, by downloading a 30-day Enterprise Edition trial or free forever Community Edition of SingleStore at singlestore.com/free .", "date": "2015-10-15"},
{"website": "Single-Store", "title": "why-memsql-placed-a-bet-on-sql", "author": ["Carlos Bueno"], "link": "https://www.singlestore.com/blog/why-memsql-placed-a-bet-on-sql/", "abstract": "In the technology industry, when products or innovations last for a long period of time, they are often here to stay. SQL is a great example of this – it has been around for over 30 years and is not going away anytime soon. When Eric Frenkiel and Nikita Shamgunov founded SingleStore in 2011, they were confident in choosing the SQL relational model as the foundation for their database. But the database industry during that era was clamoring around NoSQL, lauding it as the next great innovation, mostly on the themes of scalability and flexibility. When SingleStore graduated from Y Combinator, a prominent tech incubator, that same year it was the only distributed SQL database in a sea of non-SQL offerings. SQL has since proven its ability to scale and meet today’s needs. Business analysts seek easy interfaces and analytics for the problems they are trying to solve. Customers want SQL, and like Dan McCaffrey, VP of Analytics at Teespring, happily cite that as a reason for choosing SingleStore. Dan states: “What I really liked about SingleStore was the ANSI SQL support for dynamic querying needs at scale, in a reliable, robust, easy-to-use database.” Now, with the reconquista of SQL, we are seeing two funny things happening in the market. One, companies that monetize the Hadoop Distributed File System are adding layers of SQL on top of the Hadoop platform. Two, NoSQL databases are incorporating SQL. NoSQL databases are essentially key value stores, and adding SQL gives them the ability to do some analytics. However, adding a SQL layer is no substitute for the richness of advanced SQL that was built into the SingleStore database. SQL as a layer is just a band-aid solution. The Gartner Magic Quadrant for Operational Database Management Systems The latest Gartner Magic Quadrant for Operational Database Management Systems confirms something we have been championing for a while: “By 2017, all leading operational DBMSs will offer multiple data models, relational and NoSQL, in a single DBMS platform… by 2017, the “NoSQL label will cease to distinguish DBMSs, which will result in it falling out of use.” For years, SingleStore has supported both a fully-relational SQL model, and a “NoSQL” model, together in the same cluster of machines. This was a bet made by our original engineering team – they understood the powerful appeal of SQL to business users, but also knew the value of the “NoSQL” model of vast scale. For that reason, SingleStore is multi-model, and databases of the future will need to support multiple operations to survive. Our co-founders were confident back in 2011, and we remain confident with validation from the market, research firms like Gartner, and most importantly from our customers, that SQL is the path forward. We will continue to hone the SQL aspects of our database and champion the lingua franca of the database world. For a free copy of Gartner Magic Quadrant: OPDBMS, visit singlestore.com To read more about the resurgence of SQL, visit http://readwrite.com/2015/06/02/memsql-eric-frenkiel-interview .", "date": "2015-10-20"},
{"website": "Single-Store", "title": "technical-deep-dive-into-memsql-streamliner", "author": ["Wayne Song"], "link": "https://www.singlestore.com/blog/technical-deep-dive-into-memsql-streamliner/", "abstract": "SingleStore Streamliner, an open source tool available on GitHub, is an integrated solution for building real-time data pipelines using Apache Spark. With Streamliner, you can stream data from real-time data sources (e.g. Apache Kafka), perform data transformations within Apache Spark, and ultimately load data into SingleStore for persistence and application serving. Streamliner is great tool for developers and data scientists since little to no code is required – users can instantly build their pipelines. For instance, a non-trivial yet still no-code-required use case is: pulling data in a comma-separated value (CSV) format from a real-time data source; parsing it; then creating and populating a SingleStore table. You can do all this within the Ops web UI, depicted in the image below. As you can see, we have simulated the real-time data source with a “Test” that feeds in static CSV values. You can easily replace that with Kafka or a custom data source. The static data is then loaded into the hr.employees table in SingleStore. Sometimes, however, you need to perform more complex operations on your data before inserting it into SingleStore. Streamliner supports uploading JAR files containing your own Extractor and Transformer classes, allowing you to run extracts from custom data sources and arbitrary transformations on your data. Here is an example of how to create a custom Transformer class. It assumes we will receive strings from an input source (e.g. Kafka) that represent user IDs and their country of origin. For each user, we are going to look up the capital of their country and write out their user ID, country, and capital to SingleStore. The first thing you should do is check out the Streamliner starter repo on GitHub. This repo is a barebones template that contains everything you need to start writing code for your own Streamliner pipelines. Our example Transformer is going to have a map from ISO country code to that country’s capital. It takes in a CSV string containing a user ID (which is an integer) and a country code. It will return a DataFrame containing rows with three columns: user ID, country code, and country capital. Open up Transformers.scala and edit the BasicTransformer code so that it looks like this: class BasicTransformer extends SimpleByteArrayTransformer {\n   val countryCodeToCapital = Map(\n      \"AD\" -> \"Andorra la Vella\",\n      \"AE\" -> \"Abu Dhabi\",\n      \"AF\" -> \"Kabul\",\n      \"AG\" -> \"St. John's\",\n      … // You may add as many entries as you’d like to this map.\n   )\n   override def transform(sqlContext: SQLContext, rdd: RDD[Array[Byte]], config: UserTransformConfig, logger: PhaseLogger): DataFrame = {\n      val rowRDD = rdd.map(x => {\n         val fields = byteUtils.bytesToUTF8String(x).split(',')\n         val userId = fields(0).toLong\n         val countryCode = fields(1)\n         val capital: String = countryCodeToCapital.getOrElse(countryCode, null)\n         Row(userId, countryCode, capital)\n      })\n      val schema = StructType(Array(\n         StructField(\"user_id\", LongType, true),\n         StructField(\"country_code\", StringType, true),\n         StructField(\"capital\", StringType, true)\n      ))\n      sqlContext.createDataFrame(rowRDD, schema)\n   }\n} Compile the project using the ‘make build’ command, and upload the resulting JAR to SingleStore Ops: Then create a new pipeline. For now, we will use the test extractor, which sends a constant stream of strings to our transformer; this is very useful for testing your code. Then, choose your BasicTransformer class. Finally, choose the database and table name in SingleStore where you would like your data to go. Once you save your pipeline, it will automatically start running and populating your table with data. You can select rows from the table created in SingleStore to see that it is being populated with your transformed data: Writing your own transformers is an extremely powerful way to manipulate data in real-time pipelines. The code shown above is just one demonstration of how Streamliner gives you the ability to easily pre-process data while leveraging the power of Spark – a fast, general purpose, distributed framework. Check out the SingleStore documentation and the Streamliner examples repo for more examples of the kinds of things you can do with Streamliner.", "date": "2015-11-03"},
{"website": "Single-Store", "title": "iot-use-cases-to-maximize-efficiency-revenue", "author": ["Kevin White"], "link": "https://www.singlestore.com/blog/iot-use-cases-to-maximize-efficiency-revenue/", "abstract": "As enterprises invest billions of dollars in solutions for the Internet of Things (IoT), business leaders seek compelling IoT use cases that tap into new sources of revenue and maximize operational efficiency. At the same time, advancements in data architectures and in-memory computing continue to fuel the IoT fire, enabling organizations to affordably operate at the speed and scale of IoT. In a recent webcast , Matt Aslett, Research Director at 451 Research, shared use cases across six markets where IoT will have a clear impact: Watch the IoT and Multi-model Data Infrastructure Webcast Recording Industrial Automation Seen as the ‘roots of IoT’, organizations in the industrial automation sector are improving performance and reducing downtimes by adding automation through sensors and making data available online. Utilities When people think about IoT, household utilities like thermostats and smoke alarms often come to mind. Smart versions of these devices not only benefit consumers, but also help utility providers operate efficiently, resulting in savings for all parties. Retail Bringing radio-frequency identification (RFID) online allows retailers to implement just-in-time (JIT) stock-keeping to cut inventory costs. Additionally, retailers can provide better shopping experiences in the form of mobilized point-of-sale systems and contextually relevant offers. Healthcare Connected health equipment allows for real-time health monitoring and alerts that offer improved patient treatment, diagnosis, and awareness. Transportation and Logistics IoT is improving efficiency in transportation and logistics markets by providing benefits like just-in-time manufacturing and delivery, as well as improved customer service. Automotive The automobile industry is improving efficiencies through predictive maintenance and internet enabled fault diagnostics. Another interesting use case comes from capturing driving activity, as insurance companies can better predict driver risk and offer discounts (or premiums) based on data from the road. Finding the Internet of Your Things To take advantage of IoT, Matt notes that it is paramount to identify what top priorities are in your specific case by asking the following questions: Are there ‘things’ within your organization that would benefit from greater connectivity? Can better use be made of the ‘things’ that are already network-ready and the data they create? Are there ‘things’ outside the organization that would benefit from greater connectivity? Is there a way to reap value from your customers, partners, or suppliers’ smart devices that would be mutually beneficial? If you answered ‘yes’ to any of these questions, there is a good chance your organization can improve efficiency with an IoT solution. To get started, watch the recording of the IoT and multi-model data infrastructure webcast and view the slides here:", "date": "2015-10-22"},
{"website": "Single-Store", "title": "simulating-billions-stock-market-trades-real-time-analytics", "author": ["Conor Doherty"], "link": "https://www.singlestore.com/blog/simulating-billions-stock-market-trades-real-time-analytics/", "abstract": "I woke up around 7:30 AM on Monday August 24th, checked my phone while lying in bed, and saw that I had lost thousands of dollars in my sleep. Great way to start the week… I was not alone – few investors escaped “Black Monday” unscathed. The past several months have been full of sudden surges and declines in stock prices, and extreme volatility is apparently the “new normal” for global financial markets. Frequent and dramatic market swings put a high premium on access to real-time data. For securities traders, data processing speed and analytic latency can be the difference between getting burned and getting rich. The challenge for trading and financial services companies is not only collecting real-time market data, but also making decisions based on that data in real time. Legacy SQL database technology lacked the speed, scale, and concurrency to ingest market data and execute analytical queries simultaneously. This forced companies into buying and/or building complex and specialized systems for analytics. However, as commercially-available database technology has caught up with the speed of financial markets, it is possible to use SQL to build sophisticated applications and analyze real-time financial data. Simple Quote Generator While we tend to talk about stocks as having a single, definitive price per share, the reality of securities trading is more complex. Buyers and sellers see different prices, known as bid and ask price, respectively. Public bid and ask prices are actually the best available prices chosen among all bids placed by buyers and all asks offered by sellers. Beyond getting the best available deal, trading firms have other incentives for tracking best bid and ask prices. For one, the Securities and Exchange Commision requires that brokers always give their customers the best available price, known as National Best Bid and Offer (NBBO). The script gen.py simulates a distribution of bid and ask quotes for fluctuating stock prices. The model for the generator is very simple and easily modified. The script treats each stock as a random walk. Every iteration begins at a base value, uses a probability distribution function to generate a spread of bids and asks, then randomly fluctuates the stock value. The entire generator is around 100 lines of Python. Python interpretation is the bottleneck when running on even a small SingleStore cluster. Even still, it achieves high enough throughput for a realistic simulation. As performance bottlenecks go, “the database is faster than the data generator” isn’t such a bad problem. Bid and Ask prices for BLAH, generated using gen.py Scale of the Market The trading volume of the New York Stock Exchange (NYSE) is on the order of a few billion shares per day (most trading days in July saw between three and four billion shares change hands). Many deals are more complex than a straight cash-for-equity swap, and many shares can change hands in a given trade, so the number of trades is lower than the volume of shares traded. Note also that there can be many bid and ask offers that are never accepted, so the number of bid and ask offers is higher than the number of trades that actually occurred. While this information does not tell us precisely how many bid and ask quotes the NYSE processes per day, it gives us some clues as to the order of magnitude. Running a modest cluster on EC2 (five nodes; one aggregator, four leaves; all nodes m3.2xlarge instances), I could insert 280 to 290K records per second, or about 6.75 billion quotes in a trading day per aggregator. Resolving Best Bid and Ask with SQL Data loading is only half the story – with SingleStore, you can run SQL queries while loading huge volumes of data. The task of finding the best bid or ask quote becomes trivial with SQL. SELECT\n    ticker,\n    ask_price,\n    ask_size,\n    exchange\nFROM\n    ask_quotes\nWHERE\n    ticker='BLAH'\nORDER BY\n    ask_price ASC, ask_size DESC limit 5;\n\n+--------+-----------+----------+----------+\n| ticker | ask_price | ask_size | exchange |\n+--------+-----------+----------+----------+\n| BLAH   |      4996 |       99 | NASDAQ   |\n| BLAH   |      4996 |       98 | FRA      |\n| BLAH   |      4996 |       97 | FRA      |\n| BLAH   |      4996 |       95 | TYO      |\n| BLAH   |      4996 |       95 | FRA      |\n+--------+-----------+----------+----------+\n5 rows in set (0.00 sec) This query returns the five best (least expensive) BLAH ask prices. On my EC2 cluster, the query finishes in milliseconds. Running the query repeatedly, you see the best ask price change over time (it’s easiest to see with the UP and DOWN tickers, which monotonically increase and decrease, respectively). The query above returns the five best ask prices across all exchanges. You can also find the best ask price per exchange in a single query. SELECT ticker,\n       ask_price,\n       max(ask_size) AS ask_size,\n       exchange\nFROM   ask_quotes\n       INNER JOIN (SELECT Min(ask_price) AS ask_price,\n                          exchange,\n                          ticker\n                   FROM   ask_quotes\n                   WHERE  ticker = 'BLAH'\n                   GROUP  BY exchange) AS t2 using (ticker, exchange, ask_price)\nWHERE  ticker = 'BLAH'\nGROUP  BY exchange LIMIT 5;\n\n+--------+-----------+----------+----------+\n| ticker | ask_price | ask_size | exchange |\n+--------+-----------+----------+----------+\n| BLAH   |      4996 |       93 | NYS      |\n| BLAH   |      4996 |       98 | FRA      |\n| BLAH   |      4996 |       95 | TYO      |\n| BLAH   |      4996 |       99 | NASDAQ   |\n| BLAH   |      4996 |       94 | LON      |\n+--------+-----------+----------+----------+\n5 rows in set (0.02 sec) We write the query as a join because there could be multiple records that are tied for the minimum ask price, and we want to return the one with the largest ask size. Notice how the results are different from those of the previous query, where three of the quotes were from the FRA exchange. In the past, single server, disk-based database architectures struggled with concurrent reads and writes, forcing developers to build specialized applications for real-time applications. With a modern system like SingleStore, much of the complex application logic can be pushed into the database. In fact, finding best bid and ask is just the start of what you can do with SingleStore and market data. Playing (with) the Market With the ability to ingest real-time data and run queries simultaneously, you can interactively explore market trends as they develop. Moreover, because SingleStore is a database, rather than a more narrow stream processing engine, you can access real-time and historical data through a single interface. This simplifies the process of building, for instance, a trading application that models historical data, as well as detects and reacts to real-time market changes. For example, regress.py is an easy real-time analytics appliction that samples “reasonable” asks from the 10K most recent quotes and performs a linear regression (minimizing mean-squared error) to estimate the way a given stock is trending. $ python regress.py BLAH\n\nTicker: BLAH\nTime: 1444676831.12\nSlope: 0.000306657885407\nR squared: 0.312113434918\nStandard error: 2.28775085195e-05 The database is doing most of the work here – ingesting, sorting, and sampling – which allows the script to finish the end-to-end regression computation several times a second. Because the quote data is updating in real time, the slope of the regression line represents the direction that the stock is trending right now. It’s a real-time predictive application in 40 lines of Python! You can also tweak the generator parameters to get different bid/ask spreads and rates of change. For instance, if you decrease the first parameter in the lognormvariate function, it will narrow the bid and ask spread. For a fun, easy extension of the demo, you could write an application that takes advantage of arbitrage opportunities. Playing to Your Strengths If you let the generator and regression script run for a minute, you will see that the model can produce alarming coefficients of determination and standard errors. This makes sense – a random walk generates fluctuations in base price, meaning the data is all noise. If you work in finance, presumably you know more about what drives fluctuation in stock prices than I do, and can create a more sophisticated model. Or, better yet, you can analyze real trade data. The beauty of SingleStore is that, whether you are an expert in distributed systems or not, you can leverage the power of tens or hundreds of servers through a familiar SQL interface. Instead of spending time and resources developing analytics infrastructure, you can focus on actually doing analytics. Visit www.singlestore.com/community-hub to try SingleStore for yourself. You can download all of the code from this demo on GitHub .", "date": "2015-11-05"},
{"website": "Single-Store", "title": "coinalytics-blockchain-analytics", "author": ["Emily Friedman"], "link": "https://www.singlestore.com/blog/coinalytics-blockchain-analytics/", "abstract": "Bitcoin has occupied headlines of technology and business publications over the past several years. The concept of digital currency, or cryptocurrency, rocked the financial industry, and public opinion about the applications of Bitcoin continues to ebb and flow. Today, Bitcoin is being overshadowed by another technology: blockchain. Blockchain is a public ledger for Bitcoin and other cryptocurrencies. This is where the real money is, say payment industry experts. Blockchain is a distributed system, and even better, can never be erased. Here are a few recent headlines: Explore the Blockchain , Ignore the Bitcoin Maximalists – American Banker Technology: Banks seek the key to blockchain – Financial Times Forget Bitcoin — What Is the Blockchain and Why Should You Care? – Re/code New companies are emerging to capitalize on the increasing interest in blockchain technologies. One such company is Coinalytics . The company has focused its efforts on building a platform that can unlock actionable insights from blockchain.  Coinalytics utilizes SingleStore as the foundational database to power these real-time insights. We sat down with co-founder and CEO, Fabio Federici, to learn more about the company and how they fit into the blockchain ecosystem. What did you see in the finance and payments landscape that motivated you to start Coinalytics? At the time we started Coinalytics, companies were struggling to access blockchain data, which meant all that data was sitting untouched. We saw a great opportunity to build a platform that allows anyone to access the blockchain and derive meaningful insights. For us, the vision is to help payments companies operationalize data from blockchain. Ultimately, what that does, is enable companies to make better decisions and developers to write smarter applications. What sets the Coinalytics platform apart? We offer the ability to query our underlying data with a SQL-like query language, compared to most REST APIs which only allow for key/value lookups and maybe a couple of hard-coded filters. Our first offerings include simple APIs and visual interfaces that reduce the friction in the adoption of digital currencies by providing real-time compliance and risk assessment to leading wallet providers, payment services and exchanges. Who are some of your customers? Currently we are focused on companies in the Bitcoin space like exchanges, payment processors or merchants accepting Bitcoin. Compliance officers and risk managers use our platform to conduct enhanced due diligence or perform anti-money laundering investigations on suspicious transactions. As the very traditional financial industry starts opening up to Bitcoin and the blockchain, we will continue to carve out a space for our company and our product. We enable established institutions to engage with this emerging ecosystem in a safe and simple manner. How do you use SingleStore to propel your platform forward? SingleStore is a fully relational, SQL database. When you are in the data business, you need a database that gives you the ultimate in performance – the SingleStore in-memory rowstore does just that, allowing us run analytics quickly and efficiently on blockchain data. SingleStore is also distributed, so we can scale out and add nodes as we evolve while keeping our costs low. Moreover, the database is fault-tolerant, familiar to us because it is wire protocol compatible with MySQL, and offers strong support for Apache Spark. Oh and one more thing I would add – beyond the in-memory rowstore, SingleStore offers  a disk-based columnstore, which is ideal for our historical data. What does your SingleStore deployment look like? Currently, we are deployed across two availability zones on Google Compute Engine. We have automated rowstore snapshots every 12 hours with attached SSDs, so we can very quickly (differentially) snapshot and store cheaply. The next step for us is attaching the backup disk features as part of our snapshot workflow which will help us continue to optimize costs. What opportunities has SingleStore opened up for Coinalytics? First and foremost, SingleStore has really given us the ability to perform real-time analytics – real-time is the key component here. Companies need real time access to data, especially in the payments industry. Now, we have more fine-tuned control over the queries we perform and cost/performance trade-offs. For example, we can decide if two single column indices is good enough or if we also need a compound index of both columns. With SingleStore we are able to explore flexible SQL queries, as well as dive into Spark, thanks to the SingleStore Spark connector . Everyone wants to play around with Spark these days, ourselves included. What is next for you guys? We are still in the early days of blockchain technology and many use cases are yet to be imagined. That said, we are currently seeing a lot of interest in using blockchain as a settlement layer in the financial industry, be it FX trading or asset exchanges. Beyond those financial use cases, we see companies using the transparency the blockchain provides to verify the provenance of products for the entire supply chain, notary services that proof the ownership of any digital document, or even completely decentralized marketplaces. We also see a lot of potential opportunities around a combination of blockchain with the Internet of Things for machine-to machine transactions. In our eyes, real-time intelligence and access to actionable insights are critical factors in the success of every application. Read more about how Coinalytics is powering blockchain insights here: https://singlestore.com/media-hub/releases/coinalytics-taps-memsql-for-blockchain-analytics/ Get The SingleStore Spark Connector Guide The 79 page guide covers how to design, build, and deploy Spark applications using the SingleStore Spark Connector. Inside, you will find code samples to help you get started and performance recommendations for your production-ready Apache Spark and SingleStore implementations. Download Here", "date": "2015-11-10"},
{"website": "Single-Store", "title": "real-time-data-pipelines-webcast", "author": ["Kevin White"], "link": "https://www.singlestore.com/blog/real-time-data-pipelines-webcast/", "abstract": "In the era of universal connectivity, the faster you can move data from point A to B the better. Equipping your organization with the ability to make frequent decisions in an instant offers information and intelligence advantages, such as staying one step ahead of the competition. This is especially important when incoming data is arriving at a relentless pace, in high volume, and from a variety of devices. As our customers tap into new sources of data or modify to existing data pipelines, we are often asked questions like: What technologies should we consider? Where can we reduce data latency? How can we simplify data architectures? To eliminate the guesswork, we teamed up with Ben Lorica, Chief Data Scientist at O’Reilly Media to host a webcast centered around building real-time data pipelines. Watch the recorded webcast to learn: Ideal technology stacks for building real-time data pipelines How to simplify Lambda architectures How to use memory-optimized technologies like Kafka, Spark, and in-memory databases to build real-time data pipelines Use cases for real-time workloads, and the value they offer Examples of data architectures used by companies like Pinterest and Comcast Webcast Recording Webcast Slides About the Presenters Eric Frenkiel, CEO & Co-Founder, SingleStore — Eric Frenkiel co-founded SingleStore and has served as CEO since inception. Before SingleStore, Eric worked at Facebook on partnership development. He has worked in various engineering and sales engineering capacities at both consumer and enterprise startups. Ben Lorica, Chief Data Scientist, O’Reilly Media — Ben Lorica is the Chief Data Scientist and Director of Content Strategy for Data at O’Reilly Media, Inc. He has applied Business Intelligence, Data Mining, Machine Learning and Statistical Analysis in a variety of settings including Direct Marketing, Consumer and Market Research, Targeted Advertising, Text Mining, and Financial Engineering. His background includes stints with an investment management company, internet startups, and financial services.", "date": "2015-11-13"},
{"website": "Single-Store", "title": "everyone-deserves-nice-things", "author": ["Douglas Butler"], "link": "https://www.singlestore.com/blog/everyone-deserves-nice-things/", "abstract": "Software is eating the world! It’s a data explosion! The Internet is now of Things! Scratch that, even better – it is of Everything! Did Big Data just call out IoT on Twitter? Click here to find out. [ 1 ] I kid the Internet. In all seriousness, what a magical time we live in. Moore’s Law means cheap hardware, then next thing you know, Cloud. Internet ubiquity invites the globe to the party. Feats of software engineering that were impossible for nation-states to pull off a decade ago are de rigeur for clever teens. On their mobile phones. I became disillusioned after a few years as a sales engineer for big ticket software products. I talked to so many operations people who spent all their time putting out fires instead of automating and improving. Even worse, it seemed nearly all the actual users were sad about their applications. These amazing modern conveniences were supposed to make our lives easier, not more difficult. The actual outcome was the exact opposite of the expected outcome. Oh irony! These thoughts and feelings led me to join Atlassian in 2008. If you have never heard of that company, I reckon you have at least heard of JIRA, Confluence or HipChat. Here was a group making software that people were using voluntarily. Even tiny teams could implement it without breaking the bank, or gratis if they were building open source. Furthermore, the company was totally focused on software development. Agile was rising to prominence. Git went from non-existence to dominance in an eye blink. Software development was undergoing a sea change in the right direction. This is what brings me to SingleStore. Companies like Google, Facebook, eBay, and Netflix had to feed their databases and infrastructure all the steroids to meet the challenge of true web scale. They, among others, pioneered new ways to ingest and work with previously unimaginable mountains of data. Did you use metric prefixes above giga- in your daily life 10 years ago? Nor did I. Yottabytes of records, anyone? Being able to handle massive data sets, using them to make real-time decisions resulting in delighted customers is the new nice thing that I believe everyone deserves. That is why I am elated to join SingleStore, focusing on the Community Edition. Imagine what you could build better, stronger and faster if the database underneath was ready to handle anything thrown at it. If you are already using SingleStore Community Edition, I am very keen to hear what you’re doing with it. If you have a moment, please take this very short survey . Don’t hesitate to hit me up on our public Slack , email or elsewise. And away we go… [ 1 ] – Citations Why Software Is Eating The World Gartner Says Business Intelligence and Analytics Need to Scale Up to Support Explosive Growth in Data Sources The Internet of Things is revolutionising our lives, but standards are a must The Next Big Thing for Tech: The Internet of Everything", "date": "2015-11-17"},
{"website": "Single-Store", "title": "how-tradelab-enables-real-time-bidding-memsql", "author": ["Lesia Myroshnichenko"], "link": "https://www.singlestore.com/blog/how-tradelab-enables-real-time-bidding-memsql/", "abstract": "Tradelab, a programmatic marketing platform company based in France, uses SingleStore to process and analyze real-time bidding data for hundreds of customers. Challenge: NoSQL Data Latency The Tradelab real-time ad serving platform requires a heavy mixed read/write workload, and the NoSQL database they had in place was introducing unnecessary data latency into the ad-bidding process. The company began searching for a replacement – a true real-time data management solution with two key criteria: Capacity to store more data in-memory, granting real-time performance Ability to analyze relational and JSON data together to power the Tradelab platform Solution: High Performance Database with Horizontal Scale As a fully relational, distributed SQL database, SingleStore was the perfect solution to these latency challenges. SingleStore scales out horizontally on commodity hardware, allowing Tradelab to add nodes as it grows. Tradelab can also scale while supporting mixed reads for ad serving, and writes for impression and campaign tracking. In addition, SingleStore increases system performance by 30x, delivering millisecond response times, and reducing overall latency and cost. Tradelab was able to replace its existing solution with SingleStore in a snap – SingleStore is fully wire protocol compatible with MySQL, integrating nicely into the existing Tradelab technology stack. “We use SingleStore as an in-memory solution to analyze large amounts of data without sacrificing our competitive performance,” said Vincent Mady, CTO, Tradelab. “The effectiveness of our platform has increased significantly: we’re now able to keep pace with increasing data volume and provide faster insights for our customers. Ultimately, this is a better experience for their users.” The Tradelab real-time analytics and bidding software now runs on SingleStore, which has dramatically increased the efficiency of its media traders and the accuracy of ad campaigns. To learn more about how SingleStore provides Tradelab customers a competitive edge with instantaneous ad recommendations, read the official News Release .", "date": "2015-11-19"},
{"website": "Single-Store", "title": "using-oracle-and-memsql-together", "author": ["Steven Camina"], "link": "https://www.singlestore.com/blog/using-oracle-and-memsql-together/", "abstract": "We often hear “How can I use SingleStore together with my Oracle database?” As a relational database, SingleStore is similar to an Oracle database, and can serve as an alternative to Oracle in certain scenarios. Here is what sets SingleStore apart: SingleStore is a distributed system, designed to run on multiple machines with a massively parallel processing architecture. An Oracle database, on the other hand, resides in a single, large machine, or a smaller fixed cluster size. SingleStore has two primary data stores: an in-memory rowstore and a disk-based columnstore. An Oracle database, on the other hand, has one primary data store – a disk-based rowstore. Oracle does have an in-memory option that allows users to make a columnar copy of its disk-based rowstore data in-memory, but even with that, all in-memory data must first be created on disk. With its distinct architecture, SingleStore complements Oracle in several cases where users can deploy the two databases side by side. These include: SingleStore as the real-time analytics engine for an Oracle database SingleStore as the ingest layer for an Oracle database SingleStore as the stream processing layer for an Oracle database SingleStore as the Real-Time Analytics Engine for an Oracle Database Enterprises typically have Oracle databases in place for transactional (OLTP) workloads. In these cases, batch processes (ETL) are typically run at the end of each day to transfer data into a separate Oracle data warehouse for analytical (OLAP) workloads. Within the Oracle data warehouse, data is then aggregated and rolled up for efficient querying. SingleStore performance eliminates the need for batch processing. Data can be copied from the OLTP Oracle database into SingleStore immediately through Oracle GoldenGate or another change data capture tool, and analytical queries can be performed in real-time. By eliminating ETL, SingleStore minimizes the time between data coming into the system and analysis being gathered from that data set. Ultimately, this enhances enterprises’ ability to make decisions in real time. SingleStore as the Ingest Layer for an Oracle Database For many enterprises using Oracle databases, the rate at which data is inserted into the database can be too large for an affordable Oracle system to handle. Ingest performance for an Oracle database is limited by its disk; all inserts into Oracle need to be persisted to disk at each database commit. The Oracle in-memory option does not help with inserts, as “in-memory” data is just a copy of the disk-resident data. As such, any insert into an Oracle database is limited by disk speeds. For optimal ingest that takes full advantage of in-memory processing, you need a pure in-memory database like SingleStore. Using SingleStore as the data ingest layer on top of an Oracle database allows ingest at in-memory speed. SingleStore as the Stream Processing Layer for an Oracle Database Streaming data has become quite popular, yet the Oracle database was designed long ago, well before data streams from sources like Apache Kafka came about. These streams often have unstructured and high volume data that requires real-time transformation. Processing a data stream with these traits requires a specially designed system. To that end, SingleStore provides an integrated Apache Spark solution called Streamliner. Streamliner makes it easy to deploy Spark within SingleStore for ingesting and enriching data streams. With Streamliner, SingleStore can serve as the stream processing layer in front of an Oracle database. The Avant-Garde of New Relational Databases Alex Woodie cites a recent Gartner research report from Adam Ronthal in Meet the Avant-Garde of New Relational Databases ; the Gartner report states that “over the next three years, 70 percent of new projects requiring ‘scale-out elasticity, distributed processing and hybrid cloud capabilities for relational applications, as well as multi-data-center transactional consistency’, will prefer an ‘emerging’ database vendor over traditional vendors.” Enterprises with existing Oracle databases should consider adding an ‘emerging’ database like SingleStore into the mix for benefits of scale out, distributed processing, and memory-first technology. Learn more about SingleStore at http://www.singlestore.com , and download our Community Edition or free Enterprise Trial to get started today!", "date": "2015-12-11"},
{"website": "Single-Store", "title": "modern-database-characteristics", "author": ["Kevin White"], "link": "https://www.singlestore.com/blog/modern-database-characteristics/", "abstract": "Many legacy database systems are not equipped for modern applications. Near ubiquitous connectivity drives high-velocity, high-volume data workloads – think smartphones, connected devices, sensors – and a unique set of data management requirements. As the number of connected applications grows, businesses turn to in-memory solutions built to ingest and serve data simultaneously. Bonus Material: Free O’Reilly Ebook – learn how to build real-time data pipelines with modern database architectures To support such workloads successfully, database systems must have the following characteristics: Modern Database Characteristics Ingest and Process Data in Real Time Historically, the lag time between ingesting data and understanding that data has been hours to days. Now, companies require data access and exploration in real time to meet consumer expectations. Subsecond Response Times As organizations supply access to fresh data, demand for access rises from hundreds to thousands of analysts. Serving this workload requires memory-optimized systems that process transactions and analytics concurrently. Anomaly Detection as Events Occur Reaction time to an irregular event often correlates with a business’s financial health. The ability to detect an anomaly as it happens helps companies avoid massive losses and capitalize on opportunities. Generate Reports Over Changing Datasets Today, companies expect analytics to run on changing datasets, where results are accurate to the last transaction. This real-time query capability has become a base requirement for modern workloads. Real-Time Use Cases Today, companies are using in-memory solutions to meet these requirements. Here are a few examples: Pinterest: Real-Time Analytics Pinterest built a real-time data pipeline to ingest data into SingleStore using Spark Streaming. In this workflow, every Repin is filtered and enriched by adding geolocation and Repin category information. Enriched data is persisted to SingleStore and made available for query serving. This helps Pinterest build a better recommendation engine for showing Repins and enables their analysts to use familiar a SQL interface to explore real-time data and derive insights. Tapjoy: Personalization Tapjoy optimizes ad performance by taking advantage of the speed and scalability of in-memory computing. With the processing power to run 60,000 queries per second at a response time of less than ten milliseconds, Tapjoy is able to cross-reference user data and serve higher-performing ads to more than 500 million global users. David Abercrombie, Data Analytics Engineer, details Tapjoy’s database architecture and application in this recorded session from the 2015 In-Memory Computing Summit: Novus: Portfolio Management Novus supports more than 100 of the world’s top investment firms, helping users understand investment strengths and risks by providing a “moneyball-like” view of investment data. By taking advantage of a memory-optimized database system, Novus can deliver instant answers to hundreds of analysts querying their dataset. Noah Zucker, Vice President, shares how Novus built a scalable portfolio investment platform in this video: Concluding Thoughts As more data comes online, organizations will rush to build systems that can rapidly ingest data while simultaneously making it accessible for analysis. To help you get there successfully, we teamed up with O’Reilly Media to publish an ebook on Building Real-Time Data Pipelines through In-Memory Architectures. Download it for free here:", "date": "2015-12-15"},
{"website": "Single-Store", "title": "spark-sql-performance-boost", "author": ["Emily Friedman"], "link": "https://www.singlestore.com/blog/spark-sql-performance-boost/", "abstract": "This month’s SingleStore Ops release includes performance features for Streamliner , our integrated Apache Spark solution that simplifies creation of real-time data pipelines. Specific features in this release include the ability to run Spark SQL inside of the SingleStore database, in-browser Python programming, and NUMA-aware deployments for SingleStore. We sat down with Carl Sverre, SingleStore architect and technical lead for Ops development, to talk about the latest release. Q: What’s the coolest thing about this release for users? I think the coolest thing for users is that we now support Python as a programming language for building real-time data pipelines with Streamliner. Previously, users needed to code in Scala – Scala is less popular, more constrained, and harder to use. In contrast, Python syntax is widely in use by developers, and has a broad set of programming libraries providing extensibility beyond Spark. Users can import Python libraries like Numpy, Scipy, and Pandas, which are easy to use and feature-rich compared to corresponding Java / Scala libraries. Python also enables users to prototype a data pipeline much faster than with Scala. To allow users to code in Python, we built SingleStore infrastructure on top of PySpark and also implemented a ‘pip’ command that installs any Python package across machines in a SingleStore cluster. Q: Why focus on Python, the “language of data science”? We believe that Python is the way most generalist developers know how to write code. In fact, here at SingleStore, much of the software we write outside of the core database is written in Python – this includes Ops, our interactive management tool; Psyduck, the code testing tool; and much more. We know that developers experimenting with their own data pipelines (many using Streamliner) want to get up and running quickly. Naturally, they seek the most intuitive programming language to do so: Python! Q: Can you dig into SQL pushdown on a technical level? Spark SQL is the Apache Spark module for working with structured data. As an example, with Spark SQL you can do a command like: customersDataFrame.groupBy(\"zip_code\").count().show() It counts customers in a Spark data table (dataframe) grouped by zip code. The same query can also be run through Spark SQL, for example: sqlContext.sql(“select count(*) from customers group by zip-code”) Now, we can boost Spark SQL performance by pushing the majority of computation into SingleStore. How do we do it? First, we process the Spark SQL operator tree and turn it into SQL syntax that SingleStore understands, then execute that query directly against the SingleStore database, where performance on structured data is strictly faster than Spark. By leveraging the optimized SingleStore engine, we are able to process individual Spark SQL queries much faster than Spark can. Q: What are you most proud of in this release? I am most proud of the performance boost from running Spark SQL in SingleStore. It is awesome that we can leverage the SingleStore in-memory, distributed database to boost Spark performance without users having to change any of their application code. With SingleStore, Spark SQL is just faster. Get The SingleStore Spark Connector Guide The 79 page guide covers how to design, build, and deploy Spark applications using the SingleStore Spark Connector. Inside, you will find code samples to help you get started and performance recommendations for your production-ready Apache Spark and SingleStore implementations. Download Here", "date": "2015-12-16"},
{"website": "Single-Store", "title": "real-time-data-predictions-2016", "author": ["Eric Frenkiel"], "link": "https://www.singlestore.com/blog/real-time-data-predictions-2016/", "abstract": "Prediction 1. The industrial internet moves to real-time data pipelines The industrial internet knits together big data, machine learning, and machine-to-machine communications to detect patterns and adjust operations in near real time. Soon the industrial internet will expand by definition to include the Internet of Things. The detection of patterns and insights often comes with a price: time. While the goal of machine learning is to develop models that will prove useful, dealing with large data sets means it can take days, weeks or months to reach meaningful discoveries. We predict that in the very near future, real-time data streams will transform what is possible across the industrial internet, so users can ask critical questions, adjust a process, or see a pattern in the moment. Entire industries such as energy, pharmaceutical and even agriculture will be dramatically impacted by the ability to analyze real-time and historical data together to make business decisions faster. Prediction 2. Consumer visibility into business gets granular The world today moves at a different pace than a generation ago. Applications on handheld devices that move us through our day tell us where to eat, how to get from point A to point B, what is the fastest route, everything that is happening in the world, and even what our friends are buying. Data is driving the course of business – and dramatically impacting the consumer experience. We predict that in a few short years, consumer visibility into business operations will get more granular. For example, look at the transparency that already exists with companies such as Blue Apron and FedEx. Not only do we know exactly what is on the menu week to week at Blue Apron, we can opt out if it is something we do not like, or adjust the delivery times. And FedEx allows consumers to track the entire journey of a package and sometimes even reroute a package to a new delivery destination. More and more companies will adopt transparency for consumers, and in doing so, will build brand loyalty and satiate growing consumer appetite for on-demand services. Prediction 3. The cost of doing business declines Just a few years ago the cost of storage was a board room conversation, where CIOs had to justify the rising cost of IT associated with growing data volumes. For many CIOs, storage was an IT line item that was on track to outpace profitability. Today, the conversation around data storage has changed. Storage is cheap and highly accessible—any business unit within an organization can tap into the cloud. Access to commodity hardware makes rapidly scaling a business possible. The cost of doing business will further decline as in-memory technologies set computing on a new course. While companies like Amazon provide access to more than a terabyte of memory for just a few dollars an hour via public or private clouds, other companies have created technology that provides relatively low-cost access to terabytes of non-volatile memory, which developers can use instead of storage. In-memory databases use vast stores of memory close to the compute to rapidly process data. Access to more memory means that programmers will be able to write different types of software—propelling the industry toward what is perhaps a new era of applications built on commodity hardware. We are already seeing verticalized trends for data analytics and applications that can serve up real-time value across healthcare, manufacturing, and retail. What’s next? Prediction 4. The crowdsourcing of analytics The world of artificial intelligence (AI) used to lie solidly in the hands of physicists, scientists and researchers and well beyond mass population. Today, AI has shifted and is empowering people all over the world to participate in the analytics process. Crowdflower, for example – now Appen – blends a human-in-the-loop process alongside data science and machine learning to generate insights. Kaggle, another company crowdsourcing for analytics, has built one of the world’s largest communities of data scientists to solve complex challenges through a competitive approach to data science. Data analysis will be more pervasive, and new applications that empower the data collection process will be broadly embraced. Consider the power of Waze and INRIX, both in use today to crowdsource traffic congestion. While the requirement is the participation of a social community at large, the upside potential is felt much more broadly. The same data collection process could be applied to many more applications to affect and improve society.", "date": "2015-12-22"},
{"website": "Single-Store", "title": "rethinking-lambda-architecture", "author": ["Dale Deloy"], "link": "https://www.singlestore.com/blog/rethinking-lambda-architecture/", "abstract": "Big data, as a concept and practice, has been around for quite some time now. Most companies have responded to the influx of data by adapting their data management strategy. However, managing data in real time still poses a challenge for many enterprises. Some have successfully incorporated streaming or processing tools that provide instant access to real-time data , but most traditional enterprises are still exploring options. Complicating the matter further, most enterprises need access to both historical and real-time data, which require distinct considerations and solutions. Of the many approaches to managing real-time and historical data concurrently, the Lambda Architecture is by far the most talked about today. Like the physical aspect of the Greek letter it is named for, the Lambda architecture forks into two paths: one is a streaming (real-time) path, the other a batch path. Thus, it accommodates real-time high-speed data service along with an immutable data lake. Oftentimes a serving layer sits on top of the streaming path to power applications or dashboards. A Fork in the Road Many Internet-scale companies, like Pinterest , Zynga, Akamai, and Comcast have chosen SingleStore to deliver the high-speed data component of the Lambda architecture. Some customers have chosen to fork the input stream in order to push data into SingleStore and a data lake, like HDFS, in parallel. Here is an example of the Comcast Lambda Architecture: The great thing about SingleStore is that it can fulfill both sides of the Lambda architecture, not just the real-time component. Some customers use the SingleStore in-memory rowstore to support real-time streaming, and then use the disk-based columnstore as the batch service and data lake. Here is an example of a financial services customer using SingleStore for both layers: Real Time Analytics In this era of ubiquitous big data, it is not enough for companies to merely process data. Analyzing that data to detect patterns, which can be immediately applied to maximizing operational efficiency, is the real driver of business value. SingleStore delivers real-time analytics on a rapidly changing data set, making it an ideal match for the characteristics of the Lambda Architecture speed service. Other data stores have limitations that inhibit high-speed data ingestion, lack analytical capabilities, or cannot scale affordably. SingleStore delivers a complete solution: the ability to handle millions of transactions per second while simultaneously performing complex multi-table join queries. Let’s dig into some of the features that make SingleStore a great solution for implementing the Lambda architecture. Scalability SingleStore uses a distributed shared nothing architecture that scales on commodity hardware and local storage, supporting petabytes of data. SingleStore is a memory-first, relational database that also offers a disk-based columnstore. In-memory optimization delivers high-speed data ingestion while simultaneously delivering analytics on the changing data set. The disk-based columnstore provides historical data management and access to historical data trends to leverage in combination with the “hot” data to deliver real-time analytics. Multi-model, Multi-mode SingleStore supports the ingestion of unstructured, structured and semi-structured data . Flexibility to align a structure to data in support of analytics meets the business requirements of the operation. Real-time analytics requires a real-time data structure, which SingleStore supports through a fully relational model. Furthermore, SingleStore supports the ingestion of unstructured and semi-structured (JSON) data into key-value pairs. Full ANSI SQL support makes SingleStore readily accessible to data analysts, business analysts and data scientists reducing application code requirements. Plugging data visualization and query tools into the analytics architecture delivers immediate value from data to the business. SingleStore also has extended SQL including JSON support. Traversing a JSON document is similar to SQL with extensions to traverse the key-value pairs. Open Source Connectors SingleStore offers users several connectors for smooth integration with other data sources. One example is SingleStore Streamliner : a fully integrated Apache Spark solution. Streamliner provides easy deployment of Spark — a critical component for building real-time data pipelines that delivers advanced data enrichment and transformation. Another important connector is the SingleStore Loader, which can easily important data from HDFS, as well as import and synchronize data from Amazon S3. Marketplace Perspective Customers are investing in SingleStore as they realize the value of data in real time along with the power of SQL to analyze it. Pinterest, Akamai, Zynga, Comcast, and Tapjoy have all deployed SingleStore to power mission-critical applications. Customers from many industries have invested either for performance improvement, the power and familiarity of SQL, or the low cost to scale (shared nothing commodity servers and storage). These include financial services, advertising technology, energy, automotive, and retail, among others. To learn more about implementing SingleStore for Lambda Architecture, watch David Abercrombie, Data Analytics Engineer at Tapjoy, share his story: Download the Community Edition of SingleStore today. It is free, fully functional and eager to tackle your workloads: singlestore.com/free", "date": "2015-12-23"},
{"website": "Single-Store", "title": "what-is-lambda-architecture", "author": ["Carlos Bueno"], "link": "https://www.singlestore.com/blog/what-is-lambda-architecture/", "abstract": "The surest sign you have invented something worthwhile is when several other people invent it too. That means the creative pressure that gave birth to the idea is more general than your particular situation. Even when faced with the same pressures, people will approach an idea in different ways. When Jay Kreps was developing Kafka at LinkedIn, he called it The Log . Facebook (being Facebook) created several independent implementations of “stream-oriented processing”, including Puma and TailerSwift. Twitter has the adorably-named Summingbird. The jargon we seem to be converging on for these kinds of systems is the Lambda Architecture . [ 1 ] Lambda in a Nutshell The gist of Lambda is to model all of the stuff that goes on in a complex computing system as an ordered, immutable log of events. Processing the data (say, totting up the number of website visitors) is done as a series of transformations that output to new tables or streams. It is important to keep the input unchanged. By breaking data processing into independent pieces, each with a defined input and output, you get closer to the ideal of purely functional programming. Writing and testing each piece is made simpler and parallelization can be automated. Parts of the dataflow can be replayed (say, when code changes or machines fail) and tinker-toyed together with other flows. This is a very nice property to build into your system. A long time ago, people who did 3D modeling would “carve” digital blocks into the shapes they wanted. If they wanted to undo something 10 steps back, they were largely out of luck. Then 3DStudio introduced a brilliant feature it called the “transform stack”. The stack records every change to an object separately, and applies them in real time. This allows the modeler to modify, add, remove, and even reorder their changes on the fly. So far, all of this is just good data engineering hygiene. Any well-run batch processing or map/reduce system will follow the same principles. There’s nothing special about stream-processing that makes immutable data flows work better. The Trick of Lambda The special trick that makes Lambda “Lambda” is the technique of writing data to two places. That’s one reason why the logo for it is the symbol “λ”. In effect, one half of a Lambda system optimizes for space and the other optimizes for time. Lambda systems incorporate a slower, high-capacity batch-processing system, and a faster stream processing track. This allows existing map/reduce systems to be upgraded with a new fast track. It also leaves the system of record untouched, which is the main selling point for CIOs looking to improve the responsiveness of their data flows. This is an old and venerable technique. Document search engines of a certain age (eg, Yahoo’s Vespa) often have a “slow” index that is very compact but difficult to update. To compensate they will also have a “fast” index, perhaps in memory, where changes are cached until the next index rebuild. Under the hood a search will consult both indexes and merge the results. The thing is, design feature was an evolution on top of the slower batched index. It is not certain that you would do it that way if you were building from scratch. Lucene, for example, uses an incremental index for everything. Jay Kreps, in a thoughtful critique of Lambda, points out that you need two implementations of the same queries and data flow. And of course, you need two copies of the data. If you had a better streaming system, one that could “read a table” simply by replaying a stream, why would you need both kinds of system? The Lambda Architecture Isn’t The Lambda Architecture isn’t. What it is is a sensible set of data engineering practices, which you should be doing anyway, plus a clever (but transitional) double-write approach to help people add a low-latency fast track to existing big data systems. I actually think it is good to have all of these ideas wrapped up under a catchy rubric, as long as it’s not fetishized too much. [ 1 ] The term originally comes from the Greek word “λάμδα”, which means “to write to two places” No, I’m kidding. Lambda is actually the Greek word for “monoid in the category of endofunctors”.", "date": "2015-12-28"},
{"website": "Single-Store", "title": "linux-off-cpu-investigation", "author": ["Alex Reece"], "link": "https://www.singlestore.com/blog/linux-off-cpu-investigation/", "abstract": "The Setup As a performance engineer at SingleStore, one of my primary responsibilities is to ensure that customer Proof of Concepts (POCs) run smoothly. I was recently asked to assist with a big POC, where I was surprised to encounter an uncommon Linux performance issue. I was running a synthetic workload of 16 threads (one for each CPU core). Each one simultaneously executed a very simple query ( select count(*) from t where i > 5 ) against a columnstore table. In theory, this ought to be a CPU bound operation since it would be reading from a file that was already in disk buffer cache. In practice, our cores were spending about 50% of their time idle: In this post, I’ll walk through some of the debugging techniques and reveal exactly how we reached resolution. What were our threads doing? After confirming that our workload was indeed using 16 threads, I looked at the state of our various threads. In every refresh of my htop window, I saw that a handful of threads were in the D state corresponding to “Uninterruptible sleep”: PID USER      PRI  NI  VIRT   RES   SHR S CPU% MEM%   TIME+  Command\n55308 neil       21   1 11.6G 3825M 36804 S 530.  3.4 21h44:11 ./memsqld\n55969 neil       20   0 11.5G 3825M 36804 R 35.8  3.4 30:31.52 ./memsqld\n56121 neil       20   0 11.6G 3825M 36804 D 35.8  3.4 34:55.03 ./memsqld\n56120 neil       20   0 11.6G 3825M 36804 D 34.4  3.4 36:27.53 ./memsqld\n56109 neil       20   0 11.6G 3825M 36804 R 33.7  3.4 31:57.14 ./memsqld\n56088 neil       20   0 11.6G 3825M 36804 D 33.7  3.4 50:08.92 ./memsqld\n56099 neil       20   0 11.6G 3825M 36804 D 33.7  3.4 31:58.06 ./memsqld\n56069 neil       20   0 11.6G 3825M 36804 R 33.1  3.4 31:01.54 ./memsqld\n56101 neil       20   0 11.6G 3825M 36804 D 32.4  3.4 28:41.27 ./memsqld\n56104 neil       20   0 11.6G 3825M 36804 D 32.4  3.4 31:54.41 ./memsqld\n55976 neil       20   0 11.5G 3825M 36804 D 32.4  3.4 30:18.72 ./memsqld\n55518 neil       20   0 11.5G 3825M 36804 D 32.4  3.4 29:48.51 ./memsqld\n55966 neil       20   0 11.5G 3825M 36804 D 32.4  3.4 36:51.50 ./memsqld\n55971 neil       20   0 11.5G 3825M 36804 R 32.4  3.4 27:22.96 ./memsqld\n55959 neil       20   0 11.5G 3825M 36804 D 32.4  3.4 38:13.50 ./memsqld\n55975 neil       20   0 11.5G 3825M 36804 R 31.7  3.4 30:18.38 ./memsqld Why were we going off CPU? At this point, I generated an off-cpu flamegraph using Linux perf_events to see why we entered this state. Off-CPU means that instead of looking at what is keeping the CPU busy, you look at what is preventing it from being busy by things happening elsewhere (e.g. waiting for IO or a lock). The normal way to generate these visualizations is to use perf inject -s , but the machine I tested on did not have a new enough version of perf . Instead I had to use an awk script I had previously written: $ sudo perf record --call-graph=fp -e 'sched:sched_switch' -e 'sched:sched_stat_sleep' -e 'sched:sched_stat_blocked' --pid $(pgrep memsqld | head -n 1) -- sleep 1\n    [ perf record: Woken up 1 times to write data ]\n    [ perf record: Captured and wrote 1.343 MB perf.data (~58684 samples) ]\n$ sudo perf script -f time,comm,pid,tid,event,ip,sym,dso,trace -i sched.data | ~/FlameGraph/stackcollapse-perf-sched.awk | ~/FlameGraph/flamegraph.pl --color=io --countname=us >off-cpu.svg Note: recording scheduler events via perf record can have a very large overhead and should be used cautiously in production environments. This is why I wrap the perf record around a sleep 1 to limit the duration. In an off-cpu flamegraph, the width of a bar is proportional to the total time spent off cpu. Here we see a lot of time is spent in rwsem_down_write_failed . From the repeated calls to rwsem_down_read_failed and rwsem_down_write_failed , we see that culprit was mmap contending in the kernel on the mm->mmap_sem lock: down_write(&mm->mmap_sem);\nret = do_mmap_pgoff(file, addr, len, prot, flag, pgoff,\n                    &populate);\nup_write(&mm->mmap_sem); This was causing every mmap syscall to take 10-20ms (almost half the latency of the query itself). SingleStore was so fast that that we had inadvertently written a benchmark for Linux mmap ! $ sudo perf trace -emmap --pid $(pgrep memsqld | head -n 1) -- sleep 5\n    ... <snip> ...\n    12453.692 ( 9.060 ms): memsqld/55950 mmap(len: 1265444, prot: READ, flags: PRIVATE|POPULATE, fd: 65) = 0x7f95ece9f000\n    12453.777 ( 8.924 ms): memsqld/55956 mmap(len: 1265444, prot: READ, flags: PRIVATE|POPULATE, fd: 67) = 0x7f95ecbf5000\n    12456.748 (15.170 ms): memsqld/56112 mmap(len: 1265444, prot: READ, flags: PRIVATE|POPULATE, fd: 77) = 0x7f95ec48d000\n    12461.476 (19.846 ms): memsqld/56091 mmap(len: 1265444, prot: READ, flags: PRIVATE|POPULATE, fd: 79) = 0x7f95ec1e3000\n    12461.664 (12.226 ms): memsqld/55514 mmap(len: 1265444, prot: READ, flags: PRIVATE|POPULATE, fd: 84) = 0x7f95ebe84000\n    12461.722 (12.240 ms): memsqld/56100 mmap(len: 1265444, prot: READ, flags: PRIVATE|POPULATE, fd: 85) = 0x7f95ebd2f000\n    12461.761 (20.127 ms): memsqld/55522 mmap(len: 1265444, prot: READ, flags: PRIVATE|POPULATE, fd: 82) = 0x7f95ebfb9000\n    12463.473 (17.544 ms): memsqld/56113 mmap(len: 1265444, prot: READ, flags: PRIVATE|POPULATE, fd: 75) = 0x7f95eb990000\n    ... <snip> ... The fix was simple — we switched from using mmap to using the traditional file read interface. After this change, we nearly doubled our throughput and became CPU bound as we expected: For more information and discussion around Linux performance, check out the original post on my personal blog . Download SingleStore Community Edition to run your own performance tests for free today: singlestore.com/free", "date": "2016-01-07"},
{"website": "Single-Store", "title": "iot-infrastructure", "author": ["Gary Orenstein"], "link": "https://www.singlestore.com/blog/iot-infrastructure/", "abstract": "The infrastructure of IoT will have a real-time database behind every sensor. Soon every device with a sensor will blend seamlessly into the Internet of Things , from drones to vehicles to wearables. Device and sensor count predictions range from billions to trillions. With this tidal wave of new devices comes an increasing number of new data streams, converging to make instant analytics on real-time data a tenant of any digital transformation. Our penchant for instant gratification extends to every time we press a button or ask a question online. Today, data must move at the speed of thought and real-time information brings us as close as possible to the present. The infrastructure to make this interaction possible ranges from the edge of the network into the core of the data center, and must include a database to support new interactive applications and analytics. Let’s examine a few compelling IoT use cases where turning data into actionable insights is table stakes. Drones – Managing the Machines For drones, ongoing communication and coordination of proper flying procedures and navigation guarantees success. Companies like Airmap.io are providing accurate, dynamic, and trustworthy airspace information to power innovative drone applications. Airware.com is another company pursuing a ‘ drone operating system ’ that helps companies ‘integrate aerial data with business systems.’ This type of platform performs best when the high volume of data ingest is combined with a set of ongoing queries, providing a real-time operational view. Consider the use of drones in dense areas, where to control traffic, drones will need to report status and ask about other nearby drones. This type of interaction mandates a data center system that can operate with real-time information effectively and at scale. Vehicles – Managing Your Safety Image: https://www.flickr.com/photos/smoothgroover22/ When it comes to vehicles, autonomous or not, the primary concern becomes passenger safety. With vehicles able to send data to auto manufacturers, there is an opportunity to provide real-time feedback of changing conditions. Much of this can be automated within the vehicle itself such as when to provide the anti-lock braking mechanism. But where other vehicles are involved, such as traffic congestion, the ability for each vehicle to coordinate in real time with central databases can help manage overall traffic flow. We already see that in action today with solutions like Waze for mapping the most efficient routes. Wearables – Monitoring Your Life When we move to wearables, the relationship becomes even more intimate, and ranges from general health to critical life-saving capabilities. Health monitoring wearables, particularly those for walking, jogging or running have become quite popular, allowing users to track progress but also monitor vital signs. That information can be shared in real time to offer predictions on energy exerted or calorie loss. In more critical situations like hospitals, monitoring equipment can send information directly into control stations within the hospital, but also to regional hospital networks to track not only patient well-being, but also utilization of equipment like wheelchairs and gurneys. Choosing the Right Infrastructure for IoT The infrastructure behind the ever-growing world of IoT will need to power the systems that ingest, analyze and store all this data captured in real time by sensors, mobile devices, applications, and transactions. A powerful database can serve as the foundation for this infrastructure by having the following characteristics: Support massive data ingest across millions of devices and connections Database systems must keep up with the incoming flood of data to ensure zero loss, and that every user or device has a complete picture of its history. Serve as the system of record while simultaneously providing real-time analytics In a real-time world transferring data between systems, commonly referred to as Extract, Transform and Load (ETL), isn’t a luxury, nor can it be painful. Systems of record for the Internet of Things need to mix transactions and analytics seamlessly and simultaneously. Respond to and integrate well with familiar ecosystems With sensor data touching everything from business intelligence to ad networks, connecting to multiple systems must be painless and simple. Allow for online scaling and online operations The world stops for no one, and successful services will be judged by their ability to grow and provide enterprise level service quality. With immediate access to both real-time and historical data for IoT connection points, companies can shape customer experiences in the moment including accelerating applications for wearables, powering operational analytics for drones, and capturing real-time opportunities in vehicles from safety to services. Try a real-time database built for IoT workloads Download SingleStore Community Edition", "date": "2016-01-19"},
{"website": "Single-Store", "title": "memsql-meetups-year-in-review", "author": ["Lesia Myroshnichenko"], "link": "https://www.singlestore.com/blog/memsql-meetups-year-in-review/", "abstract": "It has been six months since we began hosting meetups regularly at SingleStore. Our office is located in the heart of SoMa , two blocks from the Caltrain station. At the new San Francisco epicenter of tech startups, we want to meet our neighbors and see what other cool technologies are out there! What better way than over cold brews, local pizza and deep tech talks. In honor of the first official meetup of 2016, we decided to take a look back at the meetups of 2015, and share highlights from each one. Hope to see you at 534th St on January 21st for an intimate conversation with Chandan Joarder on Building Real-Time Digital Insight at Macys.com! RSVP for our next meetup: Building Real-Time Digital Insight at Macys.com Without further ado, we present Meetups: A Year in Review. July 15, 2015 – In-Memory Database Performance on AWS M4 Instances Meetup Featuring Carlos Bueno, Principal Product Manager, SingleStore Focused on maximizing database cluster performance with unlimited scale and capacity of SingleStore Community Edition. Slides available here: http://www.slideshare.net/SingleStore/inmemory-database-performance-on-a July 28, 2015 – Scaling Hypergrowth Sales Meetup Featuring Jim Herbold, former Executive Vice President of Global Sales at Box Jim was the first sales hire at Box, growing the team to nearly 400 people and taking revenue from $1 million to $174 million in just six and half years. He discussed go-to-market strategies, the importance of self-disruption, and sales-driven processes that helped Box achieve incredible growth. August 19, 2015 – How to Write Compilers in Modern C++ Meetup Featuring Drew Paroski, SingleStore architect, co-creator of the HipHop Virtual Machine (HHVM) and the Hack programming language at Facebook Drew shared guidelines for building compilers to increase performance and efficiency. Slides available here: http://www.slideshare.net/SingleStore/putting-compilers-to-work September 24, 2015 – 10 Tips to Rapidly Scale Your Startup Featuring Chris Fry, former SVP of Engineering at Twitter, an angel investor and early stage technology advisor Chris offered tips for building an engineering team from the ground up. Tips include how to build tight-knit teams, develop efficient workflows and create a culture based on healthy communication, learning opportunities and trust among team members. Slides available here: http://www.slideshare.net/SingleStore/10-ways-to-scale-your-startup October 6, 2015 – Lessons Learned From Building/Operating Scuba, a Real-Time Database at Facebook Featuring Ciprian Gerea, lead engineer of Facebook Scuba Ciprian shared Facebook’s approach to achieving real-time analytics at scale. Slides available here: http://www.slideshare.net/SingleStore/lessons-learned-from-building-and-operating-scuba November 5, 2015 – How Microsoft Built and Scaled Cosmos for Business Intelligence Featuring Eric Boutin, former Microsoft engineer, current engineer at SingleStore Eric highlighted lessons learned from building Microsoft’s Cosmos, a distributed platform for big data analytics. He reviewed the technical features that allowed the system to scale. Slides available here: http://www.slideshare.net/SingleStore/how-microsoft-built-and-scaled-cosmos", "date": "2016-01-20"},
{"website": "Single-Store", "title": "dbbench-active-benchmarking", "author": ["Alex Reece"], "link": "https://www.singlestore.com/blog/dbbench-active-benchmarking/", "abstract": "In my last blog post , I investigated a Linux performance issue affecting a specific customer workload. In this post, I will introduce the tool I created to drive that investigation. Recently, a customer was running a test where data was loaded into SingleStore via LOAD DATA . The customer’s third-party benchmarking tool found that SingleStore took twice as long to load the same amount of data as a competing database; however, the numbers reported by this tool did not make sense. Local tests had shown SingleStore could process ~200 MB/s of data on a single machine [1](#fn-1) , but the third-party tool reported SingleStore was only processing ~60 MB/s of data. Looking at raw TCP socket data on both sides of the connection, I realized that the _benchmarking tool_ was inadvertently throttling itself when running against SingleStore. When I used the standard `mysql` client to run the `LOAD DATA` command directly, performance doubled as SingleStore saturated the network card and matched competitor performance at 128MB/s 2 . The problem here was that the customer was passively running their tool and only looking at the final result. I follow a methodology known as active benchmarking , a technique for validating the results of a benchmark. The original description summarizes the active benchmarking process quite well: While the benchmark is running, analyze the performance of all components involved using other tools, to identify the true limiter of the benchmark. Every active benchmarking session is a fully-fledged performance investigation that takes more time and effort. But it turns up the actual bottleneck 3 and reveals the maximum possible performance. dbbench When I first joined SingleStore, I found myself frequently writing one-off scripts to simulate complicated workloads for these active benchmarking investigations. Having previously used fio to test filesystems, I wanted a similar tool for defining a custom workload and running it efficiently. I found the existing tools to be fairly restrictive: mysqlslap is only really designed to execute one type of query at a time, and sysbench only supports a very small set of workloads as first class citizens. I wanted to test more complicated workloads. Both mysqlslap and sysbench cap the number of concurrent connections to the configured number of threads if the database cannot keep up with the workload. I want to test and observe queueing in the database (not in my benchmarking tool) as queries pile up. Both mysqlslap and sysbench are designed first and foremost be stand-alone tests: they want to create their own managed tables with their own data. HammerDB is a graphical tool that does not appear to have easy ways to configure otherwise. I wanted to use simple text files for defining workloads (à la fio ) so I could share them via email or check them into a git repository. In addition, I had concerns about the performance overhead of running these thread-based workload drivers that generate queries or data 4 . I commonly run the workload generator on the database server, where it is unacceptable for the generator to compete with the database for resources. I needed a tool to be as lightweight as possible. Ultimately, these concerns motivated me to write a new tool, dbbench . dbbench is a fast, lightweight database workload generator that executes a workload defined a flexible configuration file. dbbench can behave like the other stand-alone testing tools, but it really shines as a tool for active benchmarking complicated workloads. The following configuration file describes a common analytics workload: analytics queries on a table with streaming writes that are periodically deleted from a table: [streaming inserts]\n; Insert 100 rows per second.\nquery=insert into t (value) select rand() * 1000\nrate=100\n    \n[clean old rows from table]\n; Every 100s, clean old rows from the table\nquery=delete from t where insert_date < now() - interval 1 hour\nrate=0.01\n    \n[read nearest values]\n; 8 clients reading as fast as possible.\nquery=select count(*) from t where value between 900 and 1000\nqueue-depth=8 Given this configuration file, dbbench will run forever and log some simple statistics about each job. While it is running, I can very easily observe interesting performance behaviors such as how the performance of read queries changes dramatically after the table is cleaned up. In the snippet from the logs below, the average latency of the “read nearest values” job drops from 225ms to 56ms after the table is cleaned: 2016/01/20 02:56:06 **streaming inserts**: latency 3.078086ms±1.173286ms; 100 transactions (100.177 TPS); 100 rows (100.177 RPS)\n2016/01/20 02:56:06 **read nearest values**: latency _225.831701ms_±63.85481ms; 39 transactions (28.282 TPS); 39 rows (28.282 RPS)\n2016/01/20 02:56:06 **clean old rows from table**: latency 7.824990134s±0; 2 transactions (0.155 TPS); 3259108 rows (253306.618 RPS)\n2016/01/20 02:56:07 **streaming inserts**: latency 2.785273ms±931.039µs; 100 transactions (100.870 TPS); 100 rows (100.870 RPS)\n2016/01/20 02:56:07 **read nearest values**: latency _56.268517ms_±5.004933ms; 143 transactions (131.732 TPS); 143 rows (131.732 RPS) I recently used a very similar configuration to test a customer workload and noticed that write query throughput dropped precipitously after the table cleanup was executed. It turned out that the background log merger thread was misconfigured to be aggressive on the SingleStore instance. When the table cleanup executed, the background thread triggered and used up so many disk IOPs that the transaction log was unable to keep up with incoming writes: dbbench was instrumental in this investigation because it provided a simple way to describe the workload and a reliable way to execute it. By monitoring and questioning the data throughout the execution of the benchmark, I was able to observe and fix a complicated performance anomaly. dbbench version 0.1 is publicly available under an Apache license. I’m actively improving and stabilizing dbbench . SingleStore is already using it for many customer Proof of Concepts and performance testing. Try out or contribute to dbbench on Github today. During the investigation, I found an easy way to improve this to 1GB/s ↩ We were bottlenecked on the 1 Gigabit Ethernet card ↩ Including in the benchmarking tool itself ↩ For example, the new sysbench 0.5 Lua interface has a noticable CPU overhead ↩", "date": "2016-01-21"},
{"website": "Single-Store", "title": "lambda-architecture-simplified", "author": ["Kevin White"], "link": "https://www.singlestore.com/blog/lambda-architecture-simplified/", "abstract": "Modern businesses need to support an increasing variety of data workloads and uses cases that require both fault-tolerance and scalability. This has led to widespread adoption of the Lambda Architecture. Lambda is designed to model everything that happens in a complex computing system as an ordered, immutable log of events. Processing the data (for example, totaling the number of web visitors or transactions) is completed as a series of transformations that output to new tables or streams. SingleStore combines database and data warehouse workloads, enabling both transactional processing and analytics. Gartner refers to this as HTAP or hybrid transaction/analytical processing . SingleStore often fulfills the speed layer of the Lambda architecture, providing in-memory performance to ingest and process streaming data. In our experience with Lambda implementations, we find that most organizations get hung up on details of the Lambda Architecture, introducing unnecessary technologies and workarounds to fit within the model. It doesn’t have to be this way. Free Guide: The Lambda Architecture Simplified To make sense of the Lambda Architecture and get you on track to a successful implementation, we are launching a new guide: The Lambda Architecture Simplified . This guide demystifies complexity surrounding the Lambda Architecture, and will reframe the way you think about the model by providing simplified data frameworks and real world use cases. You’ll learn What defines the Lambda Architecture, broken down by each layer How to simplify the Lambda Architecture by consolidating the speed layer and batch layer into one system How to implement a scalable Lambda Architecture that accommodates streaming and immutable data How companies like Comcast and Tapjoy use Lambda Architectures in production", "date": "2016-01-26"},
{"website": "Single-Store", "title": "stream-processing-questions-answered", "author": ["Serena Malkani"], "link": "https://www.singlestore.com/blog/stream-processing-questions-answered/", "abstract": "The Hive think tank recently held a panel discussion on Stream Processing Systems. Panelists debated best approaches to messaging platforms, stream systems, and accompanying data stores. Other topics included scalability, latency, fault-tolerance/reliability/High Availability, ease and simplicity of deployment, maturity and popularity, enterprise features, and security. There was a lot covered! Ben Lorica, O’Reilly Media’s Chief Data Scientist, moderated the conversation, comprised of the who’s who of real-time data systems Panelists: Karthik Ramasamy, Engineering Manager, Twitter and Apache Heron M. C. Srivas, CTO/co-founder, MapR Nikita Shamgunov, CTO/co-founder, SingleStore Ram Sriharsha, Architect for Spark and Data Science, Hortonworks Jay Kreps, CEO/Co-founder, Confluent Stream Processing Questions Answered Does Spark streaming have more improvements than storm? Why does Twitter have its own messaging system? What are customer concerns about latency and streaming? What changes are happening with Kafka? What are the overall upcoming trends in the stream processing space? When it comes to security systems, how do you prevent specific data access to consumers? Watch the Panel Recording About the Hive Think Tank The Hive, founded by T. M. Ravi and Sumant Mandal is an incubation studio focused on data driven applications in enterprise, IoT and on-line segments. The Hive works actively with founders to help incubate, fund and launch data-driven startups. The Hive Think Tank is the largest community of data experts with over 8000 ‘databees’. It brings together a unique community of entrepreneurs, business and technology practitioners who are focused on the use of Big Data approaches.", "date": "2016-01-28"},
{"website": "Single-Store", "title": "10-apps-syncing-to-the-pulse-of-your-life", "author": ["Emily Friedman"], "link": "https://www.singlestore.com/blog/10-apps-syncing-to-the-pulse-of-your-life/", "abstract": "We live in a burgeoning on-demand economy. Services have transcended the physical establishments of restaurants, banks, and shopping malls with universal facilitation by mobile applications. Hordes and hordes of application choices.  And we depend on them daily. It begins with the drop of a pin. Uber arrives at your door several minutes later and off you go to the gym. Now it’s Fitbit’s turn to perform, as it tracks heart rate and chimes gently to let you know workout goals for the day have been completed. Another Uber ride later, you’re at the office, where packages are shipped with the help of Shyp, friends are reimbursed for weekend expenses with Venmo, and the witty line from your boss during this morning’s meeting has made its way into the Twittersphere. And those are only a few examples in a broad sea of apps. Today’s mantra: “There’s an app for that” The global community embraces apps for everything from gaming and shopping, to banking and transportation. Check out the slideshow below, 10 Apps Syncing to the Pulse of Your Life , to see some of the compelling statistics revealing how seamless a part of our lives these apps have become. 10 Apps Syncing to the Pulse of Your Life from SingleStore", "date": "2016-02-10"},
{"website": "Single-Store", "title": "powerstream-demo", "author": ["Steven Camina"], "link": "https://www.singlestore.com/blog/powerstream-demo/", "abstract": "At Spark Summit East in New York , we unveil PowerStream, an Internet of Things (IoT) simulation with visualizations and alerts based on real-time data from 2 million sensors across global wind farms. Renewable energy, such as wind power, is a viable alternative to traditional sources. For example, Danish wind turbines set a new world record for generating energy in 2015. According to recently published data, wind power now accounts for 42.1% of the total electricity consumption in Denmark. As sensor technology advances, it becomes possible to monitor wind turbines on wind farms, ensuring maximization of air flow and mechanical power. About PowerStream PowerStream processes and analyzes simulated data from approximately 2 million sensors on 197,000 wind turbines installed around the world. Sensors are found on individual wind turbines within wind farms, as illustrated in the diagram below: With temperature and vibration data points acquired from these sensors, plus a simple machine learning algorithm, PowerStream predicts and visualizes the health of turbines. The application predicts both behavior of individual turbines and calculates aggregate behavior of wind farms. It then displays green or red states. A red state predicts out of normal operating bounds, while a green state indicates the turbine or wind farm is within expected bounds. PowerStream Architecture PowerStream is powered by the SingleStore platform, which includes a database, Streamliner (an integrated Apache Spark solution), and Ops – the web interface for cluster deployment, management, and monitoring. Furthermore, PowerStream uses a set of simulated data producers written in Python, Apache Kafka (a real-time message queue), and a Javascript-based User Interface (UI). The architecture of PowerStream is shown below: Data producers simulate sensor activity, pushing approximately 1 million data points every second from ten sensors on each turbine. Sensor data is sent to an Apache Kafka queue, which is processed by a SingleStore Streamliner data pipeline. The pipeline predicts the health of each turbine using a pre-trained machine learning model. The sensor, turbine, and wind farm states are stored in SingleStore and further analyzed to determine their health (green / red). Finally, the PowerStream UI queries the SingleStore database to display states in the web interface.  These queries, and subsequent visual display, depend on the map geography and zoom level selected by the user. Geospatial Functionality PowerStream also utilizes SingleStore Geospatial capabilities. Geolocation (longitude and latitude) of each turbine is stored in a SingleStore table, which is joined with other data when the user changes the map area in view. If the users zooms in closely, they will see status of specific turbines (depicted below), as opposed to wind farms (depicted above). Large volumes of data are generated and manipulated in this showcase application – here are a few data points: SingleStore Streamliner processes approximately 1 million data points per second, then inserts it into a SingleStore database When a viewer moves the map on screen, several large database queries run and complete in real time. Specifically, large database JOIN operations between the sensors table (~2 million rows), turbines table (~200,000 rows) and wind farms table (~20,000 rows) occur in parallel. This produces a geospatial json file that is compressed and rendered instantly (between 50 and 500 ms) in the web-based UI. Real-time notifications push to the UI based on a ` select * ` query from an events table, which scales up to 2 million records. Powerstream runs on 7 Amazon c4.2xlarge instances, at a rough cost of $0.311 hourly apiece, equating to just under $19,100 annually. Take a look at the SingleStore Ops dashboard below, from which PowerStream application and database operators manage and monitor the platform: Green Technology PowerStream exemplifies one way to use modern technology for good. Applying IoT principles to energy challenges, like harnessing wind power, can inspire energy companies and government organizations to apply resources and contribute to a more efficient future. Real-time analytics applies globally, and will enable energy innovation to spread across countries and oceans. See Powerstream live at Spark Summit East, Booth #101. Request a Demo here: singlestore.com", "date": "2016-02-16"},
{"website": "Single-Store", "title": "introducing-memsql-5-beta", "author": ["Eric Frenkiel"], "link": "https://www.singlestore.com/blog/introducing-memsql-5-beta/", "abstract": "A post from our co-founders Eric Frenkiel, CEO and Nikita Shamgunov, CTO Today SingleStore DB 5 Beta is publicly available! SingleStore customers have been able to achieve remarkable results with our database, and we look forward to feedback on this upcoming release from our user community. Click here to jump to SingleStore DB 5 Beta download ⇒ This release aligns with our 5th anniversary as a company and it is our most ambitious release to date. Over the past five years, we have learned how to deliver real-time analytics to a broad market, all while maintaining our focus on building a mature, enterprise-ready distributed database. Our First Public Beta Starting with SingleStore DB 4 last year, we introduced a free unlimited scale and unlimited capacity Community Edition. With users of Community Edition now thriving, we felt it was appropriate to share a Beta version of our upcoming release. A Commitment to the Enterprise SingleStore has always maintained an enterprise focus, ensuring our database delivers the maturity and functionality to serve the most demanding workloads. In SingleStore DB 5, you will see our continued efforts to improve across several key areas: Full Transactional SQL SingleStore is a scalable, performant database that retains the time-tested relational properties of SQL. In SingleStore DB 5, we have expanded coverage to our SQL surface area with improved EXPLAIN, temporary tables and window functions, furthering our commitment to support a wide range of database operations on our distributed system. Query Optimization and Compilation Enhancements For analytical workloads, we have continued to refine our query optimization to deliver faster query response times. We have also improved code compilation to maximize performance. Faster Columnstore Query Execution SingleStore includes an SSD/disk-based columnstore. This helps customers store large volumes of data in SingleStore, always available with a simple SQL query. The columnstore uses SSD or disk for data storage, and RAM for caching for an optimum mix of capacity and performance. In SingleStore DB 5, we have refined query execution to deliver better performance across a range of queries. Detailed Table and Query Statistics SingleStore DB 5 includes improved table and query statistics to provide a holistic view of database performance. Building Modern Database Applications with SingleStore In addition to well-understood database models, SingleStore allows you to go beyond what previous databases or data warehouses were capable of. We’d invite you to consider some of the following options. High-Volume Transactional Workloads SingleStore excels at high volume transactional workloads, including those where real-time analytics come into play. With SingleStore you can ingest millions of records per second, and run queries with results accurate to the last transaction. Data Warehouses with Live Data In the past, data warehouses were batch-loaded with data after-the-fact. With SingleStore, you can send live data to the database and run complex analytical queries with ease, all in a non-blocking infrastructure. SingleStore allows you to take an overnight process and turn it into a continuous process. Real-Time Data Pipelines with Apache Kafka and Spark SingleStore Streamliner supports modern streaming workloads using the power of Apache Spark, and enables our customers to stream, persist, and analyze hundreds of terabytes of data a day without writing any code. Easily connect to Apache Kafka as a real-time message queue, or use a custom extract to pull data from your preferred source. SingleStore DB 5 is also our most secure, robust, and performant release. You can find the full list of improvements in our release notes . Share your feedback Please share your feedback with our product and engineering teams at feedback@singlestore.com . We look forward to working with you to help solve your database and application requirements. Eric Frenkiel, CEO and co-founder Nikita Shamgunov, CTO and co-founder Click here to jump to SingleStore DB 5 Beta download ⇒", "date": "2016-02-23"},
{"website": "Single-Store", "title": "spark-summit-goes-real-time", "author": ["Serena Malkani"], "link": "https://www.singlestore.com/blog/spark-summit-goes-real-time/", "abstract": "We spent last week in New York at Spark Summit East talking with the visionaries and data architects using Apache Spark. PowerStream Demo At the show we introduced PowerStream, an Internet of Things (IoT) showcase application with visualizations and alerts based on data from 2 million sensors across global wind farms. PowerStream ingests that data and provides actionable insights in real time, giving users a glimpse of how the future of sustainability can be fully realized by adapting data to drive innovation. Read the Technical Blog Post Get The SingleStore Spark Connector Guide The 79 page guide covers how to design, build, and deploy Spark applications using the SingleStore Spark Connector. Inside, you will find code samples to help you get started and performance recommendations for your production-ready Apache Spark and SingleStore implementations. Download Here Zoomdata and SingleStore TaxiStats Demo We also partnered with our friends at Zoomdata to demo a new real-time dashboard application we call TaxiStats. The demo simulates pickup and drop-off data from New York City taxis streamed into SingleStore in real time. From there, data is queried using SQL and simultaneously visualized in a Zoomdata dashboard. Taxi data is displayed as it is collected while exploratory analytics run simultaneously on the dataset. The dashboard includes: Real-time data for pickups by ZIP code on the map, total volume of rides, and rides by time of day A map and graph that can be filtered to explore and drill down A live stream that can be paused or rewound to examine a specific time period SingleStore also hosted an evening event included a special presentation from SingleStore Co-founder and CTO, Nikita Shamgunov. Download the Presentation Slides: Spark Summit assembles the best engineers, scientists, analysts, and executives from around the globe. We Look forward to the upcoming West Summit in San Francisco, June 6th-8th 2016!", "date": "2016-02-25"},
{"website": "Single-Store", "title": "gartner-mq-data-warehouse", "author": ["Kevin White"], "link": "https://www.singlestore.com/blog/gartner-mq-data-warehouse/", "abstract": "Gartner has named SingleStore a Visionary in the Magic Quadrant for Data Warehouse and Management Solutions for Analytics ! This marks our debut in Gartner’s report for data warehouse solutions and our second showing on a Gartner Magic Quadrant report in the last six months. The report compares top Data Warehouse solutions, as well as highlights key strengths that distinguish SingleStore from legacy vendors. SingleStore Strengths include: Focus on supporting transactional and analytical use cases with low-latency requirements An integrated version of Apache Spark to enable stream ingestion, transaction processing and analytics Roots in the operational database market and ability to address operational analytics use cases in the data warehouse market Flexibility of technology The ability to handle transactional and analytical workloads in a single database is a requirement for real-time use cases. Gartner coined the term HTAP, short for Hybrid Transaction/Analytical Processing , to categorize database solutions capable of consolidating mixed workloads into a single system. This is where SingleStore shines, offering an in-memory rowstore and disk-based columnstore that provides a robust database platform to run analytics on changing datasets and scale a growing consumer base. See how SingleStore compares by viewing the complete Gartner Magic Quadrant for Data Warehouse and Management Solutions for Analytics here: singlestore.com/gartnerdw", "date": "2016-02-26"},
{"website": "Single-Store", "title": "the-impact-of-always-on-geospatial-analytics", "author": ["Gary Orenstein"], "link": "https://www.singlestore.com/blog/the-impact-of-always-on-geospatial-analytics/", "abstract": "In the past ten years technology shifts have re-crafted the geospatial applications and analytics landscape. The iPhone and Android ecosystems have fostered a world where almost everyone is a beacon of information Large scale computing capabilities have provided companies like Google and Facebook the ability to keep track of billions of things , and companies like Amazon and Microsoft are making similar computing power available to everyone Global internet coverage continues to expand , including innovative programs with balloons and solar powered drones These trends shape billion dollar shifts in the mapping and geospatially-oriented industries, for example In August 2015, a consortium of the largest German automakers including Audi, BMW, and Daimler (Mercedes) bought Nokia’s Here mapping unit, the largest competitor to Google Maps, for $3.1 billion. In addition to automakers like the German consortium having a stake in owning and controlling mapping data and driver user experiences, the largest private companies, like Uber and Airbnb, depend on maps as an integral part of their applications. Source: VentureBeat New applications, particularly those around geospatial analytics , are harnessing the geospatial capabilities of an in-memory approach. In particular, transportation is just one of many industries undergoing dramatics shifts as new technology forces align. To understand the full shifts underway, and read about transportation analytics examples with New York Taxis, and Esri approach to in-memory databases, and real-time dashboards with Zoomdata, check out our full white paper: PDF Download: Always-on Geospatial White Paper ⇒", "date": "2016-03-01"},
{"website": "Single-Store", "title": "gartner-bi-real-time-track", "author": ["Lesia Myroshnichenko"], "link": "https://www.singlestore.com/blog/gartner-bi-real-time-track/", "abstract": "SingleStore is headed to Gartner Business Intelligence and Analytics Summit next week. We’ll be focusing on real-time analytics with our own VP of Engineering kicking off a live demo on day one of the show. For the following days, we’ll be tracking the hot topics in real-time analytics, the new Magic Quadrant for Data Warehouses , IoT, Spark, Relational Databases, In-Memory Computing, and Machine Learning There are dozens of phenomenal sessions at Gartner BI. Our favorites are listed below and if you are around, there is a good chance you’ll see us too. Drop a line via twitter @SingleStoreDB if you want to catch up. Monday, 14 March 2016, 12:45 PM – 1:05 PM SingleStore: Winning the On-Demand Economy with Spark and Predictive Analytics Ankur Goyal, Vice President of Engineering, SingleStore @ankrgyl Today’s on-demand economy drives companies to provide fast load times, personalization, and instantaneous service for hungry end-users across all types of applications. Yet most still use dated, legacy systems to process and analyze data. In this session, Ankur Goyal, VP of Engineering at SingleStore will showcase implementing a one-click Lambda Architecture with Apache Spark, Apache Kafka and an operational database, resulting in lightning fast analytics on large, changing datasets. Monday, 14 March 2016, 4:15 PM – 5:00 PM Magic Quadrant Power Session: Data Warehouse and Data Management Solutions for Analytics Mark A. Beyer, VP Distinguished Analyst, Gartner @metadatabeyer Disruption in the data warehouse technology market accelerates with the demand for broader data management solutions for analytics expanding to address multiple data types, distributed processing and repository options. Information leaders and CIOs will often find that extending existing solutions with technology combinations yields more flexible results. Old leaders are facing very significant challenges from new entrants in both vision and execution in 2016. Tuesday, 15 March 2016, 9:45 AM – 10:30 AM Six Best Practices for Real-Time Analytics W. Roy Schulte, VP Distinguished Analyst, Gartner The need for real-time analytics — including predictive analytics, complex-event processing (CEP), and other techniques — is growing quickly in almost every industry. BI and analytics leaders need to understand the wide variety of available technologies, and these six essential best practices on how to use them. Key issues: • What business situations use real-time analytics today? • How does event stream processing differ from other BI and analytics? • How will decision management change BI, analytics, BPM and app development? Tuesday, 15 March 2016, 3:00 PM – 3:30 PM To the Point: Is Apache Spark the Future of Data Analysis? Nick Heudecker, Research Director, Gartner @nheudecker With its fast, in-memory processing and analytical framework, Apache Spark has quickly attracted interest from developers and software vendors. What role will Spark play in your information management and analytics story? Wednesday, 16 March 2016, 8:15 AM – 8:45 AM To the Point: Is There a Future for Relational DBMS? Donald Feinberg, VP Distinguished Analyst, Gartner @Brazingo Is there a future for RDBMS or will it finally fall to NoSQL and other new technologies? Gartner continues to receive many questions about the future of RDBMS and whether new technologies will replace it. We will address this issue and discuss the future trends in RDBMS. Wednesday, 16 March 2016, 2:45 PM – 3:30 PM In-Memory Computing and Big Data: Architecting for New Velocity Needs Massimo Pezzini, VP & Gartner Fellow @mpezziniGartner Real-time processing of large volumes of fast moving data coming from sensors or machine generated data enables digital innovation but challenges traditional approaches to information infrastructure. Discover how emerging in-memory computing technologies can help through concrete case studies. Wednesday, 16 March 2016, 2:45 PM – 3:30 PM Machine Learning Drives Digital Business Alexander Linden, Research Director, Gartner Machine learning — already one of the most versatile technologies of the past decade — supports an amazing array of advanced analytics use cases in digital business. Here we discuss basics, benefits, pitfalls and vendor selection. See you at the show – Booth 108 SingleStore will also be exhibiting at booth 108 . Drop by for a free copy of our book on Building Real-Time Data Pipelines and see a live demo of PowerStream, our latest IoT showcase application. Book a SingleStore Demo at Gartner BI Summit ⇒", "date": "2016-03-08"},
{"website": "Single-Store", "title": "full-fledged-sql-compiler-faster-query-processing", "author": ["Nikita Shamgunov"], "link": "https://www.singlestore.com/blog/full-fledged-sql-compiler-faster-query-processing/", "abstract": "We are proud to announce general availability for SingleStore DB 5 today. A key milestone in this release is a full fledged SQL compiler resulting in faster query processing across the board. Making this happen was a result of several months of hard work, which featured a large uplift of our existing database execution engine. This new SQL compiler is using LLVM for code generation. This modern compilation strategy is capable of supporting dynamic compilation of programming languages. In addition to performance, SingleStore DB 5 has many new features and smarter query optimization. Building the Fastest SQL Compiler Query processing in database engines such as Oracle, SQL Server, or Postgres can benefit significantly from code generation, especially for complex analytical workloads. However, too often, code generation does not get enough attention, teams have lacked enough compiler experts, or the need was not acute. Today, with the rise of big data, proper code generation is critical, especially in memory optimized systems where I/O is no longer a bottleneck. Building code generation is a compilers project. We knew it would be an extremely ambitious engineering undertaking, but that the competitive advantages gained would greatly outweigh the cost. Building a new SQL compiler within months requires a world class team. For us it started with Drew Paroski , who came to SingleStore as an architect specifically to design and lead our code generation efforts. He spent his initial weeks on the project understanding the current code and its shortcomings and prototyping new designs, working closely with SingleStore engineers Michael Andrews and David Stolp (aka Pieguy). One big design decision was how far we should push code generation away from the classic volcano model for query processing. The trade-off was between using the volcano model and generating code for just parts of the SQL query, or abandoning that model to generate code for the whole query, including error handling and corner cases. We chose the latter approach for its composability and precise control over performance. This required more work, but ultimately it provides a significantly better experience for our customers. Download SingleStore DB 5 Today SingleStore DB 5 is generally available now. Download it now and enjoy its real-time capabilities. You can build real-time data pipelines and take them to unprecedented levels of scale and sophistication. Our customers solve their hardest data problems with SingleStore and we are thrilled to continue driving database innovation for the industry.", "date": "2016-03-30"},
{"website": "Single-Store", "title": "strata-san-jose-2016", "author": ["Kevin White"], "link": "https://www.singlestore.com/blog/strata-san-jose-2016/", "abstract": "At Strata+Hadoop World, we engaged with the brightest minds in data management, machine learning, and analytics. CEO and co-founder, Eric Frenkiel announced the release of SingleStore DB 5 and showcased a path for real-time processing to the Strata audience during the day one keynote. Later, he shared the stage with Kellogg in a tutorial session on predictive analytics. SingleStore DB 5 Now Generally Available Eric announced the general availability release of SingleStore DB 5, delivering breakthrough performance on database, data warehouse, and streaming workloads. SingleStore DB 5 features: LLVM Code Generation Architecture LLVM code generation for SQL queries, featuring a new SQL to LLVM to machine code compiler built into the the engine Completely revamped code generation architecture to greatly improve query compilation speed Query Optimization Improvements Updated query optimizer uses more collected statistics and histograms to choose better query plans Improvements to hash join selection, distributed and local join optimization, and better selection of bushy joins Elimination of unnecessary tables, views, and filters Intelligent selection of distributed group by execution plans Columnstore Performance Improvements Operations on compressed data Batch scanning to read data and apply filters in batches for faster scan performance Prefetching for improved query execution when the table does not fit in memory SQL Features New EXPLAIN shows an updated, informative description of query execution plans Support for temporary tables Several windows functions and the OVER clause View the full SingleStore DB 5 release notes ⇒ Driving the On-Demand Economy with Predictive Analytics – Keynote Eric explains how a trinity of real-time technologies—Kafka, Spark, SingleStore— enables predictive applications and analytics for enterprise companies. Dash forward: From Descriptive to Predictive Analytics with Apache Spark JR Cahill, Senior Solutions Architect at the Kellogg Company, joined Eric for a tutorial session outlining Kellogg’s approach to advanced analytics with SingleStore. JR shared how Kellogg moved from overnight to intraday analytics, integrating directly with Tableau. Follow SingleStore on Twitter to see the full recording as soon as it’s available. Follow SingleStore on Twitter ⇒", "date": "2016-04-08"},
{"website": "Single-Store", "title": "18-favorite-posts-and-articles-from-memsql", "author": ["Alex Hudzilin"], "link": "https://www.singlestore.com/blog/18-favorite-posts-and-articles-from-memsql/", "abstract": "The data industry has so many topics to dive into. At SingleStore, we do our best to provide coverage across a range of subjects. To make it a bit easier to find a few of our recent and historical hits, we have curated a short list of articles by topic. Enjoy! SQL SQL is the lingua franca of data processing. Understand why SingleStore has stayed true to this dominant approach. Hidden Cost of NoSQL | Network World Article | Jan 2016 Why SingleStore Placed a Bet on SQL | SingleStore Blog | Oct 2015 In-Memory The future is fast, and memory-optimized architectures will get us there. Why In-memory Is the Future of Computing | DataInformed Article | March 2016 Four Reasons Behind the Popularity and Adoption of In-Memory Computing | SingleStore Blog | July 2016 Lambda Architecture The ability to capture, process, and serve data in real time is critical for modern workloads. Lambda is one approach and we help break down the details. Rethinking Lambda Architecture for Real-Time Analytics | SingleStore Blog | Dec 2015 Lambda Architecture Isn’t | SingleStore Blog | Dec 2015 Lambda Architecture Simplified | SingleStore Whitepaper | Jan 2016 Historical Perspective Data processing: where the old is new again. Get our take on where the industry has been and is headed. Cache is a new RAM | SingleStore Blog | Nov 2015 Big Data Big Data deserves big discussion. Understand a real-time approach to tackling data challenges. 5 Big Data Trends | SingleStore Blog | Sep 2015 Closing the Batch Gap | Feb 2015 Shaping Big Data Through Constraints Analysis | InfoQ Article | May 2015 Database Specific Databases are the foundation of modern computing. We cover the basics. Characteristics of a Modern Database | SingleStore Blog | Dec 2015 Database Speed Test | SingleStore Blog | Jun 2015 Turn Up the Volume With High-Speed Counters | SingleStore Technical Blog | Mar 2015 SingleStore Ecosystem Fit Data processing solutions go far beyond a single product. Understand how SingleStore fits into a larger ecosystem. Filling the Gap Between HANA and Hadoop | SingleStore Blog | Apr 2015 Using Oracle and SingleStore Together | SingleStore Blog | Dec 2015 Geospatial In a world where every data point has a place, geospatial analytics can deliver another avenue for insight. Always-on Geospatial Analytics | SingleStore Blog and Whitepaper | March 2016 Real-Time Geospatial Intelligence with Supercar | SingleStore Blog | March 2015", "date": "2016-04-12"},
{"website": "Single-Store", "title": "should-you-use-a-rowstore-or-a-columnstore", "author": ["Lesia Myroshnichenko"], "link": "https://www.singlestore.com/blog/should-you-use-a-rowstore-or-a-columnstore/", "abstract": "The terms rowstore and columnstore have become household names for database users. The general consensus is that rowstores are superior for online transaction processing (OLTP) workloads and columnstores are superior for online analytical processing (OLAP) workloads. This is close but not quite right — we’ll dig into why in this article and provide a more fundamental way to reason about when to use each type of storage. Background One of the nice things about SQL-based databases is the separation of logical and physical concepts. You can express logical decisions as schemas (for example, the use of 3NF ) and SQL code (for example, to implement business logic), and for the most part avoid thinking about physical implementation details or runtime considerations. Then, based on what your workload is trying to do, you can make a series of physical decisions which optimize performance and cost for your workload. These physical decisions include where to put indexes, what kind of indexes to use, how to distribute data, how to tune the database, and even which database product to use (if you use ANSI SQL). Importantly, making physical decisions does not require changing SQL code. Until recently, indexes were almost always backed by B-Trees and occasionally hash tables. This started to change when Sybase IQ and then more popularly C-Store/Vertica hit the market and provided incredible cost-savings and performance for data-warehouse workloads with columnstore indexes. Columnstores have hit the mainstream and are the primary storage mechanism in modern data-warehouse technology (e.g. Redshift, Vertica, HANA) and are present in mainstream databases (Oracle, SQL Server, DB2, SingleStore). Nowadays, one of the key physical decisions for a database workload is whether to use a rowstore or columnstore index. Performance Considerations Let us frame the discussion about when to use a rowstore or columnstore by boiling down the fundamental difference in performance. It’s actually quite simple: Rowstores are better at random reads and random writes. Columnstores are better at sequential reads and sequential writes. Feeling déjà vu? This is a fairly familiar concept in computer science, and it’s pretty similar to the standard tradeoff between RAM and disk. This reasoning also obviates several myths around rowstores and columnstores: Is a RAM-based rowstore faster than a disk-based columnstore? Not necessarily — if the workload has sequential reads (e.g. an analytical workload with lots of scans) a columnstore can be significantly faster. Are writes slow in a columnstore? Not necessarily — if the writes are mostly ordered and you don’t need to run updates, then a columnstore can be as fast or even faster to write into than a rowstore, even for relatively small batches. Are columnstores bad at concurrent writes? It depends on the type of disk. Both rotational and solid-state disks are good at sequential writes, but solid-state disks tend to be significantly faster to write into concurrently; therefore, columnstores running on SSDs can be very fast at concurrent writes. Rowstores for Analytics, Columnstores for Transactions Let’s look at a few use cases which violate the common belief that rowstores are superior for transactions and columnstores are superior for analytics. These are based on workloads that we’ve seen at SingleStore , but these observations are not specific to SingleStore. In analytical workloads, a common design choice is whether to append (aka log-oriented insertion) or update/ upsert . For operational analytics, the upsert pattern is especially common because by collapsing overlapping rows together with an update, you partially-aggregate the result set as you write each row, making reads significantly faster. These workloads tend to require single-row or small-batch random writes, so a rowstore is a significantly better choice as columnstores can’t handle this pattern of writes at any reasonable volume. As an aside, the read pattern is often still scan-oriented, so if the data were magically in a columnstore, reads could be a lot faster. It still makes sense to use a rowstore, however, because of the overwhelming difference in write performance. Another example of rowstores in analytical workloads is as dimension tables in a traditional star schema analytics workload. Dimension tables often end up on the inside of a join and are seeked into while scanning an outer fact table. We’ve seen a number of customer workloads where we beat columnstore-only database systems simply because SingleStore can back dimension tables with a rowstore and benefit from very fast seeks (SingleStore even lets you use a lock-free hashtable as a type of rowstore index, so you have a pre-built hash join). In this case, rowstores are the superior choice because dimension tables need to be randomly, not sequentially, read. Finally, columnstores can be used for transactional workloads as well, in particular workloads that are computationally analytic but have operational constraints. A common use case in ad-tech is to leverage a dataset of users and groups (which users belong to) to compute overlap on-demand, i.e. the number of users who are in both Group A and Group B. Doing so requires scanning every row for all users in both Group A and Group B , which can be millions of rows. This computation is significantly faster in a columnstore than a rowstore because the cost of executing the query is dominated by the sequential scan of user ids. Furthermore, sorting by group id not only makes it easy to find the matching user ids but also to scan them with high locality (since all user ids in a group end up stored together). With SingleStore, we were able to get this query to consistently return within 100 ms over 500 billion rows stored in the columnstore. When your workload is operational and fundamentally boils down to a sequential scan, then it can run significantly better in a columnstore. As a side benefit, columnstores offer exceptional data compression so this workload has a relatively small hardware footprint of less than ten nodes on Amazon and fit in SSD. Because these workloads bleed the traditional definitions of OLTP and OLAP, they are often referred to as Hybrid Transactional/Analytical Processing (HTAP) workloads. Conclusions and Caveats The main takeaway is pretty straightforward. Rowstores excel at random reads/writes and columnstores excel at sequential reads/writes. You can look at almost any workload and determine which bucket it falls into. Of course, there are still a few caveats: If you have both random and sequential operations, it’s usually better to go with a rowstore. Rowstores tend to be better at sequential operations than columnstores are at random operations. However, if storage space is a primary design constraint, you may want to still consider using a columnstore. Some databases, namely SingleStore and Oracle, have hybrid tables with both rowstore and columnstore functionality. In SingleStore, there is a small rowstore sitting alongside every columnstore table that is used for small-batch writes (e.g. singleton inserts and updates). Oracle redundantly stores rowstore data in an in-memory columnstore to transparently speed up reads which would be faster against column-oriented data (see A Case for Fractured Mirrors ). Of course, this comes at the expense of write performance since you have to pay the cost of inserting into both stores every time, rather than whichever is cheapest. Sorting in columnstores is a big deal, especially for workloads with tight requirements on performance, because it enables consistent performance with low latency. Generally, commercial columnstores (Redshift, Vertica, SingleStore) support sorting but open source and SQL-on-Hadoop databases do not ( Kudu is a notable exception). Thanks to Gary Orenstein , Andy Pavlo , and Nikita Shamgunov for reviewing drafts of this post. This is a repost of an article by Ankur Goyal, VP of Engineering, published on Medium ⇒", "date": "2016-04-19"},
{"website": "Single-Store", "title": "memsql-raises-series-c", "author": ["Eric Frenkiel"], "link": "https://www.singlestore.com/blog/memsql-raises-series-c/", "abstract": "We are delighted to announce that SingleStore has raised more than $36m in a Series C round. The oversubscribed round includes new investors REV Capital, Caffeinated Capital, with full pro rata participation from earlier investors including, Accel (who led our Series B), Khosla Ventures, and Data Collective. When you include the ongoing support of our early investors First Round Capital, In-Q-Tel, IA Ventures, that’s quite the list. Read the news release ⇒ Our investors have looked at the SingleStore technology and how it is providing real-time analytics for the enterprise, and concluded that SingleStore is the leading real-time database platform for analytics and operational workloads. It’s a milestone day for SingleStore, but understandably the big story now though is what we do next. We have seen first hand that when it comes to real-time analytics, large enterprises want SQL, they want it fast, and they want it to be easy to use. SingleStore customers, with these capabilities, get to transform their business, and can shift direction as fast as the trends do. Easy access to real-time data processing provides opportunities to build new applications, or reinvent old ones. SingleStore powers real-time billing applications reaching 6M updates per second. Elsewhere, logistics analytics that previously ran once a week taking 24 hours now run daily and take under an hour to complete. But perhaps the strongest momentum is the business need for real-time insight. Only 10 years ago when internet connectivity was sporadic, real-time was hard to justify. Today business executives have recognized the power of real-time data to drive new revenue and reduce costs. That business approach combined with a real-time, in-memory database platform is part a new and growing market around real-time applications. SingleStore customers find real value in real-time, and the numbers speak for themselves. It is no question that things have changed in the database landscape, and the requirements of modern workloads are putting everyone to the test. This financing ensures SingleStore can expand its talented team, fund additional product development, and provide companies a real-time database platform to transform their business.", "date": "2016-04-21"},
{"website": "Single-Store", "title": "visit-memsql-in-may", "author": ["Gary Orenstein"], "link": "https://www.singlestore.com/blog/visit-memsql-in-may/", "abstract": "SingleStore has a full show schedule in May and we hope you can visit us at one of these events. GEOINT: May 15-18, Orlando http://geoint2016.com/ May 15-18, 2016, Gaylord Palms Resort and Convention Center, Orlando, Florida SingleStore is at booth 1514 GEOINT Symposium is the nation’s largest gathering of industry, academia, and government to include Defense, Intelligence and Homeland Security Communities. GEOINT is hosted by USGIF, the United States Geospatial Intelligence Foundation SingleStore will be featured in a lighting talk Real-Time Geospatial Intelligence at Scale Mike Kilrain, President, SingleStore USG Sunday, May 15th, 12:13pm at GEOINT Get the latest on approaches for analyzing real-time and historical data with an in-memory, distributed database complete with geospatial analytics. We’ll cover the latest details on SingleStore DB 5, customer use cases, and showcase applications that combine real-time insights along with machine learning algorithms. Also drop by the SingleStore Booth 1514 and see our showcase applications PowerStream for real-time geospatial across nearly 200,000 wind turbines worldwide and Supercar, highlighting real-time geospatial analytics for vehicle data in New York City. We’ll also have copies of our book, Building Real-Time Data Pipelines , at our booth. Informatica World: May 23-26, San Francisco http://www.informaticaworld.com/ May 23-26, 2016, Moscone West, San Francisco, CA SingleStore is at booth B5 At Informatica World, you’ll get all the information and insight necessary to harness one of your organization’s most important assets: data. Choose from 130+ breakout sessions featuring Informatica experts and Informatica customers and partners. SingleStore will be featured in the Big Data Ready Summit during the IoT/Streaming session on May 22nd, at 2:45pm. http://www.informaticaworld.com/iw16/agenda/big-data-ready-summit.html Also drop by the SingleStore Booth on the main show floor at Informatica World. In-Memory Computing Summit: May 23-24, San Francisco http://imcsummit.org/ May 23-24, 2016, Grand Hyatt, San Francisco, CA The In-Memory Computing Summit 2016 is the only industry-wide event of its kind, tailored to IMC related technologies and solutions. Be sure to join our CTO and co-founder for this talk Propelling IoT Innovation with Predictive Analytics Nikita Shamgunov, CTO and co-founder, SingleStore 1:45 pm – 2:35 pm In this session, Nikita Shamgunov, CTO and co-founder of SingleStore, will conduct a live demonstration based on real-time data from 2 million sensors on 197,000 wind turbines installed on wind farms around the world. This Internet of Things (IoT) simulation explores the ways utility companies can integrate new data pipelines into established infrastructure. Also visit the SingleStore booth to grab a t-shirt and a copy of our book, Building Real-Time Data Pipelines.", "date": "2016-05-11"},
{"website": "Single-Store", "title": "community-edition-anniversary", "author": ["Steven Camina"], "link": "https://www.singlestore.com/blog/community-edition-anniversary/", "abstract": "Today marks the 1-year anniversary of the SingleStore Community Edition launch. Over the course of a year, Community Edition has surpassed tens of thousands of downloads and is deployed across hundreds of active clusters. We have a thriving community user group on Slack, ranging from SQL beginners to award-winning data engineers. On our anniversary, we want to share what has contributed to our community success, and what users can expect moving forward. A Database So Scalable, Everyone Can Use It Community Edition is free forever and comes with unlimited capacity, unlimited scale, and all of the transactional and analytical features present in SingleStore Enterprise Edition. We launched Community Edition to give developers and companies a chance to explore what is possible with in-memory computing. It has advanced SQL features and runs on the same engine as SingleStore Enterprise Edition. Download Here: singlestore.com/free Helping Our Community We are dedicated to ensuring success for Community Edition users. We do so by providing free resources to the community including a public Slack Channel, open source tools, documentation, and training videos. Public Slack Channel Our Slack channel is a top resource for many of our Community Edition users. Users can post questions on everything from installation to advanced SQL queries, and get quick responses from the community. SingleStore engineers can be found answering questions during California business hours. Join the public Slack channel here: https//www.singlestore.com/forums/ Open Source Tools We have developed a number of open source tools to make using SingleStore even better. Projects posted in the last year include: SingleStore Streamliner – An integrated version of Apache Spark to assist with real-time pipeline deployments Streamliner Starter Kit : a repository to help you build real-time data pipelines for SingleStore Streamliner Streamliner Examples : a repository featuring sample code for the SingleStore Streamliner SingleStore Spark Connector : A tool reading from and writing to SingleStore in Spark. The connector provides a number of integrations with Apache Spark including a custom RDD type, DataFrame helpers and a SingleStore Context. dbbench : a fast, lightweight database workload generator that executes a workload-defined flexible configuration file. Using this dbbench, simple ‘jobs’ can be defined to describe a workload run against a server. Documentation To help our Community and Enterprise customers get up-and-running quickly with SingleStore at full capacity, we offer extensive product documentation. Our documentation is especially helpful for getting started as it includes a quick start guide , tutorials , SQL references , FAQ and more. Documentation can be found at: docs.singlestore.com Training Videos Last year, we introduced the first SingleStore training series, which comes with 6 modules that range from an architecture introduction to schema design and capacity planning. We’ve embedded the training module playlist below. Thank You Finally, we want to thank all of our community members for providing constructive feedback over the past year and for helping us build a welcoming environment for new users. Our goal is to enable any organization to operate in real time, and Community Edition is an important vehicle for making that happen. To join the SingleStore community, download Community Edition at singlestore.com/free and contribute to our public Slack channel at https//www.singlestore.com/forums/", "date": "2016-05-20"},
{"website": "Single-Store", "title": "real-time-analytics-tableau", "author": ["Arthur Gyldenege"], "link": "https://www.singlestore.com/blog/real-time-analytics-tableau/", "abstract": "This is a Guest Post from our Friends at Tableau SingleStore is the database platform for real-time analytics. One of the great features of the latest release, SingleStore DB 5, is users can connect directly from Tableau. Using the new SingleStore DB 5 named connector, currently in beta, users can analyze current and historical data together while SingleStore ingests millions of transactions per second. SingleStore is also wire-protocol compliant with MySQL, so if you already have a MySQL driver installed or you are running Tableau 9.3 or later, you can connect to SingleStore. How can you use SingleStore and Tableau? Say you work for a connected device company that helps millions of people monitor their physical activity and heart rate. Millions of devices send physical activity and heart rate data throughout the day. You just released an app that gamifies exercise and you want to see how effective it is right now. So you open Tableau and select the new named connector called “SingleStore”. Input the server name of your SingleStore aggregator node, port (default is 3306 as in MySQL), and user credentials. On the data tab, you see all your databases and tables, which include data from the SingleStore in-memory rowstore and memory and disk-based columnstore. Select the tables you want to analyze and instantly start visualizing your operational data. Using SingleStore DB 5? Sign up for the beta Tableau connector here: http://www.tableau.com/tableau-10-beta-program The new connector is part of Tableau’s upcoming 10.0 release. To learn more about 10.0, visit http://tableau.com/coming-soon For questions related to SingleStore integration with Tableau, please contact SingleStore at partners@singlestore.com", "date": "2016-05-25"},
{"website": "Single-Store", "title": "close-encounters-with-a-third-kind-of-database", "author": ["Gary Orenstein"], "link": "https://www.singlestore.com/blog/close-encounters-with-a-third-kind-of-database/", "abstract": "For years, we have lived in a data processing world with two primary kinds of database systems, one for capturing incoming data and managing transactions, and another for answering questions and analyzing the data for insight and intelligence. The first system is what we know as a database, a system designed so you can put data in and get data out reliably and quickly. The second system is generally referred to as a data warehouse. There has long been a historical divide between these two systems, but HTAP or Hybrid Transaction Analytical Processing, is a new approach that brings these two worlds together. Check out the complete article on CIO Review The historical data divide and HTAP unification Databases Data Warehouses Transactions Analytics Online Transaction Processing Online Analytical Processing OLTP OLAP Hybrid Transaction Analytical Processing HTAP merges the worlds of the worlds of transactions and analytics In a recently published article on CIO Review , we dig into more details on this new type of processing and the emergence of database platforms to fulfill HTAP requirements. Here’s a preview of the article’s table of contents. Close Encounters with a Third Kind of Database Database Beginnings Enter the Data Warehouse Origins of Transactional and Analytical Systems Benefits of an HTAP Capable Systems Elimination of the ETL Process Data Accurate to the Last Transaction An Analytical Feedback Loop for Transactions Getting Started with HTAP Memory-optimized or In-Memory Databases Distributed Systems Relational Engine Look for Capabilities in Both Operational Database and Data Warehouse Workloads Please feel free to take a look at the entire article on CIO Review .", "date": "2016-06-02"},
{"website": "Single-Store", "title": "pinterest-experiments-in-real-time", "author": ["Bryant Xiao"], "link": "https://www.singlestore.com/blog/pinterest-experiments-in-real-time/", "abstract": "This post originally appeared on the Pinterest Engineering Blog by Bryant Xiao. As a data driven company, we rely heavily on A/B experiments to make decisions on new products and features. How efficiently we run these experiments strongly affects how fast we can iterate. By providing experimenters with real-time metrics, we increase our chance to successfully run experiments and move faster. We have daily workflows to compute hundreds of metrics for each experiment. While these daily metrics provide important insights about behavior, they typically aren’t available until the next day. What if the triggering isn’t correct so that Pinners are not actually logged? What if there’s a bug that causes a big drop in the metrics? What about imbalanced groups? Before the real-time dashboard, there was no way to tell until the next day. Also, any subsequent changes / corrections would require another day to see the effect, which slows us down. The real-time experiment dashboard solves these problems. Here we’ll share how we build the real-time experiment metrics pipeline, and how we use it to set up experiments correctly, catch bugs and avoid disastrous changes early, including: Setting up the real-time data pipeline using SingleStore Building the hourly and on-demand metrics computation framework Use cases for real-time experiment metrics Data Pipeline Below is the high level architecture of the real-time experiment metrics. Streamliner Streamliner is an integrated SingleStore and Apache Spark solution for streaming data from real-time data sources such as Kafka. We use it ingest data from Kafka and persist into SingleStore (Kafka -> Spark Streaming -> SingleStore). The latency is sufficient for our near real-time analysis. SingleStore SingleStore is a high throughput, highly scalable, highly compatible distributed relational database. Its high performance querying capability allows us to do near real-time ad-hoc analysis. We use SingleStore as our real-time database to store all the data, with a 24-hour retention policy. The data stored in SingleStore is used to compute scheduled and on demand metrics. The result is then used to populate the dashboard. Dashboard SingleStore provides several ways to connect to SingleStore backend using popular MySQL-compatible clients and libraries. We use the Python library provided by SingleStore to connect to the database and run scheduled jobs or on-demand queries. The real-time experiment metrics dashboard consists of two parts: Pre-computed hourly metrics within the last 24 hours Real-time metrics within the last hour Pre-Computed Metrics Considering that more granular experiment metrics (less than one hour) can be noisy, and computation on demand isn’t efficient, we decided to have an hourly cron job to pre-compute these metrics for the past hour. When we go to the real-time dashboard, these pre-computed metrics loads instantly. Each cell is the metric for one hour. Real-Time Metrics We can also compute the metrics on demand within last hour, which gives us the capability to view insights right after an experiment launches. Applications Validate triggering Triggering issues are still among the more common, including: We may forget to activate the experiment or call the wrong API, so that no Pinners are logged Or we may activate the experiment incorrectly, so that Pinners in one group are never triggered Or Pinners seeing the treatment are incorrectly triggered into a control group Avoiding these triggering issues is key for running successful experiments. In order to prevent mistakes, it’s always a good idea to verify that triggering is working properly for all the groups before turning them on (expand the group size from 0 to 1 percent, for example). We previously used to tail Kafka topics to find triggering records, which was painful for most developers. With the real-time experiment dashboard, the validation can be done for free. Once we’ve triggered all the groups, we can compute the on-demand metrics, and it’ll show us how many Pinners were triggered into each group within a specified time interval. Confirm group size change As our new experiment platform allows the group size change to take effect almost immediately, it’d be great to also confirm the group change in real-time. After making the group size change, we can check out the pre-computed hourly metrics after an hour. For example, if we ramp up from 1 to 5 percent, the new hourly Pinners joined should be roughly 5x compared to the previous hour. If we don’t want to wait, we can also compute the metrics on-demand and do rough estimates. Check Group Balance Unbalanced groups is another common issue among experiments, and so the sooner we can detect the unbalance, the better. Again, the real-time experiment dashboard can help. After ramping up the group sizes for a few hours, we can look at the pre-computed hourly metrics, which will show how many distinct users were triggered into each group in the last few hours. Monitor Core Metrics Another use case for real-time experiment dashboard is detecting disastrous changes quickly. After ramping up the group size or making any change, we can see how the core metrics are performing. If the metrics are significantly down, we should look into the experiment immediately and see if there are any bugs. We may also want to turn off the experiment for investigation and prevent further damage. Summary Real-time experiment metrics is tremendously helpful for getting quick insights into experiments. This system has been in production for more than six months, and has already helped us in numerous situations. If you’re interested in experiment framework and analytics platforms, join us on the Data Engineering team! Acknowledgements: Multiple teams across Pinterest provide insightful feedbacks and suggestions building the realtime experiment dashboard. Major contributors include Bryant Xiao, Chunyan Wang, Justin Mejorada-Pier, Shuo Xiang, Yu Yang, John Elliott and the rest of Data Engineering team. Get The SingleStore Spark Connector Guide The 79 page guide covers how to design, build, and deploy Spark applications using the SingleStore Spark Connector. Inside, you will find code samples to help you get started and performance recommendations for your production-ready Apache Spark and SingleStore implementations. Download Here", "date": "2016-06-09"},
{"website": "Single-Store", "title": "memsql-guide-to-spark-summit-2016", "author": ["Monica Multer"], "link": "https://www.singlestore.com/blog/memsql-guide-to-spark-summit-2016/", "abstract": "Spark Summit 2016 kicks off this week with more than 90 sessions and five tracks to choose from in the heart of San Francisco. The three day marathon of learning, which includes an entire day dedicated to Spark Training, attracts more than 2,500 engineers, business professionals, scientists, and analytic enthusiasts from across the country. https://spark-summit.org/2016 Hilton San Francisco 333 O’Farrell St, San Francisco, CA 94102 USA Throughout the show, speakers will address the different ways that companies are leveraging Spark and emergent ecosystems that have developed from new user needs. SingleStore will host a speaking session on Wednesday, June 8th from 5:25-5:55pm in the Franciscan Room. Nikita Shamgunov, the CTO and co-Founder of SingleStore, will conduct a live demonstration, PowerStream, based on real-time data from 2 million sensors on 197,000 wind turbines installed on wind farms around the world. This session showcases SingleStore Streamliner, an integrated Apache Spark solution, which is at the core of PowerStream, and enables real-time predictions on the health of each wind turbine. Nikita will delve deeper than just a demo, he will also discuss building a pipeline for predictive analytics with Apache Kafka, machine learning, SingleStore, and integration with Tableau. Other sessions we are keeping an eye on include: Interactive Visualization of Streaming Data Powered by Spark Ruhollah Farchtchi from Zoomdata in the Franciscan Room, June 7th from 11:15-11:45am. Airstream: Spark Streaming at Airbnb Liyin Tang and Jingwei Lu from Airbnb in the Franciscan Room, June 7th from 11:50-12:20pm. Building Realtime Data Pipelines with Kafka Connect and Spark Streaming Guozhang Wang from Confluent in the Franciscan Room, June 7th from 12:25-12:55pm. Video Games at Scale: Improving the gaming experience with Apache Spark Colin Borys and Xiaoyang Yang from Riot Games in the Franciscan Room, June 8th from 11:50-12:20pm. Is Apache Spark the Future of Data Analysis? Nick Heudecker from Gartner in the Franciscan Room, June 8th from 2:00-2:30pm. Interested in learning more? Check out the Spark resources below. SingleStore at Spark Summit 2016 Spark Summit’s Event Page for SingleStore Session News Release about SingleStore at Spark Summit From Spark to Ignition: Fueling Your Business on Real-Time Analytics The State of In-Memory and Apache Spark Visit the SingleStore booth #C5 June 7-8th 9am-6pm to grab a free t-shirt and watch a live SingleStore demo!", "date": "2016-06-06"},
{"website": "Single-Store", "title": "case-study-novus-partners", "author": ["Gary Orenstein"], "link": "https://www.singlestore.com/blog/case-study-novus-partners/", "abstract": "Serving Investment Managers Novus is a portfolio intelligence platform that helps the world’s top investors generate higher returns. The company works with 100+ of the world’s top investment managers and institutional investors, managing approximately $2 trillion. The platform helps the industry’s top investors collectively innovate and gain valuable insights into their investments. Changing How the World Invests (An edited transcript of this blog post, with updates, can be found here . Ed.) Novus aims to change how the world invests by allowing their users to explore investments from publicly sourced data, and to understand their risks and potential returns. Users can log in to the Novus platform to analyze individual hedge fund portfolios, aggregate industry trends, or analyze their own private data. According to Noah Zucker, Vice President of Tactical Engineering at Novus, “Our mission is to help investors discover their true investment acumen, identify their skill-sets, and understand potential risks.” Initially Facing an ETL Barrier As the Novus team began to scale their platform, they encountered a barrier with ETL (Extract, Transform, Load) that was causing them to monitor a delicate process requiring handholding nearly 24/7. If there was an issue, they had to immediately spring into action. And the worst cases of having to load jobs during the day led to an application slowdown for all of the users. They had tried scaling up with larger servers but the prior database could not keep up. The team began investigating options to scale out on the cloud. In particular they wanted to find an option that would not require heavy rework of their application model. Essentially if they had had to introduce a sharding strategy on their own, it would have required a change to business logic. Before SingleStore… 24/7 ETL Handholding Overnight Failure = Business Hours Slowdown Scala worker pool limited by the database Non-trivial code changes needed to shard and scale Investing in New Technologies The team came across SingleStore and began to rework their pipeline. Clients provide data in all sorts of formats, and that data is loaded through a cleanup process into a persistent store. Then the Novus platform takes that data and sends it into a distributed compute layer of Scala nodes which then places the data in SingleStore so clients have all of that data available to them. Now high intensity computations are available to clients immediately instead of minutes or hours. The Bottom Line for the ETL Team Typical load went down from 90 minutes to 2 minutes With SingleStore Client team focuses on service, not ETL Predictable application performance Scala workers: 12 → 126 Add servers to scale – No code changes needed Keeping Developers Happy Anytime there is a database change, there is a potential to create extra work for developers, but SingleStore uses the MySQL protocol for access, making the experience familiar and comfortable for developers. There is an entire tool chain available, and the learning curve is simple. In addition, SingleStore has first class JSON support and the Novus team was able to map their JSON format from MongoDB to SingleStore. They also wrote a blog post about using the SingleStore JSON Column Type: https://singlestore.com/blog/json-column-type/ Bottom Line For Novus, the bottom line was that the client data team could focus on delivering value to customers and helping them understand data and investments instead of tending to the ETL process during the day. In the event of any kind of ETL failure during the day, there is no impact to end users. Further, the Novus architecture is not limited by the prior database and the team was able to scale from 12 to 126 workers Scala workers, more than 10x improvement. Now for further scale, they can just add more servers to the database. For Novus, SingleStore fits the operational model. As an application developer there is no need to change the code to increase scale. By having a set of well written SQL code, the ops team can scale by just adding more servers. With SingleStore, Novus can manage the installation with just one DBA and architect for few hours a week, and they do not have to devote a whole team for care and feeding of the data platform. “It pretty much takes care of itself once we have it set up” – Noah Zucker, VP, Engineering – Novus Hear directly from Novus at Strata + Hadoop World New York September 2015: https://youtu.be/RvyB_ogIDSE", "date": "2016-06-14"},
{"website": "Single-Store", "title": "rbac-security", "author": ["Doug Doan"], "link": "https://www.singlestore.com/blog/rbac-security/", "abstract": "Enterprises seek real-time data and analytics solutions to stay current in competitive, fast-evolving markets. Companies dealing in private information, such as healthcare organizations, financial institutions, and the public sector have historically been limited in their pursuit of real-time results, given stringent security requirements. Today, we announce the availability of SingleStore DB 5.1. This release adds Role-Based Access Control (RBAC) to the already powerful SingleStore DB 5, unlocking the gateway to real-time for companies with comprehensive security requirements. Industry Standard Security Protecting data from malicious users is a top priority for SingleStore customers. Granting access to specific people is a key mechanism for data protection. But managing users one at a time is difficult to when a large number of people use the system. Role-Based Access Control (RBAC) is an industry best practice used by many organizations for simplifying data access management. With this release, SingleStore Enterprise Edition enables organizations to leverage the power of roles. To enable this feature, two new objects were added to the security model: roles and groups. Roles are collections of permissions and groups are collections of users. Roles are then applied to groups and users are put into groups. Customers can create their own roles such as: Security Officer – manages users and passwords Cluster Administrator – manages the SingleStore cluster Backup Operator – performs backups Application User – executes per-application DML statements SingleStore also provides management functions that determine the set of permissions for a given user. The RBAC feature was extensively tested with a rigorous set of functional and performance tests, including inside a FIPS 140-2 environment. With our performance tests, we validated that a cluster can have up 30,000 roles and 30,000 groups without any significant degradation in query performance. Read more about RBAC in SingleStore reference documentation . Try SingleStore Today Download SingleStore today and harness your real-time data. Build streaming data pipelines in minutes, taking your data ingest and analytics to unprecedented levels of scale and sophistication while keeping your data secure.", "date": "2016-06-15"},
{"website": "Single-Store", "title": "data-and-analytics-predictions-through-2020", "author": ["Kevin White"], "link": "https://www.singlestore.com/blog/data-and-analytics-predictions-through-2020/", "abstract": "Everyone loves predictions. Especially, when said predictions are speculating on the future of technology adoption – where a slight change in user behavior could result in the next multi-billion dollar industry. Gartner Analysts, Douglas Laney and Ankush Jain, recently published a research report with Gartner’s top 100 data and analytics predictions relevant to CIOs, CDOs, and analytics leaders. This research stems from hundreds of interactions with key stakeholders at data-driven organizations, and includes predictions on topics ranging from advanced analytics and data science to changes in business functions and industries. For a limited time, SingleStore is hosting full access to this research. See all 100 predictions here ⇒ Sample of predictions by category: Core Analytics Predictions “By 2018, over half of large organizations globally will compete using advanced analytics and proprietary algorithms, causing the disruption of entire industries.” Business Intelligence “By 2019, 80% of new applications using IoT or machine data will analyze data in motion as well as collect this information for analysis of data at rest.” Information Strategy “By 2020, the IoT and digital business will drive requirements in 25% of new information governance and master data management implementations.” Information Technology Infrastructure Predictions “By 2020, more than half of major new business processes and systems will incorporate some element, large or small, of the IoT.” Digital Business/Commerce and Business Function Predictions “By 2020, smart personalization engines used to recognize customer intent will enable digital businesses to increase their profits by up to 15%.” Access all 100 Data and Analytics Predictions ⇒", "date": "2016-06-22"},
{"website": "Single-Store", "title": "geospatial-data-meetup-with-mapbox", "author": ["Emily Friedman"], "link": "https://www.singlestore.com/blog/geospatial-data-meetup-with-mapbox/", "abstract": "The global availability of mobile technology means that everyone is connected on the go. For businesses to truly penetrate the consumer market in the age of on-demand products and services, they must find a way to make use of mobile data. Every data point has a place – this is where geospatial data analytics comes into play. The ability to analyze geospatial data and build applications that utilize location will separate market domineers from names left behind. If you know when and how connected consumers interact in different places, can harness data from sensors and IoT, and machine-to-machine communication, you can deliver the most efficient, personalized experience. SingleStore Meetup with Mapbox: Visualize Your World with Geospatial Data Our next meetup spotlights geospatial data analytics. Join us in SoMa, San Francisco to learn about innovative tools for mastering geospatial analytics and building geo-enabled applications. RSVP for our Geospatial Data Meetup on Wednesday, June 29, 2016 ⇒ Featuring Matt Irwin of Mapbox Matt will showcase data visualization with the Mapbox platform. Neil Dahlke of SingleStore will follow with a live demonstration of PowerStream and Supercar, two real-time applications built on top of Mapbox. Meetup Schedule 6:00 – 6:30: Happy hour and arrivals 6:30 – 6:50: Mapbox demonstration 6:50 – 7:10: PowerStream and Supercar demonstrations 7:10 – 7:20: Q&A 7:20 – 8:00: Continued happy hour RSVP: http://www.meetup.com/SingleStore/events/231735734/ Geospatial Examples SingleStore comes equipped with geospatial capabilities. Let’s explore a few examples. PowerStream An IoT showcase application, PowerSteam models predictive analytics for global wind farm health. Individual wind turbines in these farms have a number of sensors attached to them, assessing temperature and vibrations. In order to harness this data, we constructed a real-time data pipeline, beginning with Apache Kafka, a distributed message queue, at the front. Sensor data is then streamed into SingleStore via Streamliner, an integrated Apache Spark solution. Add in a Spark MLlib predictive model to determine whether a wind turbine is likely to fail, and serve the data up through SingleStore to a front-end dashboard. In this case, the dashboard is provided by Tableau, and the scrolling map showcasing the predicted health of wind turbines is provided by Mapbox. Watch SingleStore CTO, Nikita Shamgunov, build PowerStream live below: Supercar Supercar is a geospatial application that analyzes New York taxi rides in real time. The dataset contains location information from actual rides that took place throughout Manhattan. With Supercar, you can run easy SQL queries in SingleStore to analyze the average length of ride, or average cost of ride, for example. Pinterest The data science team at Pinterest uses Apache Kafka, Spark Streaming, and SingleStore to ingest tens of thousands of events per second and aggregate that event data, which in this case is pins and repins. Kafka serves as the message queue, Spark provides the transformation tier, and the operational database offers persistence and a serving layer for an application that allows for quick analysis of real-time trending topics by geography. Watch the demo:", "date": "2016-06-23"},
{"website": "Single-Store", "title": "spark-for-machine-learning", "author": ["Nicole Nearhood"], "link": "https://www.singlestore.com/blog/spark-for-machine-learning/", "abstract": "At Spark Summit in San Francisco, we highlighted our PowerStream showcase application, which processes and analyzes data from over 2 million sensors on 200,000 wind turbines installed around the world. We sat down with one of our PowerStream engineers, John Bowler, to discuss his work on our integrated SingleStore and Apache Spark solutions. What is the relationship between SingleStore and Spark? At its core, SingleStore is a database engine, and Spark is a powerful option for writing code to transform data. Spark is a way of running arbitrary computation on data either before or after it lands in SingleStore. The first component to SingleStore and Spark integration is the SingleStore Spark Connector , an open-source library. Using the connector, we are able to use Spark as the language for writing distributed computations, and SingleStore as a distributed processing and storage engine. For those familiar with Spark, here is how the SingleStore Spark Connector allows tight integration between SingleStore and Spark: Using SingleStoreContext.sql(\"SELECT * FROM t\") , you can create a DataFrame in Spark that is backed by a SingleStore table. When you string together a bunch of SparkSQL operations and call collect() on the result, these DataFrame operations will actually run in the SingleStore database engine as direct SQL queries. This can give a major performance boost due to the SQL-optimized nature of SingleStore. Using df.saveToSingleStore() , you can take a DataFrame and persist it to SingleStore easily The second component to SingleStore and Spark integration is Streamliner , which is built on top of the Spark Connector. Streamliner enables you to use Spark as a high-level language to create Extract, Transform, Load (ETL) pipelines that run on new data in real time. We built Streamliner around a ubiquitous need to ingest data as fast as possible and query the information instantly. With Streamliner, you can write the logic of your real-time data analytics pipeline such as parsing documents, scoring a machine-learning model, or whatever else your business requires, and instantly apply it to your SingleStore cluster. As soon as you have raw analytics data available for consumption, you can process it, see the results in a SQL table, and act on it. What type of customer would benefit from the SingleStore Streamliner product? A customer who is already using Kafka to collect real-time information streaming from different sources can use Streamliner out-of-the-box. Without writing any code, you can take all the data in a Kafka topic and append it to a SingleStore table instantly. SingleStore will automatically place this in a JSON format by default so no additional work is required. However, if you want to take semi-structured or unstructured “messages” and turn them into “rows” for SingleStore, you can write arbitrary code in the Streamliner “Transform” step. Streamliner also allows you to do this inside the web browser console. Consider this example – suppose you want to make a dashboard that will monitor data from your entire company and produce real-time visualizations or alerts. Your production application is inserting into a production database, emitting events, or outputting logs. You can optimize this dashboard application by taking all of this data and routing it to a distributed message queue such as Kafka, or writing it directly to a SingleStore table. You can then write your data-transformation or anomaly-detection code in Spark. The output of this is data readily available in SingleStore for any SQL-compatible Business Intelligence tool, your own front-end application, or users in your company running ad-hoc queries. What is PowerStream? PowerStream is a showcase application that we built on top of Streamliner. It’s an end-to-end pipeline for high-throughput analytics and machine learning. We have simulation of 20,000 wind farms (200,000 individual turbines) around the world in various states of disrepair. We use this simulation to generate sample sensor data, at a rate of 1 to 2 million data points per second. Using a co-located Kafka-Spark-SingleStore cluster, we take these raw sensor values and run them through a set of regression models to determine 1) how close each turbine is to failing, and 2) which part is wearing down. In your opinion, what is the most interesting part of the PowerStream showcase application? I am personally interested in the data science use case. PowerStream demonstrates how we can deploy a machine learning model to a cluster of nodes, and “run” the model on incoming data, writing the result to SingleStore in real time. Data science is a big field and running machine learning models in production is an important part, but of course not the whole picture. Data exploration, data cleaning, feature extraction, model validation – both interactively (offline) and in production (online) – are all parts of a complete data science workflow. Watch the SingleStore PowerStream session at Spark Summit with CTO and Co-founder, Nikita Shamgunov If you would like to try SingleStore, you can download our Commununity Edition at singlestore.com/free Get The SingleStore Spark Connector Guide The 79 page guide covers how to design, build, and deploy Spark applications using the SingleStore Spark Connector. Inside, you will find code samples to help you get started and performance recommendations for your production-ready Apache Spark and SingleStore implementations. Download Here", "date": "2016-07-05"},
{"website": "Single-Store", "title": "third-normal-form", "author": ["Gary Orenstein"], "link": "https://www.singlestore.com/blog/third-normal-form/", "abstract": "Keeping it Straight Data value comes from sharing, so staying organized and providing common data access methods across different groups can bring big payoffs. Companies struggle daily to keep data formats consistent across applications, departments, people, divisions, and new software systems installed every year. Passing data between systems and applications is called ETL, which stands for Extract, Transform, and Load. It is the process everyone loves to hate. There is no glamour in reconfiguring data such as date formats from one system to another, but there is glory in minimizing the amount of ETL needed to build new applications. To minimize ETL friction, data architects often design schemas in third normal form, a database term that indicates data is well organized and unlikely to be corrupted due to user misunderstanding or system error. Getting to Third Normal Form The goal of getting to third normal form is to eliminate update, insertion, and deletion anomalies. Take this employee, city, and department table as an example: employee_id employee_name employee_city employee_dept 101 Sam New York 22 101 Sam New York 34 102 Lori Los Angeles 42 Update Anomalies If Sam moves to Boston, but stays in two departments, we need to update both records. That process could fail, leading to inconsistent data. Insertion Anomalies If we have a new employee not yet assigned to a department and the ‘employee_dept’ field does not accept blank entries, we would be unable to enter them in the system. Deletion Anomalies If the company closed department 42, deleting rows with department 42 might inadvertently delete employee’s information like Lori’s. First Normal Form to Start First normal form specifies that table values should not be divisible into smaller parts and that each cell in a table should contain a single value. So if we had a customer table with a requirement to store multiple phone numbers, the simplest method would be like this customer_id customer_name customer_phone 101 Brett 555-459-8912 555-273-2304 102 Amanda 222-874-3567 However, this does not meet first normal form requirements with multiple values in a single cell, so to conform we could adjust it to customer_id customer_name customer_phone 101 Brett 555-459-8912 101 Brett 555-273-2304 102 Amanda 222-874-3567 Second Normal Form 2nd normal form requires that Data be in 1st normal form Each non-key column is dependent on the tables complete primary key Consider the following example with the table STOCK and columns supplier_id, city, part_number, and quantity where the city is the supplier’s location. supplier_id city part_number quantity A22 New York 7647 5 B34 Boston 9263 10 The primary key is (supplier_id, part_number), which uniquely identifies a part in a single supplier’s stock. However, city only depends on the supplier_id. In this example, the table is not in 2nd normal form because city is dependent only on the supplier_id and not the full primary key (supplier_id, part_number). This causes the following anomalies: Update Anomalies If a supplier moves locations, every single stock entry must be updated with the new city. Insertion Anomalies The city has to be known at insert time in order to stock a part at a supplier. Really what matters here is the supplier_id and not the city. Also unless the city is stored elsewhere a supplier cannot have a city without having parts, which does not reflect the real world. Deletion Anomalies If the supplier is totally out of stock, and a row disappears, the information about the city in which the supplier resides is lost. Or it may be stored in another table, and city does not need to be in this table anyway. Separating this into two tables achieves 2nd normal form. supplier_id part_number quantity A22 7647 5 B34 9263 10 supplier_id city A22 New York B34 Boston Third Normal Form We’re almost there! With 1st normal form, we ensured that every column attribute only holds one value. With 2nd normal form we ensured that every column is dependent on the primary key, or more specifically that the table serves a single purpose. With 3rd normal form, we want to ensure that non-key attributes are dependent on nothing but the primary key. The more technical explanation involves “transitive dependencies” but for the purpose of this simplified explanation we’ll save that for another day. In the case of the following table, zip is an attribute generally associated with only one city and state. So it is possible with a data model below that zip could be updated without properly updating the city or state. employee_id employee_name city state zip 101 Brett Los Angeles CA 90028 102 Amanda San Diego CA 92101 103 Sam Santa Barbara CA 93101 104 Alice Los Angeles CA 90012 105 Lucy Las Vegas NV 89109 Splitting this into two tables, so there is no implied dependency between city and zip, solves the requirements for 3rd normal form. customer_id customer_name zip 101 Brett 90028 102 Amanda 92101 103 Sam 93101 104 Alice 90012 105 Lucy 89109 zip city state 90028 Los Angeles CA 92101 San Diego CA 93101 Santa Barbara CA 90012 Los Angeles CA 89109 Las Vegas NV Benefits of Normalization Normalizing data helps minimize redundancy and maintain the highest levels of integrity. By organizing column attributes and the relations between tables, data administrators can design systems for efficiency and safety. More specifically, normalization helps ensure Data is not unnecessarily repeated within a database Inserts, modifications, and deletions only have to happen once in a database Data Management with Star Schema Star schema is an approach of arranging a database into fact tables and dimension tables. Typically a fact table records a series of business events such as purchase transactions. Dimension tables generally store fewer records than fact tables but may have more specific details about a particular record. A product attributes table is one example. Star schemas are often implemented in a denormalized fashion, with typical normalization rules relaxed. The advantage of this can be simpler reporting logic and faster performance as data may be stored multiple ways to facilitate queries. The disadvantage of this approach is that integrity is not necessarily enforced through the model leaving room for an update in one place that may not successfully propagate elsewhere. Further, with normalization, a large variety of data analytics tools and approaches can be used to query data without explicit advanced knowledge. Without normalization, schemas tend to become isolated to specific functions and less flexible across a large organization. Flexible Star Schema Deployments with SingleStore Is it possible or desirable to merge normalization and star schemas? Sure. While data management strategies can be very application specific, retaining data in the most universally accessible forms benefits larger organizations. With normalization, data organization transcends application use cases and database systems. Star schemas often skip normalization for two reasons: simplicity of queries and performance. Regarding query simplicity, this is a tradeoff between application-specific approaches and data ubiquity across an organization. Independent of the database, this tradeoff remains. When it comes to performance, historical systems have had challenges with operations like fast aggregations, and a large number of joins driven by third normal form. Modern database architectures have eliminated those performance challenges. With a solution like SingleStore, a memory-optimized, relational, distributed database, it is possible to achieve normalization and performance. Even with the increased number of tables, and subsequent joins, often resulting from third normal form, SingleStore maintains stellar performance. And the core relational SQL model makes it easy to create or import a range of tables as well as maintain relations between tables. In the next sections, we’ll explore table types in SingleStore and the associated benefits. Using Multiple Table Types in SingleStore SingleStore includes two table types: A rowstore table where all the data is retained in memory and all data is persisted to disk A columnstore table where some data resides in memory and all data is persisted to disk Using these two table types is it possible to design a wide range of schema configurations. Contrary to popular belief, determining whether you use an all-memory or memory-plus-disk table has less to do with data size, and more with how you plan to interact with the data. Columnstores are useful when rows are added or removed in batches, and when queries touch all or many records but only for a few columns. Aggregations like Sum, Average, and Count are good examples.Rowstores work well when operating over whole rows at a time. This includes updates to individual attributes or point lookups.For more detail on rowstores and columnstores check out Should You Use a Rowstore or a Columnstore? From SingleStore VP of Engineering Ankur Goyal Creating a Star Schema in SingleStore Whether or not you lean towards normalization, SingleStore makes it easy to create a star schema within a single database across multiple table types. Figure: Basics of a star schema with Fact and Dimension tables Both dimension tables and fact tables can be row or column based depending on requirements, which should be focused more on data use and operations rather than just capacity. Dimension Tables Fact Tables Recommendations Row Row Most flexible option for performance across a wide range of queries. Best overall performance for small scans (1000s of rows). Row Column Best when there is lots of Fact data. Performance will be especially good when queries involve lots of scans. Column Column Good when there is lots of Dimension data and the data is append-only. Opportunities for Normalization and Star Schema Whether your preferences sway towards normalization, star schema, or a combination, SingleStore provides a simple and easy way to achieve a standard data model that leverages the relational capabilities of ANSI SQL. Beyond that, the SingleStore architecture delivers the utmost in performance and scalability, including the ability to ingest data from multiple systems and datastores for real-time analytics. This is a frequent use case for large corporations that need to track a variety of data sources with consolidated reporting. Of course handling all of this in real time brings another level of visibility and usefulness to the data. Now businesses can get a pulse of activity in the moment, allowing them to adapt and learn in real time. Interested in giving SingleStore a try? Download here http://www.singlestore.com/free References https://www.wikipedia.org/ http://beginnersbook.com/2015/05/normalization-in-dbms/ https://www.1keydata.com/database-normalization/second-normal-form-2nf.php http://www.essentialsql.com/get-ready-to-learn-sql-11-database-third-normal-form-explained-in-simple-english/", "date": "2016-07-06"},
{"website": "Single-Store", "title": "girls-who-code", "author": ["Monica Multer"], "link": "https://www.singlestore.com/blog/girls-who-code/", "abstract": "Last week, SingleStore hosted 20 young women from the Girls Who Code Summer Immersion Program. Over the course of 7 weeks, the group visits some of the Bay Area’s hottest tech companies to gain exposure to the tech industry. These aspiring female engineers, programmers, and future tech leaders are a part of a greater movement to bridge the gender divide in tech workplaces. Girls Who Code is a non-profit organization founded in 2012 dedicated to inspiring young women to pursue education and careers in STEM subjects – Science, Technology, Engineering, and Math. They are building the largest pipeline of female engineers in the United States, and their rapid expansion since genesis reinforces their mission. Beginning with just 20 girls in New York, today Girls Who Code is going strong with 10,000 girls across 42 states. These 10-11th graders with excited eyes and curious minds came to SingleStore looking for a taste of what it is like to work for a technology startup. They got to experience the combination of a challenging technical product and fun culture that characterizes our company. The girls had a full day beginning with a company introduction by Ankur Goyal, Vice President of Engineering, and ending with an extensive Q&A panel composed of SingleStore engineers. The unstructured time the young women had while at SingleStore provided important opportunities to speak and network with employees across engineering, marketing, sales, human resources, and operations for broader exposure to the kinds of teams and roles that constitute a startup. The girls also took breaks from digesting product demonstrations by playing a quick game of ping pong, enjoying a moment with the dogs in the office, or grabbing a snack from the dining area. Each moment the young women experienced reflected the real quotidian routines of the employees of SingleStore and demonstrated what it truly is like to work in the tech industry. The Q&A panel, comprised of eight employees across engineering and business teams, encouraged the young women to display their avid curiosity and hunger to learn about working in the tech world. They asked about challenges the women of SingleStore have experienced as women working in technology, the best ways to prepare for interviews for tech jobs, how SingleStore employees wound up where they are today, and why they love working for a small startup instead of a larger corporation. Each panel member spoke about their personal experiences in the industry, providing a glimpse into a possible future for these young women. In the end, the girls walked away with deep insight into the inner workings of a San Francisco technology startup, and the knowledge that many open doors await them in their future careers.", "date": "2016-07-13"},
{"website": "Single-Store", "title": "top-5-demos", "author": ["Kevin White"], "link": "https://www.singlestore.com/blog/top-5-demos/", "abstract": "We live in an age of digital realities. Most “goods” on the market are not physical entities, but more often software. However, consumers still need a tangible way to evaluate these digital products and services they are purchasing. Enter the product demonstration – the best way to showcase the value of software in a short time. At SingleStore, we pride ourselves on showcasing relatable product demonstrations from the main stage at Strata+Hadoop World to our locally hosted meetups . Here’s a look at our top five showcase applications over the past year: PowerStream IoT Application PowerStream is an Internet of Things (IoT) application that displays visualizations and alerts from approximately 20,000 wind farms and 200,000 individual wind turbines. Temperature and vibration input from millions of sensors on turbines is ingested through Apache Kafka and inserted into SingleStore for real-time data exploration. Using this data and a machine learning algorithm, PowerStream predicts and visualizes the health of each turbine up to the last second. Dataset 2 million sensors across 197,000 global wind turbines. Results Less than $20k annual cost. Runs on seven c4.2xlarge AWS instances at approximately $0.311 per instance/hour. Processing more than 1 million transactions per second. Real-Time Analytics at Pinterest Pinterest uses SingleStore Streamliner to ingest data into SingleStore using Apache Kafka and Spark Streaming. In this application, Pinterest is filtering trending Pins across the United States and enriching data by adding geolocation and Pin category information. As data streams in, Pinterest is able to run queries in SingleStore to generate engagement metrics and report on various event data like pins, repins, comments and logins. Read the full Pinterest Engineering Blog Post here: https://engineering.pinterest.com/blog/real-time-analytics-pinterest Dataset Pinterest Pin data across the United States. Results After integration, Pinterest now has a system that allows analysts to use SQL to explore data and derive insights in real time. Supercar: Real-Time Geospatial Intelligence Supercar leverages SingleStore geospatial capabilities to track and analyze 170 million real world taxi rides. By sampling this dataset and creating real-time records while simultaneously querying data, Supercar simulates the ability to achieve real-time insights across hundreds of thousands of objects on the go. Dataset 170 million real world taxi rides. Results Query and visualize geospatial data with SQL. Insights hundreds of thousands of objects in under a second. Taxistats Real-Time Geospatial Dashboard Taxistats is a showcase application that makes use of the same dataset used in Supercar. Data is streamed into SingleStore and visualized with Zoomdata to simulate pickup and drop-off information from real taxi ride data. The Taxistats dashboard displays data as it is collected while simultaneously running SQL analytics. Dataset 170 million real world taxi rides. Results Sub second SQL queries on millions of taxi rides. Exploring Wikipedia Trends with Wikistats Our Wikistats application compares the popularity of entries from Wikipedia’s public dataset consisting of more than 250 billion records. Similar to Google Trends, multiple search terms can be queried and trends are visualized over the course of a year. When a query is run, SingleStore scans over 30 billion records and returns results in less than 200 milliseconds. Dataset Wikipedia public dataset of 250 billion records. 30 billions rows scanned in under 200 milliseconds. Results 10x faster and 60x more cost effective than Oracle 12c. Runs on a 6-node cluster that costs about $5/hr on Amazon or less than $20k. Scans and aggregates tens of billions of records in milliseconds.", "date": "2016-07-21"},
{"website": "Single-Store", "title": "seven-talks-you-cant-miss-at-gartner-catalyst-2016", "author": ["Lesia Myroshnichenko"], "link": "https://www.singlestore.com/blog/seven-talks-you-cant-miss-at-gartner-catalyst-2016/", "abstract": "The 2016 Gartner Catalyst Conference kicks off on August 15-18 in San Diego and will feature over 150 sessions for technical professionals. The conference offers eight in-depth tracks on topics including data centers, data and analytics, security, software, mobility, cloud, digital workplaces, and the Internet of Things. Book an in-person SingleStore demo at Gartner Catalyst ⇒ singlestore.com This year, Gartner has chosen the following theme to guide the experience at Catalyst: Architecting the On-Demand Digital Business. Each track at the show reinforces the importance of embracing modern architecture in order to sense, adapt, and scale businesses for long-lasting impact. There will be valuable opportunities to hear directly from leading analysts, who spent months researching and analyzing industry trends. Here are six sessions and a SingleStore speaking session that we recommend to stay on top of real-time data trends: From Data to Insight to Action: Building a Modern End-to-End Data Architecture Monday, 15 August 2016, 9:30 AM – 10:15 AM Carlie J. Idoine, Research Director, Gartner @CarlieIdoine For years, IT organizations have been dealing with a steady rise in the volume, velocity and variety of data. But now, unprecedented new data sources, such as IoT, are pushing infrastructures to the limit. This session defines a bold strategy and highly-scalable data management architecture built upon technologies such as cloud computing, predictive analytics and machine learning that scale, respond automatically, and unlock enormous business value. Ingesting and Transforming IoT Data in the Cloud: Amazon AWS IoT, Microsoft Azure IoT Hub and IBM Watson IoT Platform Monday, 15 August 2016, 11:20 AM – 12:05 PM Lyn Robison, Research VP, Gartner Building a scalable data management architecture starts with data ingestion. Using their scalable infrastructure, many of the hyperscale cloud providers have begun to offer data ingestion services. But will they work? Do they truly scale? This session explores the new data ingestion and stream processing services offered by cloud service providers: AWS, Azure and IBM. SingleStore: Real-Time Supply Chain Analytics with Machine Learning, Kafka, and Spark Monday, 15 August 2016, 1:30 PM – 1:50 PM Ankur Goyal, Vice President of Engineering, SingleStore @ankrgyl In today’s real-time world, enterprises like Kellogg and Macy’s are embracing predictive analytics to improve supply chain operations and drive revenue. In this session, Ankur Goyal, Vice President of Engineering at SingleStore, will demonstrate an application for global monitoring of distribution centers to determine supply and fulfill demand. With Apache Kafka, Spark, machine learning, and SingleStore, this showcase application predicts which centers are at risk of production failures, which can result in insufficient supply. Architecting Real-Time Data — Really? Monday, 15 August 2016, 2:30 PM – 3:15 PM Svetlana Sicular, Research VP, Gartner @Sve_Sic Life is accelerating, patience is diminishing, interest in real time is increasing. But what is “real time” — today or a nanosecond? How do architects implement low-latency solutions that are just in time? What is possible now and what should wait? Architects of real-time solutions have a variety of choices that will be explored in this session: popular architectures — Lambda and Kappa, in-memory data stores, or supercomputer-type processing acceleration. Workshop: Adopt Data Federation/Virtualization to Support Analytics and Data Services Tuesday, 16 August 2016, 2:10 PM – 3:40 PM Mei Yang Selvage, Research Director, Gartner @data_mei Data virtualization (DV) — aka data federation — accelerates analytics and delivers data services for operational applications. It also provides a shared data access layer, which is essential for digital business. But DV also has many constraints such as performance and data quality. This workshop provides a systematic architectural approach to select proper DV use cases and document rationals. Assessing Hybrid Architectures for Cloud Computing Wednesday, 17 August 2016, 9:30 AM – 10:15 AM Alan D Waite, Research Director, Gartner @Alan_D_Waite Most traditional IT organizations have existing data centers that need continued support while cloud technologies are adopted. Organizations will increasingly live in a hybrid world with some applications in the public cloud and others in data centers. Gartner sees a hybrid IT world emerging in four distinct deployment scenarios, which we review in this session. Roundtable: Closing the Loop – Operationalizing Predictive and Prescriptive Models Wednesday, 17 August 2016, 1:35 PM – 2:35 PM Carlie J. Idoine, Research Director, Gartner @CarlieIdoine Developing robust predictive and prescriptive models is only the first challenge with leveraging advanced analytic successfully within an organization. Join this session to discuss techniques for deploying and operationalizing analytic models to “close the loop” on adding real business value. Stop by the SingleStore Booth There will be complimentary copies of our O’Reilly book Building Real-Time Data Pipelines and great swag! Members of the SingleStore team will be available for questions and will also be demonstrating live our latest machine learning application. Drop by to learn why SingleStore is the only startup featured on the right quadrant of Gartner’s Magic Quadrant for Operational Database Management Systems and Magic Quadrant for Data Warehouse Solutions for Analytics! After day one at the show join us for Happy Hour on Monday, August 15, from 6:00 pm to 8:00 pm at Lion’s Share located at 629 Kettner Blvd., San Diego, CA 92101. RSVP HERE ⇒", "date": "2016-07-27"},
{"website": "Single-Store", "title": "dbbench-streamlines-database-workload-testing", "author": ["Alex Reece"], "link": "https://www.singlestore.com/blog/dbbench-streamlines-database-workload-testing/", "abstract": "Our performance engineering team is committed to delivering high quality tools. Since we released dbbench 7 months ago , it has been widely adopted across our engineering and sales teams as the definitive tool for testing database workloads. Today we are announcing availability of a new version of dbbench , as well as a package of high level tools to enhance it. In this latest release, we enhanced both the flexibility and ease of use of the tool. We augmented capabilities of dbbench and added a tutorial to help new sales engineers get started. In addition to these enhancements, we released a package of internal tools specifically designed to support high level workflows that use dbbench . These changes improve our technical proof-of-concept (POC) process for our customers and open up dbbench for new workloads and use cases. This version of dbbench not only increases the performance and power of this benchmark testing tool, but also makes it easily accessible to anyone interested in using it. dbbench in Action One of the new features of dbbench is the ability to display latency histograms during the final analysis to easily detect and understand outliers. In the example below, we can see that most of the queries executed between 0.2 and 0.5 ms, but there were outliers as high as 30ms: To support more complicated and varying data, dbbench can read arguments from a .csv file and parameterize queries via the query-args-file parameter. The snippet below shows this parameter specified in a configuration.ini file for dbbench : [insert]\nquery=insert into table colors (?)\nquery-args-file=/tmp/colors.csv In addition, the query-results-file parameter allows dbbench to chain multiple jobs together, and makes it possible to have external tools in between each of these jobs. dbbench-tools dbbench-tools is a package of sales engineering tools, used by the SingleStore team to easily describe and efficiently run a customer workload during a technical POC. The most important question to answer during a POC is “how will SingleStore scale to more concurrent users?” dbbench-scaler is one of several tools provided by the dbbench-tools package to help answer this question. dbbench-scaler runs a dbbench - defined workload at different levels of concurrency to find the limits of the database installation. For example, I was doing a POC where I wanted to determine how many concurrent inserts a small SingleStore cluster could handle. I set up a simple dbbench test: [setup]\nquery=create table test(a varchar(20), b float, c float, d float, \\\n    shard key a (a), key b (b), key c (c), key d (d))\n\n[teardown]\nquery=drop table test\n\n[batch sized 128 multi-inserts]\nquery=insert into test values (?, ?, ?, ?), (?, ?, ?, ?), …, (?, ?, ?, ?)\nquery-args-file=/tmp/data.csv\nconcurrency=1 I ran dbbench-scaler with this configuration to determine that this small test cluster could sustain 100,000 inserts per second in batches of 128, as depicted in the top half of the chart below. In the bottom half of the chart, we see that using more connections allows us to insert more records per second (RPS) until we reach 32 connections, after which there is no benefit to using more connections and we see a hugely adverse effect on query latency: $ dbbench-scaler --database=db --concurrency=1,2,4,8,16,32,64,128,256,512 --output out.png /tmp/test.ini In this example, not only did dbbench-scaler answer the question “will SingleStore handle the scale of many users”, it also found the ideal number of users for this application. Access the new version of dbbench here and get dbbench-tools here . Learn even more about the new features in our tutorial .", "date": "2016-08-02"},
{"website": "Single-Store", "title": "massive-data-ingest-concurrent-analytics", "author": ["Dale Deloy"], "link": "https://www.singlestore.com/blog/massive-data-ingest-concurrent-analytics/", "abstract": "The amount of data created in the past two years surpasses all of the data previously produced in human history. Even more shocking is that for all of that data produced, only 0.5% is being analyzed and used. In order to capitalize on data that exists today, businesses need the right tools to ingest and analyze data. At SingleStore, our mission is to do exactly that. We help enterprises operate in today’s real-time world by unlocking value from data instantaneously. The first step in achieving this is ingesting large volumes of data at incredible speed. The distributed nature of the SingleStore environment makes it easy to scale up to petabytes of data! Some customers use SingleStore to process 72TB of data a day, or over 6 million transactions per second, while others use it as a replacement for legacy data warehouse environments. SingleStore offers several key features for optimizing data ingest, as well as supporting concurrent analytics: High Throughput SingleStore enables high throughput on concurrent workloads. A distributed query optimizer evenly divides the processing workload to maximize the efficiency of CPU usage. Queries are compiled to machine code and cached to expedite subsequent executions. Rather than cache the results of the query, SingleStore caches a compiled query plan to provide the most efficient execution path. The compiled query plan does not pre-specify values for the parameters, which allows SingleStore to substitute the values upon request, enabling subsequent queries of the same structure to run quickly, even with different parameter values. Moreover, due to the use of Multi-Version Concurrency Control (MVCC) and lock-free data structures, data in SingleStore remains highly accessible, even amidst a high volume of concurrent reads and writes. Query Execution Architecture SingleStore has a two-tiered architecture consisting of aggregators and leaves. Aggregators act as load balancers or network proxies, through which SQL clients interact with the cluster. Aggregators store metadata about the machines in the cluster and the partitioning of the data. In contrast, leaves function as storage and compute nodes. Highly Scalable The highly scalable distributed system allows clusters to be scaled out at any time to provide increased storage capacity and processing power. Sharding occurs automatically and the cluster re-balances data and workload distribution. Data remains highly available and nodes can go down with no effect on performance. In addition to being fast, consistent, and scalable, SingleStore persistently stores data. Transactions are committed to disk as logs and periodically compressed as snapshots of the entire database. If any node goes down, it can restart using one of these logs. In-Memory and On-Disk Storage SingleStore supports storing and processing data with an in-memory rowstore, or a memory or disk-based columnstore. The rowstore works best for optimum performance in transactional workloads because of the sheer speed of in-memory processing. The columnstore operates best for cost-effective data storage of large amounts of historical data for analysis. A combination of the rowstore and columnstore engines allows users to analyze real-time and historical data together in a single query. Streamliner In 2015, SingleStore introduced Streamliner , an integrated Apache Spark solution. Streamliner allows users to build real-time data pipelines. It extracts and transforms the data through Apache Spark, and loads it into SingleStore to persist the data and serve it up to a real-time dashboard or application. Streamliner comes with a versatile set of tools ranging from development and testing applications to personalization for managing multiple pipelines in production. You can use Streamliner through the Spark tab in the SingleStore Ops web interface and through the SingleStore Ops CLI. In addition to saving time by automating much of the work associated with building and maintaining data pipelines, Streamliner offers several technical advantages over a home-rolled solution built on Spark: Streamliner provides a single unified interface for managing many pipelines, and allows you to start and stop individual pipelines without affecting other pipelines running concurrently. Streamliner offers built-in developer tools that dramatically simplify developing, testing, and debugging data pipelines. For instance, Streamliner allows the user to trace individual batches all the way through a pipeline and observe the input and output of every stage. Streamliner handles the challenging aspects of distributed real-time data processing, allowing developers to focus on data processing logic rather than low level technical considerations. Under the hood, Streamliner leverages SingleStore and Apache Spark to provide fault tolerance and transactional semantics without sacrificing performance. The modularity of Streamliner, which separates pipelines into Extract, Transform, and Load phases, facilitates code reuse. With thoughtful design, you can mix, match, and reuse Extractors and Transformers. Out of the box Streamliner comes with built-in Extractors, such as the Kafka Extractor, and Transformers, such as a CSV parser and JSON emitter. Even if you find you need to develop custom components, the built-in pipelines make it easy to start testing without writing much or any code up front. Trial License With these extensive capabilities for massive data ingest and analytics, SingleStore provides a robust solution for large influxes of data from IoT sources, business transactions, applications, and a variety of new sources cropping up today. Try it for yourself today: singlestore.com/free Get The SingleStore Spark Connector Guide The 79 page guide covers how to design, build, and deploy Spark applications using the SingleStore Spark Connector. Inside, you will find code samples to help you get started and performance recommendations for your production-ready Apache Spark and SingleStore implementations. Download Here", "date": "2016-08-04"},
{"website": "Single-Store", "title": "new-performance-benchmark-for-live-dashboards-and-fast-updates", "author": ["Gary Orenstein"], "link": "https://www.singlestore.com/blog/new-performance-benchmark-for-live-dashboards-and-fast-updates/", "abstract": "Newest Upsert Benchmark showcases critical use case for internet billing with telcos, ISPs, and CDNs SingleStore achieves 7.9 million upserts per second , 6x faster than Cassandra Benchmark details and scripts now available on GitHub The business need for fast updates and live dashboards Businesses want insights from their data and they want it sooner rather than later. For fast-changing data, companies must rapidly glean insights in order to make the right decisions. Industry applications like IoT telemetry monitoring, mobile network usage, internet service provider (ISP) billing, and content delivery network (CDN) usage tracking depend upon real-time analytics with fast-changing data. Web traffic merits special attention since it continues to grow at an astounding rate. According to Cisco , Global IP traffic will increase nearly threefold over the next 5 years, and will have increased nearly a hundredfold from 2005 to 2020. Overall, IP traffic will grow at a compound annual growth rate (CAGR) of 22 percent from 2015 to 2020. Many businesses face the challenge of monitoring, analyzing, and monetizing large scale web traffic, so we will explore this use case. Use case example In particular, we dive into the example of a content delivery or distribution network (CDN). A CDN is a globally distributed network of web servers deployed in multiple data centers across different geographic regions and is relied upon by content providers such as media companies and e-commerce vendors to deliver content to end users. CDNs have a business need to monitor their system in real-time. In addition to logging customer usage for the purpose of billing, they want to be alerted to sudden increases and decreases in their workloads for load balancing as well as for detecting network events like “denial of service attacks”. The sheer volume of web traffic mandates a massive parallel processing (MPP) system that can scale out to support the load. The concurrent need for real-time analytics points to the direction of hybrid transaction/analytical processing, or HTAP. HTAP systems enable high speed ingest and sophisticated analytics simultaneously without data movement or ETL. Background on the Upsert Benchmark This benchmark demonstrates the raw horsepower of a database system capturing high volume updates. Update, or upsert, is the operative word here. With a conventional insert a new row is created for each new database entry. With an upsert, individual rows can be updated in place. This upsert capability allows for a more efficient database table and faster aggregations, and it is particularly useful in areas such as internet billing. For more detail on this workload in use, take a look at this blog post, Turn Up the Volume With High-Speed Counters . SingleStore delivers efficient upsert performance, achieving up to 8 million upserts per second on a 10 node cluster, using the following parameterized query: Upsert query for SingleStore insert into records (customer_code, subcustomer_id, geographic_region, billing_flag, bytes, hits) values\non duplicate key update bytes=bytes+VALUES(bytes),hits=hits+VALUES(hits); Comparing Upsert performance Legacy databases and data warehousing solutions are optimized for batch loading of data and subsequently are unable to handle fast data insertions along with ad-hoc analysis of freshly generated data. NoSQL databases like Cassandra can handle fast data insertions but have more challenges with upserts, which are critical for web traffic monitoring across end-customer behavior and tracking web requests. More importantly however, Cassandra does not provide native support for analytics and requires users to bring in additional components like SparkSQL in order to support meaningful querying of data. We created the following query for Cassandra: Upsert query for Cassandra update perfdb.records set hits = hits + 1 where timestamp_of_data=1470169743185 and customer_code=25208 and subcustomer_id='ESKWUEYXUKRB' and geographic_region=10 and billing_flag=1 and ip_address='116.215.6.236'; The upsert benchmark is based on a simulated workload that logs web traffic across ten different geographic regions. SingleStore DB 5.1 runs on a 10 node m4.10xlarge cluster on AWS, at $2.394 per Hour (effective pricing with 1-year reserved instances), and is able to execute up to 8 million upserts per second and simultaneously run live queries on the latest data to provide a real-time window on the changing shape of traffic. Cassandra running on an identical cluster achieves 1.5 million upserts per second. We tested the most recent 3.0.8 version of Apache Cassandra. In the Cassandra query, update means upsert. As noted in the following chart, SingleStore scales linearly as we increase the number of machines with a batch size of 500. Cassandra however, does not appear to support large batch sizes well. According to Cassandra, # Caution should be taken on increasing the size of this threshold as it can lead to node instability.\n# Fail any batch exceeding this value. 50kb (10x warn threshold) by default. So we set batch_size_fail_threshold_in_kb: 5000 to support a 10,000 row batch size, but we encountered numerous errors that prevented the benchmark from running on Cassandra with these settings. Additional differences between datastores It is important to note that SingleStore has consistent and durable transactions whereas Cassandra has eventually consistent, non-transactional counters with less stringent semantics. So every insert that SingleStore processes is transactionally consistent and immediately available. Cassandra’s counters instead offer eventual consistency without transactional semantics. In short SingleStore retains ACID compliance and Cassandra does not, and retaining ACID guarantees are an important part of building an application for critical environments like billing. Upserts are only half the story, analytics are the rest Of course, the incoming data is only part of the picture. A critical complement is the ability to run sophisticated queries on the data as soon as it is recorded. Unlike SingleStore, Cassandra does not support analytical queries natively and users are forced to either install SparkSQL on the same cluster or on another cluster. In addition to the extra complexity, users face additional latency due to SparkSQL having to read data into Spark dataframes before running the analytical queries. Sophisticated queries run natively in SingleStore Since SingleStore is a fully relational database with a native SQL engine, users can run sophisticated queries immediately after storing data. Here are a few sample queries that could be used in an internet billing application. The following query lists the top five customers for each of the ten geographic regions based on the number of bytes delivered. Top 5 customers in each geographic region select *\nfrom (\n    select *, row_number() over (partition by geographic_region order by bytes desc rows between unbounded preceding and current row) as rank\n    from records\n    where timestamp_of_data > now() - interval 1 minute\n    ) as REGIONS_WINDOWED\nwhere rank <= 5; CDNs need to monitor surges in traffic in order to load balance or provision additional capacity. The following query lists top ten customers in terms of increase in traffic. Top 10 increases in the last one second span with\nRECORDS_AGGREGATED as (\n    select timestamp_of_data, subcustomer_id, sum(bytes) as bytes, UNIX_TIMESTAMP(timestamp_of_data) as epoch_seconds\n    from records\n    where timestamp_of_data between (now() - interval 2 second) and (now() - interval 1 second)\n    group by timestamp_of_data, subcustomer_id\n    ),\nRECORDS_WINDOWED as (\n    select *\n        ,lead(bytes, 1) over (partition by subcustomer_id order by timestamp_of_data desc rows between unbounded preceding and current row) as bytes_prev\n        ,lead(epoch_seconds, 1) over (partition by subcustomer_id order by timestamp_of_data desc rows between unbounded preceding and current row) as epoch_seconds_prev\n    from RECORDS_AGGREGATED\n    ),\nTRAFFIC_SURGES as (\n    select timestamp_of_data, subcustomer_id, epoch_seconds, bytes,\n       bytes - if(epoch_seconds_prev = epoch_seconds - 1, bytes_prev, 0) as bytes_change\n    from RECORDS_WINDOWED\n    where epoch_seconds = unix_timestamp(now()) - 1\n    )\nselect timestamp_of_data, subcustomer_id, epoch_seconds, bytes, bytes_change\nfrom TRAFFIC_SURGES\norder by bytes_change desc\nlimit 10; Usually, steep increases in web traffic can be attributed to increased customer interest due to special events or promotions. However, in some cases, the steep increases are caused by Denial of Service attacks. The following query generates a DoS attack alert if the steep increase is caused by large number of requests from a relatively small number of IP addresses. The steep increase is defined as an increase by more than two standard deviations from the mean. with\nRECORDS_AGGREGATED as (\n    select timestamp_of_data, subcustomer_id, sum(bytes) as bytes,\n           UNIX_TIMESTAMP(timestamp_of_data) as epoch_seconds,\n           approx_count_distinct(ip_address) as unique_ip_addrs\n    from records\n    where timestamp_of_data between (now() - interval 2 second) and (now() - interval 1 second)\n    group by timestamp_of_data, subcustomer_id\n    ),\nRECORDS_WINDOWED as (\n    select *\n        ,lead(bytes, 1) over (partition by subcustomer_id order by timestamp_of_data desc rows between unbounded preceding and current row) as bytes_prev\n        ,lead(epoch_seconds, 1) over (partition by subcustomer_id order by timestamp_of_data desc rows between unbounded preceding and current row) as epoch_seconds_prev\n    from RECORDS_AGGREGATED\n    ),\nTRAFFIC_SURGES as (\n    select timestamp_of_data, subcustomer_id, epoch_seconds, bytes, unique_ip_addrs,\n       bytes - if(epoch_seconds_prev = epoch_seconds - 1, bytes_prev, 0) as bytes_change\n    from RECORDS_WINDOWED\n    where epoch_seconds = unix_timestamp(now()) - 1\n    )\nselect timestamp_of_data, subcustomer_id, epoch_seconds, bytes, bytes_change\nfrom TRAFFIC_SURGES\nwhere unique_ip_addrs < 100 and bytes_change > (select avg(bytes_change) + 2*stddev(bytes_change) from TRAFFIC_SURGES)\norder by bytes_change desc\nlimit 10; All together, SingleStore presents a compelling option both in terms of linear scale performance, but also the ability to run sophisticated queries with ease using SQL. Full benchmark details on GitHub.com For complete details of the benchmark including scripts, please visit https://github.com/memsql/upsert-benchmark . Feel free to download SingleStore if you’d like to try it yourself. Appendix Understanding the difference between update and insert in Cassandra from StackOverflow", "date": "2016-08-11"},
{"website": "Single-Store", "title": "tableau-10-includes-even-more-data-source-options", "author": ["Monica Multer"], "link": "https://www.singlestore.com/blog/tableau-10-includes-even-more-data-source-options/", "abstract": "This post originally appeared in Tableau News by Arthur Gyldenege. Many of us have data in many places, waiting to be understood. That’s why we’re thrilled to announce we’ve added even more data-source options in Tableau 10. We want to help you gain value from your data, no matter where it lives. Here’s what’s new. SingleStore Named Connector We love fast databases. It makes the experience of interacting with your database that much more enjoyable. SingleStore is a database platform for real-time analytics. Using the new SingleStore named connector, you can analyze current and historical data together while SingleStore ingests millions of transactions per second. SingleStore is also wire-protocol compliant with MySQL, so if you already have a MySQL driver installed, you can connect to SingleStore. For more information on the SingleStore connector, see our post on the SingleStore site . Presto Named Connector Presto is another fast data source that’s part of the Hadoop ecosystem. It’s a distributed query engine for interactive data analysis that scales from gigabytes to petabytes. Because it distributes queries, analysis can be pushed to where data lives. This means data living in Hive, Cassandra, relational databases, or a number of other sources can be connected to Presto for quick analysis. Presto was originally built and maintained by Facebook, and now Teradata is supporting the platform for enterprise customers. With the support of Teradata, we built a new named connector for Presto. It only connects to Teradata Presto 141t today, but we plan on supporting even more distributions in the future! Connector Updates Updates to SQL Server: Contained Databases Microsoft SQL Server and Azure SQL support a new feature called “contained databases.” When a database is “contained,” it means user information is stored in the database rather than in the DBMS, thus making the database more portable. In Tableau 10, you can connect to these databases by supplying the database name right from the connection dialog as seen below. Oracle Table Functions In Tableau 10, you can use table functions as a data source! Table functions are a great way to simplify data access to otherwise complex data structures. When you connect to Oracle, any table function you have access to will appear in the “stored procedure” section. Simply drag and drop the function to the canvas. If the table function contains any parameters, Tableau will prompt you for them. In addition to stored-procedures support, Tableau 10 also supports Kerberos for Oracle. Enterprise environments utilizing Active Directory with Tableau can now have a seamless single sign-on experience with Oracle and Tableau. Say Hello to JDBC with SAP HANA for Mac To simply say “we now support HANA for Mac” would be a significant understatement. Over the last two releases, the connectivity team has been figuring out the infrastructure for Tableau to support JDBC drivers in addition to ODBC & OLEDB. SAP HANA, not available via ODBC for Mac, is our first named connector using JDBC. In the future, we’ll enable generic JDBC connectivity and JDBC on Tableau Server, but the core work is available for you to try in Tableau 10. You can connect via JDBC, create your workbook, and seamlessly publish it to a Windows Tableau Server to have the connection handled transparently to the user by ODBC. Additional Investments in SAP HANA for Mac is big, but SAP has gotten more love in Tableau 10. Tableau now supports SAP HANA parameters that are dynamically populated by SQL statements. It’s common to use relative dates in parameters to resolve the last month, quarter, etc. Tableau 10 supports these types of parameters. We have also updated SAP HANA to support Initial SQL with parameters to enable advanced scenarios like row-level security, auditing, etc. The Biggest Little Feature in Tableau 10: Remember Connection Details Are you absolutely tired of filling in server details? Often requested, we have now delivered a little memory to save yours. Once you have successfully connected to a database, Tableau will remember every connection detail except the password. This memory is per user, so shared machines will not share connection settings. It’s already saving everyone time across the organization, and soon, it will help save customers’ time as well. We hope these new data-source options make it even easier to see and understand all of your data. If you have ideas for more connectors or database updates, let us know in the Ideas Forum! Learn More about Tableau 10 Tableau 10 includes a brand new look and feel, and a host of new features to help you prep, analyze, and share your insights even faster. Check out our Coming Soon page for details.", "date": "2016-08-12"},
{"website": "Single-Store", "title": "bpf-linux-performance", "author": ["Kyle Laracey"], "link": "https://www.singlestore.com/blog/bpf-linux-performance/", "abstract": "Performance analysis often gets bottlenecked by lack of visibility. At SingleStore, we architected our database to easily observe its inner workings. Observability allows our engineers to easily identify components that need to be faster. Faster components mean our database’s performance skyrockets. These tools also enable support engineers to react quickly and precisely to customer needs. In the spirit of using the best available tools to which we have access, the performance team is currently evaluating next-generation tools just recently available in Linux. The newest tool for observing the Linux operating system is the “Berkeley Packet Filter” (BPF). BPF allows users to run a small piece of code quickly and safely inside the operating system. Originally used for packet filtering, it has since been enhanced from its eponymous use-case to support dynamic tracing of the Linux operating system. For example, it is possible to write a small BPF program that prints every time a particular file was accessed by a user. The power of the Berkeley Packet Filter, when used with Userland Statically Defined Tracepoints (USDT), expands beyond the operating system to the database. USDT probes are well defined locations in the database where BPF programs run, allowing engineers to ask questions previously unanswerable. For example, engineers can now examine the interactions between the database and the operating system by running BPF programs in each at the same time. Adding a USDT static tracepoint is as easy as a single macro call, which declares the probe and its arguments. This probe fires when each query is executed and records the query string: DTRACE_PROBE1(memsqld, querystart, query); To use this USDT probe, we need to attach a BPF program to it. We write our program in C and use the BPF Compiler Collection to compile it to BPF and attach it to our probes. The following BPF script traces queries and records their latencies: BPF_HASH(pid_to_start_hash, u32);\nBPF_HISTOGRAM(latency);                                                            \n\n// This function runs each time a query begins. It records the current time stamp \n// (`start_ts`) and save it in the `pid_to_start_ht` hash table                                                                                \nint querystart(struct pt_regs *ctx)                     \n{                                                                                   \n    u64 start_ts = bpf_ktime_get_ns();                                              \n    u32 pid = bpf_get_current_pid_tgid();\n                                           \n    pid_to_start_hash.update(&pid, &start_ts);                                          \n    return 0;                                                                       \n} \n\n// This function runs at the end of each query. Look up the saved start timestamp \n// (`start_ts`) for the current thread’s id (pid) using the hash table \n// (`pid_to_start_hash`) and record the elapsed time (`delta_ms`) in the latency \n// histogram.\nint queryend(struct pt_regs *ctx)                       \n{                                                                                                                                   \n    u32 pid = bpf_get_current_pid_tgid();                                                                                                      \n    u64 *start_tsp = pid_to_start_hash.lookup(&pid);\n\n    // Edge case: this query began before we started tracing.                                          \n    if (!start_tsp) return 0; \n                                                                                                                                      \n    u64 delta_ms = (bpf_ktime_get_ns() - *start_tsp) / 1000 / 1000;\n    // Take the log of the elapsed time to put into the logarithmic histogram.                 \n    latency.increment(bpf_log2l(delta_ms));                                                                \n\n    // Make sure to delete values from the hash table when they are no longer needed.                                                                                    \n    pid_to_start_hash.delete(&pid);                                                             \n    return 0;                                                                       \n} We run query_latency.py , a script that wraps the above BPF program using the BCC toolchain, and get a nice histogram of query latencies: $ sudo ./query_latency.py /var/lib/memsql/master-3306/memsqld --histogram\nTracing queries. ^C to exit.\nlatency (ms):\n     value               : count     distribution\n         0 -> 1          : 9        |****************************************|\n         2 -> 3          : 1        |****                                    |\n         4 -> 7          : 1        |****                                    |\n         8 -> 15         : 2        |*******                                 |\n        16 -> 31         : 1        |****                                    |\n        32 -> 63         : 0        |                                        |\n        64 -> 127        : 0        |                                        |\n       128 -> 255        : 1        |****                                    | Once engineers have the ability to trace when a thread is executing a SingleStore query, they can ask more interesting questions about how the database interacts with Linux. For example, engineers can investigate and determine how long queries are spending acquiring locks. With BPF, engineers are able to instrument the start and end of the queries as above, and additionally instrument the futex system call itself (used in Linux to acquire and release locks) to trace how long it takes acquire locks while executing our query: futex latencies (ms) for 'select count(distinct sid_1g) where...'\n     value               : count     distribution\n         0 -> 1          : 0        |                                        |\n         2 -> 3          : 2        |****                                    |\n         4 -> 7          : 2        |****                                    |\n         8 -> 15         : 1        |**                                      |\n        16 -> 31         : 5        |***********                             |\n        32 -> 63         : 17       |****************************************| What about how a query spends its time? On- and off-CPU flamegraphs are helpful , but they are too coarse for query investigations. We instrumented the kernel scheduler tracepoints to conditionally collect information for threads that queries run on. This tracing tells us how long the thread of a query spends in various states (waiting, running, blocked, I/O, and sleeping). The power of BPF allows us to inspect our database at runtime and ask precise questions. Increased observability provided by BPF improves the rate of performance and optimizes customer interaction with the SingleStore database. Overall, BPF provides the observability necessary to build a transparent and easily accessible modern in-memory database . Access scripts, documentation, and additional reference information on BCC and BPF here . After joining the SingleStore performance team this summer, Kyle Laracey will be returning to Brown University in the fall. At Brown, he studies computer science, is a teaching assistant for CS167: Operating Systems, and is expected to graduate in May 2017.", "date": "2016-08-16"},
{"website": "Single-Store", "title": "operational-data-warehouses", "author": ["Monica Multer"], "link": "https://www.singlestore.com/blog/operational-data-warehouses/", "abstract": "Last month, we announced that SingleStore received the highest score in the Gartner Critical Capabilities Report for the “Operational Data Warehouse Use Case”. While the findings Gartner shared are gratifying, this deserves a bit of a deeper dive. For starters, let’s examine how Gartner defines an Operational Data Warehouse: “This use case manages structured data that is loaded continuously in support of embedded analytics in applications, real-time data warehousing , and operational data stores. This use case primarily supports reporting and automated queries to support operational needs , and will require high-availability and disaster recovery capabilities to meet operational needs. Managing different types of users or workloads, such as ad hoc querying and mining, will be of less importance as the major driver is to meet operational excellence.” In light of these observations, we expect the adoption rates for Operational Data Warehouses to climb rapidly. Several megatrends point directly to a growing need for Operational Data Warehouses that take full advantage of real-time transaction processing and big data analytics in an in-memory optimized architecture. Take the explosive global sensor market that BCC Research predicts will reach $154.4 billion by 2020. Image, flow, level, biosensor and chemical sensors will generate a data deluge of information across a wide range of parameters. Organizations that can efficiently analyze these data flows in real time to adapt to constantly changing market conditions will be at a distinct advantage over their competitors who do not possess this capability. To fully appreciate the extent of the data deluge we are talking about, consider forecasts about the volume of data captured by the Internet of Things (IoT). According to ABI Research, 1.6 zettabytes of IoT data will be collected by 2020. This staggering volume of data will tax latency issues already affecting enterprises in terms of data loading and query execution. Batched loading takes hours to implement in the absence of real-time ingestion capabilities causing slow query responses, reporting and applications. So what is the net effect? These challenges threaten the ability to detect and respond to business changes as soon as they occur, compromising the delivery of real-time analytics to applications with growing user bases. Throw mobile computing, risk management, personalization, portfolio tracking, clean energy and geospatial trends into the mix, and it’s easy to see why the demand for real-time analytics is ramping up quickly. The challenge of course is how to efficiently ingest copious and continuous data loads in order to support operational business intelligence queries to rapidly iterate on the greatest number of analytic models possible. What is clear is that analytic flexibility is essential to business adaptation. Interestingly, when it comes to Operational Data Warehouses, Gartner weights: 1) Operational BI queries 2) Repetitive queries 3) Continuous data loading and 4) System availability as the top ranked critical capabilities High marks for SingleStore in these strategic areas account for an overall score of 3.77 (out of a possible 5), ahead of IBM, Teradata, MongoDB and Oracle. “Gartner does not endorse any vendor, product or service depicted in its research publications, and does not advise technology users to select only those vendors with the highest ratings or other designation. Gartner research publications consist of the opinions of Gartner’s research organization and should not be construed as statements of fact. Gartner disclaims all warranties, expressed or implied, with respect to this research, including any warranties of merchantability or fitness for a particular purpose.”", "date": "2016-08-18"},
{"website": "Single-Store", "title": "the-changing-face-of-the-modern-cio", "author": ["Gene Miguel"], "link": "https://www.singlestore.com/blog/the-changing-face-of-the-modern-cio/", "abstract": "In 1981 the role of Chief Information Officer (CIO) first breaks onto the scene. Today, thirty five years since genesis, the responsibilities of the CIO have radically changed. The original CIO served as senior executive in an enterprise and was responsible for the information technology and computer systems that supported enterprise goals. However, as today’s business needs rapidly change, so too does the role of a modern CIO. Today, CIOs must adapt or they will get left behind with legacy systems. The modern CIO is expected to take on multiple responsibilities, including: Management of platforms and systems such as data governance, mobility, cloud infrastructure Investing in security, improved database speed and access, big data analytics, integration Identifying trends, threats, and partners that align with business goals Making sure a company’s data is clean, accessible, easy to understand, and secure Hiring and developing talent CIOs now face many challenges, since IT plays an even more important role in core business strategy than it has in previous years. Management of systems with methods like data analysis and cloud infrastructure allow CIOs to operate with an agile development mentality and be more fluid when it comes to identifying and implementing new business operations as a result. While much of the responsibility of a CIO has shifted away from managing server farms in a closet to managing a cloud, hardware is just as important today with the emergence of the Internet of Things (IoT). CIOs can now utilize IoT to gather valuable data across an entire logistics operation. For example, sensors can be placed on shipping containers and vehicles to analyze trip data, which can lead to designing more efficient shipping routes, resulting in higher cost savings. Instead of being a back-office executive, CIOs must use their influence over new technologies to identify cost-saving opportunities or create additional revenue streams. CIOs, with their knowledge of modern technological trends, become responsible for maintaining their company’s competitive edge. The responsibilities of modern CIOs working in the forefront of technology and information systems are changing rapidly, and those who do not adapt will quickly fall behind. To learn more about the changing landscape of IT and to network with over 100 IT executives, join us at the HMG 2016 CIO Executive Leadership Summit in Denver, CO on September 1, 2016. The speakers for this event include CIOs pushing the boundaries of what’s possible: Rob Dravenstott from DISH Network, Stephen Gold from CVS Health, and Renee Arrington from Pearson Partners International, Inc. At the Summit, the SingleStore team is available to talk about analyzing real-time data to optimize business processes and create new revenue streams. See you there!", "date": "2016-08-25"},
{"website": "Single-Store", "title": "geospatial-benchmark", "author": ["Shekhar Bapat"], "link": "https://www.singlestore.com/blog/geospatial-benchmark/", "abstract": "The mobile revolution, epitomized by the rise of GPS-enabled smartphones, is transforming our lives – the way we travel, connect with like-minded people, track our goods and shipments, manage traffic congestion, get threat alerts, and hunt for Pokemon. Many estimates place the number of smartphone subscribers at 2 billion, a number expected to grow to 6.1 billion by 2020. Each of these devices serves as a GPS-data emitting mobile sensor, providing geospatial data that can be used to track the movement of people, vehicles, and merchandise. Though this data presents rich opportunities for businesses to extract valuable insights and operational efficiencies, the sheer volume requires a modern scale-out analytic solution and technology that can process it without delay for timely actionable intelligence. SingleStore is designed from the ground up as a massively parallel scale-out solution for real-time analytics with built-in support for geospatial queries. While geospatial capabilities are also available through other solutions, only SingleStore offers them in the context of an ACID-compliant in-memory scale-out operational data warehousing solution. While SingleStore offers strong transactional semantics and ElasticSearch as a datastore offers no support for transactions at all, they both offer powerful geospatial analytics and it seems appropriate to benchmark SingleStore against ElasticSearch geolocation capabilities. In this analysis, we compare SingleStore geospatial query performance with ElasticSearch geolocation for tracking vehicles and creating alerts when the vehicles enter certain geofences. The alerts can be used to improve logistics and security, reduce congestion, and monitor vehicle fleets. We simulate an urban scenario with 10M-100M vehicles generating geospatial data points and 2000 geofences. The system is required to ingest updated geospatial data points from vehicles and identify vehicles that show up within specified geofences. SingleStore queries for creating schema for geofences and vehicles CREATE REFERENCE TABLE IF NOT EXISTS locations (id integer primary key,\n                                                name varchar(128),\n                                                polygon geography DEFAULT NULL);\n\nCREATE TABLE IF NOT EXISTS records (id integer primary key,\n                                    location geographypoint,\n                                    key(location)); SingleStore query for updating vehicles geolocation INSERT INTO records (id, location)\nVALUES      (id, location pairs)\nON          DUPLICATE KEY UPDATE location = VALUES\n            ( location); SingleStore query for checking intersection of vehicles with geofences SELECT r.id,\n       r.location,\n       l.id\nFROM   records r,\n       locations l\nWHERE  l.id = shape_id AND geography_intersects(r.location, l.polygon); ElasticSearch geolocation mapping (schema) for geofences and vehicles {\n    \"locations\": {\n        \"properties\": {\n            \"name\" : {\n                \"type\":\"string\"\n            },\n            \"polygon\": {\n                \"type\": \"geo_shape\",\n                \"tree\": \"quadtree\",\n                \"precision\": \"1m\"\n            }\n        }\n    }\n}\n\n{\n    \"driver\": {\n        \"dynamic\": \"strict\",\n        \"properties\": {\n            \"location\": {\n            \"type\": \"geo_shape\",\n            \"tree\": \"quadtree\",\n            \"points_only\":\"true\",\n            \"precision\": \"1m\"\n             }\n         }\n     }\n} ElasticSearch query for inserting vehicle geolocation {\"location\":{\"type\": \"point\",\"coordinates\":[latitude,longitude]}} ElasticSearch query for checking intersection of vehicles with geofences {\n    \"query\":\n    {\n        \"bool\":{\n            \"must\":{\n                \"match_all\":{}\n            },\n            \"filter\":{\n                \"geo_shape\":{\n                    \"location\":{\n                        \"indexed_shape\":{\n                            \"id\":shape_id,\n                            \"type\":\"locations\",\n                            \"index\":\"locations\",\n                            \"path\": \"polygon\"\n                        }\n                    }\n                }\n            }\n        }\n    }\n} We ran SingleStore DB 5.1 and ElasticSearch 2.3.5 on 1, 5, and 10 node clusters of m4.10xl instances on AWS. Our benchmarking results show that on 10 nodes, SingleStore is 24x faster than ElasticSearch for queries that update vehicle geolocations in terms of rows/second. We found SingleStore to be more than 2X faster than ElasticSearch for queries that find intersection of geolocations with geofences in terms of queries/second. SingleStore presents a compelling option because it offers superior geolocation query performance in addition to strong transactional support and the full feature set of a leading operational data warehouse. SingleStore is the preferred option for enterprises that need to monitor geolocations for people, vehicles, and goods in real time. For complete details of the benchmark including scripts, please visit https://github.com/memsql/geo-benchmark Feel free to download SingleStore if you’d like to try it yourself: http://www.singlestore.com/free", "date": "2016-08-29"},
{"website": "Single-Store", "title": "why-role-based-access-control-is-essential-to-database-security", "author": ["Emily Friedman"], "link": "https://www.singlestore.com/blog/why-role-based-access-control-is-essential-to-database-security/", "abstract": "As repositories of highly sensitive, confidential, and valuable business data, databases are the crown jewels of every organization. Successful businesses not only supply accurate and timely data, they must protect it as well. Security provides a critical competitive edge for any high functioning database. So database providers must prioritize protecting data in order to gain loyal customers who can trust the systems set in place to properly guard valuable information. In our latest enterprise release, SingleStore DB 5.1, we added Role-Based Access Control (RBAC) as a powerful tool to protect customer data. With this new security feature, SingleStore customers can now easily scale users and roles to tens of thousands without compromising performance. RBAC provides high scalability and enhanced control over user access to data that perfectly suits intensive workloads like those generated by the Internet of Things. SingleStore DB 5.1 brings enterprise level security with optimized scale of performance to real-time analytics to prove that customers should never have to sacrifice security for speed. Findings in the 2016 Verizon Data Breach Investigations Report underscore the case for RBAC as a robust shield against unauthorized access to secure data. Of the ten incident classification patterns cited in the report, privilege misuse ranks among the most common sources of data breaches along with web app attacks, denial-of-service, crime-ware, and cyber-espionage. “Incident Classification Patterns” We sat down with Rick Negrin, one of our principal product managers, to discuss the significance of RBAC, why it matters to real-time data analytics, and how it is implemented in SingleStore DB 5.1. Can you tell us a little bit about RBAC? RN: Role-Based Access Control (RBAC) provides a methodology for regulating an individual user’s access to information systems or network resources based on their role in an organization. RBAC enables security teams to efficiently create, change, or discontinue roles as the unique needs of an organization evolve over time without having to endure the hardship of updating the privileges of individual users. What are the challenges of applying effective RBAC? RN: Once you get a critical mass of users that need to be assigned varying levels of access rights to privileged data, you need a higher level of abstraction to quickly and easily assign groups, roles, and customized rules and permissions to individual users. In addition, it is essential to “spread the wealth” of managing databases among multiple authorized administrators – without giving them access to classified data – to prevent individuals from manipulating access rights to systems on their own. In a way, it is not dissimilar from the chain of command on strategic submarines where the Commanding Officer, Executive Officer, and Chief of Boat share launch code responsibilities so no one individual can unilaterally override approved procedures. How did SingleStore integrate RBAC into its latest release? RN: RBAC was incorporated into our release of SingleStore DB 5.1 this past June. To enable this feature, two new objects were added to the security model: roles and groups. Roles are collections of permissions and groups are collections of users. Roles are then applied to groups and users are put into groups. This not only greatly simplifies the process of assigning roles-based access rights, it ensures appropriate information privileges are enforced for individual users throughout the organization. Why is RBAC particularly important for public sector, financial, and energy companies dealing with private information or applying real-time analytics? RN: The legal and compliance requirements in these sectors mandate strict security measures when it comes to protecting data. RBAC plays a key role in helping these organizations address potential data breaches through the enforcement of granular information access rights. When it comes to real-time data analytics there is a dearth of sophisticated security mechanisms embedded in big data analytics systems. As a result, while the aforementioned industries are experimenting with some of these technologies, legal barriers and the relative absence of built-in security measures like RBAC create friction when it comes to more widespread adoption. We have embedded RBAC into the enterprise edition of SingleStore DB 5.1 in order to maintain flexible integration through connectors with technologies including Kafka, Spark, HDFS, S3, MySQL, and business intelligence software. Performance degradation is commonly associated with strong security measures. How is SingleStore implementing RBAC without exacting a performance penalty? RN: For most databases, security computation is done on each execution of a query. At SingleStore, instead of repeating this step for each query, we perform the computation during the execution of an RBAC command. For example, a user John is added to the Analytics Users group. This group has access to tables X and Y. When the RBAC command runs to add John to the Analytics Users group, the command also goes through each query in the system that touches table X or table Y and updates the security information on that query so John now has access. Also, all new queries that use table X or table Y that come into the system will see that John has access and save that information away for future executions of that same query. Effectively, SingleStore precomputes the allowed access for all queries ahead of time, saving time while improving security. To validate the performance quality of RBAC, we engaged an independent third-party security consultancy to run rigorous functional and performance tests, including inside a FIPS 140-2 environment. The results demonstrated that a cluster can have up to 16,000 users occupying 100,000 roles in 100,000 groups without significant degradation in query performance. What are some good examples that illustrate how to apply RBAC? RN: Take a financial company for instance. You might need an auditor to go in and see who accessed certain things but not necessarily the data itself. RBAC could restrict that auditor’s access to metadata. Or you might have a security officer who manages user names and passwords, but does not manage the data. You could also assign a storage administrator to manage backups without gaining access to the underlying data. Another good example is an application user who could be restricted to executing per-application DML statements. That is the beauty of RBAC; you can extend the breadth and depth of roles and groups and users as needed. Try SingleStore DB 5.1 for yourself: www.singlestore.com/free", "date": "2016-08-31"},
{"website": "Single-Store", "title": "real-time-analytics-with-kafka-and-memsql", "author": ["Kevin White"], "link": "https://www.singlestore.com/blog/real-time-analytics-with-kafka-and-memsql/", "abstract": "Connected devices, IoT, and on-demand user expectations push enterprises to deliver instant answers at scale. Applications that anticipate customer needs and fulfill expectations for fast, personalized services win the attention of consumers. Perceptive companies have taken note of these trends and are turning to memory-optimized technologies like Apache Kafka and SingleStore to power real-time analytics. High Speed Ingest Building real-time systems begins with capturing data at its source and using a high-throughput messaging system like Kafka. Taking advantage of a distributed architecture, Kafka is built to scale producers and consumers by simply adding servers to a given cluster. This effective use of memory, combined with commit log on disk, provides ideal performance for real-time pipelines and durability in the event of server failure. From there, data can be transformed and persisted to a database like SingleStore. Fast, Performant Data Storage SingleStore persists data from real-time streams coming from Kafka. By combining transactions and analytics in a memory-optimized system, data is rapidly ingested from Kafka, then persisted to SingleStore. Users can then build applications on top of SingleStore also supplies the application with the most recent data available. We teamed up with the folks at Confluent , the creators of Apache Kafka, to share best practices for architecting real-time systems at our latest meetup. The video recording and slides from that session are now available below. Meetup Video Recording: Real-Time Analytics with Confluent and SingleStore Watch now to: See a live demo of our new showcase application for modeling predictive analytics for global supply chain management Learn how to architect systems for IoT streaming data ingestion and real-time analytics Learn how to combine Kafka, Spark, and SingleStore for monitoring and optimizing global supply chain processes with real-time analytics Video Slides If you would like to catch upcoming tech talks and live product demonstrations, join the SingleStore meetup group here .", "date": "2016-09-02"},
{"website": "Single-Store", "title": "predictive-analytics", "author": ["Kevin White"], "link": "https://www.singlestore.com/blog/predictive-analytics/", "abstract": "Organizations once waited hours, days, or even weeks to get a handle on their data. In an earlier era, that sufficed. But with today’s endless stream of zeros and ones, data must be usable right away. It’s the crux of decision making for enterprises competing in the modern era. Recognizing cross-industry interest in massive data ingest and analytics, we teamed up with O’Reilly Media on a new book: The Path to Predictive Analytics and Machine Learning . In this book, we share the latest step in the real-time analytics journey: predictive analytics, and a playbook for building applications that take advantage of machine learning. Free Download Here: The Path to Predictive Analytics and Machine Learning What’s Inside? Chapter 1: Building Real-Time Data Pipelines We begin with a review our previous O’Reilly book: Building Real-Time Data Pipelines – Unifying Applications and Analytics with In-Memory Architectures . It covers the emergence of in-memory architectures and provides a framework for building real-time pipelines that serve as the foundation for machine learning applications. Chapter 2: Processing Transactions and Analytics in a Single Database This chapter details the shift from Online Transaction Processing (OLTP) and Online Analytical Processing (OLAP) to converged, multi-model systems designed for Hybrid Transaction/Analytical Processing (HTAP) . Chapter 3: Dawn of the Real-Time Dashboard Data visualization is arguably the most powerful method for enabling humans to understand and spot patterns in a dataset. Chapter three explores the role of Business Intelligence (BI) tools, and how they provide a visualization layer for data analysts to detect historical trends and identify future predictions. Chapter 4: Redeploying Batch Models in Real Time Applying existing batch processes based on statistical models to real-time data pipelines opens a multitude of easily accessible opportunities for machine learning and predictive analytics. In this section, we look at ways to apply machine learning to real-time problems by repurposing familiar machine learning models. Chapter 5: Applied Introduction to Machine Learning Choosing the proper machine learning technique requires evaluating a series of tradeoffs like training and scoring latency, bias and variance, and in some cases accuracy versus complexity. This chapter provides a broad introduction to applied machine learning with emphasis on resolving these tradeoffs with business objectives in mind. Chapter 6: Real-Time Machine Learning Applications Chapter six details how real-time data processing systems and the ready availability of machine learning libraries make it possible to apply machine learning and execute finely tuned decisions in the moment. Chapter 7: Preparing Data Pipelines for Predictive Analytics and Machine Learning Although certain techniques are better suited to real-time analytics and tight training or scoring latency requirements, the challenges preventing adoption relate largely to infrastructure rather than machine learning theory. This part of the book provides best practices for building database systems that are best suited for predictive analytics and machine learning. Chapter 8: Predictive Analytics in Use Expanding on the subject of taking machine learning from batch to real-time, this chapter explores Internet of Things (IoT) and renewable energy use cases. It also provides machine learning code samples and connects the dots from real-time data pipelines to BI visualizations. Chapter 9: Techniques for Predictive Analytics in Production This section makes predictive analytics more accessible by combining well-understood machine learning techniques with technology advances in software and hardware. It also includes numerous code samples to help you get started. Chapter 10: From Machine Learning to Artificial Intelligence The move from machine learning to broader artificial intelligence will happen. The final chapter paves a logical path forward for making the leap.", "date": "2016-09-07"},
{"website": "Single-Store", "title": "memex-predictive-analytics-for-global-supply-chain-management", "author": ["Monica Multer"], "link": "https://www.singlestore.com/blog/memex-predictive-analytics-for-global-supply-chain-management/", "abstract": "The Internet of Things (IoT) produces staggering amounts of data daily. Real-time analysis of this data helps businesses address the demands of consumers in today’s always-on economy. Supply chain management exemplifies IoT impact on manufacturing industries. With a multitude of moving parts like vehicles, shipping containers, and packages functioning as sources of data, companies need more advanced methods for ingesting and analyzing IoT data. Most companies use descriptive analytics, but the above statistic from Gartner reinforces that descriptive is no longer sufficient. Data analytics is evolving to predictive, and will eventually move even further beyond that to prescriptive analytics. Supply chain companies must modify their strategies to include these advanced analytics. At Gartner Catalyst earlier this year, we released MemEx , a showcase application modeling predictive analytics for global supply chain management. MemEx highlights how enterprises like FedEx, Amazon, and DHL can implement advanced analytics to improve logistical operations. About MemEx MemEx uses advanced analytics to predict throughput of supply chain warehouses. The application processes and analyzes 2 million simulated data points based on 2,000 sensors across 1,000 warehouses globally. These sensors appear on production lines and conveyor belts in and around each warehouse, measuring operational health of the machines with readings such as temperature or electrical current output. By harnessing this critical sensor data in real time, supply chain organizations can predict lapses in throughput and plan accordingly. MemEx Architecture MemEx, powered by the SingleStore database platform, includes: Streamliner , our integrated Apache Spark solution Ops , the web interface for cluster deployment, management, and monitoring A set of simulated data producers written in Python Apache Kafka Javascript-based User Interface (UI). The architecture of MemEx is shown below: Data producers simulate sensor activity, pushing approximately 2 million data points every second from sensors on warehouse machinery. Sensor data is sent to an Apache Kafka message queue, which is processed by a SingleStore Streamliner data pipeline. The pipeline predicts throughput of each warehouse using a pre-trained machine learning model. The sensor data and warehouse states are stored in SingleStore and further analyzed to determine the overall health of the supply chain. Finally, the MemEx UI queries the SingleStore database to display states in the web interface. The gears indicating warehouse functionality shift colors from red to yellow to green based on predicted throughput. Additionally, geolocation of each warehouse is stored in a SingleStore table, and queried dynamically based on user changes to the map area in view. These queries, and subsequent visual display, depend on the map geography and zoom level selected by the user. By predicting risk of production failures, MemEx enables enterprises to react to their businesses in real time, make supply adjustments, and ultimately drive revenue. The Real-Time Future of Supply Chain Management Many companies on the forefront of IoT already or will soon incorporate machine learning into their businesses: Amazon proposes automated drone deliveries , Fedex provides real-time package tracking, and Tesla released connected vehicles capable of autonomously driving . The future of supply chain and logistics companies depends on predictive analytics. Embracing advanced analytics brings businesses one step closer to running completely autonomous supply chain functions. MemEx provides a realistic view into the capabilities of using real-time applications with predictive analytics to draw businesses into the future of connected things. Watch a live demonstration of MemEx", "date": "2016-09-16"},
{"website": "Single-Store", "title": "memsql-oracle-better-together", "author": ["Lesia Myroshnichenko"], "link": "https://www.singlestore.com/blog/memsql-oracle-better-together/", "abstract": "Oracle OpenWorld 2016 kicks off on September 18th in San Francisco with ten tracks, including a Data Center track highlighting innovation in databases including SingleStore and Oracle. We built SingleStore to be a flexible ecosystem technology, as exemplified by several features. First, we offer users flexible deployments – whether it’s hybrid cloud, on-premises, VMs, or containers. Second, our connector tools are open source, such as SingleStore Spark Connector and Streamliner , which lets you build real-time pipelines and import from popular datastores like HDFS, S3, and MySQL. And SingleStore is a memory-first engine, designed for concurrent data ingest and analytics. These ingredients make SingleStore a perfect real-time addition to any stack. Several of our customers combine SingleStore with traditional systems, in particular Oracle databases. SingleStore and Oracle can be deployed side by side to enhance scalability, distributed processing, and real-time analytics. Three Ways SingleStore Complements Oracle SingleStore as the Real-Time Analytics Engine Data can be copied from Oracle to SingleStore using a data capture tool, and analytical queries can be performed in real time. SingleStore for Stream Processing with Kafka and Spark SingleStore can serve as the stream processing layer in front of an Oracle database. Use Apache Kafka or Apache Spark to help build real-time data pipelines. SingleStore as the High Speed Ingest Layer Using SingleStore as the data ingest layer on top of an Oracle database allows ingest at in-memory speed. Visit the SingleStore Booth 1821 at Oracle OpenWorld Grab a complimentary copy of our O’Reilly Book: Building Real-Time Data Pipelines , take home a SingleStore t-shirt, and enter for a chance to win an Estes Proto-X Drone during the show expo hours: Monday, September 19, 10:15AM – 5:30PM Tuesday, September 20, 10:15AM – 5:15PM Wednesday, September 21, 10:15AM – 4:15PM Members of the SingleStore team will be available for questions and will also be demonstrating our newest showcase application, MemEx, modeling real-time supply chain analytics. Get the full event details here: singlestore.com/events", "date": "2016-09-19"},
{"website": "Single-Store", "title": "pipelines", "author": ["Steven Camina"], "link": "https://www.singlestore.com/blog/pipelines/", "abstract": "Today we launched SingleStore DB 5.5 featuring SingleStore Pipelines, a new way to achieve maximum performance for real-time data ingestion at scale. This implementation enables exactly-once semantics when streaming from message brokers such as Apache Kafka. An end-to-end real-time analytics data platform requires real-time analytical queries and real-time ingestion . However, it is rare to find a data platform that satisfies both of these requirements. With the launch of SingleStore Pipelines as a native feature of our database, we now deliver an end-to-end solution from real-time ingest to analytics. Real-Time Analytical Queries and Data Ingestion Let’s define real-time analytical queries and real-time data ingestion separately. A data platform that supports real-time analytical queries quickly returns results for sophisticated analytical queries, which are usually written in SQL with lots of complex JOINs. Execution of real-time analytical queries differentiates SingleStore from competitors. In the past year, Gartner Research recognized SingleStore as the number one operational data warehouse , as well as awarded Visionary placements to the company Operational Database and Data Warehouse Magic Quadrants . A data platform that supports real-time ingestion can instantly store streaming data from sources like web traffic, sensors on machines, or edge devices. SingleStore Pipelines ingests data at scale in three steps. First, performantly pulling from data sources – Extract. Second, mapping and enriching the data – Transform. Finally, loading the data into SingleStore – Load. This all occurs within one database, or pipeline. The transactional nature of Pipelines sets it apart from other solutions. Streaming data is atomically committed in SingleStore, and exactly-once semantics are ensured by storing metadata about each pipeline in the database. At the crux of SingleStore Pipelines is a new, unique database object to SingleStore – a PIPELINE , or a top-level database element similar to a TABLE , INDEX or VIEW . Let’s explore the different properties of SingleStore Pipelines. Pipelines allow extraction from data sources (e.g. Apache Kafka) using a robust, database-native mechanism. The system that pulls data from data sources IS the SingleStore Database. Other solutions that claim to pull streaming data typically leverage separate “middleware” solutions to extract data. That not only decreases performance, it also requires additional provisioning and management. SingleStore ensures true exactly-once semantics for Kafka messages. SingleStore stores and manages Kafka offsets within the SingleStore Database. The latest loaded Kafka offsets are stored in SingleStore; only when a Kafka message is reliably extracted, transformed, and loaded in SingleStore are the offsets incremented. In the event of any error, such as Kafka connectivity, improper transforms, or malformed data, SingleStore always ensures that each Kafka message is processed exactly once. Data enrichment and transformation in Pipelines can be implemented using any programming language. SingleStore Pipelines introduces the concept of a “transform” specified as part of the Pipeline DDL. Transforms are user-defined scripts that enrich and map external data for loading into SingleStore, written in any programming language for familiarity and flexibility. Data loading into SingleStore happens efficiently and in parallel between SingleStore data partitions and Kafka brokers. SingleStore is a distributed system, and SingleStore Pipelines was implemented with this attribute in mind. Pipelines loads data between distributed systems and streams data in parallel from individual Kafka brokers directly into SingleStore data partitions. Moreover, SingleStore performs distributed data loading optimizations such as lessening the total number of threads used, sharing data buffers, and minimizing intra-cluster connections. Learn more about SingleStore Pipelines in our technical documentation . Try SingleStore Pipelines Today Build your own pipeline today: Download SingleStore DB 5.5", "date": "2016-09-26"},
{"website": "Single-Store", "title": "a-guide-to-free-memsql-educational-resources", "author": ["Seth Luersen"], "link": "https://www.singlestore.com/blog/a-guide-to-free-memsql-educational-resources/", "abstract": "For interested database developers, database administrators, and data scientists, we proudly offer several learning resources. These include YouTube educational videos, Enterprise trial Editions of SingleStore, extensive public product documentation, knowledge base articles, a SlideShare presentation repository, and a public Slack channel. Online Education Videos We have numerous videos available at youtube.com/memsql Ankur Goyal: Database Systems Class, CMU, April 2016 Learn from Ankur Goyal, our VP of Engineering, as he returns to his alma mater at Carnegie Mellon University to deliver a deep-dive lecture into SingleStore database technology. The video lecture covers in-memory storage, concurrency control, crash recovery, code generation, distributed execution, and skip list indexes. SingleStore In-Depth Training Watch Carlos Bueno deliver his SingleStore In-Depth Training video series. Carlos reviews the key concepts and terminology fundamental to SingleStore database development and administration. Topics include an overview of SingleStore architecture, high availability, geospatial data, columnstore concepts, schema design, and cluster capacity planning. Product, Architecture, and Industry Landscape In under 20 minutes, Gary Orenstein’s Executive Overview identifies the big data pain points for modern enterprises. Gary reveals how SingleStore — as the #1 operational data warehouse and leading distributed, in-memory database — powers real-time data architectures and pipelines for ingesting, managing, and analyzing big data. SingleStore Product Documentation For quick start guides, hands-on tutorials, conceptual overviews, technical references, and FAQs, visit SingleStore Documentation at docs.singlestore.com SingleStore Knowledge Base Our collection of technical articles provides tips and tricks for designing, managing, securing, and optimizing a SingleStore database cluster. Explore the library of articles here: https://docs.singlestore.com/ SlideShare Presentations Discover 40+ presentations about real-time data pipelines, geospatial intelligence, predictive analytics, machine learning, and in-memory applications at scale. To learn more, visit slideshare.net/SingleStore Join memsql-public on Slack Community members and SingleStore engineers regularly share tips and discuss innovations on our public slack channel. We invite you to join the conversation at https//www.singlestore.com/forums/ Try SingleStore for Free The Community Edition of SingleStore comes with unlimited scaling, unlimited capacity, and unlimited free use. With Community Edition, you can start building real-time, in-memory applications on commodity hardware on-premises or in the cloud. If you want full access to production features, you can download the Enterprise trial edition of SingleStore. Both editions are available at singlestore.com/free SingleStore Training Our free and publicly available learning resources strengthen our community of data analysts, database administrators, and database developers. We invite you to join our community and start using SingleStore today. And when you are ready to formalize your mastery of SingleStore, join us for SingleStore Training. From installation, schema design, query optimization, and troubleshooting, SingleStore Training is the best preparation for building high-performance, operational data warehouses and real-time, in-memory database applications. Learn more about our course offerings and public training events at singlestore.com/training", "date": "2016-10-04"},
{"website": "Single-Store", "title": "a-flying-pig-and-the-zen-of-database-proof-of-concepts", "author": ["Krishna Manoharan"], "link": "https://www.singlestore.com/blog/a-flying-pig-and-the-zen-of-database-proof-of-concepts/", "abstract": "A customer asks potential vendors – I need a pig that can fly. Whoever can get me one, wins the deal. Vendor 1 – The Engineer says “There is no such thing as a flying pig. Do not waste our time. We are not interested.” Vendor 2 – The Geneticist says “I am going to create a new species of pig – one with wings.” He goes to work on a flying pig. He never comes back. Vendor 3 – The Practical One says “Flying pig indeed! Yes, we can get you one.” Vendor 3 takes a drone, makes it look like a pig on the outside and flies it. The approach that Vendor 3 takes is a classic example of redefining the problem or finding a suitable workaround to solve an issue. Executing a database proof of concept has similar themes: There are no perfect databases and no perfect workloads either. Real-world scenarios are for the most part models that have been built over many years. You would be hard-pressed to find a good data model and well written queries. Ideally, you tweak the database to suit the workload. The alternative is attractive, but time-consuming and requires customer buy-in. Time is always short. Innovating workarounds to known limitations and focusing on strengths is important. Solutions that are realistic, simple, and effective work well for the majority. Do not let perfection become the enemy of the good. Winning a database proof of concept requires the following steps: Understand the data and the workload. By peeking into the contents, you gain insight into the actual business use case. By knowing the relations and basic thought process that went into building this model you are in a better position to make changes as needed. This is the hardest step and takes the most effort and time. However, the payoff is well worth the hard work  – winning the customer’s confidence. Load the data. This is by far the easiest part. As you load data, you perform requisites such as gathering stats, choosing the right partition strategy, and indexing. Execute the workload. It gets more interesting here. At this point, you know if your database engine can deliver out of the box or needs tweaks. If you followed step 1, you have the in-depth knowledge to solve problems or make alterations. Unfortunately, most of us who have been in the industry long enough, including myself, have biases and preconceived notions. These biases can hinder your ability to find creative solutions to problems. To quote Bruce Lee – “Empty your cup so that it may be filled; become devoid to gain totality.” An open mind makes us more willing to consider alternatives. By locking ourselves up, we limit our capabilities. Our preconceived limitations define us and box us in. Once you have executed the workload and identified how to meet your customer requirements, the next step is to package up the results and present it. Converting a Successful Proof of Concept to a Deal is the Next Challenge I have done enough proofs of concepts to realize that the winner is rarely the best engineered solution. Economics trump everything, which means cost-effective solutions that meet most of the customer requirements tend to win the deal. To summarize, the ability to innovate, adapt, and be flexible more or less wins the deal. On a closing note – Being a Star Trek fan, everytime I run into a pickle with a proof of concept, I think back to the Kobayashi Maru Training Exercise. From wikipedia (edited) “The Kobayashi Maru is a training exercise in the Star Trek universe designed to test the character of Starfleet Academy cadets in a no-win scenario. The test’s name is to describe a no-win scenario , a test of one’s character or a solution that involves redefining the problem.”", "date": "2016-10-06"},
{"website": "Single-Store", "title": "how-advanced-analytics-drives-critical-success", "author": ["Monica Multer"], "link": "https://www.singlestore.com/blog/how-advanced-analytics-drives-critical-success/", "abstract": "Analytics is trending like never before. And with good reason. Organizations spanning multiple markets – from finance to energy, as well as retail, ecommerce, and media and communications – recognize the ability to quickly apply actionable insights using real-time analytics as a game-changer. Simply put, anticipating and adapting to rapidly evolving market dynamics as they occur is now a competitive imperative. How Advanced Analytics Drives Critical Success According to a recent Gartner report titled How Data Science Teams Leverage Advanced Analytics , “More than 50% of the surveyed users reported that senior executives see advanced analytics as critical to the organization’s success.” The report goes on to add, “Growing pains aside, more organizations are investigating potential applications for advanced analytics than ever before. Gartner’s inquiries on the topic nearly doubled from January 2014 to January 2015, and are projected to grow 26% year over year in 2016.” 1 Choosing the Right Advanced Analytics Vendor One of the most telling findings from the Gartner report has to do with the selection criteria for analytics platforms. Gartner discovered that the top two reasons businesses stated for choosing specific advanced analytics platforms were ease of use and the ability to quickly build large numbers of models. These two sought after characteristics illuminate underlying historical pain points that modern advanced analytics platforms have been able to overcome. In the past real-time analytics has been hindered by slow loading and querying times. In addition, concurrency – the ability to perform multiple data import, preparation and analytics tasks at the same time – has been conspicuously missing. Relying on disparate databases for transactions and data warehouses for analytics is no longer sufficient. In order for modern enterprises to truly operate in real time, the legacy approach of deploying a piecemeal collection of tools that require data shuffling or the painfully laborious ETL processes must be reimagined. Embracing a highly durable and distributed in-memory database that provides sub-second transaction and analytical processing and reporting is the future for businesses adopting advanced analytics platforms. Businesses seek vendors providing platforms with both the ease of use created by a Hybrid Transaction/Analytical Processing (HTAP) database as well as the scalability and flexibility of a distributed system. In addition to using HTAP platforms to achieve advanced analytics, businesses must pursue operational excellence by embracing data warehouses capable of continuously loading data in real time. These platform attributes are what truly set vendors providing advanced analytics apart from one another. Footnotes Gartner, How Data Science Teams Leverage Advanced Analytics, Peter Krensky | Lisa Kart, 29 August 2016 “Gartner does not endorse any vendor, product or service depicted in its research publications, and does not advise technology users to select only those vendors with the highest ratings or other designation. Gartner research publications consist of the opinions of Gartner’s research organization and should not be construed as statements of fact. Gartner disclaims all warranties, expressed or implied, with respect to this research, including any warranties of merchantability or fitness for a particular purpose.”", "date": "2016-10-11"},
{"website": "Single-Store", "title": "memsql-opens-new-office-in-second-tech-hub-seattle-wa", "author": ["Eric Boutin"], "link": "https://www.singlestore.com/blog/memsql-opens-new-office-in-second-tech-hub-seattle-wa/", "abstract": "Behind the scenes of the world’s leading companies in finance, retail, media, and energy, sits SingleStore – the operational data warehouse powering real-time data ingest and analytics. At SingleStore, hiring exceptional talent drives innovation in real-time technology and enables us to advance the state of the art in databases. We hire top engineers from prestigious universities such as MIT, Stanford, and Carnegie Mellon University, as well as companies like Facebook, Microsoft, Oracle and Google. These comprehensive academic and career accomplishments distinguish the SingleStore workforce and shape our hiring criteria for new talent. Today, we’re announcing the opening of a brand new SingleStore office in Seattle, Washington – part of a concerted effort to attract the best talent. The SingleStore team occupies the 26th floor of Smith Tower, a historical architectural landmark located at the heart of Pioneer Square – a vibrant neighborhood with a rich history. The office offers a breathtaking 360 degree view of the Puget Sound, Mt. Rainier, the Space Needle, and downtown Seattle. The concept for this office originated at the Very Large Database Conference where the SingleStore executive team met a number of industry experts based in Seattle. Inspired by the expertise and skill of the people we met, we decided to expand to the Pacific Northwest. Located in the same time zone as San Francisco and only a short flight away, Seattle makes for the perfect location of our second official engineering office. In addition to our development in Seattle, our San Francisco team is rapidly growing. Since 2011, we have continually expanded our product offerings, and have grown our user base into the thousands. This exponential growth, coupled with the addition of customers such as Akamai, Pinterest, and Dell Technologies, reinforces the growing market interest in real-time, in-memory database solutions. We are actively hiring in Seattle for engineers with experience in databases, distributed systems, query optimization, and storage systems. To learn more, please contact: recruiting@singlestore.com", "date": "2016-10-13"},
{"website": "Single-Store", "title": "election-2016-real-time-twitter-sentiment", "author": ["Neil Dahlke"], "link": "https://www.singlestore.com/blog/election-2016-real-time-twitter-sentiment/", "abstract": "November is nearly upon us, with the spotlight on Election 2016. This election has been amplified by millions of digital touchpoints. In particular, Twitter has risen in popularity as a forum for voicing individual opinions as well as tracking statements directly from the candidates. Pew Research Center states that “In January 2016, 44% of U.S. adults reported having learned about the 2016 presidential election in the past week from social media, outpacing both local and national print newspapers.” The first 2016 Presidential debate “between Donald Trump and Hillary Clinton was the most-tweeted debate ever. All told, there were 17.1 million interactions on Twitter about the event.” By now, most people have probably seen both encouraging and deprecating tweets about two candidates: Hillary Clinton and Donald Trump. Twitter has become a real-time voice for the public watching along with debates and campaign announcements. We wanted to hone in on the sentiments expressed in real time. Using Apache Kafka, SingleStore, Machine Learning and our Pipelines Twitter Demo as a base, we are bringing real-time analytics to Election 2016. Click here to access the live demo Post-Election, we have shut down the demo. View a screencap of it running at the bottom of this post. Introducing our latest live demonstration, Election 2016: Real-Time Twitter Analytics. We analyze the sentiment –attitude, emotion, or feeling– of every tweet about Clinton and Trump as it is tweeted. Now, anyone can see how high or low in the negative or positive tweets are trending at any given point. We’re giving everyone access to the broader scope of how each candidate is doing according to the Twittersphere. How it Works First, we wrote a python script to collect tweets and retweets that contain the words Hillary, hillary, Trump, or trump directly from Twitter.com. We picked the words “Hillary” and “Trump” as descriptors since they are the most used for the candidates. The script pushes this content to an Apache Kafka queue in real time. Messages in this Kafka queue are then streamed using SingleStore Pipelines. Released in September 2016 at Strata+Hadoop World, Pipelines features a brand new SQL command CREATE PIPELINE , enabling native ingest from Apache Kafka and creation of real-time streaming pipelines. The CREATE PIPELINE statement looks like this: CREATE PIPELINE `twitter_pipeline`\nAS LOAD DATA KAFKA ‘your-kafka-host-ip:9092/your-kafka-topic’\nINTO TABLE `tweets` The CREATE TABLE statement for the tweets table in SingleStore is shown below: CREATE TABLE `tweets` (\n`id` bigint(20) DEFAULT NULL,\n`ts` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,\n`tweet` JSON COLLATE utf8_bin,\n`text` as tweet::$text PERSISTED text CHARACTER SET utf8 COLLATE utf8_general_ci,\n`retweet_count` as tweet::%retweet_count PERSISTED int(11),\n`candidate` as CASE\nWHEN (text LIKE '%illary%') THEN 'Clinton'\nWHEN (text LIKE '%rump%') THEN 'Trump'\nELSE 'Unknown' END PERSISTED text CHARACTER SET utf8 COLLATE utf8_general_ci,\n`created` as FROM_UNIXTIME(`tweet`::$created_at) PERSISTED datetime,\nKEY `id` (`id`) /*!90619 USING CLUSTERED COLUMNSTORE */,\n/*!90618 SHARD */ KEY `id_2` (`id`)\n) Note: we create tweets as a columnstore table so it can handle large amounts of data for analytics. We also utilize persisted computed columns in SingleStore to parse JSON data for categorizing each tweet by candidate. SingleStore natively supports the JSON data format. When the twitter_pipeline is run, data in the tweets table looks like this: memsql> SELECT * from tweets LIMIT 1\\G\n*************************** 1. row ***************************\nid: 786409507039485952\nts: 2016-10-13 03:33:53\ntweet: {\"created_at\":1476329611,\"favorite_count\":0,\"id\":786409507039485952,\"retweet_count\":0,\"text\":\"RT @BlackWomen4Bern: This will be an interesting Halloween this year...expect me to tweet some epic Hillary costumes...I expect there will…\",\"username\":\"hankandmya12\"}\ntext: RT @BlackWomen4Bern: This will be an interesting Halloween this year...expect me to tweet some epic Hillary costumes...I expect there will…\nretweet_count: 0\ncandidate: Clinton\ncreated: 2016-10-13 03:33:31\n1 row in set (0.03 sec) Next, we created a second pipeline that pulled from the same Kafka topic, but instead of storing directly into a table, we perform real-time sentiment analysis with a SingleStore Pipelines transform that leverages the Python Natural Language Toolkit (nltk) Vader module . The CREATE PIPELINE statement for the second pipeline looks like this: CREATE PIPELINE `twitter_sentiment_pipeline`\nAS LOAD DATA KAFKA 'your-kafka-host-ip:9092/your-kafka-topic'\nWITH TRANSFORM ('http://download.singlestore.com/pipelines-twitter-demo/transform.tar.gz' , 'transform.py' , '')\nINTO TABLE `tweet_sentiment` Combining data from these two SingleStore pipelines, we can perform analytics using SQL. For example, we can create a histogram of tweet sentiment through the following query: SELECT\nsentiment_bucket,\nSUM(IF(candidate = \"Clinton\", tweet_volume, 0)) as clinton_tweets,\nSUM(IF(candidate = \"Trump\", tweet_volume, 0)) as trump_tweets\nFROM tweets_per_sentiment_per_candidate_timeseries t\nGROUP BY sentiment_bucket\nORDER BY sentiment_bucket; Lastly we constructed a User Interface (UI). We built the graph using WebSockets and React to visualize the rolling average tweet sentiment for both candidates, drawn in real time. Click here to access the live demo Post-Election, we have shut down the demo. View a screencap of it at the bottom of this post. Stream Twitter data into SingleStore as shown here by following these instructions: https://github.com/memsql/pipelines-twitter-demo Try SingleStore Pipelines today! Download the latest version of SingleStore at singlestore.com/free", "date": "2016-10-18"},
{"website": "Single-Store", "title": "real-time-roadshow-phoenix", "author": ["Gene Miguel"], "link": "https://www.singlestore.com/blog/real-time-roadshow-phoenix/", "abstract": "We’re packing our bags and heading to the Southwest to kick off the first ever SingleStore Real-Time Roadshow! Healthcare, education, aerospace, finance, technology, and other industries play a vital role in Phoenix, home to leading corporations like Honeywell, JP Morgan, AIG, American Express, Avnet, and UnitedHealth Group. Businesses in these industries face the constant challenge of keeping up with the high expectations of users and consumers that demand personalized and immediate services. To meet these challenges and elevate their businesses above the competition, industry leaders and data engineers in the Phoenix area embrace real-time applications as the solution. We’re bringing the Real-Time Roadshow to the capital of Arizona, to directly connect with this vibrant community of businesses and developers looking to pursue and learn more about real-time initiatives. Through a series of in-depth, technical sessions and demonstrations, this event provides an opportunity for data professionals and data leaders to investigate the power of real-time solutions. Here’s what you will learn: Forces driving the need for real-time workloads How to process and translate millions of data points into actionable insights How to drive new revenue and cut operating costs with real-time data How predictive analytics gives companies a competitive advantage to anticipate outcomes Top data architectures for real-time analytics and streaming applications Use cases and examples from companies building real-time applications Speaking Sessions Driving the On-Demand Economy with Predictive Analytics SingleStore CTO and co-founder Nikita Shamgunov demonstrates how a real-time trinity of technologies — Apache Kafka, Apache Spark, and SingleStore—enables companies to power their businesses with predictive analytics and real-time applications. Real-Time Analytics with SingleStore and Apache Spark SingleStore Engineer Neil Dahlke dives deep into how Pinterest measures real-time user engagement in this technical demonstration that leverages Spark to enrich streaming data with geolocation. Real-Time Data Ingest and Analytics with Apache Kafka and SingleStore Confluent Sales Engineer Rick Conrad and SingleStore Product Manager Steven Camina present how to model predictive analytics for global supply chain management in this technical Internet of Things (IoT) demonstration. Event Details Real-Time Roadshow Kimpton Hotel Palomar Phoenix Phoenix, Arizona Friday, November 4th 2016 – 8:30am-12:30pm This event is complimentary. Seating is limited, so please RSVP to guarantee your attendance.", "date": "2016-10-20"},
{"website": "Single-Store", "title": "appsflyer-shares-journey-to-real-time-analytics-using-memsql", "author": ["Lesia Myroshnichenko"], "link": "https://www.singlestore.com/blog/appsflyer-shares-journey-to-real-time-analytics-using-memsql/", "abstract": "At Reversim Summit 2016 , AppsFlyer, a SaaS mobile marketing analytics and attribution platform based out of Israel, presented their SingleStore use case. Yulia Trakhtenberg, Data Team leader at AppsFlyer, shared her team’s story and spoke about why the SingleStore in-memory scalable database provided the best solution for their problems. Problem AppsFlyer aggregates some 8 billion daily events in real time and their solution could not handle this amount of data using their current solution. Their team dealt with issues surrounding unresponsive dashboards, faulty modeling, and difficulty introducing new features. In addition, AppsFlyer faced dangerous failures and the risk of losing mission critical data with no hope of recovery. Challenge AppsFlyer required a database that scaled in real time, comfortably processed more data, and by extension, possessed more data dimensions. They wanted a very stable database that could expand without fear of unanticipated crashing. Even with all of these capabilities, performance still had to be very fast. In their search for an in-memory database that offers all of the above and more, AppsFlyer tried various options and none provided the desired solution. They tried Druid, MongoDB, Redis, Cassandra, Amazon Redshift, and more with no success. Solution Finally, Twingo , a senior partner and an official representative of SingleStore in Israel, introduced the SingleStore in-memory scalable database to AppsFlyer. The new AppsFlyer solution that implemented real-time aggregation over SingleStore integrated with batch processing over Apache Spark. SingleStore database is recoverable in case of data corruption or loss, possesses the capability to return to 0 point in case need arises to go back and start afresh. Furthermore, SingleStore supports 30 times more data than the previous solution AppsFlyer utilized. In the end, SingleStore saved AppsFlyer $20,000 monthly compared to the fee they paid to run their previous solution per month. Now, AppsFlyer can store more data, which translates to more money. The new SingleStore architecture not only addressed the pains AppsFlyer experienced, but allowed them to aggregate 10 times the amount of data with much faster response rates, keep up with product demands, all while remaining a cheaper solution. Check out the full presentation deck from Reversim Summit:", "date": "2016-10-24"},
{"website": "Single-Store", "title": "insights", "author": ["Emily Friedman"], "link": "https://www.singlestore.com/blog/insights/", "abstract": "Each morning, millions of individuals wake up to emails sent throughout the night, text messages, Facebook notifications, and reminders that we are only 1,000 steps away from hitting our weekly goal. And though we might bemoan the occasional alert reminding us to pay our rent or hit the gym, the truth is we live in a real-time world where technology has evolved to effortlessly guide us through every aspect of our day. So how do enterprises deliver seamless experiences to millions of consumers in real time? For an application to operate in real time, the supporting infrastructure must operate even faster. This includes operating systems, data storage, data processing, and application layers. Implementation starts with identifying a team, dedicating a budget, planning deployment, and finally evaluating solutions. Companies leading the pack include GE, Pinterest , Amazon, and Pandora, who successfully deliver fast, competitive services to satisfy wide-ranging consumer interests. To help companies understand the benefits of real time, we’ve created a new visual report highlighting what it means to define, embrace, and fully utilize real-time data and analytics across industries. Introducing the Global Pulse on Real-Time Data: 16 Definitive Insights . We paired sixteen powerful images with a selection of curated statistics to showcase the evolution of real time. Download the full report now What’s Inside? You will learn facts like, according to DHL , 50 billion devices will be connected to the Internet by 2020, which is half the number of stars in the Milky Way Galaxy. As we move faster and faster into the future, the data stockpile will only increase, as will consumer appetite for personalization and immediate responses from every digital touchpoint. To learn about these trends and the future of real-time data, download the report here: singlestore.com/insights", "date": "2016-10-27"},
{"website": "Single-Store", "title": "exactly-once-semantics-with-apache-kafka", "author": ["Kevin White"], "link": "https://www.singlestore.com/blog/exactly-once-semantics-with-apache-kafka/", "abstract": "The urgency for IT leaders to bring real-time analytics to their organizations is stronger than ever. For these organizations, the ability to start with fresh data and combine streaming, transactional, and analytical workloads in a single system can revolutionize their operations. When moving from batch to real time, data architects should carefully consider what type of streaming semantics will optimize their workload. The table below highlights the nuances among different types of streaming semantics. Understanding Streaming Semantics At Most Once With at most once semantics, a message is pulled once, but is not guaranteed to be received. At most once semantics conserves bandwidth and does not produce duplicates, but leaves us with a possibility of missing data. At Least Once At least once semantics pulls messages one or multiple times to guarantee every message is received. However, doing so consumes an unnecessary amount of network bandwidth and increases the likelihood of duplicate records being entered into the system, that ultimately need to be consolidated. Exactly-Once With exactly-once semantics, messages are pulled one or more times, processed only once, and delivery is guaranteed. Exactly-once semantics is ideal for operational applications, as it guarantees no duplicates or missing data. Many enterprise applications, like those used for credit card processing, require exactly-once semantics. Exactly-Once Semantics for Apache Kafka We recently launched SingleStore Pipelines , making it easier than ever to achieve real-time data ingestion at scale. SingleStore Pipelines ensures exactly-once semantics when streaming from message brokers like Apache Kafka. The webcast recording below details how to use SingleStore Pipelines for real-time data ingest at scale and exactly-once semantics for Apache Kafka. Watch now to learn how to: Quickly build real-time data pipelines using a new SQL command: CREATE PIPELINE Enable exactly-once semantics for Apache Kafka Execute user-defined transformations written in any programming language Enable rapid parallel data loading between SingleStore partitions and Kafka brokers Webcast Recording Slides", "date": "2016-11-04"},
{"website": "Single-Store", "title": "tc16", "author": ["Nicole Nearhood"], "link": "https://www.singlestore.com/blog/tc16/", "abstract": "Tableau Conference 2016 kicks-off in Austin, Texas on November 7-11, offering data engineers and business intelligence pros a place to gather and learn how to utilize data to tell a story through analytics and visualizations. SingleStore will be exhibiting the native connectivity and high performance partnership with Tableau using the Tableau SingleStore connector at TC16. Additionally, SingleStore will present a new showcase application, SingleStore Springs: Real-Time Resort Demographic Analysis. This demonstration showcases live customer behavior by demographic across resort properties, visualized with a Tableau dashboard. Attendees will learn how to natively connect Tableau to SingleStore for enhanced dashboard performance and scalability by visiting the SingleStore booth. Get the full details in our news release: singlestore.com/media-hub/releases/tc16 With over 400 speaking sessions to choose from at Tableau Conference, deciding the right session to attend can be a challenge. Discover how to utilize Tableau to capture and analyze data faster than ever with these five recommended sessions: ETL as Easy as ABC: Data Prep for Tableau with Javascript Is your data unavailable natively in Tableau? Not a problem. There is a multitude of options for preparing it and making it available to your business users via Tableau APIs. Whether you’re a long-time data engineer looking to dive into JavaScript for the first time or a seasoned JavaScript developer looking to get her feet wet in the Tableau developer ecosystem, this session is for you. When: 12:15pm to 1:15pm Where: ACC – L3 – Room 10C, Theatre Speaker(s): Eric Peterson, Tableau Connecting to Gnarly Data Sources New and novel databases are rapidly coming on the market. Come learn how to connect and unleash the power of data stored in these systems. In this session, we will cover: – What makes named connectors special – How to use Tableau Data Connectors to improve ODBC (Open Database Connectivity) connections – When to use a Tableau extract or a Web Data Connector When: 10:45am to 11:45am Where: ACC – L3 – Room 5A, Theatre Speaker(s): Arthur Gyldenege, Tableau | John Kew, Tableau IoT Dashboards to Make Sense of Sensor Data Analyzing sensor data can be overwhelming. How should you structure the IoT (Internet of Things) data? How do you create an overview monitor? When: 1:45pm to 2:45pm Where: Hilton – L4 – Room 400, Theatre Speaker(s): Vaidy Krishnan, Tableau | Marius Kaiser, Tableau How to Build a Culture of Data-Driven Analytics Companies today are on a journey to become data-driven. The pace and growth of data demands that every employee in your organization becomes a “data person” but shedding the hangover of the last 30 years of BI might be difficult. How can you leverage empowered analytics (self-service) to change the way your organization works? When: 10:45am to 11:45am Where: JW Marriott – L3 – Lone Star Ballroom D, Theatre Speaker(s): Paul Lilford, Tableau Faster Tableau Dashboard Demo at the SingleStore Visit the SingleStore booth #804, located in Hall 5 of the Austin Convention Center, to receive a complimentary copy of the company’s O’Reilly Book: The Path to Predictive Analytics and Machine Learning , as well as a chance to win an Estes Proto-X Drone. Book a personal demo here: http://www.singlestore.com/demo", "date": "2016-11-07"},
{"website": "Single-Store", "title": "memsql-tableau-and-the-democratization-of-data", "author": ["Neil Dahlke"], "link": "https://www.singlestore.com/blog/memsql-tableau-and-the-democratization-of-data/", "abstract": "“We love fast databases. It makes the experience of interacting with your database that much more enjoyable.” – Tableau Today’s business decisions are about seconds, not minutes. To accommodate this trend, businesses have moved to evidence-backed decision making and widespread data access. Modern business intelligence tools abound, making it easier for the average analyst to create compelling visualizations. In this post, I’ll address how this new mode of thinking about data, the Democratization of Data, comes with two challenges – making data easily available and making it actionable in real time. Making Data Available Companies are migrating to a new model of data distribution – shared access to a centralized database with both historical data and real-time data. This is a far cry from the traditional approach of using many small database instances with stale data, isolated silos, and limited user access. Now, raw data is available to everyone. Employees are empowered to dive into the data, discover new opportunities, or close efficiency gaps in a way that has never been done before. The need for data now coupled with scalability has attracted many developers to in-memory, clustered databases. Making Data Actionable in Real Time Innovations in data visualization have produced powerful, usable tools that afford companies the opportunity to be data-driven. One tool we see embedded across different industries is Tableau. With its mature ecosystem and rich featureset, the business intelligence platform makes it easy for individuals to create compelling, interactive data visualizations. It is a very attractive package for different business levels because it does not require expertise or a degree in visual design or information systems. Any user can create meaningful, actionable dashboards providing views of the business from thirty thousand feet as well as at ground level. But even with a Tableau license in hand, users still face issues – the dashboards are slow or the data is stale. The problem often lies in the database layer. It is imperative that data is up-to-date to be relevant to today’s fast moving business operations. Common issues include: No access to real-time, limited to batch loads Slow query execution Concurrency limitations Siloed data Limited hardware choices / technical debt accumulation SingleStore and Tableau have collaborated to provide the best possible business intelligence solution on the market, making great strides to address the aforementioned challenges. In August, Tableau announced the named SingleStore Connector , available in Tableau 10, for native integration between the two platforms. At Tableau Conference 2016, SingleStore showcases SingleStore Springs, a real-time showcase application for resort demographic analysis. The story is simple. When a team of executives gathers in a room and ends up staring at a loading spinner, the company bleeds money. You do not have multiple minutes to wait for your dashboards to update each and every time you want to ask your data a question. And yet this is an issue that many organizations still face. Slow dashboards are a roadblock to becoming data-driven. Your business needs instant results. By combining SingleStore and Tableau, users have reduced query latency from many minutes to a few seconds. This puts time back on the clock and dollars back in businesses’ pockets. Improve your dashboard speed today by downloading SingleStore: www.singlestore.com/free Visit our booth at Tableau Conference 2016 in Austin for a personal demo.", "date": "2016-11-10"},
{"website": "Single-Store", "title": "memsql-manages-smart-meter-data-with-leading-gas-and-electric-utility-enterprise", "author": ["Dale Deloy"], "link": "https://www.singlestore.com/blog/memsql-manages-smart-meter-data-with-leading-gas-and-electric-utility-enterprise/", "abstract": "Smart gas and electric meters produce huge volumes of data. A small SingleStore cluster of 5 nodes easily handles massive quantities of data like the workloads from leading gas and electric utility enterprises. In one particular use case, over 200,000 meter readings per second load into the SingleStore database while users simultaneously process queries against that data. Millions of meters sending between 10 and 30 sensor readings every hour leading to billions of rows of data. Just an initial part of a year contained over 72 billion meter reads, which measures up to 6 TB of raw data. SingleStore compressed this data by 10x and reduced it to 624GB of storage on disk. The queries against this data used to take hours, and now only takes seconds or a few minutes. One data scientist stated his query time dropped from 22 hours to 20 seconds. Utilities continue to build out Advanced Metering Infrastructure (AMI) projects with the promise of high speed analytics, but they have not delivered. SingleStore changes the game with AMI projects by providing a highly performant, yet affordable analytics platform. SingleStore helps AMI projects meet their goals and supports data scientists using predictive analytics. SingleStore keeps costs low by delivering high performance on a cluster of shared nothing commodity hardware servers with the ability to scale to hundreds of nodes and petabytes of data. Additionally, most organizations already have the skills in-house to manage a SingleStore database. SingleStore natively leverages the MySQL wire protocol, which means most organizations that already have skilled DBAs with MySQL background have all the skills necessary to deploy SingleStore. Most data tools already speak the language of MySQL so they can connect to SingleStore without modification. Utilities can leverage existing tools for BI, ETL and Data Science. Current implementations in AMI projects include Tableau for data visualization, Business Objects for reporting and SAP Data Services for ETL. One organization is using SAP Smart Data Access to integrate SingleStore with its HANA environment. SingleStore empowers utilities companies to perform rapid analytics on Smart Meter data which leads to more accurate grid volume predictions along with proactive grid maintenance reducing costs and delivering better customer service.", "date": "2016-11-11"},
{"website": "Single-Store", "title": "kellogg-case-study", "author": ["Kevin White"], "link": "https://www.singlestore.com/blog/kellogg-case-study/", "abstract": "Background About Kellogg Kellogg Company is the world’s leading cereal company, second largest producer of cookies, crackers, and savory snacks, and a leading North American frozen foods company. With 2015 sales of $13.5 billion, Kellogg produces more than 1,600 foods across 21 countries and markets its many brands in 180 countries. Driving Revenue with Customer Logistics Data Kellogg relies on customer logistics data to make informed decisions and improve efficiencies around shopping experiences. Accuracy and speed of such data is directly tied to the profitability of Kellogg’s business. Leveraging In-Memory for Faster Access to Data Making data readily available to business users is top-of-mind for Kellogg, which is why the company sought an in-memory solution to improve data latency and concurrency. Starting with an initiative to speed access to customer logistics data, Kellogg turned to SingleStore to make its 24-hour ETL process faster. In an interview at Strata+Hadoop World, JR Cahill, Principal Architect for Global Analytics at Kellogg said: “We wanted to see how we could transform processes to make ourselves more efficient and start looking at things more intraday rather than weekly to make faster decisions.” Results Reducing Latency from 24-Hours to Minutes JR and team scaled up their SingleStore instance in AWS and within two weeks reduced the ETL process to an average of 43 minutes. On top of that, the team added three years of archiving into SingleStore, a feat not possible with their previous system, while maintaining an average ETL of 43 minutes. Making Use of External Data Kellogg expanded its use of AWS and SingleStore to include 3rd party data, including twitter analysis along with additional enterprise data subject areas. The ease of integrating additional EC2 nodes into the SingleStore cluster supported the scale Kellogg required for new user concurrency and data volumes. Additionally, SingleStore provided Kellogg the ability to leverage AWS S3 and Kafka through the SingleStore data ingestion pipelines . Kellogg can now continuously ingest data from AWS S3 buckets and Apache Kafka for up-to-date analysis with exactly-once semantics for more accurate reporting. Making Data More Accessible in Tableau Kellogg uses Tableau as a business intelligence and data visualization tool throughout its entire organization. By integrating Tableau with SingleStore, Kellogg was able to run visualizations directly on top SingleStore rather than running an extraction. By doing so, Kellogg realized a 20x improvement on analytics performance. This allows hundreds of business users to concurrently access up-to-date data and increase the profitability of customer logistics data. JR summarizes the benefit, saying: “We have to be able to provide business value on each and every project. Because every business user is requesting speed and the ability to move at an incredibly iterative pace, we need to be able to provide that and in-memory allows us to do so.” Learn more from our customers or try SingleStore .", "date": "2016-11-15"},
{"website": "Single-Store", "title": "everything-weve-known-about-data-movement-has-been-wrong", "author": ["Gary Orenstein"], "link": "https://www.singlestore.com/blog/everything-weve-known-about-data-movement-has-been-wrong/", "abstract": "Data movement remains a perennial obstacle in systems design. Many talented architects and engineers spend significant amounts of time working on data movement, often in the form of batch Extract, Transform, and Load (ETL). In general, batch ETL is the process everyone loves to hate, or put another way, I’ve never met an engineer happy with their batch ETL setup. In this post, we’ll look at the shift from batch to real time, the new topologies required to keep up with data flows, and the messaging semantics required to be successful in the enterprise. The Trouble with Data Movement There is an adage in computing that the best operations are the ones you do not have to do. Such is the thinking with data movement. Less is more. Today a large portion of time spent on data movement still revolves around batch processes, with data transferred at a periodic basis between one system and the next. However, Gartner states , Familiar data integration patterns centered on physical data movement (bulk/batch data movement, for example) are no longer a sufficient solution for enabling a digital business. And this comical Twitter message reflects the growing disdain for a batch oriented approach… I hate batch processing so much that I won’t even use the dishwasher. I just wash, dry, and put away real time. — Ed Weissman (@edw519) November 6, 2015 There are a couple of ways to make batch processing go away. One involves moving to robust database systems that can process both transactions and analytics simultaneously, essentially eliminating the ETL process for applications. Another way to reduce the time spent on batch processing is to shift to real-time workflows. While this does not change the amount of data going through the system, moving from batch to real-time helps normalize compute cycles, mitigate traffic surges, and provides timely, fresh data to drive business value. If executed well this initiative can also reduce the time data engineers spend moving data. The Enterprise Streaming Opportunity Most of the discussion regarding streaming today is happening in areas like the Internet of Things, sensors, web logs and mobile applications. But that is really just the tip of the iceberg. A much larger enterprise opportunity involves taking so much of the batch processing that is embedded into business processes and turning that into real-time workflows. Of course, streaming does not need to mean everything is happening instantly down to the microsecond. In the context of the enterprise, it might just mean moving to continuous data motion, where batch processes are converted into real-time streams to drive insights for critical applications, such as improving customer experiences. Compared to some of the new application development underway, there are still large untapped opportunities of taking existing batch processes real time. Overall, the holy grail for data movement architects is linear scalability. More specifically if you double your hardware and have double the data load, you will double your throughput while maintaining low latency. Charting New Ground With Distributed Data Movement It helps to take a look at where we have been, and where we are heading with topologies for data movement. Starting from the early days of single server systems, and unfortunately still frequently in use, is the original point-to-point topology. While this approach is beneficial in simplicity, it’s single threaded nature makes it slow and difficult to scale. With the emergence of distributed systems, we’ve seen an expansion of data movement topologies. Point to multipoint is a next step in allowing for more performance. However, since one side of the equation is based on a single node, the performance is capped by the performance of that node. The source can be distributed too, allowing us to achieve even more with multipoint to multipoint communication. However, not every implementation of this topology is ideal. For example, in the following diagram, each individual node in the origin system on the left is communicating an independent point-to-multipoint job. Since there is no coordination, the jobs compete for resources and scaling in limited. A far more elegant solution is to retain multipoint to multipoint communication but managed as a single job. This is the architecture behind SingleStore Pipelines, where multiple paths are managed as one. This does require the exact same amount of data reshuffling on the receiving nodes, but this happens once all of the data has been staged in the receiving system and can therefore be handled easily. With this architecture if you double your hardware and double your data, you will double your throughput. Building A Robust Streaming Foundation Distributed systems and multipoint to multipoint data movement represent a step change in scalability of performance. However, this alone will not solve all the needs for enterprise workloads. Unlike sensor data or web logs, enterprise information generally relies on more robust and resilient systems to guarantee data integrity and delivery. One of the reasons batch processing has been prevalent is that it is easy to confirm the number of records in the batch and reconcile with records received. This has not been the case with real-time streaming data. Messaging systems typically rely on a set of semantics that determine the integrity level of the exchange. These are often designated At Most One, At Least Once, and Exactly-Once. Let’s take a closer look. At Most Once Using At Most Once semantics, a message will be passed from the initiating to the receiving cluster only once. Those messages may or may not arrive, but they will not be sent again. This approach limits bandwidth and retry exposure, but leaves room for missing data. At Least Once The next approach is At Least Once where messages will continue to be passed until receipt is confirmed. While these semantics ensure each record is received, the tradeoff is that there are likely to be duplicates requiring cleanup after the fact. Exactly-Once Another option is Exactly-Once which is more challenging to implement, but provides a richer, more robust mechanism to track streaming data and ensure records are received only once. This fits well with enterprise data flows that require this level of integrity and consistency like customer orders as one example. The easiest way to implement Exactly-Once is to record the offsets in the same database transaction as you loaded the data which is trivial with the multipoint-to-multipoint “done right” architecture. In this fashion, SingleStore Pipelines delivers Exactly-Once semantics out of the box out of the box. Comparing Streaming Semantics All three types of semantics are detailed on the chart below. The Exactly-Once approach delivers guaranteed receipt, no duplicates, and no missing data. The Future of Data Movement Just a few years ago point-to-point data movement was the dominant topology and batch ETL was the bane of every database administrators existence. With the adoption of distributed systems like Apache Kafka, Spark, the Hadoop Distributed File System (HDFS), and distributed databases like SingleStore, a new foundation for data movement is underway. In addition, the popularity of real-time streaming has only scratched the surface of transforming enterprise batch processes. However, when coupled with more robust messaging semantics, like Exactly-Once, enterprise architects have a new playbook in their toolkit to tackle today’s data challenges. Want to try it yourself? Take SingleStore for a spin at www.singlestore.com/free Special thanks to Joseph Victor and Steven Camina for their contributions and review of this post.", "date": "2016-11-16"},
{"website": "Single-Store", "title": "teespring-real-time-analytics", "author": ["Lesia Myroshnichenko"], "link": "https://www.singlestore.com/blog/teespring-real-time-analytics/", "abstract": "About Teespring Teespring is revolutionizing retail, giving entrepreneurs the opportunity to build and grow their brands via its ecommerce platform. Teespring makes it easy for anyone to create and sell high-quality products people love, with no cost or risk. Since 2012, Teespring has shipped more than fifteen million products around the world. Understanding Real-Time Buyer Activity Teespring was growing its community at a rapid pace. To further this growth, they wanted to connect vendors with the right buyers in real time. By analyzing buyer activity, Teespring can help its community of sellers make informed decisions to optimize buyer engagement and drive purchases on the site. Teespring needed to build an analytics dashboard that would showcase clickstream metrics such as click through rates, page views, and completed purchases as they happen. SingleStore for Real-Time Dashboards In order to build a low-latency analytics dashboard, Teespring need a highly performant, scalable system. SingleStore provides: In-memory speed and a powerful SQL engine Flexible cloud deployment ANSI SQL support for dynamic queries, delivering a scalable and easy-to-use database experience Teespring Architecture SingleStore as the real-time datastore and data serving engine Apache Kafka as the message queue SingleStore Spark Connector for easy data movement between Spark and SingleStore Data persisted in SingleStore as well as separate pipeline to Amazon Web Services (AWS) Optimizing Buyer and Seller Experiences With SingleStore in place, Teespring now: Helps sellers to reach buyers more likely to make purchases Eliminates buyers searching through irrelevant content and gets them straight to products they will love Can increase customer engagement at scale Improves operational health reports resulting in faster response times to issues “What I really like about SingleStore is the ANSI SQL support for dynamic querying needs, which gives us the analytics power we need to connect buyers and sellers in real time.” - Muru Muthusamy, Analytics Solutions Architect, Teespring For more details, check out our Q/A with Dan McCaffrey, VP Analytics at Teespring Get The SingleStore Spark Connector Guide The 79 page guide covers how to design, build, and deploy Spark applications using the SingleStore Spark Connector. Inside, you will find code samples to help you get started and performance recommendations for your production-ready Apache Spark and SingleStore implementations. Download Here", "date": "2016-11-21"},
{"website": "Single-Store", "title": "sql-the-technology-that-never-left-is-back", "author": ["Gary Orenstein"], "link": "https://www.singlestore.com/blog/sql-the-technology-that-never-left-is-back/", "abstract": "The Prelude The history of SQL, or S tructured Q uery L anguage, dates back to 1970, when E.F. Codd, then of IBM Research, published a seminal paper titled, “A Relational Model of Data for Large Shared Data Banks.” Since then, SQL has remained the lingua franca of data processing, helping build the relational database market into a $36 billion behemoth. The Rise And Fall of NoSQL Starting in 2010, many companies developing datastores tossed SQL out with the bathwater after seeing the challenges in scaling traditional relational databases. A new category of datastores emerged, claiming a new level of scalability and performance. But without SQL they found themselves at a loss for enabling easy analytics. Before long, it was clear that there were many hidden costs of NoSQL . The Comeback That Never Left More recently, many point to a SQL comeback, although the irony is that it never left. In a piece last week on 9 enterprise tech trends for 2017 and beyond , InfoWorld Editor in Chief Eric Knorr notes on trend number 3: The incredible SQL comeback For a few years it seemed like all we did was talk about NoSQL databases like MongoDB or Cassandra. The flexible data modeling and scale-out advantages of these sizzling new solutions were stunning. But guess what? SQL has learned to scale out, too – that is, with products such as ClustrixDB, DeepSQL, SingleStore, and VoltDB, you can simply add commodity nodes rather than bulking up a database server. Plus, such cloud database-as-a-service offerings as Amazon Aurora and Google Cloud SQL make the scale-out problem moot. At the same time, NoSQL databases are bending over backward to offer SQL interoperability. The fact is, if you have a lot of data then you want to be able to analyze it, and the popular analytics tools (not to mention their users) still demand SQL. NoSQL in its crazy varieties still offers tremendous potential, but SQL shows no sign of fading. Everyone predicts some grand unification of SQL and NoSQL. No one knows what practical form that will take. Taking the ‘no’ out of NoSQL In an article, Who took the ‘no’ out of NoSQL? , Matt Asay writes, In the wake of the NoSQL boom, we’re seeing a great database convergence between old and new. Everybody wants to speak SQL because that’s where the primary body of skills reside, given decades of enterprise build-up around SQL queries. The article, interviewing a host of NoSQL specialists, reminds us of the false conventional wisdom that SQL doesn’t scale. Quoting a former MongoDB executive, Assay notes, But the biggest benefit of NoSQL, and the one that RDBMSes have failed to master, is its distributed architecture. The reality is that legacy vendors have had trouble applying scale to their relational databases. However, new companies using modern techniques have shown it is very possible to build scalable SQL systems with distributed architectures. SQL Reigns Supreme in Amazon Web Services There is no better bellwether for technology directions these days than Amazon Web Services. And the statistics shared by AWS tell the story. In 2015 , Andy Jassy, CEO of Amazon Web Services, noted that the fastest growing service in AWS was the data warehouse offering Redshift, based on SQL. In 2016 , he noted that the fastest growing service in AWS was the database offering Aurora, based on SQL. And one of the newest services, AWS Athena, delivers SQL on S3. This offering is conceptually similar to the wave of ‘SQL as a layer’ solutions developed by Hadoop purveyors so customers could have easy access to unstructured data in HFDS. Lo and behold there were simply not enough MapReduce experts to make sense of the data. AWS has recognized a similar analytics conundrum with S3 growth, which has been so strong it appears that objects stores are becoming the new data lakes. And what do you do when you have lots of data to examine and want to do so easily? You add SQL. SQL Not Fading Away Nick Heudecker, Research Director in the Data and Analytics group at Gartner, put his finger on it recently, Each week brings more SQL into the NoSQL market subsegment. The NoSQL term is less and less useful as a categorization. — Nick Heudecker (@nheudecker) November 8, 2016 Without a doubt the data management industry will continue to debate the wide array of approaches possible with today’s tools. But if we’ve learned one thing over the last 5 years, SQL never left, and it remains as entrenched and important as ever.", "date": "2016-12-14"},
{"website": "Single-Store", "title": "the-state-of-real-time-for-property-and-casualty-insurers", "author": ["Seth Luersen"], "link": "https://www.singlestore.com/blog/the-state-of-real-time-for-property-and-casualty-insurers/", "abstract": "In some industries, a hesitance remains in recognizing the commodification forces of real-time solutions. These industries often rely on orthodox tenets as barriers to marketplace entry, such as regulatory compliance, traditional value propositions, brand recognition, and market penetration. The term “ripe for disruption” often characterizes these industries and their respective leaders. Arguably, an illustrative industry in the midst of responding to commodification, adapting to real-time technology, and fearing disruption is the Property and Casualty Insurance industry. An examination of this industry’s most commodified products and services serves as a litmus test for our common understanding of the state of real time. Let’s consider the most commodified line of business: personal auto insurance. Here, we find that many traditional insurers capture little to no real-time data about driver behavior and vehicle performance. Instead of real-time systems that capture, analyze, learn, and predict, these insurers rely on expensive national advertising campaigns, extensive commission networks, quarter-hour policy quotes, lengthy claim processes, long call center queues, and monthly billing cycles. Bringing IoT to Property and Casualty Metromile exemplifies a Property and Casualty insurer with a modern, transformative model for personal auto insurance. Using a smartphone app and an Internet of Things (loT) telematic device called Pulse that plugs into a car’s diagnostic port, Metromile owns the customer journey. Data-driven insights and services embody the digital experience for customers beyond common coverages: gas usage optimization, engine light demystification, parked vehicle location, and street sweeping parking ticket avoidance. An obvious technological challenge for system-of-record businesses like Property and Casualty insurers is real-time processing at scale. Even with hybrid – on-premise and cloud – datacenter infrastructures, many enterprise messaging and database technologies struggle with maintaining linear scale at commodity costs when attempting to process, analyze, learn, and predict from streaming data in real time. Why Enterprise Systems Struggle to Adapt to Real-Time The reasons why enterprise systems struggle to adapt to real-time include: 1) Event-based messaging and service-oriented architectures remain overly verbose and complex for internal and external integrations. 2) Batch jobs that extract, transform, and load data require additional computing resources to schedule, coordinate, and monitor the jobs. 3) Disk-based databases read and write only as fast as the best non-volatile solid state disks and IO caches perform. Light at the End of the Tunnel In examining the state of real-time through the lens of the Property and Casualty insurance industry, there is good news! Competitors are taking notice of the technology behind usage-based insurance. In 2016, several US insurers now underwrite auto-insurance policies requiring a telematic device ( Nationwide , Progressive , and State Farm ). To better segment risk profiles and enhance claim processing, 36% of auto insurance insurers expect to implement usage-based insurance products by 2020. This trend is representative of enterprise businesses looking to benefit by using IoT devices and operating at real-time. For enterprises looking to compete today, real-time technology is available on commodity hardware. With infinite iterator messaging systems like Apache Kafka paired with real-time database like SingleStore, today’s traditional enterprises can eliminate batch jobs, reduce integration complexity, improve network operations, and replace disk-based I/O operations with magnitudes faster, in-memory operations. Such systems produce, consume, and ingest data at millions of instances times per second while simultaneously analyzing, learning, predicting, and responding to real-time data. Most importantly, they do it at linear scale, meaning that the costs to scale as data and services grow remain linear. The only question now is how enterprises like those in the Property and Casualty insurance industry and in many other industries will harness the power of massively parallel, distributed, in-memory, SingleStore database technology to make possible real-time products and solutions for their customers.", "date": "2016-12-21"},
{"website": "Single-Store", "title": "in-memory-computing-market-predictions-2017-webinar", "author": ["Emily Friedman"], "link": "https://www.singlestore.com/blog/in-memory-computing-market-predictions-2017-webinar/", "abstract": "Adoption of in-memory technology solutions is happening faster than ever. This stems from a three pronged demand – first, a greater number of users, analysts, and businesses need access to data. Second, the number of transactions is increasing globally, so companies need faster ingest and analytics engines. Finally, performance inconsistencies are the nail in the coffin for companies competing in the on-demand economy – these enterprises need the responsiveness in-memory technology provides. In addition to these rising demands for real-time data access and analytics, several other factors contribute to in-memory technology adoption as outlined in the following graphic: Companies must evaluate what in-memory solution best fits their business needs– whether it is a pure in-memory database, a disk-based/persistent database with an in-memory column store, or an in-memory data grid. Our recent 451 Research and SingleStore webcast dives deep into the current state of in-memory computing and offers enterprises strategic steps for selecting the right technology for their needs. Watch now to discover: How to classify different in-memory systems The business advantages of in-memory computing In-memory use cases from leading corporations Predictions for the future of in-memory solutions Click here for the webcast recording In-Memory Computing Webcast. Market Predictions 2017 from SingleStore", "date": "2016-12-23"},
{"website": "Single-Store", "title": "protecting-against-the-insider-threat-with-memsql", "author": ["Mike Mohler"], "link": "https://www.singlestore.com/blog/protecting-against-the-insider-threat-with-memsql/", "abstract": "Today, a number of cyber attacks are carried out by malicious insiders or inadvertent actors. Whether a large government agency or commercial company, protecting your data is critical to successful operations. A data breach can cost significant amounts of lost revenue, a tarnished brand, as well as lost customer loyalty. For government agencies, the consequences can be more severe. SingleStore has a comprehensive security focus, including the ability to protect sensitive data against the “Insider Threat”. Specifically, we this post outlines best practices for security at the database tier. The first step to secure data infrastructure is a database platform such as SingleStore that provides enterprise level security. Today, large government agencies and commercial companies count on SingleStore to secure their most sensitive data. There are three critical pillars to securing your data in a database. Separation of Administrative Duties Data Confidentially, Integrity, and Availability 360° View of all Database Activity In the rest of this post, we will focus on the Separation of Administrative Duties. The primary goal here is to disintermediate the Database Administrator (DBA) from the data. Central to this is to not allow a DBA to grant themselves privileges without approval by a 2nd Administrator. There should also be special application specific DBAs separate from Operations and Maintenance Administrators. The developers and users should not be able to execute DBA tasks. This can all be done by setting up the following recommended roles. At the organization level: 1 . Compliance Officer Manages all role permissions Most activity occurs at the beginning project stages Shared resource across the organization 2 . Security Officer Manages groups, users, passwords in SingleStore Most activity occurs at the beginning project stages Shared resource across the organization At the project level: 1 . SingleStore Maintenance and Operations DBA Minimal privileges to operate, maintain and run SingleStore Can be shared resource across projects 2 . DBA per Database Application (Application DBA) Database and schema owner Does not have permissions to view the data 3 . Developer/User per Database Application Read and write data as permitted by the Application DBA Does not have permission to modify database Once the roles and groups are established, you assign users to these groups. You can then setup row level table access filtered by the user’s identity to restrict content access at the row level. For example, an agency may want to restrict user access to data marked at higher classification levels (e.g. Top Secret) that their clearance level allows. See RBAC User Guide and Row Level Security Guide in SingleStore documentation for details. Lastly and most importantly, your security environment can be permanently locked down once deployed. This is called “Strict Mode” and is a configuration setting that is irreversible once enabled. This ensures against the rogue admin modifying configuration in a production deployed system. See Strict Mode Guide in the SingleStore documentation for details. Too frequently data architects have had to compromise on security for select applications. With SingleStore, you can achieve real-time performance and distributed SQL operations while maintaining the utmost in security controls. Visit www.singlestore.com/free to try SingleStore today!", "date": "2017-01-10"},
{"website": "Single-Store", "title": "impact-of-machine-learning-oreilly-radar-podcast", "author": ["Nicole Nearhood"], "link": "https://www.singlestore.com/blog/impact-of-machine-learning-oreilly-radar-podcast/", "abstract": "O’Reilly Media Editor, Jon Bruner, recently sat down with SingleStore VP of Engineering, Drew Paroski, and SingleStore CMO, Gary Orenstein, to discuss the rapid growth and impact that machine learning will have in 2017. In this podcast , Paroski and Orenstein share examples from companies using real-time technologies to power machine learning applications. They also identify key trends driving the adoption of machine learning and predictive analytics. Listen Here Podcast topics of discussion include: How emerging data sources have added new advancements to machine learning models How energy companies use real-time, predictive analytics to operate wind farms and oil fields How to build a real-time data pipeline and considerations for deployment What skills managers need to consider as they build teams specializing in machine learning How machine learning and data science fields overlap Interested in More? We teamed up with O’Reilly Media on a new book: The Path to Predictive Analytics and Machine Learning . In this book, we share the latest step in the real-time analytics journey: predictive analytics, and provide a playbook for building applications that take advantage of machine learning. Free PDF Download Here: The Path to Predictive Analytics and Machine Learning", "date": "2017-01-13"},
{"website": "Single-Store", "title": "jumping-the-database-s-curve", "author": ["Gary Orenstein"], "link": "https://www.singlestore.com/blog/jumping-the-database-s-curve/", "abstract": "Adaptation and Reinvention Long term success hinges on adaptation and reinvention, especially in our dynamic world where nothing lasts forever. Especially with business, we routinely see the rise and fall of products and companies. The long game mandates change, and the database ecosystem is no different. Today, megatrends of social, mobile, cloud, big data, analytics, IoT, and machine learning place us at a generational intersection. Data drives our digital world and the systems that shepherd it underpin much of our technology infrastructure. But data systems are morphing rapidly, and companies reliant on data infrastructure must keep pace with change. In other words, winning companies will need to jump the database S-Curve. The S-Curve Concept In 2011, Paul Nunes and Tim Breene of the Accenture Institute for High Performance published Jumping the S-curve: How to Beat the Growth Cycle, Get on Top, and Stay There . In the world of innovation, an S-curve explains the common evolution of a successful new technology or product. At first, early adopters provide the momentum behind uptake. A steep ascent follows, as the masses swiftly catch up. Finally, the curve levels off sharply, as the adoption approaches saturation. The book details a common dilemma that too many businesses only manage to a single S-curve of revenue growth, in which a business starts out slowly, grows rapidly until it approaches market saturation, and then levels off. They share the contrast of stand-out, high-performance businesses that manage growth across multiple S-curves. These companies continually find ways to invent new products and services that drive long term revenue and efficiency. Authors Nunes and Breene outline three traits of high-performance companies that successfully scale multiple S-curves, A Big Enough Market Insight Companies must identify, “a substantial market change on the horizon that heralds the chance to build a major business for the company that identifies and seizes the opportunity.” Threshold Competence Before Scaling Companies “understand exactly how they must be distinctive in order to create the value the market demands.” Worthy of Serious Talent “High performers attract and keep the ‘serious talent’ they need—the people with the abilities and the attitude to drive the creation of successful businesses.” Applying the S-Curve to Data Infrastructure The S-Curve applies to new technologies and products, and for the rest of this piece, we apply the concept to the evolving database and data management world. The Monolithic Era The initial era of databases and data warehouses tracked the path of monolithic, single node, scale up systems. As CPU, memory, and storage density increased, data volumes and performance requirements were typically satisfied by a single server’s capabilities. Over time this expanded to very expensive servers and often more expensive storage area networks (SANs). Architects recognized the simplicity of single server systems, and the advantages provided to build robust architectures for mission critical applications. For most of the last thirty to forty years, since the development of SQL in the 1970s, this model served the industry well. However, when data volumes and performance requirements increased dramatically, as we see with digital megatrends, the limits of a single server system become readily apparent. The Distributed Era Starting around 2007, distributed datastores like Hadoop began to take hold. Distributed architectures use clusters of low-cost servers in concert to achieve scale and economic efficiencies not possible with monolithic systems. In the past ten years, a range of distributed systems have emerged to power a new S-Curve of business progress. Examples of prominent technologies in the distributed era include, but are certainly not limited to Message queues like Apache Kafka and AWS Kinesis Transformation tiers like Apache Spark Orchestration systems like Zookeeper and Kubernetes More specifically in the datastore arena are Key-value stores like Cassandra Document stores like MongoDB Relational datastores like SingleStore Advantages of Distributed Datastores Distributed datastores provide numerous advantages over monolithic systems, including Scale – aggregating servers together enables larger capacities than single node systems Performance – the power of many far outpaces the power of one Alignment with CPU trends – while CPUs are gaining more cores, processing power per core has not grown nearly as much. Distributed systems are designed from the start to scale out to more CPUs and cores Numerous economic efficiencies also come into play with distributed datastores, including No SANs – distributed systems can store data locally to make use of low-cost server resources No sharding – scaling monolithic systems requires attention to sharding. Distributed system remove this need Deployment flexibility – well designed distributed systems will run across bare metal, containers, virtual machines, and the cloud Common core team for numerous configurations – with one type of distributed system, IT teams can configure a range of clusters for different capacities and performance requirements Industry standard servers – low cost hardware or cloud instances provide ample resources for distributed systems. No appliances required Together these architectural and economic advantages mark the rationale for jumping the Database S-Curve. The Future of AI Augmented Datastores Beyond distributed datastores, the future includes more artificial intelligence (AI) and using it to streamline data management performance. AI will appear in many ways, including Natural Language Queries – such as sophisticated queries expressed in business terminology using voice recognition Efficient Data Storage – by identifying more logical patterns, compressing effectively, and creating indexes without requiring a trained database administrator New Pattern Recognition – discerning new trends in the data without the user having to specify a query Of course, AI will likely expand data management performance far beyond these examples too. In fact, in a recent news release Gartner predicts, More than 40 percent of data science tasks will be automated by 2020, resulting in increased productivity and broader usage of data and analytics by citizen data scientists Transcending Database S-Curves The industry currently sits at the intersection of the monolithic and the distributed datastore eras, and jumping S-Curves is no easy feat. By definition, a new S-Curve is significantly different from the prior, where many technologies, skill sets, and mindsets likely do not transfer forward. Addressing this transformation, Paul Nunes advises, Pay attention to insights that arise from the periphery of the organization Perhaps Nunes is suggesting successful companies have the talent and mindset needed to jump, but need to be creative to discover it. Adopting new products and technologies mandates change, which is rarely easy and often challenging at first. But if there is one universal truth about business, staying in place rarely leads to long term success. Companies that move forward however, have a chance to jump the S-Curve and lead another wave of growth.", "date": "2017-01-17"},
{"website": "Single-Store", "title": "memsql-meetups-2016-year-in-review", "author": ["Lesia Myroshnichenko"], "link": "https://www.singlestore.com/blog/memsql-meetups-2016-year-in-review/", "abstract": "We love hosting meetups at SingleStore headquarters. Meetups give us a hands-on way to share what we have been working on, connect with the community, and get in-person feedback from people like you. Last year we hosted meetups spanning topics from building real-time data pipelines to powering geospatial dashboards. We wanted to share a few highlights from our most memorable meetups in 2016. SingleStore Meetups 2016: Year in Review January 21, 2016 – Building Real-Time Digital Insight at Macys.com Featuring Chandan Joarder, Principal Engineer of Macys.com Shared lessons learned building a real-time insight application Macy’s innovative two-pronged approach to data exploration and visualization incorporates packaged tools like Tableau and custom BI dashboards Come see Chandan Joarder from Macy’s speaking at Strata+Hadoop World San Jose on Wednesday, March 15 at 1:50pm: Building Real-Time Dashboards with with Kafka, web frameworks, and an in-memory database March 9, 2016 – Building Real-Time Data Pipelines with Spark, Kafka, and Python Featuring hands-on session on creation of a data pipeline using Python Covered Streamliner architecture, setup and interaction with Spark Slides available here: http://www.slideshare.net/SingleStore/building-a-realtime-data-pipeline-with-spark-kafka-and-python June 29, 2016 – Visualize Your World with Geospatial Data and Mapbox Featuring Matt Irwin of Mapbox, showcasing geospatial data visualizations and techniques; and Neil Dahlke of SingleStore, demonstrating real-time applications – PowerStream and Supercar Matt and Neil shared how geospatial analytics is growing and how you can harness location-based intelligence to build rich, geo-enabled applications Slides available here: http://www.slideshare.net/SingleStore/realtime-geospatial-maps-by-neil-dahlke and http://mapbox.slides.com/maxsirenko/mapbox#/ August 11, 2016 – Real-Time Analytics with Confluent and SingleStore Featuring Steven Camiña, SingleStore Product Manager, and Hans Jespersen, Principal Systems Engineer at Confluent Demonstrated SingleStore showcase application modeling predictive analytics for global supply chain management and showed the latest and greatest tools for harnessing Apache Kafka with Confluent and SingleStore Slides and video available here: http://www.slideshare.net/SingleStore/realtime-analytics-with-confluent-and-memsql and https://youtu.be/iI9k _ wCVAjg October 25, 2016 – CREATE PIPELINE: Real-Time Streaming and Exactly-Once Semantics with Kafka Featuring Carl Sverre, SingleStore Architect, and John Bowler, SingleStore Engineer Walked through the technical details of Pipelines – how we built it, how we ensure exactly-once semantics, and why it matters; showcased several real-time applications built with SingleStore Pipelines Video available here: https://www.youtube.com/watch?v=aofMlYiXAeQ&feature=youtu.be", "date": "2017-01-24"},
{"website": "Single-Store", "title": "how-manage-accelerated-data-freshness-by-10x", "author": ["Kevin White"], "link": "https://www.singlestore.com/blog/how-manage-accelerated-data-freshness-by-10x/", "abstract": "Success in the mobile advertising industry is achieved by delivering contextual ads in the moment. The faster and more personalized a display ad, the better. Any delay in ad delivery means lost bids, revenue, and ultimately, customers. Manage , a technology company specializing in programmatic mobile marketing and advertising, helps drive mobile application adoption for companies like Uber, Wish, and Amazon. In a single day, Manage generates more than a terabyte of data and processes more than 30 billion bid requests. Manage analyzes this data to know which impressions to buy on behalf of advertisers and uses machine learning models to predict the probability of clicks, app installs, and purchases. Managing Data at Scale At the start, Manage used MySQL to power their underlying statistics pipeline, but quickly ran into scaling issues as data volume grew. Manage then turned to Hadoop coupled with Apache Hive and Kafka for data management, analysis, and real-time data feeds. However, even with this optimized data architecture, Manage found that Hive was slow and caused hours of delay in data pipelines. To meet customer expectations, Manage needed a solution that could deliver fresh data for reporting, while concurrently allowing their analytics team to run ad hoc queries. Kai Sung, Manage CTO and co-founder began the search for a faster database platform, and found SingleStore. The Manage team quickly started prototyping on SingleStore, and was in production within a few months. Streaming Log Data from Apache Kafka Manage uses SingleStore Streamliner , an Apache Spark solution, to first stream log data from Apache Kafka, then store it in the SingleStore columnstore for further processing. As new data arrives, the pipeline de-duplicates data and aggregates it into various summary tables within SingleStore. This data is then made available to an external reporting dashboard and reporting API. With this architecture, manage has a highly scalable, real-time data pipeline that ingests data and summarizes data as fast as it is produced. 10x Faster Data After implementing SingleStore, Manage was able to reduce the delay in data freshness from two hours down to 10 to 15 minutes. With SingleStore, the Manage team now has the ability to run analytics much faster and can react to marketplace changes in the moment. In an EnterpriseTech article, Kai Sung said, “We’ve built a highly scalable, real-time data pipeline that ingests and summarizes data as fast as we produce it. Our analytics team is able to run ad-hoc queries on log-level data within seconds.” For more details, check out the EnterpriseTech article: Managing 30B Bid Requests, 1.5B Users per Day in (near) Real Time If you are interested in SingleStore, you can download at www.singlestore.com/free", "date": "2017-01-26"},
{"website": "Single-Store", "title": "memsql-meets-the-microservices-architecture", "author": ["Dale Deloy"], "link": "https://www.singlestore.com/blog/memsql-meets-the-microservices-architecture/", "abstract": "Microservices use distributed computing to isolate and scale application service components, and this is the hot trend for application development. SingleStore sees this trend continuing because of the prolific increase of real-time applications. These applications require deep analytics and a strategy for manageable components delivering speed and scale. Each application service component has its own independent service deployment and flexible data model. The goal is to allow developers to focus on building business logic rather than coordinating with corporate models and infrastructure constraints. Microservices has promise for improving developer productivity but also has known drawbacks that need to be addressed. Decisions and struggles with Microservices include distributed transactions, eventual consistency and high performance analytics. One approach to addressing these challenges is to define the most appropriate data repository for your services, which also provides the flexibility to modify the data model at a service level while also delivering high performance joins to other objects. In addition the repository will also need to provide high performance analytics across all the data within a domain. By making a few base decisions and leveraging SingleStore as the data repository a highly flexible, scalable and performant solution can be delivered. The first choice is to leverage a single database table per service. The second choice is to leverage JSON as the model for your service data. SingleStore and Microservices Architecture How the components work together Each Service is independent and scales by adding nodes Services manage and use data within a JSON object Services persist JSON objects to SingleStore SingleStore indexes and stores the JSON in SingleStore Tables SingleStore scales by adding either Aggregator and/or Leaf nodes Analytics tools use SQL to query the JSON data Convert to computed SQL columns from JSON for speed improvements SingleStore has extended its SQL to support the traversing of JSON objects as well as the extraction of data from a JSON object so it can be fully indexed. Additionally once indexed it can also deliver fast, distributed join performance across SingleStore tables. Microservices addresses performance and scale of independent services; by adding SingleStore you also get high speed analytics, transactions and SQL queries across a distributed platform without having to restructure or move your data. One complexity of Microservices is the need to perform transactions across JSON objects (i.e. Customer orders have to update customer spend, place the order, and reserve inventory). While spending time on application development for committing complex transactions is possible, it adds great an entire new set of programming challenges for a developer’. SingleStore supports distributed transactions across the SingleStore nodes so the developer simply submits the required SQL transaction and SingleStore handles the rest. SingleStore supports flexible service definitions by using a JSON model. Developers can add data attributes without impacting SingleStore database queries or requiring ETL modifications. Additionally, SingleStore supports the streaming of JSON data directly into tables without having to build out a table schema. Streaming of JSON data allows developers to take advantage of event driven models as well as immediately consistent transactional models. The ability to deliver real time analytics within a Microservices Architecture is uniquely enabled by SingleStore. Combining SingleStore into your Microservices Architecture supports the goals of independent scaling and flexible development while also reducing the challenges of Microservices Architectures. SingleStore helps eliminate the struggles with Microservices. SingleStore includes a distributed transaction capability so developers are not burdened with distributed transaction complexity. Using SingleStore means there is no need to settle for eventual consistency as SingleStore provides immediate consistency within its read commit model. Additionally, SingleStore provides rich analytics capabilities for today’s analytics tools to interact and deliver real time information to your business. Visit www.singlestore.com/free to try SingleStore today!", "date": "2017-01-27"},
{"website": "Single-Store", "title": "memsql-at-spark-summit-east-2017", "author": ["Lesia Myroshnichenko"], "link": "https://www.singlestore.com/blog/memsql-at-spark-summit-east-2017/", "abstract": "Last week we announced the release of the SingleStore Spark 2 Connector with support for both Apache Spark 2.0 and 2.1. At Spark Summit Boston East 2017 next week we will showcase our new connector that operationalizes powerful advanced analytics. February 7-9 John B. Hynes Convention Center 900 Boylston Street, Boston, MA 02115 https://spark-summit.org/east-2017/ SingleStore CTO and Co-founder, Nikita Shamgunov and product manager, Steven Camiña will also deliver the following talks at the conference. Spark Summit East 2017 SingleStore Speaker Sessions The Fast Path to Building Operational Applications with Spark Nikita Shamgunov, CTO and Co-founder at SingleStore, will outline architecting real-time data pipelines with the power of Apache Spark and a robust, distributed in-memory database. In particular, he will detail how some of the world’s largest companies are running business critical applications using Spark. Attendees will dive deep into the mechanics of real-time pipelines, the ability to durably store data, and how to instantly derive insights from billions of data points. Session Details Speaker: Nikita Shamgunov , CTO and Co-founder, SingleStore Date and time: Thursday, February 9 from 2:00 PM – 2:30 PM Location: Ballroom A Building the Ideal Stack for Real-Time Analytics Steven Camiña, Product Manager at SingleStore, will share tools, techniques, and use cases for integrating real-time analytics across your organization. He will walk through critical technologies needed in your real-time stack, including Apache Spark, messaging queues, data management systems, and tools for data visualization and exploration. Steven will also provide a live demo, sharing how to build a data pipeline and real-time dashboard in under 5 minutes. Session Details Speaker: Steven Camiña, Product Manager, SingleStore Date and time: Wednesday, February 8 from 5:00 PM – 5:15 PM Location: Room 311 Suggested Sessions A Scalable Hierarchical Clustering Algorithm Using Spark Chen Jin from Uber in room 312, February 8 from 12:20 PM – 12:50 PM Building Real-Time Data Pipelines with Kafka Connect and Spark Streaming Ewen Cheslack-Postava from Confluent in Ballroom A, February 8 from 2:40 PM – 3:10 PM Artificial Intelligence: How Enterprises Can Crush It with Apache Spark Mike Gualtieri from Forrester in Ballroom B, February 9 from 9:50 AM – 10:10 AM Using SparkML to Power a DSAAS (Data Science as a Service) Shekhar Agrawal from Comcast in Ballroom C, February 9 from 5:00 PM – 5:30 PM SingleStore Happy Hour Join SingleStore and Spark Summit Attendees for appetizers and drinks on Wednesday at 8:00PM. We will be hosting a happy hour at Back Bay Social Club on 867 Boylston St . Games and Giveaways Be sure to stop by Booth #308 at the event. We will be giving away t-shirts, O’Reilly books, drones and VR headsets! If you would like book a demonstration with SingleStore while at the show you can do so here . Hope to see you at Spark Summit East! Get The SingleStore Spark Connector Guide The 79 page guide covers how to design, build, and deploy Spark applications using the SingleStore Spark Connector. Inside, you will find code samples to help you get started and performance recommendations for your production-ready Apache Spark and SingleStore implementations. Download Here", "date": "2017-02-03"},
{"website": "Single-Store", "title": "how-to-move-analytics-to-real-time", "author": ["Mike Boyarski"], "link": "https://www.singlestore.com/blog/how-to-move-analytics-to-real-time/", "abstract": "3x Spend Increase “Between 2016 and 2019, spending on real-time analytics will grow three times faster than spending on non-real-time analytics.” Every organization uses some form of analytics to monitor and improve their business. The growth of data has increased the impact of analytics and is a critical ingredient for delivering a successful digital business strategy. Companies are using more real-time analytics, because of the pressure to increase the speed and accuracy of business processes – particularly for digital business and the Internet of Things (IoT). – How to Move Analytics to Real Time, by W. Roy Schulte, Gartner, September 2016 In the Gartner report, “How to Move Analytics to Real-Time”, by W. Roy Schulte, there are several recommendations to guide your data management evolution from historical based analytics to a real-time system. Relevance of Real-Time for your Business Determining the right “Real-Time” approach for your business is an important first step, ensuring the objectives of the solution are aligned with the business outcome. In the Gartner report, Roy describes two types of real-time systems: “ Engineering real time is most relevant when dealing with machines and fully automated applications that require a precise sequence and timing of interactions among multiple components” “ Business real time is about situation awareness; sensing and responding to what is happening in the world now, rather than to what happened a few hours or days ago, or what is predicted to happen based on historical data.” Technology and Design Patterns for Real-Time A real-time system must process data sets very quickly. As data grows, there are several technologies and techniques to consider including in-memory databases, parallel processing, efficient algorithms, and innovative data architectures. Match the Speed of Analytics to the Speed of the Business Decision To ensure the proper investment return of a real-time system, the response of the analytics must align with the speed of the decision. Two questions determine the proper speed for your system. How quickly will the value of the decision degrade? How much better will a decision be if more time is spent? Automate Decisions if Algorithms Can Represent the Entire Decision Logic Lastly, algorithms offers the “last mile” of the decision. However, automating algorithms requires a well described process to code against. According to Gartner, “Decision automation is possible only when the algorithms associated with the applicable business policies can be fully defined.” Required Disclaimer: Gartner does not endorse any vendor, product or service depicted in its research publications, and does not advise technology users to select only those vendors with the highest ratings or other designation. Gartner research publications consist of the opinions of Gartner’s research organization and should not be construed as statements of fact. Gartner disclaims all warranties, expressed or implied, with respect to this research, including any warranties of merchantability or fitness for a particular purpose.", "date": "2017-02-06"},
{"website": "Single-Store", "title": "real-time-analytics-s3", "author": ["Seth Luersen"], "link": "https://www.singlestore.com/blog/real-time-analytics-s3/", "abstract": "SingleStore DB 5.7 introduces a new pipeline extractor for Amazon Simple Storage Service ( S3 ). Many modern applications interface with Amazon S3 to store data objects into buckets up to 5TB providing a new modern approach for today’s enterprise data lake. Without analytics, the data is just a bunch of files For modern enterprise data warehouses, the challenge is to harness the unlimited nature of S3 for ad-hoc and real-time analytics. For traditional data warehouse applications, extracting data from S3 requires additional services and background jobs that monitor buckets for new objects and then load those objects for reporting and analysis. Eliminating duplicates, handling errors, and applying transformations to the retrieved objects often requires extensive coding, middleware, or additional Amazon offerings. From data lake to real-time data warehouse A SingleStore S3 Pipeline extracts data from a bucket’s objects, transforms the data as required, and loads the transformed data to columnstore and rowstore. SingleStore Pipelines use the power of distributed processing and in-memory computing to extract, transform, and load external data in parallel to each database partition to achieve exactly-once semantics. To stream existing and new S3 objects while querying the streaming data at sub-second performance, a SingleStore S3 Pipeline runs perpetually. Rapid and continuous data ingest for real-time analytic queries is a native component of SingleStore. The constant data ingest allows you to deliver real-time analytics with ANSI SQL and power business intelligence applications like Looker , ZoomData , or Tableau . SingleStore Pipelines are a first class database citizen. Database developers and administrators can easily create, test, alter, start, stop, and configure pipelines with basic data definition language (DDL) statements or use a graphical user interface (GUI) in SingleStore Ops. Excited to get started with SingleStore S3 Pipelines? Follow these steps: 1) Open an AWS account. AWS offers an AWS Free Tier that includes 5 GB of Amazon S3 Storage, including 20,000 Get Requests and 2,000 Put Requests. 2) Download a 30-day free trial of the SingleStore Enterprise Edition or use the SingleStore Official Docker Image to run SingleStore. 3) With an available cluster running, create your first SingleStore S3 Pipeline using our S3 Pipelines Quickstart . The guide covers creating S3 buckets, a SingleStore database, and most importantly, a SingleStore S3 Pipeline.", "date": "2017-02-07"},
{"website": "Single-Store", "title": "guest-post-real-time-big-data-ingestion-with-meterial", "author": ["Lesia Myroshnichenko"], "link": "https://www.singlestore.com/blog/guest-post-real-time-big-data-ingestion-with-meterial/", "abstract": "This post originally appeared on the Myntra Engineering Blog . Learn how Myntra gained real-time insights on rapidly growing data using their new processing and reporting framework. Background I got an opportunity to work extensively with big data and analytics in Myntra. Data Driven Intelligence is one of the core values at Myntra, so crunching and processing data and reporting meaningful insights for the company is of utmost importance. Every day, millions of users visit Myntra on our app or website, generating billions of clickstream events. This makes it very important for the data platform team to scale to such a huge number of incoming events, ingest them in real time with minimal or no loss, and process the unstructured/semi-structured data to generate insights. We use a varied set of technologies and in-house products to achieve the above, including Go, Kafka, Secor, Spark, Scala, Java, S3, Presto and Redshift. Motivation As more and more business decisions tend to be based on data and insights, batch and offline reporting from data was simply not enough. We required real-time user behavior analysis, real-time traffic, real-time notification performance, and other metrics to be available with minimal latency for business users to make decisions. We needed to ingest as well as filter/process data in real-time and also persist it in a write fast performant data store to do dashboarding and reporting on top of it. Meterial is one such pipeline which does exactly this, and includes a feedback loop for other teams to take action from the data in real time. Architecture Meterial is powered by: 1 . Apache Kafka 2 . Data transformer, based on Apache Spark 3 . SingleStore real-time database 4 . UI built on React.js Deep Dive Our event collectors, written in golang ,sit behind Amazon ELB to receive events from our app/website. They add a timestamp to the incoming clickstream events and push them into Kafka. From Kafka, the Meterial ingestion layer, based on Apache Spark streaming, ingests around 4 million events/minute, filters and transforms the incoming events based on a configuration file, and persists them to a SingleStore rowstore table every minute. SingleStore return results for queries spanning across millions of rows with sub-second latency. Our in-house dashboarding and reporting framework (UDP: Universal Dashboarding Platform) has services which query SingleStore every minute and store the results in the UDP query cache, from where it is served to all the connected clients using socket-based connections. Results are displayed in the form of graphs, charts, tables, and other numerous widgets supported by UDP. The same UDP APIs are also used by Slackbots to post data into Slack channels in real time, using Slack outgoing webhooks. As all transactional data currently lies in Redshift, and reporting of commerce data with user data every 15 minutes is needed, Meterial also enables this ad-hoc analysis on data for our team of data analysts. Every fifteen minutes, data from SingleStore for that interval is dumped into S3, from where it is loaded to Redshift using our S3—Redshift extract, transform, and load (ETL) routines. We selected Spark as our streaming engine because of its proven scale, powerful community support, expertise that we already had within the team, and easy scalability (with proper tuning). To choose a real-time datastore for Meteriel, we did a proof of concept on multiple databases. We drilled down to SingleStore. SingleStore is a high-performance, in-memory and disk-based database that combines the horizontal scalability of distributed systems with the familiarity of SQL. We have seen SingleStore to support very high concurrent reads/writes very smoothly at our scale with proper tuning. Currently we are exploring the SingleStore columnstore as the analytics database for our A/B test framework (Morpheus) and segmentation platform (Personify). Sample UI Screenshots Traffic Notification Future of Real-time Analytics at Myntra Using real-time data with predictive analytics, machine learning, and artificial intelligence opens altogether new doors to understand user behavior, what paths and funnels leads to purchases, and more. Getting such information in real time can definitely help us boost results from our e-commerce, and take corrective actions as soon as possible, when something goes wrong. Given the importance of these results, we are constantly working on improving and enhancing Meteriel.", "date": "2017-02-17"},
{"website": "Single-Store", "title": "building-ideal-stack-for-real-time-analytics", "author": ["Mason Hooten"], "link": "https://www.singlestore.com/blog/building-ideal-stack-for-real-time-analytics/", "abstract": "Building a real-time application starts with connecting the pieces of your data pipeline. To make fast and informed decisions, organizations need to rapidly ingest application data, transform it into a digestible format, store it, and make it easily accessible. All at sub-second speed. A typical real-time data pipeline is architected as follows: Application data is ingested through a distributed messaging system to capture and publish feeds. A transformation tier is called to distill information, enrich data, and deliver the right formats. Data is stored in an operational (real-time) data warehouse for persistence, easy application development, and analytics. From there, data can be queried with SQL to power real-time dashboards. As new applications generate increased data complexity and volume, it is important to build an infrastructure for fast data analysis that enables benefits like real-time dashboards, predictive analytics, and machine learning. At this year’s Spark Summit East, SingleStore Product Manager, Steven Camina shared how to build an ideal technology stack to enable real-time analytics . Video: Building the Ideal Stack for Real-Time Analytics Slides: Building the Ideal Stack for Real-Time Analytics Use Cases Featured in the Presentation Pinterest: Monitoring A/B Experiments in Real Time Learn how Pinterest built a real-time experiment metrics pipeline, and how they use it to set up experiments correctly, catch bugs, and avoid disastrous changes. More in this blog post from the Pinterest Engineering Team: Monitoring A/B Experiments in Real Time. Energy Company: Analyzing Sensor Data Learn how a leading energy company built a real-time data pipeline with Kafka and SingleStore to monitor the status of drill heads using sensor data. Doing so has dramatically reduced risk of drill bit breakage allows for more accurate forecasting for drill bit replacement.", "date": "2017-02-28"},
{"website": "Single-Store", "title": "faster-data-warehouse-track-at-gartner-data-and-analytics-summit", "author": ["Lesia Myroshnichenko"], "link": "https://www.singlestore.com/blog/faster-data-warehouse-track-at-gartner-data-and-analytics-summit/", "abstract": "Gartner Data and Analytics Summit kicks off next week in Grapevine, TX where SingleStore CTO Nikita Shamgunov will showcase the power of real-time for Chief Data Officers who are faced with transitioning their data warehouse applications to the cloud. Gartner Data and Analytics Summit features 8 tracks that will address top challenges facing data and analytics leaders. SingleStore executives and Gartner Analysts will deliver sessions on the benefits of a faster data warehouse throughout the summit. Our top sessions are listed below. Monday, 6 Mar 2017, 6:00 PM – 6:20 PM SingleStore: Creating an IoT Kafka Pipeline in Under 5 Mins Gary Orenstein, SingleStore @garyorenstein In this session, Gary Orenstein will conduct a live IoT demonstration, building a real-time data pipeline using Apache Kafka to capture real-time data from millions of sensors. This demo combines Apache Kafka a real-time data warehouse, and machine learning to highlight the power of predictive analytics in industry applications. Tuesday, 7 March 2017, 9:45 AM – 10:30 AM Magic Quadrants for Data Management and Integration, and Master Data for Analytics and Beyond Andrew White, VP Distinguished Analyst, Gartner @mdmcentral Mark A. Beyer, VP Distinguished Analyst, Gartner @metadatabeyer The old world of managing data just for transactions or just for analytics is being challenged. This session cuts across and through the old markets for data warehousing, master data management and data integration tools and covers the strengths of existing platforms as the evolve toward their new role to support broader data governance in the digital business era. Tuesday, 7 March 2017, 3:45 PM – 4:30 PM SingleStore: The Real-Time CDO and the Cloud-Forward Path to Predictive Analytics Nikita Shamgunov, CTO, SingleStore @NikitaShamgunov The success of Chief Data Officers requires a focus on business improvement, often by productizing and commercializing information assets. Learn the principles of modern enterprise data warehouse platforms, and how the multi-cloud, real-time, and machine learning present new opportunities. This session will feature business and strategy examples, as well as live technical demonstrations showing the power of an operational data warehouse – one that can ingest data in real-time and concurrently execute the most sophisticated SQL queries. Thursday, 9 March 2017, 8:00 AM – 8:45 AM Delivering on Digital Business With Stream Processing and Real-Time Analytics Roy Schulte, VP Distinguished Analyst, Gartner Event streams are data in motion, used in ERP, IoT, CRM, HCM and other digital business applications. Data and analytics leaders need to understand stream processing concepts and determine where and how to apply them. What are events, event streams and stream analytics? How do streaming applications differ from traditional BI and data management? What are the trends in open source and commercial streaming products? Thursday, 9 March 2017, 10:30 AM – 11:15 AM How Machine Learning Extracts Knowledge From Data Alexander Linden, Vice President, Gartner @Alex42Linden Machine learning — already one of the most versatile technologies of the past decade — will support an amazing array of advanced analytics use cases in digital business. In this session we will discuss basics, benefits, pitfalls and vendor selection. How does machine learning work? What are the challenges and pitfalls? How can you get started? See you at the show – Booth 423 SingleStore will also be exhibiting at booth 423. Drop by for a free copy of our book – The Path to Predictive Analytics and Machine Learning, see a live demonstration, and enter for a chance to win a drone! Book a SingleStore Demo at Gartner Data and Analytics Summit ⇒ Last week we announced that Gartner has positioned SingleStore as a Challenger in the 2017 Magic Quadrant for Data Management Solutions for Analytics. To view the complete report, visit singlestore.com/customers", "date": "2017-03-02"},
{"website": "Single-Store", "title": "gartner-magic-quadrant-analytics", "author": ["Kevin White"], "link": "https://www.singlestore.com/blog/gartner-magic-quadrant-analytics/", "abstract": "The data warehouse as we know it has changed. Growth in data size and complexity, migration to the cloud, and the rise of real-time use cases are forces pushing enterprise organizations to expect more from their data warehouse. Evidence of this trend can be found in the latest Gartner Magic Quadrant Report , which has dropped data warehouse from its title and graduated to Data Management Solutions for Analytics. This change has resulted in an expansion of types of vendors covered, while also making the inclusion criteria significantly more challenging. We are hosting complimentary access to the full Gartner Magic Quadrant for Data Management Solutions for Analytics. Access Here → Gartner Analysts Simon James Walker and Michael Patrick Moran also identify real-time data as a top priority for data and analytics leaders in the report, Three Top Trends in Master Data Management Solutions Through 2017, highlighting the following impacts and recommendations: Real-time data has emerged as an Master Data Management (MDM) priority for data and analytics leaders , with high availability master data a key capability as data volume, velocity and variety increases Multidomain and multivector MDM enable digital business to advance from “collecting” data for a single data domain to “connecting” several data domains Improved packaged industry MDM solutions are accelerating time to value , enabling data and analytics leaders to deliver successful MDM programs SingleStore Positioned as Challenger in Gartner Magic Quadrant Gartner has positioned SingleStore as a Challenger in this Magic Quadrant, citing broad use use-case support, flexible deployment options, real-time processing, and customer engagement as factors leading to our movement to the top half of the Magic Quadrant. Our support of operational, analytic, and real-time streaming use cases were also cited as key strengths by Gartner. To view the complete report, visit singlestore.com . /gartner-magic-quadrant-analytics/", "date": "2017-03-06"},
{"website": "Single-Store", "title": "strata-hadoop-machine-learning-2017", "author": ["Nicole Nearhood"], "link": "https://www.singlestore.com/blog/strata-hadoop-machine-learning-2017/", "abstract": "Strata+Hadoop World in San Jose kicks-off next week on March 14, offering data engineers and business intelligence professionals a place to gather and learn about the most challenging problems, engaging use cases, and enticing opportunities in data today. SingleStore will be showcasing real-time data as a vehicle for operationalizing machine-learning, exploring advanced tools including TensorFlow, Apache Spark, and Apache Kafka. We will also be demonstrating the power of machine learning to effect positive change in our keynote: Machines and the Magic of Fast Learning . See everything we have planned for Strata+Hadoop World including customer talks, events, demos, and giveaways at: singlestore.com/events Hand-Picked Talks at Strata+Hadoop World With over 250 speaking sessions to choose from, deciding the right session to check out at Strata can be a bit overwhelming. To help you out, we have mapped out our top five recommended sessions for the conference: Tuesday, March 14 Developing a modern enterprise data strategy Edd Wilder-James (Silicon Valley Data Science), Scott Kurth (Silicon Valley Data Science) 1:30pm–5:00pm Tuesday, March 14, 2017 – Location: 210 B/F A data strategy should guide your organization in two key areas—what actions your business should take to get started with data and where to start to realize the most value. Edd Wilder-James and Scott Kurth explain how to create a modern data strategy to power data-driven business. Wednesday, March 15 Real-time analytics at Uber scale James Burkhart (Uber) 11:00am–11:40am Wednesday, March 15, 2017 – Location: LL20 B James Burkhart, technical lead on real-time data infrastructure at Uber, will showcase how Uber is supporting millions of analytical queries daily across realtime data. Going real time: Creating online datasets for personalization Christopher Colburn (Netflix), Monal Daxini (Netflix) 11:50am–12:30pm Wednesday, March 15, 2017 Christopher Colburn and Monal Daxini explore the challenges faced when building a streaming application at scale at Netflix. Christopher and Monal share their experience with stream processing unbounded datasets in the personalization space. These datasets are both massive and product facing—they directly affect the customer’s personalized experience—which means that the impact is high and tolerance for failure is low. Thursday, March 16 Machine Learning and Predictive Analytics Rob Craft (Google) 1:50pm–2:30pm Thursday, March 16, 2017 – Location: 210 B/F Join Google’s product lead for Cloud Intelligence in a discussion of machine learning and predictive analytics. Rob will discuss how you can leverage the power of ML whether you have a machine learning team of your own or if you just want to use ML as a service. One cluster does not fit all: Architecture patterns for multicluster Apache Kafka deployments Gwen Shapira (Confluent) 2:40pm–3:20pm Thursday, March 16, 2017 In the last year, multicluster and cross-data center deployments of Apache Kafka have become the norm rather than an exception. Gwen Shapira offers an overview of several use cases, including real-time analytics and payment processing, that may require multicluster solutions and discusses real-world examples with their specific requirements. SingleStore Talks at Strata+Hadoop World We also have a handful of customers speaking at Strata+Hadoop World. SingleStore featured sessions are listed below: Real-Time Analytics at Uber-scale 11:00am–11:40am Wednesday, March 15, 2017 – Room LL20 B James Burkhart, technical lead on real-time data infrastructure at Uber, will showcase how Uber is supporting millions of analytical queries daily across realtime data. Building Real-Time Dashboards with Kafka, Web Frameworks, and an In-Memory Database 1:50pm–2:30pm Wednesday, March 15, 2017 – Room LL20 A Chandan Joarder, Principal Engineer and Raj Sriram, Engineering Manager, Digital Data Engineering at Macy’s will share how to build real-time dashboards using tools such as Kafka, Spark, and an in-memory database. Building a Real-Time Analytics Engine with Kafka and Spark for Mobile Advertising 9:30am–10:00am Tuesday, March 14, 2017 – Room LL20 A Robin Li, Director of Data Engineering and Yohan Chin, VP Data Science at Tapjoy will share how to architect the best application experience for mobile users using technologies including Apache Kafka, Apache Spark, and SingleStore. Machines and the Magic of Fast Learning Keynote: 10:10am Wednesday, March 15, 2017 – Grand Ballroom Eric Frenkiel, cofounder and CEO of SingleStore, will walk through advanced tools including TensorFlow, Spark, and Kafka, as well as compelling use cases demonstrating the power of machine learning to effect positive change. Say Hello at the SingleStore Booth #1019 Visit the SingleStore booth #1019 , located in the Expo Hall of the San Jose Convention Center, to receive a complimentary copy of the company’s O’Reilly Book: The Path to Predictive Analytics and Machine Learning . While you are there, be sure to grab our latest T-shirt and enter for a chance to win an Estes Proto-X Drone.", "date": "2017-03-07"},
{"website": "Single-Store", "title": "machine-learning-image-recognition-engineering-perspective", "author": ["Eric Boutin"], "link": "https://www.singlestore.com/blog/machine-learning-image-recognition-engineering-perspective/", "abstract": "About Thorn Thorn partners across the tech industry, government and NGOs, building technology to combat predatory behavior, identify victims, and protect vulnerable children. About Eric Boutin Eric leads an engineering team for SingleStore in our Seattle office. This is background information from Eric on our work with Thorn. How did you first get connected with Thorn? I was introduced to Federico Gomez Suarez, a volunteer working with Thorn, by a common friend. I was impressed by the work Thorn was doing, and excited about the opportunity to help them. What specific technical challenges did you see as opportunities? Thorn was working on face recognition and machine learning work to analyze pictures on the internet to protect vulnerable children. The main technical challenge they had however, was to match the fingerprint of a picture with the fingerprints of an extremely large number of other pictures. Thorn needs to match a very large number of pictures per second, all in real time, with a gigantic database of pictures that is constantly being updated. What connections were you able to draw to SingleStore capabilities? The fingerprint matching problem seemed like a natural match for SingleStore. The dataset is too large to fit in one machine, and very high parallelism is required to match pictures in real time. While the process of extracting the fingerprint out of an image is extremely complex, the process of matching fingerprints consists of linear operations of vectors. The difficulty here is the vast mountain of changing data that has to be processed in real time, and to me, this looked a lot more like a database problem than a just a machine learning problem. More specifically, it seemed like a perfect use case for SingleStore. Did you have to develop anything for SingleStore so Thorn could succeed? Overall the distributed and parallel architecture of SingleStore was a natural fit for the problem that Thorn needed to solve. The only gap was the ability to do linear algebra operations on vectors in order to match image fingerprints. I added database operators to perform the required linear algebra operations. Given the steep performance requirement, I used the AVX2 instructions set to implement the linear algebra operations to minimize the latency. A few hours later I was able to test real time fingerprint matching at scale. What improvements were made possible for Thorn by using SingleStore? When we started the project they didn’t have a solution to the problem of matching the fingerprint of images. Thorn was investigating a number of approaches, but they had not yet found an approach which would match image fingerprints in real time. Those improvements are enabling them to move forward with the project which will in turn protect children more effectively. How might this work apply to other use cases or industries? The key insight from this project is that by adding basic linear algebra operations to the SQL language, any machine learning system using models that can be evaluated by using linear algebra (logistic regression, linear regression, k-mean or k-nn using euclidean or cosinusoidal distance) could be evaluated directly in a SQL query. For example, Click Through Rate prediction is a machine learning problem where a website is trying to predict which advertisement has the highest probability of being clicked on. The problem can be modeled as a linear regression between one user and a large number of ads, and the ads with the highest probability of being clicked on is picked. Logistic regression actually consists of a simple dot product between the ‘ad’ vector and the ‘user’ vector followed by a few scalar operations. We can imagine applications where the same database is being used for click through prediction, as well as business intelligence and analytics on the real time stream of clicks and impressions. In a few line of SQL the user could express ‘select the ad with the higher predicted click through rate for a given user, from an advertiser that still has enough budget’. In the same transaction, the application could then deduct money from the advertisers budget to account for the impression. What do you see next in terms of new innovations in this arena? I would like this field to innovate from two different directions. On the one hand, I would like to see databases support more and more algebra primitives to allow expressing more complex machine learning models. For example, supporting matrices, vector/matrix operators, aggregation across multiple vectors, and so on. This would allow expressing a growing number of machine learning algorithms in SQL. Even neural networks can be expressed as a sequence of scalar and vector operations. I would then like to see machine learning framework ‘push down’ algorithms into databases using SQL. Today Business Intelligence tools commonly push down joins and filtering into databases to leverage their high performance query processing engine. We could see machine learning frameworks push down parts of the algorithm (compute the gradient of the error for example) as a SQL query in the database engine to more effectively process data at scale.", "date": "2017-03-13"},
{"website": "Single-Store", "title": "arcgis-spark-memsql-integration", "author": ["Gary Orenstein"], "link": "https://www.singlestore.com/blog/arcgis-spark-memsql-integration/", "abstract": "This is a guest post by Mansour Raad of Esri . We were fortunate to catch up with him at Strata+Hadoop World San Jose. This post is replicated from Mansour’s Thunderhead Explorer blog ArcGIS, Spark & SingleStore Integration Just got back from the fantastic Strata + Hadoop 2017 conference where the topics ranged from BigData, Spark to lots of AI/ML and not so much on Hadoop explicitly, at least not in the sessions that I attended. I think that is why the conference is renamed Strata + Data from now on as there is more to Hadoop in BigData. While strolling the exhibition hall, I walked into the booth of our friends at SingleStore and got a BIG hug from Gary . We reminisced about our co-presentations at various conferences regarding the integration of ArcGIS and SingleStore as they natively support geospatial types. This post is a refresher on the integration with a “modern” twist, where we are using the Spark Connector to ETL geo spatial data into SingleStore in a Docker container. To view the bulk loaded data, ArcGIS Pro is extended with an ArcPy toolbox to query SingleStore, aggregate and view the result set of features on a map.< Like usual, all the source code can be found here SingleStore Reference links If you are interested in learning more about SingleStore and Geospatial functions, please visit the Geospatial Guide of our documentation. Get The SingleStore Spark Connector Guide The 79 page guide covers how to design, build, and deploy Spark applications using the SingleStore Spark Connector. Inside, you will find code samples to help you get started and performance recommendations for your production-ready Apache Spark and SingleStore implementations. Download Here You can try SingleStore for free here", "date": "2017-03-20"},
{"website": "Single-Store", "title": "machines-and-the-magic-of-fast-learning", "author": ["Kevin White"], "link": "https://www.singlestore.com/blog/machines-and-the-magic-of-fast-learning/", "abstract": "How can big data and machine learning be used for good? Updated Dec. 12, 2019, including adding a transcript. This blog post discusses the use of SingleStore as a database for machine learning and artificial intelligence. See our recent Thorn case study for the latest on SingleStore’s work with Thorn, including Thorn’s use of SingleStore Managed Service. In our keynote at Strata+Hadoop World, SingleStore CEO Eric Frenkiel shared how we are working with Thorn to provide a new approach to machine learning and real-time image recognition to combat child exploitation. About Thorn Thorn partners across the technology companies and government organizations to combat predatory behavior, rescue victims, and protect vulnerable children. Thorn has to sift through a massive amount of images daily. Images are processed using facial recognition, then classified and de-duplicated, and ultimately matched against millions of open web images. If an image match can be found faster, victims of trafficking and sexual exploitation can be helped faster. With SingleStore, Thorn is able to accelerate their image recognition workflow. New vectors representing a face can be inserted and queried in real-time. This allows analyst to find image matches faster and improve law enforcement response times. For more detail on our work with Thorn, see our recent Thorn case study. You can also watch our recorded keynote and download the slides below. In addition, we have a Q&A blog post from the engineers behind the application: http://blog.memsql.com/machine-learning-image-recognition-engineering-perspective/ Video Slides Machines and the Magic of Fast Learning – Strata Keynote from SingleStore Transcript Lightly edited transcript of the video above, which describes the use of SingleStore as a database for AI and machine learning, at Thorn in particular. Eric Frenkiel speaking… Well, good morning. I’m very excited to be here to be talking about machines and the magic of fast learning. Let me go ahead and kick off the slide show in a moment, and we can talk about what we’re going to be focusing on today: using a large corpus of data, and really referencing applications as actors and data scientists as operators. This is now possible because data scientists are actually operationalizing data models and creating a synergistic feedback loop with machine learning that lets applications move faster, data scientists get more precise data, and make a lot of great things in a business. What we’ve seen is that there is a greater precision possible with this type of positive feedback loop. In addition, businesses can now tune what they’re doing in real time. And lastly, to get ahead of the game, it’s all about discovery, and the ability to use these same machine learning models and look ahead as to what will happen next. But I’m here this morning to talk about a very important issue for us: what can be done to help nonprofits working in big data. And in particular, we’re working with Thorn, which is a nonprofit dedicated to eradicating the exploitation of children on the internet today. I’m regretful to share some of these numbers, because they’re downright shocking and very, very serious. In the same way that we’ve seen big data growth, there’s been more than 5,000% increase in images of sexually exploited children online since 2007. And the fact is, nearly half of all victims meet their perpetrators online first. Thorn has a very, very important mission. It’s about creating big data capability that lets Thorn move faster and more effectively to find these children and return them to their families. Thorn has to sift through a vast amount of data daily. More than 100,000 ads are posted on the public web alone, each day, in the United States. When you look at how this has to happen, they’re using machine learning, with facial recognition, to provide an ability to understand what is going on in any given image that is posted onto the public web. This happens by taking a raster to vector conversion, creating a point map of a given face. Using more than 5,000 data points for a given face, they can deduplicate images, then they can classify a given image and correlate and match it. All to help law enforcement find that child and return them to their family. This is literally a needle in a haystack type of problem. When you convert this, you are literally sifting through multiple millions of images to identify this victim. And they have to do this every single time to make sure that they don’t miss any given child. SingleStore worked with Thorn to add a new capability that lets them actually process this at a thousand-fold improvement time. By adding a vector dot product operation into the database, and going directly to Intel’s AVX2 SIMD instruction set, you can now fully saturate a processor’s pipeline and effectively do more floating point operations in a given cycle to make this operation faster. And in terms of what this means, this is the ability to take what would be a positive match from 20 minutes down to 200 milliseconds. (A 100-fold improvement – Ed.) So when we talk about real-time data, it’s important to remember that it can have very, very important real world impact. And as a matter of fact, we have members of the Thorn team in the audience today, and if you would join me in thanking them for what they’ve done. Alone, in 2016, they found more than 2,000 children. So, if we can give these guys a round of applause. It’s incredibly important to be able to help big data challenges, both in commerce and in nonprofits. And, for me, this is one of the most important things I’ve been able to say. How amazing it is to help Thorn achieve this mission. But to take a step back and to learn how we can apply this in other areas. There are so many different datasets that are purely image-based, for which you can leverage machine learning to gain insights. Whether it’s mapping, social imagery, or even forms that have handwriting. The ability to actually apply machine learning at scale accelerates a business’s ability to operate. We’ve seen a number of our end users engaged with TensorFlow to provide a lot of this machine learning, and it is a phenomenal framework. You can use Hadoop as a phenomenal data store, to store lots and lots of this blob data, this image data. But the real question becomes, how can we actually accelerate that? And it actually turns into a Lambda architecture type of use case, where you use a message queue like Kafka to fork the data. Data can flow to the permanent data lake, Hadoop, and also to a real-time, in-memory processing system with TensorFlow, in a data store like SingleStore, which has high speed vector algebra built-in. This is then able to be rendered out to that given model or application. Later today we actually have a few customers talking about how they’re using real time analytics at scale. Both Uber for business intelligence, and how Macy’s is actually using SingleStore for real-time dashboarding with click-stream analysis. You can also join us later today to pick up a book, which goes into detail about this type of learning, called The Path to Predictive Analytics and Machine Learning , published by O’Reilly. You too can take advantage of the ease of use, ease of management, and reliability of SingleStore Managed Service. Use SingleStore for free or contact SingleStore today .", "date": "2017-03-28"},
{"website": "Single-Store", "title": "top-6-design-principles-memsql", "author": ["Gary Orenstein"], "link": "https://www.singlestore.com/blog/top-6-design-principles-memsql/", "abstract": "At SingleStore, we believe that every company should have the opportunity to become a real-time enterprise. We believe that data drives business, and that data management performance leads to successful business results. Specifically related to our products, we believe in: The Need for Performance To compete in a 24×7 business world, companies must be able to ingest data quickly, execute low latency queries, and support a large number of analytics users and data volume, all at the same time. The Scale of Distributed Systems Today, systems need to scale beyond a single server, and distributed systems remove single server performance and capacity constraints. The strength of many can act as one. Our blog post on Jumping the Database S-Curve details more. The SQL Standard The Structured Query Language has served the data industry well for decades. We see core SQL capabilities as a prerequisite to analytics success. SQL as an afterthought, or as a layer, is not sufficient for real-time applications. Recently, SQL has been making the news again as The Technology That Never Left Is Back . Of course, there is more to the world than just SQL, and SingleStore also supports JSON, key-value models, geospatial data and connectivity to Spark for more advanced functions. The Flexibility to Store Data from Memory to Disk Memory, specifically DRAM, provides performance for modern workloads. But memory must also be coupled with disk so companies can retain real-time and historical data in a single system. The Move to the Cloud and Need for Cloud Choice Computing is moving to the cloud, and companies need the ability to deploy solutions on any server configuration, on any public or private cloud, or as a service. The Convergence of Transactions and Analytics With the right architecture, transactions and analytics can occur within the same system, eliminating the ETL process, consolidating database and data warehouses, and allowing immediate insight into critical applications.", "date": "2017-03-29"},
{"website": "Single-Store", "title": "data-changing-from-big-data-to-real-time-data", "author": ["Gary Orenstein"], "link": "https://www.singlestore.com/blog/data-changing-from-big-data-to-real-time-data/", "abstract": "Data is changing. You knew that. But the dialog over the past 10 years around big data and Hadoop is rapidly moving to data and real-time. We have tackled how to capture big data at scale. We can thank the Hadoop Distributed File System for that, as well as cloud object stores like AWS S3. But we have not yet tackled the instant results part of big data. For that we need more. But first, some history. Turning Point for the Traditional Data Warehouse Internet scale workloads that emerged in the past ten years threw the traditional data warehouse model for a loop. Specifically, the last generation of data warehouses relied on Scale up models; and Appliance approaches Vast amounts of internet and mobile data have made these prior approaches uneconomical, and today customers suffer the high cost of conventional data warehouses. Images created with Infinite Design from Infinite Studio . Focus on BIG Around the time of this new workload boom, Hadoop appeared and captured the attention of companies worldwide. The origins of Hadoop derive first from Google, then Yahoo and Facebook. In a matter of just a few years, Hadoop was synonymous with big data. But throughout most of Hadoop’s existence, the focus has been on the big, more than the fast. “Big Data” became a preferred term but as Gartner analyst Nick Heudecker noted on Twitter, Increasingly, the term #BigData is used by industry laggards, not leaders. — Nick Heudecker (@nheudecker) March 27, 2017 Recognizing Hadoop For What It Is, And Not It became clear that Hadoop was really two things in one The MapReduce computation capabilities that could execute operations in batch mode The Hadoop Distributed File System which, even with 3x replication, provided far cheaper storage than traditional alternatives Unfortunately neither proved to be very effective for fast data. MapReduce has been passed over in many cases for faster processing engines like Spark. And many users find that the only way to make use of MapReduce is through pre-computing, a lengthy process that leaves data inherently out of date and restricted to a pre-defined set of dimensions. HDFS provides cheap storage, but none of the mechanisms for fast ingest or fast access. This relegates HDFS to an archive, but not much more Tackling Fast Data Requirements Hadoop realities still leave data engineers short of handling fast data requirements. In particular they need Fast Data Ingest The ability to natively ingest large volumes of data De-duplicating data on the fly with unique keys Low Latency Queries Native SQL queries, built on a SQL engine, for sophisticated, real-time adhoc analytics High Concurrency Lock-free handling of read AND write workloads Support for large numbers of simultaneous users and queries Architecting a Data Lake with A Real-Time Data Warehouse A simple architecture has emerged to help customers handle data volumes and real-time requirements. Application or Message Queue (K) Data Archive (LAKE) Real-Time Data Warehouse (RTDW) Real-Time Analytics/Application (RT APPS) K for Kafka, another message queue, or an application. The architecture often begins with Apache Kafka, the popular message queue, but could start with other message queue or even directly with applications that write to the data warehouse and data lake. LAKE Hadoop or cloud object store. Often there is a desire to store everything in a data lake, so the rest of the company has a consolidated archive of data. This approach can be retained while still incorporating a real-time solution RTDW Real-Time Data Warehouse. A real-time data warehouse enables fast ingest of changing data while simultaneously delivering instant response to analytics from real-time applications or dashboards. The real-time data warehouse can also bring data in from the data lake should there be a need. RT APPS Real-Time Applications. The goal is to have instant access to data, so that the business can run as fast as it should. Delivering a fast path for fresh analytics allows applications and dashboards to remain current. Being current becomes a currency in and of itself. Real-Time for Instant Response, Lake for the Batch The real-time data warehouse becomes the solution to enable the front lines of your business. When you count on response times and data freshness, look to your real-time data warehouse. Everything else has an option to go to the lake. Batch processing, offline operations, and long term archive of historical data suit the data management characteristics of solutions like Hadoop. Make Every Moment Work for You As more companies move to real time, they need a technical and business minded approach to new solutions. Four tenets can help get you there Drive down conventional data warehouse costs Traditional data warehouse solutions focused on appliance models, non-scalable architectures, and batch processing can be replaced by modern distributed, memory-optimized solutions that cost significantly less. Lambda Architectures Lambda architectures deliver ingest, archive, and real-time data serving. See this blog post for more. A real-time data warehouse, optionally with a data lake, gives you just that. Real-Time Analytics Businesses live or die based on the relevance, accuracy, and timeliness of their information and decisions. Take processes from days to hours, hours to minutes, and minutes to seconds or less with real-time analytics. A Native SQL Engine Today everyone recognizes the importance of retaining SQL. But adding SQL as an afterthought to a data lake can only service offline, batch process requirements. A SQL engine built natively into the real-time data warehouse provides the foundation for live analytics embedded in applications or dashboards. Launch Your Real-Time Journey Today To get ahead with a real-time data warehouse, please visit www.singlestore.com", "date": "2017-04-03"},
{"website": "Single-Store", "title": "real-time-and-the-rise-of-nano-marketing", "author": ["Gary Orenstein"], "link": "https://www.singlestore.com/blog/real-time-and-the-rise-of-nano-marketing/", "abstract": "The tracking and targeting of our online lives is no secret. Once we browse to a pair of shoes on a website, we are reminded about them in a retargeting campaign. Lesser known efforts happen behind the scenes to accumulate data and scan through it in realtime, delivering the perfect personalized campaign. Specificity and speed are converging to deliver nano-marketing. If you are a business leader, you’ll want to stay versed in these latest approaches. If not, as a consumer, you’ll likely want to understand how brands are enabling their craft to you personally. Brands seek specific customer interactions. If you sign up for a retailer’s newsletter, you might receive a preferences questionnaire so they can tailor everything to your specific wants or needs. But speed also matters, as many of the largest marketing-driven industries like fashion, TV, movies, and music, depend on relevancy in the moment. Being current is currency itself. Only through real-time interaction can this be achieved. Looking ahead, leaders of digital initiatives will expand their focus from today’s notion of personalized marketing to “nano-marketing” using tools to predict granular audience cohorts on the fly and prescribe individualized marketing experiences in real time. Brands can increase customer experience directly through context, by individual interaction, and instantaneously. For example, when you walk into a furniture showroom where you also have an online account, the sales representative should know what you were searching for before you arrived, even if it was just a few hours ago. And they should have easy access to your Pinterest page if you’ve made that public. These are the types of experiences we can expect in the future with nano-marketing. Behind nano-marketing, taking personalized marketing to the next level The concept behind personalized marketing is hardly new. Brands have always strived to create special experiences for customers in order to entice them to return. With the creation of Customer Relationship Management (CRM) systems and the proliferation of social media, this idea has become even more popular. Marketing to customer segments of one merges existing and new disciplines to the trade. The low bar for what currently qualifies as “personalized marketing” will soon rise with the advent of tools that allow finer granularity, faster. Looking ahead, we can expect three areas of marketing innovation: The Autonomous Marketing Stack Marketers have a plethora of available tools across infrastructure and analytics, including platforms like Salesforce.com, Marketo, Eloqua, Omniture, Google Analytics, and dozens of more specialized offerings. Truthfully the availability of special purpose tools has outstripped the individual’s ability to integrate them. In the coming years, we’ll move far beyond just cobbling together the tools that help us be more efficient and cater to our customers; we’ll have a marketing tool stack that implements and executes campaigns on its own. Imagine a system that watches social feeds for popular items, aggregates existing content to resurface it into the discussion, and kicks off a set of new content assets to carry the conversation forward. And this happens between Friday and Sunday with little human effort. Virtual Reality Is The New Content Today marketers often focus on generating a considerable amount of written content. Tomorrow they will put the pen down and focus on virtual experiences for customers that allow them to interact with content in ways not possible before. With attention spans getting shorter, and the firehose of new content bombarding customers, brands will need to focus on things that don’t just inform, but also entertain. Whereas today an automobile company might customize regional billboards to fit with the landscape, soon they will offer tailored virtual reality experiences in a city and driving venue of your choice. Where The Real-Time Meets The Road Finally all of this will come together in the insatiable pursuit of instant gratification. Not only will consumers not be surprised by real-time results, they will come to demand it. To stay on top, marketers, and the tools they use, will need to absorb, process, and contextualize information more quickly to deliver unique interactive experiences. This is already happening in areas like ad tech and finance , but stay tuned as the latest in real-time technologies work their way across all industries.", "date": "2017-04-07"},
{"website": "Single-Store", "title": "real-time-analytics-at-uber-scale", "author": ["Mason Hooten"], "link": "https://www.singlestore.com/blog/real-time-analytics-at-uber-scale/", "abstract": "We’ve created an updated version of this blog post with much more detail. – Editor At Strata+Hadoop World, James Burkhart, technical lead on real-time data infrastructure at Uber, shared how Uber supports millions of analytical queries daily across real-time data with Apollo, Uber’s internal analytics querying language. James covers architectural decisions and lessons learned from building an exactly-once ingest pipeline that captures raw events across in-memory row storage and on-disk columnar storage. He also details how Uber uses a custom metalanguage and query layer by leveraging partial OLAP result set caching and query canonicalization. Putting all the pieces together provides thousands of Uber employees with subsecond p95 latency analytical queries spanning hundreds of millions of recent events. Video and Slides: About James James Burkhart, is the technical lead on real-time data infrastructure at Uber. James has a strong background in time series data storage, processing, and retrieval. Previously, he worked on Blueflood, a time series database on top of Cassandra, while at Rackspace. Additional Resources Uber Engineering Blog: eng.uber.com Uber Open Source: uber.github.com Uber Eng Twitter: twitter.com/ubereng Uber Slides: http://msql.co/uberscale", "date": "2017-04-10"},
{"website": "Single-Store", "title": "verify-trusted-users", "author": ["Mike Mohler"], "link": "https://www.singlestore.com/blog/verify-trusted-users/", "abstract": "Continuing with the blog post, Protecting Against the Insider Threat , where the theme was the “Separation of Administrative Duties” as a way to disintermediate the Database Administrator (DBA) from the data, this blog will focus on the need to “Trust, but Verify” your users. It is always the assumption that employees will act in their best interest for the good of the organization and their customers. But, what if they don’t? Unfortunately, this can be the reality in some organizations and requires extended security in your database to ensure protection against this threat. SingleStore delivers the extra security you need. Of course, we support all the typical enterprise security features for maintaining data confidentiality, integrity and availability. Features such as Role Based Access Control (RBAC) as discussed in the previous blog, end-2-end SSL TLS 1.2 based encryption, 3rd party encryption stack integration for data at rest, as well as easy-to-configure high availability to guard against data loss and system downtime. However, you always need to know that the administrators and users that connect to your system are trusted. Let’s discuss two critical principles to address this threat. Specifically, deep database auditing for a 360° view of all session activity along with trusted authentication into the database to maintain confidentiality to prevent sensitive information from being accessed by the wrong people. SingleStore Deep Database Auditing SingleStore logs all database activity to an external location. Once written, analysis tools can then be used to perform information security analysis to verify access control policies are enforced, as well as investigate suspicious activity or system outages. The system also captures the User’s intention in the audit logs BEFORE returning any data back to the user. SingleStore auditing provides configurable filtering to allow for five levels of logging. These levels will capture valid and invalid SQL statements and queries as defined below: ADMIN-ONLY-INCLUDING-PARSE-FAILS\nWRITES-ONLY-INCLUDING-PARSE-FAILS\nALL-QUERIES-INCLUDING-PARSE-FAILS\nALL-QUERIES-PLAINTEXT-INCLUDING-PARSE-FAILS\nALL-RESULTS-INCLUDING-PARSE-FAILS When you remove ‘-INCLUDING-PARSE-FAILS’ from any of the above log levels, only valid SQL statements and queries are logged. This logging enables SingleStore to record administrator and/or user activity along with the results of the query. Should a command contain sensitive data, like a password, SingleStore redacts that information from the audit logs. SingleStore Authentication An organization must also reliably identify users and provide secure authentication and authorization to support data confidentiality. Using an identity federation standard such as SAML (Security Assertion Markup Language) allows an organization to support this requirement to establish trusted access to their applications and database systems. SingleStore supports SAML, as well as Kerberos, PAM, and native authentication. The process involves establishing user credentials in the identity provider and then mapping those to users inside SingleStore. When a user attempts to authenticate, the security token is validated before granting access. (See SAML Authentication and Kerberos using GSSAPI online documentation for details). Whether using SAML tokens or Kerberos principals to represent user credentials, the user credentials are mapped to an internal user within SingleStore. Adding and removing users in SingleStore is simple and explained in the Securing SingleStore online documentation. Conclusion In this blog series on “Protecting Against the Insider Threat”, we focused on the separation of duties as well as trusted verification of your database users. With SingleStore, enterprises and government organizations get an enterprise quality database that meets the triad of security principles (Confidentiality, Integrity, and Availability) while delivering very high performance. Today, SingleStore meets these requirements and also supports ICD-503 for the highest level data protections. With SingleStore, you will achieve real-time performance and distributed SQL operations with maximum security controls.", "date": "2017-04-13"},
{"website": "Single-Store", "title": "video-scoring-machine-learning-models-at-scale", "author": ["Mason Hooten"], "link": "https://www.singlestore.com/blog/video-scoring-machine-learning-models-at-scale/", "abstract": "At Strata+Hadoop World, SingleStore Software Engineer, John Bowler shared two ways of making production data pipelines in SingleStore: 1) Using Spark for general purpose computation 2) Through a transform defined in SingleStore pipeline for general purpose computation In the video below, John runs a live demonstration of SingleStore and Apache Spark for entity resolution and fraud detection across a dataset composed of a hundred thousand employees and fifty million customers. John uses SingleStore and writes a Spark job along with an open source entity resolution library called Duke to sort through and score combinations of customer and employee data. SingleStore makes this possible by reducing network overhead through the SingleStore Spark Connector along with native geospatial capabilities. John finds the top 10 million flagged customer and employee pairs across 5 trillion possible combinations in only three minutes. Finally, John uses SingleStore Pipelines and TensorFlow to write a machine learning Python script that accurately identifies thousands of handwritten numbers after training the model in seconds. Get the SingleStore Spark Connector Guide The 79 page guide covers how to design, build, and deploy Spark applications using the SingleStore Spark Connector. Inside, you will find code samples to help you get started and performance recommendations for your production-ready Apache Spark and SingleStore implementations. Download Watch the Video Recording: About the Speaker John Bowler, is a Software Engineer at SingleStore. John has a background in machine learning, algorithms, and distributed data warehouses. John is a graduate of MIT who previously interned at SpaceX where he helped write control algorithms for the SuperDraco rocket engine.", "date": "2017-05-03"},
{"website": "Single-Store", "title": "the-analytics-race-amongst-the-worlds-largest-companies", "author": ["Gary Orenstein"], "link": "https://www.singlestore.com/blog/the-analytics-race-amongst-the-worlds-largest-companies/", "abstract": "The Analytics Race Amongst The World’s Most Valuable Companies Data is fueling the world’s most valuable companies. Today the list is topped by Apple, Google, Microsoft, Amazon, and Facebook. These top companies harness data to drive outsized value. While the companies are unique, they share a more common approach to analytics than you might expect. The Rapid Rise of Data Capture for Analytics In a short span, entire industries have been born that didn’t exist previously. Each of these areas is supported by one or more of the world’s largest companies App stores from Apple and Google Online music, video, and books books Apple, Google, and Amazon Seller marketplaces from Amazon.com Social networks from Facebook These areas have common characteristics driving the data workloads – Incredibly large end user bases numbering hundreds of millions – A smaller (but still large) base of creators or sellers The platform providers (Apple, Google, Amazon, Facebook) seek analytics for – Themselves – the content producers or sellers – and often all the way to the end users All of these characteristics culminate in a stack that starts with the platform provider, extends up to the creators or sellers, and ends with consumers. At each level, there is a unique analytics requirement. The App Store Example Let’s use the App Store example to explore analytics architectures across this type of stack. App Stores are also an ideal example of new workloads that require a fresh approach to data engineering. App Store Characteristics The largest App Stores have the following characteristics 100s of millions of end users Millions of application developers Dozens of app segments One primary platform provide (Apple, Google) App Stores also represent a large, fast-growing segment of the economy. According to a recent article in the San Francisco Chronicle based on data from analytics firm App Annie, both Apple and Google are growing, with Android taking a recent lead. This year, things are changing: Android app distributors will leap ahead of the App Store, according to projections by analytics firm App Annie. In 2017, the App Store will generate $40 billion in revenue, while Android app stores run by Google and other parties will generate $41 billion, App Annie said. That gap is expected to widen in 2021, with Android app stores generating $78 billion and Apple’s store at $60 billion, according to the analytics firm’s report, which was released on Wednesday. App Store Revenue and Projections Data Workloads from App Stores App Store workloads produce and collect information on 1) The distribution of apps to end users 2) App data coming from each app from each end user – Transactional data – Log data Desired Data Engineering Capabilities To meet the needs for comprehensive, and multilevel App Store analytics, data solutions need to provide Fast data capture Including the ability to ingest data in real-time Low latency query capability To support sophisticated queries with sub-second responses High concurrency Enabling many users to access the system simultaneously without slowdown Desired Analytical Capabilities To serve all levels of requirements, App Stores (and many other areas with similar characteristics) need to deliver Analytics for the platform – Real-time analytics to understand operationally what is happening at any moment – Ad hoc analytics for impromptu drill downs on specific queries Analytics for app developers – This includes ad hoc queries, so developers can segment the data any way they want – With traditional solutions, serving many groups of analytics users often required pre-computing results. But this negated the option for ad hoc analytics Analytics for end users – Responsive, lightweight analytics for hundreds of millions of users, such as what apps are installed and up to date Analytics Architecture Strategies For App Stores, or any other large data-driven business, the following goals and implementation approaches can make analytics at scale easier to achieve. Goals Multilevel A multilevel approach provides analytics across the platform, developers, and end consumers. Using the appropriate indexing and sharding approaches, the platform provider can architect a solution to meet the needs of all three constituents Self-service Empowering self-service analytics ensures that results are instant and up-to-date without the cost and complexity of pre-computing Implementation recommendations Use a scale-out distributed system A distributed system can support both the speed and volume required for large scale analytics. Further the right indexing and sharding allows for queries to be segmented appropriately. For example, if thousands of developers are each issuing queries about data regarding their own applications, those queries can be directed to data partitions specific to those developers, and not the entirety of the distributed system. This approach allows a high degree of concurrent access. Ensure a modern query execution system Newer systems include features such as – Code compilation to facilitate sub-second responses on repetitive queries – Distributed joins for efficient operations across multiple tables – Vectorization to take advantage of the latest CPU capabilities such as Single Instruction Multiple Data (SIMD) – Bundle transactions support for enable real-time analytics In a real-time world, there is no time to wait for lengthy Extract, Transform, and Load processes. Using a system that supports transactions as well as analytics allows data to be analyzed in place. Get Started Today If you are interested in building your own multilevel analytics solution visit www.singlestore.com", "date": "2017-05-02"},
{"website": "Single-Store", "title": "durable-storage-for-real-time-analytics", "author": ["Bryan Offutt"], "link": "https://www.singlestore.com/blog/durable-storage-for-real-time-analytics/", "abstract": "Apache Spark has made a name for itself as a powerful data processing engine for transforming large datasets in a swift, distributed manner. After using Spark to complete such transformations, you often want to store your data in a persistent and efficient format for long-term access. The common solution of storing data in HDFS solves the issue of persistence, but suffers efficiency issues as a result of the HDFS disk-based architecture. The SingleStore Spark Connector solves both of these issues by providing an easy to use Spark API for reading from, writing to, and performing distributed computations with SingleStore. Using the connector, we leverage the computational power of Spark in tandem with the speedy data ingest and durable storage benefits of SingleStore. Spark Connector Architecture The SingleStore Spark Connector empowers users to achieve real-time data ingestion by connecting Spark workers directly with SingleStore partitions. This allows Spark to read and write from SingleStore in parallel, improving write performance. Utilizing this architecture Spark reads from SingleStore, allowing data to be imported from your SingleStore tables into your Spark job at lightning fast speeds. Once read in, you make use of Spark’s built in Graph and Machine learning libraries to perform additional computations on data persisted in SingleStore. The SingleStore Spark Connector also supports “SQL Pushdown”, a feature that runs Spark SQL queries directly in SingleStore for more efficient calculations on large data sets. This translation is automatic, and requires no additional work or changes to existing Spark SQL commands. As an example, ss.read.format(\"com.memsql.spark.connector\").options(Map( \"path\" -> \"db.table\")).load().filter(\"id > 2\") will automatically be pushed down and run as SELECT * FROM db.table WHERE id > 2 in SingleStore. API Basics The SingleStore Spark connector is simple and lightweight, delivering powerful results with minimal effort. The API has three basic concepts: SingleStoreContext.sql(sql_statement) : A method for preparing SQL statements to be run in SingleStore. SingleStoreContext.sql(sql_statement).collect() : A method for executing these SQL statements in SingleStore. df.saveToSingleStore() : A method for persisting a Spark DataFrame to SingleStore. To learn more about the SingleStore Spark connector, please see https://github.com/memsql/memsql-spark-connector If you’d like to implement a similar spark connection in SingleStore feel free to setup a time: singlestore.com/demo Get The SingleStore Spark Connector Guide The 79 page guide covers how to design, build, and deploy Spark applications using the SingleStore Spark Connector. Inside, you will find code samples to help you get started and performance recommendations for your production-ready Apache Spark and SingleStore implementations. Download Here", "date": "2017-05-11"},
{"website": "Single-Store", "title": "database-multi-tenancy-in-the-cloud-and-beyond", "author": ["Alec Powell"], "link": "https://www.singlestore.com/blog/database-multi-tenancy-in-the-cloud-and-beyond/", "abstract": "In today’s wave of Enterprise Cloud applications, having trust in a data store behind your software-as-a-service (SaaS) application is a must. Thus, multi-tenancy support is a critical feature for any enterprise-grade database. This blog post will cover the ways to implement multi-tenancy, and best practices for doing so in SingleStore. As customer table sizes grow, you will need to scale out your multi-tenant database across dozens of machines. To support rich analytics about your customers or as a feature for your end customers, you will want a solution that scales without bogging down reporting capabilities. Successful multi-tenant platforms require massive scalability, online patching/upgrading, the ability to process high volumes of data ingestion, and deliver high performing complex analytics. SingleStore delivers the requirements for a multi-tenant platform through the rich analytical and transactions capabilities of SQL. SingleStore also scales to hundreds of nodes in a cluster leveraging shared nothing commodity hardware. Combining these multi-tenant capabilities with extreme data ingestion makes SingleStore a truly unique product to deliver SaaS data. Most database architectural patterns for multi-tenant applications follow one of three approaches: Separated Database Separate Schema Shared Schema If isolation requirements are not accounted for early in application development, retrofitting them can be even more costly. In our conversations with customers, scale is often the driver. Let’s delve into the three models: 1) Separated Database Instantiating each tenant as its own database in SingleStore is a practical idea for data isolation guarantees. However, if you plan to scale past 50-100 tenants, spinning up a new database instance for each tenant will increase maintenance cost. While former SaaS applications make good use of separate database instances, scaling a software business in the cloud necessitates alternative solutions. With the database-per-tenant model, there are costs related to increased cloud resource sharing as well as maintaining and managing many databases. For example, you would probably need different backup strategies per tenant. SaaS application developers often struggle when making such trade-offs. Lastly, performing analytics across your customer base would result in relatively complex multi-database queries. 2) Separate Schema Keeping your data isolated logically, but in different table schemas, can be a beneficial strategy. With the “separate schema” approach, your tenant’s’ data is just as separated logically, but within the same database for less maintenance headaches or overhead. Backups are done in full, for all tenants at once. Role-Based Access Control (RBAC) can be used to keep your tenants’ data protected. See an example below of giving user permissions to tables by named TENANTID: CREATE DATABASE app;\n\nCREATE TABLE <TENANTID>.table1;\n\n CREATE ROLE 'app_<TENANTID>_schema_role';\n\nGRANT SELECT, INSERT, UPDATE, DELETE on <TENANTID> to ROLE 'app_<TENANTID>_schema_role';\n\nGRANT SHOW VIEW on <TENANTID> to ROLE 'app_<TENANTID>_schema_role';\n\nGRANT CREATE TEMPORARY TABLES on <TENANTID> to ROLE 'app_<TENANTID>_schema_role';\n\nCREATE GROUP '<TENANTID>_users';\n\nGRANT ROLE 'app_<TENANTID>_schema_role' to '<TENANTID>_users';\n\nGRANT USAGE on <TENANTID> to 'user_1' IDENTIFIED BY 'abc';\n\nGRANT GROUP '<TENANTID>_users' to 'user_1'; This example can be extended to any number of users or user groups for any tenant. Taking a Separate Schema approach has additional benefits. Since everything is inside the same logical database, hardware resources are equally shared among tenants, saving costs. Table schemas used can be different per customer, which allows you to customize a specific tenant without affecting others. 3) Shared Schema In the Shared Schema model, everything is shared – server, database, and tables. All data for your tenants are within the same table(s) in one logical database. A “TenantId” column can be used on the table to differentiate each row of data, and you can use this as your shard key in SingleStore to ensure that all data for a given tenant lies on the same physical machine. In addition to RBAC, SingleStore allows you to architect for this model with Row-Level Security (RLS) . Presuming that all data for all tenants is stored in one table, user roles are created for each tenant’s subset of the data as such: UPDATE <table> SET ACCESS_ROLES=CONCAT(ACCESS_ROLES, \"ROLE,\") WHERE ID=<TENANTID>; Given the instantiated tenant roles above, you can implement customer views. CREATE VIEW <view_name> AS SELECT COLUMNS FROM <table> WHERE SECURITY_LISTS_INTERSECT(CURRENT_ROLES(), ACCESS_ROLES); Another big benefit is code changes: with this model, schema changes become painless as you only have one spot to change code (i.e. table structure). With the other options you would have to roll out code changes to many spots. Finally, you may wish to create reports to aggregate tenant data. In the “Separated Database” model, global reporting of customer data becomes onerous.100 customers in 100 unique databases each with a clickstream table, for example, means you would have to do a 100 table cross-database union all join to provide global reporting of all customers. In practice, you would have to build an extraction process to a data warehouse to consolidate, but now there’s significant ETL to contend with. You could roll up per customer, but now you still have to send the aggregate data to one place and trust it’s accurate as well as recent. With the “Shared Schema” or “Separate Schema” models this is an achievable task. Starting with SingleStore DB 6.0, you can create a stored procedure to calculate and rollup data for each tenant (or even across tenants). See www.singlestore.com/managed-service/ for more information. SingleStore delivers what multitenant platforms require: massive scalability, online patching/upgrading, the ability to process high volumes of data ingestion and deliver high performing complex analytics on rapidly changing data. As more SaaS application providers push for scalable data warehouse solutions in the cloud, isolation of data and cost of storage are paramount to a multi-tenant environment. There are various ways to design for multi-tenancy, and there are always tradeoffs to be made for each case. With its robust security functionality and distributed scale-out performance, SingleStore can fit any model you choose. Give it a try by visiting www.singlestore.com/free .", "date": "2017-05-09"},
{"website": "Single-Store", "title": "real-time-data-warehousing-for-the-real-time-economy", "author": ["Seth Luersen"], "link": "https://www.singlestore.com/blog/real-time-data-warehousing-for-the-real-time-economy/", "abstract": "In the age of manual decision making based on predictable data formats, data feeds, and batch processing times, enterprise businesses stayed current with ad hoc analyses and periodic reports. To generate analyses and reports, businesses relied on the traditional data warehouse. Using extraction, transformation, and load batch processes, the traditional data warehouse standardized disparate data into normalized schemas and pre-computed cubes. With the data shaped into pre-configured dimensions and aggregated facts, enterprises made historical data and analyses available to front-line decision makers. In today’s age of digital business, machine learning, and artificial intelligence, decision making for companies is now very different. It is based on unpredictable data formats, massive data scale, event messaging, stream processing, models training, historical analyses, predicitve analytics, and real-time dashboards. The Enterprise Shift to Real-Time Today’s real-time economy is segmented, scored, personalized, appified, monetized, and data fueled. Aligning today’s enterprise business with the on-demand, digital economy requires real-time data storage and analytics. “Companies like Pinterest, Uber, and Pandora have achieved significant performance advantages by shifting from a batch process to a continual load of data.” –  > Eric Frenkiel, SingleStore co-founder and CEO The real-time data warehouse embraces perpetual data ingest, simultaneous reads, high user concurrency, and fast queries. This new data warehouse continually loads and transforms data, one transaction per event or message with exactly-once semantics.  The real-time data warehouse is the foundation for today’s enterprise business that is synchronized with the market and its profit-making opportunities. The Real-Time Enterprise Gets Its Data Warehouse In a SingleStore data warehouse, where there is continual data ingest at massive scale, disparate business data is made whole for the various applications running on top of the data warehouse. “For many enterprises today, moving to real time is the next logical step, and for some others it’s the only option.” –  > Nikita Shamgunov, SingleStore co-founder and CTO. Data analysts run real-time ad-hoc analyses in sub-seconds.  Data scientists train and evaluate machine learning models within minutes. Apps predicatively score and deliver personalized experiences. Armed with a customer 360 degree view of critical business metrics, front-line decision makers automate actions that drive value. Where Real-Time Data Warehousing Matters In today’s market, enterprises need the flexibility to migrate, expand, and burst their storage and compute capacity both on-premises and in the cloud. “SingleStore is the most flexible data warehouse in the market today.” –  > Eric Frenkiel, SingleStore co-founder and CEO With SingleStore, businesses push real-time workloads where it is the most economical to run them. For companies succeeding in the real-time economy, SingleStore is the hybrid cloud data warehouse that offers these critical operations and cost-savings advantages. Learn more about how and where real-time data warehousing matters from SingleStore co-founders, Eric Frenkiel (CEO) and Nikita Shamgunov (CTO), in this insightful video.", "date": "2017-05-17"},
{"website": "Single-Store", "title": "seeking-a-rescue-from-a-traditional-rdbms", "author": ["Gene Miguel"], "link": "https://www.singlestore.com/blog/seeking-a-rescue-from-a-traditional-rdbms/", "abstract": "In the Beginning Years ago, organizations used transactional databases to run analytics. Database administrators struggled to set up and maintain OLAP cubes or tune report queries. Monthly reporting cycles would slow or impact application performance because all the data was in one system. The introduction of custom hardware, appliance-based solutions helped mitigate these issues, and the resulting solutions were transactional databases with column store engines that were fast. Stemming from these changes, several data warehouse solutions sprang up from Oracle, IBM Netezza, Microsoft, SAP, Teradata, and HP Vertica, but these data warehouses were designed for the requirements of 20 years ago. Thus new challenges arose, including: Ease of use – each environment required specialty services to setup, configure, and tune Expensive – initial investments were high and needed additional capacity Scalability – performance was designed on single box configurations; the larger the box, the faster the data warehouse Batch ingestion – inability to store and analyze streaming data in real-time As new data or user requests landed on the system, database administrators (DBA) had to scale the system up from a hardware perspective. Need more scale? Buy more boxes! DBAs became tired of having to buy new expensive hardware every time their query was slow or every time they had to ingest from a new data source. An Explosion of Data The data warehouse appliance was passable back then, but today, new workloads and data growth have put a strain on traditional solutions to the point where many users are seeking rescue from the clutches of incumbent systems. Explosive data growth due to web and mobile application interactions, customer data, machine sensors, video telemetry, and cheap storage means customers are storing “everything,” this has contributed to the additional strain on traditional systems. Real-time application data, now pervasive in digital business, along with new machine and user generated data puts an increasing pressure on ingestion and query performance requirements. Real-world examples include digital personalization required of retailers, customer 360 programs , real-time IoT applications , and real-time logistic applications. To solve for the increased strain, there has been a strategic shift to cloud and distributed systems for agility and cost optimization. There Is a Way Out Each year, more data is originating in the cloud, in addition to traditional on-premises data forcing companies to make a hybrid cloud shift so they can run analytics anywhere data lives. This is where SingleStore comes to the rescue. SingleStore is a real-time data warehouse optimized for hybrid cloud deployments and excels at operational use cases. SingleStore users are drawn to the ability to load streaming data at petabyte scale while simultaneously querying that data in real-time. The distributed architecture scales for new user and data volumes with efficient, easy-to-add industry standard hardware nodes. On-premises SingleStore software can be installed and managed by the enterprise. SingleStore supports deployment on any cloud Infrastructure as a Service, and SingleStore also has a managed service called SingleStore Cloud. There are many reasons why SingleStore can rescue you from your dated system: Perform faster queries with scalable high-performance SQL Reduced maintenance costs and hardware savings Run anywhere with flexible software footprint across clouds and hardware types Analyze all your data including stream and batch formats Scalable distributed architecture for growing data and users Enterprise-grade security for assurance across on-premises and cloud deployments Get Started To get started with SingleStore, please visit www.singlestore.com", "date": "2017-05-18"},
{"website": "Single-Store", "title": "arrays-a-hidden-gem-in-memsql", "author": ["Eric Hanson"], "link": "https://www.singlestore.com/blog/arrays-a-hidden-gem-in-memsql/", "abstract": "Released this March, SingleStore DB 6 Beta 1 introduced SingleStore Procedural SQL (MPSQL). MPSQL supports the creation of: User-Defined Functions (UDFs) Stored Procedures (SPs) Table-Valued Functions (TVFs) User-Defined Aggregate Functions (UDAFs) A Hidden Gem: Array Types There’s a hidden gem in SingleStore DB 6 Beta 1 that we didn’t document at first — array types!  These make programming much more convenient. Since we compile your extensions to machine code, the performance is fantastic. And you don’t have to leave the comfort of your database language to get it. To declare and initialize a new array of 10 integers, you simply say declare a array(int) = create_array(10); You might we wondering, how does SingleStore know what kind of array to create when you call the create_array() function?  The secret is type inference. Our compiler knows the data type of the variable you’re assigning the result of create_array() to. It uses that to determine the data type of the array. Our arrays are zero-based, meaning the positions of the array of 10 integers above are numbered 0..9. Real Examples Let’s try some real examples. Here’s a function to create an array of size n containing a pseudo-random sequence of numbers: delimiter //\ncreate function random_array(n bigint)\n  returns array(bigint not null) as\ndeclare\n      result array(bigint not null) = create_array(n);\n      a bigint not null = 573829473;\n      b bigint not null = 837562837;\n      m bigint not null = 100000000;\n      crnt bigint not null = 17;\nbegin\n      for i in 0 .. (n - 1) loop\n          crnt *= a;\n          crnt += b;\n          crnt %= m;\n          result[i] = crnt;\n      end loop;\n      return result;\nend //\ndelimiter ; Suppose you want to display the array — you’ll need to convert it to text: delimiter //\ncreate function to_text(a array(bigint not null))\n  returns longtext as\ndeclare\n        result longtext = \"[\";\n        comma bool = false;\nbegin\n    for i in 0 .. (length(a) - 1) loop\n            if comma then\n                  result = concat(result, \", \");\n            end if;\n            comma = true;\n            result = concat(result, a[i]);\n    end loop;\n    return concat(result, \"]\");\nend //\ndelimiter ; Let’s put it all together: memsql> select to_text(random_array(5));\n+----------------------------------------------------+\n| to_text(random_array(5)) |\n+----------------------------------------------------+\n| [92663878, 16439131, 15870800, 37651237, 23070938] |\n+----------------------------------------------------+\n1 row in set (0.37 sec) SingleStore dynamically compiles code in functions. The first time (or sometimes first few times) that you run them, they can take a little longer to run due to compilation. Hence the 0.37 sec time above. Let’s see what happens if we run it again: memsql> select to_text(random_array(5));\n+----------------------------------------------------+\n| to_text(random_array(5)) |\n+----------------------------------------------------+\n| [92663878, 16439131, 15870800, 37651237, 23070938] |\n+----------------------------------------------------+\n1 row in set (0.00 sec) So, after we compile the function, it runs in milliseconds! Speaking of performance, in our tests, a merge-sort UDF written in MPSQL can sort an array of 200,000 numbers in 0.2 seconds. Try doing that in another database language! Multidimensional Arrays You can declare multidimensional arrays too, like so: declare b array(array(int)); Here’s an example function that creates a two-dimensional array and sums the contents of all the elements:> delimiter //\ncreate or replace function md_array_sum() returns int as\ndeclare\n      b array(array(int));\n      s int = 0;\nbegin\n      b = [[0, 100], [100, 0]];\n      for i in 0 .. length(b) - 1 loop\n        for j in 0 .. length(b[i]) - 1 loop\n            s += b[i][j];\n        end loop;\n      end loop;\n      return s;\nend //\ndelimiter ; Notice that it creates an array literal [ [ 0, 100 ] , [ 100, 0 ] ] and assigns it to b. This is another way to make an array, as an alternative to create_array(). Our multi-dimensional arrays are called “ragged arrays” because individual sub-arrays can contain different numbers of elements, although we didn’t do that in this example. Let’s take it for a spin: memsql> select md_array_sum();\n+----------------+\n| md_array_sum() |\n+----------------+\n|      200       |\n+----------------+\n1 row in set (0.00 sec) Try SingleStore DB 6 Beta Arrays are an amazing addition to SingleStore. You can learn more about arrays and SingleStore Procedural SQL in our online documentation . Download and try SingleStore DB 6 Beta 1 today . Enjoy!", "date": "2017-05-16"},
{"website": "Single-Store", "title": "partners-empower-real-time-data-warehousing", "author": ["Sandeep Brahmarouthu"], "link": "https://www.singlestore.com/blog/partners-empower-real-time-data-warehousing/", "abstract": "Leading companies such as Comcast, Akamai, Kellogg’s, Dell EMC, Pinterest, Samsung, and Pandora use SingleStore to drive value from data with real-time data warehousing and analytics. For many enterprises, however, the rapid change in the digital business landscape proves daunting. Implementing a real-time analytics and data warehousing solution seems far-fetched, if not impossible. Who can help companies drive business value from their evolving data ecosystem and become a real-time enterprise? ‘At Twingo, a leading Big Data & BI analytic company from Israel, we make sure to carefully select the best technologies in the world. Since SingleStore is a leader in the real-time analytics space with its innovative approach to handling simultaneous data ingest and analysis, we are proud to work with them to support our customers such as Appsflyer, Matomy.’ -Golan Nahum, Twingo , SingleStore Partner for real-time analytics Our partners, such as Twingo, use their expertise to design, build, embed, and implement real-time enterprise solutions for businesses around the globe in every industry sector. Twingo introduced SingleStore to AppsFlyer, a SaaS mobile marketing analytics and attribution platform based out of Israel. AppsFlyer struggled with handling billions of daily events in real-time. Druid, MongoDB, Redis, Cassandra, and Amazon Redshift did not scale in terms of performance and cost. Twingo’s new SingleStore solution allowed AppsFlyer to aggregate 10 times more data for less cost and with faster responses times. Becoming a SingleStore Partner Created to enable organizations moving towards real-time operations, the SingleStore Partner Program provides our partners with the resources, expertise, and technical support they require to thrive in the data-driven, digital economy. Currently, SingleStore partners with industry innovators such as Tableau Software, Looker, Zoomdata, Microsoft, Cisco, Informatica, Amazon Web Services, and many others. ‘Enterprises today are looking for innovative ways to harness data to get a competitive edge. SingleStore works with its partners to take advantage of this enormous opportunity to digitally transform industries by capturing and analyzing real-time data to drive change. We believe a strong partner ecosystem is critical in making this promise a reality.’ -Eric Frenkiel, CEO and Co-founder, SingleStore. Our goal is to nurture a supportive partnership that enables our partners to distinguish their business and advance their offerings. Together, we deliver better business outcomes to existing customers, expand our market reach, and connect with new opportunities. ‘ Enterprises are demanding fast visual analytics on real-time streaming data for all business users. Working with SingleStore, and utilizing its real-time data warehouse allows our joint customers to visualize fast-changing data in Zoomdata’s business intelligence solution.’ -Russ Cosentino, Co-founder and VP of Channels, Zoomdata. Our Partner Benefits We enable partners to bring innovative real-time data warehousing solutions to enterprises quickly worldwide. Becoming a partner includes these and more benefits: Best practices in implementing real-time data warehouse solutions Sales and technical enablement material including content, use cases, solutions, demos Best discount pricing for SingleStore Quick Start Training and Certification Deal registration for both referral and resale partners Partner Engineering support on integration Partner product evaluation licenses Partnership Types Our Partner Program offers both Member and Premier status levels. OEM Partners embed SingleStore into their real-time applications and products. System Integrators and Consulting Partners play a critical role in developing and implementing solutions and services to deliver scalable real-time data platforms for their clients using SingleStore. Technology Partners (ISV, IHV, and Cloud) integrate their platforms and applications with SingleStore. Value Added Resellers combine data management expertise with solution selling to drive digital transformations with SingleStore. Ready for a real-time business partnership?  Join our partner program today at www.singlestore.com/partners/", "date": "2017-05-24"},
{"website": "Single-Store", "title": "memsql-stands-with-our-environment", "author": ["Eric Frenkiel"], "link": "https://www.singlestore.com/blog/memsql-stands-with-our-environment/", "abstract": "Yesterday, the world reacted to a move by the U.S. President to withdraw from the Paris Agreement. Almost immediately, voices across America responded with a resounding commitment to maintain alignment with the Paris climate agreement recommendations. With the energy sector at the center of this conversation, the industry needs more help from technology companies than ever before. Today, the ability to see operations in real time, and have an immediate view on the state of their businesses, is critical in the effective use of energy. With the support of technology companies, leading energy providers are now able to reduce carbon emissions via smart meters in households, improve the efficiency of environmentally responsible production, and view metrics that help optimize mix fuels such as natural gas, which have some of the lowest carbon emissions of any fossil fuel on a per-unit basis. There are many people and companies across the world looking to improve our global environmental standing, and we know there is a long way to go. SingleStore salutes you and stands with you to protect our environment. Eric Frenkiel and Nikita Shamgunov, SingleStore Founders", "date": "2017-06-02"},
{"website": "Single-Store", "title": "2017-spark-summit-5-machine-learning-talks", "author": ["Nicole Nearhood"], "link": "https://www.singlestore.com/blog/2017-spark-summit-5-machine-learning-talks/", "abstract": "Spark Summit 2017 kicks off in less than two weeks with a program that includes more than 175 talks led by top experts in the Apache Spark ecosystem. From developer tutorials and research demos to real-world case studies and data science applications, these 5 sessions will take your machine learning skills to the next level. 5 Machine Learning talks to check out at Spark Summit 2017: Apache Spark MLlib’s Past Trajectory and New Directions \\\n(Joseph Bradley, Databricks) – This talk discusses the trajectory of MLlib, the Machine Learning (ML) library for Apache Spark. Review the history of the project, including major trends and efforts leading up to today. Embracing a Taxonomy of Types to Simplify Machine Learning \\\n(Leah McGuire, Salesforce) – Salesforce has created a machine learning framework on top of Spark ML that builds personalized models for businesses across a range of applications. Extending Spark Machine Learning: Adding Your Own Algorithms and Tools \\\n(Holden Karau & Seth Hendrickson, IBM) – Apache Spark’s machine learning (ML) pipelines provide a lot of power, but sometimes the tools you need for your specific problem aren’t available yet. This talk introduces Spark’s ML pipelines, and looks at how to extend them with your own custom algorithms. How Apache Spark and AI Powers UberEATS \\\n(Chen Jin & Xian Xing Zhang, Uber) – The overall relevance and health of the UberEATS marketplace is critical in order to make and maintain it as an everyday product for Uber’s users. Chen and Xian Xian explain their implementation of Apache Spark and AI to increase user retention. Real-Time Image Recognition with Apache Spark \\\n(Nikita Shamgunov, SingleStore) – Nikita’s session will examine the image recognition techniques available with Apache Spark, and how to put those techniques into production Don’t miss the chance to see a live demo at the SingleStore kiosk! Visit us at kiosk #7 in the Moscone West expo hall or book a demo here . Get The SingleStore Spark Connector Guide The 79 page guide covers how to design, build, and deploy Spark applications using the SingleStore Spark Connector. Inside, you will find code samples to help you get started and performance recommendations for your production-ready Apache Spark and SingleStore implementations.\\ Download Here", "date": "2017-05-30"},
{"website": "Single-Store", "title": "cisco-real-time-data-warehouse", "author": ["Kevin White"], "link": "https://www.singlestore.com/blog/cisco-real-time-data-warehouse/", "abstract": "Real-time analytics on live data and the ability to analyze data at scale, are key for any digital organization. Acting on insights in the moment helps deliver contextual user experiences, identify new sources of revenue, and prevent costly expenditures. To become a responsive data-driven business, organizations must address current data latency challenges. These challenges are commonly found across three general areas: Three Data Latency Challenges and Solutions Slow data loading: Loading data, moving past batch processing, and receiving analytics responses in real time remains out of reach for too many businesses. A real-time data warehouse reduces extract, transform, and load (ETL), combining transactional and analytical workloads into a single system. Lengthy query execution: Operational insights must be readily available for in-the-moment decisions. A data warehouse that delivers a fast query response can deliver insights whenever the application or users require it, ultimately providing a differentiated service or identifying opportunities. Low concurrency: Digital business assumes a large-scale use of data across an entire business or customer base. As more users engage and interact with data, the response time for those interactions must maintain a consistent experience. A scalable data warehouse will help ensure that data and user growth will not negatively affect the operational system. Real-Time Data Warehouse for the Enterprise SingleStore on Cisco UCS Integrated Infrastructure for Big Data and Analytics is built to address these challenges. SingleStore provides a scalable, real-time data warehouse platform for high-performance applications that require fast, accurate, secure, and always available data. SingleStore scales linearly to millions of events per second while analyzing petabytes of data for insights. SingleStore also enables fast ingestion and concurrent analytics needed for sensor systems, recommendation systems, and use cases that require instant, actionable insights. Learn more about how we are working with Cisco to deliver real-time analytics to the enterprise. Download the full Cisco UCS solution brief here . Check out Raghunath Nambiar’s (Cisco UCS CTO) blog post here: https://blogs.cisco.com/datacenter/memsql", "date": "2017-05-31"},
{"website": "Single-Store", "title": "direct-employers-case-study", "author": ["Mike Boyarski"], "link": "https://www.singlestore.com/blog/direct-employers-case-study/", "abstract": "About DirectEmployers Association DirectEmployers Association is a nonprofit human resources consortium of leading global employers formed to improve labor market efficiency through sharing best practices. The organization helps companies reduce recruiting costs and ensure regulatory compliance of recruitment programs, such as veteran and public disability requirements. The Impact of Data for Corporate Recruiting The DirectEmployers application collects a variety of data across the web to help recruiting managers understand job requisition performance and the fulfillment process. The application relies on up-to-the-minute web interactions, searches, and demographic details. Each data point provides real-time insights to give recruiting managers an added edge in understanding who, where, and when applicants engage with job postings. Faster Analytics with Accelerated Deployment DirectEmployers wanted to improve the performance of its existing analytics solution so customers could quickly adjust or respond to recruiting activities. The raw data needed to be processed and analyzed in aggregate and detail views from several different web sources. The data changed frequently throughout the day, requiring a more up-to-date analytics offering that was fast and responsive to customers. DirectEmployers chose SingleStore Cloud for the fast query response of several ad hoc and operational report queries, along with the minimal tuning and tooling to get the solution live. It also wanted the fully managed service to help accelerate time-to-market and make use of SingleStore expertise. “SingleStore offered a very good implementation of a column store engine and required little tuning and tooling to get running. It’s a polished product that just works.” – Darrin Thompson, Senior Developer, DirectEmployers Results With SingleStore Cloud, DirectEmployers has been able to provide new analytics that were previously not available. For existing reports, query performance improved dramatically, with count and rank queries improving by up to 50x. With these improvements, recruiters now have a more predictable experience and better visibility into their recruiting efforts.", "date": "2017-06-14"},
{"website": "Single-Store", "title": "the-real-time-data-warehouse-for-hybrid-cloud", "author": ["Gary Orenstein"], "link": "https://www.singlestore.com/blog/the-real-time-data-warehouse-for-hybrid-cloud/", "abstract": "As companies move to the cloud, the ability to span on-premises and cloud deployments remains a critical enterprise enabler. In this post, we’ll review the basics of a hybrid cloud model for SingleStore deployments to fit conventional and future needs. SingleStore Background SingleStore is a real-time data warehouse optimized for hybrid cloud deployments, and excels at operational use cases. SingleStore users are drawn to the ability to load and persist live data at petabyte scale while simultaneously querying that data in real-time. SingleStore supports deployment on any cloud including offering a managed service called SingleStore Cloud. Today, nearly half of SingleStore customers use cloud offerings in production or test and development deployments. On-premises SingleStore software can be installed and managed by the enterprise. An Introduction to the SingleStore Hybrid Cloud Model The SingleStore model begins with the fundamental design principle of a flexible software footprint . With this principle, SingleStore can be run on-premises and in the cloud. SingleStore in the Cloud In the cloud, SingleStore offers a managed service where customers can use the database, but do not have to administer the service. This provides any company with the ability to benefit from SingleStore scale and performance, with SingleStore handling cluster operations. When run in the cloud, SingleStore takes advantage of the cloud platform such as low cost scalable storage to store backups and data, and readily available compute to scale query performance and concurrency. SingleStore can also run on any cloud infrastructure-as-a-service. SingleStore On-Premises On-premises SingleStore software runs on any modern Linux operating system. It can run on one or hundreds of nodes. There are no dependencies on any hardware, and no dependencies on any cloud provider. Building on this architectural premise, SingleStore has evolved to have the most advanced database deployment models available. Building on the flexible software footprint, SingleStore can be deployed: on any server, with a minimum of 4 cores and 8GB of RAM on any modern version of Linux within a virtual machine within a container for test and development on any public cloud provider’s infrastructure-as-a-service platform as a cloud service, managed by SingleStore This unparalleled flexibility allows enterprises to make big bets on their analytics infrastructure while simultaneously controlling deployment and cost levers throughout the lifetime of an application. Sample Hybrid Cloud Scenarios With hybrid cloud as an umbrella, SingleStore fits on-premises, hybrid-cloud, and multi-cloud deployments. Let’s look at each in further detail. On-Premises Flexibility Many sensitive workloads require an on-premises deployment which SingleStore easily accommodates. When large amounts of data are produced locally, like for manufacturing analytics, an on-premises model often makes sense. In addition to running on any Linux, SingleStore provides multiple capabilities for on-premises deployment to shine. Any industry standard hardware Architects can choose the CPU, memory, flash, and disk options that suit their needs NUMA Optimizations Scale-up, multi-core servers can be put to full use Advanced Vector Extensions (AVX) optimizations SingleStore takes advantage of the latest Intel and AMD features Any standard Linux For flexibility across environments in large organizations Virtual Machine Ready SingleStore can easily run in a virtual machine with local or remote storage Containers SingleStore can easily in a Docker container, an ideal way to do development and testing. Running stateful systems like databases within containers for production workloads is still an emerging area. Together these deployment characteristics make SingleStore an ideal platform for enterprises looking to streamline data center infrastructure. End of the Appliance In particular for on-premises workloads, many companies find SingleStore to be an ideal replacement for proprietary appliances. These appliances may originate from the database side such as with Oracle Exadata or SAP HANA. They also frequently come from the data warehouse side such as Netezza or Vertica. In all cases, these customers can realize immediate cost savings of millions of dollars by moving to a database platform like SingleStore covering these workloads with a pure software approach. Hybrid Cloud Deployments SingleStore deployments span from on-premises to cloud. That flexibility begins with the simplicity of the replicate database command. Replicate Database SingleStore includes the ability to replicate a database from one SingleStore cluster to another SingleStore cluster with a single command. The two clusters do not have to be the same configuration. However, the receiving cluster must have enough capacity to absorb the data being replicated to it. High Availability and Disaster Recovery in the Cloud SingleStore replicates data locally within a single cluster across multiple nodes for instant high availability, but additional layers of availability, including disaster recovery, could be deployed by replicating the SingleStore database to the cloud. Test and Development Other cloud uses include test and development where data architects can rapidly deploy SingleStore clusters to test a variety of workloads. Experiments can be run to determine the advantages of one instance type over another. Ultimately those deployments can migrate on-premises with much of the heavy lifting already accomplished. Multi Cloud Multi cloud deployments are an extension of hybrid cloud and offer provider flexibility. SingleStore can be deployed across several popular clouds such as Amazon Web Services, Microsoft Azure, Google Compute Platform, Digital Ocean and others. This range of choice leaves companies in an advantageous position to make long-term bets on data infrastructure without being tied to a single provider. Charting An On-Premises to Cloud Security Plan One of the reasons often cited against moving to the cloud is the lack of an appropriate security model. Understandably, companies want to have this model in place before moving to the cloud. SingleStore includes robust security mechanisms such as encryption auditing role based access control data use and database architect separation (to help protect against insider threats) In addition, SingleStore has passed security audits allowing it to retain the most sensitive data at the world’s largest organizations. The SingleStore security model exists in any deployment. Organizations that begin with a comprehensive security model for an on-premises deployment have the option to retain that model as they move to the cloud. Managing SingleStore SingleStore provides a range of management capabilities for easy administration including SingleStore Cloud Managed Service SingleStore offers a managed service where customers can consume the database, but do not have to administer the service. This provides any company with the ability to benefit from SingleStore scale and performance, while SingleStore handles the cluster operations. SingleStore Ops To quickly provision, manage, and monitor SingleStore clusters Cloud formation templates To quickly deploy SingleStore on services such as AWS Fit with configuration management tools SingleStore can easily be deployed using configuration management tools like Puppet or Chef A Comprehensive Approach for Hybrid Cloud Deployments SingleStore remains the only scalable, real-time data warehouse to provide for all deployment models on-premises with complete hardware and virtual machine flexibility cloud deployments on any provider’s infrastructure-as-a-service a cloud service management by SingleStore With large companies spending millions to hundreds of millions on database and data warehouse infrastructure, the opportunities to consolidate to a single platform deployed multiple times for multiple use cases is financially compelling. Start Your Hybrid Cloud Deployment Today To get started with SingleStore, please visit www.singlestore.com", "date": "2017-06-16"},
{"website": "Single-Store", "title": "ai-secret-weapon-the-data-corpus", "author": ["Gary Orenstein"], "link": "https://www.singlestore.com/blog/ai-secret-weapon-the-data-corpus/", "abstract": "As industries latch on to the rise of machine learning and artificial intelligence, we see firsthand that the key to success is often the data itself. While discussions on the latest algorithms and learning models capture mainstream attention, the real determining factor of a company’s success is its ability to leverage a corpus of data. Recently our CEO Eric Frenkiel delivered a talk on this topic, Machines and the Magic of Fast Learning. The premise was simple, and inspired by a discussion with Alistair Croll , chair of O’Reilly’s Strata + Hadoop World conference, that collecting and managing a large corpus of data leads to definitive business benefits. To understand the implications of the corpus, consider that computing origins started with actors, or more specifically, applications. These applications initially generated moderate amounts of data, and a fixed set of interactions between the data and application. With applications and devices driving larger volumes of data, we added operators to apply data science to enhance experiences across everything from enterprise software to mobile apps. When actors and operators are brought together, real-time machine learning can be applied to drive new knowledge back into the business. But the real magic takes place when a feedback loop is developed to enrich the experience. Rise of the Data Corpus The data corpus theme stretches to the world’s largest companies. Here the rapid rise of mass data capture quickly shifts into rich analytics. Some examples of data collected to drive multibillion dollar industries include: App stores from Apple and Google Online music, video, and books books Apple, Google, and Amazon Seller marketplaces from Amazon.com Social networks from Facebook Apple specifically recently stated that during the life of the App Store, it has paid developers of $70 billion to date. With so much at stake, these companies use similar approaches to drive their businesses. We took a closer look at this phenomenon in a post on the analytics race amongst the world’s most valuable companies. We concluded that data is fueling their success. The data corpus also represents a strategic lever for image-focused business models. In a New Yorker article , What’s Wrong With Twitter’s Live-Video Strategy, Om Malik discussed the importance of assembling a large volume of face-recognized photos. …but there is an even bigger potential payoff. A large corpus of images is needed to train computer vision algorithms to distinguish between cats, dogs, and houseplants. The better its technology is at identifying the information inside the photos, the more opportunities the company will have to target advertising at specific users. A photo of your baby might offer an opportunity to place an advertisement for diapers or baby-food formula. Additionally, prominent industry data scientists like Peter Skomoroch have made the corpus a focal point, albeit with a dash of humor. Across all industries, the data corpus, not the algorithms, will be the secret weapon for machine learning and artificial intelligence. We have already seen this model create massive value with some of the companies mentioned earlier. The current question is which new businesses will emerge to create the next great data corpus.", "date": "2017-06-06"},
{"website": "Single-Store", "title": "image-recognition-at-the-speed-of-memory-bandwidth", "author": ["Michael Harris"], "link": "https://www.singlestore.com/blog/image-recognition-at-the-speed-of-memory-bandwidth/", "abstract": "SingleStore is a real-time data warehouse and a perfect system for large scale operational analytics. SingleStore provides millisecond response times for analytical queries and is a part of the critical path for real-time applications. We often hear from our customers that they want to do various types of artificial intelligence (AI) and machine learning (ML) model evaluations for IoT data, as well as imagery, in real time. A good example of this is when you need to find similar images in a large corpus of image data. For instance when you point a camera at a person and are quickly able to determine if that person is in a database. This is what is referred to as real-time facial recognition. From Images to Feature Vectors Facial recognition is a subject of ongoing research to efficiently extract feature vectors from images using deep learning. Here is a reference to a modern approach: http://www.robots.ox.ac.uk/~vgg/software/vgg _ face/ . For the purpose of this post, we will assume that this is a somewhat solved problem and we can efficiently extract feature vectors from any incoming image. Once those feature vectors are produced, all you need to do is insert them into a SingleStore table with the following simple schema. CREATE TABLE features ( id bigint(11) NOT NULL AUTO _ INCREMENT, feature _ vector binary(4096) DEFAULT NULL, KEY id (id) USING CLUSTERED COLUMNSTORE ) A typical way to insert the vectors is to use Apache Spark , which enables quick parallel data transfer into SingleStore. Similarity Search There are two frequently used approaches to measuring the similarity between vectors: cosine similarity (cosine of the angle between the vectors) and Euclidean distance. Cosine similarity is defined as the dot product of the vectors, divided by the product of the vector norms (length of the vectors). If the vectors are normalized, the cosine similarity is simply the dot product of the vectors (since the product of the norms is 1). (Yes, SingleStore is the database that does dot product and cosine similarity – the term one of our customers used in the Google search that ended with their using SingleStore for a large deployment.) To search using cosine similarity we can simply run this query to find similar images. SELECT id FROM features WHERE DOT _ PRODUCT(feature _ vector, <Input > ) > 0.9 Input is a feature vector extracted from an incoming image, and 0.9 is a constant that was experimentally tuned, which corresponds to an angle of less than 26 degrees between the feature vector and the input. Euclidean distance is also frequently used to measure similarity. It is defined as the norm of the vector resulting from the subtraction of two input vectors. The EUCLIDEAN _ DISTANCE built-in can also be used to efficiently measure the similarity between vectors. This query performs a full table scan, which seems like it might be slow, but we will share our approach to perform this computation at memory bandwidth speed. Performance Here is our set of assumptions: Memory Speed: 50GB/sec Each image feature vector contains 1024 features, resulting in 4KB/vector So, if we are limited by memory bandwidth, that means we can search 12.5 million images a second per node or 1 billion images a second on a 100 node cluster . Let’s verify that’s actually true. I developed a simple test by creating a SingleStore columnstore table with the schema above and populated it with 12.5 million random 4KB normalized feature vectors. The machine I used has a 6-core Xeon E5 processor. When I ran the search query, I got a 0.25 second response time. How can SingleStore run this faster than memory bandwidth? The answer is compression of columnstore tables. Because the random vectors were normalized, they were able to be compressed from 50GB down to a size that can be read from memory in less than 0.25 seconds. This shows that the DOT _ PRODUCT computation can be done faster than 50GB/sec, and if no compression is applied, memory bandwidth is the limiting factor. SingleStore uses a fast vectorized table scan leveraging Intel’s latest instruction sets: AVX2 and AVX512 . SingleStore also uses these instruction set extensions to compute DOT _ PRODUCT itself. Conclusion Because you can perform image recognition at in-memory speed, your bottleneck for similarity computation is not necessarily compute. We realize that there are other algorithms that gain efficiency by avoiding the full table scan and only lose a small amount of accuracy. However, you can achieve good practical results with a very straightforward implementation. Future Work Currently, we are adding more primitives to enable more machine learning use cases. We are also exploring GPUs, which have much higher memory bandwidth (up to 1TB/sec) to enable real-time scoring for more complex AI/ML problems. Try It For Yourself If you want to try real-time image recognition out for yourself, you can download the newest version of the SingleStore DB 6 beta, and look at the documentation for the DOT _ PRODUCT function.", "date": "2017-06-29"},
{"website": "Single-Store", "title": "key-considerations-for-a-cloud-data-warehouse", "author": ["Mike Boyarski"], "link": "https://www.singlestore.com/blog/key-considerations-for-a-cloud-data-warehouse/", "abstract": "Data growth and diversity has put new pressures on traditional data warehouses, resulting in a slew of new technology evaluations. The data warehouse landscape offers a variety of options, including popular cloud solutions that offer pay-as-you-go pricing in an easy-to-use and scale package. Here are some considerations to help you select the best cloud data warehouse. First, Identify Your Use Case A cloud data warehouse supports numerous use cases for a variety of business needs. Here are some common use cases along with the notable capabilities required for each. Ad Hoc Analysis Ad hoc analysis provides guided or open queries to the data warehouse, giving the end user flexibility to explore deeper questions. Users use native SQL or an interactive visual analysis tool such as Tableau or Looker. Each query result often prompts the user to dive further into the data, going from summary or aggregate views into distinct row level detail. A data warehouse that is good at ad hoc analysis delivers fast consistent responses across a variety of query types. How does a data warehouse support ad hoc analysis? Efficient query processing that can scan, join, and aggregate data in a variety of table structures. Columnstore table format for optimized disk usage and accelerated aggregate query response. Relational data format with ANSI SQL query syntax provides a familiar, easy to use structured language. Built-in statistical functions such as MAX, MIN, SUM, COUNT, STD, NTILE, and RANK, to name a few, will make it easier to build sophisticated queries. Data security ensures different users are shielded from sensitive or unauthorized data, requiring user authentication, role based access control, and row level security. Scalable concurrency for supporting thousands of users running a variety of queries simultaneously. Native connectivity to leading business intelligence tools for easier visual analysis and collaborative dashboards. Machine Learning and Data Science Data science and machine learning use a data warehouse to identify trends, discover hidden data relationships, and predict future events with sophisticated algorithms. Machine learning is a technique that can learn and improve insight discovery without explicitly being programmed to do so. Data scientists will often require large volumes of data to improve their predictions and correlations. Data is often enriched and cleaned or packaged into sample data sets for faster experimentation. Experiments are commonly performed offline due to the intense processing power required for the analysis. Advances in algorithms, hardware, machine learning and artificial intelligence tooling have led to more advanced data processing methods that can automatically identify hard to find events with relatively little human coordination. How does a data warehouse support machine learning and data science? Support a variety of data types including relational, CSV, JSON, and geospatial formats. Provide native interoperability with data preparation and statistical tooling, such as Spark, SparkML, Python, R, SAS, and TensorFlow. To maximize resource savings, offer rapid sandbox configuration for quick experimentation with easy spin-up and termination of databases as load requirements change. To support collaboration and sharing of analyses, offer native connectivity with modern business intelligence tools such as Tableau, Zoomdata, and Looker. Real-Time and Operational Analytics Operational analytics applications often manage Key Performance Indicators (KPIs) by querying data continuously. The insights might be used several times a day by people or machines. The speed of response for an operational or real-time analytics solution can vary based on the systems in place and the organizational readiness. Gartner’s Roy Schulte said it best in his report, How to Move Analytics to Real Time : “Business real time is about situation awareness; sensing and responding to what is happening in the world now, rather than to what happened a few hours or days ago, or what is predicted to happen based on historical data.” How does a data warehouse support real-time analytics? Streaming ingestion of data that can be immediately queried. Fast processing of repeat queries, potentially by thousands of users or applications. To reduce outages and maintain 24/7 operational support, high availability that includes redundancy and auto-failover. To improve accuracy and decision speeds, exactly once semantics for real-time data de-duplication and enrichment. Mixed Workload Analytics Most organizations want a single source of data to improve decision accuracy and support a variety of workloads across ad hoc, machine learning, and real-time analytics. These expanded use cases place a strong emphasis on performance, security, and user or application concurrency. Due to the variety of applications requiring sub-second data access, mixed workloads can be a challenge to tune and govern. How does a data warehouse support mixed workload analytics? A robust, efficient, and distributed query processor that can support a broad range of queries without overpaying for extra hardware resources or require hard-to-manage database configurations. Rapid easy-to-scale architecture that can address changes in workload complexity and user concurrency load. Comprehensive security to shield users from seeing sensitive data without requiring custom database schemas or views. Broad data ingestion to support real-time streaming and batch load requirements. Next Up, Understanding Cloud Data Warehouse Capabilities As you evaluate your next cloud data warehouse investment, it’s important to know the range of capabilities that are important for your project or business. Below is a list of capabilities organized by category to help you identify the right data warehouse: Usability Rapid provisioning: Setup should be self-service and take a few minutes from the point of sign up to a running functioning database Accessibility: For easy query processing and integration with existing applications, tools, and skills, the environment should support relational data using ANSI SQL Easy data loading: A guided or integrated data loading process should give users an easy integrated way to deploy a real-time data pipeline or bulk load ingestion Optimized query processing: The database should have a distributed query optimizer that can process most queries with minimal specialized tuning Simplified capacity management: As data or user growth expands, the data warehouse should provide managed or automated capacity adjustment to quickly address changing workloads Performance Ingest to analysis: Streaming data ingestion with simultaneous query processing ensures the fastest possible insights on live and historical data Fast queries: Subsecond query response against billions of rows with vectorized query processing and columnstore structure for ad-hoc dashboards or operational reports Operationally tuned: Compiled SQL queries accelerate query execution for added performance gains Cost On-demand pricing: Sometimes a data warehouse is not required for 24/7 operation; hourly billing can tightly associate the usage to payment. Annual discounts: Reserved pricing discounts should be an option for operational deployments that are always available Flexibility Multicloud: To maximize the proximity of your data and get the ultimate performance for your applications, you need the freedom to choose the cloud service provider you prefer or have standardized on Hybrid cloud: Maintain existing investments by spanning data warehouse investments across on-premises and cloud on a single platform Elastic: Driven by growth in data, users, or query sophistication, rapidly scale out or down for new capacity requirements Interoperable: To ensure compatibility with existing tools, applications, and skills, support JDBC/ODBC connectivity, MySQL wire protocol, and ANSI SQL Scalability Concurrency support: Scale-out distributed architecture ensures that high volume ingest and write queries do not degrade dashboard or report performance High Availability: Efficient replication and distributed architecture ensures no single point of failure for operational requirements Durable: All data should reside on disk for audit or regulatory requirements along with expedited recovery from unexpected failures Security Comprehensive: Data should be secured across the analysis lifecycle, from single sign-on (SSO) authentication, role based access control (RBAC), SSL encryption of data over the wire, encryption of data at rest, granular audit logging, and separation of concerns for database administrators Consistent: Ensure a consistent security model across on-premises to the cloud with strong security capabilities across deployments Conclusion: Considerations for SingleStore Cloud SingleStore Cloud offers all the desired capabilities described above as a full-featured cloud data warehouse that is easy to set up and use for supporting a mix of workloads in a single integrated platform. The product delivers a fast, flexible, and secure environment that is capable of analyzing both live and historical data. The pay-as-you-go service gives organizations an affordable approach to real-time analytics. Try SingleStore Cloud today and get a $300 free credit offer resulting in up to 300 hours of free usage. Try SingleStore Cloud Now", "date": "2017-07-10"},
{"website": "Single-Store", "title": "delivering-scalable-self-service-analytics", "author": ["Eric Hanson"], "link": "https://www.singlestore.com/blog/delivering-scalable-self-service-analytics/", "abstract": "Within 48 hours of launching Google Analytics as a free product, virtually all of Google’s servers crashed. Eric Schmidt called this Google’s “most successful disaster.” Why would a free product, whose hardware requirements melted down a datacenter, be worth it? Wesley Chan, the creator of Google Analytics, later said that, “Google Analytics generates about three billion dollars in extra revenue,” as noted in Steven Levy’s book, In The Plex . Google Analytics allowed Google’s customers to measure how good AdWords actually were, and showed them, with their own data, exactly how high they could increase bids and still make money. As Chan said, “know more, spend more.” The full potential of such an offering comes when customers are allowed to arbitrarily segment and calculate flexible aggregates. To do that, they need to be able to query raw unaggregated data . This way your company does not have to guess what the customer wants to aggregate, as all choices remain available. If raw data access is not provided, then data must be precomputed, at least on some dimensions, which limits flexibility and the extent of the insights users can get from data. Cost and technology constraints have led most companies to build analytics with this precompute approach for customers, because they need to serve analytics to many customers concurrently. The scale required to offer raw data access remained untenable. It was unthinkable to perform computations on raw data points on the scale of billions of rows per request concurrently for thousands of customers. Today, SingleStore is changing that conventional wisdom and offering companies the ability to serve raw unaggregated data performance to a range of customers. To explain this capability further, there are three major pieces of technology in this use case: Scale-out Columnstore query execution Efficient data isolation Scale-Out Knowing system performance characteristics on a per-core basis, users can calculate how much compute and storage is needed to serve analytics at scale. Once that calculation is done, the key is to utilize a distributed system allowing enough dedicated compute power to meet demand. SingleStore can be used to run one to hundreds of nodes, which lets users scale the performance appropriately. For example, if you have a million customers with one million data points each, you can say that you have one trillion data points. Imagine that at the peak, one thousand of those customers are looking at the dashboard simultaneously – essentially firing off one thousand concurrent queries against the database. Columnstore compression can store these trillion rows on a relatively small SingleStore cluster with approximately 20 nodes. Conservatively, SingleStore can scan 100 million rows per second per core, which mean that just one core can service 100 concurrent queries scanning one million rows each, and deliver sub-second results for analytical queries over raw data – below we will provide a benchmark for a columnstore query execution performance. Columnstore Query Execution A simple query over a columnstore table, such as a GROUP BY , can run at a rate of hundreds of millions to over a billion data points per second per core. To demonstrate this, we loaded a public dataset about every airline flight in the United States from 1987 until 2015. As the goal was to understand performance per core, we loaded this into a single node SingleStore cluster running on a 4 core, 8 thread Intel(R) Core(TM) i7-6700 CPU @ 3.40GHz. To repeat this experiment, download the data using the following bash script: mkdir csv\nfor s in `seq 1987 2015`\ndo\nfor m in `seq 1 12`\ndo\nwget http://www.transtats.bts.gov/Download/On_Time_On_Time_Performance_${s}_${m}.zip\ndone\ndone Create this table: CREATE TABLE ontime (\n Year INT,\n Quarter INT,\n Month INT,\n DayofMonth INT,\n DayOfWeek INT,\n FlightDate Date,\n UniqueCarrier Varchar(100),\n AirlineID INT,\n Carrier Varchar(100),\n TailNum Varchar(100),\n FlightNum Varchar(100),\n OriginAirportID INT,\n OriginAirportSeqID INT,\n OriginCityMarketID INT,\n Origin Varchar(100),\n OriginCityName Varchar(100),\n OriginState Varchar(100),\n OriginStateFips Varchar(100),\n OriginStateName Varchar(100),\n OriginWac INT,\n DestAirportID INT,\n DestAirportSeqID INT,\n DestCityMarketID INT,\n Dest Varchar(100),\n DestCityName Varchar(100),\n DestState Varchar(100),\n DestStateFips Varchar(100),\n DestStateName Varchar(100),\n DestWac INT,\n CRSDepTime INT,\n DepTime INT,\n DepDelay INT,\n DepDelayMinutes INT,\n DepDel15 INT,\n DepartureDelayGroups Varchar(100),\n DepTimeBlk Varchar(100),\n TaxiOut INT,\n WheelsOff INT,\n WheelsOn INT,\n TaxiIn INT,\n CRSArrTime INT,\n ArrTime INT,\n ArrDelay INT,\n ArrDelayMinutes INT,\n ArrDel15 INT,\n ArrivalDelayGroups INT,\n ArrTimeBlk Varchar(100),\n Cancelled INT,\n CancellationCode Varchar(100),\n Diverted INT,\n CRSElapsedTime INT,\n ActualElapsedTime INT,\n AirTime INT,\n Flights INT,\n Distance INT,\n DistanceGroup INT,\n CarrierDelay INT,\n WeatherDelay INT,\n NASDelay INT,\n SecurityDelay INT,\n LateAircraftDelay INT,\n FirstDepTime Varchar(100),\n TotalAddGTime Varchar(100),\n LongestAddGTime Varchar(100),\n DivAirportLandings Varchar(100),\n DivReachedDest Varchar(100),\n DivActualElapsedTime Varchar(100),\n DivArrDelay Varchar(100),\n DivDistance Varchar(100),\n Div1Airport Varchar(100),\n Div1AirportID INT,\n Div1AirportSeqID INT,\n Div1WheelsOn Varchar(100),\n Div1TotalGTime Varchar(100),\n Div1LongestGTime Varchar(100),\n Div1WheelsOff Varchar(100),\n Div1TailNum Varchar(100),\n Div2Airport Varchar(100),\n Div2AirportID INT,\n Div2AirportSeqID INT,\n Div2WheelsOn Varchar(100),\n Div2TotalGTime Varchar(100),\n Div2LongestGTime Varchar(100),\n Div2WheelsOff Varchar(100),\n Div2TailNum Varchar(100),\n Div3Airport Varchar(100),\n Div3AirportID INT,\n Div3AirportSeqID INT,\n Div3WheelsOn Varchar(100),\n Div3TotalGTime Varchar(100),\n Div3LongestGTime Varchar(100),\n Div3WheelsOff Varchar(100),\n Div3TailNum Varchar(100),\n Div4Airport Varchar(100),\n Div4AirportID INT,\n Div4AirportSeqID INT,\n Div4WheelsOn Varchar(100),\n Div4TotalGTime Varchar(100),\n Div4LongestGTime Varchar(100),\n Div4WheelsOff Varchar(100),\n Div4TailNum Varchar(100),\n Div5Airport Varchar(100),\n Div5AirportID INT,\n Div5AirportSeqID INT,\n Div5WheelsOn Varchar(100),\n Div5TotalGTime Varchar(100),\n Div5LongestGTime Varchar(100),\n Div5WheelsOff Varchar(100),\n Div5TailNum Varchar(100),\n key (AirlineID) using clustered columnstore\n); Then load data into the table: load data infile '/home/memsql/csv/*' into table ontime fields terminated by ',' enclosed by '\"' lines terminated by ',\\n' ignore 1 lines; Once the data is loaded, run a simple group by command. The following query performs a full table scan: SELECT OriginCityName, count(*) AS flights\nFROM ontime GROUP BY OriginCityName ORDER BY flights DESC LIMIT 20; On a machine with 4 cores, a 164 million row dataset query runs in 0.04 seconds which is 1 billion rows per second per core. No, that’s not a typo. That’s a billion rows per second per core. More complex queries will consume more CPU cycles, but with this level of baseline performance there is a lot of room across a cluster of 8, 16, or even hundreds of machines to handle multi-billion row datasets with response times under a quarter of a second. At that speed, queries appear to be instantaneous to users, leading to great user satisfaction. Try this example using SingleStore DB 6 . New vectorized query execution techniques in SingleStore DB 6, using SIMD and operations directly on encoded (compressed) data, make this speed possible. Efficient Data Isolation Per Customer Data warehouses such as Redshift and Big Query support large scale, but may not sufficiently isolate different queries in highly concurrent workloads. On top of that, both have a substantial fixed overhead on a per query basis. Redshift in particular does not support many concurrent queries: http://docs.aws.amazon.com/redshift/latest/dg/cm-c-defining-query-queues.html . Depending on the analytical requirements, SingleStore allows for an ordered and partitioned physical data layout to ensure only scanned data belongs to a single customer. In our example, the columnstore was clustered on AirlineID. SingleStore supports clustered columnstore keys that allow global sorting of columnstore tables. In this case, if you have a predicate on AirlineID a user will only scan the subset of data belonging to that airline. This allows SingleStore to deliver on very high concurrency (in the thousands of concurrent queries) with each query scanning and aggregating millions of data points. More on Query Execution At SingleStore, we are continuously innovating with new query processing capabilities. This is a list of recent innovations in our shipping product: https://docs.singlestore.com/latest/release-notes/memsql/60-release-notes/ . Bringing it All Together Going back to our original example, though our dataset is one trillion rows, because of the clustered columnstore key, each customer only needs to scan through one million rows. For a simple query like the above, scanning 500 million rows per second per core means that a single CPU core could support 500 concurrent queries and deliver sub-second performance. To recreate the work mentioned in this blog, try out SingleStore DB 6: singlestore.com/free .", "date": "2017-07-14"},
{"website": "Single-Store", "title": "5-sessions-to-see-at-aws-summit-chicago", "author": ["Nicole Nearhood"], "link": "https://www.singlestore.com/blog/5-sessions-to-see-at-aws-summit-chicago/", "abstract": "The Amazon Web Services Summit will bring members of the cloud computing community together to connect, collaborate, and learn about AWS from July 26-27, 2017 in Chicago. Whether you are a cloud beginner or an experienced user, you will learn something new at the AWS Summit. This free event is designed to educate attendees about the AWS platform, and help develop the skills to design, deploy, and operate infrastructure and applications. SingleStore is exhibiting in the HUB, Partner & Solutions Expo, so stop by our booth #405 to view a demo and speak with our subject matter experts. This year, AWS Summit Chicago will offer even more breakout sessions led by AWS subject matter experts and top AWS customers. This informative mixture of lectures, demonstrations, and guest speakers is geared towards keeping attendees informed on technical content, customer stories, and new launch announcements. Here are our top breakout session picks. DEM314 – Real-time Dashboards with Kinesis Analytics & Amazon Rekognition In this demo, we’ll show you how to easily aggregate and display streaming data from multiple mobile devices on a real-time dashboard. First, we’ll use Amazon Rekognition to find and analyze faces in photos. Then we will stream the metadata to Amazon Kinesis Streams and aggregate it with Amazon Kinesis Analytics. Allan MacInnis – Amazon Ryan Nienhuis – Amazon Wednesday, July 26, 3:00 PM – 3:30 PM– Day 1 Theater, The HUB SEC309 – Secure Your Cloud Investment: Mastering AWS Identity Access Management (IAM) The landscape of IT and data security has changed vastly since the advent of the cloud. Savvy technology leaders know that they must have visibility and control over their environment to fully leverage their cloud investments. Tools like IAM offer teams indispensable tools to proactively manage and protect their cloud environment. Join CloudCheckr CEO Aaron Newman to learn tips for effective and secure cloud deployments that you can implement today, including: How to address requirements of the AWS Shared Responsibility Model Why anticipating internal and external threats are crucial for mitigating security risks in the cloud IAM overview and how it helps ensure secure and compliant deployments Features and policies, as well as how to apply them to users and groups Advice for leveraging IAM roles to mitigate potential security risks Best practices for using IAM to configure user permissions, and other important considerations Aaron Newman – CEO, CloudCheckr Wednesday, July 26, 11:30 AM – 12:30 PM– E351 DEM201 – Transforming Your Business with Analytics on the Cloud The cloud and analytics capabilities have matured to the point that two out of every three enterprises have identified business use case(s), such as increasing ROI, agility and pace of innovation, and have started the journey to cloud. Analytics on the cloud offers a faster path to insight-driven decision-making and business outcomes. This session is intended to increase participants’ knowledge of the cloud analytics value proposition, business use cases, common pitfalls and challenges, and best practices to migrating to cloud. At the end of the session, attendees will be equipped with: A better understanding of what AWS offers in the world of data analytics, machine learning and AI Effective strategies that avoid common mistakes in a cloud journey A self-assessment (readiness) approach and high level understanding of total cost of ownership structure of cloud analytics Prasanna Rajagopalan – Analytics and Information Management, Trianz Wednesday, July 26, 1:30 PM – 2:00 PM– Kumo Theater, The HUB DEM200 – Cloudreach: Building an Effective Cloud Operating Model on AWS In this session, you will learn how to build a cloud operating model on AWS. When moving to the cloud, it is imperative that you have a plan which encompasses where you are today (your current state) and where you want to get to (your future state). The Cloud Operating Model is designed to help you identify what your target future looks like and the steps you should take along the way to get there. We will cover how to define your cloud strategy/vision, the design principles you need to capture to make it a reality, how to transform your organization to meet the new reality of getting things done, and finally how to ensure you have an operational plan in place to manage your environment. Jeff Armstrong – Cloud Architect, Cloudreach Wednesday, July 26, 12:45 PM – 1:15 PM– Kumo Theater, The HUB DEM316 – Real-Time Streaming Analysis & Visualization using Apache Flink on Amazon EMR and Kibana See how a real-time data stream can be processed, analyzed, and visualized in real time using a combination of open source technologies and managed services. In this demo, we will use Apache Flink on Amazon EMR to process NYC taxi traffic data in real-time. Then we will use the Amazon Elasticsearch service and Kibana, to analyze and visualize the data. Keith Steward – Specialist (EMR) Solutions Architect & AI SME, Amazon Web Services, Amazon Thursday, July 27, 10:45 AM – 11:15 AM– Kumo Theater, The HUB", "date": "2017-07-18"},
{"website": "Single-Store", "title": "the-curious-case-of-thread-groups-identifiers", "author": ["Rodrigo Gomes"], "link": "https://www.singlestore.com/blog/the-curious-case-of-thread-groups-identifiers/", "abstract": "At SingleStore, we are out to build awesome software and we’re always trying to solve hard problems. A few days ago, I uncovered a cool Linux mystery with some colleagues and fixed it. We thought sharing that experience might benefit others. The scene of the crime While developing an internal tool to get stack traces, we decided to use the SYS_tgkill Linux system call to send signals to specific threads. The tgkill syscall sends a signal to a specific thread based on its “thread group” identifier ( tgid ) and thread identifier ( tid ). We store the thread identifier for every thread, so that was simple to obtain, but the “thread group” identifier was a new concept to me. A Google search suggested that a simple way to get the tgid is to read it from the Linux pseudo-file /proc/self/status , which has some information about the process: cat /proc/self/status\nName:   cat\nState:  R (running)\nTgid:   26473\nNgid:   0\nPid:    26473\nPPid:   26378\n... <snip> ... The first prototype of this internal tool used the SYS_getpid Linux system call to obtain the process identifier, find the correct status pseudo-file, and read directly from that file in a rudimentary way. The prototype assumed that the tgid was always in the third line. This worked for most developers at SingleStore, but some developers were running environments with newer Linux distributions and it didn’t seem to work in those instances. A recent Linux commit added a new field to /proc/self/status before Tgid , which broke the prototype: commit 3e42979e65dace1f9268dd5440e5ab096b8dee59\nAuthor: Richard W.M. Jones <rjones@redhat.com>\nDate:   Fri May 20 17:00:05 2016 -0700\n   procfs: expose umask in /proc/<PID>/status\n... <snip> ...\n--- a/fs/proc/array.c\n+++ b/fs/proc/array.c\n@@ -162,6 +176,10 @@ static inline void task_state(struct seq_file *m, struct pid_namespace *ns,\n       ngid = task_numa_group_id(p);\n       cred = get_task_cred(p);\n+       umask = get_task_umask(p);\n+       if (umask >= 0)\n+               seq_printf(m, \"Umask:\\t%#04o\\n\", umask);\n+\n       task_lock(p);\n       if (p->files)\n               max_fds = files_fdtable(p->files)->max_fds;\n       task_unlock(p);\n       rcu_read_unlock();\n       seq_printf(m,\n               \"State:\\t%s\\n\"\n               \"Tgid:\\t%d\\n\"\n               \"Ngid:\\t%d\\n\" The motive In order to stabilize the tool, we decided to learn more about thread group identifiers to try to find a more stable way to read them. My colleagues noticed that the tgid always seemed to match the pid of a process (the thread id of the original parent thread), so we started looking at the relationship between the attributes. Indeed, the Linux reference on thread groups states: Thread groups were a feature added in Linux 2.4 to support the POSIX threads notion of a set of threads that share a single PID. Internally, this shared PID is the so-called thread group identifier (TGID) for the thread group. Since Linux 2.4, calls to getpid(2) return the TGID of the caller. We were left wondering, why did /proc/self/status report both Tgid and Pid ? The culprit I looked at the implementation of /proc/self/status in fs/proc/array.c to understand the difference between Tgid and Pid : tgid = task_tgid_nr_ns(p, ns);\n... <snip> ...\nseq_put_decimal_ull(m, \"\\nTgid:\\t\", tgid);\nseq_put_decimal_ull(m, \"\\nNgid:\\t\", ngid);\nseq_put_decimal_ull(m, \"\\nPid:\\t\", pid_nr_ns(pid, ns)); Looking at the implementation of task_tgid_nr_ns inside kernel/pid.c , I saw that the Pid and the Tgid are in fact the same: pid_t task_tgid_nr_ns(struct task_struct *tsk, struct pid_namespace *ns)\n{\n   return pid_nr_ns(task_tgid(tsk), ns);\n}\nEXPORT_SYMBOL(task_tgid_nr_ns); task_tgid in include/linux/sched.h does exactly as I’d expect, merely reading the pid of the lead process: static inline struct pid *task_tgid(struct task_struct *task)\n{\n   return task->group_leader->pids[PIDTYPE_PID].pid;\n} Elementary, my dear Watson It turned out that we didn’t need to read the status pseudo-file at all, and could instead use getpid directly. That change made it work on all environments we tested, and simplified the code significantly. At SingleStore I’ve had the chance to investigate little systems mysteries like this one, and also design and work on state-of-the-art systems. If that sounds like something you would enjoy, we are currently looking for engineers. Apply and join the team http://www.singlestore.com/careers/jobs/ .", "date": "2017-07-19"},
{"website": "Single-Store", "title": "nyc-taxi-data-ingested-into-memsql", "author": ["Seth Luersen"], "link": "https://www.singlestore.com/blog/nyc-taxi-data-ingested-into-memsql/", "abstract": "Experience teaches us that when loading data into a database, in whatever form ― normalized, denormalized, schema-less, hierarchical, key-value, document, etc ― the devil is always in the data load. For enterprise companies in the real-time economy, every second saved means improved efficiency, productivity, and profitability. Thankfully, SingleStore makes your enterprise data fast to load and easy to access. You can spin up a SingleStore cluster in minutes and load data very quickly using SingleStore Pipelines. SingleStore scales out for fast parallel processing of very large data sets. SingleStore Pipelines deliver exactly-once semantics for duplicate-free, fast data ingest. In this blog tutorial, we’ll illustrate rapid data ingest with SingleStore using real-world data from the New York City Taxi and Limousine Commission (NYC TLC) yellow taxi trip data set , which is over 200GB of CSV files. We’ll break the tutorial into several sections: Pre-configuration requirements for everything you need to complete the tutorial including AWS S3 and SingleStore Downloading, compressing, and uploading the yellow taxi data to AWS S3 Loading the data with SingleStore Pipelines in about 24 minutes Pre-configuration Requirements To complete this tutorial, you can use a Mac OS, Linux, or Windows machine, and you’ll need a: Bash compatible terminal AWS account and S3 bucket with read and write privileges AWS Command Line Interface (CLI) MySQL compatible client to execute SQL statements Running SingleStore cluster AWS S3 First, log into your AWS account. To get started with AWS S3, review Getting Started with Amazon Simple Storage Service . We will store approximately 44GB of compressed data versus 200GB of raw data, which typically costs about $4.50 a month. After loading the data into SingleStore, you can delete the S3 data and terminate the AWS account to avoid additional charges. AWS Command Line Interface (CLI) Use the AWS CLI to use shell commands to script S3 operations, such as creating a folder, listing folder items, and uploading files. Follow the User Guide for installing the AWS CLI. AWS Credentials To upload data to S3 using the AWS CLI and extract files from S3 with SingleStore Pipelines, you’ll need AWS credentials for an IAM user. For an IAM user, you can easily generate  an AWS Access Key ID and AWS Secret Access Key following this guide . S3 Bucket Log in to the AWS Console and create a new S3 bucket in the availability zone for your account. The NYC taxi data set includes yellow taxi, green vehicle, and fhv vehicle data. We’ll only use the yellow taxi data in our initial data load as it is by far the largest data set . In the case of yellow taxi data, the schema changes over time, so we’ll create yellow taxi subfolders to demarcate the schema changes. In your S3 bucket, create the following older hierarchy: |- nyc-taxi\n|--- yellow_tripdata\n|----- 200901-201412\n|----- 201501-201606\n|----- 201607-201612 MySQL Compatible Client SingleStore uses the open source MySQL protocol, so in order to execute Data Definition Language (DDL) and Data Manipulation Language (DML) statements you’ll need a MySQL compatible client. Here are some popular, free clients: MySQL Workbench Sequel Pro MySql Client for  Linux APT Repository, Debian Packages sudo apt-get update sudo apt-get install mysql-client MySQL Client for Linux RPM, Red Hat Enterprise Linux/Oracle Linux/CentOS 5 systems sudo yum install mysql-community-client mysql-5. * SingleStore Cluster A SingleStore cluster consists of at least one master aggregator node and one leaf node. Setting up SingleStore SingleStore runs natively on a 64-bit Linux operating systems. Download and install SingleStore either on-premises or in your own self-managed cloud. For Windows or Mac OS, you can use the SingleStore Quickstart Docker image, which is a standalone edition of SingleStore. If using Windows 10, consider installing Bash using the Windows Subsystem for Linux Beta . This beta feature will install Ubuntu on Windows, and you can run SingleStore experimentally in this environment. Very cool. NYC TLC Trip Data The NYC TLC data files are as small as 300MB and as large as 2.5GB. With 8 years of monthly data, yellow taxi information accounts for about 90% of all data. Many yellow taxi cab files are approximately 2.4GB. When compressed, the files range from 300MB to almost 600MB. Download, Compress, and Upload the Data Here’s a rudimentary Bash script to do the work of downloading each file, compressing the raw files, uploading the compressed file to proper folder in S3, and then deleting both the raw CSV and tar.gz files from the machine where the script runs. The shell script requires AWS CLI. #!/bin/bash\n# Change S3_BUCKET to your S3 bucket\nexport S3_BUCKET=\"s3://my-s3-bucket\"\n# Change S3_BASE_FOLDER to your base folder, but should be nyc-taxi if you are following the tutorial\nexport S3_BASE_FOLDER=\"nyc-taxi\"\n# Change AWS_ACCESS_KEY_ID to your key\nexport AWS_ACCESS_KEY_ID=MY_ACCESS_KEY\n# ChangeAWS_SECRET_ACCESS_KEY to your secret key\nexport AWS_SECRET_ACCESS_KEY=MY_SECRET_KEY\n# Change S3_BASE_FOLDER to your base folder.\nexport AWS_DEFAULT_PROFILE=my-aws-profile-user\nexport AWS_DEFAULT_REGION=us-east-1\naws configure --profile $AWS_DEFAULT_PROFILE\n# do not change URL_ROOT\nURL_ROOT=\"https://s3.amazonaws.com/nyc-tlc/trip+data/\"\n# modify if needed for smaller subsets or add green and fhv for additional cab_types\nMONTH_ORDINALS=(\"01\" \"02\" \"03\" \"04\" \"05\" \"06\" \"07\" \"08\" \"09\" \"10\" \"11\" \"12\")\nYEAR_ORDINALS=(\"2009\" \"2010\" \"2011\" \"2012\" \"2013\" \"2014\" \"2015\" \"2016\")\nCAB_TYPES=(\"yellow\")\n# leave as empty\nFILE_NAME=\"\"\nS3_FOLDER=\"\"\nS3_SUBFOLDER=\"\"\nfor name in ${CAB_TYPES[@]}\n  do\n    if [ $name == \"yellow\" ]; then\n      S3_FOLDER=\"yellow_tripdata\"\n      YEARS=${YEAR_ORDINALS[@]}\n    fi\n    for yy in ${YEARS[@]}\n      do\n        MONTHS=${MONTH_ORDINALS[@]}\n        for mm in ${MONTHS[@]}\n        do\n          FILE_NAME=${name}_tripdata_${yy}-${mm}\n          # get the csv file\n          curl -S -O \"${URL_ROOT}${FILE_NAME}.csv\" && echo \"done! curl ${FILE_NAME}.csv\" &\n          wait\n          # tarball the file\n          tar -cvzf \"${FILE_NAME}.tar.gz\" \"${FILE_NAME}.csv\"  && echo \"done! tar ${FILE_NAME}.tar.gz\" &\n          wait\n          # upload to AWS S3 the gz file\n          if [[ $name == \"yellow\"  &&  $yy == \"2015\" ]]; then\n            S3_SUBFOLDER=\"201501-201606\"\n          elif [[ $name == \"yellow\"  &&  $yy == \"2016\" &&  $mm == \"01\" ]]; then\n            S3_SUBFOLDER=\"201501-201606\"\n          elif [[ $name == \"yellow\"  &&  $yy == \"2016\" &&  $mm == \"02\" ]]; then\n            S3_SUBFOLDER=\"201501-201606\"\n          elif [[ $name == \"yellow\"  &&  $yy == \"2016\" &&  $mm == \"03\" ]]; then\n            S3_SUBFOLDER=\"201501-201606\"\n          elif [[ $name == \"yellow\"  &&  $yy == \"2016\" &&  $mm == \"04\" ]]; then\n            S3_SUBFOLDER=\"201501-201606\"\n          elif [[ $name == \"yellow\"  &&  $yy == \"2016\" &&  $mm == \"05\" ]]; then\n            S3_SUBFOLDER=\"201501-201606\"\n          elif [[ $name == \"yellow\"  &&  $yy == \"2016\" &&  $mm == \"06\" ]]; then\n            S3_SUBFOLDER=\"201501-201606\"\n          elif [[ $name == \"yellow\"  &&  $yy == \"2016\" &&  $mm == \"07\" ]]; then\n            S3_SUBFOLDER=\"201607-201612\"\n          elif [[ $name == \"yellow\"  &&  $yy == \"2016\" &&  $mm == \"08\" ]]; then\n            S3_SUBFOLDER=\"201607-201612\"\n          elif [[ $name == \"yellow\"  &&  $yy == \"2016\" &&  $mm == \"09\" ]]; then\n            S3_SUBFOLDER=\"201607-201612\"\n          elif [[ $name == \"yellow\"  &&  $yy == \"2016\" &&  $mm == \"10\" ]]; then\n            S3_SUBFOLDER=\"201607-201612\"\n          elif [[ $name == \"yellow\"  &&  $yy == \"2016\" &&  $mm == \"11\" ]]; then\n            S3_SUBFOLDER=\"201607-201612\"\n          elif [[ $name == \"yellow\"  &&  $yy == \"2016\" &&  $mm == \"12\" ]]; then\n            S3_SUBFOLDER=\"201607-201612\"\n          else\n            S3_SUBFOLDER=\"200901-201412\"\n          fi\n          if [ $name == \"yellow\" ]; then\n            aws s3 cp ${FILE_NAME}.tar.gz ${S3_BUCKET}/${S3_BASE_FOLDER}/${S3_FOLDER}/${S3_SUBFOLDER}/ --profile $AWS_DEFAULT_PROFILE && echo \"done! aws s3 cp ${FILE_NAME}.tar.gz\" &\n          fi\n          wait\n          #rm the cv files\n          rm -f \"${FILE_NAME}.csv\" && echo \"done! rm -f ${FILE_NAME}.csv\" &\n          wait\n          #rm the gz files\n          rm -f \"${FILE_NAME}.tar.gz\"  && echo \"done! rm -f ${FILE_NAME}.tar.gz\" &\n          wait\n        done\n      done\n  done In order for the script to work with your S3 bucket and folder, specify values for these variables: S3_BUCKET S3_BASE_FOLDER AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY Save your changes and name the file, nyc_tlc_taxi_files.sh. In the terminal where you are going to run the shell script, modify the file properties: chmod 777 nyc_tlc_taxi_files.sh Next, create an AWS profile using the AWS CLI: AWS_DEFAULT_PROFILE=my-profile\naws configure --profile $AWS_DEFAULT_PROFILE At the prompts, specify your values for the ‘AWS Access Key ID’ and ‘AWS Secret Access Key’. For ‘Default region name’, specify us-east-1. For ‘Default output format’, specify json. From a Bash shell, run the nyc_tlc_taxi_files.sh script. There is no need to specify the AWS profile settings again, so press enter at each prompt. The script outputs the successful processing of each file: HAL2001:shellscripts Seth$ ./nyc_tlc_taxi_files.sh\nAWS Access Key ID [****************7G6Q]:\nAWS Secret Access Key [****************EmaN]:\nDefault region name [us-east-1]:\nDefault output format [json]:\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 2420M  100 2420M    0     0  4398k      0  0:09:23  0:09:23 --:--:-- 5915k\ndone! curl yellow_tripdata_2009-01.csv\na yellow_tripdata_2009-01.csv\ndone! tar yellow_tripdata_2009-01.tar.gz\nupload: ./yellow_tripdata_2009-01.tar.gz to s3://my-s3-bucket/my-nyc-taxi-folder/yellow_tripdata/200901-201412/yellow_tripdata_2009-01.tar.gz\ndone! aws s3 cp yellow_tripdata_2009-01.tar.gz\ndone! rm -f yellow_tripdata_2009-01.csv\ndone! rm -f yellow_tripdata_2009-01.tar.gz Downloading, compressing, and uploading the 96 yellow taxi monthly data files using your local machine will take around six to eleven hours, depending on your connection speeds. So, if you are following along on your laptop, you may want to run the Bash shell script during the night, keeping your machine plugged in with sleep disabled. To Compress or Not Compress? With AWS S3, you ultimately pay for storage size and for data transmission. Depending on the availability zone, it costs on average about five dollars to store 200GB. Compressing 200GB gets that down to under 50GB. Because a SingleStore Pipeline for S3 can process a compressed file, you will save by using compression for large data sets on AWS S3. Load the NYC Yellow Taxi Trip Data After the bash shell script completes and all NYC Taxi trip data has been loaded in your various S3 bucket folders, we’ll now load all the data using SingleStore S3 Pipelines. SingleStore Database and Tables To get started, we’ll create a database and two tables. Because the yellow taxi data has several schema changes over eight years as described in the data dictionary , we’ll create a staging table to handle the various data formats. After the initial S3 Pipeline data ingest, we’ll insert the staging data into a destination table. Using a MySQL compatible client connect to your Master Aggregator to run DDL commands. To create the database and tables, execute the following SQL: DROP DATABASE IF EXISTS nyc_taxi;\nCREATE DATABASE IF NOT EXISTS nyc_taxi;\n\nUSE nyc_taxi;\n\nDROP TABLE IF EXISTS yellow_trips_staging;\nCREATE TABLE IF NOT EXISTS yellow_trips_staging (\n  vendor_id varchar(3) NOT NULL DEFAULT '0',\n  pickup_datetime datetime NOT NULL,\n  dropoff_datetime datetime NOT NULL,\n  pickup_longitude DOUBLE NOT NULL DEFAULT 0,\n  pickup_latitude DOUBLE NOT NULL DEFAULT 0,\n  passenger_count tinyint NOT NULL DEFAULT 0,\n  trip_distance decimal(6,2) NOT NULL DEFAULT 0,\n  rate_code tinyint NOT NULL DEFAULT 0,\n  store_and_fwd_flag varchar(1) NOT NULL DEFAULT 'N',\n  dropoff_longitude DOUBLE NOT NULL DEFAULT 0,\n  dropoff_latitude DOUBLE NOT NULL DEFAULT 0,\n  PULocationID smallint(3) NOT NULL DEFAULT 0,\n  DOLocationID smallint(3) NOT NULL DEFAULT 0,\n  payment_type tinyint NOT NULL DEFAULT 0,\n  fare_amount decimal(6,2) NOT NULL DEFAULT 0,\n  surcharge decimal(6,2) NOT NULL DEFAULT 0,\n  extra decimal(6,2) NOT NULL DEFAULT 0,\n  mta_tax decimal(6,2) NOT NULL DEFAULT 0,\n  tip_amount decimal(6,2) NOT NULL DEFAULT 0,\n  tolls_amount decimal(6,2) NOT NULL DEFAULT 0,\n  improvement_surcharge decimal(6,2) NOT NULL DEFAULT 0,\n  total_amount decimal(7,2) NOT NULL DEFAULT 0,\n  colA varchar(1),\n  colB varchar(1),\n  key(pickup_datetime, dropoff_datetime) USING CLUSTERED COLUMNSTORE\n);\n\nDROP TABLE IF EXISTS yellow_trips;\nCREATE TABLE IF NOT EXISTS yellow_trips (\n  vendor_id varchar(3) NOT NULL DEFAULT '0',\n  pickup_datetime datetime NOT NULL,\n  pickup_year smallint(4) NOT NULL DEFAULT 0,\n  pickup_month tinyint(2) NOT NULL DEFAULT 0,\n  pickup_day tinyint(2) NOT NULL DEFAULT 0,\n  pickup_week tinyint(2) NOT NULL DEFAULT 0,\n  pickup_time TIME NOT NULL DEFAULT '00:000:00',\n  dropoff_datetime datetime NOT NULL,\n  dropoff_year smallint(4) NOT NULL DEFAULT 0,\n  dropoff_month tinyint(2) NOT NULL DEFAULT 0,\n  dropoff_day tinyint(2) NOT NULL DEFAULT 0,\n  dropoff_week tinyint(2) NOT NULL DEFAULT 0,\n  dropoff_time TIME NOT NULL DEFAULT '00:000:00',\n  pickup_pt geographypoint  not null default 'POINT(0 0)',\n  dropoff_pt geographypoint not null default 'POINT(0 0)',\n  PULocationID smallint(3) NOT NULL DEFAULT 0,\n  DOLocationID smallint(3) NOT NULL DEFAULT 0,\n  passenger_count tinyint(2) NOT NULL DEFAULT 0,\n  trip_distance decimal(6,2) NOT NULL DEFAULT 0,\n  rate_code tinyint NOT NULL DEFAULT 0,\n  store_and_fwd_flag varchar(1) NOT NULL DEFAULT 'N',\n  payment_type tinyint NOT NULL DEFAULT 0,\n  fare_amount decimal(6,2) NOT NULL DEFAULT 0,\n  surcharge decimal(6,2) NOT NULL DEFAULT 0,\n  extra decimal(6,2) NOT NULL DEFAULT 0,\n  mta_tax decimal(6,2) NOT NULL DEFAULT 0,\n  tip_amount decimal(6,2) NOT NULL DEFAULT 0,\n  tolls_amount decimal(6,2) NOT NULL DEFAULT 0,\n  improvement_surcharge decimal(6,2) NOT NULL DEFAULT 0,\n  total_amount decimal(7,2) NOT NULL DEFAULT 0,\n  shard key(pickup_datetime, dropoff_datetime),\n  key(pickup_datetime, dropoff_datetime) USING CLUSTERED COLUMNSTORE\n); SingleStore Pipelines Declared ever so simply but ever so powerfully with a CREATE PIPELINE statement, SingleStore Pipelines are a native construct in SingleStore. The pipeline syntax allows us to extract one or more gzip compressed (or uncompressed) source files from a S3 bucket folder, then load the data to a destination database table. In other words, we’ll end up with three SingleStore Pipelines to match our S3 folders. Create Pipelines You’ll need to specify your AWS_SECRET_ACCESS_KEY and AWS_ACCESS_KEY_ID credentials in the following CREATE PIPELINE statements. In the statements, we specify to skip all CSV parsing errors. The three pipelines will load data rapidly and in parallel to the staging table: USE nyc_taxi;\nDROP PIPELINE IF EXISTS pipeline_yellow_200901_201412;\nCREATE PIPELINE IF NOT EXISTS pipeline_yellow_200901_201412 AS\n  -- IMPORTANT: CHANGE my-s3-bucket\n  LOAD DATA S3 \"my-s3-bucket/nyc-taxi/yellow_tripdata/200901-201412/\"\n  -- IMPORTANT: CHANGE my_aws_secret_access_key and my_aws_access_key_id\n  CREDENTIALS '{\"aws_secret_access_key\": \"my_aws_secret_access_key\", \"aws_access_key_id\": \"my_aws_access_key_id\"}'\n  SKIP ALL ERRORS\n  INTO TABLE yellow_trips_staging\n  FIELDS TERMINATED BY ','\n  (vendor_id,pickup_datetime,dropoff_datetime,passenger_count,trip_distance,pickup_longitude,pickup_latitude,rate_code,store_and_fwd_flag,dropoff_longitude,dropoff_latitude,payment_type,fare_amount,surcharge,mta_tax,tip_amount,tolls_amount,total_amount);\n\nDROP PIPELINE IF EXISTS pipeline_yellow_201501_201606;\nCREATE PIPELINE IF NOT EXISTS pipeline_yellow_201501_201606 AS\n  -- IMPORTANT: CHANGE my-s3-bucket\n  LOAD DATA S3 \"my-s3-bucket/nyc-taxi/yellow_tripdata/201501-201606/\"\n  -- IMPORTANT: CHANGE my_aws_secret_access_key and my_aws_access_key_id\n  CREDENTIALS '{\"aws_secret_access_key\": \"my_aws_secret_access_key\", \"aws_access_key_id\": \"my_aws_access_key_id\"}'\n  SKIP ALL ERRORS\n  INTO TABLE yellow_trips_staging\n  FIELDS TERMINATED BY ','\n  (vendor_id,pickup_datetime,dropoff_datetime,passenger_count,trip_distance,pickup_longitude,pickup_latitude,rate_code,store_and_fwd_flag,dropoff_longitude,dropoff_latitude,payment_type,fare_amount,extra,mta_tax,tip_amount,tolls_amount,improvement_surcharge,total_amount);\n\nDROP PIPELINE IF EXISTS pipeline_yellow_201607_201612;\nCREATE PIPELINE IF NOT EXISTS pipeline_yellow_201607_201612 AS\n  -- IMPORTANT: CHANGE my-s3-bucket\n  LOAD DATA S3 \"my-s3-bucket/nyc-taxi/yellow_tripdata/201607-201612/\"\n  -- IMPORTANT: CHANGE my_aws_secret_access_key and my_aws_access_key_id\n  CREDENTIALS '{\"aws_secret_access_key\": \"my_aws_secret_access_key\", \"aws_access_key_id\": \"my_aws_access_key_id\"}'\n  SKIP ALL ERRORS\n  INTO TABLE yellow_trips_staging\n  FIELDS TERMINATED BY ','\n  (vendor_id,pickup_datetime,dropoff_datetime,passenger_count,trip_distance,rate_code,store_and_fwd_flag,PULocationID,DOLocationID,payment_type,fare_amount,extra,mta_tax,tip_amount,tolls_amount,improvement_surcharge,total_amount,@,@); Now, we can start the pipelines with: USE nyc_taxi;\nSTART PIPELINE pipeline_yellow_200901_201412;\nSTART PIPELINE pipeline_yellow_201501_201606;\nSTART PIPELINE pipeline_yellow_201607_201612; Verify that your pipelines are running. USE nyc_taxi;\nSHOW PIPELINES; You can view the files in the batch as identified by BATCH_ID in the following: USE nyc_taxi;\nSELECT * FROM information_schema.PIPELINES_FILES WHERE DATABASE_NAME = 'nyc_taxi'; Parallel Loading with SingleStore Pipelines To determine the number of files in a pipeline batch, SingleStore uses the database partition count. SHOW PARTITIONS on nyc_taxi; The distinct ordinal count of master partitions reflects the total number of database partitions. Load Skew Smaller sized files in a pipeline batch will load faster than larger files. Only when all files in a batch are successfully loaded in parallel, does the next pipeline batch begin. For the NYC yellow taxi trip data set, the monthly files vary greatly in size, as great as 210MB. In other words, there will be a degree of load skew. The pipeline with the most files to process is pipeline_yellow_200901_201412. It processes 6 years of monthly data, which is 72 files. A database sized with 32 partitions will create three pipeline batches for a pipeline with 72 files: SELECT PIPELINE_NAME, BATCH_ID, count(*) as File_Count\nFROM information_schema.PIPELINES_FILES\nWHERE DATABASE_NAME = 'nyc_taxi'\nGROUP BY PIPELINE_NAME, BATCH_ID\nORDER BY PIPELINE_NAME, BATCH_ID; To view the overall batch time in minutes, the total number of rows, and rows per second, execute the following query: SELECT Round((SUM(v1.BATCH_TIME) / 60),2) as Total_Minutes, FORMAT(SUM(v1.BATCH_ROWS_WRITTEN),0) as Total_Rows\n  ,FORMAT(SUM(v1.BATCH_ROWS_WRITTEN)/ (SUM(v1.BATCH_TIME)),3) as Rows_Per_Sec\nFROM (\n  SELECT DISTINCT t1.BATCH_ID, t1.BATCH_TIME, t1.BATCH_ROWS_WRITTEN\n  FROM INFORMATION_SCHEMA.PIPELINES_BATCHES as t1\n  WHERE t1.BATCH_STATE = 'Succeeded' AND t1.DATABASE_NAME = 'nyc_taxi'\n ) as v1 The results for loading all the yellow taxi data into a large sized cluster in SingleStore are: Total_Minutes = 23.94 Total_Row = 1,308,985,065 Rows_Per_Sec = 911,305.258 Now that the load is complete, we can stop the pipelines. USE nyc_taxi;\nSTOP PIPELINE pipeline_yellow_200901_201412;\nSTOP PIPELINE pipeline_yellow_201501_201606;\nSTOP PIPELINE pipeline_yellow_201607_201612;\nSHOW PIPELINES; From Staging to Destination With 1.3 billion rows, there will be a few load errors. We can view the files with the most errors with the following query: SELECT v1.BATCH_SOURCE_PARTITION_ID, v1.ERROR_CODE, COUNT(v1.ERROR_ID) as Total_Errors\nFROM (\n  SELECT BATCH_SOURCE_PARTITION_ID,ERROR_CODE,ERROR_ID\n  FROM information_schema.PIPELINES_ERRORS\n  ) as v1\nGROUP BY v1.BATCH_SOURCE_PARTITION_ID\nORDER BY total_errors DESC; There are two files with around 650 row-related errors out of 1.308 billion rows. Of course, there are a few issues with the data itself. There are rows with 0000-00-00 00:00:00 datetimes and rows with meangliness longitudes and latitudes. We can clean these up with some basic DELETE and UPDATE statements that will take less than 20 seconds to complete. USE nyc_taxi;\nDELETE FROM yellow_trips_staging WHERE pickup_datetime = '0000-00-00 00:00:00' AND dropoff_datetime = '0000-00-00 00:00:00';\nUPDATE yellow_trips_staging SET pickup_longitude = 0.0, pickup_latitude = 0.0 WHERE (pickup_longitude < -180.00 OR pickup_latitude > 90.00);\nUPDATE yellow_trips_staging SET pickup_longitude = 0.0, pickup_latitude = 0.0 WHERE (pickup_longitude > 180.00 OR pickup_latitude < -90.00);\nUPDATE yellow_trips_staging SET dropoff_longitude = 0.0, dropoff_latitude = 0.0 WHERE (pickup_longitude = 180.00 OR  pickup_latitude = 90.00);\nUPDATE yellow_trips_staging SET dropoff_longitude = 0.0, dropoff_latitude = 0.0 WHERE (pickup_longitude = -180.00 OR  pickup_latitude = -90.00);\nUPDATE yellow_trips_staging SET dropoff_longitude = 0.0, dropoff_latitude = 0.0 WHERE (dropoff_longitude < -180.00 OR dropoff_latitude > 90.00);\nUPDATE yellow_trips_staging SET dropoff_longitude = 0.0, dropoff_latitude = 0.0 WHERE (dropoff_longitude > 180.00 OR dropoff_latitude < -90.00);\nUPDATE yellow_trips_staging SET dropoff_longitude = 0.0, dropoff_latitude = 0.0 WHERE (dropoff_longitude = 180.00 OR dropoff_latitude = 90.00);\nUPDATE yellow_trips_staging SET dropoff_longitude = 0.0, dropoff_latitude = 0.0 WHERE (dropoff_longitude = -180.00 OR dropoff_latitude = -90.00); With the data cleaned up, we are now ready to copy the data into our destination table. As you may recall, the DDL for yellow_trips table specifies the pickup and dropoff point as a SingleStore geographypoint data type, enabling us to write fast geospatial queries moving forward. USE nyc_taxi;\nINSERT INTO yellow_trips\nSELECT vendor_id, pickup_datetime,year(pickup_datetime) as pickup_year, month(pickup_datetime) as pickup_month, day(pickup_datetime) as pickup_day\n, week(pickup_datetime) as pickup_week, time(pickup_datetime) as pickup_time, dropoff_datetime, year(dropoff_datetime) as droppoff_year\n, month(dropoff_datetime) as droppoff_month, day(dropoff_datetime) as droppoff_day, week(dropoff_datetime) as droppoff_week, time(dropoff_datetime) as droppoff_time\n, GEOGRAPHY_POINT(pickup_longitude , pickup_latitude) as pickup_pt, GEOGRAPHY_POINT(dropoff_longitude, dropoff_latitude) as dropoff_pt\n, PULocationID, DOLocationID, passenger_count, trip_distance, rate_code, store_and_fwd_flag, payment_type, fare_amount, surcharge, extra, mta_tax\n, tip_amount , tolls_amount, improvement_surcharge, total_amount\nFROM yellow_trips_staging; Finally, we’ll clean up the staging table and update the statistics for the yellow_trips table. USE nyc_taxi;\nTRUNCATE TABLE yellow_trips_staging;\nANALYZE TABLE yellow_trips;\nOPTIMIZE TABLE yellow_trips; Now, we’re ready to start analyzing the data! Data Science Inspiration For those looking for inspiration about what and how to analyze the NYC yellow taxi data, take a look at Todd W. Schneider’s now famous blog “ Analyzing 1.1 Billion NYC Taxi and Uber Trips, with a Vengeance ”. As Todd writes, “taken as a whole, the detailed trip-level data is more than just a vast list of taxi pickup and drop off coordinates: it’s a story of New York.” Next Steps The SingleStore story is one of relentless innovation and brilliant engineering. Just five years ago, SingleStore posted “ Loading half a billion records in 40 minutes ”. That blog details how to chunk up manufactured data into 2000 equally sized files in order to load 500 million rows into SingleStore on a machine with 512GB memory and 64 cores. To preview the next chapter in the amazing SingleStore story that includes an 80X improvement in columnstore query execution and AI similarity matching with SIMD functions, sign up for the SingleStore Beta .", "date": "2017-08-03"},
{"website": "Single-Store", "title": "secure-real-time-mission-critical-analytics", "author": ["Mike Mohler"], "link": "https://www.singlestore.com/blog/secure-real-time-mission-critical-analytics/", "abstract": "Data today moves in extremely large volumes at light speed all while requiring high levels of protection on the content. Traditional database and data warehouse technologies are unable to handle these workloads without the need for expensive hardware appliances. The NoSQL and Hadoop-based systems are very complex and unable to support real-time analysis. A new way to process fast-moving, structured content is needed to achieve this level of scale and security. Why not a highly secure and extreme scalable SQL-based approach? Imagine the volume of telemetry data that is required for processing a swarm of hundreds of mini-drones. This data would be rich with geospatial, video, and metadata content. What if we needed to use this real-time data as input to a predictive analytics behavior model to determine the patterns of life in a specific geographic location? Maybe this content is collected from an area of interest surrounding a world event, such as the Olympics. Real-time dashboard applications and who has access to those dashboards need to be locked down to your most trusted analysts. The content needs to be highly available and remain confidential. This hypothetical use case requires a technology fabric that can ingest, transform, model, store, analyze, and secure data in terabyte and petabyte scale in real time. Welcome to SingleStore. SingleStore is a purpose-built data management system for high volume workloads and provides a fast and secure platform for mission-critical analytics. The platform provides a data pipeline framework that allows real-time ingest, transform, model, and persistent storage of streaming content. A direct connection to a distributed message queue system such as Kafka can be made. Data in flight can be transformed with a distributed processing engine such as Apache Spark to run your machine learning algorithms and store directly in the engine. The database is then locked down to secure activity, content, and real-time access to your most trusted administrators, developers, and analysts. The key security tenants behind SingleStore are: Separation of duties to protect against rogue administrators and controlled access A 360° view of all database activity through full audit logging Database vault functionality thru Strict mode User authentication and encryption for data in transit and on disk Enforced password protection Highly available data thru level 2 redundancy and cluster replication With SingleStore, high performance at large scale has never been more achievable. Query speeds range in hundreds of millions to over a billion data points per second per core on four core commodity hardware. With this capability for example, Thorn can compare facial images in real time using linear algebra operations on vectors to match image fingerprints using a basic SQL query to SingleStore. The architecture behind SingleStore is based on large scale parallelism, data compression, in-memory processing, vectorization of queries, compiled queries, and query optimization. Given these architecture principles, SingleStore is able to run in a secure configuration minimizing impact on performance. The guiding principles behind SingleStore Enterprise Security are based upon confidentiality, integrity, and availability. In conclusion, SingleStore, a real-time data warehouse, is a platform for building your highly secure systems requiring speed-of-thought analytics on streaming data into your organization. No longer does your organization need to sacrifice security over performance. To learn more about how SingleStore is keeping real-time analytics secure, come chat with us next week at the Department of Defense Intelligence Information System Worldwide Conference at booth 634.", "date": "2017-08-09"},
{"website": "Single-Store", "title": "real-time-streaming-analytics-and-visualization-with-memsql-and-zoomdata", "author": ["Gene Miguel"], "link": "https://www.singlestore.com/blog/real-time-streaming-analytics-and-visualization-with-memsql-and-zoomdata/", "abstract": "We regularly host meetups at SingleStore headquarters as a way to share what we have been working on, connect with the community, and get in-person feedback from people like you. We also invite partners and customers to join us as well. Recently, we had the pleasure of hosting a meetup with Zoomdata, where we shared two presentations on real-time streaming, analytics, and visualization using SingleStore and Zoomdata. The first presentation “Streaming in the Enterprise with SingleStore” is presented by SingleStore engineer, Neil Dahlke. Neil builds analytical and operational tools for SingleStore and our customers. In this talk, Neil discusses the SingleStore system design, how to implement SingleStore into a high-performance streaming architecture, use cases, and how to leverage Zoomdata for real-time visualization. The second presentation in the video is “Streaming Analytics on SingleStore,” presented by Zoomdata Senior Director of Product Marketing, Ian Fyfe. Ian discusses how traditional business intelligence and analytics tools were designed for a world of periodically updated static sources, and often require duplication of data into data warehouses or proprietary data stores, and lack streaming query capabilities. He provides an overview of how modern streaming visualization and analytics such as Zoomdata solves all of these issues to enable fast, usable and visually appealing analysis and insights on streaming sources such as SingleStore. You can watch both presentations in their entirety here .", "date": "2017-08-18"},
{"website": "Single-Store", "title": "memsql-journey-to-running-docker-at-scale", "author": ["Carl Sverre"], "link": "https://www.singlestore.com/blog/memsql-journey-to-running-docker-at-scale/", "abstract": "One of the main themes at DockerCon 2017 was the challenge of migrating legacy applications to containers. At SingleStore, we’re early adopters. We are already into our third year of running Docker at scale in production for our distributed software testing regime, where the performance, isolation, and cost benefits of containers are very attractive. The Challenge Before I take you through our journey to containers, let me start by outlining some of the general challenges of testing a distributed database like SingleStore. Our solutions are built for real-time data warehousing. Databases are really difficult to test — especially when they are designed to be distributed, real-time, and scalable. At SingleStore, we have millions of unique queries to test, highly variable runtimes, and some tests take hours of 100 percent CPU usage. We have over 10,000 unique tests, not to mention the number of test transformations, which may multiply that number by one hundred again. Our tests also require gigabytes of RAM and multiple cores. That’s the kind of scale you have to think about for testing a platform like SingleStore. We also ran into some interesting new testing challenges as our product can take advantage of Intel’s unique AVX technology and vectorization — to run orders of magnitude more SQL queries and speed up the sequences per cycle. These modern architectures bring awesome advantages, but they can also add to testing scenarios. We started with off-the-shelf, but once you see all the things we’re doing, you can’t just throw it onto a common test platform. Commercial testing solutions are not designed for distributed applications. We paid our dues with a lot of experimentation and DIY. Creating Psyduck We started our first test cluster about five years ago, when I first arrived at SingleStore. We named it Psyduck after the Pokémon character. Like testing, Psyduck always has a headache.  For me, it was a really fun effort to be a part of the test program because it’s key to how we continue to evolve SingleStore while maintaining its reliability. Initially we had a mixture of home grown, bare metal boxes in the office. There were 25 boxes that were just basically Dell desktop machines, manually managed. Additionally, we had a bare-bones Amazon EC2 presence — for bursty job requirements. That was our initial scaling strategy, manually managed VMs to take on additional load. From there we looked at operationalizing the whole stack. First we invested in operationalizing VMs on bare metal. We took the 25 machines, cleared them, and built an OpenStack cluster. We scaled that up to about 60 machines on-premises. This allowed us to eliminate EC2, which saved us a huge amount of money monthly. But as we scaled that cluster we experienced a lot of pain and complexity on OpenStack. So we took a portion of the cluster and ran Eucalyptus instead. That ended up being interesting, but not very mature compared to OpenStack, and we were a little burned out on VMs at that point with the infrastructure. We learned about Docker about three and a half years ago when Docker was still called DotCloud. We tested it out and prototyped what Psyduck could look like with containers. Containers matched our use case really well — software testing is basically a bunch of short lived, ephemeral jobs, and you need a way to run hundreds of thousands of tests per day, in an isolated environment. Docker gave us the ability to do that and spin up on the order of seconds (rather than the overhead of VMs in minutes). Basically we saw that Docker would give us a way to scale Psyduck in an operationally friendly way. From there, we took the plunge and over a couple of weeks we rebuilt Psyduck from the ground up using Docker. And we’ve been running on that base architecture ever since. For the last three years, we’ve been running with Docker and we built a home grown scheduling system, because at that time Kubernetes and Apache Mesos didn’t exist. Creating Pokedex We also wrote our own key value storage system that we call Pokedex — think bare bones S3, bare bones HDFS. We actually took the on-premises version of the Docker registry, and wrote an adapter for Pokedex to provide the parallelism we required during image distribution. We have over 150 test machines, physical Dell servers each running many containers. We have to deliver 5 GB of data to every machine per test run, so there’s tons of data to send around all at once. So Pokedex runs on a number of machines and takes advantage of leading performance optimizations for delivering files at scale. Pokedex backing the registry allowed us to deliver Docker images in the order of minutes, whereas with the old VM architecture we had to build out an arcane architecture based on torrets and other crazy technologies to deliver large VM image files. We’re also running appliances from a startup called Diamanti to help run the Psyduck control plane. Today, Diamanti makes use of Kubernetes, and over time we plan to expand the use of Kubernetes across our entire cluster. We expect our use of Kubernetes to be ideal for orchestrating our container environment compared to our initial homegrown scheduling. What We Learned We’re very happy with the outcome of this journey. The container abstraction is solid for performance, isolation, ephemerality — everything that matters in software unit testing. We don’t care about persistence: the test runners come up as containers, do a bunch of work, write out to a distributed file system, and disappear. This abstraction allows us to not worry about what happens to containers, only that they complete their job and then go away. On the down side, it’s really the DIY nature of tooling today for a lot of container production scenarios. In the early days, we hit issues with isolation across networking and storage. Over time, those things have become better and we have learned how to deal with Docker to make sure we eliminate noisy neighbors on the machines. A fundamental challenge that remains with containers is related to the Linux namespace abstraction — if you run a container as privileged, that container can do things on a machine that applies to every other container. We want to be able to give our engineers the ability to do what they want with our machines, but with containers we have to remove some of that functionality. With VMs they could do anything — detach or add network devices. With VMs the test is completely isolated and we have very strong controls. With Docker, it’s the opposite — you have to tell engineers what they can and can’t do. Linux namespaces are gaining better semantics over time, but it’s still a challenge. It’s also tricky debugging and testing software running in containers. Due to PID mapping normal Linux tools like perf or GDB don’t know how to properly work with the containerized software. This results in complex or impossible debugging scenarios that can make engineers very frustrated. In the end, Psyduck helped us alleviate most of the headaches around testing software at our scale. Running containers on bare metal, we can afford to put our own products through the most demanding testing regime in our industry. After all, we’re the real-time data warehouse for Akamai, Kellogg, Pinterest, Uber, and other leading enterprises. Our customers run some of the largest distributed systems in the world. They know they can rely on us to keep their own customers happy. For more technical information, see our previous blog on Psyduck, or learn more from Matt Asay and this YouTube video .", "date": "2017-07-26"},
{"website": "Single-Store", "title": "five-sessions-to-attend-at-kafka-summit-san-francisco", "author": ["Nicole Nearhood"], "link": "https://www.singlestore.com/blog/five-sessions-to-attend-at-kafka-summit-san-francisco/", "abstract": "Kafka Summit San Francisco brings together thousands of companies across the globe that build their businesses on top of Apache Kafka. The developers responsible for this revolution need a place to share their experiences on this journey. This year, Kafka Summit San Francisco will offer even more breakout sessions led by Kafka subject matter experts and top Kafka customers. This informative mixture of lectures, demonstrations, and guest speakers is geared towards keeping attendees informed on technical content, customer stories, and new launch announcements. SingleStore is exhibiting in the Sponsor Expo, so stop by our kiosk #109 to view a demo and speak with our subject matter experts. Here are our top recommended sessions for you to attend at the event. Efficient Schemas in Motion with Kafka and Schema Registry 10:30 am – 11:10 am, Pipelines Track Pat Patterson, Community Champion, StreamSets Inc. Apache Avro allows data to be self-describing, but carries an overhead when used with message queues, such as Apache Kafka. Confluent’s open source Schema Registry integrates with Kafka to allow Avro schemas to be passed ‘by reference’, minimizing overhead, and can be used with any application that uses Avro. Learn about Schema Registry, using it with Kafka, and leveraging it in your application. Kafka Stream Processing for Everyone 12:10 pm – 12:50 pm, Streams Track Nick Dearden, Director of Engineering, Confluent The rapidly expanding world of stream processing can be confusing and daunting, with new concepts to learn (various types of time semantics, windowed aggregate changelogs, and so on) but also new frameworks and programming models. Multiply this by the operational complexities of multiple distributed systems and the learning curve is steep indeed. Come hear how to simplify your streaming life. From Scaling Nightmare to Stream Dream: Real-time Stream Processing at Scale 1:50 pm – 2:30 pm, Pipelines Track Amy Boyle, Software Engineer, New Relic On the events pipeline team at New Relic, Kafka is the thread that stitches our micro-service architecture together. We receive billions of monitoring events an hour, which customers rely on us to alert on in real-time. Facing a ten fold+ growth in the system, learn how we avoided a costly scaling nightmare by switching to a streaming system, based on Kafka. We follow a DevOps philosophy at New Relic. Thus, I have a personal stake in how well our systems perform. If evaluation deadlines are missed, I loose sleep and customers loose trust. Without necessarily setting out to from the start, we’ve gone all in, using Kafka as the backbone of an event-driven pipeline, as a datastore, and for streaming updates to the system. Hear about what worked for us, what challenges we faced, and how we continue to scale our applications. Kafka Connect Best Practices – Advice from the Field 2:40 pm – 3:20 pm, Pipelines Track Randall Hauch, Engineer, Confluent This talk will review the Kafka Connect Framework and discuss building data pipelines using the library of available Connectors. We’ll deploy several data integration pipelines and demonstrate: best practices for configuring, managing, and tuning the connectors tools to monitor data flow through the pipeline using Kafka Streams applications to transform or enhance the data in flight. “One Could, But Should One?”: Streaming Data Applications on Docker 5:20 pm – 6:00 pm, Use Case Track Nikki Thean Staff Engineer, Etsy Should you containerize your Kafka Streams or Kafka Connect apps? I’ll answer this popular question by describing the evolution of streaming platforms at Etsy, which we’ve run on both Docker and bare metal, and what we learned on the way. Attendees will learn about the benefits and drawbacks of each approach, plus some tips and best practices for running your Kafka apps in production.", "date": "2017-08-25"},
{"website": "Single-Store", "title": "memsql-6-product-pillars-and-machine-learning-approach", "author": ["Gary Orenstein"], "link": "https://www.singlestore.com/blog/memsql-6-product-pillars-and-machine-learning-approach/", "abstract": "SingleStore DB 6 Pillars and Machine Learning Approach Want to try SingleStore DB 6? Click here to get started. Prefer the Release Notes? They are here . Today marks another milestone for SingleStore as we share the details of our latest release, SingleStore DB 6. This release encapsulates over one year of extensive development to continue making SingleStore the best database platform for real-time analytics with a focus on real-time data warehouse use cases. Additionally, SingleStore DB 6 brings a range of new machine learning capabilities to SingleStore, closing the gap between data science and operational applications. Product Pillars SingleStore DB 6 has three foundational pillars: Extensibility Query Performance Enhanced Online Operations Let’s explore each of these in detail. Extensibility Extensibility covers the world of stored procedures, user defined functions (UDFs), and user defined aggregates (UDAs). Together these capabilities represent a mechanism for SingleStore to offer in-database functions that provide powerful custom processing. For those familiar with other databases, you may know of PL/SQL (Procedural Language/Structured Query Language) developed by Oracle, or T-SQL (Transact-SQL) jointly developed by Sybase and Microsoft. SingleStore has developed its own approach to offering similar functions with MPSQL (Massively Parallel Structured Query Language). MPSQL takes advantage of the new code generation that was implemented in SingleStore DB 5. Essentially we are able to use that code generation to compile MPSQL functions. Specifically we implement native machine code for stored procedures, UDFs, and UDAs in-lined into the compiled code that we generate for a query. Long story short, we expect MPSQL to provide a level of peak performance not previously seen with other databases’ custom functions. SingleStore extensibility functions are also aware of our distributed system architecture. This innovation allows for custom functions to be executed in parallel across a distributed system, further enhancing overall performance. Benefits of extensibility include the ability to centralized processes in the database across multiple applications, the performance of embedded functions, and the potential to create new machine learning functions as detailed later in this post. Query Processing Performance SingleStore DB 6 includes breakthrough improvements in query processing. One area is through operations on encoded data. SingleStore DB 6 includes dictionary encoding, which can translate data into highly compressed unique values that can then be used to conduct incredibly fast scans. Consider the example of a public dataset about every airline flight in the United States from 1987 until 2015, as outlined in our blog post Delivering Scalable Self Service Analytics. With this dataset SingleStore can encode and compress the data, allowing for extremely rapid scans of up to 1 billion rows per second per core. SingleStore DB 6 also makes use of improvements to the Intel advancements with Single Instruction, Multiple Data (SIMD). This technique allows the CPU to complete multiple data operations in a single instruction, essentially vectorizing and parallel processing the query. The benefits of these query processing advancements include having a detailed data view without needing to pre-process the data. This further allows for interactive analysis on raw, unaggregated data, providing the most up-to-date and accurate query results possible. Enhanced Online Operations To power mission critical applications, data platforms must be online all the time, and with SingleStore DB 6 we have enhanced our ability for SingleStore to operate online. The benefits of these improvements include more sophisticated monitoring and recovery, easier application development, and improved overall availability. Machine Learning and SingleStore DB 6 SingleStore DB 6 helps close the gap between machine learning and operational applications in three areas: Built-in machine learning functions Real-time machine learning scoring Machine learning in SQL with extensibility Built-in Machine Learning Functions SingleStore DB 6 includes new machine learning functions like DOT_PRODUCT, which can be used for real-time image recognition but also for any application requiring the comparison of two vectors. While this function itself is not new in the world of machine learning, SingleStore now delivers this function within its distributed SQL database, enabling an unprecedented level of performance and scale. For more information, check out this blog post , An Engineering View on Real-Time Time Machine Learning. Real-Time Machine Learning Scoring SingleStore includes the ability to manage real-time data pipelines with custom transformations at ingest. This transformation can also deliver the execution and scoring using a machine learning model. For example, you may choose to take a machine learning model from SAS and export it using PMML, the predictive modeling markup language. This allows real-time scoring on ingest and co-locating the raw data and the instant score next to each other in the same row in the same table. This simple structure, sets a foundation for easy predictive analytics. Enabling Machine Learning in SQL with Extensibility The new SingleStore extensibility functions also enable a new approach to machine learning directly in SQL. This can dramatically shorten the gap between data science and production applications as operations occur on the live data, and models can be trained and updated to incorporate and reflect the most recent data. We recently showcased an example of this with k-means clustering by simply using native SQL and SingleStore. You can see the presentation here on Slideshare. Taking Machine Learning Real-Time With the new features of SingleStore DB 6 including extensibility and query performance, we expect more machine learning applications to incorporate SingleStore as the persistent data store. The SingleStore architecture is well suited to work in conjunction with other machine learning systems, and real-time data pipelines. For example, SingleStore includes: A distributed, scale out architecture well suited to performance and large scale workloads An open source SingleStore Spark Connector for high-throughput, highly-parallel, and bidirectional connectivity to Spark Native integration with Kafka message queues including the ability to support exactly-once semantics Full transactional SQL semantics so you can build production applications for the front lines of your business Together, we see these capabilities as foundational for real-time machine learning workloads, and we invite you to try the latest version of SingleStore today at singlestore.com/free .", "date": "2017-09-26"},
{"website": "Single-Store", "title": "oreilly-ai-book", "author": ["Gary Orenstein"], "link": "https://www.singlestore.com/blog/oreilly-ai-book/", "abstract": "We are enchanted by the possibility of digital disruption. New computing approaches, from cloud to artificial intelligence and machine learning, promise new business models and untold efficiencies. We are closing the gap between science fiction and business operations. A Quick Look Back Let’s take a quick look back at data processing, and then come back to the industry frontier. It started with data and the place to put it, which became the database. Then came a desire to understand the data through analytics, and that spawned the introduction of data warehouses. Data warehouses are a form of databases more suited to analytics. Ultimately databases and data warehouses are both datastores, and within the data industry these terms sometimes merge. The Current Data Landscape Today, the transactional operations of many business applications are in fine shape. In particular, transactions driven by human or business-to-business interactions, do not require significant computing resources. Often core transactional systems, or databases, can benefit from faster analytics, which do not need to disrupt the transactional workflow. For example, adding a real-time data warehouse to the architecture brings instant insights to drive business decisions. A New Class of Transaction But there is a new class of modern transactions ready to handle data ranging from IoT sensor information to website traffic logs to global financial reporting. The volume of transactions drives a need beyond what a traditional database or data warehouse can accept. Enter the real-time data warehouse. A Modern, Real-Time Data Warehouse Entirely new systems are needed to capture modern transactional, event, and streaming data. A real-time data warehouse fits this need by being able to ingest and persist data in real time while simultaneously serving low latency analytic queries to large numbers of simultaneous users. In dealing with such large volumes of data, represented by all forms of ‘modern transactions’, a real-time data warehouse also needs the ability to use machine learning to help harness insights from a vast array of live inputs. Fast Machine Learning Built-In Incorporating machine learning with your real-time data warehouse leads to a powerful, simplified data infrastructure. Getting there is straightforward. Step 1 – Identify a modern transactional workload This could be any volume of data that pushes the limits of your organization’s existing data systems. Even if you pull data from multiple sources, you want to hone your skill set at rapidly ingesting large volumes. Good examples might be data coming from a message queue such as Kafka, or coming in from Hadoop or S3. Step 2 – Derive immediate insight with SQL With a real-time data warehouse, you can ingest data, including transactional data, and immediately access that data with SQL. This provides a powerful, efficient, and universal approach to data exploration. It further opens access to a wide range of technical and business analysts at any company who are familiar with SQL. Step 3 – Delve into ML and AI Certain real-time data warehouses, such as SingleStore, have machine learning capabilities, including: DOT _ PRODUCT to compare vectors directly in SQL K-means clustering using extensibility Bi-directional, high throughput, highly parallel connector to Apache Spark By incorporating these capabilities within the real-time data warehouse, organizations can dramatically simplify data architectures and provide wide access to real-time information for faster critical decisions. Today, we launched our latest O’Reilly book, Data Warehousing in the Age of Artificial Intelligence . To learn more, check out the snapshot below and download our latest ebook. What’s Inside? Chapter 1: The Role of a Modern Data Warehouse in the Age of AI Enterprises are constantly collecting data. Having a dedicated data warehouse offers rich analytics without affecting the performance of the application. A modern data warehouse can support efficient query execution, along with delivering high performance transactional functionality to keep the application and the analysis synchronized. Chapter 2: Framing Data Processing with ML and AI The world has become enchanted with the resurgence in AI and ML to solve business problems. All of these processes need places to store and process data. For modern workloads, we have passed the monolithic and moved on to the distributed era, and we can see how ML and AI will affect data processing itself. Chapter 3: The Data Warehouse Has Changed Decades ago, organizations used transactional databases to run analytics. Then applications evolved to collect large volumes and velocity of data driven by web and mobile technologies. Recently, a new class of data warehouse has emerged to address the changes in data while simplifying the setup, management, and data accessibility. Chapter 4: The Path to the Cloud There is no question that, whether public or private, cloud computing reigns as the new industry standard. When considering the right choices for cloud, data processing infrastructure remains a critical enablement decision. Chapter 5: Historical Data All of your business’s data is historical data; it represents events that happened in the past. In the context of your business operations, “real-time data” refers to the data that is sufficiently recent to where its insights can inform time sensitive decisions. Historical data itself might not be changing, but the applications, powered by models built using historical data, will create data that needs to be captured and analyzed. Chapter 6: Building Real-Time Data Pipelines Building any real-time application requires infrastructure and technologies that accommodate ultra-fast data capture and processing. A memory-optimized data warehouse provides both persistence for real-time and historical data as well as the ability to query streaming data in a single system. Chapter 7: Combining Real Time with Machine Learning Machine learning encompasses a broad class of techniques used for many purposes, and in general no two ML applications will look identical to each other. This is especially true for real-time applications, for which the application is shaped not only by the goal of the data analysis, but also by the time constraints that come with operating in a real-time window. Chapter 8: Building the Ideal Stack for Machine Learning An ML “stack” is not one dimensional. Building an effective ML pipeline requires balance between two natural but competing impulses – use existing technology or build something yourself. Although many ML algorithms are not (or not easily) parallelizable, this is only a single step in your pipeline. Chapter 9: Strategies for Ubiquitous Deployment As companies move to the cloud, the ability to span on-premises and cloud deployments remains a critical enterprise enabler. In this chapter, we take a look at hybrid cloud strategies. Chapter 10: Real-Time Machine Learning Use Cases Real-time data warehouses help companies take advantage of modern technology and are critical to the growth of ML, big data, and AI. Companies looking to stay current need a data warehouse to support them. If your company is looking to benefit from ML and AI and needs data analytics in real time, choosing the correct data warehouse is critical to success. Chapter 11: The Future of Data Processing for Artificial Intelligence Maximizing value from ML applications hinges not only on having good models, but on having a system in which the models can continuously be made better. The reason to employ data scientists is because there is no such thing as a self-contained and complete ML solution. In the same way that the work at a growing business is never done, intelligent companies are always improving their analytics infrastructure.", "date": "2017-08-31"},
{"website": "Single-Store", "title": "migrating-from-traditional-databases-to-the-cloud", "author": ["Gary Orenstein"], "link": "https://www.singlestore.com/blog/migrating-from-traditional-databases-to-the-cloud/", "abstract": "The Current Organizational Data Dilemma Today, many companies have years of investment in traditional databases from Oracle, SAP, IBM, Microsoft and others. Frequently these databases and data warehouses have long outlived their ability to cost-effectively perform and scale. These solutions also offer mixed options when it comes to cloud deployments. Over the past decade, data-driven organizations looked to new methods such as Hadoop and NoSQL to solve data challenges. Hadoop has proven to be a good place to store data, but a challenging place for production data workflows. Gartner noted at a recent conference that only 17 percent of Hadoop deployments are in production in 2017. Also adding a ‘SQL layer” on top of the Hadoop Distributed File System is not a path for building a robust, transaction-capable datastore. Similarly, NoSQL is a great place to store simple key-value pairs, but a challenging place for analytics. A prominent Gartner analyst notes, “The term NoSQL, in my opinion is meaningless and useless… As the term NoSQL refers to a language, the better term for these DBMSs would be non-relational.” Non-relational, of course, can make sophisticated analytics tricky. Given these lackluster alternatives, organizations need a modern, scalable solution for database and data warehouse workloads that frees them to deploy flexible data architectures of their choosing. Such a solution would also include seamless paths to the cloud across all workloads. Dilemma Snapshot Organizations are awash in on-premises database and data warehouse deployments from traditional vendors These solutions are often coupled with technology lock-in, especially in the form of hardware appliances and the restriction to run in specified environments; cloud options are limited Many traditional solutions have massive complexity, including hundreds of thousands of lines of custom code due to shortcomings of legacy technology Shortcomings resulted in bolt-on solutions to address scale, speed, and concurrency issues Data Architecture Challenges Ahead Handling data securely, cost-effectively and at scale requires effort across multiple areas. Cost Reduction Organizations continually seek to reduce license, maintenance and operational costs of data solutions. Legacy offerings that require specific hardware appliances no longer meet the needs of modern workloads. Contract and Licensing Complexity Complex contract and licensing models for traditional solutions make them impractical for organizations aiming to move quickly, adapt to changing conditions, and drive results with new data streams. Performance at Scale Hardware-centric appliances or single-server solutions that do not automatically distribute the workload are not capable of delivering performance at scale. Security Requirements With data at the core of every organization, security remains critical and databases and data warehouses must provide a comprehensive and consistent security model across deployments from on-premises to the cloud. Cloud Choices As the world goes cloud, organizations need the ability to quickly and safely move data workflows and applications to a cloud of their choice. These migrations should happen seamlessly and without limitations. An option to choose clouds (public, private, hybrid, and multi-cloud) should be on the table at all times. Customer Use Cases Here’s how organizations have moved from Oracle while retaining options for on-premises and the cloud. Moving from Oracle to SingleStore instead of a Data Lake In this first use case we explore a large financial institution moving to SingleStore after trying unsuccessfully to move data applications from traditional datastores to Hadoop. Need The overarching need was driven by a desire to move off of legacy systems, such as Oracle, Netezza, SybaseIQ, and Teradata. These systems were not achieving the required levels of performance, and were becoming painfully expensive for the organization to support. Exploration The data team at the bank initially attempted to migrate to a Hadoop-based data lake. Unfortunately, data application migration was taking approximately one year per application. Much of the delay was due to requirements to retrain relational database management system (RDBMS) developers on new languages and approaches such as MapReduce, and using SQL layers that “kind of” provide SQL functionality, but frequently fall short in terms of SQL surface area and robustness. Solution Adding SingleStore as a data application destination, with complete coverage for INSERT, UPDATE, and DELETE commands, reduced application migration time down to one month. This new solution using SingleStore now supports one of the largest data warehouses in the bank. Additionally, the workflows that were migrated to SingleStore are completely cloud-capable. Moving from Oracle to SingleStore Instead of Exadata A large technology company faced a performance need for its billing system. It had plenty of Oracle experience in-house, but the Oracle solutions proposed were priced prohibitively and could not deliver the necessary performance. Need As part of an expansion plan for its billing system, the large technology company sought an effective and affordable solution to expand from 70,000 to 700,000 transactions per second. Exploration The team briefly considered a solution with Oracle Exadata which topped out at 70,000 transactions per second. Part of this is due to the fact that Oracle ingests data using the traditional I/O path, which cannot keep up with the world’s fastest workloads. SingleStore on the other hand, using a distributed, memory-optimized architecture, achieved six million UPSERTs per second in initial trials. After server and network optimizations, that same solution is nearing 10 million UPSERTs per second. Solution Implementing SingleStore helped save millions in avoiding the high cost of Oracle solutions. UPSERTs provide computational efficiency compared to traditional INSERTs. For example, with UPSERTs, data is automatically pre-aggregated allowing for faster queries. Additionally, SingleStore configurations perform on low-cost-industry-standard servers, providing dramatic savings on hardware, and an easy path to move to the cloud. Moving from Oracle to SingleStore and Introducing Tableau A global fixed-income investment firm with nearly $500 billion in assets under management set a strategy to regain control of database performance and reduce its reliance on Oracle. Need The firm faced a dilemma with its existing Oracle configurations in that direct, end user queries negatively impacted performance. At times, queries from individual users impacted the database for the company as a whole. Concurrent with this performance challenge was an organizational desire to “not bet the farm on Oracle.” Exploration The team looked for a solution that would provide the scale and performance needed from a database perspective but also the ability to implement ‘query’ governance using a business intelligence tool. Solution Ultimately the team chose SingleStore due to its RDBMS foundation, and the ability to scale using a distributed, memory-optimized architecture. For business intelligence, the firm selected Tableau, and Tableau includes a native SingleStore connector. The organization now has a cloud-capable architecture that allows it to move to any public cloud of its choice at any time. Creating a Migration Plan Migrating from traditional databases and data warehouses takes planning, but more importantly it takes organizational effort and a will to move to new solutions. Here are several tips that can streamline your journey. Assessment You can begin by taking an inventory of database and data warehouse infrastructure, including systems from Oracle, SAP, Microsoft and IBM, as well as Netezza, Vertica, Greenplum and others. These products span the database to data warehouse spectrum. For products with a long history, aim to determine the degree of stickiness due to custom code. For example, Oracle stored procedures are widely used in many organizations and reliance on this custom code factors heavily in the ease of migration. Put yourself in a position to succeed by identifying older data applications that have struggled with performance or become painfully expensive. Planning Next, identify easy applications for migration. For example, consider the following popular use cases for quick wins, high success potential, low risk, rapid cost reduction, and immediate impact. Exadata Replacements Exadata replacements are appropriate in cases where customers have been required to buy Exadata for performance, but may not need the entire feature set. Similar reasoning can be applied to SAP HANA and IBM Db2. Large-scale Databases Often for real-time data warehousing solutions, customers prefer to consolidate many databases into a single deployment. This can be accomplished with a distributed system that scales well across dozens or hundreds of nodes. High-speed Ingest For newer workloads, especially those around internet-scale tracking, IoT sensors, or geospatial data collection, capturing all of the data is critical to the solution. Legacy datastores tend to struggle with these configurations whereas newer distributed architectures handle high speed ingest easily. Low-latency Queries Query performance is a perennial database and data warehouse requirement. However, many legacy solutions are not equipped to handle the real-time needs of modern workloads, in particular the ability to handle sophisticated queries on large datasets, especially when that dataset is constantly changing. Systems that can handle low latency queries on rapidly updating data serve modern workloads effectively. High Concurrency When data applications reach a level of adoption they face a blessing and a curse. On the one hand, continued adoption validates the application success. On the other hand, too many concurrent users can negatively impact performance. Data applications that demand a high level of user concurrency are solid candidates to move to a more powerful, memory-optimized distributed system. Using SingleStore and Oracle Together SingleStore fits data ecosystems well, as exemplified by several features. First, SingleStore offers users flexible deployments – whether it’s hybrid cloud, on-premises, VMs, or containers. Second, there are tools such as the SingleStore Spark Connector for high-speed, highly-parallel connectivity to Spark, and SingleStore Pipelines which lets you build real-time pipelines and import from popular datastores, such as HDFS, S3, and MySQL. And SingleStore is a memory-first engine, designed for concurrent data ingest and analytics. These ingredients make SingleStore a perfect real-time addition to any stack. Many customers combine SingleStore with traditional systems, in particular Oracle databases. SingleStore and Oracle can be deployed side-by-side to enhance scalability, distributed processing, and real-time analytics. SingleStore for Real-Time Analytics Data can be copied from Oracle to SingleStore using a data capture tool, and analytical queries can be performed in real time. SingleStore for Stream Processing with Kafka or Spark SingleStore can serve as the stream processing layer in front of an Oracle database. Use Apache Kafka or Apache Spark to help build real-time data pipelines. SingleStore as the High-Speed Ingest Layer Using SingleStore as the data ingest layer on top of an Oracle database allows ingest at in-memory speed. Key Considerations in Migrating from Oracle In particular when migrating from Oracle, the following situations require more time, planning and potential development work. It can be complex, but ultimately provide high reward. Be sure to dedicate enough of a planning cycle and focus on parallel deployments to ensure success. Databases with heavy reliance on stored procedures. While they can be migrated, the custom code requires time and energy to move. Databases embedded deep within Oracle ERP systems (e.g. Peoplesoft) can require more effort as they can be part of a locked-down, proprietary solution. Databases connected to current critical path infrastructure should be migrated after a successful approach focused on adding analytics to an existing application. Databases connected to applications with significant trigger logic. This may require logic to be re-written outside of SingleStore. The newest version of SingleStore includes extensibility across stored procedures, user-defined functions, and user-defined aggregates. In some cases, customers will be able to rework certain procedures into the SingleStore extensibility language MP/SQL for massively parallel SQL. Development, Testing, and Deployment Once a migration candidate has been identified, testing and development can begin. With a cloud-native database such as SingleStore, customers can develop and test on any public cloud including AWS, AWS GovCloud, Azure or Google. Or they can run development and testing within their own infrastructure. SingleStore provides complete flexibility to choose industry-standard hardware configurations that run anywhere. The minimum requirements for a SingleStore node are Linux, 4 cores, and 8GB of RAM. The rest is up to customer choice. A typical test and deployment process might take the following approach. Identify the data application for migration. Export the data set. Import the data to a new solution such as SingleStore and run sample queries Scale the solution using options in the cloud or on-premises Run additional queries for full coverage Connect the appropriate applications or business intelligence tools Refine and test the solution across all operational requirements Move from development and testing to pre-production Move to production deployments Understandably there may be many more steps to migrate specific applications. Making progress begins with the first successful migration. Further Reading Jumping the Database S-Curve The Real-Time Data Warehouse for Hybrid Cloud Key Considerations for a Cloud Data Warehouse SingleStore and Oracle: Better Together For more information please visit www.singlestore.com", "date": "2017-09-08"},
{"website": "Single-Store", "title": "designing-for-a-database-whats-beyond-the-query", "author": ["Bing Tang"], "link": "https://www.singlestore.com/blog/designing-for-a-database-whats-beyond-the-query/", "abstract": "Even the most technically-minded companies need to think about design. Working on a database product at a startup is no different. But this comes with challenges, such as figuring out how to implement the human-centered design methodology at a technical company, but also contribute to building a design process that everyone agrees with across the organization. This blog will detail how product design is done at SingleStore as well as highlight how to design enterprise products at a startup. How do we define Product Design? Product Design is the end-to-end process of gathering requests and ideating in order to hand-off pixel-perfect products and iterations. There are usually two different ways of dividing work: breaking down multiple design tasks into steps handled by different team members, or each designer taking ownership of a product, or a feature, and designing in a full-stack way. At SingleStore, we develop each product with the latter model, which requires constant and proactive engagement with other folks on the team. Because everything our users experience when interacting with our product should ideally be counted in the scope of ‘Product Design’; we wouldn’t ignore email templates, docs, the help center, the support chat widget and so on. By thinking about the product as a holistic ecosystem, we are able to capture the pain points both inside and beyond the stand-alone application. What is the process? Below are the steps we work through at SingleStore before our customers see any end product or piece of collateral. Kick-off Most of the product features are requested by product management (PM). PM gathers feedback directly from customers or other departments, such as customer success and sales. Designers are also encouraged to take initiative, and lead their own project if it’s valuable to the product as well as the business. To start the project, designers need to write a proposal doc and send it to key stakeholders. The stakeholders will evaluate the idea and decide if it should be put in the roadmap or not. Research Before jumping onto the design phase, we do research to validate the goal, gather technical requirements, explore the market, understand our customer, and analyze what we have done so far. Getting involved with the engineering team First, we need to talk with the engineering team to understand the technical principles, expectations, difficulties, constraints, and context. This helps us set a clear scope of what we can implement, or not, in the project. Study your customers and beyond Customer study covers a wide range of topics. Various tools and methodologies are used, such as interviews, surveys, and focus groups. Notice that customer study is different from usability testing, which will be mentioned in the ‘Testing’ section. While usability testing focuses on the ease of use of the product, the customer study is expected to answer more strategic questions and help us evaluate the direction of the product. (We use Trello to sort out massive notes from customers and and color-code them by characteristics, which could later help us identify different target groups.) Ideation Personally, it is the most exciting phase of design, since that’s where we get to synthesize everything we learned about the problem and try to land the creativity and empathy. By ideating various ideas in a low-fi stage, we push ourselves out to seek for more innovative solutions. It’s very easy to skip ideation and go straight to design, especially when ‘ship it’ becomes rule of thumb. It might even work, sometimes, which makes people underestimate the importance of ideation. Brainstorm We start the design phase from brainstorming. We usually start with listing features and components, drawing flow maps on the whiteboard, and sketching rapid wireframes to show the idea. We will also discuss the realistic side of the projects: how/when do you think it could be implemented; would it be better if we want an MVP; what would you like to be in the next round of iteration. This allows us to scope down properly while keeping the vision wide open. There are massive ways to find inspirations, including looking into industries that seems irrelevant with your own industry (see Analogous Market ). User Flow User flow is one of the most effective ways of communicating design ideas to other team members. We use flowcharts to represent each step the customer will experience. The chart could help the team, especially the engineers to fact check with the design team so that the backend and the experience design are aligned. (We keep user flow diagrams together with the interface design in the same Figma file so that it’s always easy to go back and check the design.) Spec To consolidate the idea as well as to communicate effectively with the team, we usually write down specs and send it out to the whole team for review, spanning PM, design, and engineering. This is important because we want to make sure that all team members involved are on the same page. It is also a great place to review the goal, the metrics, and how to move forward. Design Design is an iterative process. It requires patience and commitment to the process. Good ideas could come at the beginning or the very end; however, it is important to always have a holistic view of the timeline and the priority so that we won’t get lost in the pixels and fancy animations. Wireframe Though we also do wireframe during the brainstorm process, it is crucial to have more thorough wireframes to show a few design approaches. By using wireframe, both the designer and the reviewer could focus on the function, transition, and interaction, instead of all the pretty visual elements. Wireframe could also help cover different use cases, e.g. first-time user, returning user, multiple user permissions, and more. High Fidelity Mockup Hi-fi mockup is how the product will statically look. We use Figma to do hi-fi mockups, which allows us to effectively communicate with the front-end engineer while also collaboratively iterating on design. Prototype Once the hi-fi mockup is nearly locked down, we use InVision to show the interaction. This is an important step for design review since it will be the first time that other stakeholders will see the look and feel of the product and check the flow and information is correct. We also use HTML+CSS examples to show the visual effect. Writing There are two parts of writing – technical and UX writing. Sometimes it is merged on smaller topics. Technical writing goes in the document, so customers get all the information they need. UX writing, which covers the copy in the UI, sets a tone and personality of the product and could impact the customer’s mood when interacting with the product. Implementation We are lucky to have a very good front-end team that is collaborative and open for discussions. We make sure that the front-end team implements the design as accurately as possible; if it’s not the same, there should be a reason. Handoff When we do handoff specs on Figma by using red lines, call-outs, or mockups of different steps, we try to state the interaction as clear as possible. We also use Component Library to define and reuse some common design elements, including buttons, tables, and modal windows, which helps keep the design consistent and easier to implement. (By breaking down the interactions into steps, everyone gets on same page of the design.) Bug Bash Similar to engineering bug bash, design bug bash aims to do a comprehensive click-through to fix the issues, big or small, which include browser compatibility, keyboard-tabbing, screen compatibility (retina display, readability under different brightness, contrast, color tone), error cases, error messages, breakpoints, UX writing, and more. Validation People always say that there is no good or bad in design, which might be true; however, there could be good and better in each specific case. What is the problem we are trying to solve? What are the metrics to help reveal the progress? What is the ultimate goal we are trying to achieve? We ask ourselves the tough questions and seek answers from data and follow-up studies. Task Analysis There are different forms of task analysis; thinking aloud is one commonly used. By giving participants a specific task or instruction, they will click through the product to finish the task as well as tell what they think along the way. It is great for gathering customer feedback on specific features, interactions, content, and visual components, and could be tested with a functional prototype on staging or InVision clickthrough. Data Analysis We use Heap Analytics, Google Analytics, and other tools to track customer behavior through the funnel. With the end-to-end tracking, we could clearly see how each individual user interacts with our product and each single page, which is a great way to visualize the customer’s journey. We usually pay attention to common patterns to try to identify if it is as expected or not when we design. We also look for data that could be misleading; when we see unexpected customer behavior happen again and again, it is better to conduct a Task Analysis with different types of customers to see what they say before jumping to conclusions too quickly. In other words, quantitative feedback is very helpful for validating an assumption than drawing an uninformed conclusion. Designing as a team Design is never a one-person job. As long as we are designing for someone, it is the designer’s responsibility to dig deeper into people’s words and interpret them in a way that an effective solution could be built upon. By encouraging contributions from multiple stakeholders along the way as well as unlocking communication across departments, we are able to produce better design solutions. We are still iterating the design process with feedback from both customers and internal teams, such as sales, customer success, PM, and engineering. We want the design to support the value of SingleStore and to help build products that meet our customers’ real needs.", "date": "2017-10-06"},
{"website": "Single-Store", "title": "five-sessions-to-attend-at-join-conference-in-san-francisco", "author": ["Nicole Nearhood"], "link": "https://www.singlestore.com/blog/five-sessions-to-attend-at-join-conference-in-san-francisco/", "abstract": "Join Conference in San Francisco brings together thousands of companies across the globe that build their businesses with Looker. The developers responsible for this revolution need a place to share their experiences on this journey. This year, Join will offer even more breakout sessions led by Looker subject matter experts and top customers. This informative mixture of lectures, demonstrations, and guest speakers is geared towards keeping attendees informed on technical content, customer stories, and new launch announcements. SingleStore is exhibiting in the Sponsor Expo, so stop by our kiosk #2 to view a demo and speak with our subject matter experts. Here are our top session picks for you to attend at the event. Driving Adoption — Tips, Tricks & Everything in Between Thursday, September 14, 11:15am – 12:00pm Lucas Thelosen, VP, Professional Services, Looker Looker Professional Services will present an overview of challenges faced when releasing Looker to a team of end-users, both technical and non-technical, and how they have seen solutions working out. Gain insight into how we teach everything from from the simplest concepts (what is your data) to advanced explores for the highly curious. Turn Looker Into Your To Do List Thursday, September 14, 1:00pm – 1:45pm Jamie Davidson, VP Platform, Looker Step up your game by using Looker as a platform for operations. Learn from the experts on Looker integrations that will allow you to turn your data insights into action items. Scaling to 800 Looker Users at BuzzFeed Thursday, September 14, 1:25pm – 1:45pm Nick Hardy, Senior Business Analyst, Data Science, BuzzFeed BuzzFeed has one of the largest internal Looker user bases. From data pipelines and permissions, to models and PDTs, discover what they’ve found works (and doesn’t!) on their path to 800 users. Analyzing & Optimizing Redshift Performance Friday, September 15, 2:00pm – 2:20pm Fabio Beltramini, Customer Success Analyst, Looker Learn how you can leverage our Redshift Performance blocks to analyze your redshift usage and decrease query latency. How Blue Apron Managed Rapid Growth with Looker and Google BigQuery Friday, September 15, 11:00am – 11:20am Sam Chase, Tech Lead Data Operations, Blue Apron Andrew Rabinowitz, Data Engineer, Blue Apron In the past five years, Blue Apron has grown from three founders hand-packing boxes to a public company serving millions. But as they grew so did their data, and suddenly performance was a huge problem. Hear how Blue Apron uses Kafka, Google BigQuery and Looker to store, stream and analyze data.", "date": "2017-09-12"},
{"website": "Single-Store", "title": "five-sessions-to-attend-at-strata-data-conference-new-york", "author": ["Nicole Nearhood"], "link": "https://www.singlestore.com/blog/five-sessions-to-attend-at-strata-data-conference-new-york/", "abstract": "Strata Data Conference in New York brings together thousands of companies across the globe that build their businesses with data. The developers responsible for this revolution need a place to share their experiences on this journey. This year, Strata Data Conference will offer even more breakout sessions led by data engineers and scientists. This informative mixture of lectures, demonstrations, and guest speakers is geared towards keeping attendees informed on technical content, customer stories, and new launch announcements. SingleStore is exhibiting in the Sponsor Expo, so stop by our kiosk #2 to view a demo and speak with our subject matter experts. Here are our top session picks for you to attend at the event. Geospatial big data analysis at Uber Zhenxiao Luo (Uber), Wei Yan (Uber) 11:20am–12:00pm, Wednesday, September 27, 2017 Location: 1A 23/24 Uber’s geospatial data is increasing exponentially as the company grows. As a result, its big data systems must also grow in scalability, reliability, and performance to support business decisions, user recommendations, and experiments for geospatial data. Zhenxiao and Wei will start with an overview of Uber’s big data infrastructure before explaining how Uber models geospatial data and outlining its data ingestion pipeline. They then will discuss geospatial query performance improvement techniques and experiences, focusing on geospatial data processing in big data systems, including Hadoop and Presto. Zhenxiao and Wei will conclude by sharing Uber’s use cases and roadmap. Building advanced analytics and deep learning on Apache Spark with BigDL Yuhao Yang (Intel), Zhichao Li (Intel) 1:15pm–1:55pm, Wednesday, September 27, 2017 Location: 1A 12/14 The rapid development of deep learning in recent years has greatly changed the landscape of data analytics and machine learning and helped empower the success of many applications for artificial intelligence. BigDL, a new distributed deep learning framework on Apache Spark, provides easy and seamlessly integrated big data and deep learning capabilities for users. Yuhao Yang and Zhichao Li will share real-world examples of end-to-end analytics and deep learning applications, such as speech recognition (e.g., Deep Speech 2), object detection (e.g., Single Shot Multibox Detector), and recommendations, on top of BigDL and Spark, with a particular focus on how the users leveraged the BigDL models, feature transformers, and Spark ML to build complete analytics pipelines. Yuhao and Zhichao will also explore recent developments in BigDL, including full support for Python APIs (built on top of PySpark), notebook and TensorBoard support, TensorFlow model R/W support, better recurrent and recursive net support, and 3D image convolutions. When models go rogue: Hard-earned lessons about using machine learning in production David Talby (Pacific AI) 5:25pm–6:05pm, Wednesday, September 27, 2017 Location: 1A 06/07 Much progress has been made over the past decade on process and tooling for managing large-scale, multi-tier, multicloud apps and APIs, but there is far less common knowledge on best practices for managing machine-learned models (classifiers, forecasters, etc.), especially beyond the modeling, optimization, and deployment process once these models are in production. Machine learning and data science systems often fail in production in unexpected ways. David Talby shares real-world case studies showing why this happens and explains what you can do about it, covering best practices and lessons learned from a decade of experience building and operating such systems at Fortune 500 companies across several industries. Topics include: Concept drift: Identifying and correcting for changes in the distribution of data in production, causing pretrained models to decline in accuracy A/B testing challenges: Recognizing common pitfalls like the primacy and novelty effects and best practices for avoiding them (like A/A testing) Offline versus online measurement: Why both are often needed, and best practices for getting them right (refreshing labeled datasets, judgement guidelines, etc.) Data futures: Exploring the everyday implications of increasing access to our personal data Daniel Goddemeyer (OFFC NYC), Dominikus Baur (Freelance) 11:20am–12:00pm, Thursday, September 28, 2017 Location: 1E 15/16 Increasing access to our personal data raises profound questions around ownership, ethics, and the resulting sociocultural changes in our everyday lives. Recent legislation that allows the reselling of our personal browser histories without our explicit consent proves the increased need to explore and investigate the consequences that these developments may bring about. Data Futures, an MFA class in which students observed each other through their own data, explores the social impacts that this informational omnipresence of our personal data may have on our future interactions. In the course, students are guided through a succession of exercises in which they observe each other through their personal data trails to derive assumptions about one another and their class. The changing social dynamics that are exposed by these intimate data exercises showcase how the social behavior of a whole group is affected once personal information becomes accessible. Inspired by this experiential understanding of their data, students then speculate around the future impacts of this knowledge ubiquity by telling visual interaction stories, exemplifying the implications of increasing access to our data. Daniel Goddemeyer and Dominikus Baur share the findings from Data Futures and demonstrate the results with a live experiment with the audience that showcases some of the effects when personal data becomes accessible. Executive Briefing: Machine learning—Why you need it, why it’s hard, and what to do about it Mike Olson (Cloudera) 1:15pm–1:55pm, Thursday, September 28, 2017 Location: 1E 12/13 Companies have been capturing and analyzing data with sophisticated tools for a long time. In recent years, though, two forces have combined to change what’s possible: we can collect and store vastly more data than ever before, and we have powerful new capabilities, like machine learning, to analyze it. Companies that do this well benefit in many ways. Mike Olson will share examples of real-world machine learning applications, explaining how they matter to business. Mike will also explore a variety of challenges in putting these capabilities into production—the speed with which technology is moving, cloud versus in-data-center consumption, security and regulatory compliance, and skills and agility in getting data and answers into the right hands—and will outline proven ways to meet them.", "date": "2017-09-21"},
{"website": "Single-Store", "title": "json-streaming-and-the-future-of-data-ingest", "author": ["Jeremy Wright"], "link": "https://www.singlestore.com/blog/json-streaming-and-the-future-of-data-ingest/", "abstract": "As businesses continue to become technology focused, data is more prevalent than ever. In response, companies have adopted a handful of common formats to help manage this explosive growth in data. Data Formats Today For a long time, XML has been the giant in terms of data interchange formats. Recently, JSON has become popular, catching a wave of interest due to its lightweight streaming support, and general ease of use. JSON is a common format for web applications, logging, and geographical data. For example, the CloudTrail service , part of Amazon Web Services (AWS), uses JSON for its log records. Every API request to any AWS region is stored as a JSON record in a designated S3 bucket. The amount of businesses and developers using AWS, whether directly or indirectly, is staggering at over one million enterprise customers in 2015. Even though only some of them are using CloudTrail, that’s a mountain of JSON data sitting in the cloud. However, JSON is by nature unstructured, which presents a challenge when ingested in a relational database. This is where a tool called jq, developed by Stephen Dolan, comes in handy. For those that work with UNIX tools a lot, it’s a bit like sed or awk for JSON. In fact, one of the SingleStore developers, after using jq, said: “it felt like the good parts of an unholy union between awk, bash, and web hipsters.” More generally, jq can take in a stream of JSON and apply transformations to each record, extract fields, filter arrays, and even change the structure of the JSON data completely. Additionally, jq has support for loops, conditionals, reduce, and a lot of other common programming features. You can find a jq tutorial here: https://stedolan.github.io/jq/tutorial/. Ingesting Data with SingleStore SingleStore is built around the idea of getting instant access to information. When framed against the backdrop of the ongoing data explosion, that means finding a solution for fast and reliable ingest. For this purpose, SingleStore built Pipelines , which natively integrates together the Extract, Transform, and Load elements of most ingest problems. Pipelines supports streaming data from platforms such as Kafka and S3 without the need to connect extra tools. A user operating SingleStore could implement their own custom transformer to unpack JSON and transform it to the appropriate form, but this would not be very extensible and wastes developer time. SingleStore is always trying to release features that keep everything simple and fast, so for SingleStore DB 6, we shipped native support for jq as a JSON transformer. Users can now leverage features in jq to handle JSON data out of the box. This provides a performant and easy-to-use solution to handle one of the most common data formats today. Consuming CloudTrail SingleStore uses CloudTrail for auditing our AWS usage. An example CloudTrail record looks like this: {\n    \"Records\": [\n        {\n            \"eventVersion\": \"1.01\",\n            \"userIdentity\": {\n                \"type\": \"IAMUser\",\n                \"principalId\": \"AIDAJDPLRKLG7UEXAMPLE\",\n                \"arn\": \"arn:aws:iam::123456789012:user/Alice\",\n                \"accountId\": \"123456789012\",\n                \"accessKeyId\": \"AKIAIOSFODNN7EXAMPLE\",\n                \"userName\": \"Alice\",\n \"sessionContext\": { \"attributes\": { \"mfaAuthenticated\": \"false\", \"creationDate\": \"2014-03-18T14:29:23Z\" } } }, \"eventTime\": \"2014-03-18T14:30:07Z\", \"eventSource\": \"cloudtrail.amazonaws.com\", \"eventName\": \"StartLogging\", \"awsRegion\": \"us-west-2\", \"sourceIPAddress\": \"72.21.198.64\", \"userAgent\": \"signin.amazonaws.com\", \"requestParameters\": { // arbitrary json }, \"responseElements\": { // arbitrary json }, \"requestID\": \"cdc73f9d-aea9-11e3-9d5a-835b769c0d9c\", \"eventID\": \"3074414d-c626-42aa-984b-68ff152d6ab7\" }, // additional records ] } We originally hand wrote a transform in Go to parse the records and pull individual fields into their own columns. SingleStore also has a JSON datatype so we can store and query arbitrary values without having a sparse table with lots of columns that are usually null. The table we stored the results in had this schema: CREATE TABLE cloudtrail (\n       eventTime DATETIME,\n       eventSource TEXT,\n       eventName TEXT,\n       awsRegion TEXT,\n       sourceIPAddress TEXT,\n       userAgent TEXT,\n       requestID TEXT,\n       eventID TEXT,\n       userType TEXT,\n       userARN TEXT,\n       userAccountID TEXT,\n       userAccessKeyID TEXT,\n       userName TEXT,\n       requestParameters JSON,\n       responseElements JSON\n) The Go transform was around 300 lines of streaming logic, JSON parsing and error handling, and actually mapping the JSON values to columns. It does its job, but it’s highly specific to the expected structure of the incoming JSON. In contrast, here’s the same Pipeline (with the same table) using jq: CREATE PIPELINE cloudtrail\nAS LOAD DATA S3 'my_company_logs_bucket/CloudTrailLogs'\nCREDENTIALS '{\"aws_access_key_id\": \"your_access_key_id\", \"aws_secret_access_key\": \"your_secret_access_key\"}'\nWITH TRANSFORM ('memsql://json', '', '-r \".Records[] | [.eventTime, .eventSource, .eventName, .awsRegion, .sourceIPAddress, .userAgent, .requestID, .eventID] + ((if .userIdentity != null then .userIdentity else {} end) | [.type, .arn, .accountId, .accessKeyId, .userName]) + [(.requestParameters|tostring), (.responseElements|tostring)] | @tsv\"')\nINTO TABLE cloudtrail\nFIELDS OPTIONALLY ENCLOSED BY '\"'; Here, the transform gathers the fields we want into a single JSON array, performs a couple conditional checks, then outputs a tab separated line for each log record. But we’re not just reading fields. We have to convert certain types (JSON to string, for example) and use conditionals to avoid trying to read properties of a null object. And while this looks like an eyeful, jq actually is very easy to learn for anyone familiar with basic UNIX-like tools. The tech industry is moving really fast in a lot of directions right now. On the one hand, there’s been a big shift back to relational databases and structured data. On the other, the web is ever growing and constantly producing endless unstructured JSON data. With tools such as jq integrated into SingleStore, we can see that these two seemingly disparate paradigms can definitely coexist, and allow our users to reap the benefits of both.", "date": "2017-10-27"},
{"website": "Single-Store", "title": "running-stored-procedures-on-distributed-systems-with-memsql-6", "author": ["Michael Harris"], "link": "https://www.singlestore.com/blog/running-stored-procedures-on-distributed-systems-with-memsql-6/", "abstract": "Today we’re announcing the general availability of SingleStore DB 6. This is a big milestone for the product, which comes with new features to help customers get even more value out of SingleStore. The latest release includes breakthrough query performance, enhanced online operations, and extensibility. In this blog, we’ll take a deeper look at the new Extensibility features. Why did you add Extensibility to SingleStore DB 6? The Extensibility feature was built based on market demand, and enables people to move database workloads that require stored procedure functionality into SingleStore. What are the main features that have been added with Extensibility? Extensibility is primarily split into four newly added features : table-valued functions, user-defined functions, user-defined aggregate functions, and stored procedures. These functions use the Extensibility language, have different execution characteristics, and different application use cases. For example, user-defined functions can be used in the SingleStore Pipelines code, so when data comes in from Pipelines the user-defined function can process or sanitize the incoming data. Additionally, one of our engineers tested Extensibility and created a Mandelbrot set demo ( code here ) using table-valued functions. Also known as parameterized views, table-valued functions offer more flexibility than normal views, because it can be called like a function and passed in variables. With Extensibility added in SingleStore DB 6, what can customers do now that they couldn’t before? Extensibility gives customers the ability to write custom logic in the database for the manipulation and processing of data. With Extensibility, more business logic can be moved into the database to take advantage of what the database already does well. Two examples of this are transactions and role-based security. With transactions, customers can be confident that every operation in a unit of business logic written with SingleStore Extensibility will either be totally complete or be fully rolled back. Customers can also use the existing security systems from SingleStore to give specific users the ability to create, alter, or execute Extensibility functions and procedures. This means that database administrators can be more precise about which data users are able to access or modify the underlying table. What is MPSQL? MPSQL stands for Massively Parallel SQL, and it is the custom language introduced in SingleStore DB 6 for writing Extensibility functions and procedures. MPSQL syntax is inspired by Postgres PL/pgSQL, but it also includes some SingleStore-specific additions. What added benefits are customers getting from MPSQL that they didn’t get before? The functions and procedures use the same compilation system that SingleStore uses for normal queries. This means that the functions and procedures are compiled down to the machine code and stored into a plan cache, which makes repeated execution faster. MPSQL, similar to the rest of SingleStore, is distributed, so execution won’t get bottlenecked in a machine on a single cluster and the Extensibility code will be executable on multiple machines simultaneously. Why did you create MPSQL? You could not express Extensibility functions with only the syntax that was in SingleStore DB 5. MPSQL is a Turing complete language, with all normal control flow operations supported. The language needed to be introduced into SingleStore so customers could take advantage of the new functionality. Does Extensibility enable any other larger tech trends, such as machine learning? Yes, machine learning is one of the primary use cases for Extensibility in SingleStore and in other database systems. It gives customers flexibility in processing data required for ML applications. We also added the built-in functions DOT _ PRODUCT, VECTOR _ SUB, and EUCLIDEAN _ DISTANCE. What do those functions allow customers to do? The functions use AVX2 instructions to provide execution at memory bandwidth speed of common linear algebra operations used in machine learning. How would customers take advantage of these new features today? SingleStore is a great addition to any machine learning system, especially one that has performance requirements, or one where the amount of data cannot fit on a single machine. By using Extensibility, data flows can be customized and processed to do a vast number of machine learning operations. To learn more about how customers are taking advantage of DOT _ PRODUCT, read more on this blog covering the real-time image recognition use case with Thorn.", "date": "2017-10-19"},
{"website": "Single-Store", "title": "why-you-need-a-real-time-data-warehouse-for-iot-applications", "author": ["Mike Boyarski"], "link": "https://www.singlestore.com/blog/why-you-need-a-real-time-data-warehouse-for-iot-applications/", "abstract": "As always-on devices and sensors proliferate, the data emitted from these devices provides meaningful insights to improve customer experiences, optimize costs, and identify new revenue opportunities. In a recent report, Taking the Pulse of Enterprise IoT from McKinsey & Company, 48 percent of respondents cited “managing data” as a critical capability gap related to their IoT initiatives.1 The data infrastructure behind IoT applications requires a high performing and easy-to-access platform to support immediate responses to changing conditions. At the center of an IoT data infrastructure platform there needs to be a database that supports stream data ingestion with familiar SQL query access on a scalable, highly available platform. SingleStore powers a number of IoT applications to help manage operating costs while improving the customer experience. These applications require core database capabilities including: Streaming data ingest and store The database must collect and store multiple streams of data into relational formats to support real-time and historical analysis. Ingestion often requires inserts, updates, and deletes to ensure data accuracy. Fast query response Perform instant queries across millions of events or devices to discover real-time anomalies or predict events leveraging historical data using memory-optimized SQL. Proven compatibility Leverage the familiarity of ANSI SQL with full data persistence to drive sophisticated analytics while seamlessly working with existing business intelligence and middleware tools. Scalability and availability Utilize a modern shared nothing architecture to scale out with industry-standard hardware. Built-in resilience keeps the database online across cloud or on-premises deployments. SingleStore has been able to help industry-leading organizations such as Uber, Verizon, Comcast, and Cisco deliver IoT applications powered by analytics at scale. These applications include: Real-Time monitoring and detection , which can be used to manage networks and devices with instant insights to live conditions to improve customer experience while mitigating costs. Predictive maintenance applications , which can identify potential issues before they arise to prevent outages or improve asset management for oil pumps, wind farms, vehicles, and more. Fleet optimization , which can help optimize cost by identifying the location and condition of every truck or car to streamline delivery or improve customer satisfaction. Learn more about the real-time analytic solutions for IoT applications by downloading our new IoT Analytic Solution Guide . 1 Taking the pulse of enterprise IoT , July 2017", "date": "2017-10-13"},
{"website": "Single-Store", "title": "memsql-maturity-framework", "author": ["Seth Luersen"], "link": "https://www.singlestore.com/blog/memsql-maturity-framework/", "abstract": "The SingleStore maturity framework for data warehousing in the age of artificial intelligence depicts the journey of digital transformation. Caption: Data Warehouse in the Age of AI Maturity Framework by SingleStore. Consisting of three epics and seven maturation stages that symbolize increasing strategic value, the framework outlines the transformation from descriptive analytics to predictive analytics to prescriptive analytics for modern organizations and enterprises. In this blog post, we characterize each epic and corresponding stages of maturation in detail. Data Lake The dark days of largely spreadsheet-based analyses are fading. Opportunistic departments and business units prefer to pursue their own analytics and business intelligence initiatives. While most organizations have a strong ability to collect and store historical data that is transactional in nature, they continue to endure the hardship of turning data into meaningful analysis. We have a 100 TB data lake. We don’t know what’s really in it. It’s that big.— Big Data operations team To help alleviate the burden of transforming data into analytics at larger organizations, leadership governs data with shared services, technology standards, and common data models. Here, key performance indicators characterize descriptive analytics for decision making. Caption: For many organizations, the Data Lake represents the totality of all data collection and storage efforts Collect Row-based and hierarchical data files are a routine matter for various types of data collection. Data includes logs, instrumentation, messaging, user-generated events, and application programming interfaces. The focus on data collection overshadows the initiative to examine the data. Typically, a waterfall project management methodology steers data collection. Store General practices for managing enterprise data volume and variety exist in the enterprise: relational databases manage structured application transactions, and NoSQL databases manage document and unstructured data applications. Batch processes collect and transform bulk data into pre-aggregated cubes defined by rigid snowflake and star schemas inherent to data marts and data warehouses. Disjointed technologies create some bottlenecks that hinder the workflow of turning raw data into business metrics. Big data collection and storage often consists of a hodgepodge of technology solutions strung closely together. Disparate in-house, open source, and commercial solutions assist with both extraction, transformations, and load (ETL) processes as well as middleware integrations services. Missing automation between the various steps of data ingestion, ETL, and data modeling create noticeable productivity inefficiencies. When one system fails, internal operators scramble to triage the domino chain of failures. Explore On a routine basis, individuals share ad-hoc analytical insights. Monthly, weekly, and daily cadences set the standard for departmental reporting. Business intelligence (BI) dashboards offer business users canned reporting, one-off analyses, and ad-hoc reports. Common BI tools include Looker, Tableau, and Zoomdata. The siloed roles of a data engineer, data scientist, and business user often independently explore and mine data to solve business problems. Some cross-collaboration occurs for standardizing descriptive metrics associated with key performance indicators of various business units. Internally, data teams struggle with how the greater organization properly values the descriptive metrics embedded in data reporting and BI dashboards. Difficult cross-functional collaboration, data veracity suspicions, or misaligned distrust in analytics impairs business innovation. Exactly-Once Real-Time For many organizations, real-time data processing means mastering the transactional semantics of exactly-once for data volume, variety, and velocity. By definition, the always-on economy requires real-time analytics . — Raghunath Nambiar,  Chief Technology Officer Cisco UCS Legacy messaging systems for real-time data either send ‘At Most Once’, failing to guarantee message receipt, or ‘At Least Once’, promising the guarantee of a receipt at the expense of message duplication. When these systems experience high data velocity and volume, the pain of duplicate and erroneous data amplifies logarithmically, generating data distrust and excessive post-processing. For many NoSQL solutions, streaming data stops here, waiting to be re-processed and de-duplicated. Caption: Exactly-Once semantics provides the guarantees required for real-time applications. Modern messaging services such as Apache Kafka stream huge volumes at high velocity. The semantics of ‘Exactly-Once’ ensures data receipt without message duplication and error. Enterprises that harness real-time data marry it with historical data in real-time data warehouses. The data marriage represents operationalized unification, often reflecting the leadership-driven transformation of internal cultural values. Stream Adherence to agility allows the organization to culturally embrace change and new initiatives more readily. The need to process disparate data as quickly as possible, and at growing scale, highlights data velocity as a modern requirement of sensor, geospatial, and social media data-driven applications. Real-time messaging technologies such as Amazon Kinesis and Apache Kafka introduce new found semantic challenges for transactional boundaries associated with microsecond data ingest. Data infrastructure teams struggle to maintain complex data infrastructures that span on-premises and cloud environments. Operations and infrastructure teams start to invest in technologies that accommodate ultrafast data capture and processing. Teams focus on in-memory data storage for high-speed ingest, distributed and parallelized architectures for horizontal scalability, and queryable solutions for real-time, interactive data exploration. Operationalize Recognizing that familiar data integration patterns centered on batch processing of bulk data function to disable modern digital business processes, organizations turn to leadership for data governance, unification, and innovation. Rather than solely standardizing on common data models, Chief Data Officers and Heads of Analytics embrace new topologies for messaging semantics that support exactly-once, real-time data processing. However, leadership moves the proverbial goal post and demands that business value is derived at the time of data ingest. Caption: Data pipelines operationalize time-to-value in sub-second time. Deriving immediate business value from the moment of data ingest affects architectural decisions and technical requirements. Real-time data processing requires sub-second service level agreements (SLA). As a result, organizations eliminate complex and slow processes for data sanitization including de-duplication. Achieving sub-second business value means maximizing compute cycles while mitigating the slowness associated with traffic spikes and network outages.  In sub-second speed,  data pipelines operationalize time-to-value. Keeping low latency SLAs under high concurrency at petabyte volume, necessitates adopting distributed systems for parallelized compute and storage at linear scale. Leadership tasks data engineers, applications developers, and operations teams to implement hybrid transactional and analytical processing in one data platform for real-time and historical data. Collaborative teams identify the anti-pattern: the separation between databases for transactions and data warehouses for analytics. With a distributed SQL platform, data engineers, data scientists, and business users now can explore and mine data in parallel with other distributed systems for advanced computations such as Apache Spark.  Data scientist use Jupyter notebooks to help solve business problems collaboratively. BI dashboards provide business users with real-time reporting of descriptive metrics. Daily intra-day analyses supplant monthly and weekly reporting cadences. Artificial Intelligence Enterprises that operationalize historical data from data lakes, and real-time data from messaging streams in a single data platform using a SQL interface, are best positioned to achieve digital transformation. There is no reason and no way that a human mind can keep up with an artificial intelligence machine by 2035. — Gray Scott, Futurist & Techno-Philosopher Even though descriptive metrics can optimize the performance of an organization, the promise of predicting an outcome and recommending the next action is infinitely more valuable.  Predictive metrics require training models to learn a process and gradually improve the accuracy of the metric. Deep learning harnesses neural networks for machine learning, creating the ability to simulate intelligence, resulting in specific recommendations. Caption: The journey of digital transformation in the Age of AI. The differences between “What happened?”, “What could happen?”, and “What should we do when it happens?” succinctly illustrates why modern enterprises remain wholly committed to the journey of digital transformation. Predict Conventional single-box server systems with fixed computational power and storage capacity, limit machine learning to iterative batch processing. Modern, highly scalable distributed systems for computation, streaming, and data processing, free machine learning to run in real time. Not only can models quickly training on historical data, they operationalize on real-time, streaming data. Continually retraining operationalized models using real-time and historical data together results in optimal tuning. Caption: Supervised and unsupervised machine learning algorithms allow organizations predict and score outcomes. Binary data vectorization allows for distributed, scalable, and parallelized algorithmic processing at ingest. Distributed computational frameworks for CPUs and GPUs such as Apache Spark and TensorFlow easily score streaming data and detect anomalies. The flood gates open for new use cases including real-time event processing, real-time decision making, edge monitoring, churn detection, anomaly detection, and fraud detection. With the ability to process both real-time and historical data within a single distributed system devoid of external ETL, data governance and management leaders can now own digital transformation initiatives for their organization. The reality of scoring real-time and streaming data with machine learning means that descriptive metrics can now be transformed into predictive analytics. Optimize Regression models for machine learning depend on iterative processing. At a certain point, the accuracy of the model prediction levels out or even degrades. Neural network processing for both supervised and unsupervised learning allows for the results of parallel algorithm processing to be recalibrated. Neural network processing with more functional layers of neurons is deep learning. The key to deep learning is allowing for neural network layers to transactionally and incrementally store outputs while evaluating the model for next layer inputs. Caption: Artificial Intelligence is Machine Learning and Deep Learning. A real-time data warehouse supporting historical data, real-time data ingest, and machine learning, operationalizes deep learning. Implementing standard SQL user-defined functions and stored procedures for machine learning can empower deep learning. With deep learning, predictive analytics generated from closed loop regressions and neural networks turns into prescriptive analytics for real-time recognition, personalization, and recommendations. Caption: The neural networks of deep learning algorithms power recommendations with a high degree of accuracy. The simplification of algorithmic complexity into procedural language with an SQL interface for artificial intelligence is the foundation for true digital transformation. Optimized organizations evolve from descriptive analytics to predictive analytics to prescriptive analytics. Such organizations gain new found knowledge and competitive advantages about their customers, operations, and competitors with deep context. Powered by human creativity, the real-time data warehouse represents the exactly-once, AI-powered enterprise with superior abilities to automatically refine accuracy, naturally process sensory perceptions, extrapolate experiences from multiple signal modalities, predict actions, make recommendations, and prescribe results. Discover Your Journey In less than 10 minutes, you can start to identify your data warehouse maturity with our free interactive assessment. SingleStore will then generate and share with you, “Your Maturity Framework Interactive Report”. For many enterprises today, moving to real time is the next logical step, and for some others it’s the only option. — Nikita Shamgunov, SingleStore co-founder, former CTO, current CEO Your Maturity Framework Interactive Report Designed to help your organization determine its current data warehousing maturation level, the report identifies barriers to maturation in the age of artificial intelligence and makes specific recommendations for your next steps. Caption: Your Personalized Maturity Framework Interactive Report To begin your interactive assessment about your journey, start here .", "date": "2017-11-17"},
{"website": "Single-Store", "title": "how-database-convergence-impacts-the-coming-decades-of-data-management", "author": ["Gene Miguel"], "link": "https://www.singlestore.com/blog/how-database-convergence-impacts-the-coming-decades-of-data-management/", "abstract": "Within the database industry, there are often paradigm shifts in the market that create opportunities for new types of technology to emerge. When those shifts happen, new technology requirements come up and old technologies do not satisfy those requirements. There have been many shifts in the database market including, NoSQL, big data, in-memory, Internet of Things, cloud computing, and many others. Recently, SingleStore CEO and Co-founder, Nikita Shamgunov, presented to a group of database professionals at the New York City Database Month meetup group about these shifts, and specifically, how database convergence will impact the coming decades of data management. He discussed the latest innovations in data management, and how utilizing a distributed and scalable converged database can optimize transactional and analytical workloads. He also showed the technical impact of real-time data pipelines and scalable SQL in a distributed computing platform that is designed for addressing challenging financial application scenarios. Watch to view a demo of large scale data (several billion rows) that is processing on Volume Weighted Average Price (VWAP) and real-time image recognition. View the presentation slides here . Thank you to NYC Database Month, the largest database meetup in NYC, for hosting us! To learn more and to see their upcoming list of events, please visit http://www.databasemonth.com/ .", "date": "2017-11-08"},
{"website": "Single-Store", "title": "five-sessions-to-attend-at-aws-reinvent-2017", "author": ["Nicole Nearhood"], "link": "https://www.singlestore.com/blog/five-sessions-to-attend-at-aws-reinvent-2017/", "abstract": "Amazon will finish an exciting year by bringing together thousands of people to connect, collaborate, and learn at AWS re:Invent from November 27 – December 1 in Las Vegas. Whether you are a cloud beginner or an experienced user, you will learn something new at AWS re:Invent. This event is designed to educate attendees about the AWS platform, and help develop the skills to design, deploy, and operate infrastructure and applications. SingleStore is exhibiting in the Venetian Sands Expo Hall, so stop by our booth #1200 to view a demo and talk to our subject matter experts. This year, AWS re:Invent will offer even more breakout sessions led by AWS subject matter experts and top customers. This informative mixture of lectures, demonstrations, and guest speakers is geared towards keeping attendees informed on technical content, customer stories, and new product announcements. Here are our suggested top breakout sessions for you to attend at the event. Big Data Architectural Patterns and Best Practices on AWS Siva Raghupathy – Sr. Manager, Solutions Architecture, Amazon In this session, we simplify big data processing as a data bus comprising various stages: collect, store, process, analyze, and visualize. Next, we discuss how to choose the right technology in each stage based on criteria such as data structure, query latency, cost, request rate, item size, data volume, durability, and so on. Finally, we provide reference architectures, design patterns, and best practices for assembling these technologies to solve your big data problems at the right cost. Business and Life-Altering Solutions Through AI and Image Recognition Bob Rogers – Chief Data Scientist, Intel Julie Cordua – CEO, Thorn Nikita Shamgunov – CEO, SingleStore Artificial intelligence is going to be part of every software workload in the not-too-distant future. Partnering with AWS, Intel is dedicated to bringing the best full-stack solutions to help solve business and societal problems by helping turn massive datasets into information. Thorn is a non-profit organization, co-founded by Ashton Kutcher, focused on using technology innovation to combat child sexual exploitation. It is using SingleStore to provide a new approach to machine learning and real-time image recognition by making use of the high-performance Intel SIMD vector dot product functionality. This session covers machine learning on Intel Xeon processor based platforms and features speakers from Intel, Thorn, and SingleStore. Five Ways Artificial Intelligence Will Reshape How Developers Think Noelle LaCharite – Sr. Technical Program Manager, Amazon Jenn Jinhong – Developer Advocate, Amazon Thinking in terms of AI and conversation, changes the way you approach building web services and customer experiences. In this session, we discuss five trends that we’re seeing right now in artificial intelligence and conversational UI as we work with people building new experiences with Alexa. Self-Service Analytics with AWS Big Data and Tableau Anna Terp – BI Solutions Architect, Expedia Tad Buhman – Sr. Business Analyst II, Expedia As one of the thought leaders in Expedia’s cloud migration, the Expedia Global Payments Business Intelligence group architected, designed and built a complete cloud data mart solution from the ground up using AWS and Tableau online. In this session, we will discuss our business challenge, the journey to the solution, high-level technical architecture (using S3, EMR, data pipelines, Redshift, Tableau Online) and lessons learned along the way, including best practices and optimization methods, etc. Leveraging a Cloud Policy Framework – From Zero to Well Governed Vikram Pillai – Chief Architect, Director of Engineering, CloudHealth Technologies Governing cloud infrastructure at scale requires software that enables you to capture and drive management from internal policies, best practices, and reference architectures. A policy-driven management and governance strategy is critical to successfully operate in cloud and hybrid environments. As infrastructure grows, you might leverage knowledge that extends beyond the organization. An open-source “cloud policy framework” enables users to leverage a community that can help define and tune best practice policies, and help SaaS vendors and ISVs capture the best way to manage an application and share it with customers. A well-defined management and governance strategy enables you to put automation in place that keeps your cloud running securely and efficiently without having to take it on as a full-time job. This session discusses the development of a “cloud policy framework” that enables users to leverage open source rule definition organizations can use to govern their cloud. Learn best practice policies for managing all aspects of services, applications, and infrastructure across cost, availability, performance, security and usage.", "date": "2017-11-27"},
{"website": "Single-Store", "title": "blueshift-succeeding-with-real-time-analytics", "author": ["Mike Boyarski"], "link": "https://www.singlestore.com/blog/blueshift-succeeding-with-real-time-analytics/", "abstract": "Integrated Business Planning in the Cloud Blueshift ONE combines the complexity of business planning, including commercial planning, sales forecasting and budgeting, along with supply chain planning, into an integrated cloud solution. The integrated view of data gives Blueshift ONE customers a competitive edge with accurate reliable data that is shared across corporate functions during the business planning process. The integrated platform delivers a single consensus forecast across all functions, enabling ongoing strategic decision making. The Risks of Analytic Performance for Cloud Applications Every user of the Blueshift ONE platform enjoys the insights and reports commonly used for forecasting and budget planning. The expectation is that the reports will be accurate and responsive in order to support the variety of questions that commonly come up during the business planning process. A necessary consequence of a successful product is the increased strain on the analytic capabilities, driven by more data and users, and the effect this has on delivering a consistent and reliable experience to users. Single Server Analytic Limitations As the growth in data and customers continued to rise for the Blueshift ONE platform, the underlying Microsoft SQL Server implementation that powered the analytics began to show performance issues. Select power users and customers would periodically grumble about the query performance for different reports. The Blueshift team worked on a series of optimizations and scale-up configurations to SQL Server, only to recognize the customer growth challenges of a single node database would continue to be problematic. A major factor of the degrading performance was the inability to shard data by customer, impacting aggregate views on a per customer basis. Each customer aggregation was delivered by inefficient table scans and difficult to configure query caches. This operations challenge led the Blueshift team to seek new solutions to their analytic challenges. Distributed Data Processing for Faster Analytics The Blueshift team evaluated a number of database solutions, including Cassandra, to address their analytic scalability issues. The team ultimately selected SingleStore due to the distributed data sharding support, support of ANSI SQL, and fast benchmark results. The test results were able to prove SingleStore to be 100x faster than their existing SQL Server implementation. During additional stress tests of SingleStore, the data set was expanded by 100x to mimic substantial data growth and yet the analytics performed by SingleStore continued to blow past expectations. Turning Analytics into a Competitive Advantage The Blueshift ONE platform, now powered by SingleStore, is capable of delivering analytics on nearly 2 billion records with 60 measures in under 2 seconds. The fast performance gives the application a leg up on the competition as they grow their customer base and expand into new analytic capabilities that were previously not possible. “ SingleStore helps us “10X” our analytic capabilities, providing a much richer functionality going forward .” Justin Stafford, CEO, Blueshift Additional Database Resources for Cloud Application Vendors Database Multi-Tenancy in the Cloud and Beyond Delivering Scalable Self-Service Analytics Seeking a Rescue from a Traditional RDBMS", "date": "2017-10-31"},
{"website": "Single-Store", "title": "delivering-real-time-feedback-loops-for-education", "author": ["Kristi Lewandowski"], "link": "https://www.singlestore.com/blog/delivering-real-time-feedback-loops-for-education/", "abstract": "At Strata Data Conference in September, David Mellor, VP and Chief Architect at Curriculum Associates, showed how the company creates real-time feedback loops for students and educators. David covered how to build a real-time application with SingleStore, and how his team architected a combination of data workflows for actively updating data, such as student activity through online lessons and diagnostics. One workflow relies on in-memory processing to serve responses in under 200 milliseconds. The other workflow focuses on data that can be processed on scheduled intervals. Video and Slides Curriculum Associates Strata NYC 2017 from SingleStore About David David Mellor is the vice president and chief architect at Curriculum Associates and an adjunct professor at Boston University’s Graduate School of Computer Science. Previously, David was the chief architect of the Next Generation Application Platform at Oracle. He is the author of multiple books, including The Common Warehouse Metamodel Developer’s Guide . David also holds two patents in the area of extension mechanisms for web content.", "date": "2017-12-06"},
{"website": "Single-Store", "title": "8-crucial-criteria-look-for-i-database-management-software", "author": ["Gene Miguel"], "link": "https://www.singlestore.com/blog/8-crucial-criteria-look-for-i-database-management-software/", "abstract": "Whether you’re the CTO of the Rebel Alliance or the Galactic Empire, it could be very difficult to decide on your next database technology with the distraction of both sides constantly at war. In 2018, you’ll need to make a database choice for an existing or new application. Here are the things you need to keep in mind as you shop for your next database. 8 Criteria To Keep In Mind While Looking For Your Next Database 1) Pick the right language: SQL The history of SQL, or Structured Query Language, dates back to 1970, when E.F. Codd, then of IBM Research, published a seminal paper titled, “A Relational Model of Data for Large Shared Data Banks.” Since then, SQL has remained the lingua franca of data processing, helping build the relational database market into a $36 billion behemoth.\nModern SQL supports distributed transactions, distributed queries, and distributed algorithm processing. When picking a language, engineers will want to consider: Surface area supported: Joins, Aggregates, sub-queries, CTEs, Window functions Parallelism: In a single machine, across a cluster of machines Mature query optimizer Profiling and query tuning support 2) Performance Businesses want insights from their data and they want it sooner rather than later. For fast-changing data, enterprises must derive immediate value from data changes to stay competitive. Engineers building real-time applications that aim to minimize the time-to-value of data should consider: Modern hardware AVX2 instructions / SIMD Flash/NVMe Code Generation 3) Database storage technology Today’s real-time application architectures require fast data ingest of millions of rows per second and subsecond query response against billions of rows. Key considerations for modern database storage technology includes: In-memory Rowstore On-Disk Columnstore Skiplist, B-Tree, LSM-Tree, hash table, min-max index types 4) Transactionality Nothing is more critical than ensuring that there are transactional boundaries for data commits. Database engines function to meet ACID compliance. The degree of transactionality determines business data success.  Closely investigate the following: Point-updates Mass updates File systems like HDFS  that purport to have transactional consistency 5) Protection and durability Protect your data investment. Durability is cornerstone of ACID compliance. When selecting database technology, take into account: Replication support (synchronous, asynchronous, log-based, statement-based) Built-in transparent high availability or manual setup Backup and Restore support 6) User-defined Functions and Stores Procedures Extensibility is critical for implementing new functionality for modern database applications. For massive parallel processing, database systems require distributed extensibility which includes: Custom table value functions Custom user-defined functions Custom user-defined aggregate functions Custom store procedures 7) Data Ingest For many organizations, batch processing reflects slow data load times. Examine database technology that reduces extract, transform, and load (ETL) times while combining transactional and analytical workloads. These systems should exhibit: Fast, continuous, streaming ingest Running queries concurrently with ingest 8) Security Everyday, business fall victim to data breaches. When hackers from the dark side gain access to critical data systems, all data becomes compromised.  Protect the data of your business by selecting a a modern database that supports: Encryption; Authentication Role Based Access Control Audit Logging; Strict Mode The force works in mysterious ways in the database universe. When considering a new database platform, let the force guide you to a scalable, real-time database and data warehouse platform for high-performance applications that require fast, accurate, secure, and always available data. To learn more, visit singlestore.com . This blog post is an unofficial, educational, information resource. It is not linked to Lucasfilm Ltd. or Walt Disney. All images belong to their respective trademark and copyright holders. The Official Star Wars site can be found at www.starwars.com .", "date": "2017-12-21"},
{"website": "Single-Store", "title": "data-warehouses-and-the-flying-car-dilemma", "author": ["Prashanth Ponnachath"], "link": "https://www.singlestore.com/blog/data-warehouses-and-the-flying-car-dilemma/", "abstract": "Traditional data warehouses and databases were built for workloads that manifested 20 years ago. They are sufficient for what they were built to do, but these systems are struggling to meet the demands of modern business with the volume, velocity, and user demand of data. IT departments are being challenged from both ends. On one side, companies want to analyze the deluge of data in real time, or near real time. On the other side, on the consumption end, the need to analyze and get value out of data is increasing exponentially. A decade ago, companies had a handful of analysts who ran reports and a few dashboards. Today, enterprises have armies of data scientists, analysts, and savvy business users wanting to slice and dice the latest data. The “Flying Car” Dilemma Companies have invested millions of dollars in procuring and customizing legacy systems. In addition to the capital expense, these companies have spent years hiring and training resources to support and maintain these systems. Thus, IT is under tremendous pressure to leverage these investments to meet the evolving needs of the business. However, these systems were not designed for modern data workloads that emphasize real-time insights to changing customer or machine conditions. Expecting legacy systems to do this is analogous to expecting a car to suddenly adapt and begin flying like a jet. Putting wings on the car and making modifications might work temporarily, but it’s unlikely to work long term. In essence, it is a bad idea to modify a car to fly. This is the “flying car dilemma”. Can You Replace a Car with a Jet? This is the architecture of a traditional data warehouse. The source data starts with a transaction system, which is typically a database, such as Oracle or SQL Server. Periodically, you have to use ETL tools such as Informatica to batch load data from your transactional system into a data warehouse product, such as Teradata, Oracle, or Sybase IQ. Then, you run reports in the data warehousing environment. This worked fine for nightly and weekly analytics, but in today’s world this approach has the following challenges: First, by the time data gets to the warehouse, it is 24 to 48 hours old. Companies today need real-time or near real-time data to compete against each other. To accelerate the ingest rate and speed up access to the data, you need to buy refrigerator-size appliances that are expensive and difficult to upgrade (remember putting wings on a car). Additionally, the legacy systems are not able to support the concurrency that is required to support the demand for instant data access from executives, analysts, and data scientists. The solution to this problem is not “rip and replace.” Replacing a legacy system is similar to trading your car in for a jet aircraft. First off, jets are not affordable. Secondly, even if you buy a jet aircraft, you need new infrastructure such as runways, mechanics, pilots, and to setup new processes for the jet to work. Ripping and replacing the legacy system has similar challenges: Legacy systems are heavily customized. Migration is very costly, time consuming, and complex. Training the entire staff to work with a brand new system is expensive and retraining is not easy. Changing processes, that are embedded in an organization for decades, takes time. There is a reason why we still have mainframes in modern enterprises. Augment Instead of Replace: The FedEx Drone Approach The sensible solution to the “flying car dilemma” is to take the approach a company would take with shipping. For example, let’s say a company is using FedEx Overnight for packages that need to be delivered the next day, and FedEx Ground for packages that need to be delivered in a week. Taking this approach with a data warehouse would mean isolating workloads that contain real-time analytics, high ingest, and high concurrency [ FedEx Overnight ] from the workloads that do not have these requirements [ FedEx Ground ] . Now we put the “Fedex Overnight” workloads on a modern real-time data warehouse such as SingleStore and allow the “FedEx Ground” workloads to continue on a legacy system. Here we are not using an expensive jet to replace a car, rather we are using a drone that is inexpensive, easy to use, and most of all easy to adopt. Using a product such as SingleStore has the following advantages: It works with your existing databases (integration capability) It works with your existing ETL tools It has the ability to work with open source computing frameworks and modern messaging platforms, such as Spark, Kafka, and others SingleStore is fully compliant with SQL, which means all the investments that you have made in manpower for your legacy systems can be leveraged Some of the technical advantages include: Real-Time Ingestion__: Modern, lock-free, and skip list technologies remove ingest bottlenecks Scalable__: Scale-out on commodity hardware for high concurrency requirements Fast Queries__: Low latency queries that leverage vectorization and compilation Machine Learning__: Integration with machine learning tooling Augmenting the current architecture is a pragmatic approach for solving the flying car problem. Over time, as the demand for real-time requirements increase, more and more workloads can be added to SingleStore (the drone slowly transitions into a jet). Since SingleStore converges transactional and analytical workloads, the architecture is simplified over time. As shown in the diagram below, this reduces, and sometimes eliminates, the need for costly ETL tools and complex processes. So if you have a “flying car dilemma,”  take the “FedEx approach,” and start with a drone and over time transition into a jet.", "date": "2018-01-05"},
{"website": "Single-Store", "title": "scaling-distributed-joins", "author": ["Adam Prout"], "link": "https://www.singlestore.com/blog/scaling-distributed-joins/", "abstract": "Most users of SQL databases have a good understanding of the join algorithms single-box databases employ. They understand the trade-offs and uses for nested loop joins, merge joins, and hash joins. Distributed join algorithms, on the other hand, tend not to be as well understood. Distributed databases need to make a different set of tradeoffs to account for table data that is spread around a cluster of machines instead of stored on a single machine, like in a traditional database. Because these data movement trade-offs are often misunderstood, many people have made broad claims that distributed joins don’t scale. It is true that some distributed join algorithms involve more data movement than others, but just like single-box joins, it’s possible to optimize your table schema or queries to use more scalable distributed join algorithms for queries with higher throughput requirements. Types of Distributed Joins For the purpose of this blog, let’s consider any algorithm for joining two tables in a distributed SQL database a distributed join. Using that definition, there are five ways distributed SQL databases can run a distributed join. The way table data is distributed in the cluster is also a key aspect of how join queries are executed. The most common practice is using sharding to distribute data among nodes in the cluster (as SingleStore does). So, let’s assume the data is sharded using a hash function on some columns of the table (a shard key) to make the descriptions more concrete. Local/Collocated Reference Table Join Most distributed databases support tables that are copied to every node in the cluster. In SingleStore, we call these reference tables. Reference tables are usually used for dimension tables in a star schema that are small and rarely updated. Joining a distributed table (a table whose data is sharded around the cluster) against a reference table is almost the same as doing a local single-box join at each node in the cluster. Each node in the cluster can join its part of the distributed table with the reference table locally, then the results can be merged together by another node in the cluster before being sent to the user (the merging logic is a bit trickier for left joins). Pros: Very scalable. No data movement. Cons: Requires schema tuning. Tables needs to be marked as reference tables ahead of time. Reference tables are copied on every node in the cluster using some disk and memory capacity. Local/Collocated Distributed Table Join If two distributed tables are both sharded on the columns being joined, then you can join them locally on each node the same way as a reference-sharded table join with a final merging step to produce the complete result. This is the most scalable join algorithm for joining two distributed tables, as it involves no data movement before doing the join. It’s also the most restrictive type of distributed join, as the tables need to be sharded on the columns involved in the join condition before a collocated join is possible. Most distributed databases only allow a table to be sharded on one key, so this limits the flexibility of this join type. As a result, it’s important to have columns that are regularly joined on as shard keys. These columns would often be foreign key columns in a single-box database. Pros: Very scalable. No data movement. Cons: Requires schema tuning. Shard keys need to be chosen ahead of time. Inflexible. Most databases only allow a single shard key, which limits the number of joins that can be run collocated. Remote Distributed Table Join If the distributed tables involved in a join have filters that reduce the number of rows that need to be joined to a small subset of both tables, then the rows for both sides of the join can be pulled to a single node in the cluster which can do a single-box join. This is the first join type we’ve described so far that involves data movement before joining. In this case, all nodes in the cluster send the data they have for the two sides of the join to a single node so it can run the join. This type of join only performs well when the number of rows involved in the join are small. The node doing the join will be overwhelmed if the data sizes involved grow too large. This type of join also limits the parallelism that is available on a single node, whereas other join algorithms are able to use the entire cluster to run the join. Pros: No schema tuning needed (shard keys or reference tables). Scalable when applied to joins involving a small number of rows. Very little data movement. Cons: Only applicable to joins with a small number of rows being joined. Worse case performance is extremely poor if this join algorithm is chosen when the join involves a large number of rows when a single node is doing all the work to run the join. Broadcast Join Broadcast joins work well when only one side of the join involves a small set of rows (has a selective filter). To do a broadcast join, the side of the join with a small set of rows is sent to every node in the cluster, then joined locally with the larger table on the other side of the join. You can think of a broadcast join as creating a temporary reference table for the join at every node, then running a reference table join as described in the Reference Table Join section. Pros: No schema tuning needed (shard keys or reference tables). Flexible. Applicable to more query shapes then the previous types of joins. Cons: Only feasible when one side of the join contains a small number of rows. Not as scalable. The broadcasted rows are copied to every node in the cluster requiring a lot of inter-node communication before the join is executed. Reshuffle Join If both sides of the join involve a large number of rows, then doing a broadcast join will send too much data around the cluster and may exhaust the memory or disk of nodes. Instead in this case, it’s best to repartition the data and send some part of the data in the table to each node in the cluster to run a local distributed join on each node exactly as described in the Distributed Table Join section. The typical way this happens is one side of the join is reshuffled (has its shard key recalculated with a different set of key columns) to match the shard key of the table on the other side of the join. Some joins could result in both sides of the table getting reshuffled if there is no shard key involved in the join condition. This is the most expensive, but most flexible way of running a distributed join. Pros: No schema tuning needed (shard keys or reference tables). Very flexible. Applicable to any query shape. Cons: The least scalable type of join. A lot of data movement is needed as many of the rows involved in the join are copied to other nodes to execute the join. Optimizing Distributed Joins To make distributed joins scalable for high throughput workloads, it’s best to avoid data movement as much as possible. Some options for doing this are: Make small and rarely updated tables that you regularly join against into reference tables. This avoids the need to broadcast those small tables around the cluster to join against them. As much as possible, choose shard key columns that are commonly joined on. This will allow local distributed joins to be used more often, which has less data movement and is still extremely scalable (every node in the cluster is involved in running the join in parallel). Joins that need to reshuffle both tables involved in the join will be hard to make scale for high throughput workloads. At lower levels of concurrency the reshuffled joins are fine. These queries can also be run as a remote distributed join if the number of rows involved in the join is small enough, so consider restricting the rows involved in the join if possible. For more detailed examples and tuning advice see the SingleStore documentation here .", "date": "2017-12-15"},
{"website": "Single-Store", "title": "using-memsql-within-the-aws-ecosystem", "author": ["Gary Orenstein"], "link": "https://www.singlestore.com/blog/using-memsql-within-the-aws-ecosystem/", "abstract": "The database market is large and filled with many solutions. In this post, we will take a look at what is happening within AWS, the overall data landscape, and how customers can benefit from using SingleStore within the AWS ecosystem. Understanding the AWS Juggernaut At AWS re:Invent in December 2017, AWS CEO Andy Jassy revealed that the business is at a revenue run rate of $18 billion, growing 42 percent per year. Those numbers are staggering and showcase the importance Amazon Web Services now plays in the technology plans of nearly every major corporation. Along with eye popping revenue numbers, AWS has continued to offer an unprecedented number of new features every year. At the conference, AWS claimed it has shipped 3,951 new features and services in the five years since the first keynote address by CTO Werner Vogels. This kind of feature and services proliferation can be daunting. Let’s explore a way to simplify. Navigating the Data Landscape The data landscape is broad, but can be simplified into a few key areas. In particular, we can assess solutions based on their ability to work with relational data, and their ability to support analytical or operational workloads. This breaks the market into four distinct categories: Databases, Data Warehouses, NoSQL, and Data Lakes. Let’s take a closer look at each of these categories and the AWS solutions within each. Databases AWS offers a number of database choices, and one of the popular services is Aurora, which takes single server database systems such as PostgreSQL and MySQL and implements them as a service. While these solutions offer some scalability benefits from the bare database instances, at the core, PostgreSQL and MySQL are single server systems. This works extremely well if your dataset fits into a single server, and if so, you should celebrate and enjoy the benefits of being able to deploy such an architecture. Data Warehouses Redshift is the most popular data warehouse on AWS and works well for offline analytics. In general, data processing systems can be ranked on their ability to load data quickly, deliver rapid responses to queries, and support for simultaneous users. Redshift does well on query performance, but can hit stumbling blocks for fast ingest and support for a large numbers of concurrent users. For example, the Redshift documentation advises As a best practice, we recommend using a total query slot count of 15 or lower. AWS also offers solutions for Hadoop (Elastic MapReduce) and ad hoc querying of data on object stores (Athena), but these tend to be focused on offline analytics without a need for a stringent, subsecond service level agreement. Data Lakes While Hadoop became synonymous with the Data Lake term, today more people are focusing on object stores such as S3 as a static repository for all of their data. S3 in many respects is the new Data Lake and serves this market extremely well. NoSQL The NoSQL movement was born due to legacy single server database challenges achieving scale and performance. But in achieving the performance and scale, NoSQL datastores left one thing behind…SQL! This is in part why single server databases such as MySQL and PostgreSQL continue to thrive in AWS…they support the functionality customers want. Amazon offerings in the NoSQL arena include DynamoDB and EMR. Another reason cited for the rise of NoSQL is the flexibility to work with different data types such as JSON, and the ability to change the database schema over time. Today, many new distributed solutions offer native SQL engines, flexible data models, and the ability to alter the schema while online. This begs the question of where NoSQL datastores fit going forward. Adapting to the New Data Landscape Rapid changes taking place in the data industry require a new look at the overall sector and how developers and architects should approach new application deployments. Deflated Expectations of Hadoop First is the recognition that data lakes will not solve the operational workloads that it was initially thought they would. The lack of transactional functionality such as INSERT, UPDATE, and DELETE for file systems and object stores relegated them to wonderful places to store data, but terrible places to process data. Database and data warehouse functionality is still sorely needed. NoSQL Fades to a Feature With NoSQL, the lack of analytics functionality means these datastores only form part of a solution, but not enough to provide the analytic context for modern applications. Today many people use Spark as part tow truck and part ambulance to rescue data out of NoSQL datastores where analytics are all but impossible. With new solutions offering distributed scale, persistence, and performance while retaining SQL, it begs the question how long NoSQL will survive on its own. Transactions and Analytics Together For modern applications, new datastores offer the ability to combine both transactional and analytical operations in one system. This provides a fast path to creating real-time applications. When data can land and be processed within the same system there is no need for an ETL (Extract, Transform, and Load) process. Today, all of the major analyst firms promote this vision in one way or another. Gartner uses the term HTAP, or Hybrid Transaction/Analytical Processing. Enabled by in-memory computing technologies, HTAP architectures support the needs of many new application use cases in the digital business era, which require scalability and real-time performance. Data and analytics leaders facing such demands should consider the following HTAP solutions supported by in-memory computing. 451 Research takes a slightly different approach by identifying this market as HOAP, or Hybrid Operational Analytical Processing. …we expect HOAP workloads to rapidly account for a significant proportion of incremental database revenue, and that supporting them will come to be expected in any mainstream operational database product or service. Finally, Forrester has also identified this new united transactional and analytical workload category, which they refer to as Translytical. Analytics at the speed of transactions has become an important agenda item for organizations. Translytical data platforms, an emerging technology, deliver faster access to business data to support various workloads and use cases. Enterprise architecture pros can use them to drive new business initiatives. Taking the preceding into account, we can envision a highly simplified data landscape: In this configuration, SingleStore serves as both a database and data warehouse serving operational and analytical needs. AWS S3 serves as a data lake to retain long term static data. Recapping AWS and SingleStore Data Solutions Let’s take a look again at each of the original data sectors and when to use or not use AWS solutions compared to SingleStore. Databases from AWS and SingleStore Aurora tends to dominate AWS and should be used when The dataset easily fits into a single server The performance requirements can be addressed by a single server Workloads are write-centric without a need for simultaneous reads and writes SingleStore should be used when The dataset size exceeds the capacity of a single server The performance requirements outpace those provided by a single server There is a need for simultaneous read and write workloads For more information on this topic check out a recent article in Data Center Knowledge, Graduating from Single Server Databases . Data Warehouses from AWS and SingleStore Redshift is the popular data warehouse on AWS and should be used when There are no requirements for fast data ingest or high user concurrency SingleStore is a single product that provides both database and data warehouse functionality. It should be used when There is a requirement for fast ingest; or There is a requirement for a large amount of user concurrency Data Lakes on AWS While Hadoop implementations are declining, object stores such as S3 provide a valuable service for static data and are quickly becoming the new “data lake”. SingleStore includes native ingest capabilities from S3, such as the ability to build real-time data pipelines streaming data from S3 into SingleStore. CREATE PIPELINE myS3pipeline AS\nLOAD DATA S3 \"mys3bucket/mydata/\"\nCREDENTIALS '{\"aws_secret_access_key\": \"my_aws_secret_access_key\", \"aws_access_key_id\": \"my_aws_access_key_id\"}'\nINTO TABLE mytable; NoSQL on AWS While DynamoDB provides a useful tool for key-value or document data storage, it does not provide inherent analytical capabilities, and therefore requires additional systems for a complete solution. This is also the case with datastores, such as Cassandra or MongoDB. Given that new distributed datastores such as SingleStore offer scale, performance, and SQL, it begs the question for where NoSQL solutions fit. For more on this topic please see the Network World article, The Hidden Costs of NoSQL . SingleStore and AWS Together, SingleStore and AWS provide a compelling platform for building real-time applications. SingleStore can handle both database and data warehouse workloads, which fit with the direction of new applications to combine transactional and analytical requirements. AWS provides useful services such as S3, as well as the most comprehensive Infrastructure-as-a-Service to stand up powerful solutions. Many SingleStore customers such as Tapjoy and Appsflyer deploy SingleStore on AWS today. Other customers such as Myntra have also found ways to integrate SingleStore with their existing AWS solutions such as Redshift to build a pipeline they refer to as Meterial . For more information on using SingleStore and AWS please connect with us at info@singlestore.com or visit www.singlestore.com .", "date": "2018-01-10"},
{"website": "Single-Store", "title": "a-brief-introduction-to-memsql", "author": ["Jenifer Ho"], "link": "https://www.singlestore.com/blog/a-brief-introduction-to-memsql/", "abstract": "We know choosing or evaluating a new database technology can be challenging due to the variety of choices available. In a recent webcast, we shared various use cases businesses face with traditional database and data warehouse technologies, key differentiators and architectures of SingleStore, sample applications and customer case studies, and a quick demo of SingleStore. SingleStore provides an adaptable database for real-time applications that unite transactions and analytics in a single high-performance platform. SingleStore uniquely delivers fast data ingestion with low latency while serving large numbers of simultaneous users. Here are common answers to questions from the webcast: Question: Is there an option to leverage in-memory for the column format OLAP queries? Answer: Yes, for optimal performance, SingleStore leverages both memory and disk for the columnstore engine. Question: Does SingleStore require partitioning of data for distributed ACID consistency? Answer: SingleStore automatically partitions data data, so this is built in. Question: Is there any IDE to use with SingleStore? Answer: SingleStore supports the MySQL wire protocol and works with many development IDEs including Sequel Pro, TOAD, Navicat, and MySQL Workbench to name a few. Question: Can SingleStore replace Teradata workloads and help us reduce cost? Answer: Yes. SingleStore is a distributed relational SQL database and data warehouse in one. The columnstore engine can query up to petabytes of data while providing a SQL end point to business intelligence tools, data scientists, and data analysts. SingleStore makes use of industry standard hardware for optimal cost performance compared to traditional data warehouse appliances. Question: Do you have use cases where users are storing large numbers of large images (tiff/jpg/png) such as microscopy or satellite imaging applications? Answer: While it is possible to store binary objects (such as images) in SingleStore, many customers use object stores like S3 for this purpose. However, the metadata associated with those images, and the ability to rapidly scan that structured metadata would be a good use case for SingleStore. Question: Is data automatically synced between row store and column store or does the DBA have to manually manage that transfer? Answer: Data is automatically placed in the table type or storage engine you choose. From there the DBA can move data as needed. You can learn more about how our storage engines work at our documentation page here . Question: Is the SingleStore shell using MySQL CLI or do you have your own CLI for BI access? Is there something other than ODBC? Answer: We are currently building a SingleStore CLI environment. Stay tuned for our next release! SingleStore supports JDBC and ODBC connectivity. Question: Is there a free developer version, perhaps limited size or functionality? Answer: We do offer a free developer edition with unlimited scale, although it does not include security or redundancy. Learn more about signing up at singlestore.com/free Question: Are the SingleStore features also supported for Azure as well? Answer: Yes, SingleStore can run on any cloud platform, or on-premises. Question: How are encryption keys managed? Are cryptographic hardware security modules (HSM) supported? Answer: We find it best for customers to manage their own encryption keys. Question: Regarding Geospatial support, is the data we would load into SingleStore in GeoJSON format the way it would be in a MongoDB environment? Answer: SingleStore supports string, number, or JSON format for Geospatial data. You can learn more about our Geospatial support here . Question: Does SingleStore support a cloud solution? Answer: Yes. SingleStore can be run on any public or private cloud including AWS, Microsoft Azure, and Google Cloud Platform. Question: Can you demo the ETL part? Answer: We would love to show you this. We’ll follow-up with you to schedule some time to demo this or book some time with one of our product experts at singlestore.com/demo . Question: What are the minimum memory requirements? Answer: For small development or prototyping, you can run SingleStore with 8GB of memory. Our hardware requirements are posted here . Missed the webcast? Get a copy of the recording to watch on demand here . To talk more in depth about your database challenges, please schedule time with a product expert at singlestore.com/demo .", "date": "2018-01-24"},
{"website": "Single-Store", "title": "select-5-from-memsql-blog-in-2017", "author": ["Gary Orenstein"], "link": "https://www.singlestore.com/blog/select-5-from-memsql-blog-in-2017/", "abstract": "2017 was a good year for the database market as developers, architects, and business leaders explored what is possible with new platforms. We covered several hot topics on the SingleStore blog including the integration with microservices, recognition by industry analysts, machine learning and image recognition, real-time geospatial analytics, and multi-tenancy in a cloud world. Here are a few of our favorite posts from 2017. SingleStore Meets the Microservices Architecture Link to post Microservices captured application developer attention in 2017 and our own Dale Deloy shared his take on how SingleStore fits this ecosystem. Each application service component has its own independent service deployment and flexible data model. The goal is to allow developers to focus on building business logic rather than coordinating with corporate models and infrastructure constraints. Gartner Magic Quadrant for Data Management Solutions for Analytics Link to post This Magic Quadrant showcases leaders in data warehousing, and SingleStore placed as a challenger, recognized in particular for its operational data warehouse capabilities. Gartner analysts Simon James Walker and Michael Patrick Moran also identified real-time data as a top priority for data and analytics leaders in the report, Three Top Trends in Master Data Management Solutions Through 2017, highlighting the following impacts and recommendations: Real-time data has emerged as a Master Data Management (MDM) priority for data and analytics leaders, with high availability master data a key capability as data volume, velocity and variety increases Multidomain and multivector MDM enable digital business to advance from “collecting” data for a single data domain to “connecting” several data domains Improved packaged industry MDM solutions are accelerating time to value, enabling data and analytics leaders to deliver successful MDM programs An Engineering View on Real-Time Machine Learning Link to post In this post, Eric Boutin, Director of Engineering at SingleStore, answered a few questions including: What do you see next in terms of new innovations in this arena? I would like this field to innovate from two different directions. On the one hand, I would like to see databases support more and more algebra primitives to allow expressing more complex machine learning models. For example, supporting matrices, vector/matrix operators, aggregation across multiple vectors, and so on. This would allow expressing a growing number of machine learning algorithms in SQL. Even neural networks can be expressed as a sequence of scalar and vector operations. I would then like to see machine learning framework ‘push down’ algorithms into databases using SQL. Video: Real-Time Analytics at UBER Scale Link to post Catch the details of the video and slides from Strata Data Conference San Jose, Real-Time Analytics at Uber Scale. Database Multi-Tenancy in the Cloud and Beyond Link to post Multi-tenancy is a game changer for software-as-a-service and other applications. Understand the differences between approaches from a database perspective from SingleStore teammate, Alec Powell. Separated Database Separate Schema Shared Schema Ringing in the New Year Stay tuned for more from SingleStore in 2018. If you have any topic requests feel free to ping us on Twitter @SingleStoreDB.", "date": "2017-12-27"},
{"website": "Single-Store", "title": "machine-learning-and-memsql", "author": ["Rick Negrin"], "link": "https://www.singlestore.com/blog/machine-learning-and-memsql/", "abstract": "What is Machine Learning? Machine learning (ML) is a method of analyzing data using an analytical model that is built automatically, or “learned”, from training data. The idea is that the model gets better as you feed it more data points, enabling your algorithm to automatically get better over time. Machine learning has two distinct steps, training, and operationalization. Training takes a data set you know a lot about (known as a training set), then explores the data set to find patterns, and develop your model. Once you have developed your model you move on to operationalization. This is where you deploy it to a production system where it runs to score new data, then the system returns the results to the user. How to Get Started with Machine Learning To accomplish these steps, you will commonly make use of several tools. You need a tool to bring the data in, a tool to cleanse the data, libraries to develop the calculations, and a platform for testing the algorithm. Once you are ready to operationalize the model, you need a compatible platform to run your model and an application to process and/or display the results. Using SingleStore for Machine Learning Operationalization SingleStore is a distributed database platform that excels at doing the kind of calculations typically found in a machine learning model. SingleStore is a great environment for storing training data, as the user can run it in a small configuration, such as a single node model on a laptop. Because SingleStore is compatible with MySQL, a data scientist could also use a MySQL instance for the algorithmic development. Where SingleStore really shines is the operationalization of the model. The key requirements for effectively operationalizing an algorithm are the following: Ingest the data quickly Fast calculations Scale out to handle growth Compatibility with existing libraries A powerful programming language to express the algorithm Operational management capabilities to ensure data durability, availability, and reliability SingleStore is a perfect fit for these requirements and can be used in an ML solution in a couple of different ways. Three ways to Operationalize ML with SingleStore Calculations Outside the Database SingleStore can be a fast service layer that both stores the raw data and serves the results to the customer. This is useful when the creation of the model is done with existing infrastructure, such as a Spark cluster. A real-world example of this is a large energy company that is using SingleStore for upstream production processing. The company has a set of oil drills all around the world. The drills are expensive to fix, because of the cost of parts and the cost of the labor (as the drills are often in remote locations). Keeping the drill from breaking results in a dramatic cost savings. The drills are equipped with numerous sensors (collecting heat, vibration, directionetc.) that continuously send data back to a Kafka queue. Data is pulled from this queue into a Spark cluster, where a PMML (Predictive Model Markup Language) model calculates the health of the drill. The scored data then lands in SingleStore, and is served to the drill operators in real time. This allows the operators to slow down or reposition the drill if it is in danger of damage. Having a data platform that can continuously ingest the scored data at high throughput, while still allowing the models to be run, is critical to delivering this scenario. Because SingleStore is a modern scale-out architecture and sophisticated query processor, it can handle data processing better than any other database in the industry. Calculations on Ingest Some customers don’t want to maintain a separate calculation cluster, but still want to make use of existing statistical or ML libraries. In this case, they can use the SingleStore Pipelines feature to easily ingest data into the database. Customers can then execute the ML scoring algorithm as data arrives, using the transform capability of Pipelines. Transforms are a feature that allow customers to execute any code on the data prior to its insertion in the database. This code can easily integrate or call out to existing libraries, such as TensorFlow. The results of the calculations are then inserted in the database. Because SingleStore is a distributed system and SingleStore Pipelines run in parallel, the workload is evenly distributed over the resources of the cluster. Calculations in the Database Sometimes it is more efficient to do the scoring calculations as close to the data as possible, especially when the new data needs to be compared with a larger historical set of data. In this case, you need a language to encode the algorithm in the database. It is important the language is expressive enough to enable the algorithm and core operations fast, allowing efficient querying over the existing data, and can be composed with other functionality. One example of an organization that has successfully used this approach is Thorn, a non-profit that uses image recognition to find missing and exploited children. The application keeps a set of pictures of exploited children in its system, and matches the faces of those children to new pictures that are continuously culled from websites around the country. The new pictures are reduced to vectors using a deep learning-based approach, and are matched against vectors representing the base pictures. Prior to using SingleStore, the matching process would take hours or days. By using the SingleStore high-performance vector DOT _ PRODUCT built-in function, processing the incoming pictures can be done in minutes or seconds. Another image recognition example is Nyris.io, which uses a similar technique to match product photos using deep learning coupled with fast in-database DOT _ PRODUCT calculations. The application quickly matches user provided images with reference product images to enable ecommerce transactions. To build an operational ML application with SingleStore, please visit singlestore.com.", "date": "2018-01-30"},
{"website": "Single-Store", "title": "how-machine-learning-artificial-intelligence-affects-workplace", "author": ["Kristi Lewandowski"], "link": "https://www.singlestore.com/blog/how-machine-learning-artificial-intelligence-affects-workplace/", "abstract": "Recently, SingleStore commissioned a survey with O’Reilly Media to learn more about the adoption of artificial intelligence (AI) and machine learning (ML) in the workplace. According to the survey of over 1,600 respondents, 61 percent of respondents, regardless of company size, indicated ML and AI as their companies’ most significant data initiative for next year, when asked to pick from several options likely to be important concerns in today’s climate; with big data and business analytics initiatives coming in at a close second (58 percent). Additional Key Survey Findings 65 percent respondents using and planning to use ML/AI cited that a key aspect of adopting ML and AI was to enable more informed business decision making, underscoring the importance of these technologies for analytics. 74 percent of all respondents consider ML and AI to be a game changer, indicating it had the potential to transform their job and industry. Of those indicating they actively use ML and AI, 58 percent indicated they ran models in production. The findings also suggest that uses of such technologies are evolving rapidly, with 77 percent of respondents actively using ML/AI indicating that creating new models was part of their short-term goals. As ML and AI technologies spread through organizations, the need for data scientists and other technical professionals is growing. To make the jobs of these professionals easier, they will need to assess the technology infrastructure stacks already in place to support the new technologies. These survey results underscore the need for an intelligent database that can support advanced workloads. SingleStore is the database for transactions and analytics at scale, allowing data scientists to do model and operationalize advanced computations quickly. SingleStore has native integrations with Apache Kafka, including exactly once semantics, and a high performance parallel connector to Apache Spark. To learn more about SingleStore, visit singlestore.com to get started on modernizing your data and application infrastructure.", "date": "2018-02-07"},
{"website": "Single-Store", "title": "2018-gartner-magic-quadrant-for-data-management-solutions-for-analytics", "author": ["Kristi Lewandowski"], "link": "https://www.singlestore.com/blog/2018-gartner-magic-quadrant-for-data-management-solutions-for-analytics/", "abstract": "The data management solutions for analytics market is evolving. Disruption is accelerating in this market, with more demand for broad solutions that address multiple data types and offer different delivery models. We are hosting complimentary access to the full Gartner Magic Quadrant for Data Management Solutions for Analytics, so you can learn more about what’s happening in this space. Access Here → SingleStore Positioned as Challenger in Gartner Magic Quadrant SingleStore has been positioned in this Magic Quadrant for three consecutive years, and this year, it was named a Challenger based on its ability to execute and completeness of vision. To view the complete report, visit singlestore.com .", "date": "2018-02-14"},
{"website": "Single-Store", "title": "go-beyond-legacy-data-with-change-data-capture-memsql-and-real-time-applications", "author": ["Kamran Hussain"], "link": "https://www.singlestore.com/blog/go-beyond-legacy-data-with-change-data-capture-memsql-and-real-time-applications/", "abstract": "Data is driving innovative customer experiences, operation optimization, and new revenue streams. Data infrastructure teams are being asked to update their legacy infrastructure to respond to changing business conditions without disrupting existing operations. The future of data management is modernizing legacy systems using real-time data synchronization with modern databases that can accelerate innovation without impacting existing applications. The Challenge of Legacy Systems A constant challenge of legacy systems is the need to create new growth opportunities with a lower total cost of ownership while keeping current production systems running. In other words, without impacting production systems used by both enterprise users and application services, there needs to be a way to transform batch-based applications to real-time applications. In this blog post, we examine how easy it is to “have our cake and eat it, too”. SingleStore helps enterprises implement modern data applications without disrupting existing enterprise application architectures. For example, take the fairly typical and traditional approach below for deriving value from business data. Built over 20 years ago, this approach requires batch processing of data, which is time consuming and subject to costly failures. Data is batch loaded from the transactional database into the analytics data warehouse. A tremendous amount of time and processing power is required to extract, transform, and load the batched data. Once loaded into the data warehouse, business intelligence tools generate reports and ad-hoc analyses. A Failed Batch is like a Clogged Pipe However, if one batch contains errors and needs to be reloaded, the time it takes to reload the repaired data immediately starts to impact the queue of current batches. Very quickly, a backlog of batches grows. This creates a clogged pipe of enterprise data. And for this reason, legacy architectures that rely on batch processing fails to meet the real-time requirements that modern applications demand. CDC and SingleStore to the Rescue Using a Change Data Capture (CDC) solution and SingleStore, it is easy to augment the traditional enterprise data architecture, and transform it into a modern powerhouse. SingleStore quickly and easily plugs into the legacy ecosystem without disrupting it, as do many CDC solutions. This allows companies to leverage legacy investments while enabling real-time applications, machine learning, and artificial intelligence. Implement CDC and SingleStore In today’s market, there are various CDC vendors including Attunity, DBVisit, GoldenGate, HVR Replicate, and others. For all CDC solutions, the steps to implement them are relatively similar. You can start migrating data to SingleStore with any of these CDC solutions without impacting the source database application. Step 1: Create the source database objects in SingleStore With SingleStore as part of your ecosystem, it’s easy to create the existing database objects. Some of the CDC solutions will generate the required Data Definition Language (DDL) and automatically create them in SingleStore as part of the initial-load process.  We also have a small Java utility ‘GenMemDDL’ to generate the DDL if needed. Step 2: Load a snapshot of the source data into SingleStore First, start the process to create a snapshot of the source database. The data will get batched and written to SingleStore. During the snapshot load, a change-synchronization process captures the incremental data changes made on the source system. Those changes are applied to SingleStore after the snapshot load is complete. Depending on the network speed and the data size, the copying of historical data can take hours or days. But since this is a background process managed by the CDC tool, there is no downtime to active users in the production environment. Using Attunity Replicate for Building CDC Applications Attunity Replicate is an industry-standard CDC solution. Attunity enables transactional change data capture between operational and analytical enterprise systems. Attunity also supports generating DDL from the source database directly into SingleStore, eliminating the need to do the conversion manually. The initial data load method extracts data from an Oracle source table to a CSV file format. Next, using a LOAD DATA command, Attunity loads data file into SingleStore. Depending on your dataset, you can specify the file size. For a sanity check, we’ll do a count of the tables in the Oracle database source: SOE> @counts LOGON WAREHOUSES ORDER _ ITEMS  CUSTOMERS  ADDRESSES INVENTORIES ORDERS CARD _ DETAILS PRODUCT _ DESCRIPTIONS ———- ———- ———– ———- ———- ———– ———- ———— ——————– PRODUCT _ INFORMATION ORDERENTRY _ METADATA ——————- ——————- 714895 1000  1286818     300000 450000      899903 428937     450000    1000 1000      0 First, create the Oracle source and SingleStore target connection definition in Attunity. The connection information is in the odbc.ini file in /etc on your Linux midtier server. Next, create an Attunity Task to define the source and target: As part of the Task setup, select the source tables to replicate: Start the task, and Attunity will replicate the data for the selected table from Oracle to SingleStore. Once the task is complete, you can do a count on SingleStore to confirm the data matches. memsql> source counts.sql+——–+————+————-+———–+———–+————-+——–+————–+———————-+———————+———————+ | LOGON  | WAREHOUSES | ORDER _ ITEMS | CUSTOMERS | ADDRESSES | INVENTORIES | ORDERS | CARD _ DETAILS | PRODUCT _ DESCRIPTIONS | PRODUCT _ INFORMATION | ORDERENTRY _ METADATA | +——–+————+————-+———–+———–+————-+——–+————–+———————-+———————+———————+ | 714895 |       1000 |     1286265 |    300000 |    450000 |      899903 | 428937 |       450000 |                 1000 |                1000 |                   0 | +——–+————+————-+———–+———–+————-+——–+————–+———————-+———————+———————+ 1 row in set (0.05 sec) Step 3: Apply the CDC data into SingleStore While the initial load task runs, Attunity also captures all the changes made by the application users and services on the source system. The CDC solution serializes a queue of the data changes. Once the initial load completes, Attunity applies the queue of changes to SingleStore. The active queue of changes serves as the synchronization mechanism between the source database and SingleStore. The CDC solution captures all newly committed transactions and replicates the new transactions to SingleStore. Depending on how much data has been queued, it will take some time to apply those changes. Once all the changes have been applied, SingleStore is now completely synchronized with the source. With CDC and SingleStore, Data is Ready for Real Time With CDC solutions such as Attunity Replicate, it is easy to migrate and synchronize production databases and data warehouses with SingleStore. Most importantly, the production systems are not affected. System users and application services continue to run without downtime. And you don’t have to worry about batch processing causing clogs in your data pipes. With SingleStore as a vital technology in your enterprise ecosystem, you can leverage your legacy investment and start building real-time applications. For SingleStore to work with Attunity Replicate, you will need a patch for your Attunity environment. Contact us to get the patch, and we’ll help you get started.", "date": "2018-02-23"},
{"website": "Single-Store", "title": "top-5-advanced-tips-tricks-memsql-201-webinar", "author": ["Alec Powell"], "link": "https://www.singlestore.com/blog/top-5-advanced-tips-tricks-memsql-201-webinar/", "abstract": "In a recent webcast , we shared tips and tricks for understanding SingleStore, best practices for implementation, and demoed SingleStore with a real-world use case. Here are five top tips and tricks we shared: When moving an application to, or creating one on SingleStore, start by thinking about whether rowstore or columnstore storage (or both) is ideal If you have high throughput requirements, consider using SingleStore Pipelines Use EXPLAIN and PROFILE operators to identify query bottlenecks Take advantage of reference tables to eliminate distributed joins Using and monitoring Management Views within the information schema is a great way to profile your cluster workload We also took questions from webcast attendees, and here’s what we said: Question: What if my use case doesn’t fit into rowstore or columnstore? What if it has some transactional elements and some analytical? Answer: Some customers have a notion of “active” data that fits in memory, and once that data is expired or finished updating, it is moved to a columnstore table with the same schema. Ultimately, this is workload dependent. With our customers, we scope test workloads and look at the performance of queries on each model. Our sales engineering team members are experts in this domain. Question: What are the benefits of the SingleStore columnstore compared to other Massively Parallel Processing columnar systems? Answer: SingleStore columnstore facilitates real-time streaming data loading through in-memory optimized data structures, and caching metadata allows greater performance via segment elimination during query processing. We also use query vectorization techniques that take advantage of encodings and hardware instruction sets to get performance gains. Question: Why are Pipelines the best way to ingest data into SingleStore? Answer: Within SingleStore Pipelines, each leaf node partition in SingleStore has its own “worker” process to ingest data in parallel, as opposed to inserting data normally through an aggregator node. This allows the ingest to leverage the distributed computing power of the cluster and to run concurrently, in contrast to traditional serialized ingest methods. Question: How do the shard key and sort key differ in practice? Answer: The shard key applies to every SingleStore table and refers to the distribution of data among the leaf nodes in a cluster. The sort key only applies to columnstore tables, and facilitates better performance by aligning the ordering of data on disk with the ordering in which it is likely to be accessed by queries. Hence, it is necessary to look at the most frequently run queries on your table when determining the ideal ordering of fields in the sort key. Question: What is the difference between a broadcast and a reshuffle operation? Answer: A broadcast operation moves a set of rows cleanly across the network to each of the other nodes to complete a distributed join. In a reshuffle (or repartition) operation, one side of the join is reshuffled (has its shard key recalculated with a different set of key columns) to match the shard key of the table on the other side of the join. Each respective operation is useful in different contexts when optimizing the join performance of a workload. Question: Why would I want to run PROFILE? Answer: The query profiler will give you runtime statistics on your query, including operators, execution time per operator, and amount of data flowing through the network. Question: How can I monitor the SingleStore SHOW STATUS EXTENDED or Management Views metrics? Answer: Come to our next webinar, Operationalizing SingleStore , for tips on how to do so! Missed the webcast? Get a copy of the recording to watch on demand here . To learn more or ask specific product questions, reach out to team@singlestore.com .", "date": "2018-02-16"},
{"website": "Single-Store", "title": "full-text-search-in-memsql", "author": ["Rick Negrin"], "link": "https://www.singlestore.com/blog/full-text-search-in-memsql/", "abstract": "Today, we are sharing that SingleStore now has Full-Text Search, a highly requested feature, built into the product. Thanks to customer feedback, we are delighted to make it available for all companies building real-time applications. What is Full-Text Search? You might be thinking, “SingleStore is pretty fast at searching things and they already support large strings, so why do they need to add anything?” So let’s start with a description of Full-Text Search (FTS). Full-Text Search is different than querying for a string value in two ways. The first is a performance difference. Sometimes when you search for text you know exactly what you are looking for. For example, if you want to see a list of mystery books you would ask the database to find all the books with a subject of “Mystery”, easy! But what if you have many documents, each with a lot of words in it, and you don’t know what information the documents contain. Specifically, suppose you wanted to find all the documents that talk about “Canada”. You could use a LIKE query or a regular expression, but that is going to be pretty slow with a regular database index, no matter how fast your database is. What you need is a different kind of index that keeps track of all the words in the document and how often they appear.  So when you search for Canada, the system can quickly determine if the word Canada appears in the document and how many times. This is called an inverted index. Secondly, your search might have a lot of matches. FTS includes the notion of relevance. Relevance tells you not just whether there is a match but also how good the match is. This allows you to sort the results showing the most relevant results first. The combination of the inverted index data structure, stemming capabilities and relevance scoring are why Full-Text Searching is advantageous over searching strings using regular database queries. How is Full-Text Search integrated with SingleStore? SingleStore is a system that understands the notion of an index, so integrating FTS into SingleStore was relatively straightforward. We just had to teach SingleStore to understand this new kind of index. Like most technologies that support full-text indexing, we took advantage of the open-source Lucene technology. Lucene is a robust existing technology that is essentially the standard for full-text searching in the industry. It has been around a long time, so we didn’t have to reinvent the wheel. To create an index, a user simply has to specify which columns they want to be full-text indexed when creating the table. Under the covers this causes a full-text index to be created. The index is partitioned using the same sharding pattern as the table, allowing for a distributed index that can also participate efficiently in local joins. The lifecycle of the index is governed by the lifecycle of the table, so you don’t have to do any extra work. All the management is handled by the system (failover, rebalance, node addition and removal, backup/restore, etc.). In addition, the index is bound to the same security mechanisms as the table, so the data is protected by the role-based access control features in SingleStore. This ensures the data in your index is only available to the users who have been granted permission to see the data. So all the management of the full-text capability is transparent to the user. To make use of the index, you simply include the MATCH statement directly in your SQL query. This is the great part about the integration with SingleStore. You can mix and match your full-text search along with structured search in your filter. This makes the process very easy for someone familiar with SQL to take advantage of the full-text index. Because the index is automatically sharded, users get to take advantage of all the machines in the cluster to execute the lookup in the index resulting in blazing fast results. The full-text query itself is the standard Lucene syntax, which will be familiar to those who have used other full-text systems. How would I use it? Utilizing FTS in conjunction with a regular database application is helpful across a number of different use cases. Here are a few examples. AutoComplete The simplest case is doing something such as an autocomplete functionality in a search box for your application. In this case, there are no documents in the application, it simply wants to quickly identify which results of a string column match a subset of characters. A simple query against the table using the match statement and returning the full-text of the column accomplishes the task. Mixing Structured and Unstructured Query A more complex example shows up in industries such as insurance or real-estate, which are inherently document-centric. In these use cases, the documents are a core part of the data set. Let’s take a real estate transaction as an example. There is a lot of data that is well understood (and therefore easy to structure) such as location, number of bedrooms and bathrooms, year built, etc. So you can imagine if someone wants to query the system using a structured query to limit a specific location and age of house, but also look for agreements that have a particular phrase in the doc. In this case, the user constructs a query with a structured search filter for location and age and adds a match statement in the filter to look for the phrase. Some Examples First let’s create a table. This looks like a regular CREATE TABLE statement. The only thing added is the FULLTEXT statement with the column names “body” and “title” added. This means we want a full-text index on the body column and the title column. Note you can only do full-text on string types. That’s it, your full-text index is ready to use. Just insert some data and you can start writing queries. CREATE TABLE books(\n  id INT UNSIGNED,\n  title VARCHAR(100),\n  publish_year INT UNSIGNED,\n  body TEXT,\n  KEY (id) USING CLUSTERED COLUMNSTORE,  \n  FULLTEXT (title, body)); Now let’s put some data into the table. INSERT INTO books VALUES (1, 'Typing Test', 1981, 'The quick brown fox jumped over the lazy dog');\nINSERT INTO books VALUES(2, 'Ylvis', 2014, 'What does the fox say');\nINSERT INTO books VALUES(3, 'Gone With the Wind', 1936, 'In her face were too sharply blended the delicate features of her mother, a Coast aristocrat of French descent, and the heavy ones of her florid Irish father. But it was an arresting face, pointed of chin, square of jaw. Her eyes were pale green without a touch of hazel, starred with bristly black lashes and slightly tilted at the ends. Above them, her thick black brows slanted upward, cutting a startling oblique line in her magnolia-white skin--that skin so prized by Southern women and so carefully guarded with bonnets, veils and mittens against hot Georgia suns.'); Now we run optimized tables to ensure the index is updated. (Inserts are flushed asynchronously to the index). You don’t need to do this in practice as it will flush reasonably quickly, but for the purposes of this example you want to make sure the inserts are in the index. OPTIMIZE TABLE books FLUSH; Here is an example of a basic full-text query. The query looks for all rows that have a title or body with the string “fox” in it. SELECT *\nFROM books\nWHERE MATCH (title, body) AGAINST ('fox'); This example uses a structured predicate (matching only books published in 2014) with a full-text predicate (books with the word fox in it). SELECT count(*)\nFROM books\nWHERE publish_year = 2014 AND MATCH (body) AGAINST ('fox'); We also support returning a relevance score, which is a score for how successful the match is. You can put the relevance as an output column in your query. SELECT id, title, MATCH (body) AGAINST ('Fox') relevance\nFROM books\nWHERE MATCH (body) AGAINST ('Fox'); You can also use the relevance score in the WHERE clause to filter the results. SELECT id, title, MATCH (body) AGAINST ('fox')\nFROM books\nWHERE MATCH (body) AGAINST ('fox') >= 0.12; Doing FTS with SingleStore is that simple. But don’t take my word for it. Download the beta , and start playing with the Full-Text Index feature right now.", "date": "2018-03-06"},
{"website": "Single-Store", "title": "database-evaluations-distributed-database-architectures-more", "author": ["Gene Miguel"], "link": "https://www.singlestore.com/blog/database-evaluations-distributed-database-architectures-more/", "abstract": "Recently, we hosted a special meetup at our headquarters in San Francisco for the community, and shared some great talks. The slides for each talk and the video presentations have been made available below. Drew Paroski, SingleStore VP of Engineering and Adam Prout, SingleStore Chief Architect delivered a fun talk about taking a methodical approach for making a decision, dug into interesting tradeoffs, and gave tips about what to look for under the hood and how to evaluate the tech behind the database, all wrapped within a Star Wars theme. Slides for “ An Engineering Approach to Database Evaluations ” Rodrigo Gomes, a SingleStore engineer, highlighted some of the challenges to building a fault tolerant distributed architecture, and how the SingleStore architecture tackles these challenges. Slides for “ Building a Fault Tolerant Distributed Architecture ” Tewei Luo, another SingleStore engineer, demoed an upcoming feature in SingleStore DB 6.5 showing how advanced stream processing use cases can be tackled with a combination of stored procedures (new in 6.0) and the SingleStore Pipelines feature. Slides for “ Stream Processing with Pipelines and Stored Procedures ” Check out the below video to see all the presentations from the meetup. To stay up to date with all of our meetups, follow us on Meetup.com where we post our upcoming events.", "date": "2018-02-27"},
{"website": "Single-Store", "title": "memsql-processing-shatters-trillion-rows-per-second-barrier", "author": ["Eric Hanson"], "link": "https://www.singlestore.com/blog/memsql-processing-shatters-trillion-rows-per-second-barrier/", "abstract": "Last week at the Strata Data Conference in San Jose, I had the privilege of demonstrating SingleStore processing over a trillion rows per second on the latest Intel Skylake servers. It’s well known that having an interactive response time of under a quarter of a second gives people incredible satisfaction. When you deliver response time that drops down to about a quarter of a second, results seem to be instantaneous to users. But with large data sets and concurrency needs, giving all customers that level of speed can seem beyond reach. So developers sometimes take shortcuts, such as precomputing summary aggregates. That can lead to a rigid user experience where if you tweak your query a little, for example adding an extra grouping column, suddenly it runs orders of magnitude slower. And it also means your answers are not real time, i.e. not on the latest data. SingleStore gives you the interactive response time your users want, on huge data sets, with concurrent access, without resorting to precomputing results. Running at a Trillion Rows Per Second SingleStore DB 6, which shipped in late 2017, contains new technology for executing single-table group-by/aggregate queries on columnstore data incredibly fast. The implementation is based on these methods: (1) operations done directly on encoded (compressed) data in the columnstore, (2) compilation of queries to machine code, (3) vectorized execution, and (4) use of Intel AVX2 single instruction, multiple data (SIMD) enhancements. When the group-by columns are encoded with dictionary, integer value, or run-length encoding, SingleStore runs a one-table group-by/aggregate at rates exceeding three billion rows per second per core at its peak. The fewer the number of groups and the simpler the aggregate functions, the faster SingleStore goes. This incredible per-core speed gave us the idea to shoot for the trillion-rows-per-second mark. To accomplish this, with a realistic query, I wrote a data generator to build a data set that simulates stock trades on the NASDAQ. Then we talked to our partners at Intel, and they generously gave us access to servers in their lab with the latest Skylake processors. These machines have two Intel® Xeon® Platinum 8180 processors each, which have 28 cores, for a total of 56 cores per server. I created a SingleStore cluster with one aggregator node and eight leaf nodes, with one server for each node, as shown in Figure 1. This cluster had 2 * 28 * 8 = 448 total cores on the leaves — the most important number that determined the overall rows-per-second rate we could get. Figure 1. The hardware arrangement used to break a trillion rows per second. I installed SingleStore on this cluster with two SingleStore leaf nodes on each leaf server, with non-uniform memory access (NUMA) optimizations enabled, so each SingleStore leaf node software process would run on a dedicated Skylake chip. Then I created a database trades with one partition per core to get optimal scan performance. Once that was complete, I loaded billions of rows of data representing stock trades (really decades worth of trades) into a table called trade. The larger capitalization a stock is, the more trades it has. Here’s a tiny sample of the data: memsql> select id, stock_symbol, shares, share_price, trade_time\nfrom trade\nlimit 10;\n\n+-------------+--------------+-----------+-------------+----------------------------+\n| id          | stock_symbol | shares    | share_price | trade_time                 |\n+-------------+--------------+-----------+-------------+----------------------------+\n| 10183273878 | CXRX         | 600.0000  | 22.0000     | 2018-03-08 08:50:57.000000 |\n| 10184155113 | CXRX         | 700.0000  | 31.0000     | 2018-03-08 08:50:57.000000 |\n| 10183273871 | CXRX         | 500.0000  | 8.0000      | 2018-03-08 07:50:57.000000 |\n| 10185917724 | CXRX         | 1000.0000 | 63.0000     | 2018-03-08 06:50:57.000000 |\n| 10183273873 | CXRX         | 1000.0000 | 74.0000     | 2018-03-08 04:50:57.000000 |\n| 10183273874 | CXRX         | 800.0000  | 96.0000     | 2018-03-08 10:50:57.000000 |\n| 10183273865 | CXRX         | 600.0000  | 82.0000     | 2018-03-08 06:50:57.000000 |\n| 10183273876 | CXRX         | 600.0000  | 40.0000     | 2018-03-08 05:50:57.000000 |\n| 10183273877 | CXRX         | 700.0000  | 47.0000     | 2018-03-08 05:50:57.000000 |\n| 10183273869 | CXRX         | 100.0000  | 15.0000     | 2018-03-08 05:50:57.000000 |\n+-------------+--------------+-----------+-------------+----------------------------+ This resulted in how many rows? Check this out: memsql> select format(count(*), 0) as c from trade;\n+----------------+\n| c              |\n+----------------+\n| 57,756,221,440 |\n+----------------+ That’s right, about 57.8 billion rows! Then I ran this query to find the top 10 most traded stocks of all time: select stock_symbol, count(*) as c\nfrom trade\ngroup by stock_symbol\norder by c desc\nlimit 10; And got this result: +--------------+----------+\n| stock_symbol | c        |\n+--------------+----------+\n| GILD         | 39321600 |\n| AABA         | 39190528 |\n| KHC          | 39190528 |\n| AMZN         | 39190528 |\n| ASML         | 39059456 |\n| CHTR         | 39059456 |\n| TXN          | 39059456 |\n| FB           | 39059456 |\n| CELG         | 39059456 |\n| TIG          | 39059456 |\n+--------------+----------+\n10 rows in set (0.05 sec) 10 rows in set (0.05 sec) Yes, this query ran in 5 one-hundredths of a second! To get an accurate scan rate, we need some more significant figures though. So I wrote a stored procedure to run it 100 times in a loop: memsql> call profile_query(100);\nQuery OK, 0 rows affected (4.51 sec) This gives an average response time for the query of 0.0451 second, which yields the following scan rate: memsql> select format(count(*)/0.0451, 0) as c from trade;\n+-------------------+\n| c                 |\n+-------------------+\n| 1,280,625,752,550 |\n+-------------------+ Yes, that is 1.28 trillion rows per second! We’re scanning and processing 2.86 billion rows per second per core, through the beauty of operations on encoded data, vectorization, and SIMD. We’re actually spending some time on every row, and not precalculating the result, to achieve this. What does it mean? The fact that we can break the trillion-row-per-second barrier with SingleStore on 448 cores worth of Intel’s latest chips is significant in a couple of ways. First, you can get interactive response time on mammoth data sets without precalculating results. This allows more flexible interaction from users, and encourages them to explore the data. It also enables real-time analytics. Second, it allows highly concurrent access by hundreds of users on smaller data sets, with all of them getting interactive response times. Again, these users will get the benefit of analytics on the latest data, not a precalculated summary snapshot. SingleStore enables all this through standard SQL that your developers already are trained to use. Also, we achieved this on industry-standard hardware from Intel, not a special-purpose database machine. These are machines that your organization can afford to buy, or that you can rent from your favorite cloud provider, without zeroing out your bank account. What could you do for your users with SingleStore on today’s Intel chips? Try us out , and watch below how we achieved a trillion rows per second. Appendix: For you hardware geeks out there If you’re craving more details about the Skylake chips we used, here’s the output of the Linux lscpu utility run on one of the servers making up the 9 total servers in the cluster: Architecture: x86_64\nCPU op-mode(s): 32-bit, 64-bit\nByte Order: Little Endian\nCPU(s): 112\nOn-line CPU(s) list: 0-111\nThread(s) per core: 2\nCore(s) per socket: 28\nSocket(s): 2\nNUMA node(s): 2\nVendor ID: GenuineIntel\nCPU family: 6\nModel: 85\nModel name: Intel(R) Xeon(R) Platinum 8180 CPU @ 2.50GHz\nStepping: 4\nCPU MHz: 2501.000\nBogoMIPS: 4993.28\nVirtualization: VT-x\nL1d cache: 32K\nL1i cache: 32K\nL2 cache: 1024K\nL3 cache: 39424K\nNUMA node0 CPU(s): 0-27,56-83\nNUMA node1 CPU(s): 28-55,84-111 The data was stored on SSDs, though that isn’t really significant since we ran the queries warm-start, so the data was already cached in memory. The network is 10-gigabit ethernet.", "date": "2018-03-13"},
{"website": "Single-Store", "title": "understanding-query-optimization-ai-enterprise-applications", "author": ["Gary Orenstein"], "link": "https://www.singlestore.com/blog/understanding-query-optimization-ai-enterprise-applications/", "abstract": "Just like IoT, around in various forms for years, AI has been prevalent in nearly every enterprise for decades. The catch? It was hidden inside the databases and data warehouses in use under the banner of Query Optimization. Query optimization is the process where a database or data warehouse takes the input from a user’s question, and reworks that question to deliver a quick response with as few compute resources as possible. The number of query plan choices reach far beyond what a human can calculate. An algorithm sifting through choices for the best plan makes applications faster since it receives data more quickly. Let’s take a closer look at the basics of query optimization, how it aligns with the principles of artificial intelligence, and the benefits delivered to enterprises. Understanding Query Optimization When an application or analyst requests information from a database or data warehouse, the question is phrased as a query, most frequently in SQL, the Structured Query Language. An extremely simple query might be the following: SELECT * FROM Customers\nWHERE Country='Canada'; This simple question requires searching everything in the Customers table to find results where the Country equals Canada. But SQL is a rich language that offers many permutations of how users or applications can write or assemble queries. Turning the query into a function the database can execute is the core of query optimization. And in general, there is a massive space of possible plans, which can answer a specific question asked by a user. A more complex query might be the following Query (#17) from the well known TPCH benchmark. According to the benchmark report: This query determines how much average yearly revenue would be lost if orders were no longer filled for small quantities of certain parts. This may reduce overhead expenses by concentrating sales on larger shipments. Answering the following Business Question The Small-Quantity-Order Revenue Query considers parts of a given brand and with a given container type and determines the average lineitem quantity of such parts ordered for all orders (past and pending) in the 7-year database. What would be the average yearly gross (undiscounted) loss in revenue if orders for these parts with a quantity of less than 20% of this average were no longer taken? The query is expressed as: SELECT Sum(l_extendedprice) / 7.0 AS avg_yearly\nFROM   lineitem,\n      part\nWHERE  p_partkey = l_partkey\n      AND p_brand = 'Brand#43'\n      AND p_container = 'LG PACK'\n      AND l_quantity < (SELECT 0.2 * Avg(l_quantity)\n                        FROM   lineitem\n                        WHERE  l_partkey = p_partkey); Given the quantity section of the query SELECT 0.2 * Avg(l_quantity)\nFROM   lineitem\nWHERE  l_partkey = p_partkey The system would have to run this SELECT to compute the average quantity order for each part individually. As an example, a query optimizer can rewrite this correlated subselect to the following: SELECT Sum(l_extendedprice) / 7.0 AS avg_yearly\nFROM   lineitem,\n      (SELECT 0.2 * Avg(l_quantity) AS s_avg,\n              l_partkey             AS s_partkey\n       FROM   lineitem,\n              part\n       WHERE  p_brand = 'Brand#43'\n              AND p_container = 'LG PACK'\n              AND p_partkey = l_partkey\n       GROUP  BY l_partkey) sub\nWHERE  s_partkey = l_partkey\n      AND l_quantity < s_avg This speeds up the query by requiring fewer scans of the lineitem table. In the original query, the computation is needed for every distinct part. But with the query optimization rewrite, we compute all of the averages at once for the different parts then use those results to complete the query. More information on the technical details of query optimization are available in this paper from the Very Large Data Bases (VLDB) Conference. So the query optimizer will make decisions based on the query and how the data is located in the database across tables and columns. There are standard rules that apply for certain query shapes, but very few cases where the query plan chosen is always the same. World class query optimization generally relies on three things. Cardinality Estimates Cardinality estimates include predictions such as how many rows are going to get matched from every table with this query, and once you join the tables, what is the intermediate size of the dataset. Query optimizers will use this information to estimate the costs of potential query plans. Costing Costing simulates the expense of the algorithm choices and picks the one it thinks is best. You need to consider and calculate alternative query plans with costing to pick the most appropriate choice. Heuristics and rules can lead towards certain directions, but as with many things, past performance is not a predictor of future results. The statistics are never perfect and the query optimization process involves mistakes and missed estimates that result from some guesses. In general, query optimization is trying to make the best choice, but the goal is not to pick the single best plan, as much as it is to pick among the best plans so that the downside is limited. Limitations are a fact of life as the statistics will never be perfectly predictive of query results. This process closely resembles many AI workflows in use elsewhere. In thinking about Estimates and Costing, remember that the estimates do not take anything into account about the data itself. An estimate on one database would be just as valid as an estimate on another database, whereas the Cost represents how expensive a query would be to execute on a specific dataset and potentially a specific cluster configuration. Data Distribution Query optimizers need to understand the data distribution in order to figure out how the query plan will work. SQL provides a functional declaration of the desired results but it is up to the database to figure out how to orchestrate the query. Understanding data placement in the system makes this possible. The process is similar to the classic salesperson dilemma of wanting to visit several customers in one day and figuring out how to do it with the most efficiency in traveling between sites. They need a map and a way to measure distance to make an accurate assessment. How Query Optimization Aligns with AI Many AI workflows are based on analyzing large quantities of information with algorithms to search for patterns. Query optimization is similar with a very focused approach of calculating predictions for how the system can and should run a particular query. With queries, it does not have to be complex for there to be a million ways to execute it. These combinations require large amounts of compute power and sophisticated algorithms to calculate the best outcomes. Query optimization studies the data in your table and learns from the data to make predictions about your query. It then uses those predictions to identify the most efficient ways to run operations. Even when query optimization gets a prediction wrong, it will work over time by collecting more statistics to provide more context. This approach also follows the basic patterns of AI. Benefits of Query Optimization for Enterprises Query optimization directly benefits enterprises running data-rich applications in the following ways. Faster results for better performance Applications that can access query results quickly allow businesses to operate in real time. Queries that may have taken hours can complete in minutes, and queries that may have taken minutes can complete in seconds. With a powerful query processing engine, many queries can be returned in well under one second. All of this allows businesses to react faster with better decision making. Fewer compute resource for reduced costs Well constructed query plans, generated through optimization, consume fewer compute resources allowing companies to process more data at a lower cost. Support more users with greater efficiency With powerful query optimization, systems can support more users at a single time. This may include internal analysts where a large number can access a single well-curated dataset. Or it may involve keeping up with thousands or millions of end users interacting with a mobile application simultaneously. Understanding Query Optimization and Distributed Systems In a new world of distributed systems, query optimization becomes even more important. Traditional databases had cost models that involved both CPU and disk I/O use. But in newer distributed systems, an additional important metric is network usage, and how much processing will take place across system nodes, instead of within a single node’s processor and disk. There are different join algorithm choices that will affect network utilization. For example, the choice of algorithm to evaluate a join between two tables that are distributed among multiple nodes has to choose between broadcasting one or more tables to other nodes compared to  joining the tables directly on the nodes. All of this has a direct impact on the networks. The Future of Query Optimization and AI We are likely to see continued use of AI techniques in query optimization for years to come. Today, some databases will explore or run parts of the query to see the results then make a dynamic choice on the plan. This puts more “smarts” in your query in that you don’t know the estimates so you can try running two different queries then pick one, or run pieces and decide on A or B. Research papers explore how to use machine learning in the database to optimize data layout. TensorFlow can be used to run experiments and adapt. If we adapt the cost model based on experiments, it is easy to envision real-time adjustments in the cost model over time. Statistics help with that over time by learning from the data. Although a lot of work goes into cost models and statistics, every database workload is different. With the right feedback loop, and with every query acting as another experiment, the database can improve itself over time. However, one company’s database will improve itself over time for THAT company’s queries. Long term there will continue to be millions, perhaps billions or trillions, of choices for query plans. This mandates an intelligent scheme to explore the entire space, meaning that machine learning and AI techniques will be a large part of query optimization well into the future. A special thanks to Robert Walzer, Nick Kline, Adam Prout, and Jack Chen from SingleStore engineering for input on this blog post. If you’re interested in joining our team, check out our open positions.", "date": "2018-03-21"},
{"website": "Single-Store", "title": "operationalizing-memsql", "author": ["Jeremy Althouse"], "link": "https://www.singlestore.com/blog/operationalizing-memsql/", "abstract": "In a recent webcast , we shared some tips and tricks on how you can operationalize SingleStore for configuring SingleStore Ops and SingleStore root user passwords; memory settings and health metrics; and how you can take backups and add nodes securely to your cluster. Here are the topics we covered: Permissioning a new cluster by adding a super-user to SingleStore Ops and via GRANT statements on the cluster itself Best practices for configuring memory limits Best practices for basic cluster monitoring, and how to retrieve the relevant metrics How to take physical backups of a cluster, and how backups are stored in the filesystem How to add new nodes to a cluster using the SingleStore Ops Command Line Interface (CLI) We also received questions from webcast attendees, and here are the answers: Question: In SingleStore DB 6.x, do all backup files need to be present on all nodes when attempting a restore? Any particular backup file needs to be accessible to the node that will restore the data on it. This is easiest to understand with an example. Consider a cluster with one Master Aggregator, 2 Leaves, and 4 partitions per leaf, and a database named “my _ database”. The data within my _ database will by default consist of 8 partitions, along with metadata. When my _ database is backed up, it goes to a single metadata backup file of the same name, along with 8 partition backup files named my _ database _ 0, my _ database _ 1, my _ database _ 2, etc. Restoring the cluster successfully does not require that each of the partition backup files be accessible on every node, but it does require that each of them be accessible to at least one leaf node and that the metadata backup file be accessible to the master aggregator. Question: How can a new Master Aggregator be deployed/assigned to a cluster? Currently, the only way to create a new Master Aggregator is through the promotion of an existing Child Aggregator, using the AGGREGATOR SET AS MASTER command. Question: Since SingleStore backups are distributed across the cluster in the same way as the data they back up, how can they be made robust against hardware failures? If SingleStore backups are stored to a location that represents a locally-mounted filesystem on each host, then there are two approaches to reduce the risk of losing backup data due to hardware failure: Instead of backing up to a local filesystem, the backups can be written directly to a network file system visible to all of the nodes in the cluster. This segregates the integrity of the backup files from the health of the hardware on which they were generated, at the cost of incurring some overhead during the backup process itself due to network latency. Backups can be written locally, then moved or copied via a scripted process to either a network location or object storage system. This approach avoids the potential performance impact of the previous option, but requires adding an additional layer of complexity to the deployment, and is asynchronous. Question: Does the node addition and rebalancing process change when operating a cluster in High Availability mode? The operational procedure for adding new nodes remains the same in HA mode, and from the user perspective the rebalancing operation also does not change. An even number of nodes must be used to preserve the HA topology; Ops will distribute the new nodes automatically among the two availability groups as they are added. Question: What are the architectural requirements for using the Ops CLI to add nodes, as was done in the webcast? When these requirements cannot be met, how can nodes be added? Using Ops to deploy new SingleStore nodes throughout the cluster requires it be able to SSH between nodes, but (as was done in the webcast) this is not necessary if the new SingleStore node is simply deployed from a colocated agent downloaded onto the new host. This process does still require that Ops be able to access the SingleStore download site to download the SingleStore engine binaries. If, for security or other concerns, the SingleStore engine binaries cannot be downloaded directly by Ops, the binaries can be transferred to the host manually, and Ops can complete the rest of the installation process using them. In this scenario, the binaries should be unzipped manually into the location specified in Ops’ settings.conf file . If Ops cannot be used at all, SingleStore nodes can still be installed and added manually via the ADD LEAF command. In this case, we recommend engaging with a [SingleStore representative] (mailto: sales@singlestore.com ) to ensure best practices are followed during the node addition. Question: How does the monitoring picture change if I’m mostly using columnstore? The fundamental monitoring picture does not change. Even in a purely columnstore use case, SingleStore still leverages the rowstore during ingest and allocates memory aggressively for query execution performance, so it is still important to monitor memory utilization as described in the webcast. Columnstore data is typically compressed enough on disk that a properly sized cluster is unlikely to be at risk of encountering an out-of-disk condition. However, it is still more likely to be the case than with the rowstore, so system-level disk space monitoring is more important. Question: What is a secure way to clear buffers in memory? A best practice for SingleStore deployments precludes running other memory-intensive processes on the same hosts, and buffered memory should not cause an issue on a properly-sized and configured cluster. If scenarios arise during operational use of SingleStore that appear to require flushing the memory buffer, we recommend reaching out to SingleStore support. Did you miss the webcast? You can get a copy of the recording to watch on demand here . To learn more or ask specific product questions, reach out to team@singlestore.com .", "date": "2018-03-28"},
{"website": "Single-Store", "title": "visual-takeaways-from-gartner-data-and-analytics-2018", "author": ["Gary Orenstein"], "link": "https://www.singlestore.com/blog/visual-takeaways-from-gartner-data-and-analytics-2018/", "abstract": "We attended the Gartner Data and Analytics Summit in Grapevine, Texas in early March. This series is part of its global events schedule and similar conferences happen around the world. One fun hallway display was a series of animated summaries with key themes, tracks, and sessions of the conference. They were created by Katherine Torrini at Creative Catalyst . The first display covered the primary show themes of scaling the value of data and analytics: – Establishing Trust in the data foundation – Promote a culture of Diversity – Building the Data Literacy of your workforce – Mastering Complexity of running a digital business #GartnerDA keynote: The value of data & #analytics is based on mastering #trust #diversity #literacy and #complexity . pic.twitter.com/dQlTiaHFiQ — Doug Laney (@Doug _ Laney) March 5, 2018 The second display focused on Unleashing Creativity in your team by noted Gartner analyst Frank Buytendijk The third display covered the keynote from Joe Inzerillo of BAMTECH, on how video analysis is disrupting the sports industry. His presentation was one of the most impressive at the conference. The future of sports? How about #AI analysis of players or boxers says Joe Inzerillo of BAMTECH #GartnerDA pic.twitter.com/WhoMYxBuQo — Gartner Events (@Gartner _ Events) March 6, 2018 As they say, pictures are worth a thousand words, and these animated boards are likely worth more. Explore and enjoy!", "date": "2018-04-11"},
{"website": "Single-Store", "title": "architecting-data-in-the-aws-ecosystem", "author": ["Gene Miguel"], "link": "https://www.singlestore.com/blog/architecting-data-in-the-aws-ecosystem/", "abstract": "Amazon Web Services (AWS) is a juggernaut of a platform, and many of today’s database solutions can run on AWS, including SingleStore. Recently, we held a meetup at our office in San Francisco during AWS Summit 2018 to discuss how customers can benefit from using SingleStore within the AWS ecosystem. In the talk, “Architecting Data in the AWS Ecosystem,” Seth Luersen from SingleStore took a look at the overall data landscape related to the purpose-built databases from Amazon Web Services. He illustrated how the shape, size, and computing needs for data-driven workloads dictate modern data architectures. The talk also shared how modern enterprises, that are architected for mixed workloads with SingleStore, can easily handle fast data ingest, manage high concurrency, and achieve sub-second query latency. Together, SingleStore and AWS provide a compelling platform for building real-time applications. SingleStore can handle both database and data warehouse workloads to combine transactional and analytical requirements, which fits with the direction of new applications. AWS provides useful services such as S3, as well as the most comprehensive Infrastructure-as-a-Service, to stand up powerful solutions. The slides and video have been made available below. Architecting Data in the AWS Ecosystem from SingleStore", "date": "2018-04-25"},
{"website": "Single-Store", "title": "creating-an-iot-kafka-pipeline-in-under-five-minutes", "author": ["Seth Luersen"], "link": "https://www.singlestore.com/blog/creating-an-iot-kafka-pipeline-in-under-five-minutes/", "abstract": "In a recent SingleStore webcast, we discussed how modern enterprises can easily adopt new data management tools to manage data size, growth, and complexity. Then we demonstrated how to use Apache Kafka and SingleStore to build interactive, real-time data pipelines. Data pipelines capture, process, and serve massive amounts of data to millions of users. During the webcast we also shared how to: Build new data pipelines with modern tools Enable data workflows to support machine learning and predictive analytics with data pipelines Deploy a real-time data pipeline using Apache Kafka and SingleStore Pipelines in under five minutes We received additional questions from the webcast attendees, and wanted to share the answers publicly. Question: Is SingleStore a NoSQL database or a Relational Database Management System (RDBMS)? SingleStore is a modern, in-memory optimized, massively parallel processing, shared-nothing, real-time database. SingleStore stores data in tables, and supports standard SQL data types. Geospatial and JSON data types are first-class citizens in SingleStore. With SingleStore, you can store and query structured, semi-structured, and unstructured data. Question: What are the minimum memory requirements for SingleStore? SingleStore is a distributed system consisting of one or more nodes. You can find out more about system and hardware requirements in our documentation . Question: Is loading JSON into SingleStore similar to MongoDB? Behind the scenes, MongoDB represents JSON documents in a binary-encoded format called BSON. BSON is also the wire format for MongoDB. JSON is a first-class citizen in SingleStore. The JSON data type validates JSON. Behind the scenes, SingleStore stores validated JSON as text. For those who want to preserve BSON in SingleStore, the supported SQL data type is VARBINARY or any variation thereof: LONGBLOB, MEDIUMBLOB, BLOB, or TINYBLOB. Question: What is the infrastructure requirement for running SingleStore and Apache Kafka? Like Apache Kafka, SingleStore is a distributed system consisting of one or more nodes that run as a cluster of nodes. At a minimum, the infrastructure requirement is for a stand-alone Apache Kafka producer and broker, and a stand-alone SingleStore cluster consisting of one master aggregator and one leaf node. In a production environment, there are two clusters: an Apache Kafka cluster and SingleStore cluster. A SingleStore Pipeline ingests streaming data from an Apache Kafka topic into SingleStore leaf nodes by default. SingleStore leaf nodes contain individual database partitions. Each database partition stores the data from the Kafka stream into its own destination table. Parallelism between the number of SingleStore database partitions and Kafka broker partitions for the given topic determines optimal performance. Question: In order to digest data from Apache Kafka, are there any consumer concepts in SingleStore? Apache Kafka follows a more traditional design, shared by most messaging systems, where data is pushed to the broker from the producer and pulled from the broker by the consumer. In a pull-based system, the consumer can catch up when the system falls behind. A SingleStore Pipeline for Apache Kafka uses a pipeline extractor for Kafka. This extractor is a Kafka consumer. Question: Does a SingleStore Pipeline using the Apache Kafka extractor only support real-time ingestion into a SingleStore “rowstore” table? With a SingleStore Pipeline, you can massively ingest data in parallel into distributed tables. In SingleStore, a table is either distributed or non-distributed (reference table). There are two storage types for tables: in-memory rowstore and columnstore. All columnstore tables have an unexposed, in-memory rowstore table. SingleStore automatically spills rows from the in-memory rowstore to columnstore. All data, including the hidden rowstore table, is queryable for the columnstore table. Question: Is it possible to move data from an in-memory rowstore table to a columnstore table? Yes. It is as simple as: INSERT INTO columnstore_table SELECT * FROM rowstore_table; Question: In this webinar demonstration, where is this data being ingested from and is it pre-generated data? The webinar demo showcases a SingleStore Pipeline for Apache Kafka. SingleStore hosts the Apache Kafka cluster. A Python program generates data and writes it to a Kafka Producer for the adtech topic. The SingleStore Pipeline consumes this topic from the Kafka broker endpoint. Question: What happens if you need to adjust or change your data schema? You can modify a table in SingleStore with a data definition language (DDL) ALTER TABLE… statement. By default, SingleStore supports an online ALTER statement. Since SingleStore Pipelines are written with DDL, you can also alter a SingleStore Pipeline with an ALTER PIPELINE… statement. Typically, the procedure to handle schema changes is simply: STOP PIPELINE mypipeline;\nALTER TABLE mytable… ;\nALTER PIPELINE mypipeline…;\nTEST PIPELINE mypipeline;\nSTART PIPELINE mypipeline; Question: Will you please provide a code example of how a SingleStore Pipeline for Apache Kafka transforms a JSON message? Here is a simple example of JSON in a Kafka message: {“id”:1,”item”:”cherry”,”quantity”:1} Written in Python, here is a basic transform script that extracts the id field from JSON: #!/usr/bin/env python\nimport struct\nimport sys\nimport json\ndef transform_records():\n    while True:\n        byte_len = sys.stdin.read(8)\n        if len(byte_len) == 8:\n            byte_len = struct.unpack(\"L\", byte_len)[0]\n            result = sys.stdin.read(byte_len)\n            yield result\n        else:\n            return\nfor l in transform_records():\n    parsed_json = json.loads(l)\n    sys.stdout.write(\"%s\\t%s\\n\" % (parsed_json[\"id\"], l)) Question: How is it possible to persist complex master detail records with foreign keys with a SingleStore Pipeline? SingleStore DB 6 does not enforce foreign key constraints, does not support triggers, and a SingleStore Pipeline only supports loading data into a single table. Recently, a SingleStore engineer demoed a beta release of SingleStore DB 6.5 in which a SingleStore Pipeline can load data into a stored procedure! This architecture allows for conditional logic within the stored procedure to manage complex scenarios such as data ingest into related tables. To learn more about streaming data into stored procedures with SingleStore Pipelines, see Recapping An Evening with SingleStore Engineering . Question: Usually the data format for data in Apache Kafka is in a binary form such as Apache Avro, so how does SingleStore support user-defined decoding? SingleStore Pipelines support data ingest that is in either a CSV or TSV data format. One way to ingest compressed Avro data from a Kafka topic is to create a data pipeline with Apache Spark. Spark Streaming allows Spark to consume a Kafka topic directly. Using the SingleStore Spark Connector , it is possible to decode the binary formats and save data directly into SingleStore. You can learn more about working with Spark in our SingleStore Spark Connector Guide . Another approach is to use an Avro to JSON converter. Once converted, the Kafka message is essentially a JSON blob. In a SingleStore Pipeline, you can easily transform JSON using any Linux supported API that efficiently parses JSON. SingleStore Pipelines also supports jq, a lightweight and flexible command-line JSON processor. To learn more, read JSON Streaming and the Future of Data Ingest . JSON is a first-class citizen in SingleStore. With built-in JSON functions, it is possible to parse JSON key-values into persisted, computed columns. SingleStore supports indexing of computed columns. With SingleStore, you can easily index and parse JSON with standard SQL. Question: Does SingleStore handle back-pressure automatically? A large Apache Kafka cluster in production is capable of delivering messages at a very fast rate, in order of millions of messages per second. For use cases where there is high volume and a high rate, many API-driven consumers reach a point where they cannot keep pace and fall behind, a behavior known as back-pressure. Because SingleStore is a modern, in-memory optimized, massively parallel processing, shared-nothing, real-time database, a SingleStore Pipeline for Apache Kafka can easily consume and ingest messages at very high volume and high rates. A SingleStore Pipeline ingests streaming data from an Apache Kafka topic into SingleStore leaf nodes by default. SingleStore leaf nodes contain individual database partitions. Each database partition consumes the Kafka stream into the designated destination table. By default, tables in SingleStore are in-memory, rowstore tables. Parallelism between the number of SingleStore database partitions and Kafka broker partitions for the given topic determines optimal performance as this parallelism dictates the total batch size. SingleStore keeps track of the Kafka earliest and latest offsets in the  information_schema.PIPELINES_BATCHES table. Question: What are the advantages to using Apache Kafka versus Amazon S3? Apache Kafka is a modern, distributed messaging system. Amazon S3 is cloud object storage built to store and retrieve files. SingleStore Pipelines has extractors for both Apache Kafka and Amazon S3. For both extractors, the number of database partitions in SingleStore determines the degree of parallelism for data ingest. For S3, the number of database partitions in SingleStore equates to the number of files in a pipeline batch. Each database partition ingests a specific S3 file from a folder in an S3 bucket. The files can be compressed. Amazon S3 has known limits for GET request speeds that start at over 100 requests per second. In addition, the pricing model for S3 is based on the volume of data egress. To learn more about SingleStore S3 Pipelines at scale, check out 1.3 Billion NYC Taxi Rows . An Apache Kafka cluster is able to support reads and writes in the millions per second. Of course, this is very dependent on the package size. As package sizes increase, messaging throughput decreases. That said, as a distributed system, you can scale out Apache Kafka to meet your requirements. Question: How can I get the O’Reilly eBook trilogy from SingleStore? You can download our O’Reilly eBooks covering topics that range from Predictive Analytics to Artificial Intelligence at SingleStore O’Reilly Trilogy . Question: What do I need to get started with Apache Kafka and SingleStore? You can deploy a SingleStore cluster on-premises, with Docker, with Amazon Web Services, or with Microsoft Azure. To learn more, check out our Quick Start Guides . To learn how to quickly setup a SingleStore Pipeline for Apache Kafka, review our Quick Start for Kafka Pipelines . You can watch the recording of the Pipelines in Five Minutes webcast here . If you prefer a quick demo of SingleStore, SingleStore Pipelines, and Apache Kafka, sign up at SingleStore Demo .", "date": "2018-04-04"},
{"website": "Single-Store", "title": "matching-modern-databases-with-ml-and-ai", "author": ["Gary Orenstein"], "link": "https://www.singlestore.com/blog/matching-modern-databases-with-ml-and-ai/", "abstract": "Introduction Machine Learning (ML) and Artificial Intelligence (AI) have stirred the technology sector into a flurry of activity over the past couple of years. However, it is important to remember that it all comes back to data. As Hilary Mason, a prominent data scientist, noted in Harvard Business Review * , …you can’t do AI without machine learning . You also can’t do machine learning without analytics , and you can’t do analytics without data infrastructure . Over the last year we assembled a number of blogs, videos, and presentations on using ML and AI with SingleStore. SingleStore with ML and AI As a foundational datastore, SingleStore incorporates machine learning functions in one of three ways: Calculations outside the database Calculations on ingest Calculations within the database Read an overall summary in our blog post Machine Learning and SingleStore from Rick Negrin. Outside the Database For integrating ML and AI outside the database, two popular methods are integrating with Spark and TensorFlow. For Spark, SingleStore offers and open source SingleStore Spark Connector, which delivers high-throughput, bi-directional, and highly-parallel operations from partition to partition. This connector opens up unlimited ML and AI possibilities that can be combined with a scalable, durable datastore from SingleStore. One example of this integration is real-time machine learning scoring. A stereotypical pipeline might be: IIoT Collection &gt; Kafka &gt; Spark &gt; SingleStore &gt; Queries For example, some popular statistical software packages allow export as PMML, the predictive machine learning markup language. These or similar models, can be exported into Spark, or even the database itself, to score incoming data in real time. From there, the datapoint and the score of the model on the datapoint, can be persisted together for easy analysis. SingleStore PowerStream is a specific example of combining machine learning with a database to score data in real time and predict the likelihood of an equipment failure. An interactive demonstration using SingleStore PowerStream simulates 200,000 wind turbines sending sensor information at a rate of approximately 2 million inserts per second to SingleStore. From there, the user interface shows live status on real-time information, and ML scoring predicts the likelihood of turbine failures. Read more in our blog IoT at Global Scale: PowerStream Wind Farm Analytics with Spark . TensorFlow, and the results of applying machine learning models in TensorFlow, are shown in the this video Scoring Machine Learning Models at Scale from Strata New York . TensorFlow is often used in real-time image recognition. This presentation and video from Spark Summit Dublin 2017 highlights the popular use case. And here’s a relevant course on TensorFlow from a provider who focuses on data science, AI, and machine learning: Artificial Intelligence Course and Training . ML On Ingest Another area to run machine learning is as the data arrives in the database. SingleStore enables this with native pipelines from Kafka, including exactly-once semantics, a critical capability for deduplication in event-driven pipelines. Further, the pipeline ingest capability includes the option for executing a custom transformation. A typical real-time data pipeline in this scenario might be: Kafka &gt; SingleStore &gt; Query/Visualization SingleStore UI for Custom Transformations Within the Database The third area for ML integration with databases is within the database itself. This occurs with a couple of secret ingredients. The first enabler for machine learning within a database is extensibility, or more specifically the inclusion of stored procedures, user-defined functions, and user-defined aggregates. These functions allow for customizations that can quickly and easily execute elements of an overall machine learning pipeline. Another enabler is the inclusion of popular algebraic operations such as DOT_PRODUCT , which can take two vectors and quickly compare them for similarity. DOT_PRODUCT can also easily compare one vector against hundreds of millions of vectors to deliver a degree of similarity. Thorn, an organization dedicated to ending child sex trafficking and the sexual exploitation of children, relies on image recognition to help fulfill its mission. Through the use of DOT_PRODUCT within SingleStore, Thorn was able to dramatically improve its real-time image recognition capabilities. Read more on An Engineering View on Real-Time Machine Learning , or watch the video from AWS re:Invent 2017, Business and Life-Altering Solutions Through AI and Image Recognition . Beyond image recognition, DOT_PRODUCT has applicability in a wide range of use cases. For example, we demoed K-means clustering on YouTube tags within this presentation from Gartner Catalyst 2017, The Data Warehouse Blueprint for ML, AI, and Hybrid Cloud . Skip to slide 23 for the demonstration section. Another example comes in this presentation from Gartner Data and Analytics 2018 in Texas, Building a Machine Learning Recommendation Engine in SQL . Skip ahead to slide 48 for the demonstration. Additional Resources If you’d like to read more, feel free to check out, SingleStore DB 6 Product Pillars and Machine Learning Approach . This post includes more on: – Built-in machine learning functions – Real-time machine learning scoring – Machine learning in SQL with extensibility And if you would like to see where 1600 IT professionals think the industry is headed with Machine Learning and Artificial Intelligence, see the 2018 Outlook: Machine Learning and Artificial Intelligence Survey . Developed in conjunction with O’Reilly Media, this survey helps shape the general sentiment and industry trends around ML and AI. Of course, for more information, or to see some of the above demonstrations in action, feel free to book a demo with SingleStore anytime! * Harvard Business Review: https://hbr.org/2017/07/how-ai-fits-into-your-data-science-team", "date": "2018-05-02"},
{"website": "Single-Store", "title": "the-database-guide-for-sparkai-summit-2018", "author": ["Mike Boyarski"], "link": "https://www.singlestore.com/blog/the-database-guide-for-sparkai-summit-2018/", "abstract": "Spark+AI Conference is in San Francisco June 4-6. It’s the preeminent conference covering all things Spark. Since 2013 the conference has helped developers learn and explore the possibilities of delivering modern data-driven applications at scale. The overarching theme of this years show is the unification of data and AI. New applications that include AI require massive amounts of constantly changing data, and Apache Spark is at the center of this data processing opportunity. Sessions of particular interest include Spark Streaming case studies along with operationalizing Spark for production: A Deep Dive into Stateful Stream Processing in Structured Streaming Tathagata Das will discuss different stateful operations in Structured Streaming, how state data is stored in a distributed, fault-tolerant manner using State Stores.  Tues, June 5 @ 11am Automating and Productionizing Machine Learning Pipelines for Real-Time Scoring David Crespi and Jared Piedt will be talking about taking your machine learning models to production.  Tues, June 5 @ 11:40am Zipline: Airbnb’s Machine Learning Data Management Platform Nikhil Simha and Varant Zanoyan will talk about Zipline, Airbnb’s data management platform for ML. The purpose of the platform is to reduce the time collecting and writing transformations for ML tasks, claiming a month to days improvement. Tues, June 5 @ 12:20pm Near Real-Time Netflix Recommendations Using Apache Spark Streaming Nitin Sharma and Elliot Chow will talk about how Netflix leverages real-time data for model training to deliver the right personalized video for users.  Tues, June 5 @ 4:20pm Optimizing Apache Spark Throughput Using Intel Optane and Intel Memory Drive Technology Ravikanth Durgavajhala will talk about the new Intel Optane SSD with Intel Memory Drive Technology to address memory limitations for large scale processing deployments. Wed, June 6 @ 11am The Real-Time Database for Spark SingleStore has been working with Apache Spark since 2015 by delivering scalable ingestion, storage, and analysis of data for real-time insights. Our 79-page Spark Connector Guide covers how to design, build, and deploy Spark applications using the SingleStore Spark Connector. You’ll find code samples to help you get started and performance recommendations for your production-ready Apache Spark and SingleStore implementations. Download the SingleStore Spark Connector Guide .", "date": "2018-06-01"},
{"website": "Single-Store", "title": "diversifying-your-enterprise-tech-stack", "author": ["Kristi Lewandowski"], "link": "https://www.singlestore.com/blog/diversifying-your-enterprise-tech-stack/", "abstract": "Companies are living organisms. As they mature they tend to slow down a bit and align to familiar routines. This pattern has the benefit of keeping the primary corporate economic engine stable and avoiding unintentional disruption. However, that same ‘disruption avoidance’ can cause companies to become so sedentary that they miss the next innovation wave. Conceptually, we understand the desire and need for large companies to stay innovative, and they try many different approaches to avoid this trap, including developing innovation conferences, corporate venture investments, internal skunkworks projects, and adopting technologies outside of their dominant suppliers. In particular, while it can take time and energy to bring a smaller vendor on board to assist a global multinational company, the payoff can be enormous. Let’s explore four reasons why large companies are bringing smaller technology vendors into their stack. Stay ahead of new technologies In the technology ecosystem, innovation is largely fueled by smaller companies reimagining long held enterprise beliefs. The success of these small companies has been evident during 2018 with the flurry of enterprise technology initial public offerings, including DocuSign, Smartsheet, Zuora, Pivotal, Dropbox, Carbon Black, SendGrid, and Zscaler. For large enterprises looking to innovate, choosing a smaller vendor can help inject new technology and approaches into a large organization quickly to stay competitive. Most of the time, these changes are evolutionary. However, in times of rapid change such as now with digital transformation, the changes can be far more necessary and extensive. Recruiting talent Attracting top talent is one of the biggest obstacles large companies face today. In a recent global study , Korn Ferry found that a global talent shortage could lead to a deficit of more than 85 million workers by 2030, and a lost revenue opportunity of nearly $8.5 trillion dollars. When trying to attract new technical talent to a large company, one tactic is to use the lure of working with the latest and greatest tools. This cannot happen when enterprises rely only on the most established vendors, many of which are protecting revenues on older products. By the very nature of the technology industry, newer tools from less established vendors tend to be more innovative and have a higher appeal for new graduates. So diversifying the technology stack with the latest advances not only helps deliver an innovation boost but further attracts the type of talent corporations seek to keep their businesses running. Balance with megavendors Large companies tend to spend the bulk of their technology budget with other larger vendors. This creates a consolidation of spending, and at times can also lead to a partial or complete “lock-in” to those larger vendors. For example, once you choose your applications and database from the same vendor, your leverage to negotiate on the whole solution degrades. If you choose best-of-breed offerings, there is a chance you can switch out one solution for another, and the buyer leverage increases compared to the supplier leverage. Enterprise storage is another market where this can have an effect with companies combining larger conventional suppliers such as Dell, HP, and IBM with newer vendors such as Pure Storage. With smaller technology partners, large companies can balance their spending with megavendors. Technology users in I.T. are not going to remove the solutions from larger suppliers on a wim, but might add smaller suppliers to the list to foster a competitive marketplace, ensure that they receive products and solutions at market rates, and continually refresh their stack with the latest capabilities. Drive company-specific needs With established suppliers the chance of having company-specific requests implemented into the product declines. You have to be a very large customer in order to see your own company-specific needs appear in a product. However, with smaller suppliers the stakes are completely different. Large companies can more readily influence the product roadmap of small suppliers to meet their specific needs. This provides options for more efficient and more competitive technology solutions. Ultimately, this helps large companies move more quickly to solve problems and drive new revenue. Steps to diversifying your tech stack Once you recognize the need and rewards of diversifying your technology stack, here are a few tips to help you get started. Foster a culture inside your company that values stretching the vendor portfolio beyond the established megavendors. Point to innovative examples within your company of using newer solutions from smaller suppliers. Aim to include one to two smaller vendors in the round up for new projects. Even the established analyst firms such as Gartner try hard to showcase emerging vendors in the context of established vendors to highlight significant innovation and provide a choice. Make time to invite smaller vendors to share their technology and roadmap, even in the context of general education. You may be surprised how much new thinking can influence your own company through something such as a brown bag lunch education session on a new technology or means for implementation. Whatever the method, large companies have many ways they can benefit from working with smaller companies. There are a few links in the reference section below showcasing these efforts. Reference links of large company innovation efforts Deutsche Bank video on their Innovation labs Bank of America Merrill Lynch Technology Innovation Summit Morgan Stanley video on their Annual CTO Innovation Summit Visa’s Everywhere Initiative Citigroup Corporate Ventures Johnson and Johnson Innovation Labs Challenge Shell Technology Ventures", "date": "2018-06-08"},
{"website": "Single-Store", "title": "handytec-provides-fast-geoanalytics-with-memsql", "author": ["Jorge Jaramillo"], "link": "https://www.singlestore.com/blog/handytec-provides-fast-geoanalytics-with-memsql/", "abstract": "By Jorge Jaramillo, Managing Partner – handytec Data is constantly being collected and analyzed. In business, we typically analyze that data with charts, graphs, or lists to create actionable insights. However, there is one element missing with traditional data analysis, and that is the “where” element. This is when geoanalytics becomes relevant. Geoanalytics is the correlation between data and its physical location. This means that more insights on the data can be created, such as dynamic pricing and operational efficiency. How handytec Delivers Geoanalytics handytec is an Ecuadorian company specializing in providing customized big data analytics solutions. It works with cutting-edge technologies to help companies adopt and take advantage of the transformational power of data science to achieve their most ambitious, strategic, and challenging objectives. Over time, handytec has explored how a geoanalytics offering with a dimension of analysis would allow companies to be strategic and localized to understand spatial variables that influence the behavior of entire industries. With all this in mind, handytec created the helio.geo platform in 2018. helio.geo is a cloud-based geoanalytics platform, optimized to work extremely fast and enriched with dozens of datasets nationwide. The back-end works with SingleStore, which allows the team to securely store large datasets with geospatial information and query them efficiently with the highest transactional power possible. SingleStore was chosen due to its versatility since helio.geo needs to add new modules based on semi-structured data (JSON) processing and real-time analytics. Use Cases for handytec To maximize the benefits of geoanalytics, different predictive and personalized models have been created for three main sectors where it is currently in use. Retail helio.geo analyzes different data and individual variables to predict the demand and market share of a particular area. It also gives permanent visibility of the behaviour of the area for different products or customer profiles. The points marked as “F\"s are retail stores and the different dots are two different client segments. This is a classic clustering analysis (customer segmentation). Insurance helio.geo characterizes different variables of potential risk for each insured property, allowing companies to take preventive actions or even modify the premium of each insurance claim in real time. Dots are different types of insured properties (mainly buildings, and physical assets). The shapes (almost a heat map) are the result of  the handytec catastrophic risk model. Banking helio.geo uses its capabilities to estimate risk on expansion strategies and microcredit placement. The main objective is to create credit risk models, complementary to the traditional ones, to financially support the base of the lending pyramid. The orange circle is the proposed coverage area for a new bank agency. Blue dots are current customers, and gray dots are potential customers. The heat map at the back is the handytec default risk model. Getting Started with Geoanalytics Companies looking to add a location element into their data tracking and get the analysis of the data back quickly, should consider handytec. The user experience has been significantly improved compared to other platforms that limit access to information to ensure good performance. With SingleStore, helio.geo can process hundreds of datasets 10 times faster than our old traditional databases, allowing us to do more with data analytics and visualizations.These benefits would not be possible without SingleStore, as its support for data-intensive applications allows us to deliver the scale and speed our customers need. To learn more, visit www.handytec.mobi and get started with our geoanalytics application.", "date": "2018-06-22"},
{"website": "Single-Store", "title": "how-to-process-trillion-rows-per-second-ad-hoc-analytic-queries", "author": ["Eric Boutin"], "link": "https://www.singlestore.com/blog/how-to-process-trillion-rows-per-second-ad-hoc-analytic-queries/", "abstract": "On March 13, we published a blog demonstrating the performance of SingleStore in the context of ad hoc analytical queries. Specifically, we showed that the query SELECT stock_symbol, count(*) as c\nFROM trade\nGROUP BY stock_symbol\nORDER BY c desc\nLIMIT 10; can process 1,280,625,752,550 rows per seconds on a SingleStore cluster containing 448 Intel Skylake cores clocked at 2.5GHz. In this blog post, we drill down into how this was made possible by carefully designing code, exploiting distributed execution, and instruction-level and data-level parallelism. Why is such high throughput needed? Users of applications expect a response time of less than a quarter of a second. Higher throughput means more data can be processed within that time frame. Parallelism Through Distributed Execution For a single threaded execution of the query to process 1,280,625,752,550 rows in one second, it would require processing a row in less than 0.002 clock cycles (at a 2.5GHz clock). This isn’t possible on today’s hardware, so SingleStore scales beyond a single core, beyond a single server, and executes the query on an entire cluster. Distributed execution is one of the key strengths of SingleStore. Improving performance is easy. Step 1: Add hardware, step 2: Query gets faster. It is a well understood tradeoff. With 448 cores, a row needs to be processed in 0.87 clock cycles on average. However, 0.87 clock cycles per row is a very small number. Processing a row requires quite a bit more than 0.87 instructions. We could add 50x more Skylake cores, but that starts getting expensive. Is there more parallelism we could extract to further speed up the query? Yes. This can be done by carefully crafting code. Lucky for you, we already did that work in SingleStore. Let’s get into the technical aspects of the work. Instruction-Level Parallelism How do we extract more parallelism out of the system in order to process a row every 0.87 clock cycles on average? The first form of parallelism that we look at is instruction-level parallelism, or pipelining. Modern microprocessors have multiple execution units, and are able to execute multiple instructions in parallel. Instead of executing a single instruction at a time, modern microprocessors have multiple execution units, which can each execute instructions in parallel. The key to leverage this parallelism is to write predictable code , i.e. code with branches that the microprocessor can easily predict, and to avoid write-write contention. [ Execution units in the Haswell processor. Modern microprocessors contain multiple execution units allowing them to execute multiple instructions in parallel. In the Haswell architecture, a CPU core has 4 integer arithmetic units, so a single core can process 4 arithmetic operations in parallel. This is in addition to multi-core parallelism. Source: https://software.intel.com/en-us/forums/intel-isa-extensions/topic/737959\\] Let’s consider the query SELECT key, COUNT( * ) FROM t GROUP BY key. A naive implementation of the query would go as follows. For each key, increment a counter for that key. To make it simple, we consider that the column ‘key’ is a set of small, non-sparse, integers, contained in an array. Every key is included in the range (0, MAX _ KEY). int counters[MAX_KEY];\nmemset(counters, 0, sizeof(counters));\nfor (int i = 0; i < NUM_ROWS; ++i)\n++counters[key[i]]; On a MacBook, this single threaded algorithms can process 1,343,784,994 keys per second for very simple data. Let’s do better. There is a branch in the for-loop to identify the termination condition of the algorithm. This branch can be predicted very well by the processor, as most of the time the outcome of the condition i < NUM _ ROWS is false. This allows the processor to speculatively execute another iteration successfully. The actual problem here is that with a small number of distinct keys, there are write-write conflicts when updating the counters array. For example, if the key array contains [ 0,1, 4, 4, 4, 2, 4 ] , then the processor pipeline will stall. The microprocessor tries to process the second ‘4’ before the first ‘4’ has been completed (because of the pipelining). The processor pipeline will stall because two instructions try to update the same memory location, which limits the instruction-level parallelism. How do we fix this? We can use two separate set of counters, this way the second ‘4’ will update a different memory location than the first ‘4’. The third ‘4’ will update the same memory location as the first ‘4’, and that can be further improved by just adding more counter arrays. int counters1[MAX_KEY];\nint counters2[MAX_KEY];\nmemset(counters1, 0, sizeof(counters1));\nmemset(counters2, 0, sizeof(counters2));\n\nfor (int i = 0; i < NUM_ROWS; i += 2)\n{\n++counters1[key[i]];\n++counters2[key[i+1]];\n} Of course the example would need to be modified to handle the tail of the array if the number of rows is not even and the counters would have to be merged, but we keep the example simple for ease of understanding. On a MacBook Pro, this example processes rows 2.67 times faster than the naive example. How does this work in practice? The query select stock _ symbol, count( * ) as c from trades group by stock _ symbol, doesn’t group by a small integer, it groups by stock _ symbol, a VARCHAR column. We organize the data such that trades with the same stock _ symbols are stored close to each other. The trades are stored in a columnar format, which is organized as a set of immutable segments. A segment is simply a large collection of records, and is the unit of compression. Due to the nature of the data, a segment only contains trades for a handful of different stocks. When creating the segment, we identify the set of distinct stock _ symbol in a segment, and store them in a dictionary . The dictionary provides a way to represent the stock symbol of a trade as a small index in the dictionary, rather than a string. It is a common compression technique in databases. So instead of: AAPL AMZN AAPL AMZN AMZN …etc… We produce a dictionary with the entries: AAPL 0 AMZN 1 … … This allows us to represent the stock symbols column as: 0 1 0 1 1 …etc… Now, when grouping by the stock symbol, we can actually group by a small integer. But Wait, There’s More! Data-Level Parallelism, or Single Instruction, Multiple Data (SIMD) is the other form of parallelism that we leverage for this query. It allows us to use a few instructions to process multiple rows at once. SIMD offers instructions executing on very wide registers (256 bits in AVX2, 512 bits in AVX-512). The registers are separated into lanes , each of which contains a separate value. Given that stock _ symbols are represented using small dictionary indices, we can fit the stock symbols of 32 trades in a single register, allowing us to process 32 records from the trade table at once. There are challenges in implementing the query using SIMD. If we continue to derive the naive implementation that was described in the previous section, we notice that there are data-dependent writes (e.g. ++counters1 [ key [ i ] ] ). There is no way to naively translate the above code to update the count of all 32 stocks in a given register at the same time. Further, many of the lanes of a register could need to update the same memory location. This amplifies the write-write conflict problem, that was described in the previous section, by an order of magnitude. The algorithm that SingleStore uses to execute a vectorized group by, called in-register aggregation , is detailed in the peer-reviewed paper BIPie: Fast Selection and Aggregation on Encoded Data using Operator Specialization , which is being published at the SIGMOD conference. We now give a glimpse of how in-register aggregation works. In the naive code of the previous section, we used a counter array, then two counter arrays to track the count of each keys. In-register aggregation pushes this further, and uses 32 counters for each stock _ symbol. In fact, we use an entire SIMD register for each stock _ symbol that we expect to see in a segment. We call those registers the counter registers. The counter registers are split into lanes, and the sum of all the lanes of a register gives us the number of times the stock appeared. Below is an example of registers using 4 lanes, although AVX2 allows us to use up to 32 lanes. Sample Dictionary: stock _ symbol Dictionary id A 0 AAPL 1 AMZN 2 Counter Registers: (There is a counter register for each distinct stock symbol.) A: 2 0 0 0 \\= 2 trades total AAPL: 0 2 1 0 \\= 3 trades total AMZN: 10 2 1 54 \\= 67 trades total As we process the trades, we read an entire register worth of stock symbols (well, the dictionary ids of stock symbols). We compare the content of the register with each dictionary id, and update the count register accordingly. Each iteration of the algorithm processes 32 trades at a time. In the following example, we read a batch of stock _ symbols in registers. 1 (AAPL) 1 (AAPL) 1 (AAPL) 2 (AMZN) For each stock symbol s , we execute a SIMD comparison of the register allowing us to identify which of the lanes contain a trade for s . If we compare the above register with ‘AAPL’, we obtain the following SIMD register: 1 1 1 0 The register contains ‘1’ when the register contains a trade for AAPL, and 0 when it doesn’t. Then, we add the resulting register to the AAPL register, which becomes: AAPL 1 3 2 0 \\= 6 trades total Each register is compared against each stock _ symbols. This technique is optimized for a small number of distinct stock _ symbols in a segment. For a large number of groups and small registers, this technique wouldn’t work very well. However, with the large AVX2 and AVX-512 registers, this works quite well. Depending on the number of groups, the speedup of implementing this algorithm is quite spectacular. For a small number of groups, we can see beyond 10x of speedup when the data is loaded from L3 cache. The bottleneck at this point shifts to the memory bandwidth of main memory. To further optimize performance, we use techniques such as bitpacking, which improves the number of trades we can read per seconds from main memory. This increases the processing cost but allows us to skip past the memory bandwidth bottleneck. Let’s save this topic for another day. What about the order by and limit also in the query? Small potatoes. In this query, the bulk of the work is to execute the select stock _ symbol, COUNT( * ) from trades GROUP BY stock _ symbol. Key Takeaways Modern computer systems have a lot of parallelism to exploit. Distributed query execution techniques can dramatically increase the computing power available to respond to a query. In addition, with careful design of algorithms, it is possible to extract an order of magnitude more parallelism by leveraging pipelining and SIMD. It is amazing how much work can be done in a clock cycle.", "date": "2018-05-10"},
{"website": "Single-Store", "title": "helping-enterprises-derive-value-from-data-with-our-series-d", "author": ["Nikita Shamgunov"], "link": "https://www.singlestore.com/blog/helping-enterprises-derive-value-from-data-with-our-series-d/", "abstract": "We are thrilled to share that we raised $30 million in our Series D. This round of fundraising was led by GV (formerly Google Ventures) and Glynn Capital, and brings our total fundraising to $110 million. Why are firms continuing to invest in SingleStore? Database technology sets the foundation for business leaders looking to derive value from data. Across the industries and companies we support, SingleStore is driving digital transformation for some of the world’s business giants including companies such as, Comcast, Pandora, Kellogg, Dell/EMC, and Uber. What makes SingleStore different than other databases on the market? It’s the architecture. Unlike legacy relational databases, SingleStore is architected for the cloud, to scale, to be operational, and to deliver high speed analytics. As data continues being the center of enterprises, SingleStore powers the data-intensive applications that businesses need to thrive. How will the new funds be used at SingleStore? This money will allow us to expand the engineering, support, customer management, and operations teams. It will let us continue delivering record-breaking triple digit company growth . What’s next for SingleStore? We are working hard to build the next great database. The future of relational databases is about consuming data as a utility and delivering both transactions and analytics on the same data. This allows customers to derive more value from their data, focus on business growth, and reduce infrastructure costs. To learn more about this round of fundraising, read our news release .", "date": "2018-05-15"},
{"website": "Single-Store", "title": "6-5-performance", "author": ["Eric Hanson"], "link": "https://www.singlestore.com/blog/6-5-performance/", "abstract": "With SingleStore DB 6.5, the fastest database just got a lot faster. SingleStore is a performance-oriented product that capitalizes on several techniques to give excellent overall speed. These techniques include optimized disk-based and in-memory data structures, parallelism, scale out, compilation of queries to machine code, vectorization, and single instruction, multiple data (SIMD). In SingleStore DB 6.5, we advanced performance in many dimensions, including query execution (QE), query optimization (QO), data loading, and system management. SingleStore DB 6.0 was already fast. Very fast. But we were not satisfied. QE performance is always a high priority for us since it’s what users see most when exploring data. We improved QE performance for the following operations: Shuffle (data redistribution) High-cardinality GROUP BY IN-list filtering Filtering of data stored with integer run-length encoding (RLE) Internal memory allocation For many users, the improved shuffle and high-cardinality GROUP BY performance will be the most noticeable advancements. In an internal test workload that we run based on the TPC-H benchmark, the average query speed improved by 2.2X , over a set of 22 queries. The IN-list filter, integer RLE filtering, and memory allocation improvements can all give 10X or more query speedups in the right situation. The query optimizer and statistics systems have improved significantly. The blazing-fast SIMD QE we used to demonstrate a trillion-row-per-second query with our 6.0 release had gotten so fast that our query optimizer didn’t always know when to use it. So we re-calibrated the optimizer to use it when it’s best suited. Also, we’ve improved our statistics (histograms) to have much higher resolution, which will lead to better query plans. Finally, loading and system management have improved. Loading from Amazon S3 cloud storage using our Pipelines feature can start in seconds in SingleStore DB 6.5 versus minutes in SingleStore DB 6.0 for large S3 buckets. Load speed for wide comma-separated value list files with quoted strings is over twice as fast . Columnstore loading and all other operations that require large sorts are far faster. And finally, restart recover for clusters with hundreds of databases is more than ten times as fast . Query Performance Here, we’ll examine all the query performance improvements in SingleStore DB 6.5 in more depth. Fast Shuffle SingleStore is a distributed, shared-nothing database. Applications connect to aggregator nodes. Aggregators compile queries, start execution, and assemble results. The data is stored across multiple leaf nodes using hash partitioning (sharding). The execution of queries requires data to be moved between leaf nodes, as well as from leaves to aggregators. Data movement takes time. And we don’t like it when queries take too much time. Which lead us to create Project Sanic . When Ya Gotta Go Fast! (Google images of “ Sanic ” and you’ll see our project mascot). Sanic is all about speeding up data movement in SingleStore. Before SingleStore DB 6.5, shuffle heavily relied on converting data to text form, then back to an internal binary form for processing at the receiving node. 6.5 shuffles data primarily in binary format. For certain types of data, especially numbers and dates, this saves a lot of CPU time on both the sending and receiving ends, because we don’t have to take time to convert to string form and back again. Consider a simple example. Suppose we create this table: create table t(a int, b int, shard(a), key(a)); Then we insert about 164 million rows into it, with unique integer values for every row for both columns a and b . This table is sharded (hash partitioned) by column a across leaf nodes. Each leaf node has an index on column a . Now, suppose we want to run this query: select count(distinct b) from t; A highly parallelizable version of this query repartitions data by column b using hashing, then performs a local distincting operation on each leaf. Finally, these results are concatenated together at the aggregator, and the result is returned to the client. The query above runs on a 4-leaf system with 16 cores and 16 partitions total in the following times: SingleStore DB 6.0 SingleStore DB 6.5 Speedup (times) 26.57 sec 8.74 sec 3.04 This healthy 3.04 times speedup is due completely to shuffle performance improvements. High-Cardinality GROUP BY High-cardinality GROUP BY queries are ones that have many distinct grouping keys (say 10s of thousands or more). In 6.0, these queries computed a local grouping for each partition, and each of these was forwarded to the aggregator, which did the final grouping. If there were many partitions per node, this could have involved a lot of data transmission causing a network bottleneck and leaving the aggregator with a lot of work to do to form the final aggregate set. In 6.5, performance of queries like this is improved by (a) doing a local aggregate at the leaf level of all the partitions on that leaf, (b) spilling the rows to the network if it turns out almost every GROUP BY key is distinct so the local aggregation is not helping, and (c) reducing memory usage by having each thread handle a subset of the keys. The realistic best-case scenario query for this improvement is a GROUP BY that is shuffling (not shard key matching) and each partition has a large set of keys but the set of keys for each partition is roughly the same. Shard-key-matching GROUP BY is not affected by this change. The key part of this improvement is the local aggregate at the leaf level, since it allows more of the work to be done by the leaves and less by the aggregators, and it reduces network traffic. Performance Improvement: This improvement can give several-times speedups for some queries. For example, query q22 in one of our internal test workloads derived from TPC-H speeded up by more than a factor of 2 based solely on this change. IN-list Filter Improvement Performance has been improved for queries that use IN-list filters. 6.0 required a full scan of the IN-list for each row. 6.5 uses a Bloom filter to check if there is any match to help increase scan speed. For IN-list filters that disqualify the large majority of rows, this can give order-of-magnitude speedups. This improvement works for row store and column store tables. It is always enabled. Here’s an example that shows the dramatic speedup possible for IN-list queries in 6.5. For a table t(i int) with 8 million rows of unique integers, in row store format, we ran the following queries using 6.0 and 6.5: Query 6.5 6.0 select count(*) from t where i in (1, 100, 200); 0.23 sec same select count(*) from t where i in (1, 2, ... 904); 0.20 sec ( 60x speedup!) 12.0 sec The second query, with an IN-list containing 904 elements, speeds up tremendously, while the first one, with a short IN-list, stays the same. Integer RLE Filter Pushdown Run-length encoding (RLE) is one of several compression strategies used for our columnstore tables. SingleStore DB 6.5 can now push integer filters down to RLE-encoded data in the columnstore scan layer, giving significant speedup.Here’s a performance example. The table t(i int) is a columnstore with 10 million rows and 1,000 unique values, keyed on i. So it will be encoded with RLE. select count(*) from t where i = 2; The performance results on two cores with two partitions are as follows: 6.0 6.5 0.11 sec 0.002 sec As you can see, this enhancement can provide order-of-magnitude improvements. It’s always on; there is no need to set any switches to control it. Internal Memory Allocation Speedup Query execution memory allocation performance was significantly improved. Some TPC-DS benchmark queries speed up by over 2X based on this change. This microbenchmark speeded up by a factor of 20 compared to SingleStore DB 6.0: select count(substr(str, 0, 9)) from t; The improvement is in the internal memory allocator used by queries that contain non-trivial string expressions or create temporary rows. So queries such as this can become far faster. Query Optimizer and Statistics QE improvements are fantastic, but the QO system is always critical to make sure the right QE operators are used in the best possible way. Our query optimizer is the guidance system of the SingleStore DBMS. Automatic Selection of Hash Group to Get Operations on Encoded Data As we publicized in our Trillion Rows Per Second demo , in SingleStore DB 6.0 we improved our query execution dramatically for single-table GROUP BY/aggregate queries, by using a new implementation of HashGroupBy that uses SIMD and operations on encoded data. In 6.5, we’ve improved the costing code in the query optimizer by calibrating it so it understands more accurately the relative speed of HashGroupBy and StreamingGroupBy operations. So queries like this now routinely get a HashGroupBy instead of StreamingGroupBy: select stock_symbol, count(*) as c\nfrom trade\ngroup by stock_symbol\norder by c desc\nlimit 10; The table trade is a columnstore, sharded on id or stock_symbol and with key on stock_symbol . In effect, our query optimizer has caught up with the new capabilities available in our query execution system. The result is that queries may now run up to 80 times faster, automatically, with no need for hints or other workarounds. Advanced Histograms for Improved Filter Cardinality Estimation SingleStore DB 6.5 introduces a new type of histogram to represent what we sometimes refer to as “range” statistics. These histograms are termed “advanced histograms” and the existing histograms will be called “legacy histograms.” The advanced histograms hold substantially more information than the legacy histograms for certain data distributions. So, non-uniform data distributions with skew typically have much better estimates for less frequent values, and also better average estimates, than in SingleStore DB 6.0. For example, for this data distribution: +------+----------+\n| v    | count(*) |\n+------+----------+\n|    1 |   320000 |\n|    2 |      320 |\n|    3 |      352 |\n|    4 |      256 |\n|    5 |        1 |\n|    6 |      512 |\n|    7 |      512 |\n|    8 |      544 |\n|    9 |      512 |\n|   10 |      608 |\n|   11 |        3 |\n|   12 |      608 |\n|   13 |      576 |\n|   14 |      288 |\n|   15 |      288 |\n|   16 |      288 |\n|   17 |      160 |\n|   18 |      352 |\n|   19 |        2 |\n|   20 |      480 |\n+------+----------+ And this query: select count(*) from t where v = <constant>; We get these estimates: Value of <constant\\> actual 6.0 estimated 6.5 estimated 1 320,000 320,047 320,174 3 352 450 366 5 1 450 1 In 6.0, the high-frequency values (such as 1) crowded out information about the lower-frequency values (such as 5) in the histogram, causing loss of resolution. In 6.5, this crowding-out effect happens much less quickly. Normally up to 50 data values can be represented nearly exactly, regardless of data frequency skew. In a larger query, errors such as this can compound and cause a sub-optimal strategy to be chosen for a join (e.g. hash instead of nested loop), or a distributed operation (e.g. shuffle instead of broadcast). Since these errors are reduced in 6.5, overall query performance will improve. Before-and-After Results, 6.0 to 6.5, TPC-H Query performance in one of our internal test workloads derived from the TPC-H benchmark improved dramatically, about 2.2X on average, from 6.0 to 6.5. The primary reason for this is improved shuffle performance and, to a lesser extent, the high-cardinality GROUP BY performance improvement. Also, query q19 improved due to a rewrite to extract common conditions for OR ((a or b) or (a or c)) → (a and (b or c)). SingleStore TPC-H Scale Factor 1000, May 2018, Hardware: 3 Amazon Web Services (AWS) r3.8xlarge leaves, 32 vcores each Query 6.0 Time (sec) 6.5 Time (sec) Speedup (%) Speedup (times) Notes q1 5.824 5.38 7.62% 1.1 q2 3.537 2.977 15.83% 1.2 q3 45.661 18.052 60.47% 2.5 q4 6.437 6.47 - 0.51% 1.0 q5 22.127 12.669 42.74% 1.7 q6 3.334 3.392 - 1.74% 1.0 q7 17.478 9.641 44.84% 1.8 q8 4.97 4.023 19.05% 1.2 q9 78.55 33.866 56.89% 2.3 q10 15.395 11.287 26.68% 1.4 q11 3.654 3.495 4.35% 1.0 q12 3.821 4.252 - 11.28% 0.9 q13 62.558 39.001 37.66% 1.6 q14 75.244 21.528 71.39% 3.5 q15 24.668 9.931 59.74% 2.5 q16 40.461 12.24 69.75% 3.3 q17 7.21 3.21 55.48% 2.2 q18 300 72.387 75.87% 4.1 query timed out with error on 6.0 q19 6.691 4.998 25.30% 1.3 q20 70.276 26 63.00% 2.7 q21 149.832 143.061 4.52% 1.0 q22 46.297 9.69 79.07% 4.8 Averages 33.0 20.8 37% 2.0 (GEOMEAN) (GEOMEAN) (AVERAGE) (AVERAGE) †q18 did not run in under 300 sec on 6.0 so its time is given as the timeout of 300 sec. Load Performance Before you can experience the benefits of improved query speed, you have to get data into your database. Load performance is historically a strength of SingleStore. We’ve made it even better 6.5. Speedup of CSV File Parsing LOAD DATA commands have been made substantially faster. For wide tables with long comma-separated value (CSV) input lines, using strings enclosed with quotes or other characters, speedup of more than 2X is possible. We micro-optimized parsing. The most important change only applies to LOAD DATA with a FIELDS [ OPTIONALLY ] ENCLOSED BY clause, and is always “on.” It’s expected to be most beneficial for tables with many columns and relatively long lines. It only affects work done on the aggregator during the LOAD, and it doesn’t apply to pipelines. Loading 6GB of data – with average CSV line length of 1KB – into a 170 column table on a cluster on AWS with an m4.10xlarge aggregator + 4 m4.4xlarge leaves and 32 partitions total, we saw: MySQL [pm]> LOAD DATA INFILE '/home/admin/portable/LOAN_KEY.csv' INTO TABLE LOAN_KEY FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\"' LINES TERMINATED BY '\\n';\nQuery OK, 5029152 rows affected (31.14 sec) Before the change, the same load took about twice as long: MySQL [pm]> LOAD DATA INFILE '/home/admin/portable/LOAN_KEY.csv' INTO TABLE LOAN_KEY FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\"' LINES TERMINATED BY '\\n';\nQuery OK, 5029152 rows affected (1 min 9.76 sec) Pipelines from S3 Speedup The time for the CREATE PIPELINE statement to return, when used on Amazon S3 cloud storage, has been reduced dramatically for S3 prefixes with very large numbers of files under them. For example, for a million files under a prefix, it previously took about 5 minutes, and now it typically takes under 2 seconds. In addition, under an S3 path with a large number of files, when a new file is added, it will be loaded and visible in the SingleStore database much more quickly than before, typically in seconds rather than minutes. Previously the time taken to create an S3 pipeline grows with the number of objects in the source S3 bucket (more precisely, the number of objects matching the specified prefix) and is unbounded. For example, create pipeline took about 8 minutes on a bucket with 1.6 million objects. The root cause of the issue is the long time it can take to list out all the files under and S3 bucket. In 6.5 we fixed this issue and the number of network round trips taken equals (number of SingleStore partitions) / 1,000 , rounded up. Usually this translates to no more than 2 seconds. The second use case that is improved in 6.5 follows the “continuous loading” pattern where news files are added to the bucket and we expect to be able to query the data in SingleStore as soon as possible after the files are uploaded. Previously, this latency grew linearly with the number of existing files in the bucket. For example, suppose there are 200,000 files and a new file is uploaded. The rows from these files will not start to be loaded for about 20 seconds. If there are 2 million files, the latency would be over 3 minutes. Since S3 is often used for historical data, it is not uncommon that buckets grow to the extent that real-time analytics becomes infeasible. (Note that the overall bandwidth is not affected and therefore one-time load from S3 using SingleStore pipelines prior to 6.5 is not problematic.) In 6.5, this latency is not affected by the number of files in the bucket, given that newly added keys are greater in sort order. Best practices : To take best advantage of this improvement, it is recommended that customers add keys to their S3 bucket in ascending sort order. A random order of names in a large set of files can still cause a long delay before seeing the contents of a new file in the database. For example, if you name files with a string of the form “prefix_name _ <timestamp > ” where <timestamp > is of the form YYYY-MM-DD-HH:MM:SS.FFF then the file names will naturally be in sort order, so you’ll benefit from this new improvement. Columnstore Compression Performance Columnstore compression performance has been improved, particularly for dictionary-compressed column segments with under 2^16 values. Overall, LOAD and heavy INSERT workloads for low- to medium-cardinality strings are about 2X faster. This enhancement improves the speed of the background merger and flusher too. Sort Speed Performance The speed of the internal sort routine was improved. This is related to the improvements in the internal memory allocator that were previously mentioned. This improves performance of the background merger used by the columnstore, and any other internal system processes that sort data. Conclusion SingleStore has fundamental architectural advantages for performance compared with legacy SQL database systems, including scale out, compilation, in-memory structures, vectorization, and use of SIMD. In the 6.5 release, we’ve built on these architectural pillars to deliver tremendous performance gains. Some queries have speeded up over 60X, and some entire workloads have more than doubled in speed, with no application changes required. SingleStore is a database on the move. We have a talented engineering team that is keeping up a tremendous pace of improvement from release to release. Given our fundamental advantages, run-anywhere flexibility, and great performance that is only getting better, SingleStore is a great train to get aboard with your demanding data management applications.", "date": "2018-07-24"},
{"website": "Single-Store", "title": "workload-management", "author": ["Daryl Neubieser"], "link": "https://www.singlestore.com/blog/workload-management/", "abstract": "With the launch of SingleStore DB 6.5, we’ve added several new features to advance performance, accelerate time-to-insight, and simplify operations. With the goal of making SingleStore not only the world’s best database, but truly the easiest database to use. Many customers choose SingleStore for its potential to scale, but oftentimes a workload can scale in unanticipated ways at unexpected times. Problems can manifest as temporary spikes in load, skew, or concurrency. In SingleStore DB 6.5, we added workload management to more gracefully handle the unpredictable spikes in queries, while still maintaining high performance during normal load. In this blog post, I will walk through a few examples that workload management will improve. Then I’ll explain the system design that allows us to handle those cases. And finally, I’ll share some benchmarks showing the effect workload management can have. Maybe This Sounds Familiar Here is an example scenario where a customer might experience unanticipated load. Suppose we have a columnstore purchases table sharded on item_unique_id . During a one hour flash sale, a retailer sells an unexpected number of one specific item, generating an overwhelming amount of logging data for the item and creating temporary ingest skew on the associated node. Though the other nodes are running fine, queries on the skewed node get further and further behind, and eventually result in downtime of their website, losing them sales. Workload management is able to solve the problem by limiting key resource-intensive queries until the node has a chance to get back to a normal load. How Workload Management Works Workload management can be broken into three components that work together to address cases of heavy load: Detection, Prediction, and Management. Detection refers to identifying when any node is struggling. SingleStore differentiates between memory used for table data and memory used temporarily in queries to determine if it is safe to continue forwarding more queries to the target nodes. Prediction refers to estimating the resource usage of queries and classifying them into groups based on memory usage and expected runtime. In SingleStore DB 5.8, we introduced management views to allow users to see resource usage statistics of previously-run queries. Workload management uses these same statistics to determine how resource intensive each query is from a memory consumption perspective. The last component is Management, which admits queries in three tiers. The cheapest queries such as single-partition selects, inserts, or updates avoid being queued entirely. Queries that use moderate resource amounts are queued on a FIFO (first-in first-out) basis at a rate dependent on the highest load among leaves. Lastly, the most expensive queries are queued with a maximum concurrency limit and the Master Aggregator decides when each will run. Since there is a latency cost to the coordination, only the most resource intensive queries fall into this category. Putting it into our retailer example from before, SingleStore would first detect the struggling node. Queries that could be predicted to be fast and cheap would still complete with no regression in latency. The rate of expensive queries would be scaled down to compensate and allow the leaf to keep up. Once the leaf was able to free up more memory, the expensive queries could resume their normal rate of admission and run freely. In the context of our example online retailer, once the load spike finished, their memory-intensive analytical queries would return to their previous level of performance. Workload Management Benchmark We can compare individual query performance between SingleStore with the workload management feature turned off versus on when spiking to 4x normal concurrency. In our benchmark we tested 8 concurrent users (normal concurrency) and 32 concurrent users (spike concurrency). We used a SingleStore configuration with 1 core per partition to ensure that even the normal concurrency mode would be bottlenecked on CPU. Each user would send queries to SingleStore in a random order with 1 in 25 of the queries being expensive (high cardinality distributed group-by) and the rest cheap (various low cardinality group-by and distributed joins). Query latency with concurrency spike: Workload Management Off Workload Management On Cheap Query Average Latency Expensive Query Latency Cheap Query Average Latency Expensive Query Latency Normal Concurrency 2.7 sec 37 sec 2.5 sec 20 sec 4x spike concurrency 7.5 sec Queries Fail due to running out of Memory 6.4 sec 115 sec Under normal concurrency, workload management schedules the expensive queries more efficiently, providing a slight improvement to latency overall. Another side effect of enabling workload management is that SingleStore actually scales better than the factor by which we scaled concurrency on the more latency-sensitive cheap queries (56% better) with a tradeoff with the expensive query (44% worse). In contrast, without workload management, the concurrency spike causes expensive queries to fail while also slowing down cheaper queries. We’re excited about this new feature, and think it will help you manage your workloads more effortlessly than before. Go ahead and try SingleStore DB 6.5 , or read more about what’s new in our blog .", "date": "2018-07-24"},
{"website": "Single-Store", "title": "multi-tenancy", "author": ["Rick Negrin"], "link": "https://www.singlestore.com/blog/multi-tenancy/", "abstract": "Many organizations find they need to support a database-as-a-service workload. This is a model where multiple organizations have data living side by side to reduce management costs but each organization requires a strong namespace and security boundary around its data. Organizations also often want to leverage the existing ecosystem of tooling available on top of the data layer. This is a challenge to do using legacy databases because they do not scale out and a challenge with NoSQL systems because they lack the tooling ecosystem and structure their users want. This pattern appears in two different scenarios. The “Enterprise Database-as-a-Service” This is where a large enterprise with an IT team that wants to enable self-service for departments that build their own applications or manages their own data for doing analytics. They need a database to do it but don’t want to be responsible for managing the hardware or system software. The data owners own the logical management (defining their schema, tuning their queries, creating indexes, etc.) but the IT department manages the physical aspects (hardware, system software, and capacity management). The number of databases is usually a multiple of the number of departments that need this functionality, which means there could be hundreds or thousands of databases in a large organization. Some databases are small (tens or hundreds of gigabytes) with a few that are large (multiple terabytes). The activity on the database (i.e. how many users or applications are querying it at the same time) will also vary wildly depending on the use case. Data owners will also have varying levels of SLAs on availability and durability of the data. This makes resource consumption highly variable. Given these requirements it is a challenge for IT to operate and manage the databases and maintain the SLAs required by the data owners. The “Multi-tenant SaaS Service” This is where a company is building a multi-tenant service that is sold to organizations where each organization owns its data. An example of this would be a marketing analytics service where the service takes in data about how a marketing campaign did (with data coming from many sources), then offering canned and/or ad-hoc analytics over the campaign results. In this case, the service owner wants the ability to easily separate the data for each of its customers for security, namespace, and performance reasons while still retaining a single control point for management (i.e. a single cluster to manage). Each database likely has the same schema, and the schema needs to be updated to keep it in sync, with perhaps small customizations. This amortizes the cost of management so that the service owner can maintain profitability as it acquires more customers. In addition, customers want to support both very large and very small customer databases without worrying about over-provisioning, under-provisioning, or hitting scale limits of a single node. SingleStore is a great platform for building such a system. A database in a SingleStore cluster is a natural namespace and security boundary. A cluster is a perfect single control point for managing the system. Additionally, SingleStore is a distributed database so you don’t have to worry about one of your customers hitting a scalability limit, like you would for single box databases, such as Azure SQL DB or Aurora. In legacy database systems the largest databases have to be manually sharded in the application layer because they outgrow a single node. Manually sharding like this is very difficult and error prone. SingleStore handles this naturally and transparently. Customers can grow their usage as needed and simply use more resources in their cluster. These operations are online and therefore transparent to the data owners. Customers, especially smaller ones, share resources, which keeps costs low. Because of the sharding model of SingleStore, the workload is naturally spread evenly across the cluster. When your aggregate usage grows larger than the cluster can handle, a cluster can be increased by simply adding more nodes to handle the load. SingleStore also has a resource governance mechanism to prevent one database from unfairly using more than its fair share of resources. Last, SingleStore supports both transactional and analytical workloads making it appealing regardless of the workload type that you need to support. So if you are an enterprise architect tasked with building a database-as-a-service model or a company building a new software-as-a-service offering, then SingleStore is a great option for your data layer.", "date": "2018-07-24"},
{"website": "Single-Store", "title": "6-5-pipelines", "author": ["Bryan Offutt"], "link": "https://www.singlestore.com/blog/6-5-pipelines/", "abstract": "With the release of SingleStore DB 6.5, the SingleStore team is announcing our latest innovation in data ingestion: Pipelines to stored procedures. By marrying the speed and exactly once semantics of SingleStore Pipelines with the flexibility and transactional guarantees of stored procedures, we’ve eliminated the need for complex transforms and unlocked the doors to a variety of new, groundbreaking use cases. This blog post will walk you through how to use Pipelines to stored procedures to break data streams up into multiple tables, join them with existing data, and update existing rows, but this is just the tip of the iceberg of what is possible with this exciting new functionality. How Does it Work? Extract → Transform (Optional) → Stored Procedure OR Traditional Single Table Load Pipelines to stored procedures augments the existing SingleStore Pipelines data flow by providing the option to replace the default Pipelines load phase with a stored procedure. The default Pipelines load phase only supports simple insertions into a single table, with the data being loaded either directly after extraction or following an optional transform. Replacing this default loading phase with a stored procedure opens up the possibility for much more complex processing, providing the ability to insert into multiple tables, enrich incoming streams using existing data, and leverage the full power of SingleStore Extensibility. Pipelines Transforms vs. Stored Procedures Those familiar with SingleStore Pipelines might be wondering, “I already have a transform in my pipeline….should I use a stored procedure instead?” Leveraging both a transform and a stored procedure in the same Pipeline allows you to combine the third-party library support of a traditional transform alongside the multi-insert and data-enrichment capabilities of a stored procedure. Traditional Transform Stored Procedure Pros - Can be written in any language (BASH, python, Go, etc.) - Can leverage third-party libraries - Easily portable to systems outside of SingleStore Cons - More difficult to debug and manage - Less performant for most use cases - Only allows inserting into a single table - No transactional guarantees Pros - Insert into multiple tables - SingleStore native - High performance querying of existing SingleStore tables - Transactional guarantees Cons - SingleStore-specific language - No access to third-party libraries for more complex transformations Example Use Cases Insert Into Multiple Tables Pipelines to stored procedures now enables you to insert data from a single stream into multiple tables in SingleStore. Consider the following stored procedure: CREATE PROCEDURE proc(batch query(tweet json))\nAS\nBEGIN\n    INSERT INTO tweets(tweet_id, user_id, text)\n      SELECT tweet::tweet_id, tweet::user_id, tweet::text\n      FROM batch;\n    INSERT INTO users(user_id, user_name)\n      SELECT tweet::user_id, tweet::user_name\n      FROM batch;\nEND This procedure takes in tweet data in JSON format, separates out the tweet text from the user information, and inserts them into their respective tables in SingleStore. The entire stored procedure is wrapped in a single transaction, ensuring that data is never inserted into one table but not the other. Load Into Table and Update Aggregation There are many use cases where maintaining an aggregation table is a more performant and sensible alternative to running aggregation queries across raw data. Pipelines to stored procedures allows you to both insert raw data and update aggregation counters using data streamed from any Pipelines source. Consider the following stored procedure: CREATE PROCEDURE proc(batch query(tweet json))\nAS\nBEGIN\n    INSERT INTO tweets(tweet_id, user_id, text)\n      SELECT tweet::tweet_id, tweet::user_id, tweet::text\n      FROM batch;\n    INSERT INTO retweets_counter(user_id, num_retweets)\n      SELECT tweet::retweeted_user_id, 1\n      FROM batch\n      ON DUPLICATE KEY UPDATE num_retweets = num_retweets + 1\n      WHERE tweet::retweeted_user_id is not null;\nEND This procedure takes in tweet data as JSON, inserts the raw tweets into a “tweets” table, and updates a second table which tracks the number of retweets per user. Again, the transactional boundaries of the stored procedure ensures that the aggregations in retweets_counter are always in sync with the raw data in the tweets table. Use Existing Data to Enrich a Stream It’s also possible to use a stored procedure to enrich an incoming stream using data that already exists in SingleStore. Consider the following stored procedure, which uses an existing SingleStore table to join an incoming IP address batch with existing geo data about its location: CREATE PROCEDURE proc(batch query(ip varchar, ...))\nAS\nBEGIN\n    INSERT INTO t\n      SELECT batch.*, ip_to_point_table.geopoint\n      FROM batch\n      JOIN ip_to_point_table\n      ON ip_prefix(ip) = ip_to_point_table.ip;\nEND These use cases only scratch the surface of what is possible using Pipelines to stored procedures. By joining the speed of Pipelines with the flexibility of stored procedures, SingleStore DB 6.5 gives you total control over all of your streaming data. For more information on the full capabilities of SingleStore Extensibility, please see our documentation .", "date": "2018-07-24"},
{"website": "Single-Store", "title": "6-5-overview", "author": ["Rick Negrin"], "link": "https://www.singlestore.com/blog/6-5-overview/", "abstract": "The World’s Fastest Database Just Got Faster At SingleStore, we’re on a mission to create the world’s best database. On our path to achieve that goal, today we announced our latest product, SingleStore DB 6.5, is generally available. As the no-limits database™, SingleStore provides maximum performance, scalability, and concurrency for your most important applications and analytical systems. This latest release further improves on what customers love about SingleStore, advancing performance and adding capabilities to accelerate time to insight and simplify operations. Today, the pace of innovation and competition means that businesses are asking more and more of their technology teams. You must provide flawless customer experiences. The business relies on you for consistently fast insights. And you must create and scale these systems as quickly as possible. To meet these demands, SingleStore allows customers to run both transactional and analytical workloads at web scale with a single database — all without having to abandon familiar, standard SQL. This means that whether you are building a new application, modernizing existing applications or infrastructure, or trying to improve database performance, SingleStore can help everyone quickly achieve the results business demands without having to learn a host of new tools or abandon existing application logic. New advancements in SingleStore DB 6.5 take these abilities even further. Experience Latency-Free Queries The world’s fastest SQL database just got faster. Customers – whether end or internal users – have no tolerance for latency. Faster database response times means more data can be analyzed more frequently while improving the accuracy of insights. SingleStore DB 6.5 has made queries 2-4x faster than the previous version (which was already 10x faster than legacy database providers) and improves data ingestion speeds for high data volume applications. A series of query improvements extends breakthrough performance of group-by, joins, and filter queries to deliver insights in milliseconds across billions of rows. Deliver Performance at High Concurrency with Enhanced Workload Optimization Data professionals spend hours trying to manage database performance for fast growing data and user access. In the face of extreme workloads, automated workload management provides consistent database response without the help of experts. While under high concurrency, SingleStore automatically delivers reliable database performance to easily support growth in application users and queries.Our autonomous workload manager inspects and queues high volume queries to deliver the best possible response for all types of queries. Build Real-time Data Pipelines with Stored Procedures Building scalable data pipelines to handle data ingestion often requires additional tooling that is costly, complex to use, and difficult to deliver reliable performance. SingleStore provides built-in pipelines for technologies such as Kafka, Amazon S3, and others to provide a more efficient architecture that delivers record breaking ingestion performance without the complexity of third-party tools. With 6.5, SingleStore Pipelines extends its industry leading “transform-as-you-ingest” capabilities with new in-database transformations using stored procedures. The result is more powerful programmatic pipelines that give developers more control for machine learning and data enrichment operations on streams of data, eliminating the cost and complexity of separate ETL tooling. Easily Scale Your Multi-tenant Cloud Application Managing a multi-tenant service can prove challenging due to unpredictable data and user growth along with limitations on where the service can run. The result is over or under provisioning resources along with getting locked into a cloud provider impacting service performance and costs. With SingleStore DB 6.5, new resource optimizations help deliver a more tailored, scalable multi-tenant database environment you can deploy anywhere , including on-premises or public cloud with the ease of scale to meet the changing demands of your application. The result is a no-limits database that performs in the most optimized and efficient way, regardless of the size or scale of your application. Extend Insights to More Data with Full-Text Search Roughly 80 percent of all data is unstructured, including text data, so there is a need to provide a solution that organizes and analyzes that data for modern applications. Building text-search into modern applications often means specialized tooling that requires additional expertise and management overhead. SingleStore has full-text search built in , so developers can more easily deliver fast, scalable full text capabilities with keyword-based and regular query conditions in a single SQL statement. The addition of text search to SingleStore DB 6.5 eliminates the need for a separate specialized data store, allowing you to simplify architecture and improve overall performance. Get Improved Data Pipeline Support for Hadoop While Hadoop provides a cost-effective environment to store data, it has become insufficient for high performance analytics at-scale. With a fast relational database, data architects and scientists can combine HDFS with scalable SQL to deliver faster insights with the familiar language everyone understands. The new SingleStore HDFS data pipeline delivers continuous ingestion from data lakes into a structured relational format to deliver faster analytics using the familiarity of ANSI SQL. Want to Learn More? Attend our webinar : Learn more details and see a demo of the new capabilities in SingleStore DB 7.3 Try Now : Get a free evaluation of SingleStore DB 7.3", "date": "2018-07-24"},
{"website": "Single-Store", "title": "pandora", "author": ["Kaan Erdener"], "link": "https://www.singlestore.com/blog/pandora/", "abstract": "In 2016, Pandora chose SingleStore to serve as the data store and foundation for solving a specific business problem. In this post, we explore: The Business Problem — what were we trying to solve? The Previous System — a few details Requirements — business requirements (and a few technical ones too) The Contenders — what did the options look like About SingleStore How We Implemented SingleStore Data Strategy: Columnstore vs. Rowstore Hardware Details Into the Weeds Summary The Business Problem The first step in finding a solution is to clearly define the problem. The goal was not to find some way to solve the problem, but the best way to solve it. What were we trying to solve? We wanted a dashboard that would allow our Ad sales team to view data about current and former ad campaigns. There were of course more details that described the goal as well as a number of technical challenges along the way, but from a business perspective, the core success criteria really was to simply “build a dashboard”. Without a usable dashboard, no amount of engineering heroics would matter. The Previous System When this project started, we already had a system that various teams across the company used on a daily basis. As is the case with many projects, it solved specific needs that made sense at some point, but it was increasingly difficult to keep up with changing requirements and new features. Over time, limitations and shortcomings became clearer and more impactful. These were some of the biggest limitations in our previous system: There was a two-day delay prior to new data being available. Answers about Monday weren’t available until Wednesday. Answers to the hardest, most interesting queries were pre-computed. This was great for query performance, but it resulted in a fairly rigid user experience where it wasn’t easy to ask new questions or even slight variations of existing questions. Performance was a concern. Keeping up with each new 24-hour period of data took ~20 hours elapsed time on a large Hadoop cluster. As data sets continued to grow, more and more processing time was required to handle one day’s worth of data. Requirements The requirements are listed below, in no particular order. At the start of this project, there were many internal conversations that started with, “Have you looked at X? It’s a way to store data and it’s great.” There were many different “X” things that came up. In some cases, we had looked at X, other times not. By starting with a list of requirements, it was easy to see which data storage options might work and which would miss the mark. It’s an Apache Incubator project? We’re not going to use it. Scaling the cluster requires the whole thing to be offline for hours or days? Not doing that. It’s a columnar data store and serves queries quickly, but you have to rebuild the columnar indexes for new data to be visible? Nope. Deliver results via real-time queries — Instead of pre-computing answers for specific, predefined questions, we wanted a data store that would be fast enough to support real-time queries in less than one second. New data should be available as soon as possible — We wanted new data to be available much faster than two days. Some performant data stores address query speed by pre-computing fixed answers hours in advance, so this requirement came to mean “no pre-computed answers”. We wanted to get away from the cycle of new data arriving, waiting for nightly jobs, view the results in a dashboard. Real-time queries would also be faster when we have new data to show in the UI in the future. This requirement narrowed the field quite a bit. Support two years of raw click and impression data — A window of two years exceeded the longest business needs, so we started there. The initial plan was to keep a rolling window of no more than two years’ worth of data due to concerns about increased storage (for columnar data) as well as performance degradation (for both reads and writes). This requirement translated into “hundreds of billions of rows” and “terabytes of data”. As it turns out, the “two year” mark was quite conservative — we’re currently approaching four years of accumulated data (we backfilled prior to 2016) with no observed performance impact and plenty of storage remaining (thanks in large part to a steady 85–90% on-disk compression for columnar data). It should be fast — As a concrete goal, we wanted sub-second query response over 100’s of billions rows of data. Some data stores claim to be “fast” or “X times faster than Y” (“Presto can be 100 or more times faster than Hive”), but what are the real numbers? For example: if a table has 10 billion rows, how much time will pass between issuing a query and getting a result? Many “fast” data stores produce an answer anywhere from “several seconds” to minutes (or more). Being 100 times faster than a 2-hour Hive query isn’t nearly fast enough to satisfy a user clicking around a dashboard. Product maturity — While we didn’t have hard requirements about what this meant, it was clear that some options in the mix were significantly more experimental than we were comfortable with. Why was this important? We didn’t want to rely on unproven technology to serve as the foundation for this project. We understood that it was important to choose something which had real production use, ideally at a scale similar to ours, rather than jumping straight into the weeds trying things that have never been done. Some projects can afford the risk vs. reward tradeoff, as well as open-ended timeline, but we could not. We wanted something that would work out of the gate. Minimal barriers to internal adoption — This requirement addressed the reality of introducing new technology to the team. Some of the options looked like interesting contenders, but had a totally unique, unfamiliar query language. Some options had significant limitations in database drivers. Ideally, we would end up with something that supported standard SQL and had plenty of database drivers available. 24/7 support — When (not “if”!) things go wrong, would we need to rely on our own engineers to understand problems, find solutions, and implement fixes? Hardware issues, operational problems, software bugs, incorrect configuration, human error… we knew from experience that, over time, something will go wrong. What we don’t know is what, when or how to fix it. Is there an option for immediate help? Needs to be scalable — As our data set grows and usage patterns change, we needed a path to grow capacity. This could mean adding more disk to increase storage capacity, or more compute to maintain query performance. Be willing to spend money — We wanted to be sure we weren’t making a choice based solely on up-front dollar cost. There were several “no cost” options that were great in many regards, but clearly would have required that we pay salaries over time (months? years?) for our engineers to learn, understand, implement, operate, maintain, troubleshoot and fix things. For these, it was easy to see that there would in fact be plenty of cost, even though the software itself was free. We knew that whatever we chose, we would need to do things like get a system up and running, design the system, understand and measure performance, and troubleshoot problems. We were willing to spend money up front if it would speed up those things up. The Contenders Based on our requirements, we narrowed the initial group down to three finalists. Each of these finalists addressed all (or nearly all) of our needs. RedShift — features and performance were great, but dollar cost was a significant negative. Over a three year period, it would have cost many times more to pursue RedShift compared to purchasing on-prem hardware with similar compute/memory/disk with something like SingleStore or CitusDB. Additionally, using RedShift would have meant overcoming another hurdle — shipping the data into Amazon data centers. CitusDB — it checked most of the boxes we were looking for, but it wasn’t as fully-featured as the other finalists. Note that we were performing this evaluation during the first half of 2016 — more than two years ago — and it would be a safe bet that CitusDB has improved significantly since then. It looked like they were headed in the right direction with a great team, but simply needed more time to meet what Pandora was looking for. SingleStore — this offered the right mix of features, performance, and flexibility to address the problems we wanted to solve. Below are some of the original 30 contenders that were weeded out along the way. Obviously, each tool has intended uses along with strengths and weaknesses. Our evaluation phase set out to check each option as a possible fit for our requirements. Below you’ll see that only the negatives for our usage scenarios are called out. This in no way means to portray any of these data storage options as generally bad. Google BigQuery — was dinged for a number of reasons, among them: data being read-only, limitations in SQL joins, and requires Google storage. Also, BigQuery was not built to serve sub-second queries (over any size data set, small or large). Apache Impala — was dinged for performance reasons. One data point showed query response time of 12 seconds for 10 concurrent users on 15TB data set. Kylin — was dinged for requiring offline processing to pre-calculate data cubes for anything new. Apache Phoenix — was dinged for performance reasons. Performance results published on the Phoenix website discuss query times of several seconds on tables of 10 million rows. VoltDB — was dinged on cost. It relies fully on system memory, so we would have needed to throw a LOT of money at hardware to have enough memory to store hundreds of billions of rows (terabytes) of data. Additionally, it’s not standard SQL (doesn’t support things like foreign keys or auto-increment) and everything is a stored procedure. About SingleStore SingleStore Inc. was started by Eric Frenkiel and Nikita Shamgunov in 2011. The SingleStore database was first available in 2013. Customers include Akamai, Comcast, and Uber — read more about them (and others) here . SingleStore Inc. provides 24/7 support as part of an enterprise license. For more information and background, this post by SingleStore has a nice overview about what SingleStore is and how it works. This Wikipedia entry also has some good overview information. How We Implemented SingleStore Our goal was to consolidate data from multiple sources into a single data store. To that end, we built a few different ingestion paths to get data from Hive, Postgres, and data files. One of those data sources consisted of the “100’s of billions rows of data” mentioned earlier and we focused right away on building a solution that would maximize performance for queries against that data. We worked directly with the SingleStore team to dig into a number of important steps. This was invaluable in saving us time. We continued working with their team from initial discussions, through building a test cluster, to building a production cluster and going fully live. Some of the things we did: build realistic sample data sets understand how SingleStore works, both in general as well as specifically with our data understand how things change with different hardware configurations and cluster settings understand what optimal query performance looks like understand how to build and configure a cluster to provide the best redundancy in the event of hardware problems learn how to measure and evaluate query and cluster performance learn basic cluster administration to keep things going ourselves, and also learn which “red flags” to keep an eye out for (things where we should escalate straight to their support team) Over time, we were able to understand and build a suitable test cluster that focused on columnar data (as opposed to rowstore) performance. Rowstore tables have a number of advantages over columnar (not just speed), but the impact on memory usage is significant — rowstore tables are stored directly in cluster memory, whereas columnar tables are stored primarily on disk. There are many factors to consider, but a simple rule that guided us pretty well over time was: “if the table has less than 1 billion rows, start with rowstore; more than 1 billion, start with columnstore”. We built a few data ingestion paths, including one that maximized write performance (for the largest data set). In our test cluster, even before we were good at writing data quickly, we were able to sustain ~150,000 inserts per second into a columnar table from a single machine, for periods of hours. With several machines writing into the cluster, we could sustain 500,000 inserts per second. A rate of 1M per second was possible but created pressure elsewhere. This meant we could, for example, do performance tests with a test data set of 10 billion rows, change things (configuration, table definitions, cluster settings, etc.), reload the same 10 billion rows in 5–6 hours elapsed, then do additional performance tests. Another skill we developed over time was writing performant, sub-second queries to drive the new dashboard against hundreds of billions of rows of data. Like all performance tuning, this was a mix of art and science, but we got pretty good at making things run fast. And thankfully, whenever we were stuck, we could reach out to SingleStore for input on how to tune / tweak. Data Strategy: Columnstore vs. Rowstore SingleStore supports a few ways of storing your data, defined on a per-table basis: columnstore or rowstore (and rowstore has two variants: normal or reference). It’s powerful to be able to define some tables as columnstore, others as rowstore, and some as reference tables. It provides flexibility in how you arrange and organize your data, and allows you to choose the trade-offs between things like query performance, memory usage, disk usage, and network traffic. So for a given table, which do you pick? SingleStore’s documentation covers this topic (along with many, many others) — read about it here — but below I included some of the points that were relevant for us. Reference Tables If the table is small enough (hundreds or maybe thousands of rows), a reference table can be great. Data in a reference table isn’t distributed across the nodes in your cluster, but instead each node has a full copy of the entire table. If you have 32 nodes in your cluster, you will have 32 copies of that table. By keeping a full copy of the table on each node, you might be able to speed up query execution. The cost of using reference tables is additional system memory usage. For small tables, the trade-off might be worth the improved query performance since individual nodes would no longer have to ask other nodes about that table. The cluster-wide memory usage adds up quickly though. Columnar Deletes and Updates All CRUD operations are supported on columnar tables, but it’s important to understand the implications of performing each operation on columnar data. Inserts and reads are generally fast. Under the covers, SingleStore does a great job of handling writes quickly and also making those new rows readable right away. Each columnar table has a smaller, hidden rowstore table where new data accumulates. This allows writes to land quickly without waiting on disk I/O. Reads can be served up quickly too, as a blend of these two tables. This makes it possible to write 10 million rows into a columnar table and see those rows in query results almost immediately. For read performance, SingleStore can query millions of rows per core per second. With hundreds of billions of rows in a table, the read and write performance is so fast it kind of feels like magic. Updates and deletes work differently and are significantly slower. This makes sense when you consider that columnar tables are primarily stored on disk, so any changes to a row involve disk I/O. Using a columnar table for fact-style data is a great fit, as it can keep up with huge quantities of data and serve read queries very quickly. By keeping columnar deletes and updates to a minimum (or never!), your cluster will have more cpu and disk I/O available to serve your read queries. If your intent is to use a columnar table for changing data (such that you’d be doing frequent updates and/or deletes), it’d be a good idea to take a step back and try to approach the problem differently. Reads vs. Writes Between reads and writes, we knew that it was more important to have fast reads than fast writes — powering a dashboard required it. This point didn’t surface too often but occasionally it was helpful in making a decision about how to proceed. Indexes Rowstore tables support one or more indexes, including unique indexes. Columnstore tables can have no more than one index, and unfortunately it cannot enforce uniqueness. Depending on your data usage, you might require multiple and/or unique indexes, in which case using rowstore might already be decided for you. For our largest tables, we could have used a unique index (as well as additional indexes for performance) but got by without them. System Memory “Good, fast, cheap” — pick two. There is definitely a balance to strike here. Rowstore tables are the “good, fast” choice and a great starting point if you’re not sure which to choose. With enough ram, you could theoretically put anything in rowstore and never user columnar tables, but you’ll have to swallow the hardware purchase first. Columnar tables are the “fast, cheap” choice. With enough compute available, reads and inserts have excellent performance. But the limitations for indexes and performance of updates and deletes take the “Good” out of the picture. Hardware Details There are decisions to make in hardware for a SingleStore cluster that depend on how the cluster will be used. Our hardware choices emphasized columnar performance — reads, writes, and overall disk capacity. Read Performance Read performance for columnar tables is affected by disk performance as well as available cpu cores. Fast disks alone might not result in fast reads if there are too few cpu cores available. For example, given two machines with identical disk setups, a machine with 16-cores would probably serve columnar reads faster than a 4-core machine. Similarly, many available cpu cores would almost definitely produce slow reads with 10,000 rpm spinning disks. Fast reads required cpu cores as well as fast disk read performance. In the early days, we did tests with different cpu+disk configurations to build experience and understanding with how SingleStore worked. Would it be possible to build a cluster using 10k drives instead of SSDs? Could we use 8-core machines instead of 32 or 64-cores? How much would these variables affect performance? The tests we performed were extremely valuable when making hardware decisions. Write Performance Write performance for columnar tables is affected primarily by disk write performance. In our test cluster, we addressed this by using multiple SSD drives in a RAID 0 (striped) configuration. For our production cluster, per SingleStore’s recommendation, we opted for the additional hardware redundancy of RAID 10 — a combination of RAID 1 (fully mirrored copy of the data) and RAID 0 (faster performance by striping data across multiple disks). SingleStore itself has powerful redundancy features which we rely on if a node fully drops offline for any reason, whether expected or planned. Behind the scenes, the cluster spreads data out across all nodes on all physical hosts and maintains a second copy of everything. That way, if a single host goes away, the data on each node will be immediately available on another node on another machine. It’s pretty cool how this works — we did tests where we hit a cluster with repeated reads and writes in a loop, hard killed a node, saw a flurry of failures with the query looper, then 1–2 seconds later everything was working great again. SingleStore does a lot of heavy lifting to make failover work and it involves copying data around behind the scenes. Each piece of data lives in two places — the first is the master, the second is the replica. SingleStore automatically manages this for you, including keeping track of who is the current master, who is the replica, when to promote the replica if the master goes away, and who takes over as new replica if the old replica goes away. With the scale of our data, tests showed that these failover operations took 20–30 minutes. During that time, the cluster would be subjected to additional cpu, disk I/O and network activity, all of which resulted in slightly degraded query performance. Without a mirrored RAID configuration, a simple drive failure would trigger that failover scenario. By using RAID 10, we removed drive failures from doing anything beyond triggering an alert for drive replacement. Disk Capacity Overall disk capacity was more involved than simply adding up the size and quantity of each SSD disk. At a minimum, we needed four (or more) physical disks so that we can use RAID 10. As we sketched out different disk configurations, we found other limitations. As the number of physical disks per machine increases, you might need to use a larger chassis, which might change motherboards, which might change all kinds of other things. A larger chassis also obviously can use more space in the physical rack, which you may not want to do. So while it was initially appealing to trim costs by purchasing a large quantity of small SSDs for each machine, that cost savings would have disappeared as the rest of the machine changed to accomodate the physical disks. Based on our tests, we were able to make accurate estimates for total disk usage requirements over time as data continued to grow. In the end, we settled on four disks per machine. This gave us a comfortable balance of up front cost, disk capacity for future growth, and simpler hardware for each machine. Into the Weeds Here are some additional learnings we accumulated along the way, presented in no particular order. These topics get into the inner-workings of SingleStore a bit, and turned out to be pretty interesting as we learned more about them. Columnar Index Data Types While optimizing query performance, we measured query speeds when using different data types for columnar indexes. We looked specifically at VARCHAR, VARBINARY, and BIGINT. In our tests, we found that a numeric columnar index key data type (BIGINT) was faster than non-numeric (VARCHAR or VARBINARY). We measured BIGINT at ~0.02 seconds vs. five times longer for both non-numeric variations. Same data, same hardware, same queries. The only variable was changing the data type of the columnar index key. While the “slower” options were still quite fast, these findings were useful. Execution Plans and the Plan Cache SingleStore utilizes an embedded compiler to enable low-level optimizations that are unavailable via the standard interpreter-based database execution model. SingleStore uses these optimizations in query compilation. After a query is first run on each node, a compiled query and query plan are cached on disk for future use. All future occurrences of that query running will use the compiled query and execution plan. There are ways of writing your SQL that can work well in this regard, but it’s also possible to inadvertently write queries that have to be recompiled each time. Understanding how to look at the plan cache is useful. It’s also sometimes useful to manually remove a cached query if you’ve made changes and you want to try them out fresh. Caches reset when nodes are restarted, which can be helpful (if you want to force reset everything in the cluster) or confusing (bounce a few nodes then wonder why things seem slow). The plan cache also has an expiration to it, so the cluster will periodically build new plans over time. We ran into this when our users would get back to the office on Monday morning — after a weekend of cluster inactivity — and discover that everything was slow. Once everything warmed up, performance would remain great through the week. With SingleStore’s guidance, we were able to tune the cache to accommodate this. OR vs. IN One of the scenarios we uncovered was using “OR” clauses as opposed to “IN” clauses in our SQL statements. Consider the following two queries: Query A: SELECT * FROM foo WHERE id=1 OR id=2; Query B: SELECT * FROM foo WHERE id=1 OR id=2 OR id=3; As written, these will result in different execution plans even though the queries are very similar. In our usage, the “id=1 OR id=2” clause is dynamically generated, and could have anywhere from 1 to dozens or more OR statements. Since each slight variation results in a new execution plan, we ended up seeing intermittent slowness. This one took a while to identify. It turns out there’s an easy fix — use IN clauses instead, like this: Query A: SELECT * FROM foo WHERE id IN (1,2); Query B: SELECT * FROM foo WHERE id IN (1,2,3); Both of these queries (along with all other variations of numbers in the IN clause) land on the same execution plan, so they will all be fast. Data Skew Data skew occurs when data in a table is unevenly distributed across nodes in the cluster. SingleStore will distribute the data across the nodes using a shard key that you specify, or base it off the primary key if you don’t explicitly define one. A healthy distributed database should have approximately 1/N-th of your table data present on each of N nodes in your cluster. Measuring and observing data skew was useful to understand if we picked the best shard key. Summary When we started working on the new Ads dashboard in 2016, it wasn’t clear that we would be able to satisfy the project requirements. It was, after all, a challenging problem to solve. In fact, our initial research included me asking around to see how other large companies addressed this problem, and — without naming names — I found that several highly recognizable companies dealt with their ads dashboards much the same as we had already been doing (daily aggregation jobs that had problems and limitations). Surely, if this were easy to do, it would have already been done. From the start it was very clear that we needed to pick a suitable data store or the entire project wouldn’t be a success. By focusing our options and narrowing down the field based on our requirements, we ended up choosing SingleStore. It fit very well with our project requirements, features, and performance needs. It’s also great that, a few years later, all of those have remained true. As we built experience using SingleStore, we began looking for other data projects at Pandora that might be a good fit. In a few cases, there was a project that looked promising but a deeper dive showed things weren’t as good a fit after all (those projects either remained as is, or went a different, non-SingleStore direction). In other cases, we’ve found a great match between project needs and SingleStore’s strengths, so our usage of SingleStore has expanded beyond our initial Ads dashboard project. The original posting of this blog can be found here .", "date": "2018-08-16"},
{"website": "Single-Store", "title": "gartner-catalyst-san-diego-its-a-wrap", "author": ["Mike Boyarski"], "link": "https://www.singlestore.com/blog/gartner-catalyst-san-diego-its-a-wrap/", "abstract": "Once again, the Gartner Catalyst event in San Diego, California provided another perfect setting for the 2,500 attendees and roughly 150 sessions. Many technology initiatives and topics were discussed, so we wanted to highlight our favorite takeaways. New this year, we provided a lounge area to give attendees a place to rest, recharge, and get caught up on email. We were excited to unveil our latest messaging at the event: SingleStore, the No-Limits Database™ for modern applications. Here were some of our favorite data and analytic sessions: Preparing for 2019: Future-Proof Your Data and Analytic Strategy With New Architectural Patterns That Matter by Carlton Sapp. In this talk, Carlton described the importance of preparing for the known and unknown data architecture requirements ahead. In the same vein that we all should have our personal safety and well being prepared or “prepped”, the same should be done with your data and analytics strategy. Salient points included evolving data ingestion with high reactive or stream processing. Persisting data with more structure leveraging data lake zones, and adding AI/ML in data management through embedded intelligence in data warehouses. Kafka – The Essential Broker by Thornton Jared Craig. Thornton, or TJ for those “in the know”, did a great job explaining the essentials of Apache Kafka, a modern distributed messaging platform for real-time workloads. A key takeaway from the talk included using Kafka for enabling a reactive data ecosystem that can deliver in-stream functions on the data including transformation, data quality, and filtering. How Kafka and Modern Databases Benefit Apps and Analytics by Neil Dahlke. Our very own solutions architect Neil, spoke to a standing-room-only crowd about the advantages of pairing Kafka with SingleStore for the ultimate analytics platform. Neil explained the architecture of SingleStore and why it’s perfectly suited to ingest, persist, and analyze data from Kafka topics. Note: A replay of the popular talk is available here . Using AI/Machine Learning With Your Data Warehouse by Henry Cook. This was an extremely rich and practical session on how to leverage advanced analytic techniques including AI/ML functions with your data warehouse. Henry covered a number of analytic functions including linear regression, association or basket analysis, text analysis , and more. An exciting new innovation is the use of ML to automate data transformation and classification including time series, gap filling, and image processing . Five Ways Database Modernization Simplifies Your Data Life by Mike Boyarski. In my talk, the goal was to describe some of the latest challenges faced by database professionals and how modern techniques can be used to solve them. The talk highlighted five specific database limitations with real-world customer examples for each. The limits included: speeding up the event to insight process, improving concurrency, delivering cost-effective performance, accelerating big data, and addressing deployment flexibility. The session slides can be viewed here . While the topics ranged from cloud platform selection, new DevOps models, ML/AI, IoT architectures and more, one thing remained steady throughout, the pace of innovation continues to accelerate and the vendors available to help remain vast. This suggests shows such as Gartner Catalyst will continue to provide help and guidance on what tools to use for organizations trying to innovate their technology strategy.", "date": "2018-08-27"},
{"website": "Single-Store", "title": "webinar-how-kafka-and-modern-databases-benefit-apps-and-analytics", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/webinar-how-kafka-and-modern-databases-benefit-apps-and-analytics/", "abstract": "Apache Kafka is widely used to transmit and store messages between applications. Kafka is fast and scalable, like SingleStore. SingleStore and Kafka work well together. You can find out more, and see a demo, in our recent webinar . In the webinar, Senior Sales Engineer Neil Dahlke of SingleStore explains how Apache Kafka and SingleStore can be used together to help reduce latency. Latency has three main causes: Slow data loading – due to database single-node architectures and locking mechanisms, it can take many minutes – even hours – to load datasets into the system. Kafka and SingleStore both address this problem. Lengthy query execution – as data sets grow to millions and even billions of rows, queries take a lot of time to construct, optimize, and run. SingleStore executes queries quickly. Limited user access – single-node databases can only handle a small number of users at a time. SingleStore is distributed and has high concurrency, allowing many more active users and apps to run against your database. The solutions to performance problems often involve expense, complexity, and workarounds: Scaling up . Adding more CPUs, adding more memory, adding caching tiers, buying specialized hardware racks, or paying for expensive add-on options to existing databases. NoSQL solutions . An object store can improve the problem of slow data loading, but at a big cost on the back (analytics) end: slow analytics, developer-intensive queries, and loss of compatibility with business intelligence tools. These solutions can generate more problems – additional layers, components to manage, and tough trade-offs – all at additional cost. SingleStore addresses all these problems. As the name implies, SingleStore was first introduced as an in-memory database. Today, it flexibly uses memory and disk for optimal performance. SingleStore is also referred to as a NewSQL database, or modern database, because it offers a familiar SQL interface along with distributed capabilities for fast ingest and high concurrency, and other desirable features. SingleStore is fast for loading; fast for queries; and scales for access by large numbers of concurrent users. And that comes with a familiar interface: the secret sauce of SingleStore is that it offers distributed SQL for both transactions and analytics. SingleStore is a NewSQL database, rather than a NoSQL database The ability of SingleStore to run combined operations quickly makes it perfect for what are called “translytical” applications, combining a steady diet of transactions with robust performance for always-on analytics against current, not stale, data. Put another way, SingleStore gives you HOAP: hybrid operational and analytical processing. (View our webinar on HOAP databases and data management .) SingleStore plays nice with all the elements of your existing ecosystem: it runs on-premise on bare metal; is cloud native for public, private, and hybrid clouds; and runs well in virtual machines and containers. SingleStore takes in real-time data and data from messaging and transform components, such as Kafka (the topic of this webinar) or Spark. And it can take in data from other data stores, such as relational databases, HDFS, or Amazon S3. You can then access your data from your own business intelligence dashboards, such as Looker, Microstrategy, and Tableau, and many other applications. SingleStore serves as the fast, modern database at the core of an ecosystem of data tools In the webinar, you’ll see a demo of a SingleStore data ingestion tool called CREATE PIPELINE. With a pipeline, you can do fast, reliable stream ingestion or batch loading. CREATE PIPELINE supports arbitrary transformations and works with any computer language, with the Go language and Python being popular with users. SingleStore can work across multiple Kafka brokers. A second demo in the webinar shows SingleStore tied to Kafka inputs to do sentiment analysis on tweets in a basketball-related Twitter feed. The webinar ends with a Q&A, covering topics such as the need for star schemas, the ability to ingest data from Oracle directly, and how to support high availability on AWS. Webinar Recording and Slides You can easily access the webinar recording and slides to learn more about SingleStore. Free Trial We also offer a free trial of SingleStore for 30 days. Download the free trial today.", "date": "2018-10-09"},
{"website": "Single-Store", "title": "three-ways-hoap-databases-modernize-data-management", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/three-ways-hoap-databases-modernize-data-management/", "abstract": "HOAP databases combine transaction processing and analytics processing in a single fast, responsive, SQL-compliant database. HOAP databases, such as SingleStore, greatly simplify your data architecture, while offering improvements in both performance and price. SingleStore is an outstanding example of a modern HOAP database, with a rapidly growing customer base, and is already used for critical processing in high tech, media, finance, and government, among other sectors. SingleStore is also simply a very fast relational database, maintaining the highest levels of performance, and cost-effective at enterprise scale. To learn more, view our webinar , or read on – and then view the webinar . The Growth of HOAP for Systems of Engagement HOAP databases are a new class of databases that promise to revolutionize how data is gathered, managed, analyzed, and used. These databases combine transaction processing and analytics processing in real time, enabling a series of workloads – including operational analytics, fraud detection, telemetry of systems or Internet of Things sensors, and real-time analytics powering dashboards – on currently updated data. HOAP databases generally support the SQL format – which means that they’re relational databases, unlike NoSQL stores for unstructured data – and thereby support business intelligence tools and user queries that require SQL. Names for this group of databases include hybrid operational and analytics databases (HOAP), hybrid transactional and analytics databases (HTAP), and translytical databases (for “transactional plus analytical”). Most NewSQL databases fit in the HOAP, HTAP, and translytical categories as well. In the webinar, Matt Aslett, Research VP at 451 Research , shows that digital transformation is happening now. In a 2018 survey, 40 percent of respondents are actively digitizing their business process and/or assets, within a formal strategy. 47 percent are considering or evaluating developing a strategy, and only 14% are making no effort. (Numbers do not add up to 100 percent due to rounding.) Companies leading in digital transformation are ahead in key technologydrivers such as personalization, machine learning and AI Leaders in digital transformation show more adoption of machine learning and AI, more use of cloud resources, and greater investments in intelligent personalization of their online offerings – a key use of HOAP databases. HOAP databases deliver high performance while reducing the cost and complexity of the analytics services that drive personalization. To do this, HOAP databases must provide simplicity, performance (combining transactions and analytics responsiveness with sub-second response time), and cost-effectiveness. Often, a HOAP database is able to deliver equal or faster performance than an analytics-focused OLAP database, on commodity hardware, and at a cost that is less expensive than a transaction-oriented OLTP database. In this way, HOAP databases provide the best of both worlds, OLTP and OLAP, but faster and cheaper than either. Investment is shifting to new systems of engagement solutions, which drive personalization, and more responsive services that are difficult to deliver with mature system of record databases. HOAP databases are currently a small, but growing share of the overall database market, projected to grow from around 10 percent of incremental revenue today to more than 30 percent in five years. How SingleStore Helps Drive the Growth of HOAP Mike Boyarski , senior director of product marketing at SingleStore, describes the advantages of SingleStore, a charter member of the HOAP camp: SingleStore is extremely performant, with responsiveness in the area of 10x over legacy alternatives; simple, with an easy to use and easy to access SQL architecture; and cost-effective, delivering unmatched responsiveness for a third or less the cost of alternatives. SingleStore achieves these benefits through a converged architecture, which combines the distributed nature of NoSQL solutions with the benefits of a relational SQL architecture. With this architecture, SingleStore can combine the high-speed ingest of a NoSQL database and the real-time analytics of an OLAP database, but against updated, not stale, structured data. SingleStore was originally introduced as an in-memory database, but now supports in-memory processing for rowstore tables and disk-based storage for columnstore. A single SingleStore database can replace key value stores, expensive in-memory caching, a data warehouse, an operational database, and more. A top five global bank uses SingleStore for several crucial use cases Use cases for SingleStore include fraud detection in real time; ad personalization; ad and streaming media analytics; real-time analytics for dashboards; and simultaneous data ingest and analytics for the Internet of Things (IoT). SingleStore is also a strong alternative when a traditional OLTP-ETL-OLAP processing chain is overwhelmed. Traditional architectures are seeing increasing volumes of incoming data on the online transaction processing (OLTP) side; increasing queries on the online analytics processing (OLAP) side; and an overburdened extract, transform, and load (ETL) process to move and condition data from OLTP into OLAP. In these use cases, the change in functionality and performance from making the relatively easy move to SingleStore, collapsing many cumbersome processes onto one powerful database, can be simply revolutionary. Webinar Recording and Slides You can easily access the webinar recording and slides to learn more about SingleStore. You can also download the research paper that complements the webinar, written by 451 Research . Free Trial We also offer a free trial of SingleStore for 30 days. Download the free trial today .", "date": "2018-10-18"},
{"website": "Single-Store", "title": "five-reasons-to-switch-from-oracle-to-memsql-webinar", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/five-reasons-to-switch-from-oracle-to-memsql-webinar/", "abstract": "In a recent webinar , we shared a detailed description of SingleStore’s strengths, Oracle’s weaknesses, customer case studies, and key use cases where the SingleStore advantage shines. The demands on databases are increasing – and legacy databases are struggling to keep up. Outdated architectures built around a single core process can’t take in data in large volumes quickly, nor respond adequately to ever-rising levels of queries – let alone handle both at the same time. SingleStore, featuring a modern architecture, scales out to support true multiprocessing on a pool of machines, in the cloud and on-premises equally, in virtual machines and containers. This makes SingleStore responsive to modern demands such as real-time analytics on frequently updated data, IoT applications, machine learning, and AI. Customers use SingleStore for data-intensive applications in a wide variety of industries, including financial services, media and communications, healthcare, and many others. Demands on Databases Are Rising Databases are ever more important to businesses and as a result the types of databases and options for databases have exploded. Organizations need to move past reporting and optimizing their business with operational data and must quickly respond and predict using new innovative techniques to accelerate business performance. Legacy databases struggle with new demands. They’re hard to scale out, expensive, and complex. They lack interoperability, full citizenship in the cloud, and DevOps friendliness. Decision-makers need faster andfaster insights from more and more data. SingleStore, on the other hand, is rightly described as “The No-Limits Database”. SingleStore supports ANSI SQL; is optimized for both structured and semi-structured data; and offers very high performance, especially through the ability to scale out a single database across multiple machines. SingleStore was built with a modern architecture to run well on both on-premises hardware and cloud infrastructure. Most importantly, SingleStore offers breakthrough levels of performance at a lower price than competitors, giving SingleStore a very strong story in price-performance as well. One representative customer achieved 10x the performance of Oracle at one-third the cost. SingleStore Out-Performs Oracle at Real Customer Sites In the webinar, the hosts dive deep into a comparison between SingleStore, with its high performance and modern architecture, and Oracle, with limited performance due to its legacy architecture. Oracle often struggles when price-performance is considered – due to Oracle’s high prices and the company’s tendency to sell expensive add-ons that incrementally improve performance, while adding complexity for installation, management, and maintenance. With SingleStore, Akamai achieved a price-performance improvementof two orders of magnitude over Oracle Exadata The hosts also introduce case studies with both named customers, such as Akamai and Verizon, and anonymized customers in energy and healthcare. These customers implement solutions in areas such as real-time financial reporting – in one case, with ultra wealthy clients who expect near-zero latency at all time – and operational analytics. Webinar Recording and Slides You can easily access the webinar recording and slides to learn more about SingleStore. Free Trial We also offer a free trial of SingleStore for 30 days. Download the free trial today .", "date": "2018-10-31"},
{"website": "Single-Store", "title": "performance-for-memsql-67", "author": ["Eric Hanson"], "link": "https://www.singlestore.com/blog/performance-for-memsql-67/", "abstract": "Performance is in SingleStore’s DNA. Last March, we shattered the trillion-rows-per-second scan barrier for processing a single SQL query on industry-standard Intel servers. That query processed data from a single table, and of course, lots of analytical queries use multiple tables, particularly star schemas. So we’ve broadened our vectorized, single instruction, multiple data (SIMD) query execution technology beyond single-table, group-by aggregate queries to star join group-by aggregate queries. A star join is a common kind of query that operates on what’s known as a star schema, used in most data warehouses and data marts, and some operational analytics applications as well. In a star schema, a single large table called a fact table is linked to several smaller tables called dimension tables. Star join queries typically join these tables together, aggregate numeric “measure” columns from the fact table, and group by descriptive fields from the dimension tables. Our star join performance was already excellent. This improvement makes it ludicrously good. In addition to star join performance improvements, we’ve made many other improvements to query and load speed, as I’ll describe in this blog post. When combined with SingleStore’s existing high performance, what this improvement means for application developers building real-time analytics systems, data warehouses, and data marts is that they can use SingleStore to get stunning performance at concurrency levels they couldn’t have dreamed of before. And they can do it with a database that supports mixed workloads and runs ANSI SQL. Star Join Performance Improvements We’ve added proprietary, patent-pending new algorithms for star join that make use of vectorization and SIMD. These algorithms operate directly on our compressed columnstore data formats, which we call encoded data. Instead of doing a hash join in the traditional way, where each row of the “probe-side” table is used to do a function call to search into a hash table created from the “build-side” table, we now have a special implementation of hash join that doesn’t do function calls in the inner loop. Instead, it uses generated, templatized code to process part of the work for multiple probes at once in a single SIMD instruction, operating directly on encoded data. To demonstrate this, I created a basic star schema data set. Since we have quite a few customers in the media market, I made a media-oriented example with one fact table and three dimensions, like so: Table Number of rows Description media_view_fact 1,310,720,000 Fact table describing a view event for a media object (e.g., a web page) date_dim 2,556 One row with descriptive properties for each day for 7 years user_dim 1,000 One row for each system user item_dim 10,000 One row for each media item being tracked I created a basic, but realistic, star join query that forces processing of every single row from the fact table, does a join, and groups by columns from a dimension, as follows: select d_daynuminweek, d_dayofweek, count(*) as c\nfrom media_view_fact f, date_dim d\nwhere f_datekey = d_datekey\ngroup by 1, 2\norder by 1 asc; The hardware I used was a single Intel Skylake server with two 2.6Ghz processors and 56 total cores. Data was partitioned evenly with one partition per core. I configured the system with one SingleStore aggregator node and two SingleStore leaf nodes, one leaf for each processor. The system had non-uniform memory access (NUMA) enabled and each leaf was on a separate NUMA node. I ran this query twice – once with, and once without, the new encoded join capability enabled. The results are summarized below: Encoded joins enabled? Average runtime 30 runs no 3.01s yes 0.0297s speedup (times) 101 No, this is not a typo. This is a 101 times speedup! The data is not pre-aggregated. The speedup is due to operating directly on encoded data, using SIMD and the enhanced join algorithm. What does this mean? Now, all existing applications can get far faster response times and concurrent throughput. Even more exciting is that you can create new applications that allow interactive analytics on large data sets with rapidly changing data, without resorting to the complexity of pre-aggregating data. More complex queries also show substantial speedups. For example, here’s a four-way star join with filters. select d_dayofweek, count(*)\nfrom media_view_fact, date_dim, user_dim, item_dim\nwhere f_datekey = d_datekey\nand f_userkey = u_userkey\nand f_itemkey = i_itemkey\nand i_itemuri regexp 'http://www.c000[0-5].*'\nand u_zipcode like '002%' and\nd_year = 1997 and d_month = \"January\"\ngroup by d_dayofweek; The results are as follows: Encoded joins enabled? Runtime no 0.09s yes 0.03s speedup (times) 3 The fact table is sorted (keyed) by f_datekey, and we’ve supported range (segment) elimination via join since our 6.0 release. So the date range filter implied by this join requires us to only read one year of data, not all seven years. And the filter effects of the join are employed during the scan of the fact table both with and without encoded joins enabled, via Bloom filters or a related approach we use in SingleStore DB 6.7. So the speedup is not as dramatic as the 101X speedup for the previous query. But a 3X speedup, on top of SingleStore’s generally high performance, is amazing nevertheless! Star Schema Benchmark Release-to-Release Gains The well-known star schema benchmark (SSB) was already performing very well in SingleStore DB 6.5, mainly because the joins in the queries in SSB are highly selective. That means that most rows in the fact table are filtered out by joins with the dimension tables. So vectorized testing of Bloom filters, available in 6.5, worked well to speed up these queries. Still, the new star join query execution technology in 6.7 has further improved this workload. The hardware for this test used four AWS m4.2xlarge leaf nodes (8 logical cores, 32GB RAM each). The primary fact table has 600 million rows. There are no precomputed aggregates. Query results are computed from scratch. This chart illustrates the gains: Notice that the most expensive queries, q3_1a and q4_1a, speeded up the most. And now, all the queries run in less than one second — truly interactive. What do I have to do to get this speedup? To get this speedup, join fact tables to dimension tables on integer columns and use group-by and aggregate operations in your queries. Using a star or snowflake schema with integer surrogate keys is a best practice and has been taught for years as part of the Kimball methodology, the most popular schema approach for data warehouses, data marts, and business intelligence systems. If you use this methodology, or something close to it, you’ll naturally benefit from the star join performance enhancements in SingleStore DB 6.7. As the number of rows that qualify from a dimension table in your query increase, the time to process each fact table row will increase, due to the need to do random memory access into a larger and larger hash table as part of the hash join operation. Eventually this may scale beyond cache memory size, which will result in more time spent per row. Best performance will be achieved for smaller sets of rows on the build side (tens of thousands or less). Additional Performance Improvements SingleStore DB 6.7 has a number of great performance improvements beyond this star join improvement, including: Just-in-time compilation of queries (experimental) IN-list performance improvement Resource governor enhancements for CPU and concurrency Columnstore range filter improvements Intra-cluster network compression for load data Fast sampled rowstore scans Optimizer and statistics improvements I elaborate on these additional improvements below. Just-in-time compilation of queries SingleStore compiles queries to machine code, which allows us to get amazing performance, particularly when querying our in-memory rowstore tables. By spending a bit more time compiling than most databases – which interpret queries, rather than compiling them – we get high performance during execution. This works great for repetitive query workloads. But our customers have been asking for better performance the first time a query is run, which is especially applicable for ad hoc workloads. In SingleStore DB 6.5, we introduced an undocumented feature that you enable with “set interpreter_mode = interpret_first”. In 6.5, this caused the first run of a query to be interpreted rather than compiled. This works well unless the first run of the query touches a lot of data, in which case, there is a noticeable loss of performance compared with just compiling and running the first time. In SingleStore DB 6.7, interpret_first has been improved to essentially be a just-in-time (JIT) compiler. JIT is an advanced compiler technique that compiles code once interpretation has been done enough times. We compile each operator of the query in the background, and switch to compiled mode from interpreted mode dynamically, while the first execution of the query is running. This can speed up the first run of a large and complex query (say with more than seven joins) several times by reducing compile overhead, with no loss of performance on longer-running queries for their first run. The interpret_first setting of interpreter_mode is still experimental. The default is llvm (compiled). Users can start using it for new application development, as we expect to make it fully supported for production next year. Let us know if you have feedback on it. IN-list performance improvement Queries with IN-lists are quite common in analytical applications. We’ve made them faster in SingleStore DB 6.7 in a couple of ways: Aggregators only send queries to leaves that hold the partitions responsible for the IN-list elements IN conditions are tested with a hash table, to allow fast qualification of whether a row is in, or not in, the list The first item primarily improves queries with smaller IN lists. Resource governor enhancements for CPU and concurrency In SingleStore DB 6.7, we extended our resource governor (RG) to allow you to limit use of CPU by resource pools, and limit the number of queries concurrently executing in a pool. This will not speed up an individual query on a lightly loaded system, but it can improve overall performance on a busy system for high-priority work items, and improve overall throughput. This requires correctly configuring RG for the characteristics of your application. For example, you can set the SOFT_CPU_LIMIT_PERCENTAGE for your pool named “high_priority” to be 30 percent. This guarantees that work in the high-priority pool always can have at least 30 percent of the CPU when the system is heavily loaded. It’s a soft limit, but if other pools are using more than 70 percent total, when needed, the CPU can be taken away from them for the high_priority pool in a fraction of a second. In addition, if you have a well-known gigantic query, you can put it in its own pool and limit the concurrency of that pool to 1, so only a single instance of it can run at a time. This can avoid thrashing and out-of-memory errors, while letting smaller reports run without waiting. Columnstore filter improvements Range filters on integer-encoded columns in columnstore tables are now faster. Range filters are those filters that use <, <=, >, >=, or BETWEEN. In SingleStore DB 6.7, these are implemented using Intel AVX2 SIMD instructions that operate directly on encoded data. Simple tests show speedups in the range of roughly three to six times for integer range filters. In addition, filter operations on integer run-length-encoded columns have speeded up by about a factor of two compared with SingleStore DB 6.5. This was achieved with low-level performance tuning, continuing to use SIMD. Intra-cluster network compression for load data During loading with the standard SingleStore loader, the LOAD DATA command, data needs to be copied from one node to another, which can make the cluster network the limiting factor in load speed. As of SingleStore DB 6.7, we sense when there is a network delay (based on the number of outstanding requests to send data from one node to another). If there is, then we switch to use data compression on the stream of data flowing between nodes. See our blog post on the development of this feature. This is fully automatic and on by default. As a result, some LOAD DATA commands will more than double in speed! This will be most visible when CPU is more plentiful than network bandwidth in your cluster. Fast sampled rowstore scans A new SingleStore DB 6.7 feature, which was introduced to help the query optimization process but can help with query execution as well, is the sampled rowstore scan. This uses the randomized nature of the SingleStore in-memory skiplist that is used for rowstore tables. It allows you to sample a percentage, p, of the rows of a table, t, in an amount of time that’s directly proportional to the size of the sample. And this is a high-quality, row-level random sample where individual rows are independently selected (a Bernoulli sample). Repeated samples at the same sampling ratio may retrieve the same or nearly the same set of rows, however. Legacy database systems sometimes support sampling, but typically either they support random samples of pages (which is not truly random, due to clustering) or they require a full scan of a table to compute a row-level sample (which can be slow). SingleStore samples are both high quality and fast. A user with statistical savvy could use this feature to compute averages with, say, a 99 percent confidence interval, very fast, on a large and rapidly-changing data set in a transaction processing application. As an example, for a table, f, that contains 100 million rows, on the hardware described earlier, this query takes on average about 74 milliseconds: select avg(duration_s) from f; And this query takes on average about 2.2 msec: select avg(duration_s) from f with (sample_ratio = 0.00001); Yet the sampled version produces results with accuracy within 1 percent of the full scan version for random values of duration_s in the range of 0 to 120. Optimizer and Statistics Improvements The query optimizer doesn’t make queries execute faster, but a good query optimizer of course can improve performance of your application overall by choosing good query plans for queries. We are in the process of improving the query optimizer and statistics subsystem of SingleStore over multiple releases, to make it easy to get great query plans with little or no tuning required. Rowstore random samples In SingleStore DB 6.7, we have updated our sampling strategy for rowstore tables to use a true, row-level random sample, as described above, rather than read an entire single partition, as was done in SingleStore DB 6.5 and earlier. This has two benefits: The sample can run faster because less data has to be sampled The sample is a uniform, random one, so it is high quality and not subject to skew A cardinality estimate is an estimate produced by the optimizer of the number of rows that are produced by a query plan operator. There are a number of trouble spots that make it hard for the optimizer to get good cardinality estimates with just statistics. Trouble spots include: Complex predicates, such as myUDF(column_name) = 1 Correlation, such as the famous problem, make = 'Honda' and model = 'Accord'\" Situations where there are no statistics So, when your queries contain complex predicates, correlated filters, or when statistics are not available, you will still be able to get good estimates of cardinality based on a sample when using rowstores. Better IN-list filter estimation SingleStore DB 6.7 now can use advanced histograms, introduced in 6.5, to more accurately estimate the selectivity of IN conditions. Query optimization performance We’ve made a several improvements to query optimization performance (i.e., the amount of time it takes to generate a query plan tree from SQL input). These are related to metadata loading performance and internal caching techniques to reduce optimizer run time. Optimization time for some customer queries has improved up to sixfold. New information_schema views for tuning Tuning your physical database design well can give you big performance gains across your workload. With SingleStore, you have to choose indexes (rowstore vs. columnar), table types, and make decisions about sharding. In SingleStore DB 6.7, we’ve introduced new management views, in information_schema, to help you make these decisions. The first is mv_query_column_usage . This view helps you understand which queries are using different columns in your database and how. It has these columns to help you know which queries are using a column, whether the column is used in joins, filters, group-bys, order-bys, reshuffles, or is output to the client application. Based on the understanding of the frequency and cost of use of columns for different purposes, you can make better decisions about where to create indexes and how to shard your data. The management view mv_aggregated_column_usage provides a summary of mv_query_column_usage , aggregating over the columns without providing detail for individual queries. Query plan warnings Application developers sometimes introduce simple things in their queries that might affect performance in an unanticipated way, such as comparing an integer column to a string or vice versa unintentionally. SingleStore DB 6.7 now warns you about these situations in EXPLAIN plan output, as well as in a new information_schema.mv_queries view. You can find all queries with warnings like this: select * from information_schema.mv_queries where PLAN_WARNINGS != \"\" Statistics views Statistics are fundamental information used by the optimizer to help it find the best-performing query plan. We’ve added or changed these information_schema views to help you understand what statistics are available, how old they are, and whether potentially useful statistics are missing. View Description mv_query_prospective_histograms Shows what queries could have used histograms on a particular column but they weren’t available optimizer_statistics Shows what statistics are available for each column that has any kind of statistics, and when they were last updated Summary SingleStore DB 6.7 has been a short release focused on usability. Nevertheless, we’ve delivered amazing performance gains in the database engine, especially for star joins. If you have an analytical application and are joining on integer keys then grouping and aggregating, you may see stunning speed improvements, over 100X in some cases. And we’ve continued to improve other areas including the speed of IN-list filters, columnstore integer column filters, and LOAD DATA. Finally, general improvements to our optimizer and statistics/sampling framework, as well as maturation of our resource governor to handle CPU and concurrency limits, make it easier to get great performance and maintain it over time with SingleStore DB 6.7. Try SingleStore DB 6.7 today to see what gains you can get for your workload! References Shattering the Trillion-Rows-Per-Second Barrier With SingleStore (SingleStore blog post). Kimball, Ralph, The Data Warehouse Lifecycle Toolkit , 2nd Edition, 2008. Note: this is a great reference on dimensional modeling (the star schema approach). For more information on SingleStore DB 6.7, see the press release and all eight blog posts on the new version: Product launch ; managing and monitoring SingleStore DB 6.7 (includes SingleStore Studio); the new free tier ; performance improvements (this post); the Areeba case study ; the Wag! case study ; the creation of the Visual Explain feature ; and how we developed adaptive compression .", "date": "2018-11-06"},
{"website": "Single-Store", "title": "creating-visual-explain", "author": ["Brian Chen"], "link": "https://www.singlestore.com/blog/creating-visual-explain/", "abstract": "SingleStore has a long tradition of sharing details about its efforts to optimize performance in order to help customers get the most out of our products and contribute to our efforts to optimize performance further. This blog post, from recent SingleStore intern Brian Chen, continues that tradition. I interned at SingleStore last summer on the team that developed SingleStore Studio , a new product which provides a friendly interface for monitoring and managing SingleStore clusters. The main focus of my work was Visual Explain, a tool that helps customers optimize their database schema for maximum performance, and that also helps SingleStore engineers discover bottlenecks and improve the operations of the core SingleStore engine. In this blog post, I’ll explain how and why Visual Explain was developed and how we worked to optimize information display. I’ll also share the key resources we drew on to help us make development decisions. How We Worked The team that developed SingleStore Studio is quite small. There were two engineers, myself and SingleStore software engineer David Gomes, and two designers. Our team lead, Carl Sverre, and other members of the platform team stepped in regularly to review code. The upside of the small team size was how closely we could work together. For most Studio features, the designers would first make mockups, then present them to the rest of the platform team during a design sync meeting, which happened twice a week. All of us could provide feedback and discuss alternatives; I learned a lot about design through these syncs. When a design was finalized, Gomes and I would implement it, occasionally making some judgment calls about consistency or the workings of animations that weren’t fully specified in the design. I was also able to shadow some user tests, where we recruited outside participants to try using the functionality we created, to help us discover usability problems. The Studio feature I worked on the most, Visual Explain, is needed as part of the bread and butter of what we do at SingleStore, which is making queries run fast . To make a query faster, it’s important to know what operations the database engine is performing behind the scenes to execute it. Our customers can use this information to figure out whether their database schema is optimized for the query, and improve it for better performance; our database engineers can use it to discover bottlenecks and improve the engine’s operations. Like other database engines, SingleStore provides this information as the output of EXPLAIN or PROFILE commands. Unfortunately, the output of these commands is a giant blob of text, with lines that are hundreds of characters long, decorated with a tiny dash of ASCII art — not exactly easy to read. So we developed Visual Explain as a tool for displaying the output of the EXPLAIN or PROFILE commands graphically, in a way that humans can interact with and understand. Starting Visual Explain The concept of “prettyprinting” the operations of a database engine certainly isn’t original. For example, Alex Tatiyants’s Postgres EXPLAIN Visualizer (Pev) is a similar tool for Postgres queries. Pev influenced the design of our tool a lot. At SingleStore, we’ve had an internal tool called PlanViewer, which is also a past intern project. Our goal with Visual Explain was to build something that was similar to these existing tools, but that would be further optimized, and that would fit in with the aesthetics and functionality of the rest of SingleStore Studio. We also wanted Visual Explain to be able to handle very complex queries, as well as gracefully accommodate changes in the SingleStore engine’s EXPLAIN or OUTPUT format as much as possible. We started working on Visual Explain by writing and reviewing a detailed 20-page technical design document. Since Gomes works remotely from the rest of the Studio development team, in a different time zone, it was really important to have a design that allowed us to work on implementing different parts independently. To accommodate this, we split Visual Explain up into discrete components and carefully documented the interfaces between the components with Flow types. This process allowed us to quickly write placeholders for each part so that we could later replace each placeholder with a unit-testable implementation. The tool’s user interface and appearance also required a lot of thoughtful design, so the design team spent many days coming up with a complete mockup that included details for every node and interaction in Visual Explain. They designed the interaction down to the last icon and progress bar color, and we spent a lot of time discussing each decision and different aspects of the implementation during our design syncs. Creating the Layout Algorithm The component of Visual Explain that I spent the most time on was the layout algorithm. The structure of an EXPLAIN or PROFILE output is a tree of nodes, each node representing an operation. In Visual Explain, we wanted to display this tree graphically. The question seems simple: given a tree, how do you lay out its nodes in a 2D plane as to show the tree’s hierarchical structure in an intuitive and aesthetically pleasing way? This turns out to be a problem with a rich history. It’s easy to draw a specific tree on a piece of paper in some way or another that looks nice. However, it’s not at all obvious how to objectively describe what a nice tree drawing looks like in general. It’s even less obvious to know how to write an algorithm to do this. Fortunately, there are quite a few papers that tackle the tree-drawing problem. One such paper, published in 1981 (more than 35 years ago!) but still widely cited, is Reingold and Tilford’s Tidier Drawings of Trees ( PDF ). In this paper, they propose four aesthetic criteria for drawing “tidy” binary trees, and suggest an algorithm for producing drawings of trees that satisfies these criteria while producing drawings that are reasonably narrow, for onscreen display and print. The criteria are: Nodes at the same level of the tree should lie along a straight line, and the straight lines defining each of the levels should be parallel. A left child should be positioned to the left of its parent and a right child to the right. A parent should be centered over its children. A tree and its mirror image should produce drawings that are reflections of one another; moreover, a subtree should be drawn the same way regardless of where it occurs in the tree. The problem and algorithm were generalized to trees of arbitrary degree, but still with ordered children, by Walker in 1989 ( PDF ), and Walker’s algorithm was improved to linear time by Buchheim et al. in 2002 ( PDF ). This is also the algorithm that the D3 layout editor uses to render trees, by the way. If you are looking for an overview of the subject, the introductory sections of the paper by Buchheim et al. are well-written descriptions of the earlier algorithms, their requirements, and their shortcomings. Bill Mill’s article, Drawing Presentable Trees , provides an even more complete history of this problem and solution attempts, with code, starting from Knuth (of course it’s Knuth) in 1971. Finally, I should mention Kennedy’s Drawing Trees (1996), which describes how to satisfy the aesthetic criteria functionally, using simple SML. Although the algorithm, as presented in the paper, is not linear time, it is nicer to work with. Note that all of these papers concern drawing rooted trees in which siblings are ordered. The problem of drawing arbitrary graphs in aesthetically pleasing ways has a different history altogether. A standard algorithm for this problem is given by Gansner et al. (1993) ( PDF ), which is used by graphviz ’s dot drawing-output tool and the JavaScript library dagre , among others. Fine-Tuning the Implementation However, it turns out that a lot of the aesthetics aren’t directly applicable to our use case. Rules 2 and 4 are unobjectionable. Rule 1 is broken by Pev’s layout, and we also chose to violate this rule. The reason is that, for our use case, it usually isn’t meaningful to know if two nodes are on the same level if they aren’t direct siblings. Nodes in the output of EXPLAIN and PROFILE and their relationships with their children can be drastically different. What’s more, some of our nodes have different heights: for table scan nodes, we add the name of the table on the bottom of the node, making it taller. For us, enforcing that nodes on the same level should be vertically aligned could actually suggest a misleading uniformity among those nodes. Pev also breaks Rule 3 by centering a node over the bounding box of the subtree underneath it, as a natural consequence of its DOM structure. This aesthetic makes sense if you prefer to visualize nodes as the roots of the entire subtree underneath them, rather than the parents of their direct children. (Incidentally, given Pev’s other choices, this is equivalent to being centered at the weighted average of the children, where each child is weighted by the number of leaf nodes it has as descendants. This suggests some ways to interpolate between this strategy and Rule 3 — for instance, you could take the average of the children weighted by the square root of the number of leaf nodes under each child.) The preference for narrow drawings is also generally applicable, but we have to trade it off against clarity. We have a lot of information we want to display in our nodes. As a result, we can’t devote as much space to drawing the edges. So, instead of drawing edges as straight lines directly from node to node, we draw horizontal and vertical connecting lines like Pev. (By comparison, in the drawings in Reingold and Tilford’s paper, adjacent nodes are so far apart that you could fit additional nodes between them.) The downside of our approach is that edges, or their absence, become easier to miss compared to the detailed information in nodes. So to avoid confusion, we may not want to lay out nodes too close to other nodes they aren’t connected to. Therefore, we heuristically add additional whitespace between some subtrees, albeit at the cost of making our trees wider. Here’s an example of a subtree with horizontal and vertical connecting lines laid out more or less directly following Walker’s algorithm: The ColumnStoreScan, Broadcast, and BloomFilter nodes on the fourth row are aligned, but they aren’t strongly related. They just all happen to be two nodes away from the root of this subtree, but those two nodes are totally different between the ColumnStoreScan and the other two nodes. In addition, the HashJoin is centered over its children rather than the subtree. Here’s the Visual Explain layout as I ultimately implemented it. We’ve made the brackets between a parent and its multiple children taller than the line between a parent and its only child, and added space between the left and right subtrees, which separates the ColumnStoreScan from the Broadcast and BloomFilter that used to be on the fourth row vertically and horizontally. We’ve also centered the HashJoin over the subtree rather than its children. I think that the method you prefer for centering a parent over its children is really a matter of personal taste, but the separation between subtrees helps clarify the structure of the tree on a quick glance. Note that Pev draws a tree essentially by arranging the subtrees in a row, so that their bounding boxes do not overlap. While this layout algorithm has the virtue of being extremely simple (it is actually a natural consequence of the way Pev’s DOM is laid out), it often creates drawings that are significantly wider than necessary. Here’s an example of what Visual Explain output would look like if we specified that subtrees’ bounding boxes should not overlap: And here is the same area of this tree laid out using, roughly, Walker’s algorithm, allowing us to fit more nodes on screen: In the particular PROFILE output that is partially depicted above, which has 317 nodes, shifting the subtrees together made the diagram narrower by about 20 percent. There is still room for tweaking here; perhaps, for instance, it is still better to keep a wider horizontal or vertical gap between faraway subtrees for clarity. Having said all that, the ultimate layout algorithm I wrote still very much uses the general strategy of Walker’s algorithm. Namely, when drawing a tree, it draws each subtree underneath the root independently, horizontally shifts the subtrees away from each other until there is enough space between every pair of subtrees, then positions the root above the subtrees. This strategy, and many of the optimizations proposed to use it, are just as applicable even if parents are not centered over their children, if nodes in different subtrees are not horizontally aligned with each other, and if we want to use more complex heuristics to decide how much space is “enough space” between subtrees. (My algorithm requires more space between subtrees that are wider; to enforce vertical space, it treats the bounding boxes of leaf nodes as taller than they actually are.) It also still uses Walker’s idea of evenly distributing additional space when shifting non-adjacent subtrees away from each other, which spreads out small subtrees nicely. The caveat to all this is that my layout algorithm is not linear time, but this turns out to be perfectly acceptable. A reasonable quadratic implementation can already handle thousands of nodes, and above that point, around the upper limit of reasonable EXPLAIN/PROFILE sizes, rendering the nodes in the DOM becomes just as important to optimize. Further optimizing the layout algorithm would make it harder to tweak the various heuristics I mentioned. Nevertheless, even with our differing aesthetic goals, nearly all of the optimizations proposed in the various cited papers could still be applied to make the algorithm linear in the number of nodes, if we find the need in the future. Visual Explain has now shipped, as part of SingleStore Studio , and I’m excited to let our engineers and customers use the tool to visualize the operations behind their queries. Brian Chen, the author of this blog post, is a senior at MIT and has interned at Dropbox. He worked on SingleStore Visual Explain, part of SingleStore Studio, this past summer. – Editor For more information on SingleStore DB 6.7, see the press release and all eight blog posts on the new version: Product launch ; managing and monitoring SingleStore DB 6.7 (includes SingleStore Studio); the new free tier ; performance improvements ; the Areeba case study ; the Wag! case study ; the creation of the Visual Explain feature (this post); and how we developed adaptive compression .", "date": "2018-11-06"},
{"website": "Single-Store", "title": "how-fanatics-powered-their-way-to-a-better-future", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/how-fanatics-powered-their-way-to-a-better-future/", "abstract": "Deriving insights from your data is a competitive advantage that can no longer be ignored. But many companies are finding it difficult to make the most of their data, because traditional data technology wasn’t built with such huge scale in mind. A traditional SQL-based database could handle almost any workload, but the workload had to fit on a single machine. If you needed to scale – to support real-time analytics, for instance – you had to buy into NoSQL solutions – a multitude of specialized components to tackle the rising demands on your data infrastructure. Here’s a representative data architecture. You might be dealing with a similarly complex data infrastructure at your company today. A representative corporate infrastructure for big data and analytics There’s a better way – and Fanatics can show all of us how one fast-growing, global, multi-billion dollar company did it. How Fanatics Grew into Complexity – and Beyond As the global leader in licensed sports merchandise, Fanatics is changing the way fans purchase their favorite team apparel and jerseys across retail channels through an innovative, tech-infused approach to making and selling fan gear in today’s on-demand culture. Fanatics has its own supersite, Fanatics.com , but also runs the online stores of all major North American sports leagues, more than 200 professional and collegiate teams, and several of the world’s largest global football (soccer) franchises. Even as it grows rapidly, Fanatics is profitable , with revenues exceeding $2B a year. Three years ago, Fanatics decided to completely revamp its technology architecture. It had to meet ever-growing user needs for responsiveness, reliability, and scalability as Fanatics added several leagues, teams, and franchises globally. The real-time nature of Fanatics’ innovative mobile and ecommerce platform allows it to react quickly to both planned and unplanned moments throughout the sports calendar, including Championships, player trades, and record-breaking performances, where fan demand for products quickly spikes. Fanatics moved from siloed and monolithic applications to a completely event-driven architecture. At the core of event-driven architecture is a Kafka-based messaging backbone called Fanflow. Fanflow moves user and system events toward fulfillment and analytics Fanflow uses Kafka to move events from online stores, mobile interactions, and point of sale systems through the payments backend and into the analytics system. Events include user behavior interaction on the site, type ahead suggestions, searches, product recommendations, etc. Events are also generated for all the state changes within an order life cycle, such as additions to cart, checkout, fulfillment, and delivery. At the same time, Fanatics had intensive analytics needs – needs that were not being met by the old way of doing things. Open Source Analytics Analytics is crucial to Fanatics’ business. The company has to work with partners to anticipate, respond to, and fulfill customer ordering peaks and valleys. Team wins and losses can generate hard-to-fulfill surges, or a team’s loss or a player trade could result in merchandise markdowns. As part of the move to Fanflow, Fanatics adopted an analytics approach that combined a few open source technologies running against cloud-based storage. Event information was stored in JSON, which caused difficulties when Fanatics later evaluated traditional SQL database solutions. The company used Flink and Redis for state management as it computed business KPIs and time-series metrics in real time. It created Spark applications for in-depth analytics, such as subscription/order attribution, activity, and clickstream models. Fanatics used Lucene-based indexers for its primary persistence layer, and for data discovery and simplified dashboarding. Before: The previous Fanflow analytics architectureused different tools for different audiences There was a split between the tools and functionality available to different audiences. Managers and executives used a Lucene-based index for queries and limited data discovery. Data scientists and data engineers used Hive and Zeppelin to analyze the same events at a deeper level. This caused challenges. Fanatics’ workflows were very complex and difficult to manage during peak traffic events, such as a championship game. Business needs evolve frequently at Fanatics, meaning schemas had to change to match but these changes were very difficult to manage with the Lucene-based indexers. Maintaining the different query platforms and the underlying analytics infrastructure cost Fanatics a lot of time to keep things running and meet SLAs. So the company decided on a new approach. Re-Engineering Analytics with SingleStore As a solution to its analytics issues, Fanatics decided to adopt SingleStore, which offers high performance on ingest and queries and a widely used SQL interface for queries and analytics applications. SingleStore combines in-memory and disk-based technology, including a columnstore engine. SingleStore also offers built-in interoperability with Apache Kafka and JSON, supporting previous Fanatics technology choices. SingleStore replaced the Lucene-based indexers. Spark and Flink jobs were converted to SQL-based processing, which allows for consistent, predictable development life cycles and more predictability in service level agreements (SLAs). After: Fanflow feeds into robust analytics capabilities powered by SingleStore The Fanatics event stream runs at an average of 2,500 events per second, or 4 million events per day, and the SingleStore database has grown to roughly 3 billion rows and counting. Yet users are still able to run ad hoc queries and scheduled reports, with excellent performance. With the previous mix of tools, Fanatics was not able to do data integration across enterprise sources. With SingleStore, Fanatics regained that capability. The company ingests all its enterprise sources into SingleStore, integrates the data, and gains a comprehensive view of the current state of the business. Fanatics was also able to unify its audiences onto a single, SQL-based platform. This sharply lowers the barriers to entry, as SQL is so widely known. Fanatics no longer has to set up different kinds of platforms for different audiences. The development team now spends a lot more time deriving deeper insights and developing new capabilities, such as self-service analytics platforms, washboarding, and others. The team also supports an increasing number of use cases, such as order visibility, which powers several customer service applications internally. The teams are able to spend more time providing deeper insights from the data rather than just keeping a plethora of platforms up and running. The Future for Fanatics A tech company at heart, Fanatics is remaining ahead of the curve in today’s lightning fast, mobile economy through significant investments in technology, data, on-demand manufacturing and global infrastructure. The company has reinvented the way high-quality fan gear and apparel is quickly designed, manufactured and delivered through its innovative vertical-commerce (v-commerce) model, built to satisfy the increasingly real-time expectations of fans and retailers worldwide with a wider assortment of high-quality products. Fanatics continues to grow rapidly, opening offices and manufacturing/distribution facilities in several of the world’s sports-crazed markets. Fanatics is backed by some of the world’s foremost investors, most recently having secured a $1B injection from SoftBank’s Vision Fund, the largest tech fund in history, which identified Fanatics as one the world’s transformative companies helping to shape the future of commerce. Executive chairman Michael Rubin has been named one of the Ten Influencers of 2018 by media site SportsPro. As the company grows, so will Fanatics’ commitment to continually improving its application architecture and analytics capabilities. Click to view last month’s Strata Data keynote , featuring SingleStore’s Drew Paroski and Aatif Din of Fanatics.", "date": "2018-10-22"},
{"website": "Single-Store", "title": "adaptive-compression-in-memsql", "author": ["Sarah Wooders"], "link": "https://www.singlestore.com/blog/adaptive-compression-in-memsql/", "abstract": "This summer, SingleStore summer intern Sarah Wooders contributed substantially to an important area of performance at SingleStore: adaptive data compression that responds optimally to network conditions. For more on the difference this makes, see the performance blog for SingleStore DB 6.7 . – Editor I’m a student at MIT majoring the computer science and mathematics. My current interests are in multicore and distributed systems, as well as performance engineering. My work became part of SingleStore DB 6.7. SingleStore Load Data In SingleStore, data is divided into partitions and stored on corresponding leaf nodes in the cluster. When a user loads a file into SingleStore, a specialized aggregator node does a minimal parse of the file. The aggregator determines the partition to which each rows belongs and then forwards rows to the corresponding leaves, where most operations are completed. In order to maximize performance, the database code should work around bottlenecks – any operation that runs relatively slowly compared to other operations that are part of a larger process. Often times, the bottleneck in the load pipeline is the network speed between the aggregator and the leaves, especially when parallel loads are being performed. With recent performance improvements throughout the data loading pipeline, including improved SIMD parsing on aggregators, we expect network bottlenecks to be increasingly common. When the network is the bottleneck, aggregator CPUs spend a lot of time idle as they wait for the network to free up for more packets to be sent. So, in order to work around the bottleneck, we utilize the CPUs by having them compress packets while they wait. By having the aggregator threads compress packets while they wait on the network, we utilize otherwise unused resources to make better use of the network. Adaptive Network Compression If a cluster network is overloaded, while the aggregator has a large amount of CPU power at its disposal, it makes sense to compress all outgoing packets. However, when there is a less extreme disparity between network bandwidth and aggregator CPU power, compressing every buffer can actually result in the compression step being the bottleneck. The aggregator should only compress packets when it is optimal to invest time compressing buffers instead of immediately trying to send. Thus, the primary challenge with using compression to reduce network congestion is identifying when to spend time compressing versus actually trying to send the packet. When we started, if an aggregator tried to send a packet but experienced a failure due to congestion, the aggregator would sleep, wake up, and re-try. So the first approach we tried was to have the aggregator compress packets whenever it was supposed to sleep. However, after implementing this approach, we found that the aggregator did not compress nearly as often as it should. Often times, the aggregator would start sleeping in the middle of sending a buffer. Partially sent buffers cannot be compressed, so the aggregator would rarely have an opportunity to compress packets, even with severe network bottlenecks. Before we describe our final design, let’s go into some more detail about how the aggregator works. The aggregator is simultaneously parsing data and sending packets to leaves. One thread on the aggregator does the parsing, while another thread sends packets to leaves. When the parsing thread reads a row from the loaded file, it determines which leaf partition the buffer belongs to and pushes it to a corresponding queue. The sending thread pops buffers from the same queue to send to leaves. If the network speed allows the sending thread to send buffers quickly enough to keep up with the inflow from the buffer queue, the queue will remain mostly empty. Any buffers added to the queue will immediately be popped. Thus, we decided how to determine when we have a network bottleneck by looking at the size of the queue. If the queue grows, we can conclude that the sending thread is falling behind the parsing thread, which probably means we have a network bottleneck. So now, when the sending thread looks at the queue and see that more than four buffers are queued, it recognizes this as a network bottleneck. Before attempting to send the buffers, the sending thread will compress all four of the buffers in the queue. We liked this approach a lot, not only because it was much better at immediately detecting network bottlenecks, but also because it ended up being just a couple of lines of code. Results We tested our new aggregator protocol using real customer data on an EC2 cluster with the following specs: 1 x R3.8xlarge aggregator (32 cores) 5 x R3.4xlarge leaves (16 cores) ~1 Gb/s Network 2.5GB input x 16 Parallel Loads = 40GB total, uncompressed 3x compression ratio using LZ4 We chose an aggregator with a large number of CPUs in order to try to saturate the 1GB/s network with 40GB of loaded data. We experimented with a full data load as well as a data load that skips the last stage of the load pipeline: the leaf table inserts. We chose to experiment with a load without leaf inserts as they were a bottleneck, because we were limited in the number of leaves we could include in our cluster. Description Without leaf inserts Full load With compression ~20s ~24s Without compression ~54s ~24s Speedup 2.7x 2.3x Average compression ratio 2.7x 2.9x Data ingest rate 2GB/s 1.67GB/s We see that network compression allows us to load data at 1.67GB/s, which is faster than network speed! The LZ4 compression ratio with our experiment data was 3x per packet, which means our speedup is bounded by 3x. The Level 3 load shows a speedup equal to the compression ratio, as the leaf insert time is excluded. The full load is also bottlenecked by the network; however, the full compression speedup cannot be achieved due the a bottleneck by the leaves. As a result, the full load has a higher compression ratio but lower speedup. We expect that with more leaf nodes available the speedup will approach 3x. Concluding Thoughts We expect a more dramatic speedup in clusters with weaker networks and more leaves. The speedup is bounded by the compression ratio, so in especially slow networks, LZ4 on a higher compression ratio setting can be used to increase the speedup. One issue is that our protocol has no way of differentiating whether aggregator packets are failing to send because of a network bottleneck or a leaf bottleneck. In our experiment, the full load had a higher compression ratio, most likely because the pipeline was also bottlenecked by the leaves in addition to the network. In the case where slow leaf inserts are the bottleneck, compressing packets may actually exacerbate the problem by requiring the leafs to perform decompression on the packets they receive. However, since LZ4 decompression is very fast (3900MB/s), we decided the performance impacts were negligible. Acknowledgements I’d like to thank Sasha Poldolsky and Joseph Victor for their mentorship throughout this project. For more information on SingleStore DB 6.7, see the press release and all eight blog posts on the new version: Product launch ; managing and monitoring SingleStore DB 6.7 (includes SingleStore Studio); the new free tier ; performance improvements ; the Areeba case study ; the Wag! case study ; the creation of the Visual Explain feature ; and how we developed adaptive compression (this post).", "date": "2018-11-06"},
{"website": "Single-Store", "title": "memsql-shows-strength-against-oracle-openworld-2018", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/memsql-shows-strength-against-oracle-openworld-2018/", "abstract": "Conventioneers took over part of San Francisco’s South of Market Area (SoMA) last week for Oracle OpenWorld 2018. SingleStore hosted a booth on the show floor and a party across the street from the main venue, cheekily dubbed “The Backup & Recovery Happy Hour”. We used the occasion to highlight three key takeaways for Oracle users: The ability of SingleStore, running on industry standard servers, to augment or replace existing installations of Exadata and Real Application Clusters (Oracle RAC), with SingleStore providing huge increases in performance and significant cost savings. The fact that SingleStore, due to its truly cloud-native architecture, delivers consistent performance across any deployment platform, including on-premises, public or private cloud, and in virtual machines or containers, without requiring specialized hardware or support as in Oracle Cloud. The way in which SingleStore’s modern architecture, combining transactions and analytics on a fast, scalable database platform, allows organizations to free themselves from complex, hard to manage and outdated data processing architectures with onerous Oracle contract restrictions. SingleStore Invited Attendees of Oracle OpenWorld 2018to the SingleStore Backup & Recovery Happy Hour In ironic contradistinction to the trade show name, Oracle strongly promoted Oracle Cloud, their closed ecosystem – and the only cloud program with optimized Oracle. They also announced an update to the Oracle Autonomous Database, which uses AI to help provide security and other manageability features. “We’ve added lots and lots of more robots to protect every aspect of the cloud,” said Larry Ellison , Oracle cofounder and CTO. (We think he meant “bots”, which are software, rather than a software/hardware combination.) To find out more about SingleStore’s advantages over Oracle and other traditional database platforms, sign up for our upcoming webinar, Five Reasons to Switch to SingleStore .", "date": "2018-10-30"},
{"website": "Single-Store", "title": "wag-labs-case-study", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/wag-labs-case-study/", "abstract": "Wag! has been called the “Uber for dogs”. The service matches dog owners with dog walkers, tracking and visualizing each walk in real time. The service is now available in more than 100 cities across America. Now, Wag! is using SingleStore to help them meet the demands of rapid growth. Dog owners start with a live map of dog walkers near them. They connect with a walker, set up the walk for their dog, and watch it proceed live, on a map of their neighborhood. Every walk ends with an activity report. The report features a photo of the dog, the time and distance for the walk, and more. The Wag! app is highly interactive and personalized at every step. Every single walk generates significant amounts of real-time data. Wag! needs to handle the traffic smoothly and be ready for spikes and future growth. App performance is key to business success at Wag!, so the technical challenges of scaling data ingest have serious business implications. JOINing Business and Technical Challenges Earlier this year, Wag!’s core database was reaching its maximum computing capacity. One of the main reasons was the large amount of writes that the main database had to handle. The writes also caused delays on database Replicas, which prevented Wag! from scaling read queries. “We didn’t have much head room for growth,” said Franck Leveneur, data service tech lead at Wag! “The goal was to move our ‘log type’ writes out of MySQL and to SingleStore, to free up resources. This move would diminish our lag, freeing up the MySQL database to handle read-only queries.” Leveneur was familiar with SingleStore and knew it was the best fit for Wag! based on several key capabilities: Drop-in solution . MySQL and SingleStore both support SQL, so there was no learning curve and implementation was quick. “SingleStore has done a great job in building a database that is very easy to use if you’re coming from the MySQL world. Our engineers had no issues using SingleStore; it felt natural,” said Leveneur. In-memory rowstore and on-disk columnstore support . Wag! performs initial processing in-memory, in rowstore format, with very high performance. They then use the columnstore engine to compress data up to 70 percent. SingleStore can then run aggregate queries much faster than MySQL, which is a huge win. Pipelines . SingleStore can plug directly into S3 and ingest data at amazing speeds. “Think backfilling a 700M row table in less than 20 minutes on a live SingleStore database in production,” said Leveneur. “I’ve dealt with large backfilling before; it’s challenging. SingleStore makes this process a walk in the park”. Support . Like other SingleStore customers, Wag! finds SingleStore’s support to be outstanding. “The team is always available to answer questions, to help us optimize queries, or to share Python code for implementing transformations. Their training material and documentation are also excellent.” The overall results have been excellent. “Today, after migrating all the needed tables from MySQL to SingleStore, we have reduced our average CPU utilization to the high teens,” said Leveneur. “We have not yet used other great SingleStore features such as geospatial data type, full-text search, and windowing yet, but we’re planning to explore them.” Next Steps for Wag! with SingleStore Although he wouldn’t say so himself, Leveneur is an expert in solving thorny problems, such as improving ingest performance. A few years ago, he gave a talk on ultra-fast data reporting with very large data sets at the Los Angeles MySQL Meetup group. He leads a data management course at UCLA and includes SingleStore in the curriculum . Wag! still has more to do with SingleStore. One feature it will look at soon was introduced in SingleStore DB 6.5: Pipelines to Stored Procedures . Existing Pipelines integrate the Extract, Transform, and Load (ETL) elements of a wide range of ingest problems. The new, upgraded Pipelines to Stored Procedures can break a data stream up into multiple tables, JOIN them to existing data, and update existing rows with the enriched data, with high performance. SingleStore can flexibly run a Pipeline to Stored Procedures,with or without a Transform operation, or an ETL pipeline Leveneur is also excited about a publicly accessible database of taxi data from New York that would be suitable for student lab work. “They love that! Coding and learning, on the cutting edge.” Wag!, too, is on the cutting edge. For more information on SingleStore DB 6.7, see the press release and all eight blog posts on the new version: Product launch ; managing and monitoring SingleStore DB 6.7 (includes SingleStore Studio); the new free tier ; performance improvements ; the Areeba case study ; the Wag! case study (this post); the creation of the Visual Explain feature ; and how we developed adaptive compression .", "date": "2018-11-06"},
{"website": "Single-Store", "title": "memsql67", "author": ["Rick Negrin"], "link": "https://www.singlestore.com/blog/memsql67/", "abstract": "Today, we announced the general availability of SingleStore DB 6.7. This latest release introduces features that provide faster query performance and improved usability with more robust monitoring, simplified deployment tooling, and a new free tier that anyone can use. Data and the ability to analyze and act on it are the most critical capabilities of modern businesses. Leveraging data quickly and effectively is key to delivering new customer experiences, enabling competitive advantage, and optimizing operations. Query performance, concurrency, and scalability are critical requirements for modern applications and analytical systems, yet legacy databases and big data systems struggle to keep up. SingleStore addresses all these challenges, and SingleStore DB 6.7 makes processing and analyzing data easier and faster for both streaming data and big data, while continuing to support familiar, standard SQL, and the broad ecosystem that uses it. New advances include a free production license for SingleStore DB 6.7, within limits; dramatically faster queries; new tooling for monitoring and management; new native connections; and a new developer forum for tips and troubleshooting. SingleStore DB 6.7 Now Free to Use, Making Maximum Performance Available to Everyone SingleStore DB 6.7 can be used for free in production , up to a limit of 128GB of RAM capacity, with unlimited disk/solid state drive (SSD) usage. Unlike many of our competitors, the free tier of SingleStore offers all the capabilities found in the Enterprise licensing tier of the database, including the full performance and security features. The free tier offers tremendous value. Customers of legacy databases are paying $100,000 and more per year for similar capacity for operational systems, data warehouses, and data marts. Yet these existing systems have fundamental scalability limits that are not present with SingleStore. The free tier is backed by extensive documentation and a community of users, but does not include professional SingleStore support. Companies looking for interactive, ticket-based support and guidance, or who want more than 128GB of RAM capacity (with no limits on disk/SSD usage), will need to upgrade to an Enterprise license. Contact SingleStore for more details. Faster Queries Out of the Box Getting excellent database performance often requires experimentation. Developers and analysts try various query hints and tuning steps as stop-gaps when query optimization, execution, or indexing don’t work fast enough. The result is more time spent tuning, and less time discovering and acting on new insights. Star Joins are a type of query commonly used in data analytics, in which a large table is joined to one or more smaller tables. SingleStore DB 6.7 has made Star Joins faster – up to 100x faster, in some cases – by leveraging query vectorization and single instruction, multiple data (SIMD) technology, a kind of parallel processing. Queries will also execute faster at first run with improved sampling eliminating cumbersome hints and tuning often associated with new query development. You can read more about SingleStore DB 6.7 performance results . New Monitoring and Management Tooling Simplifies Tuning and Deployment SingleStore Studio gives users a simple way to visualize the health of SingleStore clusters across resources, events, and queries. The new query inspector, for example, enables immediate discovery and debugging of query bottlenecks across the spectrum of CPU, memory, disk, and IO, which enables faster tuning and improved integration with other management and DevOps tools. Additional improvements include command line tooling that simplifies multi-node setup and cloud formation templates to automate cluster deployments and configuration. With these changes, the power of SingleStore, and the ability to optimize it as part of speeding data updating and access, becomes more accessible to everyone, from new users to established industry partners. We have a separate blog post where you can learn more about SingleStore Studio and the associated tooling . Improved Loading and Backups In SingleStore DB 6.7, we’ve added native ingest for the JavaScript Object Notation (JSON) and Avro data formats. (Avro is a protocol for remote procedure calls and data serialization that uses JSON format.) You can now load Avro or JSON from files or Apache Kafka directly into a relational schema or into a semi-structured JSON column. While SingleStore ingests both file types at high speed, Avro, being a modern binary format, provides the highest performance ingest possible. Both JSON and Avro allow for schema flexibility on ingest. Developer Forums for Tips and Troubleshooting For people who prefer to run our free tier, we have introduced a new developer forum for developers to post, research, and answer questions about SingleStore. Developers can learn best practices, engage with experts, and mature as a member of the No-Limits Database™ community. Check out our new forum . Want to Learn More? Join our SingleStore DB 6.7 webinar , where we’ll explain the new capabilities of SingleStore DB 6.7, the free tier, and deliver a live demo of our new monitoring and management tools. For more information on SingleStore DB 6.7, see the press release and all eight blog posts on the new version: Product launch (this post); managing and monitoring SingleStore DB 6.7 (includes SingleStore Studio); the new free tier ; performance improvements ; the Areeba case study ; the Wag! case study ; the creation of the Visual Explain feature ; and how we developed adaptive compression .", "date": "2018-11-06"},
{"website": "Single-Store", "title": "new-toolset-for-managing-monitoring-memsql", "author": ["Rick Negrin"], "link": "https://www.singlestore.com/blog/new-toolset-for-managing-monitoring-memsql/", "abstract": "Today, we introduced a new toolset for SingleStore that makes it easier to deploy, manage, troubleshoot, and monitor SingleStore. The new tools are easier to use, integrate more broadly, and contain new capabilities. SingleStore already has management tools for these operations, so why create new ones? First, the existing toolset is a closed loop. The tools only work with SingleStore and it is not easy to integrate with them. We’ve had many customers request integration with the broader ecosystem of tools they already use for configuration, alerting, etc. The new tools have been designed from the ground up to be easily integrated. In addition, the new toolset takes advantage of the role-based access control already built into the SingleStore engine. This gives you one place to manage which capabilities are available to each user. Lastly, the new tools are completely separate from the SingleStore engine, which allows us to iterate on them faster. So you will see new releases of the tools on a faster cadence going forward. The tools are broken down into two packages: command-line tools for managing SingleStore, provided in the toolbox package, and the monitoring UI provided in SingleStore Studio. All of the tools can be used together. Management Tools The three command-line tools for managing SingleStore are memsql-deploy, memsql-admin, and memsql-report: memsql-deploy is used to deploy the SingleStore software (the engine) to the various host machines that make up a SingleStore cluster. It can also uninstall and upgrade the software. memsql-deploy is also used for simple cluster installs, where it wraps a number of calls to the other tools. memsql-admin is used for creating and managing a cluster of nodes that have SingleStore running on them. You use it to create and delete nodes, assign each node a role, and stitch nodes into a cluster. You also use it for miscellaneous operations, such as setting a license for a cluster and setting configuration variables. memsql-report ensures that you have an optimal environment for running SingleStore. It runs checks to evaluate the health of the hardware, operating system, and cluster. The output of the checks come out as a simple Pass/Fail so you can easily spot potential trouble before it affects your running cluster. In fact, we recommend running memsql-report on your hosts before you install SingleStore, to ensure that your hardware and operating system are optimally configured. memsql-admin is a core part of the new toolset for managing SingleStore We have a recorded demonstration of each of the tools in use (no sound), which includes the screenshot shown above. SingleStore Studio SingleStore Studio provides a new UI for monitoring SingleStore and troubleshooting queries. It displays a top-level view so you can quickly get a handle on the health of a cluster, the databases, and the nodes. You can see details of how to use it in the SingleStore Studio documentation . The cluster dashboard offers health statistics and usage of SingleStore clusters Studio also provides more detailed information on the events happening in the cluster and on specific features, such as pipelines. It provides a query editor for composing queries and a profiling mechanism to capture the state of the cluster over a period of time to allow you to examine what queries and what resources the cluster is spending the most time on. Probably the most exciting feature of Studio is the new Visual Explain feature. SingleStore has had the Explain capability in the engine for several releases. The Explain feature details out the plan the engine will use to implement the query. Visual Explain provides a faster approach to identifying query bottlenecks This information is invaluable for determining how to optimize the query for your data set. The explain plan is rendered as a JSON document, so it can be tough to understand without a lot of practice. Visual Explain is a way of visualizing the information in the explain plan in such a way that it makes it easy to drill down into the plan, identifying which places are the biggest bottleneck, and provide the greatest opportunity for optimization. Developers and DBAs alike will find this a delight. Integration with Existing Tools One of the most important aspects of the new tools is how well they will integrate with existing configuration and deployment tools. All the command-line tools were built with integration in mind, so they provide non-interactive modes and return well-structured information, all of which makes it easy to integrate them. We are shipping an AWS Cloudformation template as part of this release that integrates with the tools and allows you to spin up a cluster, with all the best practices encoded, in AWS in less than 4 minutes. AWS Cloudformation is just the beginning. We will provide templates for all the popular configuration and deployment technologies (such as Puppet, Chef, etc…) in the near future. We also have documentation on how to integrate the SingleStore deployment tools in your own custom scripts as well. The new SingleStore tools for deployment, management, troubleshooting, and monitoring are a significant upgrade in terms of ease of use and compatibility. Download and try them out today! For more information on SingleStore DB 6.7, see the press release and all eight blog posts on the new version: Product launch ; managing and monitoring SingleStore DB 6.7 (this post – includes SingleStore Studio); the new free tier ; performance improvements ; the Areeba case study ; the Wag! case study ; the creation of the Visual Explain feature ; and how we developed adaptive compression .", "date": "2018-11-06"},
{"website": "Single-Store", "title": "areeba-case-study", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/areeba-case-study/", "abstract": "Areeba is an innovator in payments. It was using Hadoop for unstructured data and MariaDB as a relational database. Now, Areeba has added SingleStore to the mix for several projects, including fraud detection, anti-money laundering (AML), and product recommendations. Areeba is a financial services company that was spun out of Bank Audi, a leading Middle Eastern bank, last year. Previously, Areeba was the payment cards and electronic services division for Bank Audi. Now, Areeba has more than half a dozen banks as clients, running credit card loyalty programs and acting as a payment gateway for e-commerce in countries across the Middle East and Africa. Areeba is leading the way in advanced tech offeringsfor their financial services company customers. For the first project, SingleStore and Apache Spark work together to detect potentially fraudulent transactions and implement anti-money laundering (AML) regulations . This is a critical function for Areeba. “SingleStore is helping Areeba cover all our operational analytical needs,” said Elie Soukayem, director of data and analytics at Areeba. For the second project, SingleStore will store the insights generated by customer activity. Then, Areeba will use predictive analytics to recommend special offers, on a real-time basis, for each customer. “I was first exposed to SingleStore when we were still part of Bank Audi,” said Soukayem. “A partner company used SingleStore to build a prototype for card processing, including suggesting offers. When I left Bank Audi and came to Areeba, to save time, we went straight to SingleStore.” Areeba Moves Faster and Faster “Now” is always the best time to detect – or prevent – fraud, money laundering, and other illicit transactions. Currently, fraud detection and AML checks are run against each day’s transactions, so response to fraud and money laundering attempts is reactive. Fraud and AML reporting needs to run as fast as possible so the response can begin immediately. In the near future, it’s expected that fraud and AML detection will be enabled for transactions as they occur. That way, an illicit transaction can either be prevented or responded to very quickly after it occurs. Illicit activity can then be prevented for subsequent transactions on the same card, or from the same customer, shortly after the initial suspect use. When that change happens, real-time responsiveness will become critical. For the recommendation engine, SingleStore will need to combine the latest activity for each customer with the latest special offers. Then, each time a customer makes a purchase, they can be presented with an offer most relevant to their updated purchasing history, in real time. Only with real-time processing can the best offer be determined. In both cases, with stale data, the risks of a negative outcome for the vendor and a negative experience for the customer are high. Areeba has to move beyond traditional solutions as it works to create the best outcomes for its customers. That’s where SingleStore comes in. “Translytical” to the Rescue SingleStore is part of Areeba’s solution on both challenges – detecting illicit transactions and real-time recommendations. Because SingleStore combines transactional and analytical processing, each task gets full analytics power on the latest, continually updated data; not old data that’s been put through batch processes. On the fraud and AML side, Areeba’s Soukayem said, “We’re using SingleStore with Apache Spark to dig for suspicious activities. Performance with SingleStore is impressive.” “Our biggest table is around 100GB in size and holds hundreds of millions of authorizations, covering a period of three years,” continued Soukayem. “We have another similar table, and a smaller table for transactions, that holds the settlement of pending authorizations.” All of this data has to be processed in real time for effective results. It will also be used to extract hints for the merchant to use to make recommendations for add-on purchases. “I’ve been in this business since 2005, and performance has always been a challenge,” Soukayem said. “Storage in memory and on disk is a plus.” Which is great, because those are the default options for SingleStore’s rowstore and columnstore. The Future is Faster Areeba is solving problems today and busily preparing for the future. “We might leverage blockchain to make payments easier in the region,” said Soukayem. In addition, Areeba is increasing its usage of the cloud. Soon, it will establish an innovation garage to work on even more advanced technologies. Areeba’s founding principles are to upgrade technology to higher levels; innovate on both products and services; and establish reliable, long-term relationships with customers. Areeba is looking to take SingleStore along as they move to the cutting edge. For more information on SingleStore DB 6.7, see the press release and all eight blog posts on the new version: Product launch ; managing and monitoring SingleStore DB 6.7 (includes SingleStore Studio); the new free tier ; performance improvements ; the Areeba case study (this post); the Wag! case study ; the creation of the Visual Explain feature ; and how we developed adaptive compression .", "date": "2018-11-06"},
{"website": "Single-Store", "title": "announcing-memsql-free-tier", "author": ["Mike Boyarski"], "link": "https://www.singlestore.com/blog/announcing-memsql-free-tier/", "abstract": "Today, we announced our latest product version, SingleStore DB 6.7 . With this release, SingleStore is now free for everyone to use for databases with up to 128GB of RAM usage, and no limit on database size on disk, including solid state drive (SSD). Unlike what customers get from other database providers, the free tier of SingleStore is full featured and includes all enterprise capabilities, including high availability and security. What Can You Do with the Free Tier? You can do almost anything with SingleStore, using the free tier, that you can do if you have an Enterprise license, including capabilities and production use. The differences are that you can only configure the free tier of SingleStore to use up to 128GB of RAM usage, with unlimited database size on disk, and support is only community support ; for paid SingleStore support, you need an Enterprise license. The free tier allows customers to: Learn . You can learn about SingleStore and how it works on your own or in conjunction with SingleStore Training . Develop and test . You can develop and test using SingleStore and full features, but without professional SingleStore support. Deploy into production . If you’re willing and able to run your app within the RAM usage limitation, and without professional SingleStore support, you can deploy an app running into production. However, if you use the free version you may reach the point where you need to go beyond the configuration requirements of the free tier or want professional SingleStore support. At that point, you can seamlessly move to a time-limited Enterprise trial license, and arrange with SingleStore to buy a full Enterprise subscription before the time limit expires. What Kinds of Applications and Workloads Can the Free Tier Support? You get 128GB of RAM capacity in the free tier of SingleStore, with unlimited database size on disk. This means you can run robust applications. Check out what some of our customers are already doing today that would be technically possible with the new, free tier of SingleStore. Real-Time Geospatial Analytics with Wag! Customer experience is of the utmost importance for service-driven organizations. That’s why Wag!, the “Uber for dogs”, chose SingleStore to help accelerate the tracking and visualization of dog walkers in real time. Location data is captured every five seconds in JSON format and is structured for SQL queries to track time, walk distance, and other statistics. The ingest and store requirements run within the 128GB memory range due to the use of AWS S3 as a historical archive. The end result is a fast data architecture that enables low-latency join queries of hot and archived data, resulting in a comprehensive yet real-time view of the dog walk in progress, delivered instantly to the customer’s mobile app. To learn more about Wag! and its use of SingleStore, read our blog post . Trading Application Managing and analyzing financial markets while delivering high-performance transactions for a fast-growing online brokerage can be a challenging task. This production application uses SingleStore to support up to 12,000 transactions a minute while simultaneously supporting queries for 250,000 unique users, with sub-second response time. The high-performance application is capable of running within the 128GB memory footprint due to the efficient use of memory and disk. Image Recognition for Premium User Experience with Nyris Retail companies must continuously innovate with breakthrough buying experiences. Nyris is a company delivering visual search for retailers, manufacturers, and more by leveraging fast image recognition with SingleStore. Nyris is capable of matching user-submitted photos against a repository of 100,000 products in milliseconds, helping each customer quickly match their photo with a product that’s available for sale. The application leverages SingleStore’s built-in ML scoring algorithms, known as DOT_PRODUCT, to perform vector similarity matching with sub-second response. The application is able to run on less than 128GB of memory while delivering a premium experience to customers. Operating SingleStore’s Free Tier in Production When you sign up for SingleStore, you receive a license key and instructions for installing the software in the cloud or in your data center. The free tier of SingleStore is community-supported, backed by extensive documentation and a community of users . Companies looking for 24×7, ticket-based support and guidance, or those who require more than 128GB of RAM, will need to upgrade to an Enterprise license. With the SingleStore free tier, you can configure up to 128GB of RAM capacity. It will not allow provisioning a cluster larger than this configuration. Users who need more compute for their application can contact us for an evaluation or subscription for an Enterprise license. Updating to an Enterprise subscription is as simple as changing your license key. This can be done while online and requires no export/import or cumbersome upgrade steps. Try SingleStore for your applications and analytical systems today. You can install SingleStore DB 6.7 for free . For more information on SingleStore DB 6.7, see the press release and all eight blog posts on the new version: Product launch ; managing and monitoring SingleStore DB 6.7 (includes SingleStore Studio); the new free tier (this post); performance improvements ; the Areeba case study ; the Wag! case study ; the creation of the Visual Explain feature ; and how we developed adaptive compression .", "date": "2018-11-06"},
{"website": "Single-Store", "title": "webinar-data-lake-age-of-ml-ai", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/webinar-data-lake-age-of-ml-ai/", "abstract": "Bill Vorhies of Data Science Central and Rick Negrin of SingleStore recently delivered a webinar on data lakes and the use of Hadoop in the age of operational machine learning (ML) and artificial intelligence (AI). In this blog post, we summarize some of the key points from the webinar, as a preview for the webinar. You can also view the recorded webinar at any point. Businesses need flexible access to their data to make the best decisions. Unfortunately, the systems in use today impose complexity and structural delays. SingleStore, a fast, fully scalable relational database that supports ANSI SQL, can simplify your data processing architecture and speed decision-making. SingleStore can also make current, accurate data available for use by ML and AI. The Promise – and Peril – of Data Lakes Businesses know that they need to understand their customers better and make fast, accurate decisions to deliver superior customer experiences. Data lakes are meant to be the answer to this need. The promise of data lakes was to take in masses of data quickly and make it available for exploration and decision-making. This prospect has led to very rapid growth in Hadoop, the technology underlying most data lake implementations. Hadoop and big data are a growing market in which SingleStore will play a larger role Unfortunately, only the first part of this promise – taking in masses of data quickly – has been kept. Hadoop is fully scalable, so it can use large numbers of servers based on industry-standard hardware to handle large ingest volumes. However, the databases used with Hadoop are NoSQL databases, not relational databases that support SQL. We here at SingleStore have an excellent blog post on why NoSQL is no longer needed for most use cases. Briefly, NoSQL databases were created largely to offer the scalability that was previously lacking in relational databases that support SQL. However, they offered scalability at the cost of removing most relational functionality, transaction capability, and SQL support. This forced architects and developers to choose between non-scalable traditional databases on the one hand and scalable NoSQL databases on the other. Now that NewSQL databases in general, and SingleStore in particular, offer fully scalable relational databases that support SQL, most of the use cases for NoSQL disappear. (Many of the remaining use cases fall into two groups: rapid ingest of data and long-term storage of data as an archive, both at low cost.) The lack of relational functionality, transactions, and SQL support means that data ingested into Hadoop is not readily available for use in meeting the second part of the promise – making the data available for exploration and decision-making. It may also not be available for many of the features you might want to add to an application, as explained in this post about a failed implementation of MongoDB . A data lake architecture – before SingleStore To address this problem, additional data processing steps, tools, and new databases were developed. A query layer composed of tools such as Apache Spark and Hive promises access to Hadoop data. An extract, transform, and load (ETL) process takes data stored in Hadoop’s NoSQL database, HDFS, and stores it in data warehouses, data marts, and operational data stores (ODSes). The use of all of these additional layers and processes creates a complex infrastructure that is slow, complicated, hard to maintain and use. How SingleStore Makes a Difference SingleStore is a fast, scalable, fully relational database that supports ANSI standard SQL. SingleStore offers the best of both worlds: it’s fully scalable on industry-standard servers such as Hadoop, and it is fully relational, offering a consistent database with transaction support and a standard SQL interface. SingleStore is described as “translytical”; a SingleStore database can process transactions and queries for data analytics simultaneously. There is no ETL step, and no need for specialized tools for making unstructured data into (more or less) structured data. Your data is never not in a relational database; it’s always stored in records, always consistent, and always accessible via familiar SQL queries and all the SQL-compatible tools out there. SingleStore interacts with all the elements of a data lake – and more The advantages of this for machine learning and AI are clear. You can run machine learning against all your data, old and new, in one process. AI programs are always operating on all the data your organization has. AI programs have instant access to the newest information as it comes in. The advantages of a full implementation of SingleStore are clear, both for operational simplicity and for ML/AI readiness. SingleStore-Based Data Lake Alternatives If you have a traditional data lake architecture, you can use SingleStore in two different ways: SingleStore with Hadoop . SingleStore can replace the tools in the query layer of a traditional data lake and eliminate the need for a separate ETL process and separate data warehouses, data marts, and operational data stores, creating a much simpler and faster system. SingleStore may be used in a smaller role, for ingest or as a query accelerator, with Hadoop retained as a long-term data store. SingleStore Replaces Hadoop . You can simply load data directly into SingleStore and query data directly from SingleStore. Hadoop is designed out. (This is what you would have done if SingleStore had been available when you first invested in Hadoop, query tools, ETL processes, and data warehouses, data marts, and operational data stores.) SingleStore can be used alongside Hadoop in a data lake The webinar includes a case study of how Kellogg, the food company, replaced Hadoop and Hive with SingleStore. It got much faster query performance, ETL-like processing in minutes instead of hours, and the ability to use SQL again. All in all, a huge win. At Kellogg, SingleStore replaced Hadoop in a data lake implementation Q&A Attendee questions, and Rick’s answers, included: Does SingleStore support semi-structured data such as text, JSON, Avro, and Parquet-formatted files? Yes. You can pull in CSV files, JSON, and Avro natively now; a future release will include native Parquet support. For queries, can I use Java or Python Spark ML or sklearn functions? Any technology that has a MySQL connector can connect directly with SingleStore and run queries. This works with Spark or just about any language, as most languages have a MySQL connector or an ODBC or JDBC driver. In addition, SingleStore has developed native connectors for key technologies such as Spark and Python, optimized for use with SingleStore and supporting all of its functionality. Can SingleStore integrate with pretrained models in TensorFlow? Yes, and we’ve demonstrated this. You can use TensorFlow libraries to evaluate the data and create a model and then use SingleStore pipelines to ingest data against it. We have a video (about 12 minutes long) showing the TensorFlow/SingleStore combination in action. What is the largest datastore that SingleStore currently supports? There’s really no known limit as to how much data SingleStore supports. We have customers that have hundreds of terabytes stored in SingleStore. We have one customer who has hundreds, and soon will have thousands of databases stored in a single cluster of SingleStore. We have some that are doing 10-15 million inserts per second by using a very large cluster and mostly using the rowstore – we scale in lots of ways. Do you integrate with SAS? Yes, we have a formal certification from SAS that SingleStore and SAS work together . It’s easy; you just use the existing MySQL connector. Watch the SingleStore data lakes webinar to get the whole story.", "date": "2018-11-13"},
{"website": "Single-Store", "title": "epigen-powers-facial-recognition-in-the-cloud-with-memsql-case-study", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/epigen-powers-facial-recognition-in-the-cloud-with-memsql-case-study/", "abstract": "Epigen Technology depends heavily on SingleStore as part of its core toolkit. “Without SingleStore, I can’t do what I do,” says Terry Rice, founder of Epigen. Visit SingleStore at AWS re:Invent booth #1905 in Las Vegas, November 27-30. Epigen Technology uses software to innovate in many fast-changing areas of technology, including cybersecurity, analytics, machine learning, artificial intelligence (AI), and the emerging area of cognitive computing . Now, Epigen has developed a framework for facial recognition in the cloud that combines AWS S3, SingleStore, machine learning, AI, and visualization. Terry Rice, founder of Epigen, is a noted IT consultant and is trusted by senior figures in business and government to deliver solutions to challenging problems, including national security threats. He’s also a fifth-generation Marine. Terry graduated early from the University of Oklahoma with dual degrees in Information Systems and Statistics. After finishing college, he enlisted as a Marine, following a family tradition going back more than a century. He served for six years. Terry then moved into roles as an IT consultant and software architect for companies, such as Capgemini, Lockheed Martin, Network Solutions, and Verizon. He’s also worked for government, serving as a solutions architect for the Department of Homeland Security. In his current role, Terry is helping to lead the development of innovative systems in several areas. He speaks at conferences on blockchain, big data, machine learning, and AI. And he uses SingleStore as a core part of Epigen’s toolkit. Recognition Faces Challenges Facial recognition is one of the most challenging topics in AI. Facial recognition for isolated static images is improving – as experienced by anyone who uses Facebook, which can often suggest names for most of the faces in a photograph of family or friends. The Facebook example shows some of the characteristics that make facial recognition easier: A well-lit, high-resolution, static image. A small list of candidate names for each face (the user’s Facebook friends). Subjects not trying to disguise themselves. Low consequences for failure – in the Facebook case, a non-identification or misidentification is an annoyance, or even humorous, rather than a serious problem. However, Epigen’s clients in areas such as the military, law enforcement, and air transportation need facial recognition that works under increasingly challenging circumstances, including working at scale and using video frames, not just static images. Currently, facial recognition is being used at some airports to verify the identity of passengers on a per-flight basis. The comparison is a four-step process: For a specific flight, passport images for all ticket holders are vectorized and stored. People who appear at the gate to board a flight are photographed live. Live passenger photos are compared to the stored passport photos. If a live passenger photo doesn’t match any of the stored passport photos, the passenger is asked to verify their identity as a match against their passport. After years of testing, this system is now in use at more than a dozen airports and at land ports of entry to the United States. In one recent case, it prevented a passenger who was using a passport that was valid, but not their own, from boarding a flight at Dulles Airport in Washington DC. The current approach requires that each live photo be compared to as many as nearly a thousand passport photos (given that an Airbus A380, the largest passenger airplane, can carry more than 800 passengers). This is only the beginning. Current, and potential future, users of facial recognition systems need more capability. Epigen is working on a series of improvements: Higher accuracy – for the current, one in a thousand use case. The system used to check passengers against the passport photo database is only about 85 percent accurate. There is a short-term goal to improve accuracy to 97 percent, then further improvement will be required from there. Compare against other databases – for use cases of one to thousands and one to millions. Today, each live photo is only compared to the passport photos of one specific flight’s passengers. In the future, the system should also compare live photos to databases from Immigration and Customs Enforcement, the FBI, other law enforcement agencies, motor vehicle departments, and others. Use full-motion video – lower-resolution, moving images. Video cameras are everywhere, even more so in some countries than others. Automatically spotting wanted people on the move is an important goal for law enforcement. One high-priority use case, hard to deter by other means, is recognizing missing and kidnapped children. AI-based recognition from video could augment existing systems, such as the AMBER Alert system used in the US. One target application for video is to shorten lines at airports. If people can be identified and matched against passport photos as they move through the terminal, they can be, in essence, pre-approved. The small number of people who raise concern can be contacted by officials before reaching a gate. This pre-checking can avoid the need for people to queue at entry points, potentially eliminating the long lines at Customs, for instance, when entering a country. Throwing Resources – and SingleStore – at the Problem How can facial recognition be made to work better, and to scale to handle more comparisons, at higher speeds, and to handle video? Epigen is developing a scalable architecture for facial recognition. Requirements are strict. For instance, “We need less than 10ms response time, round trip,” according to Rice. SingleStore is a great fit for this kind of challenge. The Pipelines feature combines rapid data ingest and data processing. “I like SingleStore because it’s lightweight and runs on bare metal,” Terry says. “I can make the technology disappear.” Epigen is incorporating GPUs, rather than CPUs, for vectorization of facial images from video and comparison of those images to a database of candidates for matching, such as a law enforcement watchlist. GPUs are an excellent tool for solving many problems that require the rapid execution of complex mathematical algorithms – a class that includes many challenging problems in machine learning and AI, including this one. (Or, as recently described in Forbes: for AI, GPUs are the new CPUs .) Just as GPUs are designed to be used alongside CPUs, with the GPU serving as an engine for graphical calculations, GPUs can also be used for the mathematical calculations in machine learning and AI – whether those calculations are specifically graphical or not. The problem faced by many solutions that pair CPUs and GPUs together is getting data into the GPUs fast enough. “The GPUs are typically starved for data,” says Terry. So SingleStore is used to partially process data, then rapidly transfer that data to GPUs for further processing. Epigen’s facial recognition platform takes full advantage of several SingleStore capabilities: Ingest – rapidly bringing in photographic data from Amazon S3 and, in the future, from other sources. CPU-based processing – SingleStore’s ability to handle the “transform” part of an ETL operation within a pipeline is crucial to rapid processing, as is SingleStore’s Pipeline to Stored Procedures capability when more complex processing is required. GPU-based processing – rapid transfer of processed data into the GPUs, which is crucial to performance of the entire system. Data is initially read into an Amazon S3 database in the cloud. Epigen then uses a SingleStore Pipeline to extract the data from S3 at top speed, then rapidly transform and load it into SingleStore. From there, the data goes to the GPU array. The advantage of SingleStore is that it’s scalable at every point in the process. Ingest is crucial when one photo is being compared to thousands or millions. But so is initial processing in SingleStore and total throughput, especially the ability to flow data smoothly into the GPUs. “GPUs are often only used at about 10 percent of capacity,” says Terry. “With SingleStore, I’m already up into the 40-50 percent range,” and he’s working to bring the GPUs to full utilization. In this kind of implementation, Terry has come to expect roughly a 4-5x increase in performance. “I was running Python scripts against S3, and now I don’t have to write or run those scripts and performance is optimized,” says Terry. “I was then reading the data into MongoDB, which worked poorly,” he continues. “MongoDB doesn’t have the ability to roll back the system.” (Because SingleStore is transaction-based, transactions can be rolled back with no lost of data. NoSQL solutions such as MongoDB lack this capability.) “You need to be able to do that with visual recognition.” “We want throughput and productivity. And people are scared of the price tag that typically comes with that,” offers Terry. With SingleStore, “I can use 100 percent of system capacity and a smaller number of machines,” he said. “Direct costs might drop from around $2 million a year on a different stack to about $150,000.” And, with SingleStore, “I can have fewer people running that infrastructure.” Technology as Teamwork Epigen often pairs SingleStore with a visualization tool named Graphistry , which is frequently used in developing and using AI applications. Graphistry helps expose relationships among different kinds of data. Like so many other analytics and visualization tools, the Graphistry API has built-in functionality for working with SQL data, making it a good match for SingleStore. Graphistry promises “the analytical power of GPUswithout having to think about GPUs.\" Developing technology of this sort is a collaborative venture. When Epigen provides a system, the client may put in as much work customizing and optimizing it for their use case as Epigen did to develop the system in the first place. “I set the reference architecture,” explains Terry. “But, in order for the technology to work, the client has to be able to use the tools I provide them to improve the system further. SingleStore is an important part of that set of tools.” Next Steps for Epigen with SingleStore Facial recognition is only one of the areas in which SingleStore is helping Epigen offer advanced technology for customers. “Some of the problems we are investigating for future work include replacing a data warehouse implementation so as to improve fraud detection; preventing opioid abuse; geospatial applications; and Oracle database consolidation, combining many disparate database instances into a single data platform on SingleStore. Also, most of what we learn from facial recognition, we can apply to audio and voice detection as well.” With this initial facial recognition application, Epigen is creating a reference architecture for cognitive computing. This design goes beyond today’s machine learning and AI tools to make decisions and offer support for decision-making by humans. For instance, a narrowly targeted AI tool might be able to show that someone boarding an airplane is not the same person as shown on their passport photo. With cognitive computing, the live photo would then be matched against law enforcement databases, with possible matches identified and each possible match assigned a probability. A tool such as Graphistry would then be used to present a complete picture of the information uncovered to security personnel for them to act on. Epigen is creatively using the latest technology – SingleStore as the database, GPUs for vector processing, Graphistry in visualization, and machine learning and AI tools – to tackle some of the toughest problems facing business, the military, and government. Its prospects for success are promising.", "date": "2018-11-27"},
{"website": "Single-Store", "title": "selecting-the-right-database-for-your-scale-demands-2", "author": ["Rick Negrin"], "link": "https://www.singlestore.com/blog/selecting-the-right-database-for-your-scale-demands-2/", "abstract": "Scaling distributed systems is hard. Scaling a distributed database is really hard. Databases are particularly hard to scale because there are so many different aspects you have to consider. For example, which dimension is growing, and how fast, will dictate what resources you need to increase in order to handle the workload and maintain your service level agreements (SLAs). Some problems require more CPU, some more RAM, and some more storage. Many times, you need a combination. Knowing how much you need up front is tough to determine. In addition, requirements change over time. When demand increases, you need to be able to dynamically add hardware resources, without compromising your availability. Most legacy databases only support a single-box model making it difficult to scale and maintain availability. SingleStore is a scale-out relational database with ANSI SQL support and, as such, is a much better solution for workloads where scaling up on a single machine is not viable. Let’s look at the different dimensions. Scalability limits are a key challenge fordata platforms and SingleStore is an important solution Growth in the Amount of Data Ingested per Unit of Time Customers often see rapid growth in the amount of data being ingested. An example is Pinterest, where the ingest increases with growth of the user base and with rising user engagement. This pattern is typical of a consumer service experiencing significant user growth. Another example is an IoT workload where the company is growing the number of devices sending back data. Ingestion is often bottlenecked on computing cycles , so increased ingest requires adding more CPUs. Growth in Concurrent Queries There is a culture change happening in business that is driving companies to be more data-driven. This means empowering all of your employees to use data to make decisions across the business. Uber is a good example of a SingleStore customer that has internalized this ethos and maximizes employee access to data so they can do their jobs more effectively. Customers often start out by having only a few people, such as dedicated analysts or data scientists, accessing data to answer questions for the business. As data sources become more mature, and other people see the results of using the data, they demand direct access. An organization’s data warehouse and other data resources are often not sized appropriately to deal with additional capacity demands that arise after the system is configured. To handle the additional queries without long delays, or even making the data resource unstable, requires more CPU and memory. Growth in Working Set Growth in the working set can occur for a number of different reasons. For example, a customer could decide to change the window of time they want to look at the data from three days to seven days. Another example is that the ingest rate increased (because of one of the scenarios above) and the amount of data for the same time window is now bigger. The customer could be running a Customer 360 Dashboard, or similar dashboard, and the size of the working set grows as they acquire more customers. Another way the working set can grow is changes in the queries to accommodate more data. For example, new dimension data is added and the number of joins over large tables increases. Growing the working set generally means needing more CPU (to do the processing) and additional memory (to hold interim and final results). Growth in the Number of SQL Objects Enterprise customers with large data warehouses often have a larger number of SQL objects. One big bank we worked with had a large data warehouse with hundreds of thousands of tables. Another pattern needing a high number of SQL objects is a multi-tenant service. In this pattern, the customer offers a service (often to an enterprise) where the data from each of its customers needs to be kept separate, both for security and for namespace reasons. Using namespace boundaries allows the system to enforce security effectively and keeps users of one company from seeing another’s data. Having this separation encoded into the system makes it easier on the application programmer. The namespace boundary allows the tables in each customer group to have the same names, making it easier to write, debug, and maintain the application code that accesses the tables. The result is that you get a set of tables per customer. So if there are a standard set of 30 tables for a customer and 1000 customers, you have 30,000 tables. One of the most common problems in this model is that the size of customers follows a long tail distribution, where there are a few customers who each have a large amount of data, plus many small customers who each have a small amount of data. So you need a database system that can scale a single database for the large customers and also efficiently support many small databases for the other customers. Most of the PaaS database services in the market do not handle this case very well. Growth in SQL objects that represent customers requires more storage, more CPU, and more memory to handle the increase in ingest and query from the additional customers. Growth in SQL objects for other reasons requires more storage, CPU, and memory to handle additional ingest for the additional objects and additional queries against them. Growth in Data Storage Growth in data storage is another example of a scaling issue. A customer wants to regularly query the last week of data. The query is run regularly, but only by a set number of users (i.e. no change in concurrency). The amount of data produced in the week is constant. However, the customer wants to retain data for two years to occasionally run longer historical queries. This requires more storage, but does not require an increase in CPU or memory, since the amount of data required for most queries stays constant. Handling the Growth Most existing systems assume the customer can correctly forecast the amount of future growth and build a cluster that will accommodate it. If they forecast too little demand, the system will get bogged down, or become unstable as the demand outstrips the resources available. If they forecast too much demand, they spend too much money in over-provisioning the system. The table below shows the different areas in which you need to scale for growth in the different kinds of demands described above. CPU RAM Storage Ingest per unit time ✓ ✓ Query count ✓ ✓ Working set ✓ ✓ # of SQL objects ✓ ✓ ✓ Data range ✓ Most legacy databases only support a single-box model. A single box limits how much you can scale. In addition, a single-box model makes it difficult to scale as an online operation, increasing downtime whenever there is unexpected growth. SingleStore is a scale-out relational database. It runs on commodity hardware using any modern version of Linux. SingleStore scales easily by allowing an administrator to add new nodes to a cluster. Nodes can be added and data rebalanced as an online operation, with no downtime. If you have a data intensive application and need flexibility and availability in your data operations, SingleStore should be on the list of alternatives which you consider for new requirements and improving existing operations. For more information, view our webinar, Three Steps to Modernizing Your Data Platform .", "date": "2018-11-28"},
{"website": "Single-Store", "title": "webinar-an-overview-of-memsql67", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/webinar-an-overview-of-memsql67/", "abstract": "In this webinar, Eric Hanson, Principal Product Manager at SingleStore, describes the key features of SingleStore DB 6.7, which is focused on usability, compatibility, and (as with every release) performance. View the webinar now , download the slides , or read all about it below. Highlights of SingleStore DB 6.7 include the new, and very popular free tier for SingleStore , which ZDNet describes as having “full functionality to support real-world use cases”. Also included in SingleStore DB 6.7 are improved ease of use , faster queries , and new tools for monitoring and monitoring SingleStore . The new release also includes an upgraded customer portal and a new, open developer forum to interact with SingleStore and your fellow developers. Users of the new free tier can access the forum as a form of community support, and can also freely access SingleStore documentation . As Eric pointed out in the webinar, the new, free tier of SingleStore is proving very popular. And not only with organizations that are new to SingleStore – many organizations that are already customers are using the free tier for new projects, with plans to convert the instances to an enterprise license, with full SingleStore support, as each project moves into production. The new SingleStore free tier is popular with new and longtime SingleStore users. A highlight of the webinar is a demo of SingleStore Studio , the new monitoring UI for SingleStore. SingleStore Studio is delivered as software as a service (SaaS) and will be updated regularly, every month or two, to add features and respond to customer feedback. Every current or prospective SingleStore user should watch the webinar for the detailed SingleStore Studio demo. With Studio, and using the techniques shown in the demo, your ability to optimize your queries will skyrocket. Webinar attendees asked many interesting questions, including the following. Q: With the free tier, is the limit 128GB per cluster? \\\nA: Yes. The limit is 128GB per cluster, which is aggregator and leaf nodes combined. Disk usage is unlimited. You’ll find informal support in our forum and you can take SingleStore training courses leading to certification. Q: How hard is it to move from SingleStore Ops to SingleStore Studio? \\\nA: It took Eric a couple of hours to learn to use the Ops tools for basics, and there are analogous commands in the new tools. The new toolset has tremendous benefits. SingleStore Ops is a distributed system that runs alongside SingleStore, and has its own failure modes. The new toolset is basically stateless and much more robust. It also has additional features that add a lot of power, including the ability to work easily with management tools like Ansible, Chef, and Puppet. Q: Is SingleStore Ops going away? \\\nA: Eventually, yes. From here forward, new engineering will go into the new toolset. But SingleStore Ops will be supported for at least a couple of years alongside the new toolset. Q: Are there plans for range partitioning? \\\nA: Range partitioning helps you manage extremely large data sets. We think we can address these needs in other ways, with the current product and in future releases. For instance, I was recently able to delete 100 million rows of a 300 million table in seconds; a similar operation on most relational databases takes hours. Q: Is there any new information about cloud deployment? \\\nA: There’s some really good new material in our documentation about how to deploy SingleStore and there’s a new AWS cloud formation template . Q: How does the free tier for SingleStore compare to the free tier for SAP Hana? \\\nA: The SAP Hana free tier is much more limited, in two different ways. The first is that the memory allotment is only 32GB for the SAP Hana free tier, vs. 128GB for SingleStore. But the really big difference is that SAP Hana puts the entire database in memory, whereas SingleStore only has aggregator nodes and rowstore tables in memory; columnstore tables live on disk, with a relatively small amount of RAM used for management. So if you use all rowstore tables in SingleStore, which is rare, you might have 100GB of data and the remaining 28GB or so for aggregator nodes. If you mix rowstore and columnstore tables, you might run four servers as nodes, with 32GB of RAM per server, and hundreds of gigabytes of data in the overall database. This is an order of magnitude more total data than the SAP Hana free tier. Q: You’ve added support for SIMD instructions on Intel. How about AMD? \\\nA: The newest AMD chips include compatibility with the SIMD instructions that we support on Intel, so you can get the benefit of the Intel SIMD instructions on both Intel and AMD. Q: Is SingleStore Studio SaaS or on-prem? \\\nA: You actually install SingleStore Studio on-premises, or on your own virtual machines in the cloud. Q: Can you back up and restore SingleStore to AWS S3? \\\nA: Yes, you can both back up and restore directly to S3. Q: Is SingleStore more widely used on-premises, or in the cloud? \\\nA: Both. About half of our customers are on-premises, and about half in the cloud. Q: How easy is it to get my admins to be experts in SingleStore? \\\nA: Because it’s a SQL database, it’s very similar to manage to the most popular databases out there, including Oracle, Microsoft SQL Server, IBM DB2, MySQL, MariaDB, PostgreSQL, Teradata, and Vertica, as well as many others. The new part will be learning how to manage the distributed aspects of the system, in particular aggregator and leaf nodes, rebalancing data, and a few other aspects. However, SingleStore helps customers with initial setup, and you may only need to adjust the system every few quarters or so. View the recorded webinar for SingleStore DB 6.7 and download the slides . For even more information on SingleStore DB 6.7, see the press release and all eight blog posts on the new version: Product launch ; managing and monitoring SingleStore DB 6.7 (includes SingleStore Studio); the new free tier ; performance improvements ; the Areeba case study ; the Wag! case study ; the creation of the Visual Explain feature ; and how we developed adaptive compression .", "date": "2018-11-29"},
{"website": "Single-Store", "title": "aws-reinvent-sees-innovation-around-ai-blockchain-databases-and-space", "author": ["Mike Boyarski"], "link": "https://www.singlestore.com/blog/aws-reinvent-sees-innovation-around-ai-blockchain-databases-and-space/", "abstract": "AWS re:Invent, which ended yesterday, had an estimated 50,000-plus people in attendance. Since the first AWS re:Invent 6 years ago, Amazon has gone from an up-and-coming innovator in the IT software space to the 800-pound gorilla dominating enterprise software. Given their outsized role for our customers and partners, we thought we would share news and a few highlights from our time at the show. Unsurprisingly, AWS spent a lot of time discussing the emerging fields of AI and machine learning. The main focus was on tooling for delivering advanced analytics easily. While most of the tooling is software, Amazon also announced hardware: a new processor, called Inferentia , that is promised to run AI applications with high performance and at low cost. This follows a similar move into AI processors by Google. Inferentia is a new Amazon processor for AI. (Image courtesy AWS.) AWS had a lot to say about databases. They are continuing to take popular open source databases and package them as services, a worrying trend that already affected the creators of MongoDB and Elastic. Now, the creators of Kafka are the next victims, with the launch of AWS Managed Kafka as a public preview. The history of AWS monetizing open source projects without contributing cash or code to the projects has some calling out the negative impact of AWS Managed Kafka and similar business tactics on these communities, and has many pushing for a change in the open source licensing model to protect the viability of these projects. AWS usually supports trends fairly early in Gartner’s famous Hype Cycle , and blockchain is no exception. The Amazon Quantum Ledger Database is a new, managed blockchain database for distributed hyperledger applications. The topic that got the most attention was about Amazon’s own infrastructure. Oracle CEO Larry Ellison has made much of the fact that Amazon has made heavy use of Oracle databases, and Amazon, in return, keeps promising to get completely off of any Oracle licenses. Amazon is promising to be mostly Oracle-free by year-end 2018 and completely so before 2020. This follows a larger trend we are seeing across the industry; as more and more enterprises and governments move to the cloud, they are taking the opportunity to replace aging data infrastructure with NewSQL databases like SingleStore , Amazon Aurora, and Google Spanner. AWS also announced new databases and new database capabilities. Amazon Timestream is a timeseries database. (It’s possible to structure data flows so both time-stamped changes and updated records come out of it, but Timestream is about splitting those processes.) The new offering is serverless, which means that customers have little visibility into the operational basis for their costs. There’s a new feature for Amazon’s premier relational database, Amazon Aurora Global Database, which promises replication across regions, in support of high availability and disaster recovery. And Amazon DynamoDB added transactional support and read/write capacity provisioning. Amazon knows that getting control of their customers data locks them into the AWS platform long-term, and these new features will allow the company to capture even more data workloads and storage. The data announcements at AWS culminated in near-earth orbit. AWS Ground Station provides data download and processing support for any of the many thousands of satellites currently circling the planet. AWS Ground Station provides data services for satellites. (Image courtesy AWS.) The SingleStore booth was very busy, with discussions mirroring the larger event topics of modernizing data infrastructure and the creation of new, more demanding, data-intensive applications. And Oracle was not the only database vendor that organizations are wary of. We heard continued concerns expressed about the creeping costs and lock-in on Amazon’s closed ecosystem, where the lure of the ease of consumption of the services available comes with unexpected barbs and long-term risks. Gartner’s prediction that 90% of enterprises will adopt hybrid infrastructure management appears to be well corroborated by the attitudes of the mature IT organizations we have spoken with. New SingleStore offerings, such as the new SingleStore DB 6.7 release and the new free tier for SingleStore , attracted strong interest. For more on both, view our recent webinar on SingleStore DB 6.7 .", "date": "2018-12-01"},
{"website": "Single-Store", "title": "how-to-build-real-time-dashboards-at-scale", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/how-to-build-real-time-dashboards-at-scale/", "abstract": "With a real-time dashboard, you can stop problems before they start and seize opportunities that your competition is not aware of yet. SingleStore is a tool uniquely capable of powering real-time dashboards. In this blog post, we give some high points – but to get all the details, you should view our recorded webinar and download the PowerPoint slides here . A doctor can get a good idea of your health just by shining a light in your eyes and seeing if they’re clear or cloudy. Similarly, a business that’s running well, with capable networks and up-to-date data, can run their business smoothly off a real-time dashboard. Until recently, the database technology that was designed to power real-time dashboards has often proven inadequate to handle the task – especially when operating at enterprise scale. But it’s at the enterprise level that you really need the simplicity and clarity that a dashboard can bring. Luckily, with SingleStore, you can cut through the fog. You can ingest, process, and drive analytics from current data, allowing you to keep your business up-to-date in real time. In the webinar , Alec Powell and Alexia Smith of SingleStore take you through the use of dashboards with SingleStore. Not only why the dashboard itself is useful, but how SingleStore makes it rich and interactive with real-time responsiveness. Why Dashboards Matter Dashboards serve two main functions: alerting and responding. With a real-time dashboard, the numbers that matter most are continuously in your face. For an e-commerce business, are sales at the level they should be? Is your website responsive to customers? For a parts supplier, are the trains running on time? That is, the ships and trains bringing in raw materials in bulk – and the trains, planes, and automobiles that carry finished goods to market. Real-time dashboards both show the latest data and tell what’s happening behind it. A well-crafted dashboard can tell you whether your business is working the way it should. Just as importantly, it can support you in drilling down on the data. How does order flow – or raw materials coming in across the loading dock – compare to a year ago? A month ago? The same day last week? When the dashboard shows you trouble, query support helps you drill down and find out whether you really have a problem – or an opportunity. It may sound challenging to develop and deliver a tool like this, and it is. But, until recently, many larger organizations have found that real-time dashboard capability is an impossible dream. That’s because large organizations that use legacy databases can’t get recent data in fast enough. One database handles transactions; there’s an ETL (extract, transform, and load) process; and another handles analytics. Often, one database or both are overloaded. The analytics capability, in particular, is often oversubscribed. So the real-time data needed to drive a dashboard simply can’t be accessed in a timely manner. In the webinar, the hosts talk about how real-time dashboards help businesses raise their data-driven decision-making game. Customers are able to deliver data when they need it, with continuous feeds, without impacting query performance associated with interactive dashboards. A proper real-time dashboard must deliver the best of both worlds, which is live operational data that is also responsive and interactive. SingleStore to the Rescue SingleStore is unique because it’s the leading relational database that has built-in SQL support. As such, you can handle transactions and analytics in a single database. No cumbersome ETL process separates the two. Processing times for the latest data go from days and hours to minutes and seconds. SingleStore serves as the central database connecting data inputs </ br> to analytics and queries. This is exactly what’s needed to power a real-time dashboard: the latest data, available now. SingleStore’s unique architecture enables a real-time dashboard design that can ingest, process, and make available the hundreds of megabytes and terabytes of data that are needed to run a modern enterprise effectively. How SingleStore Powers Real-Time Dashboards for Pandora Pandora is a great example of the importance of real-time dashboards. The latest, greatest, and most popular songs are the stock in trade of a business like Pandora. The hits list must literally be up to the minute. And it must then be mapped against the predilections of each and every visitor to the site. With SingleStore, Pandora is able to update their offering many times a day. Customers get better songlists and higher engagement with the service. Pandora uses SingleStore to power real-time data and analytics flows. The development and operations staff like SingleStore too. With SingleStore, they have standard ANSI SQL, which analysts and executives – and their business intelligence (BI) tools – know and love. But SingleStore operates fast, at any needed scale, unlike most other relational databases. Not only customer choices, but ad and clickstream data flows through the business seamlessly. Advertisers have the information they need to make smart ad buying decisions. The business has endless choices as to what data they want to see and instant feedback on what works. For More Information The highlights in this blog post may give you a flavor, but you can learn much more by viewing the webinar and downloading the slides, here . You may then want to consider trying the free tier of SingleStore.", "date": "2018-12-18"},
{"website": "Single-Store", "title": "free-download-designing-data-intensive-applications", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/free-download-designing-data-intensive-applications/", "abstract": "You can take advantage of a free download of key chapters from the exciting e-book, Designing Data-Intensive Applications . The chapters included in the free download cover two areas of vital concern to most SingleStore users, and indeed most developers and architects: transactions and streaming data. Designing Data-Intensive Applications, as a complete book, is more than 500 pages long. It takes as its premise that data is at the center of many of the challenges in system design today. (This is not a premise that is a surprise to us at SingleStore.) Author Martin Kleppman is an expert on this topic. He’s a researcher in distributed systems at Cambridge University. Before that, he was a software engineer tackling these topics at several companies, including LinkedIn and Rapportive. Martin is a frequent conference speaker, blogger, and contributor to open source projects. Besides the work under discussion here, his other currently available book is a free e-book on stream processing , which goes into depth on Apache Kafka and other streaming data platforms. Check it out! You’ll find that reading the excerpted chapters, the free e-book, or the complete Designing Data-Intensive Applications book will be good for you in every possible way. You’ll grow professionally and personally. Martin’s work is that good. However, we here at SingleStore will also challenge you to go beyond even Martin’s ambitious thesis and book. Rather than focusing on data-intensive applications as some kind of special category, as the author does, we believe that most applications under development today should be treated as data-intensive. That is, we believe that almost every application should be reconsidered, during design, around key data-related questions: What is the core data needed as input to this application? What additional data could usefully be gathered as part of the application? What is the short-term and long-term value of the core data? What is the short-term and long-term value of the potential, additional data? Where will the operational data store be? Where will the archival data store be? (Which database; in the cloud vs. on-prem; etc.) Do you have the data science and data analytics resources in-house to make the best use of the data? If not, should you consider licensing the data, partnering around it, and other means to ensure that you make full use of it? Once your data plans are fully in place, do any additional application opportunities open up? Machine learning and AI, in particular, bring these questions to life. You can’t do machine learning and AI without data; with machine learning, AI, and the data needed to power relevant and related applications, you may be able to accomplish things you had not previously believed to be achievable. We hope we’ve made the case that our free download for Designing Data-Intensive Applications is not only desirable, but a necessity, to help you prepare for the future use of data in your applications, and that you download the free chapters today.", "date": "2018-12-01"},
{"website": "Single-Store", "title": "media-analytics-memsql-kollective-and-looker", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/media-analytics-memsql-kollective-and-looker/", "abstract": "Our webinar brought together three seasoned presenters: Mike Boyarski from SingleStore; Garrett Gladden from Kollective, which helps customers distribute video at enterprise scale; and Erin Franz from leading analytics platform company Looker. They described the creation of a fast, scalable, and affordable solution for media analytics. You can view the webinar recording and download the presentation slides here . A few months ago, SingleStore presented a webinar on building a media analytics solution. We’re taking the time now to highlight some of the key points from that webinar, both to share valuable information here and to encourage you to view the webinar. Today, enterprises are finding themselves in the video delivery business. They need to deliver a fast, flexible, and reliable video experience for customers, employees, suppliers, and partners. And they need analytics. Once upon a time, television broadcasters had only Nielsen ratings to help them estimate who was watching. Today, digital delivery promises the ability to have much more useful and detailed metrics as to who watches what, and for how long, and with what messages delivered. However, delivering a YouTube-like experience to users, and a fast, flexible, and rich analytics environment for internal use, is still more than a notion. Together, SingleStore can move video data internally; Kollective can help deliver it to users and generate reams of statistics; and Looker can make the data easy to report on, analyze, and explore. We offer some highlights in this blog post. To get the full story, you can view the webinar recording and download the presentation slides here . How SingleStore Moves Data SingleStore is the best scalable relational database , top-ranked as both a general purpose database and a database for data warehousing. The company, which is growing fast, is ranked as a Challenger in the Gartner Magic Quadrant for Data Management Solutions, with a solid ability to execute and steadily increasing completeness of vision. SingleStore delivers, together, several things that are usually only available separately: speed, scalability, and SQL. Traditional relational databases aren’t speedy and aren’t scalable. NoSQL databases are speedy and scalable, but they’re not relational; therefore, as the name implies, SQL is not an option. The lack of relational structure and of SQL support for queries is a stopper for most analytics use cases. SingleStore’s unified architecture brings transactions and analyticsinto a single fast, scalable, relational database. With SingleStore, Kollective can drive live data into a SingleStore-based operational data store. Looker can deliver a combination of live and historical data out, with otherwise unachievable speed and reliability. The whole architecture is fully scalable – no sudden cliffs to climb (or to fall off of) when an enterprise needs more capability. How Kollective Delivers Data Kollective has a simple approach to developing and delivering their offering: put the customer first. And clients who were delivering data need three simple things: Smarter networks . Delivering data reliably and scalably requires a fast, flexible network. Kollective customers need to know about bottlenecks in advance so operations can deliver a richly immersive experience. Scoreboards and dashboards . Kollective customers want to know that video is being delivered effectively, and where problems are cropping up, both in real time and retrospectively. Baselines and averages . When a Kollective customer is getting good at video delivery, they want to see their numbers improving. When a Kollective customer gets good at video delivery, they want to keep it up, while managing costs. Kollective delivers all this across a varied landscape of incoming operational and business data. Using Kafka as a unified messaging solution, they gather data from a wide range of inputs. Kollective gathers data from a wide range of information sourcesand delivers it in a uniform and usable format. When customers reach out to Kollective, they tend to have slow, static, inflexible architecture for incoming data. There’s little in the way of a user experience. How to bring all this together? Kollectiv often uses SingleStore for data aggregation and delivery and Looker for a rich user experience in reporting, analytics, and exploration of data. SingleStore, Kollective, and Looker make media reportingand analytics easy and even fun. Looker Helps Bring Intelligence to Business Legacy solutions often try to do too much while running off of an out-of-date combination of technological underpinnings. Looker takes full advantage of the best of modern technologies to deliver exactly what users need: the ability to easily explore, analyze, and share real-time business analytics. Looker does not make their customers go through yet another data transformation process before their data is deemed ready for analytics. They operate directly against the user’s database. Of course, it helps if that underlying database is fast, scalable, and relational – three qualities that are not usually found together, but that are unified in SingleStore. Backed by SingleStore’s speed and flexibility, Looker excelsat delivering valuable analytical information. With Looker and SingleStore running together, all data transformation can be accomplished in the SingleStore database. Looker leverages SingleStore’s fast, in-memory performance and its ability to stay constantly updated with the latest data. Looker, in turn, makes the data stored in SingleStore available to users in a customized and user-friendly way. Because SingleStore combines transactions and analytics support, it serves as a single source of truth for users. The LookML modeling layer can directly leverage newer, advanced SingleStore functionality such as geospatial functions and the ability to store and query semi-structured JSON and Avro data . Drawing on SingleStore data, Looker also serves as a connecting point for machine learning and AI . Conclusion SingleStore, Kollective, and Looker each deliver novel, up-to-date solutions based on the latest advances in technology. Together, they deliver a robust toolset for video data delivery and analytics. You can view the webinar recording and download the presentation slides here .", "date": "2018-12-26"},
{"website": "Single-Store", "title": "how-memsql-works", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/how-memsql-works/", "abstract": "What makes SingleStore unique? Its ability to combine the scalability of NoSQL databases and the familiar structure, transaction capabilities, and SQL support of traditional relational databases. SingleStore does all this while offering high performance on ingest, fast query response, and high concurrency – support for many active users at the same time. Because of its SQLs support, SingleStore is “plug and play” with existing database systems and with all the people and tools that are part of the SQL ecosystem. The combination of capabilities that are offered in SingleStore are not found together anywhere else. In this blog post we show the underlying features of SingleStore that allow it to offer all of these capabilities at the same time. After you read this blog post, you will be able to assess SingleStore as a solution for your own database-related issues. Introduction to SingleStore SingleStore describes its database software as the No-Limits Database™ because it’s able to scale massively. SingleStore is a distributed data store that allows for no limits in terms of the number of concurrent queries you have on the environment. SingleStore can handle transactions and analytics, simultaneously, all in one system. Ultimately what you get with SingleStore is a distributed datastore with relational SQL that gives you a lot of the efficiencies that distributed systems provide, all running on commodity standard hardware, as with NoSQL databases. But with SingleStore, you also get the SQL structure that so many organizations depend on. The architecture of SingleStore has many similarities to existing NoSQL databases, while also having a relational database structure and SQL support, among other important differences: Scale-out architecture . True distributed processing across arbitrary numbers of servers, easily scaling CPU capacity, RAM, and disk storage. The ability to run on commodity hardware and in the cloud . For SingleStore, any machine or cloud instance that can run Linux can run SingleStore. High-speed ingest . For a typical NoSQL database, ingest is only in batch mode, whereas ingest with SingleStore can handle both batch and streaming data and in-line data transformations via SingleStore Pipelines . Flexible data formats . SingleStore supports structured, relational data that fully supports SQL queries, as well as semi-structured data such as JSON , AVRO, text, and spatial data; NoSQL databases can also handle unstructured data. Tiered storage . Most NoSQL databases are disk-first, with some support for in-memory operations. SingleStore includes both and allows you to flexibly manage memory vs. disk usage. SingleStore, Feature by Feature SingleStore’s unique architecture allows it to offer a combination of capabilities that isn’t available elsewhere. Here’s a quick summary: Tiered storage across memory and disk. Each uses a separate table type. A query optimizer that works across table types. A memory and CPU manager with the ability to segregate user groups. Distributed ingest which is lock-free across reads and writes, for bulk and streaming ingest, using a skiplist index . A management console with a GUI (SingleStore Studio) and command-line tools. Multi-cluster management . Enterprise security with RBAC, encryption, auditing, and strict mode to isolate data from administrators. A SQL API . Following are highlights of these capabilities. Tiered Storage, Memory to Disk SingleStore takes advantage of tiered storage , which means it can spill data to memory and/or to disk. So you can have the choice of taking advantage of SingleStore’s memory-optimized rowstore tables for very fast, efficient query processing or ingestion. However, SingleStore also runs queries on disk, using columnstore tables. For both table types, everything is distributed – so SingleStore is highly elastic, and allows for scale-out processing of that data. Query Optimizer across Rowstore and Columnstore Tables SingleStore has a SQL query optimizer that runs on the two different table types, row-based and column-based . This gives you the ability to do transactional processing, analytic processing, or both at the same time, using the best table structure for each workload. Workload Management for CPU and RAM On top of the query optimizer, SingleStore has a memory and CPU management governor. Part of that is a workload manager. It allows you to identify how to segment processes or users so that ultimately your query processing can prevent outages. If you’re going to run out of memory, for example, SingleStore can help ensure that that doesn’t happen. Or you can segment users so that you can have one class of user or application get a certain amount of resources. It’s a very sophisticated workload manager that lets you get the best out of the system. Distributed Ingest, Bulk or Streaming, Lock-free SingleStore offers a lock-free architecture. This is based on the skiplist index, which does a very efficient job of doing transactions and updates without necessarily locking or blocking other reads and/or writes. That allows SingleStore to deliver bulk and/or streaming ingestion. So transactional support on SingleStore is similar to any NoSQL system, in that both SingleStore and a NoSQL system can do continuous loading. The difference is that SingleStore is ACID compliant . SingleStore carries every transaction all the way to the disk, so there’s no risk of data loss. SingleStore has a classic logging mechanism, so we can do HA configurations and replication of transactions to other nodes or clusters to ensure availability. Multi-Cluster Management We have a full management console called SingleStore Studio . It’s a GUI-based environment. We also have command-line utilities that gives you functions that can deploy, manage, repartition, rebalance nodes, all sort of built into the system. The Studio product and command-line tools make it very easy to identify exactly where bottlenecks are. You can do things like query planning analysis, where you can inspect where query bottlenecks might be and troubleshoot those. All through a built-in, web-based monitoring environment. Enterprise Security with RBAC, Encryption, Auditing, and Strict Mode SingleStore is very stringent in its support for security. As a company, we have a number of financial services and public sector customers that require the highest levels of security. So SingleStore security includes role-based access control (RBAC), encryption, and support for auditing. We have another component, called “strict mode”, which imposes a separation of concerns model. So your administrators cannot have visibility to data, but they can help administer the environment. SQL API and Queries Lastly, what you get with SingleStore is a SQL API. Other distributed databases, forthrightly called NoSQL databases , lack this essential component. (As well as several of the other features listed above.) The SingleStore database is relational SQL through and through. SingleStore’s table structures are all in relational format. That is ultimately what gives you that easy to work with data set that a lot of data analysts and folks in the business community enjoy. The SingleStore Ecosystem This is a summary of the SingleStore ecosystem in terms of the type of ingestion mechanisms that SingleStore supports, notably including Kafka and Spark streaming. SingleStore supports an HDFS Pipeline connector. We have SingleStore Pipelines that allow you to essentially ingest data as data lands in the source system. Pipelines to Stored Procedures give Pipelines even more power. So the notion of continuous loading from a relational database, a Hadoop cluster, an Amazon S3 database or other sources is supported by SingleStore’s architecture. In the diagram, you can see that we have the two different table types, rowstore and columnstore. Lastly, you can connect a number of off-the-shelf business intelligence (BI) tools, such as Tableau or Looker, to build your dashboards and reports. SingleStore runs on any Linux-based system, whether that’s on-premises or AWS, Azure, or Google’s cloud service. You can run anywhere that there’s a Linux-based environment. Conclusion You can see a version of this blog post as a presentation within the recent webinar, Hadoop Acceleration Strategies for Broader Use and Simplicity, led by Mike Boyarski. To learn more, click here to view the webinar and download the slides.", "date": "2018-12-28"},
{"website": "Single-Store", "title": "memsql-live-nikita-shamgunov-on-the-data-engineering-podcast", "author": ["David Gomes"], "link": "https://www.singlestore.com/blog/memsql-live-nikita-shamgunov-on-the-data-engineering-podcast/", "abstract": "SingleStore CEO Nikita Shamgunov recently joined host Tobias Macey on the Data Engineering Podcast. Tobias drew out Nikita on SingleStore’s origins, our exciting present, and where we’re going in the future. In this blog post, we’ll try to get you excited about some of the business and engineering topics discussed in the podcast, but we urge you to listen to the full episode . For your interest, here is a full list of the topics – with a few of them highlighted in this blog post: What is SingleStore and how did it start? What are the typical use cases for SingleStore? Why did we build data ingest in the database? The SingleStore architecture SingleStore performance Building SingleStore: how much open source do we use? SingleStore and the MySQL wire protocol How to set up SingleStore Challenges faced by customers when migrating to SingleStore Elasticity of SingleStore SingleStore licensing model Challenges with building a business around SingleStore When is SingleStore not the right choice The future of SingleStore Modern tooling for data management Now, let’s dive into some of the business and technical concepts that were discussed in the podcast as a way to get you excited to listen to the entire thing . Note : You might also be interested to read this recent interview with Nikita on HackerNoon. How SingleStore Started Nikita grew up and was educated in Russia, where he earned his PhD in computer science and became involved in programming competitions, winning several. (Quite a few SingleStore engineers have shared the same passion, including me.) Shortly after graduation, Nikita was hired by Microsoft. He flew to Seattle to join the SQL Server team and spent five years there. Nikita describes his early days as a “trial by fire,” followed by a “death march”: testing and debugging SQL Server 2005, a landmark release that was five years in the making. The SQL Server effort was a “very, very good training ground.” It was also where the seeds of SingleStore were planted. Internally, Microsoft worked on a scalable transactional database, knowing that it was sorely needed – but never shipped it. Nikita and Microsoft colleague Adam Prout, who became the chief architect and co-founder, with Nikita, of SingleStore, found that “incredibly frustrating”. After Microsoft, Nikita joined Facebook, where “it became very apparent to me what the power and the value of such a system is.” So Nikita co-founded SingleStore in 2013, and the team soon delivered the first widely usable distributed transactional system. SingleStore started out as an in-memory system. This “allowed us to onboard various high value workloads” and to start building a business. Later, SingleStore added disk-based columnstore capability, making it a very flexible, memory-led database. Early in SingleStore’s history, most of the workloads that fit the new system were around analytics on live data. SingleStore can handle “a high volume of inserts, updates, deletes hitting the (transactional) system, but at the same time actually drive insights, drive analytics”. But, notes Nikita, “the amount of technology that goes into a product like this is absolutely enormous.” That meant it would take time to complete the full vision: a “fully scalable, elastic transactional system that can support transactions and analytics in one product.” Note : The photograph below comes from a talk by Nikita at the SQL NYC group in 2017. The talk is available on YouTube . SingleStore CEO Nikita Shamgunov speaks ondatabase convergence at SQL NYC, 2017 Key Architectural Decisions Nikita discusses many of the key points of SingleStore’s architecture. When the work on SingleStore was started, Nikita and the rest of the team bet on RAM as the storage of the future, since it was predicted that the cost of RAM would go down by a lot. This hasn’t really happened for RAM as such, though flash memory is now dropping rapidly in cost, so SingleStore’s architecture had to evolve to support both disk and memory. Nikita also explains how SingleStore’s distributed nature is key for its scalability. Nikita goes on to answer a very interesting question about SingleStore and the consistency, availability, and partition tolerance (referred to as the CAP theorem ) of distributed database design – which holds that you can only have two out of the three. Partition tolerance is held to be mandatory in a distributed system. In his answer, Nikita explains why SingleStore is not fully available (the A in CAP), in order to guarantee consistency and partition tolerance (the C and P). “The greatest sin of a database is to lose data”, explains Nikita, making the choice of building a CP system very natural. Nikita also explores the ins and outs of converting from row-oriented data to column-oriented data (and vice-versa) in SingleStore. Since SingleStore supports row-oriented tables in memory and column-oriented tables on disk, but clients (e.g. SQL clients) expect row-oriented data, one might argue that the conversion from rowstore to columnstore could affect SingleStore’s performance. However, that’s not (usually) the case, as Nikita explains in the podcast. Succinctly, the conversion from rowstore to columnstore when data is being inserted is actually I/O-bound and not, perhaps surprisingly, CPU-bound. Since CPUs are so fast at converting from one format to the other, disk performance actually matters most, and thus the conversion overhead becomes much less relevant. Secondly, since most columnstore queries are analytical in nature (e.g. reporting queries), the number of results (in number of rows) is usually quite low. This means that most query operations will be applied on internal representation of columnstore data as vectors, which is very performant. One exception to this performance is when clients request a lot of columnstore data. Optimizing Performance Nikita was asked about performance optimization in SingleStore. This question touches on so many areas that Nikita gave an answer in which he organized some of SingleStore’s key performance optimizations into three separate buckets: In-memory query optimization – for queries against in-memory data, careful code generation is fundamental for amazing performance Network optimization — why and how SingleStore engineers are super careful about data serialization and deserialization across the cluster Compression and vectorization — how SingleStore has built a general purpose system for performing computations on encoded data. Not much of this performance optimization, or other parts of SingleStore, are built with open source code. The vast majority of SingleStore’s code was written internally. SingleStore does everage a few open source libraries for some geospatial capabilities ( S2 Geometry ), columnstore compression, and query parsing/binding (from PostgreSQL source code). Setting Up and Running SingleStore Tobias asks Nikita about setting up SingleStore. Nikita replies: “Setting up and deploying is trivial. Once you commit to a certain size of the cluster, it’s basically a push-button experience, and then SingleStore will be running on that cluster.” The main obstacle to implementing SingleStore is “understanding the distributed nature of the system.” However, “once you realize the value, then not only do we have passionate fans in the enterprise – they tend to get promoted too.” One key innovation is SingleStore Pipelines . “If you have data coming from Kafka or you have data in S3 or HDFS, you can literally type one command, and this data is flowing into SingleStore in real-time with arbitrary scale.” With the addition of Pipelines to Stored Procedures , you can add much more processing, running at high speed. With Pipelines, says Nikita, “We pulled in data ingest as a first-class citizen into the engine… We have customers that are moving data into SingleStore tables at multiple gigabytes a second.” SingleStore Pipelines can handle extract, transform,and load (ETL) operations quickly and smoothly SingleStore is notable for combining scalability and a very high level of performance – which were, for a time, only available from NoSQL databases – with ANSI SQL support. “It gives the user the ability to have as much structure as the user needs and wants, but at the same time it doesn’t sacrifice performance.” The advantages include “compatibility with BI tools… you can easily craft visualizations, and it simplifies building apps as well.” Typical Use Cases Continuing the interview, Tobias asks Nikita about typical use cases for SingleStore. Nikita replies in detail: “At the very, very high level, we support general purpose database workloads that require scale. We give you unlimited storage and unlimited compute and in one product. We can scale your applications in an elegant way.” Nikita cites real-time dashboards, Web 2.0 and mobile applications, portfolio analysis, portfolio management, and real-time financial reporting as some specific applications that need “to deliver a great user experience and lower latencies,” thus requiring nearly limitless compute capability – that is, a fully scalable system. For data warehousing, with SingleStore, “you don’t need to extract your data and put it into the data warehouse; you do it on the data that sits in the database. So the advantage is really simplicity.” There are also “use cases by industry.” SingleStore started out being “most successful in financial services and media,” with “health care coming more and more into the platform.” The Future of SingleStore Finishing up the interview, Tobias presses Nikita on the need for SingleStore developers to choose row-store, in-memory tables vs. column-store, disk-based tables for different data. In the podcast, Nikita explains the trade-offs in detail, then goes further. “In the future, I believe databases will get to a place where there are no choices, you just create tables. You have a cloud service, you have a SQL API to it, and that SQL API has endless capacity. You pay for better latencies and better throughputs.” Nikita continues, “On the way toward that vision, we’re investing in managed services and Kubernetes integration. We are constantly strengthening the engine itself, making it more elastic, making it more robust, having the ability to run on multiple data centers, and improving the developer experience.” Also, “AI and ML workloads are rising exponentially, but they need to store data somewhere, and there is no clear choice for that just yet. That’s an exciting world, and we are absolutely going to be part of that.” Tune In for More While these highlights are intriguing, there’s much more depth and breadth in the full podcast episode . We encourage you to listen to the episode and subscribe to the Data Engineering Podcast , which is available on Downcast, Instacast, iTunes, Podgrasp, Spotify, and other platforms.", "date": "2019-01-04"},
{"website": "Single-Store", "title": "porting-30k-lines-of-code-from-flow-to-typescript", "author": ["David Gomes"], "link": "https://www.singlestore.com/blog/porting-30k-lines-of-code-from-flow-to-typescript/", "abstract": "We recently ported the 30,000 lines of JavaScript code in SingleStore Studio from Flow to TypeScript . In this article, I describe why we ported our codebase, how we carried out the process, and how it has been working out for us. I’d like to start by saying that my goal with this blog post is not to condemn Flow or usage of Flow. I highly admire the project, and I think that there is enough space in the JavaScript community for both type checkers. At the end of the day, each team should study all their options and pick what’s best for them. I sincerely hope this article helps you with that choice. Let’s begin by setting some context. Here at SingleStore, we are big fans of statically and strongly typing our JavaScript code. This allows us to avoid many of the problems associated with dynamic and weak typing, such as: Runtime type errors, due to different parts of the code not agreeing on implicit type contracts. Wasted developer time, due to the need to write tests for trivial things such as parameter type checking. Increased bundle size caused by runtime type checking. The lack of editor/IDE integration for dynamically and weakly typed code, because editor/IDE features such as jump-to-definition, mechanical refactoring, and others work better with statically and strongly typed code. The inability to write code based on a data model. With statically and strongly typed code, we can base our code on a data model. We can design our data types first, then much of our code basically just “writes itself”. These are just some of the advantages of static typing. I describe a few more in a recent blog post about Flow . (Note: A previous version of this blog post appeared on the author’s personal blog .) Starting with Flow In early 2016, we started using tcomb to ensure some runtime type safety in one of our internal JavaScript projects (disclaimer: I was not a part of that project). While runtime type checking is sometimes useful, it doesn’t doesn’t even begin to scratch the power of static typing [ 1 ] . With that in mind, we decided to start using Flow for another project we started in 2016. At the time, Flow was a great choice because: Flow is backed by Facebook, which has done an amazing job at growing React and the React community. (They also develop React using Flow). We didn’t have to buy into an entirely new ecosystem of JavaScript development. Dropping Babel for tsc (TypeScript compiler) was scary because it wouldn’t give us the flexibility to switch to Flow or another type checker in the future. (Obviously, this has changed since then). We didn’t have to type our entire codebase from the beginning, which allowed us to get a feel for statically typed JavaScript before we went all-in. Rather, we could just type a subset of the files. Nowadays, both Flow and TypeScript allow you to do this. TypeScript, at the time, was lacking some basic features that Flow already supported, such as lookup types , generic parameter defaults , and others. When we started working on SingleStore Studio in late 2017, we set out to achieve full type coverage across the entire application. (All of it is written in JavaScript, and both the frontend and backend run inside the browser). We decided to continue using Flow for this project, as that’s what we had been successfully using in the past. However, Babel 7 being released with TypeScript support definitely got my attention. This release meant that adopting TypeScript no longer meant buying into the entire TypeScript ecosystem, and that we could keep using Babel to emit JavaScript. More importantly, this meant that we could actually use TypeScript as a type checker , and not so much as a “language” per se. Personally, I consider that separating the type checker from the emitter is a more elegant way of achieving static (and strong) typing in JavaScript because: It’s a good idea to have some separation of concerns between what emits ES5 and what does type checking. This allows for less lock-in around type checkers and it accelerates development speed; if the type checker is slow for whatever reason, your code will still be emitted right away. [ 2 ] Babel has amazing plugins and great features that TypeScript’s emitter doesn’t have. As an example, Babel allows you to specify which browsers you want to support, and it will automatically emit code that is valid on those browsers. This is very complicated to implement, and it makes more sense to only have Babel implement it, instead of duplicating this effort in two different projects. Except for the lack of static typing, I like JavaScript as a programming language. I trust that ECMAScript will be around for a good long while, whereas I have no idea how long TypeScript will be around for. For this reason, I prefer to keep writing and “thinking” in JavaScript. (Note that I always say “using Flow” or “using TypeScript”, instead of “in Flow” or “in TypeScript”, because I always think about these two projects as tools and not as full programming languages). There are some downsides to this approach, of course: The TypeScript compiler could theoretically perform bundle optimizations based on types, and you are missing on that by having a separate emitter and type checker. Project configuration becomes a bit more complicated when you have more tools and development dependencies. I think this is a weaker argument than most people make of it, because having both Babel + Flow was never a source of configuration issues in our projects; we expect this to continue with the move to TypeScript. Investigating TypeScript as an Alternative to Flow I had been noticing a growing interest in TypeScript in both online and local JavaScript communities. As such, when I first found out that Babel 7 supported TypeScript, I started investigating a potential move away from Flow. On top of the growing interest in TypeScript, we had encountered various frustrations with Flow: Lower quality editor/IDE integrations (when compared to TypeScript). Nuclide (Facebook’s own IDE, which had the best Flow integration) being deprecated did not help. Smaller community [ 3 ] and therefore fewer and overall lower quality type definitions for various libraries (more on this later). Lack of a public roadmap, and little interaction between the Flow team at Facebook and the community. You can read this comment by a Facebook employee for some more details. High memory consumption and frequent memory leaks — various engineers in our team have experienced Flow taking up almost 10 gigabytes of RAM every now and then. Of course, we also had to research whether TypeScript was sufficient for us. This was very complicated, but it involved a thorough reading of the documentation that helped us figure out that every feature in Flow has an equivalent in TypeScript. I then investigated the TypeScript public roadmap and was delighted with the features that lay ahead (e.g. partial type argument inference , which is a feature we used in Flow). Porting 30K+ Lines of Code from Flow to TypeScript The first step to actually porting all of our code from using Flow to TypeScript was to upgrade Babel from 6 to 7. This was somewhat straightforward, but it took us around two engineer days, since we decided to also upgrade from Webpack 3 to 4 at the same time. Since we have some legacy dependencies vendored in our source code, this was harder for us than it should be for the vast majority of JavaScript projects. After upgrading was done, we were able to replace Babel’s Flow preset with the new TypeScript preset, then run the TypeScript compiler for the very first time against our full source code written using Flow. It resulted in 8245 syntax errors . (The tsc CLI doesn’t give you the real errors for the full project until you have 0 syntax errors.) This number scared us (a lot) at first, but we quickly figured out that most of these errors were related to TypeScript not supporting .js files. After some investigation, I found out that TypeScript files have to end in either “.ts” or “.tsx” (if they have JSX in them). I don’t want to think about whether a new file I’m creating should have a “.ts” or “.tsx” extension, and I think that’s a poor developer experience. For that reason, I just renamed every single to “.tsx”. (Ideally, all of our files would have a “.js” extension like in Flow, but I would also be okay with “.ts”). After that change, we had around 4000 syntax errors – about half as many. Most of them were related to import type , which can be replaced with just “import” using TypeScript and also sealed object notation in Flow ( {||} vs {} ). After a couple of quick RegExes, we were down to 414 syntax errors. These remaining errors had to be manually fixed: The existential type that we use for partial generic type argument inference had to be replaced with either explicitly naming the various type arguments or using the unknown typeto tell TypeScript that we don’t care about some of the type arguments. The $Keys type and other Flow advanced types have a different syntax in TypeScript (e.g. $Shape<\\> corresponds to Partial<\\> in TypeScript). After all the syntax errors were fixed, tsc (the TypeScript compiler) finally told us how many real type errors our codebase had — just around 1300. This is when we had to sit down and decide whether it made sense to keep going or not. After all, if it would take us weeks of development time, it could not be worth it to go forward with the port. However, we figured it should take us less than 1 week of a single engineer’s time to perform the port, so we charged ahead. Note that during the transition, we had to stop other work on this codebase. However, it should be possible to contribute new work in parallel to such a port — but you’ll have to work on top of potentially hundreds of type errors, which will not be an easy feat. What Were All These Type Errors? TypeScript and Flow make different assumptions about many different things, which in practice means that they let your JavaScript code do different things. Flow is more strict about some things, and TypeScript is more strict about other things. A full in-depth comparison between the two type checkers would be really long, so in this blog post we’ll just study a few examples. Note: all the TypeScript playground links in this article assume that all the “strict” settings have been turned on. However, unfortunately, when you share a TypeScript playground link, those settings are not saved in the URL. For this reason, you have to manually set them when you open any TypeScript playground link from this article. invariant.js A very common function in our source code is the invariant function. I can’t explain it any better than the documentation does, so I’ll just quote it here: var invariant = require('invariant');\n\ninvariant(someTruthyVal, 'This will not throw');\n// No errors\n\ninvariant(someFalseyVal, 'This will throw an error with this message');\n// Error raised: Invariant Violation: This will throw an error with this message The idea is very simple — a simple function that will potentially throw an error based on some condition. Let’s see how we could implement it and use it with Flow: type Maybe = T | void;\n\nfunction invariant(condition: boolean, message: string) {\n  if (!condition) {\n    throw new Error(message);\n  }\n}\n\nfunction f(x: Maybe, c: number) {\n  if (c > 0) {\n    invariant(x !== undefined, \"When c is positive, x should never be undefined\");\n\n    (x + 1); // works because x has been refined to \"number\"\n  }\n} Now let’s run the exact same snippet through TypeScript. As you can see in the link, we get an error from TypeScript, since it can’t figure out that “x” is actually guaranteed to not be undefined on the last line. This is actually a known issue with TypeScript — it can’t perform this type of inference through a function (yet). However, since it’s a very common pattern in our code base, we had to replace every instance of invariant (over 150 of them) with more manual code that just throws an error in-place: type Maybe = T | void;\n\nfunction f(x: Maybe, c: number) {\n  if (c > 0) {\n    if (x === undefined) {\n      throw new Error(\"When c is positive, x should never be undefined\");\n    }\n\n    (x + 1); // works because x has been refined to \"number\"\n  }\n} This is not as nice as invariant , but it’s not a huge deal either. $ExpectError vs @ts-ignore Flow has a very interesting feature that is similar to @ts-ignore except that it will error if the next line is not an error. This is very useful for writing “type tests” which are tests that ensure that our type checker (be it TypeScript or Flow) is finding certain type errors that we want it to find. Unfortunately, TypeScript does not have this feature, which means that our type tests lost some value. It’s something that I’m looking forward to TypeScript implementing. General Type Errors and Type Inference Often times, TypeScript can be more explicit than Flow, as in this example : type Leaf = {\n  host: string;\n  port: number;\n  type: \"LEAF\";\n};\n\ntype Aggregator = {\n  host: string;\n  port: number;\n  type: \"AGGREGATOR\";\n}\n\ntype SingleStoreNode = Leaf | Aggregator;\n\nfunction f(leaves: Array, aggregators: Array): Array {\n  // The next line errors because you cannot concat aggregators to leaves.\n  return leaves.concat(aggregators);\n} Flow infers the type of leaves.concat(aggregators) to be Array, which can then be cast to Array<SingleStoreNode\\> . I think this is a good example of how sometimes Flow can be a little smarter, whereas TypeScript sometimes needs a little bit of help. (We can use a type assertion to help TypeScript in this case, but using type assertions is dangerous and should be done very carefully). Even though I have no formal proof that allows me to state this, I consider Flow to be quite superior to TypeScript around type inference. I’m very hopeful that TypeScript will get to Flow’s level, seeing as it is very actively developed, and as many recent improvements to TypeScript have been in this exact area. Throughout many parts of our source code, we had to give TypeScript a bit of help via annotations or type assertions (though we avoided type assertions as much as possible). Let’s look at one more example ; we had perhaps over 200 instances of this type of error: type Player = {\n    name: string;\n    age: number;\n    position: \"STRIKER\" | \"GOALKEEPER\",\n};\n\ntype F = () => Promise<Array>;\n\nconst f1: F = () => {\n    return Promise.all([\n        {\n            name: \"David Gomes\",\n            age: 23,\n            position: \"GOALKEEPER\",\n        }, {\n            name: \"Cristiano Ronaldo\",\n            age: 33,\n            position: \"STRIKER\",\n        }\n    ]);\n}; TypeScript will not let you write this because it can’t let you cast { name: \"David Gomes\", age: 23, type: \"GOALKEEPER\" } as an object of type Player (open the Playground link to see the exact error). This is another instance where I consider TypeScript to not be “smart enough” (at least when compared to Flow, which understands this code ). In order to make this work, you have a few options: Assert “STRIKER” as “STRIKER” so that TypeScript understands that the string is a valid enum of type \"STRIKER\" | \"GOALKEEPER\" . Assert both objects as Player . Or what I consider to be the best solution, just help TypeScript without using any type assertions by writing Promise.all<Player\\>(...) . Another example is the following (TypeScript), where Flow once again comes out as having better type inference : type Connection = { id: number };\n\ndeclare function getConnection(): Connection;\n\nfunction resolveConnection() {\n  return new Promise(resolve => {\n    return resolve(getConnection());\n  })\n}\n\nresolveConnection().then(conn => {\n  // TypeScript errors in the next line because it does not understand\n  // that conn is of type Connection. We have to manually annotate\n  // resolveConnection as Promise.\n  (conn.id);\n}); A very small but nevertheless interesting example is that Flow types Array<T\\>.pop() as T , whereas TypeScript considers that it is T | void . This is a point in favor of TypeScript, because it forces you to double check that the item exists (if the array is empty, Array.pop returns undefined ). There are some other small examples like this one where TypeScript outshines Flow. TypeScript Definitions for Third-Party Dependencies Of course, when writing any JavaScript application, the chances are you’ll have at least a handful of dependencies. These need to be typed, otherwise you’re losing out on much of the power of static type analysis (as explained in the beginning of this article). Libraries that you import from npm can ship with Flow type definitions, TypeScript type definitions, both of these, or neither. It’s very common that (smaller) libraries don’t ship with either meaning, so that you have to either write your own type definitions for them or grab some from the community. Both the Flow and the TypeScript community have a standard repository of third-party type definitions for JavaScript packages: flow-typed and DefinitelyTyped . I have to say that we had a much better time with DefinitelyTyped. With flow-typed, we had to use its CLI tool to bring in type definitions for various dependencies into our project. DefinitelyTyped has figured out a way to merge this functionality with npm’s CLI tool by shipping @types/package-name packages in npm’s package repository. This is amazing, and made it much easier to bring in type definitions for our dependencies (jest, react, lodash, and react-redux, just to name a few). On top of this, I also had a great time contributing to DefinitelyTyped. (But don’t expect the type definitions to be equivalent when porting code from Flow to TypeScript.) I’ve already sent a couple of pull requests ( here , here , and here ) and all of them were a breeze. Just clone, edit the type definitions, add tests, and send a pull request. The DefinitelyTyped GitHub bot will tag people who have contributed to the type definitions you edited for reviews. If none of them provide a review in 7 days, a DefinitelyTyped maintainer will review the PR. After getting merged to master, a new version of the dependency’s package is shipped to npm. For instance, when I first updated the @types/redux-form package, the version 7.4.14 was automatically pushed to npm after it got merged to master. This makes it super easy for us to just update our package.json file to get the new type definitions. If you can’t wait for the PR to be accepted, you can always override the type definitions that are being used in your project, as I explained in a recent blog post . Overall, the quality of type definitions in DefinitelyTyped is much better, due to the larger and more thriving community behind TypeScript. In fact, our type coverage increased from 88% to 96% after porting our project from Flow to TypeScript, mostly due to better third-party dependency type definitions that have fewer any types in them. Linting and Tests We moved from eslint to tslint (we found it more complicated to get started with eslint for TypeScript, so we just went with tslint). We are using ts-jest for running our tests that are using TypeScript. Some of our tests are typed, whereas others are untyped. (When it’s too much work to type tests, we save them as .js files.) What Happened after we Fixed all of our Type Errors? After one engineer week of work we got down to the very last type error, which we postponed for the short term with @ts-ignore . After addressing some code review comments and fixing a couple of bugs – unfortunately, we had to change a very small amount of runtime code to fix some logic that TypeScript could not understand – the PR landed, and we have been using TypeScript since then. (And yes, we fixed the final @ts-ignore in a followup PR). Apart from the editor integration, working with TypeScript has been very similar to working with Flow. The performance of Flow’s server is slightly faster, but this doesn’t turn out to be a huge problem, since they are equally fast at giving you inline errors for the file you’re currently looking at. The only performance difference is that TypeScript takes a little bit longer (~0.5 to 1 second) to tell you whether there are any new errors in your project after you save a file. The server startup time is about the same (~2 minutes), but that doesn’t matter as much. So far, we haven’t had any issues with memory consumption, and tsc seems to consistently use around 600 megabytes of RAM. It may seem that Flow’s type inference makes it much better than TypeScript, but there are two reasons why that isn’t a big deal: We converted a codebase that was adapted to Flow to TypeScript. This means that we obviously only found things that Flow can express but TypeScript can’t. If the port had been the other way around, I’m sure we would have found things that TypeScript can infer/express better than Flow. Type inference is important and it helps keep our code less verbose. However, at the end of the day, things like a strong community and availability of type definitions are more important, because weaker type inference can be solved by “handholding” the type checker a bit more. Code Statistics $ npm run type-coverage # https://github.com/plantain-00/type-coverage\n43330 / 45047 96.19%\n\n$ cloc # ignoring tests and dependencies\n--------------------------------------------------------------------------------\nLanguage                      files         blank       comment           code\n--------------------------------------------------------------------------------\nTypeScript                      330         5179        1405          31463 What’s Next? We’re not done with improving the static type analysis in our code. We have other projects at SingleStore that will eventually drop Flow in favor of TypeScript (and some JavaScript projects that may start using TypeScript), and we want to make our TypeScript configuration stricter. We currently have “ strictNullChecks ” turned on, but “ noImplicitAny ” is still disabled. We’re also going to remove a couple of dangerous type assertions in our code. I am excited to share other things I learned in my adventures with JavaScript type systems in future blog posts. If there is a topic you would like to see me cover, please let me know! Footnotes Combining static typing with runtime type checking sounds like it could be interesting for certain use cases, and io-ts allows for this with tcomb and TypeScript, but I have never tried it. ↩︎ If you’re using tsc by itself with Babel, you can actually configure it to achieve this same behavior. ↩︎ As of now, the DefinitelyTyped repository has 19682 GitHub stars, compared to 3070 in the flow-typed repository. ↩︎", "date": "2019-01-15"},
{"website": "Single-Store", "title": "case-study-insite360-memsql-iot-cloud", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/case-study-insite360-memsql-iot-cloud/", "abstract": "Insite360 is a complete solution for fuel management and environmental services. It’s used in gas stations and by other suppliers, distributors, and buyers of petroleum products and other energy commodities. Insite360 is delivered as software as a service (SaaS) and is the market leader worldwide. Insite360 customers see increased operational efficiency through the automation of relevant processes and continuous monitoring. They are able to cut costs, increase revenues, and reduce risk by using the reporting, advanced analytics, and decision support tools built into Insite360. In terms of its IT architecture, Insite360 is in the midst of a three-step process toward the optimal architecture. In the first step, it moved operations to the cloud. In the second step, it accelerated analytics with SingleStore. In the third step, to be implemented soon, it will add ingress and transaction processing to its existing SingleStore-based analytics operation, thus “killing three birds with one stone” – SingleStore being that stone. Jay Dave is the business intelligence team lead for the Insite360 transformation. He’s been given the opportunity to help his team innovate at a high level, and SingleStore is helping him do it. Moving Fuel Fast Insite360 is the market leader in fuel management and related business and operations services for gasoline and other fuels. In the US alone, nearly 400 million gallons of gasoline are sold every day – more than a billion dollars worth a day, at typical retail prices, totaling roughly $400 billion a year. Insite360 is used to support the ordering and delivery of the majority of this gasoline and related fuels, and for fueling and refueling, retail operations in gas station convenience stores, back-end business operations, delivery scheduling, and much more. As Jay describes it, “Sensors gather data from fuel lines and dispensers. Data is collected by the automated tank gauge (ATG) and sent to Insite360 through the cloud.” Sensor/IoT data is not the only part of the picture. The ever-fluctuating price and availability of oil and gasoline are among the most important numbers in the world. The retail operations attached to gas stations are a big part of food and sundry sales worldwide, and getting bigger. Millennials, in particular, are said to favor gas stations over grocery stores for food purchases. Costs must be rigorously controlled, as gas stations are low-margin businesses. Yet all the cash that’s handled, and the volatile nature of the main product, means that security is an ongoing concern with robbery, burglary, organized crime, and terrorism all posing ongoing risks. Insite360 has an ATG Alarm Management team. They remotely monitor ATG equipment and monitor compliance status to comply with local, state, and federal environmental regulations, all at low cost. With the help of this team, companies can more effectively manage regulatory and related business issues. The technicians can fix many alarms remotely and quickly, sending others out for dispatch to the field. Insite360 Alarm Management handles more than a thousand different kinds of alarms related to fuel tank, dispenser, lines and other sources. Dozens of analysts in control rooms watch all the alarms and try to fix the problem remotely. If not, they dispatch a technician to find the problem and solve it. So Insite360 needs all the data, from all the sources, all the time, processed instantly. And their infrastructure is rapidly evolving to allow them to deliver better and better offerings to the demanding trade they serve. Step 1: Cloud Operations through PostgreSQL and Redshift It makes sense for Insite360 to deliver its service through the cloud. Its customers, and the customers’ IoT “things”, such as the fuel pumps that drivers use every day, are distributed to the most far-flung corners of the globe. Only through the cloud can Insite360 get affordable access to the distributed worldwide computing infrastructure that it requires. The first cloud architecture for InSite360 is a customer database offered by the company, which combined cloud and on-premises data sources and sent all the data into PostgreSQL, the open source, row-oriented database, hosted on AWS. Then, for analytics, the data was moved to the AWS Redshift data warehouse, which is column-oriented. Microsoft’s Power BI tool provided data visualization to Insite360’s end customers. The original architecture included AWS Elastic MapReduce</ br> batch processing for big data. This setup offered many advantages. Database offerings for cloud service providers have solved some of the scalability problems that have afflicted traditional, single-node databases for decades, making SaaS offerings much more practical. The move from proof of concept to deployment in the cloud was smooth and simple, as reprovisioning was fast and easy. Creating a workable infrastructure for an offering as ambitious as Insite360 was quite an accomplishment. The original architecture showed that Insite360 could work effectively in the cloud. However, there were five main problems with the original setup: Speed . The original architecture was slow – often too slow to meet customer expectations. Cost . The original infrastructure was also expensive to run. Running AWS Elastic MapReduce (EMR) against typical IoT big data, to prepare data for PostgreSQL, is complex operationally and can easily cost tens of thousands of dollars a month. AWS costs scale with data volume handled, even if end customers are not willing to ratchet up their payments to the same extent. Complexity . The original cloud architecture for Insite360 had lots of moving parts, each needing specialized skills from a wide range of architects, developers, and operations people, not to mention contract managers and accounting folk. Interfaces such as the connection between Oracle, running onsite, and Postgres in the cloud needed constant care and feeding from DevOps personnel. Rigidity . In the original infrastructure, Power BI connected directly to RedShift. Making this happen was a lot of work. RedShift is always column-oriented and disk-based, but some analytics operations benefit from running against row-oriented tables or from storing more data in memory, which are not possible with RedShift. Faux scalability . Cloud databases do not truly offer scalable SQL, they simply manage around the lack of scalability inherent to their SQL offerings somewhat smoothly. A truly large operation, such as Insite360’s fully global IoT architecture, can run into limitations and roadblocks that are hard, or impossible, to design around. Cloud vendor lock-in . Today, cloud means multi-cloud. Committing to AWS services means committing your data to, and optimizing your people’s skill sets for, a single cloud. As the leader in its market, Insite360 needs the flexibility that deep dependence on AWS services takes away. Step 2: Leveraging Kafka Messaging and SingleStore-Based Analytics for Speed and Simplicity Many users move to SingleStore for scalable compute capability in analytics, and Insite360 is no exception. Their current architecture continues to run in the cloud. However, it makes two several changes from the previous design, taking advantage of newer technology: Use Kafka throughout . In the current architecture, Kafka is used as a messaging bus to convey data from both NoSQL and most SQL databases. Batch processing in Python puts the data in the new data warehouse. Use AWS Database Migration Service for Oracle data . This service, introduced three years ago, replicates a source database without too much operational burden on the source database. Move ETL and analytics processing to SingleStore . Instead of amalgamating data in PostgreSQL, running an ETL process to get it into RedShift, then running Power BI against Redshift, the new architecture amalgamates data in SingleStore, then runs Power BI against SingleStore directly. The new architecture carries a heavy load. The SingleStore database runs hundreds of tables to host all the data coming from sensors, which arrive as Kafka events. Python processes Kafka messages and puts the clean and processed data into new tables. The new architecture must meet a wide range of requirements, including the Payment Card Industry Data Security Standard (PCI DSS) and other data security requirements. The new architecture addresses all of the problems found in the original architecture: Less cost . Big cost savings are achieved by eliminating the need to run AWS Elastic MapReduce (EMR) 24/7.The new architecture uses a SingleStore Kafka pipeline and Python for batch processing, with little processing cost. Less complexity . Instead of a two-headed core engine, PostgreSQL plus Redshift, SingleStore handles core analytics processing, simplifying management and maintenance. Less rigidity . Getting most data sources into a single stream of Kafka messages, all processed through Python, makes the architecture more flexible and simpler at the same time. More speed . The current architecture is much faster. With fewer moving parts, finding any bottlenecks and addressing them is much easier, and SingleStore is a very fast analytics engine. More scalability . Kafka and SingleStore are both fully scalable. Only the Oracle component is not able to scale out. Less cloud vendor lock-in . The only AWS-specific service used heavily in the current architecture is the AWS Database Migration Service. As such, the new architecture could be moved to a different cloud, on-premises, or to a blended architecture, giving Insite360 much more flexibility and negotiating power with all its vendors. In addition to the benefits from SingleStore, Kafka is an important part of the picture. “All the sensor and IoT data is coming from the ATGs using Kafka,” says Jay. “So the Kafka pipeline is a very welcome feature of SingleStore. It gets us away from managing AWS EMR, which is not an easy task. Not running AWS EMR saves us tens of thousands of dollars a month.” A few months ago, Insite360 began to take advantage of a new feature, the ability to backup S3 databases into SingleStore. The company has already done several backup and restore operations, and say that it “works like a charm” for the database and also for Kafka events. The cloud application is based on a microservices architecture, with each microservice using its own data store. All the data goes into tables in SingleStore routed thru Kafka, then customers run analytics through Power BI for data visualization. Insite360 also does machine learning from SingleStore data for alarm management and natural language processing (NLP) on alarm and compliance data. Internal customers include the product team, marketing, sales, and other groups within the company. External customers are the service stations. With the up-to-date information from Power BI, they can make informed decisions about operating and maintaining their stations. Step 3: Using SingleStore Pipelines for Ultimate Simplicity The need to separate transactions from analytics is nothing but an artifact of the processing limitations of traditional SQL databases such as Oracle compared to NewSQL databases such as SingleStore. However, over decades, expectations, and existing architectures, have hardened. The model of having an OLTP database, followed by an ETL process, feeding an OLAP database, is accepted as the norm. As a result, a SingleStore implementation usually goes through two stages. In the first stage, either the OLTP or OLAP side is breaking down or already broken. Due to its high performance and scalability, SingleStore is used to either augment or replace the challenged transactional or analytical processing systems. Then, an organization becomes more familiar with SingleStore’s strengths. In particular, it realizes that SingleStore, and its Pipelines capability, can break down the barrier between OLTP and OLAP. SingleStore can simultaneously process transactions, transform data, and respond to queries. Once this realization takes hold, a radically simpler architecture suggests itself. The entire traditional ETL structure can be moved into SingleStore, which operates in three steps: Extract . The original purpose of SingleStore Pipelines is to extract data rapidly, and with minimal load on the source database. Transform . Simple transformations can be handled within a SingleStore Pipeline. The newer SingleStore capability, Pipelines to Stored Procedures, allows for more complex transformation steps. Load . Data from Pipelines is loaded directly into SingleStore, and analytics queries against updated data run continually throughout. Insite360 is preparing to implement such a radically simpler architecture and reap the benefits, which include seven highly desirable attributes: Up-to-date data . In the new architecture, batch processing is eliminated. Cutting out batch processing and taking advantage of unified transactions and analytics means that analytics always runs against current data. Speed dial . Transaction processing and analytics speed is easily dialed up as needed by using more processors for SingleStore. Lowest cost . SingleStore price/performance is stellar for either transactions or analytics. Combining transactions and analytics in a single SingleStore database cuts costs further. Ultimate simplicity . Standardizing on a single database and consistent processes for bringing in and transforming data makes the architecture extremely simple and reduces the load on operations personnel. Ultimate flexibility . Any data source can be added without changing the core processing and analytics architecture, due to the flexibility of SingleStore Pipelines and the processing power of stored procedures. And any analytics tool can be used, due to SingleStore’s SQL support. Seamless scalability . With transaction processing and analytics support running on the same, fully scalable engine, you can literally throw hardware at problems. No cloud vendor lock-in . You can set up SingleStore on any cloud or on-premises, and change hosting at any time. If you do use cloud vendor-specific services as a data source or on the analytics side, the dependency is more tactical than strategic, as you have flexibility at the core. Machine learning and AI-friendly . Because data is current and bottlenecks can be eliminated by adding hardware, and because more data sources can be added as needed, machine learning programs and AI always have all the up-to-date data they need. Next Steps for Insite360 Insite360 will be moving to the new, radically simplified architecture in the months ahead. Compared to the original cloud infrastructure, costs are expected to drop roughly in half. The number of people needed to operate the system will also be cut roughly in half to run a system that’s much more functional than the original. One area in which this new functionality will bring benefits is enhancing the ability to do machine learning and AI. “Machine learning and AI work better with more data to aggregate,” says Jay. Insite360 uses advanced variance analytics to solve problems like missing fuel. If, for example, 50 gallons of fuel is missing out of thousands of gallons pumped, that can be due to various causes, such as theft, over-pumping, or a leak, which could be occurring in any of several different locations. Once the data is in SingleStore, Insite360 can use machine learning and AI to help operators prioritize where to look for problems first. Improving the underlying architecture is also a positive for end customers. They get better results faster. Also, they can easily understand how data comes into, through, and out of the system. This increases their confidence in the results and their ability to use the system effectively. All involved are looking forward to these benefits and more. The organization will indeed have “killed three birds with one stone” – SingleStore.", "date": "2018-12-17"},
{"website": "Single-Store", "title": "webinar-building-analytics-app-memsql", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/webinar-building-analytics-app-memsql/", "abstract": "In this webinar, SingleStore Product Manager Jacky Liang took a live audience through the process of building an analytics app in just a few minutes. You can view the webinar and download the slides here . In just a few minutes in the middle of the webinar, Jacky created a free SingleStore account; installed SingleStore on AWS; loaded in some data; and created a dashboard using Looker, a popular analytics tool. Jacky also explained some SingleStore basics and gave a quick overview of SingleStore’s architecture. We encourage you to read about the details here, then view the webinar to see SingleStore in action. SingleStore Now Free to Use It’s already widely known that SingleStore is a highly capable database. What’s not so widely known is that SingleStore is now free to use , up to 128GB of RAM usage. Because SingleStore is memory-led, not memory-only, the actual database size supported may be as much as several terabytes. A memory footprint of 128GB is more than enough storage to get a project started, through the proof of concept phase, and into deployment and initial scaling. One of the points that Jacky makes during the webinar is that there are many projects that can be handled within the 128GB RAM capacity in which SingleStore is free to use, even when the project goes into production. At the point where you need SingleStore’s legendary support , and/or more capacity, a simple license switch and a reasonable monthly payment get you both. SingleStore Basics We published a somewhat detailed description of how SingleStore works just a few weeks ago. To sum it up in a list: SingleStore’s core is tiered storage, memory to disk . In this memory-led architecture, you keep rowstore tables in memory and columnstore tables on disk, with a relatively large memory cache. You get the best of both worlds, with high performance and a manageable memory footprint. Query optimizer across tables . SingleStore distributes storage across multiple clusters, with aggregator nodes holding schema and leaf nodes storing data. The SingleStore query optimizer compiles and speeds queries across table types and across nodes. Memory and CPU management . The SingleStore memory and CPU manager uses available memory effectively and optimizes performance. Lock-free distributed ingest and analytics . Write to and simultaneously read from an ACID-compliant SQL database that can ingest data in streaming or batch modes. SingleStore Studio and command-line tools . Using the Studio interface and our command-line tools, you can easily manage multiple clusters and use what you learn to optimize both data storage and queries. Enterprise security . SingleStore operates to very strict security standards and supports role-based access control, encryption, auditing support, and more. SQL API and SQL queries . The secret of SingleStore’s success is our thoroughgoing support for ANSI SQL (like traditional transactional and analytics databases), combined with our support for distributed storage (like NoSQL databases) – the best of both worlds. SingleStore fits in very well into existing, often complex data processing architectures. The database works with a variety of other tools for data ingest and analytics. SingleStore runs on premises or in a public cloud. In containers and on virtual machines – anywhere you can run Linux. The Demo The webinar features a straightforward demo of implementing an analytics app with SingleStore. There are just four steps: Create a free SingleStore account . This is quick and easy. Install SingleStore . For the demo, this is done on AWS. Load data . For the demo, we used the 100GB TPC-H data set. Run a dashboard . For the demo, Looker is used to create the dashboard. It’s worth taking a quick, well, look at our use of Looker to create the demo dashboard. Due to its thoroughgoing ANSI SQL support, SingleStore works well with the wide range of business intelligence (BI) tools that use SQL. A SingleStore database is also easily queried by anyone who knows standard SQL – a number that is certainly in the hundreds of thousands, and has been estimated to be more than a million people . We used Looker for the demo because it gives users straightforward and direct access to the underlying database(s) being queried. This is especially advantageous for us at SingleStore, as our architecture is actually rather elegant , garnering a lot of praise – especially from users who come to SingleStore with a lot of previous database experience and some feel for what an “ideal”, distributed SQL database might look like. (You don’t have to take our word for it – find a few such people and ask them directly.) Looker is easiest to use with standard SQL databases like SingleStore. And like many BI tools, it is also highly optimized specifically for use with SingleStore, and vice versa. SingleStore’s Architecture To get the most out of SingleStore, it’s very helpful to understand a few things about our distributed architecture . The webinar concludes with a brief description of it. A SingleStore database is implemented as a series of master nodes and leaf nodes. Each master node has a copy of the database schema and a list of the data elements it’s responsible for. Several leaf nodes are managed by each master node. Applications and other clients connect to one of the aggregators, the Master Aggregator, that fields queries. Q&A This webinar concluded with a lively Q&A period. The questions and answers include: Can SingleStore connect to other BI tools? Yes, very much so. SingleStore is built around support for the SQL standard and, specifically, the MySQL wire protocol. This allows you to easily use SingleStore with the many hundreds of data integration and BI tools that support MySQL connectivity. A few of the many such tools include Looker (as used in our demo), Microstrategy, Tableau, and Zoomdata. Does SingleStore run on Windows? SingleStore runs on Linux systems, so it runs on Windows in a virtual machine. (Microsoft has been working on improving their support for virtual machine environments on Windows in general, and Linux in particular.) See our System Requirements and Recommendations page for more. What size restrictions do you have? You can use SingleStore for free, but without official SingleStore support, on a system with up to 128GB of RAM. For customers with an Enterprise subscription, there are no physical limitations on the amount of RAM allocated. Customers decide the balance of RAM to disk usage based on performance requirements. Many SingleStore customers use a few terabytes of RAM to support hundreds of terabytes on disk. There were also comparative questions about other database solutions: How is SingleStore different from Snowflake? Snowflake is a managed service offering for AWS and the Azure cloud platform only. SingleStore runs on any system that can run Linux, including in containers and virtual machines, on all cloud platforms, on-premises, and in mixed deployments. Both support high performance data warehouse workloads, but Snowflake is optimized for back office analytics and priced on usage. SingleStore is optimized for operational, “live” business requirements that need always-on, continuously updated data with fast ingest, as well as back office analytics. SingleStore also offers transactional support. The billing differences mean that Snowflake’s costs are variable, depending on the size of semi-regular data loading and the frequency and complexity of queries. SingleStore cost of ownership is known up front and fixed for a given deployment size. How does SingleStore ingest performance compare to Azure SQL Server data warehouse? While any comparison can be subject to criticism, SingleStore utilizes a very fast, high-throughput Skip List index methodology. This is backed by a distributed, memory-optimized platform, enabling ingest rates in the millions of events per second. Azure SQL Server Data Warehouse, on the other hand, uses a traditional B-tree index and other older methodologies, backed by a single-node architecture that limits parallel throughput. This is likely to result in ingest speeds more on the order of tens of thousands of events per second. Next Steps If you watch the webinar video, and have questions or feedback, feel free to email the main presenter, Jacky Liang, directly. He would also like to hear how you’re using the free tier. You can reach him at jliang@singlestore.com . We urge you to go here to view the webinar video and download the slides. Then download SingleStore and give it a try yourself, either in a public cloud or using hardware you have on-premises.", "date": "2019-01-17"},
{"website": "Single-Store", "title": "using-streaming-analytics-to-identify-and-visualise-fraudulent-atm-transactions-in-real-time", "author": ["Saeed Barghi"], "link": "https://www.singlestore.com/blog/using-streaming-analytics-to-identify-and-visualise-fraudulent-atm-transactions-in-real-time/", "abstract": "Storing and analysing larger and larger amounts of data is no longer what drives the decision-making process at successful companies. In today’s world, the key is how fast decision makers are provided with the right information, enabling them to make the right decision before it’s too late. Streaming analytics help you identify perishable insights, which are the subject of their very own Forrester Report . Perishable insights are results “which must be acted on within a given timeframe or else the chance to influence business outcomes will pass,” according to the IBM Cloud Blog . The trouble is, many companies see implementing a streaming analytics platform as a very challenging and costly project. This is because, when talking to internal experts or outside consultants, they are usually presented with a very complex architecture – one that needs a lot of effort and money to set up and get going. That is not the reality of streaming analytics. When the proper set of technologies and tools is used, such a platform can be set up very quickly and effectively. In facing challenges of this type, I’ve identified a platform that works perfectly. The high-level architecture of this platform is shown below. Looks beautiful and simple, doesn’t it? We’re going to use this architecture and build a solution that identifies fraudulent ATM transactions in real time. This platform can solve many other problems as well.) I’ll describe the solution in three steps. Step 1: Build and Analyze Streams of Data Everyone in the field of Big Data has heard of Kafka. It’s the backbone of most streaming applications. Kafka was invented because, for most streaming data, the speed at which the data is generated at the source is a lot faster than the speed at which it can be consumed at the destination. Therefore, a Kafka connection, called a topic, is required to act as a buffer. It receives data from a source, called a publisher, and holds onto it until it’s read by the destination, called a consumer. Like other streaming solutions, we’re going to use Kafka here as well. But not just any edition of it; we’ll be using Confluent Kafka. Confluent has not only put great add-ons around Kafka, and made it a lot easier to deploy and manage; they are the pioneer in stream processing. Furthermore, Confluent Kafka scales very efficiently, and is able to work very fast, and with very big data, without any hiccups. The most interesting component in Confluent platform for me is KSQL. It provides a SQL-like querying capability on top of streams of data. And that sounds like heaven for someone like me, who has spent most of his professional life writing and optimizing SQL queries. ] For the first part of this solution, I followed this blog post and created the streams and processed them with KSQL. The steps I took were: Download and set up Confluent platform: https://www.confluent.io/download-cp/ Start your Confluent platform: ./bin/confluent start Download and set up gess: https://github.com/rmoff/gess Create the topic, using Confluent’s “kafka-topics” command: ./bin/kafka-topics –topic atm _ txns –zookeeper localhost:2181 —create –partitions 1 –replication-factor 1 Follow the blog post and create and process the stream. Just as a reference, your final Stream should look like this: CREATE STREAM ATM _ POSSIBLE _ FRAUD \\ WITH (PARTITIONS=1) AS \\ SELECT TIMESTAMPTOSTRING(T1.ROWTIME, ‘yyyy-MM-dd HH:mm:ss’) AS T1 _ TIMESTAMP, TIMESTAMPTOSTRING(T2.ROWTIME, ‘yyyy-MM-dd HH:mm:ss’) AS T2 _ TIMESTAMP, \\ GEO _ DISTANCE(T1.location->lat, T1.location->lon, T2.location->lat, T2.location->lon, ‘KM’) AS DISTANCE _ BETWEEN _ TXN _ KM, \\ (T2.ROWTIME – T1.ROWTIME) AS MILLISECONDS _ DIFFERENCE, \\ (CAST(T2.ROWTIME AS DOUBLE) – CAST(T1.ROWTIME AS DOUBLE)) / 1000 / 60 AS MINUTES _ DIFFERENCE, \\ GEO _ DISTANCE(T1.location->lat, T1.location->lon, T2.location->lat, T2.location->lon, ‘KM’) / ((CAST(T2.ROWTIME AS DOUBLE) – CAST(T1.ROWTIME AS DOUBLE)) / 1000 / 60 / 60) AS KMH _ REQUIRED, \\ T1.ACCOUNT _ ID AS ACCOUNT _ ID, \\ T1.TRANSACTION _ ID, T2.TRANSACTION _ ID, \\ T1.AMOUNT, T2.AMOUNT, \\ T1.ATM, T2.ATM, \\ T1.location->lat AS T1 _ LAT, \\ T1.location->lon AS T1 _ LON, \\ T2.location->lat AS T2 _ LAT, \\ T2.location->lon AS T2 _ LON \\ FROM ATM _ TXNS T1 \\ INNER JOIN ATM _ TXNS _ 02 T2 \\ WITHIN (0 MINUTES, 10 MINUTES) \\ ON T1.ACCOUNT _ ID = T2.ACCOUNT _ ID \\ WHERE T1.TRANSACTION _ ID != T2.TRANSACTION _ ID \\ AND (T1.location->lat != T2.location->lat OR \\ T1.location->lon != T2.location->lon) \\ AND T2.ROWTIME != T1.ROWTIME; Step 2: Ingest Streams of Data into Data Store in Real-Time The next layer we need to implement in this architecture is data ingestion and storage. There are different tools in the market that are able to ingest data in close to real time, such as Nifi, StreamSets, and maybe Talend. And then for storage, depending on your preference as to on-premise or cloud, HDFS or Object Storage are the options. The number one factor that I always consider when suggesting a solution to my clients is integrity and homogeneity in all layers of the purpose-built solutions. And when it comes to streaming, where performance is the number one factor, I can’t think of a solution more reliable and faster than SingleStore . If you’re curious to know how fast the database is, watch this video . And be prepared for your mind to be blown! Another reason I love SingleStore for streaming use cases is how well it integrates with Kafka through SingleStore Pipelines . Take the following steps to set up SingleStore and integrate it with your Confluent platform: Install SingleStore on the environment of your choice: https://docs.singlestore.com/guides/latest/install-memsql/ Fire up the SingleStore command-line interface and create a new database: create database streaming _ demo _ database; Create a new table for the records you receive from Confluent: CREATE TABLE atm _ possible _ fraud (INGESTION _ TIME TIMESTAMP DEFAULT CURRENT _ TIMESTAMP ON UPDATE CURRENT _ TIMESTAMP , MESSAGE _ FROM _ KAFKA JSON , T1 _ TIMESTAMP AS MESSAGE _ FROM _ KAFKA::$T1 _ TIMESTAMP PERSISTED DATETIME , T2 _ TIMESTAMP AS MESSAGE _ FROM _ KAFKA::$T2 _ TIMESTAMP PERSISTED DATETIME , DISTANCE _ BETWEEN _ TXN _ KM AS MESSAGE _ FROM _ KAFKA::$DISTANCE _ BETWEEN _ TXN _ KM PERSISTED DOUBLE ,MILLISECONDS _ DIFFERENCE AS MESSAGE _ FROM _ KAFKA::$MILLISECONDS _ DIFFERENCE PERSISTED DOUBLE ,MINUTES _ DIFFERENCE AS MESSAGE _ FROM _ KAFKA::$MINUTES _ DIFFERENCE PERSISTED DOUBLE ,KMH _ REQUIRED AS MESSAGE _ FROM _ KAFKA::$KMH _ REQUIRED PERSISTED DOUBLE ,ACCOUNT _ ID AS MESSAGE _ FROM _ KAFKA::$ACCOUNT _ ID PERSISTED CHAR(100) ,T1 _ TRANSACTION _ ID AS MESSAGE _ FROM _ KAFKA::$T1 _ TRANSACTION _ ID PERSISTED CHAR(100) ,T2 _ TRANSACTION _ ID AS MESSAGE _ FROM _ KAFKA::$T2 _ TRANSACTION _ ID PERSISTED CHAR(100) ,T1 _ AMOUNT AS MESSAGE _ FROM _ KAFKA::$T1 _ AMOUNT PERSISTED DOUBLE ,T2 _ AMOUNT AS MESSAGE _ FROM _ KAFKA::$T2 _ AMOUNT PERSISTED DOUBLE ,T1 _ ATM AS MESSAGE _ FROM _ KAFKA::$T1 _ ATM PERSISTED CHAR(100) ,T2 _ ATM AS MESSAGE _ FROM _ KAFKA::$T2 _ ATM PERSISTED CHAR(100) ,T1 _ LAT AS MESSAGE _ FROM _ KAFKA::$T1 _ LAT PERSISTED DOUBLE ,T1 _ LON AS MESSAGE _ FROM _ KAFKA::$T1 _ LON PERSISTED DOUBLE ,T2 _ LAT AS MESSAGE _ FROM _ KAFKA::$T2 _ LAT PERSISTED DOUBLE ,T2 _ LON AS MESSAGE _ FROM _ KAFKA::$T2 _ LON PERSISTED DOUBLE ); Note : A few points about this create table script: The first column, INGESTION _ TIME, is populated automatically when every record is ingested The second column, MESSAGE _ FROM _ KAFKA, holds the records received from Confluent topics in JSON format The rest are Persistent Computed columns in the table that are computed and populated when each JSON record lands in the table. This is another cool feature of SingleStore, makes it incredible easy to parse JSON data without the need to call any additional script or coding. Create an index on INGESTION _ TIME column. This is needed when we get to build our visualisation work in real-time with Zoomdata: CREATE INDEX inserttime _ index ON atm _ possible _ fraud (Ingestion _ Time); Create a pipeline that reads data from Confluent topics and inserts into SingleStore in real-time: CREATE PIPELINE atm_possible_fraud AS LOAD DATA KAFKA ‘ [ IP _ ADDRESS ] :9092/ATM _ POSSIBLE _ FRAUD’ INTO TABLE atm_possible_fraud (MESSAGE _ FROM _ KAFKA); Test and start the pipeline: TEST PIPELINE atm _ possible _ fraud; START PIPELINE atm _ possible _ fraud; And we’re done. Now you can start ingesting data into Confluent Kafka topics and they will be replicated in your SingleStore table in real time. On your Confluent server, make sure you have started your Confluent platform by running: ./bin/confluent status Then go to the folder where you have downloaded gess and run following commands: ./gess.sh start nc -v -u -l 6900 | [ CONFLUENT _ DIRECTORY ] /bin/kafka-console-producer –broker-list localhost:9092 –topic atm _ txns …and start querying your SingleStore table. You should see records ingested in there as they are generated from the source and delivered into the appropriate Kafka topic in Congruent. Step 3: Visualise Your Data in Real Time There are so many visualisation tools out there in the market, some of which claim they can visualise your data in real-time. But none of them can truly achieve that. Why? Because they all need to take custody of data to be able to visualise it: each and every record needs to be moved to the server where the visualisation engine runs, processed there, and then visualised to users in the form of graphs and dashboards. There is one visualisation tool that is different from every other tool in the market, in that it pushes down the query processing to the source where data is stored. That tool is Zoomdata. Zoomdata doesn’t move data. It doesn’t take custody of the data. How does it work? I’m glad you asked. Zoomdata’s smart query engine takes the query that is meant to grab the data for the dashboard, applies its knowledge of the underlying data store and metadata about the tables involved in the visualisation, and breaks the query into many smaller queries called micro-queries . The idea is that, instead of sending a big query down to the source and waiting for the result to come back, it makes more sense to send those smaller queries down, then progressively sharpen the visualisation as the results are returned from each micro-query. Another very important point about Zoomdata is that it is truly self-service, unlike some other tools, which require a certification to master. To create a dashboard pointing to the data on our SingleStore database, follow these steps. Open Zoomdata UI and login as admin. From the top menu, click on Setting, then select Sources. From installed connectors, click on the SingleStore logo and start configuring the data source. Give your data source and click Next. This is the page where you give the details and credentials to connect to your SingleStore cluster. Fill it up and then click Next. You’ll see the list of tables that exist in the data source you created, in our case the SingleStore database. Select “atm _ possible _ fraud.” The list of columns and sample the top 10 rows of the table will be loaded. Remember to toggle CACHING off, since we are building a real time dashboard. Then click Next. The next page has the list of columns and their data types. Zoomdata infers the data types for each column from the data source. Change them if they don’t match the type of analysis you want to do, or if they are not correct. Read more about this tab here. After you review the data types for each column, click Next. The next tab is where you define how often you would like to refresh the cached data. We don’t need to make any changes here since we’re not caching any data. Click Save & Next. Now we’re on the Charts tab. This tab is used to define the default behaviour of each visualisation type. This means you can define which columns to used to render each visualisation when the dashboard loads. (Zoomdata is 100% self service, meaning that users can change the dashboards at runtime without the need for IT or highly technical resources). Another very important and interesting feature of your visualisation dashboard will be defined in this tab as well: the Time Bar. Zoomdata follows a streaming architecture, which enables it to connect in “Live Mode” to any data source capable of real-time data. The technology behind this feature is called Data DVR. In Live Mode, Zoomdata visualisations immediately reflect changes in the source data as data streams in. The live stream can be paused, rewound and replayed, essentially treating it as a digital video recorder (DVR) for your data. Follow these steps to set it up: Select Ingestion Time from the Time Attribute drop-down. (That’s the column we had our SingleStore table partitioned by, remember?) This is the column driving our time bar, and it makes sense to choose it: our real-time dashboard needs to be able to get the values from our real-time data source, based on this column, very fast. Click the Enable Live Mode checkbox. Select 1 for Refresh Rate and 1 second for Delay By. The idea is that Zoomdata will look for records added to the data source with 1 second delay. (In a future version, Zoomdata will be able to support one millisecond delays.) Click Finish. You will be redirected to the source page. You’ll see on the top that your new data source has been created. Click on New Chart and Dashboard and start building live visualisations. Finish visualisations as needed. Here is a short video showing the incredible performance this streaming solution can provide. On the left side of the video I kick off my script that publishes records to Confluent Kafka topics, and it takes less than three seconds from that point until the visualisation is updated. Our solution was so simple and easy to implement that I’ve summarized it here in a single blog post. Yet, at the same time, it’s capable of providing incredible performance running on just three servers – one Confluent, one SingleStore, and one Zoomdata. This post is from Saeed Barghi’s blog, The Business Intelligence Palace. The original version of this post appears on his blog . Want to know more about any of these products? Reach out to the author at his blog or contact SingleStore .", "date": "2019-01-24"},
{"website": "Single-Store", "title": "case-study-hadoop-memsql-fortune-50", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/case-study-hadoop-memsql-fortune-50/", "abstract": "A Fortune 50 company tried traditional SQL databases, then Hadoop, to manage financial updates company-wide. They finally found a successful solution by augmenting Hadoop with SingleStore. Many companies today are straining to meet the needs for faster ad hoc analytics, dashboards, reporting, and support for modern applications that depend on fast access to the latest data – in particular, financial data. Facing competitive and customer pressures, IT organizations must respond to a rapid increase in the volume of incoming data and the needs of the business to use this data for rapid decision-making, improving operations, and providing better customer experiences. These problems are especially severe in larger traditional companies that need to innovate. Traditional relational databases, lacking scalability, have run out of gas for solving these problems. Many companies have turned to Hadoop, a NoSQL solution, to rapidly ingest data and move it to a data lake. However, the lack of SQL support and performance issues for queries – which affect ad hoc queries, reporting, dashboards, and apps – have made this approach unproductive for many. A diversified Fortune 50 company approached SingleStore to help solve exactly this problem. The company first attempted to meet its analytics needs with an existing, legacy relational database. They then introduced a NoSQL solution based on Hadoop. The company found each of these solutions to be inadequate. Ultimately, this company augmented Hadoop with SingleStore. They now use SingleStore to support an operational data store for company-wide data analytics, including access by executives. Hadoop still powers a data lake, which is used largely to support data scientists. Detailed below is our customer’s journey and the benefits they saw from moving their core analytics processing to SingleStore. Included is a reference architecture for a combined implementation of SingleStore alongside Hadoop. Displacing Hadoop for Analytics and Business Intelligence The SingleStore customer in this case study needed fast analytics at the divisional level, as well as the ability to continually roll up data across the company for quarter-end reporting and communications to investors. Originally, the company used a legacy relational database to drive analytics. Business analysts used business intelligence (BI) tools such as Business Objects and Tableau to derive insights. Analysts, knowledgeable in the needs of the business, were able to deliver real value to the company. However, data volumes continued to increase, while more users wanted analytics access. Analytics users, from business analysts to top management, demanded greater responsiveness. And the analytics system needed to be ready to handle demands from machine learning and artificial intelligence software on both operational and legacy data. The system that drove analytics was unable to keep up with the transaction speeds, query responsiveness, and concurrency levels required by the business. As a first attempt to solve the problem, the company augmented the legacy system with a Hadoop solution, with the final goal being to fully replace the legacy system with a Hadoop/Big Data solution. The company’s first try at a solution, a data lake supported by Hadoop and HDFS, has become a common analytics architecture Unfortunately, the move to Hadoop was unable to deliver for this customer – as it has for many others. Many data lake projects fail to make it into production , and others experience serious problems after implementation. Hadoop is designed for unstructured data, and this causes many serious problems: Batch processing . With Hadoop, input data is processed in batches, which is time-consuming. The data available to analytics users is not current. Large appetite for hardware . Both batch processing and analytics responses take a lot of processing power, so companies spend a great deal to power Hadoop solutions. Governance and security challenges . Because Hadoop data is less organized than data in a relational database, it’s hard to identify and protect the most sensitive items. Complex system . Big Data solutions involve an entire ecosystem with many specialized components, which make for a difficult system to manage. Complex queries . Queries have to be written against complex data, with careful study of each data source needed. Slow queries . Queries run slowly, and users must either spend time up front optimizing them, or spend extra time waiting for results. No SQL . NoSQL eliminates the ease, speed, and business intelligence (BI) tool support found with SQL, preventing companies from getting maximum value from their data. Queries, Sorted: Augmenting Hadoop with SingleStore The company had two competing sets of needs. Data scientists wanted access to raw data, and were willing to do the work to transform, catalog, and analyze data as needed. Business analytics users (including the executive team), BI tools, and existing applications and dashboards needed structured data and SQL support. By augmenting Hadoop with SingleStore, the company can meet all these needs at once. SingleStore is a massively-scalable, high-performance relational database. SingleStore is a leader in a new category of databases, commonly described as NewSQL. As a leading NewSQL database, SingleStore offers a cloud-native, distributed architecture (details here ). It combines the ability to handle streaming ingest, transactions, and analytics, including SQL support. With SingleStore holding the operational data store, the company is once again able to unleash its business analysts on its most important data. The same operational data store is available for executive queries and financial reporting. Hadoop and HDFS are still used, but as a data archive, to support audits, and as a data lake for use by data scientists. (Who, of course, use the operational data store as well.) Work in the data lake yields valuable results, while the operational data store supports the ease of access, efficiency, and rapid response needed by the rest of the business. Choosing an Ingest Strategy With SingleStore used as an operational data store for real-time analytics, and Hadoop as a data lake, the customer had three choices as to how to ingest data: SingleStore-first ingest . The customer could replicate the data, using change data capture (CDC) from some sources and SingleStore Pipelines from others. Some or all of the data could then be extracted and loaded to Hadoop/Hive. Simultaneous ingest . The customer could replicate the data to both the targets in parallel. In either case, some data could be discarded on ingest, but the default choice for any incoming data would be to go to both databases at once. Hadoop-first ingest . The customer could bring all data into Hadoop in a batch operation, then transfer all of the data, or a subset, to SingleStore as needed. Each approach has either advantages or disadvantages. Advantages Disadvantages Use Case SingleStore-first ETL, CDC and SingleStore Pipelines stream data; analytics run against live data Fast analytics critical; SingleStore needs most or all of the data Simultaneous Added complexity, as both streams need to be managed as primary Fast analytics critical; both SingleStore and Hadoop need most or all of the data Hadoop-first Major delay for SingleStore ingest & analytics, happening after Hadoop batch ingest Many users try this, only to realize they need an operational data store w/SQL The SingleStore-first approach gets you analytics against live data, while other approaches add complexity and delays The customer chose direct ingest to SingleStore. Data is also sent to an operational data store, then batched to Hadoop Their whole reason for using SingleStore was to speed up analytics, so putting Hadoop first – leading to delays in data getting into SingleStore – was a poor option. Because both SingleStore and Hadoop would be getting most of the data, and to simplify troubleshooting if a problem occurred, the customer decided to keep SingleStore and Hadoop ingest separate. With SingleStore, the customer has the same advantages that customers have long sought from a traditional data warehouse: direct access to data for faster analytics performance. A traditional data warehouse requires pre-aggregation of data, where this is needed less, or not at all for SingleStore. And SingleStore adds advantages of its own: better performance than traditional data warehousing products and the ability to query live data. SingleStore compares even more positively to the customer’s former Hadoop-based data warehouse-type implementation. Hadoop is not designed to support ad hoc queries and dashboards. It lacks SQL support, meaning that it can’t be used easily with most business intelligence (BI) tools, and it can’t be queried conveniently by business analysts and others who want to use SQL for ad hoc queries, not write a computer program. Hadoop SingleStore Performance Poor Excellent Concurrency Poor Excellent Queries Custom SQL ad hoc queries & BI tools Accessible by Data scientists Business analysts, executives, data scientists Governance support Poor Excellent Hadoop lacks key analytics features supported by SingleStore, especially SQL support Analytics with No Limits The company’s use of data is now on much firmer footing. Thousands of BI tool users and more than five hundred direct users, spread across roughly a dozen divisions, use SingleStore, BI tools, and SQL queries for all their analytics needs. Many terabytes of data are accessible to users across the globe. Analysts and other users work with up-to-date data, at high concurrency, with fast query performance. Analytics work has dramatically improved: From batch mode to near-real-time : Analytics now run against near-real-time data. Information that used to take days to arrive now takes minutes. From spreadsheets to BI : This Fortune 50 company (like many) was doing much of its core analytics work by exporting data from Hadoop into Excel spreadsheets. With SingleStore, the full range of BI tools is now available as needed. Instant access for live queries : Using SQL, any analyst can make any query, anytime, and get results quickly. In combination with BI tools, this supports the kind of interactive probing that leads to real, timely, actionable insights. Executive accessibility : There’s now a single source of truth for investor reporting. The CEO and senior executives have direct access to insight into operations. Hadoop + Hive SingleStore Processing mode Batch Real-time Analytics tools used Excel spreadsheets Wide range of BI tools Access mode Custom queries SQL queries Executive accessibility Inaccessible Direct SingleStore has improved analytics performance and accessibility In addition to the outcomes, the deployment process and operations for SingleStore are also considered a success for the IT team: Time to go live was less than three quarters. Deployment across the company was managed by a single team of just two people. SingleStore runs faster on a six-node cluster than Hadoop + Hive did on fifty nodes. SingleStore offered superior total cost of ownership, with operational savings more than offsetting licensing costs for SingleStore. As impressive as it is, this use case only shows part of what SingleStore can do. The same company is now implementing SingleStore in a different use case that takes advantage of a wider range of SingleStore’s capabilities. Spotlight on Pipelines SingleStore’s Pipelines capability makes it uniquely easy to ingest data. With Pipelines, SingleStore streams data in from any source and transforms it “in flight”, then loads it rapidly into SingleStore. The traditional extract, transform, and load (ETL) process is performed on streaming data in minutes, rather than on batched data in a period of hours. To extend the Pipelines capability, SingleStore has introduced Pipelines to stored procedures . Stored procedures add flexibility and transactional guarantees to the streaming capabilities of Pipelines. Hadoop and HDFS, by contrast, ingest data in a slow batch process, with little transformation capability, and no access to stored procedures. Conclusion The Fortune 50 company described here is facing the same challenges with its use of data as most other enterprises worldwide. SingleStore has allowed them to provide better access to data, to more people across their company, at a lower cost than other solutions. The results that they have achieved are encouraging them to increase their use of SingleStore for analytical, and also for transactional use cases. The same may be true for your organization. For more discussion about the use of SingleStore with Hadoop, view our recent webinar and access the slides. And consider trying SingleStore for free today.", "date": "2019-01-29"},
{"website": "Single-Store", "title": "case-study-mondaydotcom-bi", "author": ["Daniel Mittelman"], "link": "https://www.singlestore.com/blog/case-study-mondaydotcom-bi/", "abstract": "Everything at monday.com starts and ends with data. We’re driven to succeed, and to do that, we must measure everything with precision and accuracy. That’s why we built our own business intelligence (BI) solution from scratch: our beloved BigBrain . It tracks every single KPI we have here at monday.com and in the spirit of transparency, we make these numbers readily accessible to everyone on our team. Our BigBrain team—currently 4 engineers and counting ( we’re hiring! )—just wrapped up a big project to supercharge BigBrain as we embark on a new chapter of accelerated growth in our company. In this three-part series, BigBrain developer Daniel Mittelman will share how and why we moved to SingleStore and why it’s awesome. Here at monday.com, we’re growing at an incredibly fast rate. More and more people are visiting our website, signing up for new accounts, and trying out the platform. In addition, an increasing number of happy customers are upgrading their plans to let more people on their team work with monday.com. As we mentioned before , we collect and analyze data about our users’ behavior (with privacy concerns taken into account, of course). This helps us shape our business plan for years to come and make better decisions about how to improve our product. But…there’s a cost. An increasing number of users, combined with an increasing number of available features in the product, means we’re accumulating data at an incredibly fast rate. Check it out in the chart below: monday.com’s total number of engagement records generated by users over time. In January 2016, we had around 50 million records in our engagement data. One year later in January 2017, this number had grown to 265 million records. We kickstarted 2018 with over 1 billion records. This is not considered a very large amount in Big Data, but simple math dictates that by 2019, our data pool will surpass 4 billion records, and by 2020, we’ll face data storage of 15 billion records. Armed with this knowledge, we realized that our existing Elasticsearch-based solution would reach its limits within a year. Don’t get me wrong—Elasticsearch is a great database for token-based searching. It also performs very well when given basic data analysis tasks (mainly counting and bucketing stuff.) However, Elasticsearch lacks the ability to cope with greater challenges, such as performing aggregations over multiple indices and joining them together, or making complex funnel computations. Unsurprisingly, these more complex analyses are the ones we want most here at monday.com, as they reveal a more complete picture about user engagement. We set out on the search for a new analytical database in mid-2017. We looked at (almost) every possible database that’s out there: from the cloud-based, ready-to-use data warehouses like Amazon’s Redshift, Google BigQuery and Microsoft’s Cosmos DB, to the expanding new field of GPU-based databases like MapD and Kinetica. We even checked out Amazon’s Athena backed by a JSON-formatted S3 bucket. Each database had its advantages and downsides. Overwhelmed by all the options out there, we mapped our exact requirements for this new solution. Our checklist was, in descending order of importance: Speed! Requests shouldn’t exceed 6 seconds in most cases, and 30 seconds at most Versatile : Capable of joining different data types during a single computation Distributed : Highly available with replication and fault tolerance Secure : It must meet our needs for encryption, both at rest and in transit Integrates with Ruby on Rails , the framework we use for backend development Easily scalable : We’re growing and we don’t expect to stop anytime soon SQL : Elasticsearch’s JSON DSL is a nightmare Reasonably priced : We paid $3,600 a month for our 9-node Elasticsearch cluster and did not want to exceed this price point Speed and versatility were our most important requirements, and are also the places where most of the cloud-based solutions fail. We tested some of our more complex queries, both in Redshift and BigQuery, and got disappointing execution times, ranging from 50 seconds to several minutes. Cosmos DB actually showed impressive benchmark results, but doesn’t have official Ruby support. Same for MapD and Kinetica. Our last contender was a memory-based database solution called SingleStore. At first sight, its Enterprise edition seemed to match all of our requirements, including versatility and support for SQL. But the question of whether it would live up to our speed requirements still remained to be seen. Their website boasts the database’s superior speed over many traditional database engines, but this did not sway us from putting the database to test. The results, as can be predicted by reading this blog post up to this point, were stellar, and we will dive more into the technical aspects in Part 2 (coming soon!) Here are some real-life examples of how we use SingleStore within BigBrain for non-trivial analysis (everything beyond charts showing aggregations over time): 1 . End-to-end marketing performance : Many marketing tools, like AdWords Account Manager or the Facebook Ad Manager, give detailed information about how campaigns perform by displaying the campaign’s cost and how many impressions/clicks it generated. We took it a few steps further to include analysis of marketing sources, campaigns, and even single banners. We display the number of visitors each banner and associated campaign brought to our website, how many people signed up, and how many people eventually became paying customers. This allows us to calculate KPIs for each segment, such as its customer acquisition cost (CAC) and the return on investment (ROI). This kind of computation requires us to join six tables, sometimes over tens of millions of records—something that our octa-core PostgreSQL database couldn’t accomplish in under a minute. SingleStore, however, completes the request in a matter of seconds. 2 . Funnels: Our funnels tool allows us to construct an event-based process (out of our 1,800 different engagement types) and see how many visitors, users, and accounts pass through the process within a given time frame. More importantly, it lets us identify weak points where the user experience is not optimal, and in turn fine-tune it in the platform. You can think of it as our UX debugging tool. We can also segment our funnels when running A/B tests, so that we can analyze the behavior of users that are served each of the test’s variants. We build funnels using an iterative process of “join and reduce,” such that each step in the funnel requires a different OUTER JOIN to the engagement storage, with special constraints in place. Here’s an example for our signup process funnel: Our signup process statistics in H2 2017, segmented by browser, starting with 3.6 million unique visitors. Interestingly, people who visit our website on Chrome have a statistically significant higher chance of actually signing up to our trial than Firefox. This funnel was computed in 18 seconds. In my next post, we will dive deeper into how we store our data in SingleStore, how different parts of SingleStore work, and how we adjusted them to run our queries as quickly as possible. We will also show how we built our funnels system so that it can compute any funnel, including non-trivial ones where events don’t necessarily occur in the order specified in the funnel. (Spoiler alert—it’s not as easy as shown in most of the tutorials you will find online). In the third part, we will review the process of launching a SingleStore cluster from scratch and how to set it up as a critical piece of infrastructure in production. The original posting of this blog can be found here .", "date": "2019-01-18"},
{"website": "Single-Store", "title": "choosing-a-time-series-database", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/choosing-a-time-series-database/", "abstract": "Time series data is as old as databases themselves – and also the hot new thing. Interest in the topic has more than doubled during this decade. In a previous blog post, we described how time series data is used and stored. In this blog post, we look at the desirable characteristics of a time series database and evaluate our own database, SingleStore, against those requirements. Time series data is more and more widely used. It’s seen in everything from managing oil drilling rigs to machine learning and AI. Part of the growth comes from the fact that time series data is at the core of the Internet of Things, with frequent sensor readings coming in from all sorts of devices. Uses of time series data include storage for data from Internet of Things (IoT) devices, such as cars, airplanes, and card readers; for financial services applications, such as stock trading, portfolio management, and risk analysis; for e-commerce and marketing platforms, to manage ads, maintain inventory, and fulfill orders; and, increasingly, in day-to-day business management. Companies need to rapidly ingest original transactional data, use it to instantly trigger responses, and make it available for both real-time and longer-term analytics. Time series data is so voluminous that one of the first questions you face is whether to store it at all, or some fraction, sample, or summary of it. The right database can make it much easier and cheaper to store more of the data that you need. The right database can also make it easier to query and analyze, which increases the value of the data. Editor’s Note : Also see our blog post on implementing time series functions with SingleStore . We also have two recorded webinars (from DZone and SingleStore ) and an O’Reilly ebook download covering these topics. Time Series Database Requirements Here’s an example of time series data from our introductory blog post . This record contains all the readings from a sensor for one specific minute of time. The Values field contains semi-structured JSON data; it can contain from 0 to 60 values, with each item containing the second the data was stored and the value when the recording was made. Sensor ID Date Time (min) Values 4257 01012019 0113 [ 04,233518 ] , [ 14,233649 ] What are the requirements for a database that stores data recorded as a time series? In-memory for value alerting . As data arrives, it has to be compared immediately to any alarm trigger values that have been set. In a pipeline, for instance, a high value for pressure may require urgent action due to the danger of a rupture, and a low value may indicate a problem upstream from the sensor location. In-memory for trend alerting . Also, as data arrives, it has to be compared to previous values to see if any trend alarms have been set. For instance, a pipeline may trigger an alert if pressure rises more than 20psi in one minute. This comparison runs much faster if all of the needed records for evaluating the trend are held in memory, or if logic holds relevant high and low values from previous records for comparison purposes. In-memory for applications and dashboards . Applications (which act based on data values) and dashboards (which need to display up-to-date values) need live data in memory to support rapid action (for applications) and continual display updates (for dashboards). Fast access for real-time analytics, machine learning, and AI . Business intelligence programs, ad hoc queries, reports, machine learning algorithms, and AI programs all need fast responsiveness from the data store. This may require data to be in-memory, heavily cached, or efficiently accessed from a combination of memory and disk. High concurrency for real-time analytics . Demand for access to data of all types is growing – and time series data, representing the latest readings, can be among the most valuable data of all. A wide range of people need to be able to access this data at the same time, with no artificial limits as to how many logins are given out or how many queries arrive at once. High capacity . A time series database needs to be both fast and scalable in order to accommodate huge amounts of data – scanning and comparing it on input for alarms, storing it accessibly, and responding robustly to queries against large data sets. Standard SQL functions . The SQL standard was promulgated nearly 50 years ago, and has benefited from decades of hard work on defining, widening, and optimizing performance for SQL-standard functionality. This includes academic research and lots of blood, sweat, and tears in implementation, most of which has been shared back and forth across companies. So high performance for SQL functions is a key asset for a time series database. Custom time-series functions . Time series data can benefit from functionality that isn’t part of the SQL standard, and that has been optimized for performance. Examples include functions to toss incoming data where a sensor reading hasn’t changed significantly, to return only the records with the lowest and highest readings from a large set of records, etc. Earthquake fault creep in the San Francisco Bay Area. Anomaliesgreater than two standard deviations from the norm are marked in red. Earthquake fault data, as shown in the figure, is a good example of the demands placed on a time series database. By monitoring specific sensor readings as they come in, geologists may be able to warn of an upcoming quake in time to save lives. Conversely, by finely examining huge amounts of historical fault data, geologists may learn the underlying patterns that enable them to produce future earthquakes and, again, save lives. Purpose-Built Time Series Databases The open source movement led to an explosion of creativity in many areas, and databases have seen their share of innovation. Most of this has centered in the NoSQL movement, and most time series databases are based on NoSQL. These databases show the strengths and weaknesses of the NoSQL movement as a whole. Firstly, it’s easy to create a new NoSQL database. Since most of them are based on open source, a time series database can be easily, and inexpensively, created by adding time series-specific features to an existing NoSQL code base. Time series databases based on NoSQL also benefit from scalability, a key aspect of NoSQL that is lacking in traditional transactional and analytics databases that have SQL support. However, as the NoSQL name implies, NoSQL databases lack both inherent structure – a key, and valuable, attribute of time series data – and SQL support, making them much harder to query or to use with standard business intelligence (BI) and analytics programs. The lack of structure causes problems beyond the important, but narrow, issue of SQL support. To query a NoSQL database means carefully examining the schema and writing a custom query against it. Complex operations such as different kinds of joins, which have benefited from decades of innovation on the SQL side, are likely to be slow and even buggy in the NoSQL camp. To highlight the strengths and weaknesses of these purpose-built time series databases, we can briefly examine four entries that are, each in their own way, a leader in the field : Graphite . The oldest time series database that’s still viable, Graphite has a wide range of functions. However, it stagnated for several years, as some open source projects do, before receiving a burst of energy with the recent strong interest in time series data. According to the project’s website , graphite does two things: store numeric time-series data and render graphs of the stored data on demand. InfluxDB . The current leader in the time series field, it’s a well-organized project, having fixed early problems with its storage layer. It still has problems with query performance and has recently come up with a new language, IFQL, to address the problem. OpenTSDB . OpenTSDB is based on an internal Google monitoring tool called Borgmon. OpenTSDB is highly scalable and has an up-to-date data model; however, you need to create and maintain a Hadoop cluster to run it , meaning that its scalability requires a fair amount of effort on the part of users. kdb+ . kdb+ is a column-based, largely in-memory database specially designed for time series data that has gained popularity in finance. kdb+ has specific functions for time series data, such as specialized joins that select data around a specific point in time or a specific record. Time Series Database: Why Choose SingleStore? SingleStore is a very strong general-purpose database . It’s in the group of databases called NewSQL, which means SQL databases that are scalable to multiple machines – unlike transactional and analytical databases that gave up scalability, while offering structure and SQL support. SingleStore is among the most mature of the NewSQL offerings; in particular, it’s platform-agnostic, running on all the major public clouds, in virtual machines and containers, and on premises. In addition, it has several features that particularly recommend it for use with time series data. These features include: Scalable speed . SingleStore is very fast on a single machine, and you can scale out for performance by adding machines seamlessly to a cluster. Memory-optimized with on-disk compression . SingleStore started as an in-memory database, then added columnstore support with on-disk compression, both needed for voluminous time series data. Pipelines SingleStore Pipelines apply transformations to data on ingest. They streamline the traditional ETL process into a very fast set of operations, so you can pre-process time series data as needed for analytics. Pipelines to stored procedures SingleStore can run streaming data against stored procedures on ingest, expanding the range of operations you can quickly run on incoming data. SQL support As mentioned, SingleStore has full ANSI SQL support, with all the advantages this implies for queries, analytics, and machine learning and AI programs which use SQL. Because SingleStore has specialized functionality for smoothly interacting with data across memory and disk, including large memory caches for columnstore tables and compression that’s high-performance for both reads and writes, the usual strong disparity between in-memory and on-disk performance is greatly reduced with SingleStore. This allows you to use timestore data more flexibly across a range of use cases, from alerting to reporting and predictive analytics. SingleStore currently lacks some specialized time series functionality. SingleStore does have extra-fast SQL operations and scalability, which can result in better overall query performance and responsiveness than dedicated time series databases. It’s also highly extensible, and has already been extended by SingleStore and specific customers to add time series functionality. Here are three ways to add needed functionality: Create the join you need in ANSI SQL. If needed (ie if it will be run repeatedly), optimize it using the query profiler in SingleStore Studio . If using SingleStore Pipelines, run the join as a stored procedure tied to a Pipeline. The table below summarizes pluses and minuses of dedicated time series databases, largely based on NoSQL, vs. SingleStore for time series data. Dedicated time series database SingleStore In-memory support Strong Strong Columnstore/compression Fair Strong Ingest performance Varies Strong SQL support Poor Strong Query performance Poor Strong Time series-specific optimizations Strong Fair; use ANSI SQL, user-defined functions and stored procedures for specific query needs Time Series Databases: Final Thoughts To sum up, SingleStore is a strong general-purpose database that has the majority of the functionality you need for time series, but lacks some time series-specific optimizations. The specialty time series alternatives are optimized for purpose. They have the weaknesses, however, of both the limitations of NoSQL – lack of structure, lack of easy queries with SQL, and overall slow query performance. Adopting a new database is a big decision. We recommend that you review our case studies and then try SingleStore for yourself . Or, contact us to schedule a consultation to learn how SingleStore can support your use case.", "date": "2019-01-31"},
{"website": "Single-Store", "title": "an-introduction-to-memsql", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/an-introduction-to-memsql/", "abstract": "If you’ve wondered what makes SingleStore different, how it fits in the database landscape, and why customers are saying great things about it, our updated webinar should help. In the webinar, Mike Boyarski, senior director of product marketing at SingleStore, and an industry veteran, describes the high points of SingleStore’s evolution and how it’s helping customers today. SingleStore is well-recognized and widely used The Limitations of Current Databases Today, organizations leverage data for competitive advantage. In a Harvard Business Review study ( PDF ), organizations that use “data-driven decisions were, on average, 5% more productive and 6% more profitable than their competitors.” Both important and routine decisions are made and implemented on the fly. To quote a recent Capgemini study , “77% of respondents agreed that decision-makers increasingly require data in real-time.” How do current databases try to meet these needs – and where do they fall short? Here are a few trends, and their impact: Data democratization . As more businesspeople require the most up-to-date data for data-driven decisions, they encounter database scale limits, which result in throttled access to data, and cached (stale) views on data. These inefficiencies put organizations behind more nimble competitors, limiting growth. Performance degradation . The need for 24/7 data collection and monitoring can generate very large tables, resulting in slow-performing queries. A common trade-off is to break up the data into separate silos, or batch (that is, delay) the data into low-cost data lakes – which require expert tuning and more caching. This leads to customer complaints about query delays, inaccurate results that can ultimately cause customer attrition, and increased costs for performance workarounds. Expensive upgrades . Established database architectures were never designed to scale to today’s data requirements, and indeed, they don’t scale well. The companies that control these aging solutions charge high prices for add-ons, maintenance contracts, and upgrades that only address part of the performance problems. Customers face decreasing flexibility, reduced competitiveness, and increasing costs. Common Solutions to Today’s Problems There are three commonly tried solutions for getting the needed results from current database infrastructure: Double down on the existing database . Customers can throw hardware at the problem and/or pay for pricey upgrades and add-ons to database software. Often, the result is an incremental improvement in performance, or merely keeping pace with increased data collection and increasing user demands, all at the expense of higher costs and greater complexity. Add specialized caching tiers . One common hardware/software solution is the use of specialized caching tiers. However, these tiers duplicate slowly, can potentially experience data loss in unexpected outages, and are also limited in their SQL coverage. Store objects in NoSQL solutions . Semi-structured data can be saved somewhat more quickly in a NoSQL object store. But this kind of data can break business intelligence tools and requires developer work to construct custom queries, which then run slowly. SingleStore Introduced as an In-Memory and Real-Time Database SingleStore offers a converged architecture with the high-speed ingest performance traditionally found only with NoSQL, tables optimized as needed for memory and disk performance, and a very widely used, industry-standard SQL interface. SingleStore offers five attributes not usually found in a single offering: Extreme performance Massive scalability An easy-to-use SQL architecture Cloud-native, able to run on any cloud infrastructure or commodity hardware Reasonable price, with strong price-performance For a wide range of applicable use cases, SingleStore runs ten times faster, and at one-third the cost of legacy database providers. SingleStore’s converged architecture offers fast ingest and SQL accessto in-memory-optimized and disk-optimized data tables SingleStore was originally introduced as an in-memory database that, unusually, can scale across multiple servers while maintaining ACID compliance, and that offers real-time analytics response. Now, SingleStore is highly flexible, with rowstore usually implemented fully in-memory and columnstore running with a combination of in-memory and disk storage. This gives users a wide range of options for price and performance flexibility. Where SingleStore is Making a Difference SingleStore is making a difference for a variety of use cases across a wide range of customers. The initial customer use case is usually a high-leverage implementation that fixes an important business problem that no other database can. For instance, SingleStore is often used as the database for a real-time dashboard with high volumes of “live” data to ingest and a high number of concurrent queries, including automated queries and those generated directly by users. However, once a customer has SingleStore up and running, they typically come to see SingleStore as a high-performance, reasonably-priced, general-use database; SingleStore then gets implemented broadly across an organization. SingleStore customer use cases include: Financial services . Solutions for banks, insurance companies, and other financial services companies include real-time decision-making, risk management, and fraud detection. Media and communications companies . Ad analytics help media companies show the right ad to the right customer in real time. SingleStore is also frequently used for content personalization and streaming media analytics. And more . SingleStore powers real-time dashboards, IoT analytics, and database modernization projects. Fanatics unified their analytics architecture with SingleStore at the core. As the leading vendor of licensed sports merchandise, Fanatics needs to compete at the highest level of e-commerce. Fanatics created an analytics implementation that takes full advantage of SingleStore’s capabilities. Fanatics put SingleStore at the core of its analytics architecture. The resulting solution is responsive, easy to manage and maintain, and supports queries from business analytics tools, custom reports, and ad hoc access, all using an industry-standard SQL interface. Webinar Recording and Slides You can easily access the webinar recording and slides to learn more about SingleStore. Free Trial You can now use SingleStore for free . Try SingleStore today!", "date": "2019-01-31"},
{"website": "Single-Store", "title": "what-is-time-series-data", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/what-is-time-series-data/", "abstract": "Time series data is as old as databases themselves – and also the hot new thing. Interest in the topic has more than doubled during this decade. In this blog post, we’ll explain what time series data is, why there’s an increasing focus on it, and how SingleStore handles it. In a companion blog post, we explain the considerations that go into choosing a time series database . Time series data is at the core of the Internet of Things, with frequent sensor readings coming in from all sorts of devices. But what is time series data? We provide a brief answer below, for quick reference. For more detail, please see Chapter 1 of our free excerpt from the O’Reilly book, Time Series Databases . For related information, see our companion blog posts on choosing a time series database and implementing time series functions with SingleStore . Also see our time series webinars from DZone and SingleStore . Time series data is inherently well-structured… except when it isn’t. For a simple time series data record in IoT, for example, you might have the sensor number or other identifier, the date, the time, and a reading – that’s it. Sensor ID Date Time (sec) Value 4257 01012019 011304 233518 Notice that, in this simple example, the data is inherently well-structured – and therefore suitable for processing by both traditional transactional databases and analytical data warehouses with SQL support, or more effectively through a “translytical” NewSQL database such as SingleStore. Also notice the timestamp field. More precise – that is, longer – timestamps make time series data more useful. However, they also make the data more voluminous, both in flight and in storage, as well as slower to process for comparison and queries. Given the frequent use of time series data for alerting, as discussed in our next article on this topic, the ability to do quick comparisons on time series data is an especially important consideration. When you begin to collect and work with time series data at scale, the data can overwhelm the capacity of traditional databases at every stage of the data lifecycle: ingest, storage, transactions (if any), and queries. For example, a modern airliner generates half a terabyte of data per flight. A connected car can generate 300TB of data a year . And data that used to be considered disposable, such as routine transaction data, is now being seen as worth capturing and keeping online. Now, how much of this kind of data can you afford to throw away? Using our example above, cars are subject to recalls, safety investigations, lawsuits, and much more. Their manufacturing and performance can be optimized to a significant extent – if you have the information needed to perform the analysis necessary to do it. Cases such as these, where companies collect, analyze, and act on huge amounts of time series data, are expected to grow exponentially in the next 10 years. The sheer volume of time series data, paired with its increasing value, is where the trouble starts; trouble which is (partly) handled by creating more complex data structures (see below). Transaction Records as Time Series Data Time series data has traditionally been associated with simple processes that produce lots of data, such as sensor measurements within a machine. But those of us who focus on transactions have been using – and, in some cases, ignoring – time series data for years. Think of a customer database that includes the customer’s address. Every time the customer moves, that’s treated as a database update. The transaction record comes in, is held, and might append or overwrite the previous customer record. And the transaction record is thrown away – or, at best, is held in a transaction log somewhere in cold storage. In a strict transactional scenario, you now no longer know all sorts of things you could have known. How often do your customers move? Is a given customer moving from lower- to higher-income zipcodes, or heading in the other direction? Does this correlate with – or even predict the direction of – their credit score and creditworthiness? The answers to these questions, and many more, in a strict transactional scenario, are all the same: I don’t know, I don’t know, and I don’t know. Furthermore, if the transactions aren’t online, you have no way of ever knowing these potentially important facts again – at least not without mounting a bunch of tapes currently gathering dust in a storage facility. Part of the current mania for storing every transaction record you get, in raw or lightly processed form, in storage that is at least warm, comes from management’s expecting to be able to answer simple questions like those mentioned above. To be able to answer these questions, the organization must collect, store, and be able to quickly analyze data that was once thought irrelevant. Per-Minute Time Series Data Some of the complexity around processing time series data comes from a clever method used to battle its voluminousness. Sensors are likely to report (or, to be polled) at varying intervals. Instead of creating a separate record for each new reading, you can create one record, for example, per minute (or other time period). Within the record of that minute, you can store as many values as come in. (Or, you can only record changed values, or values that change more than a prescribed amount, storing the time at which the change occurred and the new value.) Because this data is not prescriptively structured – the number of values is not known in advance, so the size of the data in the field can vary – the field that holds the data qualifies as a blob . Sensor ID Date Time (min) Values 4257 01012019 0113 [ 04,233518 ] , [ 14,233649 ] The use of blobs, such as this one, in time series data has long been used as a rationale for using NoSQL databases to store it. For a long time, if you had anything but structured data – if you had unstructured data, or even semi-structured data, such as this – a traditional transactional database, the kind that had SQL support, couldn’t efficiently store it. However, the emergence of JSON as a standard enables a new approach that makes it possible to answer this question differently. You can use key-value pairs in JSON format to store this variable data, and this approach is getting increasingly common. Over the last few years, SingleStore has steadily increased both its ability to store JSON data and the performance of processing and queries against data stored in JSON format. SingleStore is now able to provide excellent performance for JSON processing and queries against JSON data. This capability makes SingleStore fully competitive with bespoke time series databases for many use cases for the things they’re optimized for. At the same time, SingleStore provides capabilities, such as fast transaction processing, support for many simultaneous users (concurrency), and query performance at volume, that most time series databases lack. Time Series Data, Machine Learning, and AI One of the big reasons for the newfound importance of time series data – and the increasing drive to keep as much data as possible, of all types, online – is the increasing use of machine learning and AI. At the most basic level, executives are going to use machine learning and AI to ask the same questions they might have asked before – about such things as a customer’s house moves and their likely income. But now they might also ask far more detailed questions – about customers’ movements within a retail store, for instance, or across a website. But machine learning and AI can also be more or less self-powered . Machine learning algorithms can run against a database and find out interesting things for themselves – things that no one could ever have predicted, such as a tendency among customers signed up at different times of year to be more or less valuable. (Companies have even gotten in hot water for sending baby product discounts to people whose families didn’t know they were expecting.) Machine learning algorithms can identify valuable “hot spots\"among seemingly random correlations. (Source: SingleStore) Machine learning algorithms can identify valuable “hot spots” among seemingly random correlations. (Source: SingleStore) The algorithms can only do their work, though, if the data is there to support this kind of investigation. Companies with more data will have a competitive advantage against their smaller competitors, as well as against those who ran their data storage policies in a more “lean and mean” fashion. Don’t Isolate Your Time Series Data Many organizations have only partially learned their lesson about the value of time series data. There is an increasing drive to retain data and to keep it readily accessible for analytics and transactions, machine learning, and AI. However, the data is often kept in NoSQL databases, such as a Hadoop/HDFS data lake, where it’s harder to analyze . Querying capability slows greatly when each query that you process has to do the work that a database with the right kind of structure – including, where needed, the ability to support semi-structured data in JSON format – has already done for you. SingleStore gives you the best of both worlds. You can keep massive volumes of time series data in SingleStore, using it as an ultra-fast operational data store that also has excellent analytics support (something that NoSQL databases are inherently unsuited for). That way nothing is out of reach of your business. For much more about choosing the right database for your time series data, see our blog post on choosing a time series database . For more about using SingleStore for time series applications, please watch our webinar on the topic .", "date": "2019-01-31"},
{"website": "Single-Store", "title": "case-study-goguardian-fast-analytics", "author": ["JK Kim"], "link": "https://www.singlestore.com/blog/case-study-goguardian-fast-analytics/", "abstract": "GoGuardian is an Education Technology company that specializes in moderating student web activities by using machine learning to facilitate a better learning environment. We combine dynamic data inputs to calibrate student engagement and help educators draw conclusions about their class sessions. This means that there are a large number of events happening from more than 5 million students every single day. As one can imagine, handling all of these events is quite a challenge. This is a reposting of a blog post that first appeared on the GoGuardian site. The original posting of this blog can be found here . This article will detail how we solved our data storage issues and querying challenges with our friends at SingleStore . OUR ENGINEERING CHALLENGES Here at GoGuardian, we understand the value and importance of our customers’ data. We consider and evaluate all of the following points, constantly, for all of our projects and products: – Security : Ensuring the security of our users’ data is our primary concern. – Data fidelity and retention : We want to reconstruct web activity and any gap in data is a functional failure for our products. – Scalability and availability : Our systems must scale to meet the need of millions of concurrent users. – Queryability : Data that is not accessible to our users is useless and should be avoided. While data security is a priority of its own, and deserves its own write-up, I will not be spending much time discussing it here; it is beyond the scope and intent of this article. However, to address the requirements of data retention and queryability, we have some specific technical challenges: 1 . Data generation is cyclical: Most of our users are students in schools, and many schools choose to disable our products when school is not in session. This means the rate of data generation outside of school hours is drastically lower than when school is in session. This is not as difficult to solve as other challenges, but it does pose a headache for resource allocation because the difference between our peak and trough traffic is quite large. 2 . High data throughput: Our servers receive traffic that is generated by more than 5 million students in real time, and each event translates into multiple writes across different tables and databases. (An event roughly corresponds to a collection of web clicks or navigation events.) 3 . Data duplication: A piece of data we saw at time T0 may be updated and reappear at T0 + t aggregated. These two pieces of data are not identical, but consist of the same key with expanded or updated data. For example, an event may have an array of start and end time pairs of [ [ T0, T1 ] ] . However, an event with the same key may appear later with start and end time pairs of [ [ T0, T1 ] , [ T2, T3 ] ] if the event re-occurred within a certain time threshold. The updated event encapsulates both new and old data. By storing only the most up-to-date version of each event, we save considerably on row count and storage for many tables, thus reducing overall compute time. This means that event data is mutable and in some cases we need to update rather than insert . This poses challenges for some databases that are not mutation-friendly. To get around this, we could have redesigned our data generation to support immutable inserts only. However, this would have meant retaining the entire payload of all the generated data, which would make write performance faster but cause the row count to increase, leading to more expensive reads. We chose to optimize for read performance over write performance due to the dynamic nature of our reads, which is discussed more in the next point. 4 . Read query pattern: Our reads are quite dynamic over many dimensions. We group, aggregate and filter by time, classrooms, student, school, URL, and many other factors. Also, most of our queries are aggregate in nature: less than 6 percent are at the row level while over 94 percent require some kind of ranking or aggregation over a dimension. We did discuss pre-calculating some of the requests, but in order to make it feasible we would have had to reduce the degree of dimensions and also reduce how dynamic our queries can be. Doing so would have resulted in removing features from our products, which is unacceptable for us and our customers’ experience. One consolation for us is that our read throughput for this data is not nearly as high as the write throughput. Thanks to various caching strategies, there are about 400 reads per minute. LEGACY SOLUTIONS To address our challenges, we had previously implemented various solutions to meet our query needs. Each solution worked well originally. However, we quickly outgrew the legacy implementations that we once relied on. Before we discuss these, it is important to understand that our intention is not to convey that these solutions are inherently inadequate or insufficient. Instead, what we are trying to say is that when we designed these systems, we had different product requirements than we do now. Eventually, these other solutions no longer fit our needs. Sharded SQL DBs: We started out with a single SQL database. At a certain scale, we could no longer rely on a single-instance database. We implemented a sharding solution to split writes across multiple databases based on a key, with each database holding a subset of the data. One key thing to note here is that these are sets of relational databases that handle high-throughput browser events. This results in a large quantity of rows per table on each shard. When a table is at such a high scale, queries without indexes will have unreasonable latency and thus the queries need to be carefully crafted, especially when joining with other tables. Otherwise, the databases would lock up and result in cascading effects all the way to our customers. The relational, row-based SQL databases handled writes relatively well, as expected. However, reads were problematic, especially considering that most of our queries are aggregate in nature, with many dimensions. Adding more SQL DBs and resharding would obviously help, but we were quickly approaching a point where the cadence of potential resharding couldn’t keep up with our growth. When we talk about databases, one of the most often overlooked factors is maintainability. We are often too focused on latency, performance, and cost, but rarely ever talk about maintainability. Shards do not score very high on the maintainability metric for two reasons: the resharding process and the need for a shard router. Resharding is a resource-straining task and isn’t as simple as adding a database cluster. It needs to be registered with the shard router, load the data for the keys it is now serving, ensure even distribution of the load, etc. These are all possible tasks, but the dull, mundane, and time-consuming nature of that particular work was something we were not thrilled about having to do. The shard router itself was another problem we faced. As you can see in the architecture diagram above, the operation of these shards is dependent on the shard router service that knows which shard is responsible for each key. The reason why we used this stateful mapping is because not all keys are equal in traffic load, and the degree of variance is quite high. To handle such a variance in workload, we decided to allocate keys to shards based on the expected traffic, which resulted in the need for the shard router service. Our database uptime and performance dependency on this shard router service was an undesirable situation and became even more challenging when resharding was involved. pros: - Writes are fast - Fast simple index based fetch (key based query without aggregation) cons: - Aggregate queries are slow (94 percent of our queries) - Not easy to do cross-shard queries - We need to maintain a shard router service - Resharding is an expensive operation Druid: Druid is, unlike the sharded SQL DBs we were using, a columnar database that we had adopted to complement our shards. Our shards were great at inserts, but terrible at aggregate queries, so Druid was the response to supplement our aggregate query requirements. The first point to note about Druid is that it doesn’t do mutation at the row level; there is no row update or delete. The only option for data mutation is to run an index replacement job, which replaces an entire data block in a batch process. Because of the way our data is generated, it necessitates updates of individual rows. This was a major roadblock for us. We ended up having to create non-trivial logic to eliminate the duplicated data during the time threshold when the newer, more correct data could show up. Once the data was finalized, we would trigger a batch job to replace the duplicate data with the finalized, condensed data. Although we no longer had to maintain a shard router like we did in the case of the SQL shards, we now had to maintain another outside component: the index replacement job. While this is largely facilitated by Druid using a Hadoop cluster, it is yet another dependency we didn’t want to manage. Druid is not a relational database, so there are no joins. Though this was not a dealbreaker for us, it was definitely something that the engineering team had to adapt to. The way we design queries and tables, as well as how we think about our data, had to change. On top of that, at the time, Druid did not have basic support for SQL, so the query DSL required us to change a lot of the code that we used to query the data. Druid is a great database that does aggregations across vast amounts of data and dimensions at terrifying speeds (with some query results being “approximate” by design if you read its docs: topn and approx-histograms ). I don’t think there was ever a time where we had to worry about the read latency of Druid that wasn’t induced by process or infrastructure failure, which is quite impressive. However, as we continued to use Druid it became painfully obvious that it did not fit our “upsert” use case. Druid is meant for “insert only” where reads can be very dynamic yet still maintain fast latency through various caches and approximations. I’ll be the first to admit that we abused Druid because it wasn’t a perfect fit for the data we were putting into it. pros: - Fast aggregate queries - Distributed by nature, so scaling is easier cons: - Had to maintain index replacement job - Many moving parts (Hadoop, SQL DB, Zookeeper, and various node types) - No joins and limited SQL support REDESIGN FROM THE GROUND UP When we sat down and looked at our incoming data and query patterns, we were at an all-too-familiar place for an engineering team: we needed the best of both worlds. We had fast writes but slow reads with the SQL shards. We also had fast reads but slow, duplicated writes with Druid. What we needed was the fast writes of row-based databases and the fast aggregate reads of columnar databases. Normally this is where we engineers begin to use our “trade-off” and “expectation management” skills. Nonetheless, in hopes that there were better and cheaper solutions that existed, we began experimenting. What we have tried: Again, I cannot emphasize enough that all the databases below have their own strengths and use cases. 1 . Phoenix: Phoenix is an Apache project that adds a layer on top of HBase that allows SQL queries and joins. Configurations, adaptations, and usage were rather straightforward and we were excited for the potential of Phoenix. However, during our testing we got into an odd state where the entire database was bugged out and no amount of restarts or configuration changes could bring the database back to a functional state. It’s very possible that something went wrong during configuration or usage. However, our production database should be resilient and versatile to the point where any operations should not be able to bring the entire database into an unintentional, unrecoverable and inoperable state. 2 . Druid: Another option was to redesign not only how the data was generated so that updates would no longer be necessary, but to also redesign our Druid schema and services to adapt to such data. However, the transition and implementation for this is difficult. For zero downtime, we would have had to replicate the data ingestion and storage for an extended period of time. Time, cost, and engineering effort for this was significant. Furthermore, we weren’t completely convinced that insert-only data generation was a better choice over our current method of data generation. 3 . BigQuery | Presto | Athena: Although each of these products are different, the principal idea of the query engine decoupled from the storage is similar; they have similar characteristics of great parallel wide queries but not-so-ideal write throughput. Of these solutions, BigQuery has the most optimal write throughput, when writing to native BigQuery storage rather than imposing a schema on top of files. However, we would still need to redesign our data generation to reduce write throughput because even BigQuery didn’t fully address our write needs. Overall, despite us trying various partition strategies and schemas, we couldn’t come up with a confident solution for any of the above. We either ran into another transition difficulty, as we did with Druid, or we had to make compromises in business requirements. They are great for the kind of ad-hoc, non-latency-sensitive queries that are run by our analytics team, but not for our customer-facing products. 4 . Spanner: Spanner is Google’s proprietary, geographically-distributed database that was recently made available to the public via GCP. It is another relational database that shines on strong consistency across multiple regions. For this use case, we didn’t necessarily need the tight and strong consistency that Spanner is known for, but it was a very fast and exciting database to work with. Spanner is a great product with in depth concepts and fascinating features (such as interleaved-tables ) that I was really excited about, and it was one of the most impressive candidates during our testing phase. The problem we ran into was that the cost projection for our usage was higher than that of our existing legacy systems. SingleStore We first learned about SingleStore in the crowded vendor hall of AWS re:Invent 2017. Another engineer and I started sharing our problems with one of their representatives and ended up talking about data throughput, consistency, high availability, transaction isolation, and databases in general. It was one of the most exciting and enlightening conversations I’ve had, and it changed how we served data at GoGuardian. Why SingleStore: SingleStore is a distributed, SQL-compliant database. There are multiple aggregator nodes that serve as the brains of the operation, and multiple leaf nodes that serve as data storage, and the database is coordinated by a single master aggregator. Through simplicity of design, SingleStore was able to achieve complex operations at low latency. 1 . Both row and columnar: If someone were to ask “is SingleStore columnar or row-based?”, my answer would be “yes”. SingleStore supports both types of storage, defined at table creation time. Perhaps most importantly, it allows unions and joins across row-based and columnar tables. I cannot stress enough how important this feature is to us, as it fundamentally changed how we served data by giving us the best of both worlds: the fast writes of a row-store and the fast aggregate reads of a column-store. I don’t know of many database solutions that can support both row and columnar storage types. I certainly don’t know many database solutions that support seamless joins and unions across both types. These features allowed us a degree of flexibility we never had previously. 2 . High availability Machine failure is inevitable and it is something all engineers anticipate and prepare for. We create as many preventions as we can while also preparing for incident mitigation. SingleStore achieves this by making it so that every write is replicated into both a master and a secondary partition. If the original master fails over, the secondary partition becomes the master. 3 . Speed It’s fast. There are some databases that, by design, cannot get better than 500ms latency, regardless of how small or simple the data being queried is. With SingleStore, we are able to see some queries under 30ms when using proper indexes and partition keys. 4 . Friendly support I’ve worked with many big and small companies representing various databases and people. Sometimes, we as technologists run into a product that is new and difficult for us to understand and we need to ask new questions. Sometimes, companies or representative do not handle or communicate very well either by documentation or direct questioning. I’ve been reluctant to use some products based on unresponsiveness and the perceived difficulty of their documentation and community. The folks at SingleStore were generally very helpful. Ever since our conversation on the crowded floor of AWS re:Invent , all the way through the time when we dropped the multiple legacy databases that were replaced by SingleStore, we have always enjoyed their assistance and friendliness; either in the form of 100+ long email threads or support tickets. It has definitely been a pleasant working experience. Our architecture: Let’s recap our challenges. Data duplication : data is volatile for a period of time after creation. High throughpu t: capturing all browsing events of five million students. Aggregate queries : most queries are not simple row retrieval, but aggregates over several dynamic dimensions. What if we write to the row-based table and pay a read latency penalty for the short period of time while the data is volatile, and union that table with a columnar table that holds the final data past that volatile time period? SingleStore allowed us to do this by allowing unions and joins across row and columnar tables seamlessly. As described above, our stream processor continuously writes our data in real time into a row table. Periodically, our batch process dumps data that has become stable into a columnar table with the same schema. When we read the data, the queries are run against a view that is the union of the row and columnar tables. Once we figured out the appropriate partition keys that would minimize our data skew, the speed and efficiency we were able to achieve from this architecture was quite stellar. It is also important to mention that joins are now possible for us. Previously, we couldn’t do joins at all in our columnar storage, and heavily frowned upon it in our row storage due to inefficiency. However, because SingleStore allows us to do both (and it actually works pretty well) we can now have the properly organized and normalized data we data engineers dream about. Tests: Any engineering decision requires a sufficient amount of testing and questioning. Below are some of the results of the tests we ran during our experiments with SingleStore. Here are some points to mention: These tests were run during our proof-of-concept phase. These test results are based on our specific use case. Setup, configuration, design, and code were done by us. Each event was inserted into multiple, semi-normalized tables. Reads may be joined, aggregated, filtered, or ordered based on sample production load. Ran against SingleStore DB 6.5. Our goal was to prove that it could work. Our POC cluster definitely could not handle our production load and our finalized prod environment is bigger and more optimized than the POC. You should do your own testing. 1 . Isolated read throughput test Test Setup: Node Type Count Master m4.2xlarge 1 Aggregator m4.2xlarge 3 Leaf m4.2xlarge 4 Test result: QPS Test Duration Latency 1 qps ~20 mins <200 ms 5 qps 12 mins <200 ms 10 qps 48 mins <200 ms 20 qps 96 mins <300 ms 40 qps 32 mins <600 ms 100 qps 8 mins ~15 sec - Number of parallel read queries per second was continuously increased during the test. - Read queries were our top most frequent queries with a distribution similar to our production load. 2 . Isolated write to row table throughput test Test Setup: Node Type Count Master m4.2xlarge 1 Aggregator m4.2xlarge 3 Leaf m4.2xlarge 4 Test result: Stats Number Avg Latency per 100 events 27ms Avg Throughput 2.49m events / min SingleStore Row (RAM usage) 14.04 gb AVG leaf CPU 62.34% AVG aggregator CPU 36.33% AVG master CPU 13.43% - The reason why there was reduced activity around 17:40 to 17:55 was due to faulty code within our test writer that caused out of memory and the test server was restarted and terminated soon after. 3 . Read while write Test Setup: Node Type Count Master m4.2xlarge 1 Aggregator m4.2xlarge 3 Leaf m4.2xlarge 8 Test result: - Read throughput was pegged at 40 qps. - Write throughput we saw was around 750,000 per second. - Read latency bump we saw was due to us running ANALYZE and OPTIMIZE query during the run to observe its effects. Final thoughts At the end of the day, all the databases that are listed and not listed in here, including SingleStore, Druid, MySQL, Spanner, BigQuery, Presto, Athena, Phoenix, and more, have their own place in this world. The question always comes down to what it takes to make it work for your company’s specific use cases. For us at GoGuardian , we found SingleStore to be the path of least resistance. The ability to perform joins and unions across row and columnar tables is definitely a game-changer that allows us to do so much more. SingleStore isn’t without its own problems; but no solution is perfect. There are some workarounds we had to do to make it work for us and there were a few times when we were disappointed. But they are listening to our feedback and improving the product accordingly. For example, when we told them that we really needed the ability to backup our data to S3, one of their engineers sent us an unreleased version with that feature to start testing with. From there we were able to establish a line of communication with their engineering team to round out the feature and even iron out a bug on their end. This line of communication we were able to establish increased our confidence in adopting SingleStore. Now that everything is up and running smoothly, I’m both proud of what we have accomplished and thankful for the product and the support we have received from the  SingleStore team. I have more confidence than ever before in our infrastructure and its ability to handle strenuous loads during peak hours. Oh, and have I mentioned that we are saving about $30,000 a month over our previous solutions? The original posting of this blog can be found here .", "date": "2019-02-06"},
{"website": "Single-Store", "title": "case-study-memsql-powering-ai-breakthroughs-at-diwo", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/case-study-memsql-powering-ai-breakthroughs-at-diwo/", "abstract": "diwo ® is a new, AI-powered platform that promises to help business users answer real-life challenges, acting as its “cognitive decision-making superpower.” diwo – which stands for “data in, wisdom out” – uses SingleStore to power a significant part of its functionality. diwo was developed to reveal hidden business opportunities and empower users to act on them in a timely manner. The software can run in an AI-powered conversational mode, rather than through a programming or scripting language (though many layers of sophisticated coding underline the system). diwo’s conversational persona, ASK, is powered by a series of distributed microservices, and uses SingleStore for transactions and queries. diwo has been built by a team with strong experience in decision science and data engineering, united by a strong desire to create useful, business-first solutions. “We came to see data science as always in research mode—geared toward exploration and experimentation, rather than value. Businesses were not getting what they should in terms of results,” says the project’s visionary leader, Krishna Kallakuri. “We want to bridge that gap, to build something designed primarily for use by the business community, something that actually demonstrates value on day one.” Going beyond “Self-Service BI,\" diwo provides contextual informationand actionable insights about causes—not just “answering the question.\" diwo runs in the background of day-to-day operations, learning about your business and discovering opportunities to maximize revenues or minimize risks. The software then guides users through optimized, customizable strategies to address those opportunities in real time. While most such systems focus on generating insights—leaving the application of these suggestions up to the user—diwo goes further, achieving what early AI scientists envisioned: talking to a computer to walk you through your real-life business decisions. SingleStore Powers Interactive Querying With diwo, the platform is an active partner in making business decisions. The diwo platform features three personas, ASK, WATCH, and DECIDE. Each has a different role in the information-gathering and decision-making process. Currently, SingleStore helps to power the ASK persona. diwo ASK goes beyond traditional Search BI—it’s designed as a system-led conversation that works to identify the user’s intent in order to solve underlying issues and reach the optimal decision. In responding to a query, diwo needs to assemble, generate, and plumb a dataset that can easily reach dozens of terabytes – quickly and easily. The process is often recursive, with one question/answer pair generating another. Because the user asks queries in natural language, it’s up to diwo to convert that query into machine-actionable requests, steadily working through all the data available to diwo and converging on a useful answer. The distributed nature of diwo’s architecture, and its extreme demands on both data processing and query performance, make SingleStore a natural fit. “Every piece of code we have is distributed, on several levels,” said Kallakuri. After founding one of the fastest growing analytics companies in the midwest 15 years ago, he is now leading the launch of Loven Systems, which owns the diwo project. SingleStore is used in diwo’s ASK conversational persona,which draws on a wide range of data sources in real time. What Drove the Adoption of SingleStore? The development team needed a fast, scalable database to underpin the diwo platform. The team was initially attracted by the speed and flexibility offered by Redis, an in-memory database that runs properly composed queries quite quickly. They also tried Cassandra, the open source database and data management system. However, they found difficulty on two sides: Composing useful queries Getting acceptable performance from queries “The more we dug into it, the more we found that the ability to query is a bottleneck,” said Kallakuri. The problem that the diwo project encountered is the same problem that users so often find with NoSQL: you’re not made to put structure on your data, which initially seems to make things easier. But then you don’t get the benefit of decades of SQL query optimization that SQL databases inherit and can build on. SingleStore has additional advantages, offering the best of both worlds along several different axes: SQL vs. scalability . Traditionally, you could either have structure and SQL, in traditional transactional and analytics databases, or you could have scalability across multiple machine instances, in a NoSQL system. SingleStore is a NewSQL system—SQL that scales. Rowstore vs. columnstore . Most databases offer rowstore, which is in-memory friendly for data ingest and transaction processing, or columnstore, which requires the larger capacity of disk to power analytics. SingleStore supports both, with a memory-led architecture that allows you to decide just how many machine instances and how much RAM, SSD, and disk to use for the performance you need. In-memory vs. disk-based . Some databases are in-memory-only or in-memory-mostly, while others prioritize disk-based storage. SingleStore is truly open to both, separately or at the same time, as needed for your data structure and performance requirements. Structured vs. semi-structured and unstructured . SQL databases long forced you to structure data, or pushed you to NoSQL if you had semi-structured data (including JSON, which is more and more popular) or unstructured data. But SingleStore has high-performance support for geospatial data types and JSON data, supporting hybrid data structures such as BLOBs. This allows you to use semi-structured data freely, with performance close to that of structured data, and with everything in one place. These advantages are part of what attracted diwo to SingleStore. Another aspect is high performance across all of SingleStore’s features. Capabilities such as scanning a trillion rows per second are very useful indeed when you have to offer interactive, conversational-speed responses to business questions that demand complex processing to answer. “We tried to push SingleStore to the worst possible extent to see if it would break,” said Kallakuri. It didn’t. diwo has had to add functionality on top of SingleStore to support its project needs, mostly around dynamic SQL and dynamic stored procedures. The performance and stability of SingleStore make it a solid base to build on. What’s Next for diwo? diwo is just getting started, showing its technology to interested customers—who are generally enthralled. “Nearly every time we do a demo, we get an order,” says Kallakuri, noting that diwo’s Cognitive Decision-Making architecture is industry-agnostic. “We’re in the early stages of working with retail, financial, and automotive organizations, and have taken initial steps into gaming as well.” Now that SingleStore is part of its tech stack, diwo is likely to keep finding novel ways to use it.", "date": "2019-02-15"},
{"website": "Single-Store", "title": "case-study-update-how-novus-partners-manages-2-trillion-with-help-from-memsql", "author": ["Noah Zucker"], "link": "https://www.singlestore.com/blog/case-study-update-how-novus-partners-manages-2-trillion-with-help-from-memsql/", "abstract": "This case study was first published as a video with a brief description. Noah Zucker, Senior Vice President for Technology at Novus Partners , talks about how they use SingleStore for portfolio intelligence, applied to their $2 trillion – yes, two trillion dollars – in assets under management. The case study has proven popular, and the content is still highly relevant today, so we’re releasing a transcript of the content as a blog post. I’m here today to talk about Novus Partners and how we’re using SingleStore to change how the world invests. What is Novus Partners? We are a portfolio intelligence company. We provide a platform used by over 100 investment managers – that’s hedge funds, pension funds, large allocators, home offices – to gain better insights on their investments and to better understand risk. Our platform currently encompasses over two trillion dollars in assets under management. In addition, we have a research platform that our clients use to explore investments from publicly sourced data. Any hedge fund that’s large enough has to file 13F data about their investments. Our users can log in and explore and understand other hedge funds out there, where the risk is, and get trading ideas from that. How Novus Partners Helps Investors Essentially, our mission is to help investors discover their true investment acumen – where their true strengths are, and also understand their risk. Our users log in to the platform, the Alpha platform, and they are presented with a series of pages of interactive graphs, charts, and other tools they can use to explore their investments and get deeper insights than they previously had when they were just looking at things in spreadsheets, or just maybe looking at a graph with just simply returns from the last 10 years. The Novus Alpha platform lets investors take a deep dive into investments You know, lots of these hedge funds have glossy brochures where they show how they beat the market from the last 10 years, but it doesn’t show the deeper picture of where they got those returns from. Do they actually have high returns from last quarter – but also they have a large risk, like an illiquid position, or their investments are in some area that they probably didn’t truly understand. So our users are able to gain deeper insights and bring a kind of moneyball approach to investing, whereas the hedge fund industry traditionally has been more sort of gut instinct investing. Here are some of our clients that use the platform today, who use Novus. Some of the top investment managers in the world. Novus clients include top names in investing Before SingleStore: ETL Headaches Let’s talk about the story before SingleStore, because Novus Partners didn’t always use SingleStore as its main database backing our investment analytics platform. Before SingleStore, we used MongoDB, and when I joined Novus in 2013 I immediately saw that we had some problems. Novus faced big ETL and business headaches before SingleStore You know, we have a client data team. That’s our team that works with our investor customers. The members of our team are very skilled portfolio analysis analysts themselves. They understand investments. They understand the data. But they were spending most of their time not actually doing that, but managing our ETL ( extract, transform, and load – Ed. ) pipeline. What that meant was they had a 24-7 operation. They had to babysit, handhold the ETL process loading the metrics into our platform. If there was a job failure then they’d have to spring into action, shuffling around their data load schedule. In the worst case they’d have to load a large job during the day and that would mean an application slowdown for all of our users while the database was under strain. So, being the new guy, I asked why we only have 12 of these compute nodes implemented in Scala? Why can’t we just put them out on the cloud, just scale out, and have one 100 of these just blasting through all that data? The answer I got back was a little bit interesting. You know actually what I was told was they tried doing that, they tried scaling up, but they really couldn’t go much higher because the database that we’re using, MongoDB, just couldn’t keep up. So it got to a point where we had to actually investigate making a change. We either had to learn to scale out our existing, do the work to scale our existing database, or we had to investigate using something else. And of course, that was an opportunity to investigate other technologies, and SingleStore was one of those. You know, one reason why we decided to make a change was that, using Mongo, there are well understood ways to scale out, and it basically would be a full re-write of our application. You know, we’d have to revisit our data model, introduce sharding, and as an application developer now you’re having to think about scalability and that sort of stuff alongside your business logic. So that is a big undertaking. SingleStore Cuts Load Times by 98% So this is where SingleStore comes in. This is what our actual data pipeline looks like. So as I mentioned our clients, they provide us with data in all sorts of formats, flat text files, Excel spreadsheets, even PDF. We scrape data off PDF format and we load that through a pretty standard ETL process, just data clean-up, and it’s stored in a persistent store of record. Then our platform takes that data out of our store of record, sends it into our Scala-based distributed compute layer, and that does the computations of the portfolio analytics and the metrics that I referred to earlier. It caches that in SingleStore so that, when our customers log in, all that data is available to them at their fingertips. From their perspective these high intensity computations are being done immediately, something that they’re not able to do without us. They might be waiting minutes or hours if they’re trying to crunch those numbers in a spreadsheet or on a traditional database. So that’s of great value to them. And the bottom line for our ETL team was that a typical hedge fund data load went down from 90 minutes to two minutes. So, even if there’s a failure, we can just re-run it. It doesn’t cause a load on our system inter-day. And from a developer’s perspective – and actually, I’m a Scala developer myself, not a data engineer – so from my perspective, SingleStore brought a lot of value. “The Learning Curve is Basically Non-Existent” Now you hear that we’re moving to a new database and the first thing you want to know is like, what’s its interface? And the answer you get back, it just uses the MySQL interface. I think that’s something overlooked perhaps in the SingleStore buzz, but as a developer, that’s a huge win. The learning curve is basically non-existent. You have the whole tool chain available to you. There’s a lot of documentation online. So that’s a great value as a developer. Novus is a heavy user of SingleStore’s first-class JSON support In addition, SingleStore has first class JSON support. Being a Mongo shop, that’s really important, because we did have a stable schema at the time that we were doing the migration. We were able to map a lot of our data to a relational schema, but there are parts that we had to leave in JSON – or maybe that we were iterating quickly over it. (SingleStore’s JSON support is being used by more and more SingleStore customers as both IoT and the use of time series data for IoT and a range of other purposes, continue to increase. – Ed.) We want to do more rapid development and it’s changing. So we want to leave it in JSON format and so that means that there’s a whole lot of code that we don’t have to re-write, that we can just leave as is. Novus is also known for this open source library developed at Novus, the Scala case class serialization library, that works with Mongo, Salat, and we could just leave a lot of that code in place. Moving from Mongo to SingleStore didn’t mean we had to scrap a lot of code. We just left a lot of it as is. So that was a big win for us. So the bottom line in terms of what the impact was of SingleStore for our business, that client data team that I mentioned earlier, they’re focusing more on delivering value to our customers, helping them understand, and their data, and their investments during the integration, the data integration process. SingleStore supports 10x the workers – and cuts ops overhead by more than half You know, when we have a failure inter-day we don’t have that data application slowdown. So that’s important. Our end users don’t have these bad days where things are going slow because we had to run a big job during the middle of the day. Our architecture is not limited by the database at this point. 10X the Workers – with No Code Changes You know, we are able to scale up from just 12 workers to 126. So that’s over a 10X improvement. And, if we want to scale even further, with SingleStore we can just add more servers and we’re not limited by our database architecture. As an application developer I don’t even have to think about changing my code for this increase in scale. I don’t have to revisit the data model. I just have a set of really well written SQL queries, well indexed. I just ask the operations team, “We need to add more servers,” and they get on provisioning those and now we have more scale. So it’s really convenient. Now with Half the Operations Workload One more thing to mention, from a system administration perspective. Before, on Mongo, we had two full-time sysadmins and an architect putting in a significant amount of time during their week, just the care and feeding of that Mongo operation. If we had scaled out Mongo, that was going to potentially be more of that type of work. But with SingleStore we now actually have just one DBA and an architect, maybe a few hours a week, if that, working on SingleStore. For a small company like Novus, less than 100 employees, it really fits our operational model. We don’t have to devote a whole team just to the care and feeding of this data platform. It pretty much takes care of itself, once we have it configured and set up. You can try SingleStore for free today.", "date": "2019-02-26"},
{"website": "Single-Store", "title": "dzone-webinar-time-series-real-time", "author": ["Eric Hanson"], "link": "https://www.singlestore.com/blog/dzone-webinar-time-series-real-time/", "abstract": "Eric Hanson, Principal Product Manager at SingleStore, is an accomplished data professional with decades of relevant experience. This is an edited transcript of a webinar on time series data that he recently delivered for developer website DZone. Eric provided an architect’s view on how the legacy database limits of the past can be solved with scalable SQL. He shows how challenging workloads like time series and big data analytics are addressed by SingleStore, without sacrificing the familiarity of ANSI SQL. You can view the webinar on DZone . Time series data is getting more and more interest as companies seek to get more value out of the data they have – and the data they can get in the future. SingleStore is the world’s fastest database – typically 10 times faster, and three times more cost effective, than competing databases. SingleStore is a fully scalable relational database that supports structured and semi-structured data, with schema and ANSI SQL compatibility. SingleStore has features that support time series use cases. For time series data, key strengths of SingleStore include a very high rate of data ingest, with processing on ingest as needed; very fast queries; and high concurrency on queries. Key industries with intensive time series requirements that are using SingleStore today include energy and utilities; financial services; media and telecommunications; and high technology. These are not all the industries that are using SingleStore, but these four in particular, we have a lot of customers in these industries and these industries use time series data. Editor’s Note : Also see our blog posts on time series data , choosing a time series database , and implementing time series functions with SingleStore . We also have an additional recorded webinar (from us here at SingleStore ) and an O’Reilly ebook download covering these topics. Introduction to SingleStore SingleStore has a very wide range of attributes that make it a strong candidate for time series workloads. You can see from the chart that SingleStore connects to a very wide range of other data technologies; supports applications, business intelligence (BI) tools, and ad hoc queries; and runs everywhere – on bare metal or in the cloud, in virtual machines or containers, or as a service. No matter where you run it, SingleStore is highly scalable. It has a scale-out, shared-nothing architecture. Ingest The SingleStore database has a very broad ecosystem of tools that work well with it. On the left of the diagram are major data sources that work with SingleStore. SingleStore has a very high-speed ability to ingest data. That includes fast bulk loading and streaming data with our Pipelines feature. SingleStore is partnering with a company to build connectors to relational data stores. We support loading and conductivity with data lakes including Hadoop, HDFS, and Amazon S3. For example, you can stream data in real time into SingleStore using Kafka streaming into pipelines in SingleStore. It’s a very convenient way to load data into SingleStore. Just dump data into a Kafka queue and SingleStore will subscribe to that queue, and be able to load data in real time into tables without writing any code to do that. Also, we support transformations with tools such as Spark and Informatica. Rowstore and Columnstore SingleStore stores data in two types of tables: memory-optimized rowstore tables and columnstore tables that combine the speed of memory and the capacity of disk storage. In-memory rowstore-oriented tables are used for extremely low latency transactions, as well as very fast analytics on smaller quantities of data. Disk-optimized columnstore tables support tremendous scale – petabyte scale. They include built-in compression on write, analytics and queries against compressed data, and high performance. Querying and Reporting Lots of application software already can connect to SingleStore because we support ANSI SQL and use standard relational database connectivity capabilities like ODBC. Pretty much anything that connect to a MySQL system can connect to SingleStore. You can write custom apps to connect to SingleStore easily with APIs that you’re already accustomed to. In addition, a broad range of BI tools and Dashboarding systems can connect to SingleStore, such as Tableau, Looker, and Microstrategy. SingleStore for Time Series With these broad capabilities, and some specific features we’ll describe here, SingleStore is a fantastic time series database. Where SingleStore really shines, and does a fantastic job for time series data, is in those time series use cases where there are one, two, or three of the following requirements: High ingest rate . You’ve got a lot of events per second coming in, or you need to load data extremely fast, perhaps periodically. So high ingest rate. Low-latency queries . You need a very fast, interactive response time for queries. These can come from apps, from BI tools, from ad hoc queries, or a combination. Concurrency needs . SingleStore shines when you have a strong need for concurrency. You simply scale the number of machine instances supporting SingleStore to support as many simultaneous users as you need. When you have one, two, or three of these requirements, SingleStore is a really good choice for time series data. ANSI SQL SingleStore’s built-in support for schema and ANSI SQL is first and foremost in the features that make us a good fit for time series data. SQL supports a lot of powerful capability for filtering, joining, aggregating, and you need that kind of capability when processing time series data. SQL support is a general purpose capability, and it’s needed and applicable for time series as well. Unlike many databases that more or less specialize in time series data, SingleStore supports transactions. So if you’re doing time series, as with any application, you want your data to be permanent and secure and consistent. Transaction support in SingleStore makes that possible. Ingest One of the thing that really people love about processing time series data with SingleStore is fast and easy ingest. You can ingest data in multiple ways, including inserting the data with regular insert statements, a bulk loader to load data, as well as SingleStore Pipelines. You can use whichever technique is most convenient for you and performance will be very fast. I’d like to give some more detail on SingleStore Pipelines. You can create a pipeline that references a Kafka queue, or a file folder in your Linux file system, or AWS S3, or an Azure Blob store. Then you start the pipeline and we directly load the messages from a Kafka queue, or files that land in the file folder that you pointed us at. You don’t need a fetch-execute loop in your application; we handle that for you with our pipelines approach. You can also transform the data with pipelines using external scripts written in Python, or any language you want, using a standard interface. It’s convenient to load with pipelines, and you can transform the data if you need to. We support transactional consistency in our loader. If you load a file, it’s either all going to load, or none of it’s going to load. Queries will see all of the file or none of it. We support exactly-once semantics with Kafka pipelines. You can ingest data into SingleStore phenomenally fast. I can’t emphasize this enough. I got the opportunity to work with our partner Intel using a Xeon Platinum server – a single server with two Xeon Platinum chips, with 28 cores each, and high performance solid state disks (SSDs) attached. I configured it as a cluster – some people call it a cluster in a box. It had two leaf nodes and one aggregator, but all installed on this same machine. Then I just loaded data with a driver that was highly concurrent – using lots and lots of concurrent connections that were running inserts and updates, or upserts, simultaneously, and was able to drive 2.85 million rows per second insert and update on a single server. That is a phenomenal rate, I mean very few applications need much more than that. And, if you need to scale to ingest data faster than that, we can do it. You just have to add more nodes and scale out. Queries SingleStore also supports fast query processing with vectorization and compilation. We compile queries to machine code. I described earlier about how SingleStore is typically an order of magnitude or more faster than legacy, rowstore-oriented databases. One key reason for that is that we compile our queries to machine code. When a query accesses a rowstore table, it’s executing code directly, rather than being interpreted as legacy databases do. We also support window functions, which are very helpful for time series data, and I’ll get into that in detail later on. We support extensibility extensively: stored procedures, user-defined functions, and user-defined aggregate functions. All three of these are useful for managing time series. SingleStore also has excellent data compression. Time series event data can be very voluminous, and so it’s important to be able to compress data to save storage. SingleStore’s columnar data compression does an excellent job of compressing time series events. So let’s talk about how to query data effectively with time, when it’s time series data. Window functions allow you to aggregate data over a window of rows. You have to specify your input set of rows by a partition key. Then within each partition, you will have an ordering or sort order. The window functions compute a result for each row within a partition, so the window function result depends on the order of the rows within that partition. So, as an example, as illustrated on the right above, you may have an input set of rows like that entire larger block. You see the purple color between start and end is your window, and there’s a concept of the beginning of the window, the end of the window and the current row within the window that you can specify with SQL using this new extension in SQL for Window functions. SingleStore supports a bunch of different Window functions. We support ranking functions, as well as value functions like LAG and LEAD, and aggregate functions like SUM, MEAN, MAX, AVERAGE, COUNT, et cetera. Then special percentile functions to give you a percentile value. SingleStore for Time Series Examples I’m going to give a simple example of a time series application, just based on a Tick stream. Imagine that you have very simple financial Tick stream which has a table called Tick, which has a high resolution daytime, daytime six. SingleStore supports a daytime or timestamp type, which has six digits after the decimal place. So it supports resolution down to the microsecond, which is a high resolution timestamp you may need for some of your time series of applications. In this example table, I’ve got a high resolution timestamp, then a stock symbol, and then the trade price. This is oversimplified, but I just want to use this simple example for some future queries I’m going to show you. One important thing that you may need to do with time series data is to smooth the data. So you might have Ticks coming in, or you might have events coming into a time series. Perhaps there’s a lot of noise in those events, and the curve that looks jagged, you want to smooth it out for easier display, easier understanding for your users. You can do that with Window functions and SingleStore. So this is an example of a query that computes the moving average over the last four entries in a time series. In this case, the example is just pulling out the stock ABC, and we want to know what the stock symbol is, what the timestamp is, the original price and the smooth price. So you can see there in the tabular output below how price moves up and down, a little bit chunky. Then the smooth price averages over a sliding window of between three rows preceding and the current row. So that’s how we get the four entries that we’re averaging over for the smooth price. You can define that window any way you want. You could average over the entire window, or rows preceding and the current row, or rows from preceding the current row to after the current row. You can define the window whoever you like. Another important operation that people want to do on time series data is to aggregate over well-defined time buckets. This is called time bucketing. Maybe you want to convert an irregular time series to a regular time series. So an irregular time series has entries at irregular intervals. They may arrive at random intervals, so you might not have say one entry every second. You might have on arrival time on average, one entry every half a second, but maybe like a statistical process, like a Poisson process of arrival time. You might have several seconds between arrivals of Ticks, and you might want to convert that so you’ve got one entry every second. That’s something you can do with time bucketing. Another application of time bucketing is if you may want to convert a high resolution time series to a lower resolution. For example, you might have one entry per second, and you might want to convert it to have one entry per minute. That’s reducing the resolution of your time series. There are a couple of different ways you can do time bucketing in SingleStore. One is to group by a time expression. So, for example, that query that says select ts:> daytime and so on. That first one, that expression ts :> daytime, that’s a typecast in SingleStore. It converts that timestamp to the daytime, not a daytime six. So that is going to have resolution of a single second. We won’t have fractional seconds. You can convert to daytime, take an aggregate, then group by the first expression, and order by the first expression. That’ll convert a high-resolution time series to a one-second granularity time series. You can also use user-defined functions to do time bucketing. I’ve written a time bucket function which takes two arguments. The first argument is a time pattern that says something like one second, one minute, three seconds, et cetera. So you can define with a phrase what your time bucket is. The second argument is the timestamp. You can do this with an expression in the query, but just as a simplification, I’ve written this time bucket user-defined function and there’s a blog post that’ll be coming out soon. (For more detail, and for an additional example with candlestick charts, view the webinar . We will link to the more detailed blog post when it’s available – Ed.) More on SingleStore for Time Series SingleStore can solve time series development challenges for you in a number of different ways, because we have incredibly high performance through our scale-out, our query compilation, and our vectorization. Your time series queries will run fast and we can scale to handle large workloads and large data volumes. Also, we support very fast ingest of time series events through just regular insert statements, upserts, load, or pipelines. Also, we support powerful SQL Window function extensions that are terrific for time series. They’re fully built into SingleStore, as native implementations. If you want, you can make your time series processing a little bit easier using user-defined functions, and user-defined aggregate functions, and stored procedures, just to add more power and flexibility to your time series processing. Finally, SingleStore provides all the capabilities of a distributed SQL DBMS. So if you’re going to build a time series application, you may have a choice between using a purpose-built time series database that’s specific to time series, or a general purpose database like SingleStore. If you choose a general purpose database, you’re going to have a lot of extra benefits that come with that, including SQL transactions, backup and restore, full cluster management, rowstore, indexes on the rowstore, columnstore, concurrency, high availability. You can handle general purpose applications that are transactional. You can handle analytical applications like data warehouses, data marts, operational data stores, or analytical extensions to your operational apps. You have full SQL capability to do outer joins and other kinds of things that may be difficult to do in a time series specific database. So the generality of SingleStore can handle a lot of different application needs. Moreover, SingleStore can still handle time series really effectively with powerful Window functions and user-defined extensions. We invite you to try SingleStore today, if you haven’t tried it already. You can download SingleStore today . SingleStore can be used free in production for up to 128GB of RAM capacity; if some of your data is in columnstore format, that can represent hundreds of gigabytes of total database size. Or, you can use our Enterprise trial, which has unlimited scale, for up to 30 days for development. Q&A Q: There are a few technologies out there that build time series functions on top of PostgreSQL. So the general question is, what would make the SingleStore offering different vs. something that is implemented on top of a PostgreSQL-based solution? A: The main difference is the performance that you’re going to get with SingleStore for ingest and for large-scale query. So a PostgreSQL-oriented system, depending on which one you’re looking at, it may not provide scale-out. Also, SingleStore has an in-memory rowstore and a disk-based columnstore. We compile queries to machine code and use vectorization for columnar queries. We have a higher-performance query processing engine than you might get for standard query processing operations on PostgreSQL. Q. What would be some differences or advantages between a NoSQL-based time series implementation, versus something that SingleStore offers? A. Standard NoSQL systems don’t necessarily have special time series support, and they often don’t support full SQL. So one thing that you find different about SingleStore is that we do support full SQL and Window functions, which are good for processing time series style queries. So that’s the main difference that I see. Q. What are techniques that can allow SingleStore to extend the throughput from an ingest perspective? A. You saw, earlier in the talk, I showed ingesting 2.85 million rows per second into a single server transactionally. So that’s a pretty phenomenal rate. As I said, if you need to scale more than that because we support scale-out, you can add more servers and we can load data – basically limited only by how much hardware you’re providing. You can add extra aggregators and extra leaves. If you’re inserting data directly by adding aggregators, you can increase the insert rate that you can handle. Also, our pipelines capability is fully parallel, at the leaf level. So if you want to do ingest through pipelines, you can scale that out by adding more leaves to your cluster. In addition, our loader is pretty high performance and we’ve done some work in that area recently. In the last release, the 6.7 release, we introduced dynamic data compression into our loader – because when you’re loading data, you may need to shuffle the data across your nodes as part of that operation. We have an adaptive data compression strategy that compresses data as it’s flowing between the leaves over our internal network, if we’re bandwidth limited on the network. So there’s a bunch of techniques you can use to increase the performance of SingleStore for loading by scaling. Then just the implementation of our load is pretty high performance, through techniques like compilation to machine code, and also dynamic compression. Q. Are there limitations to pipelines in terms of the number of concurrent, or parallel pipelines used. Is there any sort of physical limitation there? A. I’m not aware of any limitations other than the capacity of your cluster, the hardware capacity of the cluster. I don’t think there are any fixed limits. Click to view the webinar on DZone .", "date": "2019-02-28"},
{"website": "Single-Store", "title": "memsql-looker-real-time-analytics", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/memsql-looker-real-time-analytics/", "abstract": "SingleStore is a fast, scalable SQL database. Looker is a fast, scalable analytics platform. You can use SingleStore and Looker to create a fast, scalable – yes, those words again – analytics solution that works well across a wide range of data ingest, transaction processing, and analytics needs. Both SingleStore and Looker are flexible and powerful tools. With the ability to provide full ANSI SQL support, SingleStore has the ability to works with a wide range of analytics tools. For Looker, its ability to connect to any SQL data source allows it to work well with a vast number of databases. Looker also optimizes its database interface to take advantage of specific database features, as you will see below. When paired together, SingleStore and Looker combine these areas of strength to deliver consistent and concrete results. For instance, one of the most popular applications for real-time analytics is to create a real-time dashboard. There may not be an easier or more effective way to create such dashboards than to first implement SingleStore and Looker together atop your existing architecture. Use Looker to make creating your dashboard easy, and use SingleStore to make it fast. Speeding Up Analytics with SingleStore and Looker You can use the combination of Looker and SingleStore atop an existing data architecture to make data much easier to access and greatly speed up performance. SingleStore is faster than competing solutions; often twice as fast, at half the cost. You can also use SingleStore to take over some or all of the work currently done by an existing SQL or NoSQL database, further improving performance. A solid example of an organization using SingleStore to speed up analytics performance is the online retail company Fanatics. Fanatics sources and sells branded merchandise for some of the world’s leading sports teams, including the NBA and the NFL, along with global brands such as Manchester United. Fanatics uses SingleStore to create a fast and reliable data architecture for all their analytics needs – including apps, business intelligence (BI) tools, and ad hoc SQL queries. Looker can also be used alongside existing BI and analytics tools. Where you use Looker, you’ll gain high-performance SQL query performance and ease of use, thanks to the LookML modeling layer. By implementing Looker, you can begin to create and foster a true data culture at your organization. One company that has done this is Kollective, which has the demanding job of distributing video content for a wide range of customers, including Fortune 500 companies like ExxonMobil, HSBC, and T-Mobile. Kollective chose SingleStore and Looker and uses the tools together for real-time analytics. You can read the case study or watch a joint webinar presented by people from both companies. You can also use SingleStore to replace both your existing database types – transactional and analytical – with a single, converged database, SingleStore. As you do so, you’re removing existing batch loading and extract, transform, and load (ETL) processes from your overall data flow. This change brings apps and analytics tools – including Looker – closer to your source data. With a properly architected solution, you can achieve near-real-time or real-time analytics. Together, SingleStore and Looker support much broader access to your data. By making data much easier to access, Looker increases the number of people who want to dig into data and the frequency with which they access it. SingleStore contributes, with its outstanding performance and its high degree of concurrency. As a scalable SQL database, SingleStore lets you power your data with the amount of hardware that you need to get the performance that you want, for all the users who need it. SingleStore’s unique ability to offer this kind of solution is mentioned by Looker in Looker’s Pocket Guide to Databases . Looker describes SingleStore as a database that is: Powered by both rowstore functionality, traditionally used mostly for transactions, and columnstore functionality, traditionally used mostly for analytics. A massively parallel processing (MPP) database, capable of smoothly scaling out across multiple nodes. Both a self-managed (aka on-premises) MPP database and an on-demand (aka cloud) MPP database. Looker’s guide to databases features SingleStore Setting Up SingleStore to Work Well with Looker A typical “small” SingleStore implementation has two aggregator nodes, four leaf nodes, 128GB of RAM, and – through the use of mixed rowstore and columnstore data – up to perhaps a terabyte of total data. You add nodes to support larger and larger amounts of data. In its early implementations, SingleStore worked as a very fast, rowstore, in-memory database. Several years ago, SingleStore added columnstore functionality, which keeps data – including strongly compressed data – on disk, with a solid chunk of RAM dedicated for use as a cache over the columnstore. Because SingleStore functions as both a rowstore and columnstore database in one, most operations proceed at or near in-memory speed. This allows data that’s presented in Looker to appear as near-real-time analytics, at a cost closer to that of a disk-based system. More recently, SingleStore has added support for semi-structured data. Geospatial data, JSON data, and AVRO data (a specialized, compressed format based on JSON) are all supported, easy to manage, and with performance very close to fully structured data. You don’t really need to do anything special to SingleStore to make it work well with Looker. In fact, Looker is designed to take full advantage of SingleStore’s capabilities. Looker supports SingleStore’s semi-structured data formats. You don’t have to limit their use in order to keep your data available for analytics, and you don’t have to worry that Looker, as your analytics tool, will bog down on semi-structured data. (In order to take advantage of the data, you first need to schematize it into a relational format.) You can store and manage your data in the way that makes sense for the specific data you’re storing and the queries you’ll be making against it. Second, Looker can flexibly use rowstore or columnstore data. This allows you to maximize your use of either, or both, without worrying about the needs of your analytics program. For example, if you really feel the need for speed, you can keep more of your SingleStore data in memory, assumedly in rowstore format. You can then let Looker do the work needed to efficiently run queries that would normally only work well against columnstore data. Setting Up Looker to Work Well with SingleStore One of the advantages of using Looker with SingleStore is that Looker “gets” SingleStore. Looker works smoothly and well across rowstore and columnstore tables, hiding the implementation details from the people and applications generating the queries, with excellent performance. Looker has specific setup instructions for use with a set of databases that are deeply MySQL-compatible: MySQL, Clustrix, MariaDB, and SingleStore. For all of these databases, you can enable either persistent or regular derived tables . Derived tables are powerful tools that can give you more capability in LookML and more performance from your SQL queries. Looker and SingleStore together also help users to resolve a challenge in using any database that supports SQL, including SingleStore. The challenge is to easily generate SQL that’s optimized for the database in question. With Looker and SingleStore together, you have four options: Write your own SQL . Many people are so SQL-conversant that this is an easy option for them, for simple queries. Let LookML generate SQL for you . Looker generates highly-performant SQL queries that query your database, and are optimized for it, directly from LookML’s modeling layer. Use Looker’s SQL Runner to optimize your query . SQL Runner has a wide range of capabilities, including the ability to test derived tables. Use SingleStore Studio and SingleStore’s command-line tools . With these tools, you can profile and optimize your database for maximum performance against the queries generated as ad hoc SQL queries (#1 above), from Looker (#2 and #3 above), and from other sources, including machine learning and AI programs. Note : Code generated from LookML by Looker (see #2 above) is likely to run faster than handwritten SQL. For instance, Looker takes advantage of the SingleStore Persistent Derived Tables capability to generate optimized tables – in SingleStore – for extremely fast performance of one-time or repeated queries. You can further speed up analytics by optimizing your data storage choices in many ways, taking advantage of SingleStore’s flexible use of rowstore and columnstore tables. For instance, you can construct a dashboard in Looker that’s backed entirely by rowstore tables in SQL for optimal performance. Or you can mix columnstore and rowstore data flexibly to target the price-performance that you need. You can use Looker and SingleStore together to iteratively optimize your database structure and analytics needs. The two companies have been working together for years. For a quick demo of building an analytics app with SingleStore, using Looker as the analytics tool, please view our webinar on the topic. Our SingleStore webinar shows how to set up SingleStore,connect to it from Looker, and quickly create an analytics app. Ready To Get Started? Want to learn more and get started with SingleStore? You can get started for free . Or reach out to our team to learn more about how SingleStore can work for you. And if you’re ready to find out how Looker can enable data-driven insights at your organization, contact the Looker team to request a demo and connect with their experts.", "date": "2019-02-23"},
{"website": "Single-Store", "title": "case-study-improving-risk-management-performance-memsql", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/case-study-improving-risk-management-performance-memsql/", "abstract": "Risk management is a critical task throughout the world of finance (and increasingly in other disciplines as well). It is a significant area of investment for IT teams across banks, investors, insurers, and other financial institutions. SingleStore has proven to be very well suited to support risk management and decisioning applications and analytics, as well as related areas such as fraud detection and wealth management . In this case study we’ll show how one major financial services provider improved the performance and ease of development of their risk management decisioning by replacing Oracle with SingleStore and Kafka. We’ll also include some lessons learned from other, similar SingleStore implementations. Starting with an Oracle-based Data Warehouse At many of the financial services institutions we work with, Oracle is used as a database for transaction processing and, separately, as a data warehouse. In this architecture, an extract, transform, and load (ETL) process moves data between the operational database and the analytics data warehouse. Other ETL processes are also typically used to load additional data sources into the data warehouse. The original architecture was slowed by ETL processes that ran atirregular intervals and required disparate operations skills This architecture, while functional and scalable, is not ideal to meet the growing concurrency and performance expectations that risk management systems at financial institutions need to meet. SingleStore customers have seen a number of problems with these existing approaches: Stale data . Fresh transaction data that analytics users want is always a batch load (into the transaction database), a transaction processing cycle, and an ETL process away from showing up in the OLAP database. Variably aged data . Because there are different data sources with different processing schedules, comprehensive reporting and wide-ranging queries might have to wait until the slowest process has had a chance to come up to date. Operational complexity . Each ETL process is its own hassle, taking up operators’ time, and confusingly different from the others. Fragility . With multiple processes to juggle, a problem in one area causes problems for all the analytics users. Expense . The company has too many expensive contracts for databases and related technology and needs too many people with varied, specialized skills in operations. What’s Needed in a Database Used for Risk Management The requirements for a database used to support risk management are an intensification of the requirements for other data-related projects. A database used for risk management must power a data architecture that is: Fast . Under intense regulatory pressure, financial services companies are responsible for using all the data they have in their possession, now. Slow answers to questions are not acceptable. Up-to-date . The common cycle of running data through an OLTP database, an ETL process, and into an OLAP database / data warehouse results in stale data for analytics. This is increasingly unacceptable for risk management. Streaming-ready . There is increasing pressure for financial services institutions to stream incoming data into and through a database for immediate analytics availability. Today, Kafka provides the fast connections; databases must do their part to process data and move it along smartly. High concurrency . Top management wants analytics visibility across the entire company, while more and more people throughout the company see analytics as necessary for their daily work. This means that the database powering analytics must support large numbers of simultaneous users, with good responsiveness for all. Flexible . A risk management database may need to be hosted near to where its incoming data is, near to where its users are, or a combination. So it should be able to run in any public cloud, on premises, in a container or virtual machine, or in a blended environment to mix and match strengths, as needed to meet these requirements. Scalable . Ingest and processing requirements can grow rapidly in any part of the data transmission chain. A database must be scalable so as to provide arbitrarily large capacity wherever needed. SQL-enabled . Scores of popular business intelligence tools use SQL, and many users know how to compose ad hoc queries in SQL. Also, SQL operations have been optimized over a period of decades, meaning a SQL-capable database is more likely to meet performance requirements. Two important capabilities for a risk management system highlight the importance of these valuable characteristics in the database driving the risk management system. The first area is the need for pre-trade analysis. Traders want active feedback to their queries about the risk profile of a trade. They – and the organization – also need background analysis and alerting for trades that are unusually risky, or beyond a pre-set risk threshold. Pre-trade analysis is computationally intense, but must not slow other work. (See “fast” and “high concurrency” above.) This analysis can be run as a trade is executed, or can be run as a precondition to executing the trade – and the trade can be flagged, or even held up, if the analysis is outside the organization’s guidelines. What-if analysis – or its logical complement, exposure analysis – is a second area that is highly important for risk management. An exposure analysis answers questions such as, “What is our direct exposure to the Japanese yen?” That is, what part of our assets are denominated in yen? It’s equally important to ask questions about indirect exposure – all the assets that are affected if the yen’s value moves strongly up or down. With this kind of analysis, an organization can avoid serious problems that might arise if its portfolios, as a group, drift too strongly into a given country, currency, commodity, and so on. A what-if analysis addresses these same questions, but makes them more specific. “What if the yen goes up by 5% and the Chinese renmibi drops by 2%?” This is the kind of related set of currency movements that might occur if one country’s economy heats up and the other’s slows down. These questions are computationally intense, require wide swaths of all the available data to answer – and must be able to run without slowing down other work, such as executing trades or powering real-time analytics dashboards. SingleStore characteristics such as speed, scalability, and support for a high degree of concurrency allow these risk management-specific needs to be addressed smoothly. Improving Performance, Scale, and Ease of Development with SingleStore Oracle, and other legacy relational databases, are relatively slow. They can only serve as an OLTP or OLAP database (not both in one); they do not support high concurrency or scale without significant added cost and complexity; and they require specialized hardware for acceleration. These legacy relational databases are also very expensive to license and operate compared to modern databases. Oracle has worked to address many of these problems as their customers’ needs have changed. The scalability requirement for its single node architecture foundation can be partly met by scaling up — albeit to a massively expensive and hard to manage system, Exadata. Oracle also meets the SQL requirement, which gives it an advantage over NoSQL systems – but not over modern “NewSQL” databases like SingleStore. After due consideration, the customer chose to move their analytics support from an Oracle data warehouse to SingleStore. The Solution: A Dedicated Database for Analytics To address the challenges with an Oracle-centric legacy architecture, one company we work with decided to move to a dedicated analytics database. This approach puts all the data that’s needed by the company on an ongoing basis into a single data store and makes it available for rapid, ongoing decision-making. It also seeks to reduce the lag time from the original creation of a data item to its reflection in the data store. As part of this effort, all messaging between data sources and data stores is moved to a single messaging system, such as Apache Kafka. ETL processes are eliminated where possible, and standardized as loads into the messaging system where not. This data store does a lot – but not everything. It very much supports ad hoc analytics queries, reporting, business intelligence tools, and operational uses of machine learning and AI. What it doesn’t do is store all of the data for all of the time. There are cost, logistical, and speed advantages to not have all potentially relevant company data kept in this data store. Data not needed for analytics is either deleted or – an increasingly common alternative – batch loaded into a data lake, often powered by Hadoop/HDFS, where it can be stored long-term, and also plumbed as needed by data scientists. The data lake also serves a valuable governance function by allowing the organization to keep large amounts of raw or lightly processed data, enabling audits and far-reaching analytical efforts to access the widest possible range of data, without interfering with operational requirements. SingleStore is well suited for use as a dedicated analytics database. SingleStore features fast ingest via its Pipeline features . It can also handle transactions on data coming in via the Pipeline – either directly, for lighter processing, or through the use of Pipelines to stored procedures for more complex work. Stored procedures add capability to the ingest and transformation process. SingleStore can support data ingest, transactions, and queries, all running at the same time. Because it’s a distributed system, SingleStore can scale out to handle as much data ingest, transformational processing, and query traffic as needed. A separate instance of SingleStore can also be used for the data lake, but that function is more often handled by Hadoop/HDFS or another system explicitly designed as a data lake. Implementing SingleStore The financial services company described above wanted to significantly improve their portfolio risk management capabilities, as well as other analytics capabilities. They also wanted to support both real-time use and research use of machine learning and AI. In support of these goals, the company implemented an increasingly common architecture based on three modern data tools: Messaging with Apache Kafka . The company standardized on Kafka for messaging, speeding data flows and simplifying operations. Analytics database consolidation to SingleStore . A single data store running on SingleStore was chosen as the engine and source of truth for analytics. Standalone data lake with Apache Hadoop . The data lake was taken out of the analytics flow and used to store a superset of the data. As you can see, the core of the architecture became much simpler after the move to SingleStore as the dedicated analytics database. The architecture is made up of four silos. Inputs Each system, every external data source, and each internal source of behavioral data outputs to the same destination – a data streaming cluster running a Kafka-based streaming platform from Confluent . Streaming Data Ingestion The data streaming cluster receives all inputs and data to two different destinations: Analytics database . Most of the data goes to the analytics database. Data science sandbox . Some structured and semi-structured data goes to the data science sandbox. Hadoop/HDFS . All of the data is sent to Hadoop/HDFS for long-term storage. Data Stores SingleStore stores the analytics database and the data science sandbox. Hadoop/HDFS holds the data lake. Queries Queries come from several sources: ad hoc SQL queries; business apps; Tableau, the company’s main business intelligence tool; Microsoft Excel; SAS, the statistics tool; and data science tools. Benefits of the Updated Data Platform The customer who implemented risk management and other analytics, moving from ETL into Oracle to Kafka, SingleStore, and Hadoop, achieved a wide range of benefits. They had begun with nightly batch loads for data, but needed to move to more frequent, intraday updates – without causing long waits or delays in analytics performance. For analytics, they needed sub-second response times for dozens of queries per second. With SingleStore, the customer was able to load data in as soon as it became available. This led to better query performance, with query results that include the latest data. The customer has achieved greater performance, more uptime, and simpler application development. Risk managers have access to much more recent data. Risk management users, analytics users overall, and data scientists share in a wide range of overall benefits, including: Reduction from Oracle licensing costs Reduced costs due to less need for servers, compute cores, and RAM Fresher data – new data available much faster Less coding for new apps Lower TCO Cloud connectivity and flexibility Reduction in operations costs Elimination of maintenance costs for outmoded batch apps More analytics users supported Faster analytics results Faster data science results New business opportunities Why SingleStore for Risk Management? SingleStore is fast – with the ability to scan up to one trillion rows per second . It’s a distributed SQL database, fully scalable. SingleStore supports streaming, in combination with messaging platforms such as Apache Kafka, and supports exactly-once guarantees . SingleStore supports high levels of concurrency and runs everywhere – on premises or in the cloud, in containers or virtual machines. SingleStore customers often begin by moving some or all of their analytics to SingleStore for better responsiveness, greater concurrency, and reduced costs for the platform – including software licensing, hardware requirements, and operations expenses. Customers then tend to find that SingleStore can take over more and more of the data pipeline. The combination of Kafka for messaging, SingleStore for data processing, Hadoop/HDFS as a data lake, and BYOBI (bring your own business intelligence, or BI, tools), can serve as a core architecture for a wide range of data analytics needs. You can try SingleStore today for free . Or, contact us to speak with a technical professional who can describe how SingleStore can help you achieve your goals.", "date": "2019-03-05"},
{"website": "Single-Store", "title": "webinar-choosing-database-time-series-data", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/webinar-choosing-database-time-series-data/", "abstract": "In this webinar, SingleStore Product Marketing Manager Mike Boyarski describes the growth in popularity of time series data and talks about the best options for a time series database, including a live Q&A. You can view the webinar and download the slides here . Here at SingleStore, we’ve had a lot of interest in our blog posts on time series data and choosing a time series database , as well as our O’Reilly time series ebook download . Additionally, we have a webinar on architecture for time series databases from DZone . This webinar, by contrast, does a particularly good job of explaining what you would want in a time series database, and how that fits with SingleStore. We encourage you to read this blog post, then view the webinar . Time series data is growing in use because it’s getting easier and cheaper to generate time series data (more and cheaper sensors), transmit it (faster online and wireless connections), store it (better databases), act on it (more responsive websites and other online systems), and report on it (better analytics tools). In the last twelve months, interest in time series databases has risen sharply. Time series data is used for device monitoring, for energy systems such as oil wells, for manufacturing, for computer operations, in financial pricing and trading, and for marketing automation. You can use it for alerting, monitoring, and – a usage that’s getting more and more important – for real-time response to all sorts of signals. Just for one example, an e-commerce site can monitor current sales of hot products. Combining sales trends with relevancy data, the site can offer each visitor the hottest product that they’re most likely to buy. Time series data goes through a life cycle. It’s generated by software to reflect a real-world event, such as a pressure valve reading or a completed transaction. The data then often goes into a pipeline, to move it on from the issuer and provide functionality such as data recovery and the ability to move to multiple potential consumers. From the pipeline, the data is then transformed by software – for instance, it can be normalized or reformatted. A series of readings that tend to be several seconds apart, for instance, can be consolidated into a single record per minute, using JSON to handle the resulting semi-structured data. (With SingleStore, this can happen during ingest, via the Pipelines capability, including the use of Pipelines to stored procedures .) Time series data has its own life cycle. To effectively support time series, a database needs to meet specific requirements in terms of its ability to support transactions; its scalability; its effectiveness as an operational database; and its usefulness and responsiveness for analytics. Mike delivers a summary assessment of different kinds of database on each of these axes – transaction support, scalability, operational capabilities, and analytics support. For instance, a NoSQL database is likely to be scalable – but unlikely to be all that useful for analytics, because of the very fact that it doesn’t support SQL. Different kinds of databases have differentstrengths and weaknesses for time series data. Fanatics, the leader in branded team merchandise from the NBA, NFL, Champions League football (soccer) teams, and many others, is a proud user of SingleStore . They are also a great example of time series data in use. All the data that Fanatics takes in – from their website, from mobile users, and from point of sale (POS) systems – has time series aspects to it. In the NFL playoffs, for example, sales of team jerseys for the Super Bowl contenders and winners are going to spike. Fanatics can use time series data in the run-up to the big game to predict jersey sales for the winning team and gear up production accordingly. Heck, maybe they even have advance insight into who’s going to win each Super Bowl – but if so, they haven’t shared it with us. Fanatics’ FanFlow analytics architecture, which ingeststime series data of several kinds, is driven by SingleStore. The webinar finished with a brief Q&A, including these questions and answers on time series, implementing SingleStore, and SingleStore vs. other databases. Does SingleStore perform integrity constraints while streaming? Yes, of course. It depends on how fast the data is coming in, whereas full checks. What do you do if you get bursts of old data? For example, some of our devices don’t have an Internet connection, so the data comes in in bursts. With SingleStore, you can use the transaction capability to integrate the out-of-sequence data. And you can use tools that come with SingleStore to help you write queries that give you good answers to data series that have gaps. How does SingleStore perform against Redis? Redis has somewhat limited analytics, it’s not really suited to exploratory analytics. To support that, you then have to copy the data into something else. SingleStore avoids that by Can you say that, using SingleStore, we can skip using data lakes? Yes, we have a customer that is doing this, using SingleStore instead. However, we also have an HDFS connector, so you can also use Hadoop as a data lake, then move appropriate data – or all the data – into SingleStore. How is SingleStore compared to Snowflake? Snowflake is a one-workload environment that’s very good at data warehousing, as is SingleStore. Where we differ is that SingleStore was additionally designed for fast data ingestion, and will give you much better performance. SingleStore also runs many more places – basically everywhere, vs. just two public clouds for Snowflake. Snowflake can also become quite expensive if you run it continually. Intrigued? It’s easy to learn more. View the webinar, including Q&A, and download the slides here .", "date": "2019-02-26"},
{"website": "Single-Store", "title": "webinar-data-innovation-in-financial-services", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/webinar-data-innovation-in-financial-services/", "abstract": "In this webinar , SingleStore Product Marketing Manager Mike Boyarski describes trends in data initiatives for banks and other financial services companies. Data is the lifeblood of modern financial services companies, and SingleStore, as the world’s fastest database , is rapidly growing its footprint in financial services. Financial services companies are leaders in digital transformation initiatives, working diligently to wring business value from the latest initiatives. According to Gartner’s 2019 CIO Agenda survey, Digital transformation is the top priority for banks, followed by growth in revenue; operational excellence; customer experience; cost optimization/reduction; and data and analytics initiatives (which are also likely to show up in the “digital transformation” bucket). As Gartner puts it, “… the digital transformation of banks creates new sources of revenue, supports new enterprise operating models and delivers digital products and services.” We encourage you to review these highlights, then view the webinar recording for Mike’s in-depth treatment of this important topic. Banks Bring Digital Transformation to Life The key data initiatives that banks are pursuing as part of digital transformation include: Premium customer experiences . Areas such as wealth management, always-on advisory services, and personalization are receiving focused attention to improving the customer experience. Smart risk management . Areas of interest include automating compliance reporting, risk management analysis carried out before a trade is made, and governance for big data, as well as greater responsiveness for risk management analysis. Cloud computing . Cloud initiatives include agile machine learning (ML) and AI services, elastic computing and storage, and hybrid architectures across cloud providers and on-premises data processing. AI/ML . Machine learning and AI are being used for fraud detection (often, in real time rather than in daily batch analysis), “robo advisors” that use computer-based intelligence to provide trading and investment advice, and portfolio analysis, including for risk management. How do data innovations support improvements to banking applications? Mike cites the elimination of latency between real-world events, such as a stock trade, and actionable insights, such as offering relevant services; faster data architectures for greater agility; and reducing operational burdens, even while improving responsiveness and concurrency. How Does SingleStore Help? SingleStore helps to meet requirements that are commonly found in banking: Single-node database augmentation. Banks are often looking to improve performance for legacy, single-node relational databases such as Oracle, Sybase, and Netezza. Hadoop acceleration. Hadoop/HDFS implementations are often slow and hard to query, even when additional layers such as Hive and Spark are used to try to improve performance. SingleStore provides scalable SQL for performance and usability. Path to cloud. As a cloud-native database that also excels in on-premises deployments, SingleStore provides a path for banks to innovate while keeping the door open between cloud and on-premises use. Real-time AI/ML. As machine learning and AI move from research to deployment, scalable SingleStore has the capacity, concurrency support, and performance to deliver business value. In particular, banks have strong needs in both of the data table types that mark different use cases in data management. SingleStore is unique in performing strongly on both while offering flexibility between the two: Row-oriented tables (often memory-based). Frequently updated data tables and transactions commonly run in row-oriented tables, as do data with critical timing requirements for accessibility. (Think device control, as in IoT.) Column-oriented tables (usually disk-based). Less frequently updated data tables, often featuring different data aggregations and sort orders for stellar query performance. These tables tend to have high concurrency requirements. (Think business intelligence, operational databases for app support, and operational AI/ML.) Q&A Q. How does SingleStore work with Apache Spark and what use cases does it support? A. Spark is great at doing acceleration for your Hadoop architecture, but in terms of how it works with SingleStore, we see folks using it in two ways: they use it for transforming data in the stream and for using the ML library for processing the data before it lands in SingleStore. We see Spark used more as a transformation layer, especially for the ML logic. Q. How is SingleStore used for geospatial support? A. This is crucial functionality for several users, and Uber has really led the way in taking advantage of our geospatial support (see the Uber engineering blog , view a live video presentation , and visit our customers page – Ed.). But for details, let me refer you to the SingleStore documentation . Q. Can SingleStore work as a dedicated data warehouse, like Netezza? A. SingleStore definitely can work that way. But SingleStore is a unique technology. We provide an ingest engine that’s largely rowstore based, and in memory based. That’s the capability that traditional data warehouses just don’t have. We absolutely can also function as a dedicated data warehouse, and we have customers using us for that. We look forward to publishing some benchmarks to show just how fast we are at these functions. Following Up Intrigued? It’s easy to learn more. Read the case studies referred to in the webinar – wealth management dashboards and portfolio risk management . View the webinar, including Q&A, and download the slides here .", "date": "2019-03-07"},
{"website": "Single-Store", "title": "how-we-use-exactly-once-semantics-with-apache-kafka", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/how-we-use-exactly-once-semantics-with-apache-kafka/", "abstract": "A version of this blog post first appeared in the developer-oriented website, The New Stack . It describes how SingleStore works with Apache Kafka to guarantee exactly-once semantics within a data stream. Apache Kafka usage is becoming more and more widespread. As the amount of data that companies deal with explodes, and as demands on data continue to grow, Kafka serves a valuable purpose. This includes its use as a standardized messaging bus due to several key attributes. One of the most important attributes of Kafka is its ability to support exactly-once semantics. With exactly-once semantics, you avoid losing data in transit, but you also avoid receiving the same data multiple times. This avoids problems such as a resend of an old database update overwriting a newer update that was processed successfully the first time. However, because Kafka is used for messaging, it can’t keep the exactly-once promise on its own. Other components in the data stream have to cooperate – if a data store, for example, were to make the same update multiple times, it would violate the exactly-once promise of the Kafka stream as a whole. Kafka and SingleStore are a very powerful combination. Our resources on the topic include instructions on quickly creating an IoT Kafka pipeline ; how to do real-time analytics with Kafka and SingleStore ; a webinar on using Kafka with SingleStore ; and an overview of using SingleStore pipelines with Kafka in SingleStore’s documentation. How SingleStore Works with Kafka SingleStore is fast, scalable, relational database software, with SQL support. SingleStore works in containers, virtual machines, and in multiple clouds – anywhere you can run Linux. This is a novel combination of attributes: the scalability formerly available only with NoSQL , along with the power, compatibility, and usability of a relational, SQL database. This makes SingleStore a leading light in the NewSQL movement – along with Amazon Aurora, Google Spanner, and others. The ability to combine scalable performance, ACID guarantees, and SQL access to data is relevant anywhere that people want to store, update, and analyze data, from a venerable on-premise transactional database to ephemeral workloads running in a microservices architecture. NewSQL allows database users to gain both the main benefit of NoSQL – scalability across industry-standard servers – and the many benefits of traditional relational databases, which can be summarized as schema (structure) and SQL support. In our role as NewSQL stalwarts, Apache Kafka is one of our favorite things. One of the main reasons is that Kafka, like SingleStore, supports exactly-once semantics. In fact, Kafka is somewhat famous for this, as shown in my favorite headline from The New Stack: Apache Kafka 1.0 Released Exactly Once . What Is Exactly-Once? To briefly describe exactly-once, it’s one of three alternatives for processing a stream event – or a database update: At-most-once . This is the “fire and forget” of event processing. The initiator puts an event on the wire, or sends an update to a database, and doesn’t check whether it’s received or not. Some lower-value Internet of Things streams work this way, because updates are so voluminous, or may be of a type that won’t be missed much. (Though you’ll want an alert if updates stop completely.) At-least-once . This is checking whether an event landed, but not making sure that it hasn’t landed multiple times. The initiator sends an event, waits for an acknowledgement, and resends if none is received. Sending is repeated until the sender gets an acknowledgement. However, the initiator doesn’t bother to check whether one or more of the non-acknowledged event(s) got processed, along with the final, acknowledged one that terminated the send attempts. (Think of adding the same record to a database multiple times; in some cases, this will cause problems, and in others, it won’t.) Exactly-once . This is checking whether an event landed, and freezing and rolling back the system if it doesn’t. Then, the sender will resend and repeat until the event is accepted and acknowledged. When an event doesn’t make it (doesn’t get acknowledged), all the operators on the stream stop and roll back to a “known good” state. Then, processing is restarted. This cycle is repeated until the errant event is processed successfully. SingleStore Pipelines provide exactly-once semantics when connected to the right message broker How SingleStore Joins In with Pipelines The availability of exactly-once semantics in Kafka gives an opportunity to other participants in the processing of streaming data, such as database makers, to support that capability in their software. SingleStore saw this early. The SingleStore Pipelines capability was first launched in the fall of 2016, as part of SingleStore DB 5.5 ; you can see a video here . There’s also more about the Pipelines feature in our documentation – original and updated . We also have specific documentation on connecting a Pipeline to Kafka . The Pipelines feature basically hotwires the data transfer process, replacing the well known ETL (Extract, Transform, and Load) process by a direction connection between the database and a data source. Some limited changes are available to the data as it streams in, and it’s then loaded into the SingleStore database. From the beginning, Pipelines have supported exactly-once semantics. When you connect a message broker with exactly-once semantics, such as Kafka, to SingleStore Pipelines, we support exactly-once semantics on database operations. The key feature of a Pipeline is that it’s fast. That’s vital to exactly-once semantics, which represent a promise to back up and try again whenever an operation fails. Like most things worth having in life, exactly-once semantics places certain demands on those who wish to benefit from it. Making the exactly-once promise make sense requires two things: Having few operations fail. Running each operation so fast that retries, when needed, are not too extensive or time-consuming. If these two conditions are both met, you get the benefits of exactly-once semantics without a lot of performance overhead, even when a certain number of crashes occur. If either of these conditions is not met, the costs can start to outweigh the benefits. SingleStore DB 5.5 met these challenges, and the Pipelines capability is popular with our customers. But to help people get the most out of it, we needed to widen the pipe. So, in the recent SingleStore DB 6.5 release, we announced Pipelines to stored procedures . This feature does what it says on the tin: you can write SQL code and attach it to a SingleStore Pipeline. Adding custom code greatly extends the transformation capability of Pipelines. Stored procedures can both query SingleStore tables and insert into them, which means the feature is quite powerful. However, in order to meet the desiderata for exactly-once semantics, there are limitations on it. Stored procedures are SingleStore-specific; third-party libraries are not supported; and developers have to be thoughtful as to overall system throughput when using stored procedures. Because SingleStore is SQL-compliant, stored procedures are written in standard ANSI SQL. And because SingleStore is very fast , developers can fit a lot of functionality into them, without disrupting exactly-once semantics. Pipelines are Fast and Flexible The Pipelines capability is not only fast – it’s also flexible, both on its own, and when used with other tools. That’s because more and more data processing components can support exactly-once semantics. For instance, here are two ways to enrich a stream with outside data. The first is to create a stored procedure to do the work in SingleStore. The following stored procedure uses an existing SingleStore table to join an incoming IP address batch with existing geospatial data about its location: CREATE PROCEDURE proc(batch query(ip varchar, ...))\nAS\nBEGIN\nINSERT INTO t\nSELECT batch.*, ip_to_point_table.geopoint\nFROM batch\nJOIN ip_to_point_table\nON ip_prefix(ip) = ip_to_point_table.ip;\nEND (For a lot more on what you can do with stored procedures, see our documentation , which also describes how to add SSL and Kerberos to a Kafka pipeline.) You can also handle the transformation with Apache Spark, and you can do it in such a way as to support exactly-once semantics, as described in this article . (As the article’s author, Ji Zhang, puts it: “But surely knowing how to achieve exactly-once is a good chance of learning, and it’s a great fun.”) Once Apache Spark has done its work, stream the results right on into SingleStore via Pipelines. (Which were not available when we first described using Kafka, Spark, and SingleStore to power a model city .) Use Kafka, Spark, SingleStore Pipelines, and stored proceduresfor operational flexibility with exactly-once semantics Try it Yourself You can try all of this yourself, quickly and easily. SingleStore software is now available for free , with community support, up to a fairly powerful cluster. This allows you to develop, experiment, test, and even deploy for free. If you want to discuss a specific use case with us, contact SingleStore .", "date": "2019-03-18"},
{"website": "Single-Store", "title": "case-study-wealth-management-dashboards-powered-by-memsql", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/case-study-wealth-management-dashboards-powered-by-memsql/", "abstract": "In this case study, we describe how SingleStore powers wealth management dashboards – one of the most demanding financial services applications. SingleStore’s scalability and support for fast, SQL-based analytics, with high concurrency, mean that it’s well-suited to serve as the database behind these highly interactive tools. Dashboards have become a hugely popular technique for monitoring and interacting with a range of disparate data. Like the dashboard in a car or an airplane, an effective dashboard consolidates data from many inputs into a consolidated, easy to understand display that responds instantly to both external conditions and user actions. SingleStore is widely used to power dashboards of many different kinds, including one of the most demanding: wealth management dashboards for families, individuals, and institutions. Banks and other financial services companies work hard to meet the needs of these highly valuable customers. These users are highly desired as customers, and as such have high expectations and hold those who provide them services to a very high standard. Data is the lifeblood of financial services companies. More than one bank has described themselves as “a technology company that happens to deal with money,” and many now employ more technology professionals than some large software companies. These financial institutions differentiate themselves on the basis of the breadth, depth, and speed of their information and trading support. So wealth management dashboards offer an important opportunity for these companies to provide the highest possible level of service and stand out from the competition. A wealth management example from Private Wealth Systems. The Wealth Management Opportunity Technology allows wealth management divisions of financial services companies to offer a previously unheard-of level of high-touch, custom services to customers. Little more than a decade ago, much of the banking business was transacted by telephone and paper mail. Financial reporting, even for quite wealthy clients, was in the form of a quarterly statement, compiled and mailed several weeks after the close of a quarter. The proliferation of digitized financial services has made the old way of doing things insufficient – and also offered an alternative. The financial options available to high net worth clients is almost unlimited, and the complexity of one well-off person’s or family’s portfolio may rival what was once considered normal for a medium-sized company. Reporting and regulatory requirements have also exploded, especially since the financial crash of 2007-2009 – when the S&P 500, a broad and “safe” index of stocks, lost more than 50% of its value . Banks are responsible for everything they recommend to clients, as well as being held liable for some recommendations or warnings that they don’t offer. Some clients have successfully sued to recover a large portion of serious financial losses, if the financial institution(s) involved have made actionable mistakes or failures to disclose relevant information. The size of both the opportunity, in terms of varied investment options, and the risk, from market losses, loss of customers, and legal and regulatory exposure, makes customers and those serving them both eager and anxious. Timely, accurate, and voluminous information helps both sides of this charged investment environment: providing accurate information to clients increases their opportunities for productive action and reduces the risk of misunderstandings and mistakes that can prove very expensive for all concerned. Financial service providers combine “make” and “buy” approaches in delivering wealth management systems. At the “buy” end, companies like Private Wealth Systems deliver purpose-built wealth management systems. These can be provided to end clients directly or through white labeling under a financial institutions brand. The financial institution can endeavour to provide more or less added value on top of the third-party service. At the “make” end, a financial services institution can create a service all its own – designing, architecting, building, and running it in-house. Such a service will have privileged access to a financial services company’s proprietary offerings, but must also provide information – if not trading capability – for externally controlled assets. The Requirements and Technical Architecture of Portfolio Dashboard Platforms Providing portfolio management opportunities to wealthy individuals occurs in two steps: the creation of a portfolio dashboard platform, and extending that platform to the individuals and families being served. A portfolio dashboard solution can be thought of as a kind of command center for all the financial information flows available to anyone, anywhere in the world. A financial services company’s clients are likely, as a group, to be invested in nearly every possible kind of investment, in every country in the world. And their asset allocations are likely to shift continuously as they enact complex trading strategies to reduce risk and maximize reward. So the platform itself must ingest all relevant financial data, with no lags or delays, and process it quickly. For instance, a customer may have purchased a financial services company’s index fund, made up of many individual investments. Calculations of the index’s current value and its risk exposures must be made accurately and instantly. The platform must also serve its clients in multi-tenant fashion – the same software platform must support many customers at once, smoothly and efficiently, with no lags, delays, or barriers. Traditional software architectures face many barriers in meeting demands of this kind. It’s become the norm, for example, for one system to bring data in – often in batch mode, imposing delays – and process it. An extract, transform, and load (ETL) process then “lifts and shifts” the data to a data warehouse system, which supports apps, business intelligence (BI) tools, and ad hoc queries. Note that the data warehouse is now separated from incoming data by a batch process and an ETL process – totalling many minutes, or even several hours, in a financial environment where competitive advantage is measured in fractions of a second. Traditional dashboards are slowed by a complex ETL process. The Challenges of Operating Portfolio Dashboards Today The goal of a system serving portfolio dashboards is to provide end-to-end real-time decisioning. Streaming data powers this capability, requiring data streams from a very wide range of reporting sources. The data is processed as it arrives and is immediately made available to users. Traditional systems are inherently unable to meet these requirements. The technical requirements for a portfolio dashboard include: Fast, lock-free transactions . There can be no delays in, or barriers to, bringing incoming data into the system, while also ensuring precision and accuracy of the data. Fast, scalable data processing . Data may be normalized, calculated, merged, or combined to create output data streams. Fast query response . Specific queries – whether generated by an app, a BI tool, or an ad hoc user query – must be answered in tens of milliseconds. The number of queries that can be answered per second gates the performance of apps, BI tools, and analysts. High concurrency . Concurrency is a key requirement for any multi-tenant system. Given the market demands placed on portfolio dashboards, when markets shift, dramatically increased utilization of the dashboard occurs. Given the importance of the insights provided by the system during fast-changing market events, as many customers as reasonably possible must be supported, with no delays. SQL-compliant . Custom apps, BI tools, and ad hoc queries from savvy individuals nearly all use SQL as a lingua franca for communication across the boundaries of messaging systems and data stores. Non-SQL systems impose delays and heavy development burdens on app developers, BI tool providers, and end users. Underlying all of these changes is a step change in the characteristics of the user base. First, the number of people who are wealthy has increased sharply. In addition, more and more of them are “digital natives” – people who either grew up in an always-online environment, or who have adapted rapidly to the new digital reality. These users expect constant access and constant interaction, whenever they feel the need for it. As a result, even the wealth management divisions of banks, where the number of clients numbers in the thousands, rather than the millions, are facing what IT providers used to refer to as “webscale” problems. In addition, all of these requirements are only growing more stringent as new query profiles arrive, driven by predictive analytics, machine learning, and AI. These algorithms differentiate themselves, and their providers, by running more times per second against more streams of data, and providing results faster – further straining the platform. The additional workload can push response times up for all users of a system, whether their own requirements are simple or demanding. And slow ingest or slow data processing can mean the difference between an actionable insight and watching an asset’s value crash, long seconds behind other market actors. The Case of the Single Slow Query In one telling incident, a high net worth individual became accustomed to the fast data updates and sub-second response times that were reliably provided by their wealth management dashboard. But then one query went awry. It took six seconds for the customer to get a response. Six… long… seconds. The customer complained. His bank scrambled to find out what happened. They pinned down the problem, fixed it, and showed the customer how such a delay would never darken their day again. But it wasn’t enough. A rival promised the customer uninterrupted access to their portfolio, with sub-second response times – 24 hours a day, seven days a week, forever. The customer changed banks. The key to solving these problems is in a reliable high performance data architecture, followed by relentless tuning and quality control. How To Improve Performance, Reduce Latency, and Eliminate Complexity with NewSQL Wealth management systems are only one example – if a somewhat extreme one – of the increasing demands on messaging, data processing, and query responsiveness, as needed to support modern applications. For wealth management systems, the challenges involved are highlighted by the demands of the clients that consume these services, But versions of these challenges have arisen – and, in many cases, not been met – for years, across a wide variety of use cases. One crucial underlying problem is that traditional relational database systems are mostly single-core. That is, their core process can only run on one machine at a time. So they can scale up – that is, they get faster if the single machine they run on is replaced by a more powerful one. But they can’t efficiently scale out; that is, they can’t quickly and cost-effectively use the power of multiple servers, yoked together, to deliver faster performance or support more concurrent users. NewSQL is a new class of databases that combines the scalability of NoSQL with the schema and SQL support of traditional databases. This kind of software is hard to create, and the category is still maturing. Some of the leading NewSQL offerings are limited to a specific cloud provider, for instance. SingleStore is the leading database of its kind: a platform-independent, fully scalable, relational solution that fully supports schema and ANSI SQL. Caching as a Limited Solution Wealth management dashboards are one of the most intense examples of a problem that has plagued the database world ever since relational databases standardized around SQL in the 1970s and 1980s. These databases were relational, and fast for small and medium-sized data loads. But they were not horizontally scalable; at the core, performance was restricted by the capabilities of the most robust single server that can be brought to bear on the problem. The companies that offered these database systems were largely unable to work past them. (Oracle’s RAC offering is a brave, but expensive and fragile, attempt to do so.) What the industry tried to offer, instead of a scalable solution, is in-memory caches. RAC is an example of a database provider offering a caching-based solution. Numerous third parties also offered caches to bolt onto existing, single-core database solutions. Unfortunately, such caches bring with them several problems: Unexpectedly slow performance . The increasing demands of users lead to increasing numbers of cache misses. A cache miss is more expensive than a direct read from disk, with no cache at all. It doesn’t take too many cache misses to render a cache counterproductive. Response delays . Even the suspicion of a cache becoming stale leads to the cache being dumped and reloaded. This process causes a delay for all processing, again in excess of the time required for a direct read. SQL breakage . SQL queries that produce an optimized response from a disk-based system, or an even faster response from an entirely in-memory system, produce long waits or even fail when some of the answer is in the cache and some isn’t. Incorrect results . Caching creators face a tough trade-off between allowing a relatively small number of potentially incorrect results and frequently jettisoning cache contents in favor of a cache reload. It’s all too easy for these design choices, or unanticipated conditions, to lead to some number of incorrect results. The answer to these problems is a system – and, in particular, a query processor – built from the ground up to make smart decisions about in-memory, on-disk, and cached data. Legacy database providers have not made the investments required for this kind of holistic solution. It’s been left to new relational database providers, described by the label NewSQL, to find new ways to offer a relational database that supports SQL and works flexibly with disk and memory. Modernizing Wealth Management Dashboards with Kafka and SingleStore A wealth management dashboard must incorporate many streams of data – structured data such as account records, time series data such as stock market updates, and unstructured data such as video feeds. And new feeds may need to be added at any time. These requirements militate for a standardized messaging interface within the data architecture, and Kafka can provide this. Kafka is now widely used as a messaging queue within data architectures, and it integrates well with SingleStore. SingleStore then interacts with a standardized input source, simplifying system design and reducing operational burdens. Among the desirable attributes of Kafka that work well with SingleStore: Streaming data support . Kafka can be used in either asynchronous (batch) or synchronous (streaming) mode to accommodate stock position data, news, and valuable research data. SingleStore’s high performance supports both options well. Distributed . Like SingleStore, Kafka is distributed, so it’s scalable. As a result, Kafka can handle scale and bursts of data without incurring costly offline re-sharding or shuffling. Persistent . Kafka is resilient against data loss, with the ability to copy data into one or more stores before successful receipt of the data is acknowledged. SingleStore is also fully persistent, even for rowstore tables, making the “chain of custody” for data much easier to manage. Publish-subscribe model . Kafka can accommodate a wide range of data inputs (publishers) and data consumers (subscribers). SingleStore then sees a simplified range of inputs, as they’re mostly or entirely coming in through Kafka, and has a robust ability to support analytics outputs, due to its native support for SQL. Exactly-once semantics . Kafka can be used to guarantee that data is accepted into a Kafka pipeline once and only once, eliminating duplicates or incomplete data. SingleStore works with Kafka to help provide end to end exactly-once guarantees. “Source of truth.” Kafka’s attributes make it a good candidate as a source of truth for incoming data that may then be divided among other processes and data stores. SingleStore can ingest most, or all, of the data streams distributed in Kafka. The wealth management use case is a good fit for SingleStore’s Pipelines feature . Data feeds from Kafka – or AWS S3, Hadoop/HDFS, and other sources – can be ingested with a simple Create Pipeline command. Ingest is scalable, distributed, and can be fed directly into a memory optimized rowstore, a disk optimized columnstore, or both at the same time, to deliver dramatic results in the millions of events per second. The code to create a pipeline is quite simple. In this example, from a recent Kafka and SingleStore webinar , a pipeline is created to load tweets from Twitter into a table: CREATE PIPELINE twitter _ pipeline AS LOAD DATA KAFKA “public-kafka.memcompute.com:9092/tweets-json” INTO TABLE tweets WITH TRANSFORM (‘/path/to/executable’, ‘arg1’, ‘arg2’) (id, tweet); START PIPELINE twitter _ pipeline; The combination of Kafka data pipelines and SingleStore’s Pipelines for data ingest make the architecture supporting the dashboard display very simple. (Even though the overall architecture of the system may be complex, with different data sources, each of which requires more or less processing.) The core wealth management dashboard architecture is simple and easy to manage. SingleStore, The Secret Ingredient for Reliably Fast Wealth Management Dashboards SingleStore, along with Kafka pipelines, has been selected and implemented to power wealth management dashboards at a Top 10 US financial institution – one of the five such institutions that have already adopted SingleStore. As in other implementations, SingleStore is paired with Kafka as a kind of messaging bus. Because SingleStore can handle all the inputs, process them as needed, and support a very wide range of user demands – with high concurrency – the SingleStore customer is able to cost-effectively provide an outstanding level of service. Requirements are strict – stricter in the private wealth management area that supports wealth management dashboards than in the general banking part of the business. In the private wealth management area only, the customer is willing to overprovision systems in order to give SingleStore all the resources it needs for optimal performance. In this environment, SingleStore must reliably meet query service level agreements (SLAs) of less than a quarter of a second, and these must be met while simultaneously ingesting batch loads of new data while under heavy and variable load. The customer also demands low variance – no single query can take much longer than the SLA, even if the average query time stays low. SingleStore has not only met these strict requirements for several years running, the company is expanding its footprint within this Top 10 institution. At the same time, SingleStore adoption is growing right across the financial services industry. If you are providing wealth management dashboards, similar financial services, or implementing other kinds of dashboards, this case study may demonstrate that SingleStore can serve as an important part of your solution.", "date": "2019-03-01"},
{"website": "Single-Store", "title": "webinar-data-predictive-analytics-and-ml", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/webinar-data-predictive-analytics-and-ml/", "abstract": "In this webinar, Mike Boyarski and Eric Hanson of SingleStore describe the promise of machine learning and AI. They show how businesses need to upgrade their data infrastructure for predictive analytics, machine learning, and AI. They then dive deep into using SingleStore to power operational machine learning and AI. In this blog post, we will first describe how SingleStore helps you master the data challenges associated with machine learning (ML) and artificial intelligence (AI). We’ll then show how to implement ML/AI functions in SingleStore. At any point, feel free to view the (excellent) webinar. Challenges to Machine Learning and AI Predictive analytics is helping to transform how companies do business, and machine learning and AI are a huge part of that. The McKinsey Global Institute analysis shows ML/AI having trillions of dollars of impact in industry sectors ranging from telecommunications to banking to retail. AI investments are focused in automation, analytics, and fraud, among other areas. However, McKinsey goes on to report that only 15% of organizations have the right technology infrastructure, and only 8% of the needed data is available to AI systems across an organization. The vast majority of AI projects have serious challenges in moving from concept to production, and half the time needed to deploy an AI project is spent in preparation and aggregation of large datasets. Most CIOs expect an increase in AI investment. The machine learning and AI lifecycle has ten steps, and several of them have data-related challenges. SingleStore addresses many of the toughest ones: Define ML use cases . Find specific ML use cases for the project. Data exploration . Perform exploratory data analysis. SingleStore’s ability to aggregate disparate sources of data and its speed and responsiveness, even on large data sets, help here. Select algorithm . Choose the ML algorithm that will best perform the task. Data pipeline and feature engineering . Profile your incoming data to identify the relevant features that will support the ML task. SingleStore helps here due to its flexibility in dealing with data in memory (rowstore) and on disk (columnstore), as well as its scalability to handle arbitrarily large data volumes. Build ML model . Develop the first iteration of the ML model. Iterate ML model . Refine the model to improve performance and efficacy – increasingly, this process can itself be ML-assisted. Present results . Present results of the model in a way that demonstrates its value to stakeholders. SingleStore can power dashboards or other tools for showing the results of ML model refinement. Plan for deployment . Prepare for deployment to production. Operationalize model . Deploy and operationalize ML model in production. This is where SingleStore makes the biggest difference. SingleStore’s speed, scalability, and SQL support all make the implemented model faster and more effective. Monitor model . Monitor the model in production; retrain or rebuild it to add features or improve performance. SingleStore has built-in monitoring tools that can play an important role in the overall monitoring process. SingleStore addresses many of the key challenges of ML & AI, especially operationalization. To sum up, key challenges in ML/AI implementation that are addressed by SingleStore include modernizing data infrastructure; simplifying and accelerating query performance against big data; and adding scalability and convergence to the process of operationalizing AI. Overview of AI/ML Support in SingleStore SingleStore has features that support key aspects of the machine learning and AI lifecycle: Integration with ML/AI tools The transforms capability in SingleStore Pipelines, for seamless scoring of relevant features as data is loaded Using SingleStore extensibility for additional and more complex scoring Built-in vector similarity functions with very fast performance Here’s an example of using the transforms capability in SingleStore Pipelines: CREATE PIPELINE mypipeline AS\nLOAD DATA KAFKA '192.168.1.100:9092/my-topic'\nWITH TRANSFORM ('http://www.singlestore.com/my-transform.tar.gz', 'my-executable.py', '')\nINTO TABLE t For more information, see the SingleStore Documentation . Image recognition is an important capability enabled by ML/AI, and SingleStore has several customers using this today. You can train the model with other data components that connect well to SingleStore, including Apache Spark, TensorFlow, and Gluon. You can then use your model to extract feature vectors (called embeddings) from images. The feature vectors can then be stored in a SingleStore table for fast processing. There are several SingleStore functions that are directly useful for vector similarity matching: DOT _ PRODUCT(vector, vector) EUCLIDEAN _ DISTANCE(vector, vector) JSON _ ARRAY _ PACK(‘ [ float [ , … ] ] ’) SingleStore’s capabilities are applicable to a variety of different job tasks in the machine learning and AI lifecycle. SingleStore makes people more productive right across the ML and AI lifecycle. SingleStore’s connectivity, capabilities, and speed make it a solid choice for machine learning and AI development and deployment. For more information, view the webinar .", "date": "2019-03-28"},
{"website": "Single-Store", "title": "pre-modern-databases-oltp-olap-and-nosql", "author": ["Rick Negrin"], "link": "https://www.singlestore.com/blog/pre-modern-databases-oltp-olap-and-nosql/", "abstract": "In this blog post, the first in a two-part series, I’m going to describe pre-modern databases: traditional relational databases, which support SQL but don’t scale out, and NoSQL databases, which scale out but don’t support SQL. In the next part, I’m going to talk about modern databases – which scale out, and which do support SQL – and how they are well suited for an important new workload: operational analytics . In the Beginning: OLTP Online transaction processing (OLTP) emerged several decades ago as a way to enable database customers to create an interactive experience for users, powered by back-end systems. Prior to the existence of OLTP, a customer would perform an activity. Only at some point, relatively far off in the future, would the back-end system be updated to reflect the activity. If the activity caused a problem, that problem would not be known for several hours, or even a day or more, after the problematic activity or activities. The classic example (and one of the main drivers for the emergence of the OLTP pattern) was the ATM. Prior to the arrival of ATMs, a customer would go to the bank counter to withdraw or deposit money. Back-end systems, either paper or computer, would be updated at the end of the day. This was slow, inefficient, error prone, and did not allow for a real-time understanding of the current state. For instance, a customer might withdraw more money than they had in their account. With the arrival of ATMs, around 1970, a customer could self-serve the cash withdrawal, deposits, or other transactions. The customer moved from nine to five access to 24/7 access. ATMs also allowed a customer to understand in real time what the state of their account was. With these new features, the requirements for the backend systems became a lot more complex. Specifically data lookups, transactionality, availability, reliability, and scalability – the latter being more and more important as customers demanded access to their information and money from any point on the globe. The data access pattern for OLTP is to retrieve a small set of data, usually by doing a lookup on an ID. For example, the account information for a given customer ID. The system also must be able to write back a small amount of information based on the given ID. So the system needs the ability to do fast lookups, fast point inserts, and fast updates or deletes. Transaction support is arguably the most important characteristic that OLTP offers, as reflected in the name itself. A database transaction means a set of actions that are either all completed, or none of them are completed; there is no middle ground. For example, an ATM has to guarantee that it either gave the customer the money and debited their account, or did not give the customer money and did not debit their account. Only giving the customer money, but not debiting their account, harms the bank; only debiting the account, but not giving the customer money, harms the customer. Note that doing neither of the actions – not giving the money, and not debiting the account – is an unhappy customer experience, but still leaves the system in a consistent state. This is why the notion of a database transaction is so powerful. It guarantees the atomicity of a set of actions, where atomicity means that related actions happen, or don’t happen, as a unit. Reliability is another key requirement. ATMs need to be always available, so customers can use one at any time. Uptime for the ATM is critical, even overcoming hardware or software failures, without human intervention. The system needs to be reliable because the interface is with the end customer and banks win on how well they deliver a positive customer experience. If the ATM fails every few times a customer tries it, the customer will get annoyed and switch to another bank. Scalability is also a key requirement. Banks have millions of customers, and they will have tens of thousands of people hitting the back-end system at any given time. But the usage is not uniform. There are peak times when a lot more people hit the system. For example, Friday is a common payday for companies. That means many customers will all be using the system around the same time to check on the balance and withdraw money. They will be seriously inconvenienced – and very unimpressed – if one, or some, or all of the ATMs go down at that point. So banks need to scale to hundreds of thousands of users hitting the system concurrently on Friday afternoons. Hard to predict, one-off events, such as a hurricane or an earthquake, are among other examples that can also cause peaks. The worst case is often the one you didn’t see coming, so you need a very high level of resiliency even without having planned for the specific event that ends up occurring. These requirements for the OLTP workload show up in many other use cases, such as retail transactions, billing, enterprise resource planning (widely known as ERP), customer relationship management (CRM), and just about any application where an end user is reviewing and manipulating data they have access to and where they expect to see the results of those changes immediately. The existing legacy database systems were founded to solve these use cases over the last few decades, and they do a very good job of it, for the most part. The market for OLTP-oriented database software is in the tens of billions of dollars a year. However, with the rise of the Internet, and more and more transactional systems being built for orders of magnitude more people, legacy database systems have fallen behind in scaling to the level needed by modern applications. The lack of scale out also makes it difficult for OLTP databases to handle analytical queries while successfully, reliably, and quickly running transactions. In addition, they lack the key technologies to perform the analytical queries efficiently. This has contributed to the need for separate, analytics-oriented databases, as described in the next section. A key limitation is that OLTP databases have typically run on a single computing node. This means that the transactions that are the core of an OLTP database can only happen at the speed and volume dictated by the single system at the center of operations. In an IT world that is increasingly about scaling out – spreading operations across arbitrary numbers of servers – this has proven to be a very serious flaw indeed. OLAP Emerges to Complement OLTP After the evolution of OLTP, the other major pattern that has emerged is OLAP. OLAP emerged a bit after OLTP, as enterprises realized they needed fast and flexible access to the data stored in their OLTP systems. OLTP system owners could, of course, directly query the OLTP system itself. However, OLTP systems were busy with transactions – any analytics use beyond the occasional query threatened to bog the OLTP systems down, limited to a single node as they were. And the OLAP queries quickly became important enough to have their own performance demands. Analytics use would tax the resources of the OLTP system. Since the availability and reliability of the OLTP system were so important, it wasn’t safe to have just anyone running queries that might use up resources to any extent which would jeopardize the availability and reliability of the OLTP system. In addition, people found that the kinds of analytics they wanted to do worked better with a different schema for the data than was optimal for the OLTP system. So they started copying the data over into another system, often called a data warehouse or a data mart. As part of the copying process, they would change the database schema to be optimal for the analytics queries they needed to do. At first, OLTP databases worked reasonably well for analytics needs (as long as they ran analytics on a different server than the main OLTP workload). The legacy OLTP vendors included features such as grouping and aggregation in the SQL language to enable more complex analytics. However, the requirements of the analytics systems were different enough that a new breed of technology emerged that could satisfy analytics needs better, with features such as column-storage and read-only scale-out. Thus, the modern data warehouse was born. The requirements for a data warehouse were the ability to run complex queries very fast; the ability to scale to handle large data sets (orders of magnitude larger than the original data from the OLTP system); and the ability to ingest large amounts of data in batches, from OLTP systems and other sources. Query Patterns Unlike the OLTP data access patterns that were relatively simple, the query patterns for analytics are a lot more complicated. Trying to answer a question such as, “Show me the sales of product X, grouped by region and sales team, over the last two quarters,” requires a query that uses more complex functions and joins between multiple data sets. These kinds of operations tend to work on aggregates of data records, grouping them across a large amount of data. Even though the result might be a small amount of data, the query has to scan a large amount of data to get to it. Picking the right query plan to optimally fetch the data from disk requires a query optimizer. Query optimization has evolved into a specialty niche within the realm of computer science; there are only a small number of people in the world with deep expertise in it. This specialization is key to the performance of database queries, especially in the face of large data sets. Building a really good query optimizer and query execution system in a distributed database system is hard. It requires a number of sophisticated components including statistics, cardinality estimation, plan space search, the right storage structures, fast query execution operators, intelligent shuffle, both broadcast and point-to-point data transmission, and more. Each of these components can take months or years of skilled developer effort to create, and more months and years to fine-tune. Scaling Datasets for data warehouses can get quite big. This is because you are not just storing a copy of the current transactional data, but taking a snapshot of the state periodically and storing each snapshot going back in time. Businesses often have a requirement to go back months, or even years, to understand how the business was doing previously and to look for trends. So while operational data sets range from a few gigabytes (GBs) to a few terabytes (TBs), a data warehouse ranges from hundreds of GBs to hundreds of TBs. For the raw data in the biggest systems, data sets can reach petabytes (PBs). For example, imagine a bank that is storing the transactions for every customer account. The operational system just has to store the current balance for the account. But the analytics system needs to record every transaction in that account, going back for years. As the systems grew into the multiple TBs, and into the PB range, it was a struggle to get enough computing and storage power into a single box to handle the load required. As a result, a modern data warehouse needs to be able to scale out to store and manage the data. Scaling out a data warehouse is easier than scaling an OLTP system. This is because scaling queries is easier than scaling changes – inserts, updates, and deletes. You don’t need as much sophistication in your distributed transaction manager to maintain consistency. But the query processing needs to be aware of the fact that data is distributed over many machines, and it needs to have access to specific information about how the data is stored. Because building a distributed query processor is not easy, there have been only a few companies who have succeeded at doing this well. Getting the Data In Another big difference is how data is put into a data warehouse. In an OLTP system, data is entered by a user through interaction with the application. With a data warehouse, by contrast, data comes from other systems programmatically. Often, it arrives in batches and at off-peak times. The timing is chosen so that the work of sending data does not interfere with the availability of the OLTP system where the data is coming from. Because the data is moved programmatically by data engineers, you don’t need the database platform to enforce constraints on the data to keep it consistent. Because it comes in batches, you want an API that can load large amounts of data quickly. (Many data warehouses have specialized APIs for this purpose.) Lastly, the data warehouse is not typically available for queries during data loading. Historically, this process worked well for most businesses. For example, in a bank, customers would carry out transactions against the OLTP system, and the results could be batched and periodically pushed into the analytics system. Since statements were only sent out once a month, it didn’t matter if it took a couple of days before the data made it over to the analytics system. So the result is a data warehouse that is queryable by a small number of data analysts. The analysts run a small number of complex queries during the day, and the system is offline for queries while loading data during the night. The availability and reliability requirements are lower than an OLTP system because it is not as big a deal if your analysts are offline. You don’t need transactions of the type supported by the OLTP system, because the data loading is controlled by your internal process. The NoSQL Work Around For more information on this topic, read our previous blog post: Thank You for Your Help, NoSQL, But We Got It from Here . As the world “goes digital,” the amount of information available increases exponentially. In addition, the number of OLTP systems has increased dramatically, as has the number of users consuming them. The growth in data size and in the number of people who want to take advantage of the data has outstripped the capabilities of legacy databases to manage. As scale-out patterns have permeated more and more areas within the application tier, developers have started looking for scale-out alternatives for their data infrastructure. In addition, the separation of OLTP and OLAP has meant that a lot of time, energy, and money go into extracting, transforming, and loading data – widely known as the ETL process – between the OLTP and OLAP sides of the house. ETL is a huge problem. Companies spend billions of dollars on people and technology to keep the data moving. In addition to the cost, the consequence of ETL is that users are guaranteed to be working on stale data, with the newest data up to a day old. With the crazy growth in the amount of data – and in demand for different ways of looking at the data – the OLAP systems fall further and further behind. One of my favorite quotes, from a data engineer at a large tech company facing this problem, is: “We deliver yesterday’s insights, tomorrow!”. NoSQL came along promising an end to all this. NoSQL offered: Scalability . NoSQL systems offered a scale-out model that broke through the limits of the legacy database systems. No schema . NoSQL abandoned schema for unstructured and semi-structured formats, abandoning the rigid data typing and input checking that make database management challenging. Big data support . Massive processing power for large data sets. All of this, though, came at several costs: No schema, no SQL . The lack of schema meant that SQL support was not only lacking from the get-go, but hard to achieve. Moreover, NoSQL application code is so intertwined with the organization of the data that application evolution becomes difficult. In other words, NoSQL systems lack the data independence found in SQL systems. No transactions . It’s very hard to run traditional transactions on unstructured or semi-structured data. So data was left unreconciled, but discoverable by applications, that would then have to sort things out. Slow analytics . Many of the NoSQL systems made it very easy to scale and to get data into the system (i.e., the data lake). While these systems did allow the ability to process larger amounts of data than ever before, they are pretty slow. Queries could take hours or even tens of hours. It was still better than not being able to ask the question at all, but it meant you had to wait a long while for the answer. NoSQL was needed as a complement to OLTP and OLAP systems, to work around the lack of scaling. While it had great promise and solved some key problems, it did not live up to all its expectations. The Emergence of Modern Databases With the emergence of NewSQL systems such as SingleStore, much of the rationale for using NoSQL in production has dissipated. We have seen many of the NoSQL systems try to add back important, missing features – such as transaction support and SQL language support – but the underlying NoSQL databases are simply not architected to handle them well. NoSQL is most useful for niche use cases, such as a data lake for storing large amounts of data, or as a kind of data storage scratchpad for application data in a large web application. The core problems still remain. How do you keep up with all the data flowing in and still make it available instantly to the people who need it? How can you reduce the cost of moving and transforming the data? How can you scale to meet the demands of all the users who want access to the data, while maintaining an interactive query response time? These are the challenges giving rise to a new workload, operational analytics. Read our upcoming blog post to learn about the operational analytics workload, and how NewSQL systems like SingleStore allow you to handle the challenges of these modern workloads.", "date": "2019-04-10"},
{"website": "Single-Store", "title": "what-makes-a-database-cloud-native", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/what-makes-a-database-cloud-native/", "abstract": "SingleStore has been designed and developed as a distributed relational database, bringing the effectiveness of the relational database model into the new world of the cloud, containers, and other software-defined infrastructure – as described in a new report from 451 Research . Today, most of our customers run our software using some combination of the cloud and containers, with many also running it on-premises. Today, we are purveyors of the leading platform-independent NewSQL database . Having recently joined the Cloud Native Computing Federation , we’d like to take this opportunity to answer the question: “What makes a database cloud-native?” Cloud-Native Software Definition There are many definitions of “cloud-native software” available. 451 Research states that cloud-native software is “designed from the ground up to take advantage of cloud computing architectures and automated environments, and to leverage API- driven provisioning, auto-scaling and other operational functions.” The company continues: “Cloud-native architecture and software include applications that have been redesigned to take advantage of cloud computing architectures, but are not limited to cloud applications – we see cloud-native technologies and practices present in on-premises environments in the enterprise.” The point is repeated in one of the major headings in the report: “Cloud-native isn’t only in the cloud.” 451 Research commonly finds cloud-native technologies and practices being used in on-premises environments. What Cloud-Native Means for SingleStore Let’s break down the 451 Research definition of cloud-native and see how it applies to SingleStore. Takes Advantage of Cloud Features The first point from the 451 Research report states that cloud-native software is “ designed from the ground up to take advantage of cloud computing architectures and automated environments ”. SingleStore has been available on the major public cloud platforms for years , and deployments are balanced across cloud and on-premises environments. More importantly, SingleStore’s unique internal architecture gives it both the scalability that are inherent to the cloud and the ability to support SQL for transactions and analytics. An important step has been qualifying SingleStore for use in containers. SingleStore has been running in containers for a long time, and we use a containerized environment for testing our software . Leverages Software Automation The report then goes into more detail on this point. Cloud-native software will “ leverage API-driven provisioning, auto-scaling and other operational functions .” The ultimate goal here is software-defined infrastructure , in which the software stack is platform-independent and can be managed automatically, by other software. SingleStore has command-line tools that integrate easily with on-premises deployment tools, such as Ansible, Chef, and Puppet, and cloud deployment mechanisms such as Azure Resource Management and CloudFormation. This is crucial to the definition and nature of cloud-native, and SingleStore’s automatability is crucial to its inclusion as cloud-native software. SingleStore Studio provides a monitoring environment for SingleStore across deployment platforms – that is, across public cloud providers, private cloud, and on-premises. Not Limited to Cloud Applications Concluding their key points, 451 Research then states: “ Cloud-native architecture and software include applications that have been redesigned to take advantage of cloud computing architectures, but are not limited to cloud applications… .” The point here is that “cloud-native” doesn’t mean “cloud-only”. Cloud-native describes a set of capabilities that can be deployed anywhere — in public cloud providers, in modernized data centers, and increasingly at the edge. The cloud-native movement combines with the unique features of SingleStore to create something really exceptional: a database that can leverage different deployment locations with ease. Flexibility and portability are creating a capability that hasn’t been available before. Specific SingleStore features make it particularly suitable for cloud-native deployments: Container-friendly . As mentioned above, SingleStore runs well in containers – which is a defining characteristic for cloud-native software. Fully scalable . Like NoSQL databases , and unlike traditional relational databases, SingleStore is fully scalable within a cloud, on-premises, or across clouds and on-prem. Kafka and Spark integration . Apache Kafka and Apache Spark are widely used for data transfer in cloud-native applications, and both work very smoothly with SingleStore Pipelines. Microservices support . SingleStore’s performance, scalability, and flexibility are useful in microservices implementations, considered emblematic of cloud-native software. Next Steps SingleStore’s architecture and capabilities are unique and allow for unbeatable performance and effortless scale — especially when paired with elastic cloud infrastructure. An example is customers who want to move from on-premises Oracle deployments to cloud-native technologies. SingleStore improves on Oracle’s performance and reduces cost while modernizing data infrastructure. Try SingleStore for free today, or contact us to learn how we can help support your cloud adoption plans", "date": "2019-04-10"},
{"website": "Single-Store", "title": "case-study-how-customers-are-using-memsql-for-free", "author": ["Kristi Lewandowski"], "link": "https://www.singlestore.com/blog/case-study-how-customers-are-using-memsql-for-free/", "abstract": "On November 6, 2018, we made our then-new production product, SingleStore DB 6.7, free to use, up to certain limits, described below. To date, we’ve had more than 4,500 sign-ups for this version, with numerous individuals and companies using SingleStore to do some amazing things. To quickly recap, with SingleStore any customer can sign up and start using our full featured product, with all enterprise capabilities, including high availability and security, for free. In talking to customers about their experience and what they’ve built, feedback has been astounding, with folks telling us they can’t believe what we’re giving away for free. We originally stated that what could be done for free with SingleStore, legacy database companies would charge you up to $100,000 a year. Now, we want to tell you what companies are actually doing with SingleStore. Culture Amp Culture Amp is an employee feedback platform and was looking for a way to improve data-driven decision making with internal reporting tools. The company’s initial solution had low adoption due to poor performance, mostly due to a slow database engine. Scott Arbeitman, analytics and data engineering lead at Culture Amp, started investigating a better database to power its reports. “Trying SingleStore for free was a no-brainer,” according to Scott. The results were outstanding. Tests running the company’s new data model on SingleStore versus the previous database saw an improvement of more than 28x improvement in speed. This speed-up increased reporting usage, made everyone more productive, and helped Culture Amp incorporate more data into the decision-making process. Nikkei Nikkei is a large media corporation based in Japan that shares news about Asia in English. Nikkei needed a better way to get real-time analytics on the readers coming to its website and its widely used mobile app. Having better reader data allows Nikkei to understand what articles are resonating with readers, and what type of ads to show readers. With its previous database, Nikkei was only able to get reader analytics 45 minutes after someone was on the site. Now, with SingleStore, Nikkei is able to get analytics on readers in about 200 milliseconds. That’s an improvement of 13,500 times – four orders of magnitude – in the time until data is available. This allows Nikkei to actually respond to their site visitors’ activities in real time. Because SingleStore is compatible with MySQL, Nikkei is easily able to integrate the data collected into SingleStore with its other databases. The company is getting all of this performance improvement and flexibility for free. How to Use SingleStore for Free If you missed the original announcement, here is a quick recap on what you get when using SingleStore for free: You can create four leaf nodes, with up to eight virtual CPUs per node, and up to 32GB of RAM. You also create aggregator nodes freely; we recommend at least two, for redundancy. You can create as many rowstore tables, stored entirely in-memory, as you like. If your database is entirely made up of rowstore tables, and you have 128GB of RAM in your leaf nodes, that’s the total database size limit. You can also create as many columnstore tables, stored on disk, as you like. A mostly-columnstore database might comfortably reach 1TB in size. Free use of the product includes community support . For dedicated professional support, or more than four leaf nodes, you’ll need a paid license . Response to the free tier has been positive . Most experimental, proof of concept, and in-house-only projects run easily within a four-(leaf)-node configuration, and don’t require professional support. For projects that move to production, both dedicated professional support and the ability to add more nodes – in particular, for paired development and production environments, and for robust high availability implementations – make sense, and are enabled by an easy switch to our enterprise offering. The case study snapshots for Culture Amp and Nikkei are good examples of what our customers have accomplished while using SingleStore for free. It’s always fun sharing the benefits our customers achieve with SingleStore versus other databases, but we get even more excited showing these performance increases when they’re achieved using the free option. We consider all users of SingleStore to be our customers, whether you’re starting with us for the first time for free or you’ve been with SingleStore from the early days. These are just some examples of the cool work happening with free use of SingleStore. To get performance improvements similar to Culture Amp and Nikkei, all you have to do is download SingleStore .", "date": "2019-04-17"},
{"website": "Single-Store", "title": "how-ceos-can-stay-relevant-in-the-age-of-ai", "author": ["Peter Guagenti"], "link": "https://www.singlestore.com/blog/how-ceos-can-stay-relevant-in-the-age-of-ai/", "abstract": "The most important new skills for business leaders are not what you might think. You’ve read the headlines. Data is the new oil ; it’s the new currency ; data capital can create competitive advantage . We also hear, over and over again, that machine learning (ML) and artificial intelligence (AI) are the future . Few will dispute that these things are true, but the trite language masks a deeper challenge. Data must be collected, analyzed, and acted upon in the right way and at the right time for a business to create value from it . ML and AI are only as powerful as the data that drive them. In this world, big companies – which may throw away as much data in a day as a startup will generate in a year – should have a significant competitive advantage. However, new approaches are needed to move forward effectively in the age of ML and AI. And the new approaches all start with the data itself. To build and maintain a successful business in today’s insight-driven economy, business leaders need to develop a new set of skills. We outline what we believe those skills are below. Skill #1: A Drive to Find Key Data and Make it Useful Business leaders need to be on a mission to collect and (more importantly) expose to their organizations all of the data that might create a competitive advantage. We don’t always know exactly what data or insights might be the ones that will allow us to break away from the pack until after we have analyzed and acted on that data, then measured the results and repeated the cycle, over and over. Business leaders need to encourage collecting as much data as possible in the day-to-day operations of the business, with a particular eye towards where your organization has advantages or challenges. Make sure that the data is not simply collected, but stored in such a way that your teams can easily access, understand, and analyze it. “Big data” was a great start to enabling the future of our businesses, but what we need today instead is “fast data” – data made available to everyone, to drive fast insight. Skill #2: The Ability to Create a Culture of Constant Analysis and Action As the French writer Antoine de Saint-Exupéry stated, “If you want to build a ship, don’t drum up people together to collect wood and don’t assign them tasks and work, but rather teach them to long for the vast and endless sea.” This adage applies to becoming an insight-driven business. Data is not insight, and insights are not outcomes. What we seek in collecting and analyzing data is to identify and carry out the actions that will accelerate and transform our business. The best way to leverage data for creating competitive advantage is to encourage a culture of inquisitiveness, of always asking “ the 5 Whys ” – a series of “why” questions that take us to the root of what’s important, and why. Compel your teams to constantly look for ways to not just gather and share insights, but to look for ways to turn insights into immediate actions that add value to the business. Innovations such as ecommerce product recommendations, dynamic pricing based on demand, or sensor-based maintenance are all insight-driven innovations that have arisen in the last decade or so and that have generated dramatic competitive advantage. ML and deep learning – the most practical form of AI currently available to business – accelerate this process. You can use them together to test multivariate alternatives, to vary assumptions and audiences around your current performance, to help you maximize the value of the insights that you find and implement today, and then to help you take your insights to another higher level. Skill #3: The Insight to Choose the Right Tools and Technologies The agile movement does not get nearly enough credit for the transformative effect it’s had, and continues to have, on business. But a business can only be agile with the right tools and technologies, and the ability to use them to drive action and change. It’s no surprise that, up to this point, most of the companies and leaders that are making the best use of data to drive their businesses are digital natives – think Google, Facebook, Uber, Airbnb, et al. They have done this by applying the agile mindset of software development to data architecture, data engineering, and data-driven decisioning. While the large digital players may have leapt to the forefront in the last 10 years, the traditional enterprise can use its long operational history, its existing volumes of data, and its ability to generate fresh, useful data, to level the playing field and compete effectively in the modern economy. In order to maximize and utilize these resources, business leaders need to lead the decision making around data infrastructure. The insight-driven enterprise needs the best possible tools and technology to enable fast, flexible, and efficient use of the company’s data. This means shifting the traditional IT mindset from maintaining legacy data infrastructure, overly strict controls, and inflexibility, to one that puts agility first. Analysts, data scientists, and application developers need access to real-time or near-real-time data sources. And they, and the businesspeople who work with them most closely, need to be empowered to act on that data – be it for rapid decision making or to create insight-driven, dynamic experiences for customers and employees. This shift requires a new set of tools, processes, and culture that is so critical to the future of the business that business leaders – all the way up to the CEO – needs to ensure that agility is the primary order of the day. Peter Guagenti is CMO at SingleStore, and is an advisor and a board member for several AI-focused companies. Peter spent more than a decade helping Fortune 500 companies to embrace digital transformation and to use real-time and predictive decisions to improve their businesses.", "date": "2019-04-02"},
{"website": "Single-Store", "title": "webinar-kafka-memsql-deliver", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/webinar-kafka-memsql-deliver/", "abstract": "Using Apache Kafka and SingleStore together makes it much easier to create and deliver intelligent, real-time applications. In a live webinar, which you can view here , SingleStore’s Alec Powell discussed the value that Kafka and SingleStore each bring to the table, shows reference architectures for solving common data management problems, and demonstrates how to implement real-time data pipelines with Kafka and SingleStore. Kafka is an open source messaging queue that works on a publish-subscribe model. It’s distributed (like SingleStore) and durable. Kafka can serve as a source of truth for data across your organization. What Kafka Does for Enterprise IT Today, enterprise IT is held back by a few easy to identify, but seemingly hard to remedy, factors: Slow data loading Lengthy query execution Limited user access These factors interact in a negative way. Limited data messaging and computing capabilities limit user access. The users who do get on suffer from slower data loading and lengthy query execution. Increasing organizational needs for data access – for reporting, business intelligence (BI) queries, apps, machine learning, and artificial intelligence – are either blocked, preventing progress, or allowed, further overloading the system and slowing performance further. Organizations try a number of fixes for these problems – offered by both existing and new vendors, usually with high price tags for solutions that add complexity and provide limited relief. Solutions include additional CPUs and memory, specialized hardware racks, pricey database add-ons, and caching tiers with limited data durability, weak SQL coverage, and high management costs and complexity. NoSQL solutions offer fast ingest and scalability. However, they run queries slowly, demand limited developer time for even basic query optimization, and break compatibility with BI tools. How SingleStore and Kafka Work Together SingleStore offers a new data architecture that solves these problems. Unlike NoSQL solutions , SingleStore offers both scalability – which affords extreme performance – and an easy-to-use SQL architecture. SingleStore is fully cloud-native ; it is neither tied to just one or two cloud platforms, nor cloud-unfriendly, as with most alternatives. In the webinar , Alec shows how SingleStore works. Running as a Linux daemon, SingleStore offers a fully distributed system, and is cloud-native – running in the cloud and on-premises, in containers or virtual machines, and integrating with a wide range of existing systems. Within a SingleStore cluster, an aggregator node communicates with the database client, manages schema, and shares work across leaf nodes. (A master aggregator serves as a front-end to multiple aggregator nodes, if the scale of the database requires it.) SingleStore Pipelines integrate tightly with Kafka, supporting the exactly-once semantics for which Kafka has long been well-known. (See the announcement blog post in The New Stack: Apache Kafka 1.0 Released Exactly Once .) SingleStore polls for changes, pulls in new data, and executes transactions atomically (and exactly once). Pipelines are mapped directly to SingleStore leaf nodes for maximum performance. Together, Kafka and SingleStore allow live loading of data, which is a widely needed, but rarely found capability. Used with Kafka, or in other infrastructure, SingleStore handles mixed workloads and meets tight SLAs for responsiveness – including with streaming data and strong demands for concurrency. Kafka-SingleStore Q&A There was a lively Q&A session. The questions and answers here include some that were handled in the webinar and some that could not be answered in the live webinar because of time constraints. Q. Can Kafka and SingleStore run in the cloud? A. Both Kafka and SingleStore are cloud-native software. Roughly half of SingleStore’s deployments today are in the cloud; for instance, SingleStore often ingests data from AWS S3, and has been used to replace Redshift. The cloud’s share of SingleStore deployments is expected to increase rapidly in the future. Q. Can SingleStore replace Oracle? A. Yes, very much so – and other legacy systems too. Because of the complexities of many data architectures, however, SingleStore is often used first to augment Oracle. For instance, customers will use a change data capture (CDC) to copy data processed by Oracle to SingleStore. Then, analytics run against SingleStore, offloading Oracle (so transactions run faster) and leveraging SingleStore’s faster performance, superior price-performance, scalability, and much greater concurrency support for analytics. Q. How large can deployments be? A. We have customers running from the hundreds of megabytes up into the petabytes. Q. With SingleStore Pipelines, can we parse JSON records? A. Yes, SingleStore has robust JSON support. Summing Up Kafka+SingleStore and the Webinar In summary, SingleStore offers live loading of batch and streaming data, fast queries, and fully scalable user access. Together, Kafka and SingleStore remove access to data streaming and data access right across your organization. You can view the webinar now . You can also try SingleStore for free today or contact us to learn how we can help support your implementation plans.", "date": "2019-04-18"},
{"website": "Single-Store", "title": "the-need-for-operational-analytics", "author": ["Rick Negrin"], "link": "https://www.singlestore.com/blog/the-need-for-operational-analytics/", "abstract": "The proliferation of streaming analytics and instant decisions to power dynamic applications, as well as the rise of predictive analytics, machine learning, and operationalized artificial intelligence, have introduced a requirement for a new type of database workload: operational analytics. The two worlds of transactions and analytics, set apart from each other, are a relic of a time before data became an organization’s most valuable asset. Operational analytics is a new set of database requirements and system demands that are integral to achieving competitive advantage for the modern enterprise. This new approach was called for by Gartner as a Top 10 technology for 2019 , under the name “continuous analytics.” Delivering operational analytics at scale is the key to real-time dashboards, predictive analytics, machine learning, and enhanced customer experiences which differentiate digital transformation leaders from the followers. However, companies are struggling to build these new solutions because existing legacy database architectures cannot meet the demands placed on them. The existing data infrastructure cannot scale to the load put on it, and it doesn’t natively handle all the new sources of data. The separation of technologies between the transactional and analytic technologies results in hard tradeoffs that leave solutions lacking in operational capability, analytics performance, or both. There have been many attempts in the NoSQL space to bridge the gap, but all have fallen short of meeting the needs of this new workload. Operational analytics enables businesses to leverage data to enhance productivity, expand customer and partner engagement, and support orders of magnitude more simultaneous users. But these requirements demand a new breed of database software that goes beyond the legacy architecture. The industry calls these systems by several names: hybrid transaction and analytics processing (HTAP) from Gartner ; hybrid operational/analytics processing (HOAP) from 451 Research ; and translytical from Forrester . Consulting firms typically use the term we have chosen here, operational analytics, and CapGemini has even established a full operational analytics consultancy practice around it. The Emergence of Operational Analytics Operational Analytics has emerged alongside the existing workloads of Online Transaction Processing (OLTP) and Online Analytical Processing (OLAP). I outlined the requirements of those workloads in this previous blog entry . To summarize, OLTP requires data lookups, transactionality, availability, reliability, and scalability. Whereas OLAP requires support for running complex queries very fast, large data sets, and batch ingest of large amounts of data. The OLTP and OLAP based systems served us well for a long time. But over the last few years things have changed. Decisions should not have to wait for the data It is no longer acceptable to wait for the next quarter, or week, or even day to get the data needed to make a business decision. Companies are increasingly online all the time; “down for maintenance” and “business hours” are quickly becoming a thing of the past. Companies that have a streaming real-time data flow have a significant edge over their competitors. Existing legacy analytics systems were simply not designed to work like this. Companies must become insight driven This means that, instead of a handful of analysts querying the data, you have hundreds or thousands of employees hammering your analytics systems every day in order to make informed decisions about the business. In addition, there will be automated systems – ML/AI and others – also running queries to get the current state of the world to feed their algorithms. The existing legacy analytics systems were simply not designed for this kind of usage. Companies must act on insights to improve customer experience Companies want to expose their data to their customers and partners. This improves the customer experience and potentially adds net new capabilities. For example, a cable company tracks users as they try to set up their cable modems so they can proactively reach out if they see there is a problem. This requires a system that can analyze and react in real time. Another example is an electronics company that sells smart TVs and wants to expose which shows customers are watching to its advertisers. This dramatically increases the number of users trying to access its analytics systems. In addition, the expectations of availability and reliability are much higher for customers and partners. So you need a system that can deliver an operational service level agreement (SLA). Since your partners don’t work in your company, it means you are exposing the content outside the corporate firewall, so strong security is a must. The existing legacy analytics systems were simply not designed for this kind of usage. Data is coming from many new sources and in many types and formats The amount of data being collected is growing tremendously. Not only is it being collected from operational systems within the company; data is also coming from edge devices. The explosion of IoT devices, such as oil drills, smart meters, household appliances, factory machinery, etc., are the key contributors to the growth. All this data needs to be fed into the analytics system. This leads to an increased complexity in the types of data sources (such as Kafka, Spark, etc…) as well as data types and formats (geospatial, JSON, AVRO, Parquet, raw text, etc.) and throughput requirements for ingest of the data. Again, the existing legacy analytics systems were simply not designed for this kind of usage. The Rise of Operational Analytics These changes have given rise to a new database workload, operational analytics. The short description of operational analytics is an analytical workload that needs an operational SLA. Now let’s unpack what that looks like. Operational Analytics as a Database Workload Operational analytics primarily describes analytical workloads. So the query shapes and complexity are similar to OLAP queries. In addition, the data sets for operational analytics are just as large as OLAP data sets, although often it is the most recent data that is more important. (This is usually a fraction of the total data set.) Data loading is similar to OLAP workloads, in that data comes from an external source and is loaded independent of the applications or dashboards that are running the queries. But this is where the similarities end. Operational analytics has several characteristics that set it apart from pure OLAP workloads. Specifically, the speed of data ingestion, scaling for concurrency, availability and reliability, and speed of query response. Operational analytics workloads require an SLA on how fast the data needs to be available. Sometimes this is measured in seconds or minutes, which means the data infrastructure must allow streaming the data in constantly, while still allowing queries to be run. Sometimes this means there’s a window of time (usually a single-digit number of hours) during all the data must be ingested. As data sets grow, the existing data warehouse (DW) technologies have had trouble loading the data within the time window (and certainly don’t allow streaming). Data engineers often have to do complex tricks to continue meeting data loading SLAs with existing DW technologies. Data also has to be loaded from a larger set of data sources than in the past. It used to be that data was batch-loaded from an operational system during non-business hours. Now data comes in from many different systems. In addition, data can flow from various IoT devices far afield from the company data center. The data gets routed through various types of technologies (in-memory queues like Kafka, processing engines like Spark, etc.). Operational analytics workloads need to easily handle ingesting from these disparate data sources. Operational analytics workloads also need to scale to large numbers of concurrent queries. With the drive towards being data driven and exposing data to customers and partners, the number of concurrent users (also queries) in the system has increased dramatically. In an OLAP workload, five to ten queries at a time was the norm. Operational analytics workloads often must be able to handle high tens, hundreds, or even thousands of concurrent queries. As in an OLTP workload, availability and reliability are also key requirements. Because these systems are now exposed to customers or partners, the SLA required is a lot stricter than for internal employees. Customers expect a 99.9% or better uptime and they expect the system to behave reliably. They are also less tolerant of planned maintenance windows. So the data infrastructure backing these systems needs to have support for high availability, with the ability to handle hardware and other types of failure. Maintenance operations (such as upgrading the system software or rebalancing data) need to become transparent, online operations that are not noticeable to the users of the system. In addition, the system should self-heal when a problem occurs, rather than waiting for an operator to get alerted to an issue and respond. Strong durability is important as well. This is because even though data that is lost could be reloaded, the reloading may cause the system to break the availability SLA. The ability to retrieve the data you are looking for very quickly is the hallmark feature of database systems. Getting access to the right data quickly is a huge competitive advantage. Whether it is internal users trying to get insights into the business, or you are presenting analytics results to a customer, the expectation is that the data they need is available instantly. The speed of the query needs to be maintained regardless of the load on the system. It doesn’t matter if there is a peak number of users online, the data size has expanded, or there are failures in the system. Customers expect you to meet their expectations on every query with no excuses. This requires a solid distributed query processor that can pick the right plan to answer any query and get it right every time. It means the algorithms used must scale smoothly with the system as it grows in every dimension. Supporting Operational Analytics Use Cases with SingleStore SingleStore was built to address these requirements in a single converged system. SingleStore is a distributed relational database that supports ANSI SQL. It has a shared-nothing, scale-out architecture that runs well on industry standard hardware. This allows SingleStore to scale in a linear fashion simply by adding machines to a cluster. SingleStore supports all the analytical SQL language features you would find in a standard OLAP system, such as joins, group by, aggregates, etc. It has its own extensibility mechanism so you can add stored procedures and functions to meet your application requirements. SingleStore also supports the key features of an OLTP system: transactions, high availability, self-healing, online operations, and robust security. It has two storage subsystems: an on-disk column store that gives you the advantage of compression and extremely fast aggregate queries, as well as an in-memory row store that supports fast point queries, aggregates, indices, and more. The two table types can be mixed in one database to get the optimal design for your workload. Finally, SingleStore has a native data ingestion feature, called Pipelines, that allows you to easily and very quickly ingest data from a variety of data sources (such as Kafka, AWS S3, Azure Blob, and HDFS). All these capabilities offered in a single integrated system add up to making it the best data infrastructure for an operational analytics workload, bar none. Describing the workload in general terms is a bit abstract, so let’s dig into some of the specific use cases where operational analytics is the most useful. Portfolio Analytics One of the most common use cases we see in financial services is portfolio analytics . Multiple SingleStore customers have written financial portfolio management and analysis systems that are designed to provide premium services to elite users. These elite users can be private banking customers with high net worth or fund managers who control a large number of assets. They will have large portfolios with hundreds or thousands of positions. They want to be able to analyze their portfolio in real-time, with graphical displays that are refreshed instantaneously as they filter, sort, or change views in the application. The superb performance of SingleStore allows sub-second refresh of the entire screen with real-time data, including multiple tables and charts, even for large portfolios. These systems also need to scale to hundreds or thousands of users concurrently hitting the system, especially when the market is volatile. Lastly, they need to bring in the freshest market data, without compromising the ability to deliver the strict latency SLAs for their query response times. They need to do all of this securely without violating relevant compliance requirements nor the trust of their users. High availability and reliability are key requirements, because the market won’t wait. SingleStore is the ideal data infrastructure for this operational analytics use case as it solves the key requirements of fast data ingest, high scale concurrent user access, and fast query response. Predictive Maintenance Another common use case we see is predictive maintenance. Customers who have services or devices that are running continuously want to know as quickly as possible if there is a problem. This is a common scenario for media companies that do streaming video. They want to know if there is a problem with the quality of the streaming so they can fix it, ideally before the user notices the degradation. This use case also comes up in the energy industry. Energy companies have devices (such as oil drills, wind turbines, etc.) in remote locations. Tracking the health of those devices and making adjustments can extend their lifetime and save millions of dollars in labor and equipment to replace them. The key requirements are the ability to stream the data about the device or service, analyze the data – often using a form of ML that leverages complex queries – and then send an alert if the results show any issues that need to be addressed. The data infrastructure needs to be online 24/7 to ensure there is no delay in identifying these issues. Personalization A third use case is personalization. Personalization is about customizing the experience for a customer. This use case pops in a number of different verticals, such as a user visiting a retail web site, playing a game in an online arcade, or even visiting a brick and mortar store. The ability to see a user’s activity and, more importantly, learn what is attractive to them, gives you the information to meet their needs more effectively and efficiently. One of SingleStore’s customers is a gaming company. They stream information about the user’s activity in the games, process the results against a model in SingleStore, and use the results to offer the user discounts for new games and other in-app purchases. Another example is a popular music delivery service that uses SingleStore to analyze usage of the service to optimize ad spend. The size of data and the number of employees using the system made it challenging to deliver the data in a timely way to the organization and allow them to query the data interactively. SingleStore significantly improved their ability to ingest and process the data and allowed their users to get a dramatic speedup in their query response times. Summary Operational analytics is a new workload that encompasses the operational requirements of an OLTP workload – data lookups, transactionality, availability, reliability, and scalability – as well as the analytical requirements of an OLAP workload – large data sets and fast queries. Coupled with the new requirements of high user concurrency and fast ingestion, the operational analytics workload is tough to support with a legacy database architecture or by cobbling together a series of disparate tools. As businesses continue along their digital transformation journey they are finding more and more of their workloads fit this pattern and are searching for new modern data infrastructure, like SingleStore, that has the performance and scale capabilities to handle them.", "date": "2019-04-26"},
{"website": "Single-Store", "title": "streaming-systems-oreilly", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/streaming-systems-oreilly/", "abstract": "More and more, SingleStore is used to help add streaming characteristics to existing systems, and to build new systems that feature streaming data from end to end. Our new ebook excerpt from O’Reilly introduces the basics of streaming systems. You can then read on – in the full ebook and here on the SingleStore blog – to learn about how you can make streaming part of all your projects, existing and new. Streaming has been largely defined by three technologies – one that’s old, one that’s newer, and one that’s out-and-out new. Streaming Systems covers the waterfront thoroughly. Originally, Tyler Akidau, one of the book’s authors, wrote two very popular blog posts: Streaming 101: The World Beyond Batch , and Streaming 102 , both on the O’Reilly site. The popularity of the blog posts led to the popular O’Reilly book, Streaming Systems: The What, Where, When, and How of Large-Scale Data Processing . In the excerpt that we offer here , you will see a solid definition of streaming and how it works with different kinds of data. The authors address the role of streaming in the entire data processing lifecycle with admirable thoroughness. They also describe the major concerns you’ll face when working with streaming data. One of these is the difference between the order in which data is received and the order in which processing on it is completed. Reducing such disparities as much as possible is a major topic in streaming systems. In this figure from the Streaming Systems excerpt, the authors plot eventarrival time vs. processing completion time. In both the excerpt, and the full ebook , the authors also tackle three key streaming technologies that continue to play key roles in the evolution of SingleStore: Apache Kafka, Apache Spark, and – perhaps surprisingly, in this context – SQL. Apache Kafka and Streaming Apache Kafka saw its 1.0 version introduced by Confluent in late 2017. (See Apache Kafka 1.0 Introduced Exactly Once at The New Stack.) SingleStore works extremely well with Kafka . Both Kafka and SingleStore are unusual in supporting exactly-once updates , a key feature that not only adds valuable capabilities, but affects how you think about data movement within your organization. It’s very easy to connect Kafka streams to SingleStore Pipelines for rapid ingest. And SingleStore’s Pipelines to stored procedures feature lets you handle complex transformations without interfering with the streaming process. In this figure from the full Streaming Systems ebook, Kafkaand Spark both appear as relatively recent arrivals. Apache Spark and Streaming Apache Spark is an older streaming solution, initially released in 2014 . (One of the key components included in the 1.0 release was Spark SQL, for ingesting structured data into Spark.) Spark was first developed to address concerns with Google’s MapReduce data processing approach. While widely used, Spark is perhaps as well known today for its machine learning and AI capabilities as for its core streaming functionality. SingleStore first introduced the SingleStore Spark Connector in 2015, then included full Spark support in SingleStore Pipelines and Pipelines to stored procedures. Today, Spark and SingleStore work very well together. SingleStore customer Teespring used Kafka and Spark together for machine learning implementations. The SingleStore case study for Teespring shows Kafkaand Spark used together for machine learning. SQL and Streaming Ironically, one of the foundational data technologies, SQL, plays a big role in Streaming Systems, and in the future of streaming. SQL is all over the full book’s Table of Contents : Streaming SQL is Chapter 8 of the full ebook. In this chapter, the authors discuss how to use SQL robustly in a streaming environment. Streaming Joins is Chapter 9. Joins are foundational to analytics, and optimizing them has been the topic of decades of work in the SQL community. Yet joins are often neglected in the NoSQL movement that is most closely associated with streaming. Streaming Systems shows how to use joins in a streaming environment. SingleStore is, of course, a leading database in the NewSQL movement . NewSQL databases combine the best of traditional relational databases – transactions, structured data, and SQL support – with the best of NoSQL: scalability, speed, and flexibility. Saeed Barghi of SingleStore partner Zoomdata shows Kafka and SingleStore used together for business intelligence. SingleStore partner ZoomData uses Kafka and aSingleStore Pipeline for real-time data visualization. Next Steps to Streaming We recommend that you download and read our book excerpt from Streaming Systems today. If you find it especially valuable, consider getting the full ebook from O’Reilly. If you wish to move to implementation, you can start with SingleStore today for free . Or, contact us to speak with a technical professional who can describe how SingleStore can help you achieve your goals.", "date": "2019-04-26"},
{"website": "Single-Store", "title": "how-to-use-memsql-with-intels-optane-persistent-memory", "author": ["Eric Hanson"], "link": "https://www.singlestore.com/blog/how-to-use-memsql-with-intels-optane-persistent-memory/", "abstract": "Intel’s new Optane DC persistent memory adds a new performance option for SingleStore users. After careful analysis, we’ve identified one area in which SingleStore customers and others can solve a potentially urgent problem using Optane today, and we describe that opportunity in this blog post. We also point out other areas where SingleStore customers and others should keep an eye on Optane-related developments for the future. If you need broader information than what’s offered here, there are many sources for more comprehensive information about Optane and what it can do for you, beginning with Intel itself . Intel Optane extends a system’s “hot tier\" with slower, but persistent and capaciousmemory. SSD serves as a warm tier, and HDD and tape as the cold tier. What Optane Offers Intel’s Optane is a new kind of memory option that’s less expensive than RAM, and also nonvolatile. Optane is only available on systems running some of the latest hardware and software from Intel. Optane offers a new way to improve the performance of systems, especially servers. However, it will take the industry – from hardware vendors, to operating system developers, to application developers – a good while to take full advantage of it. Optane runs in two modes, each with different advantages and trade-offs: memory mode, which we will concentrate on here, and application mode. In memory mode, the computer’s RAM is used as a fast, volatile cache above larger, lower-cost Optane memory modules, managed entirely by Intel’s built-in memory management. In this mode, taking advantage of Optane does not require any changes to application code. In application mode, Optane can be explicitly written to and read from, separate from traditional RAM or hard disk storage. We will only discuss application mode briefly here. SingleStore databases with large rowstore tables can benefit from systemsthat offer Intel Optane persistent memory in memory mode. Optane has two features that make it interesting: price and capacity. First, Optane is much cheaper, on a like-for-like basis, than traditional RAM. For traditional DRAM, a 128GB ECC DDR4 module is the largest available today, and costs roughly $4,500. For Optane, a 128GB DIMM is the smallest available, and costs about $850 at this writing – more than 80% less than traditional RAM. Of just as much interest, however, is capacity. Optane offers DIMMs up to 512GB in capacity – four times the capacity of traditional DRAM. A 512GB Optane DIMM module costs about $7,000, or nearly ten times the price of the 128GB DIMM. However, the Optane 512GB DIMM is still less than half the price of four 128GB ECC DDR4 modules of traditional RAM, at about $18,000. A Brief Mention of Application Mode Use cases for Optane in application mode are interesting, but we won’t go into much depth on that here. When running Optane in application mode, since you can address Optane memory directly, you can treat it almost as a replacement for disk. That’s because writes to the Optane memory become permanent, or “persistent.” The main benefit of application mode is persistence. This allows databases that keep information in memory to restart faster, and experiment with new architectures for making update transactions permanent. SingleStore’s scale-out architecture already allows us to restart quite fast, since each processor core reads data locally to re-populate RAM, in a fully parallel fashion. Restart time has not been a significant problem for our customers to date. So while we are intrigued by the possibilities of using application mode for SingleStore, we have not so far made changes to our code to use it. Under the Covers of Memory Mode What’s more widely interesting for now is the use of Optane in memory mode. When you add Optane persistent memory to your system, the total Optane capacity acts as your addressable RAM capacity. Traditional RAM then serves as a cache over the Optane persistent memory. In memory mode, you need to consider three factors in gauging performance: Traditional RAM performance . Adding Optane does not change performance when the data you need resides in traditional RAM. The Intel memory controller and drivers work to maximize the number of times needed data is fetched from traditional RAM rather than Optane memory. Optane memory performance . When you add Optane, and application code and data in your address space exceed the capacity of traditional RAM, some RAM accesses will miss in traditional RAM. These accesses will then be served by Optane memory instead. The total time to access will be the traditional RAM access time plus the Optane memory access time, which can be a multiple of the traditional RAM access time. Cache hit percentage . When you access data in RAM, some of it will be served by a cache hit – the data is in traditional RAM – and some by a cache miss – the access to traditional RAM fails, and an additional access to Optane memory is required. The percentage of cache hits determines whether you get something close to traditional RAM performance or something closer to Optane memory performance. For most applications, with Optane memory at perhaps four times the size of traditional RAM, the overall performance reduction may average around 25%, compared to the same total RAM size made up of traditional RAM only. This applies across the board, including for queries as well as size-of-data operations like backup, restore, and snapshots. But the traditional RAM + Optane memory combination will be much less expensive – and, as we describe below, you can get far more Optane memory into a single server than traditional RAM. Because no one has to rewrite their software to take advantage of memory mode, and because operationally savvy users can take full advantage of Optane against suitable workloads, it’s expected that Optane will be very popular with cloud providers and in large data centers. These deployments can offer large cost savings for the server owners, with an impact on end users that may not even be noticeable. The “Hidden” Feature of Optane – and the Impact on SingleStore Customers What’s most exciting about Optane, when used in memory mode, is not necessarily the direct cost savings on like-for-like servers. The exciting part is that the total RAM capacity of a single machine can be multiplied by a large amount. For instance, where an organization currently supports servers with 256GB of RAM, it can cost-effectively add support for servers with 256GB of traditional RAM plus, perhaps, 1TB of Optane memory. Where logical RAM capacity is the gating factor for the number of servers used, one such Optane-enabled server may be able to replace up to four traditional servers. This applies directly to SingleStore customers who have very large rowstore tables. If the total rowstore table size is 2TB, it would take eight or nine servers with 256GB of RAM to support the leaf nodes for the tables, plus additional servers as aggregators (this assumes no redundancy — using high availability with “redundancy 2” would double the number of needed servers). It would only require two leaf node servers, though, if each is provisioned with a mix of 256GB of traditional RAM and 1TB of Optane memory. The total hardware cost would be far less than for eight servers sporting traditional RAM, and management and maintenance of the server pool would be far easier. SingleStore is so efficient that it’s common for a server that’s maxed out on RAM to be only using 20-30% of its CPU capacity. However, past RAM limitations prevented some customers from getting more useful performance from each server. With Optane, effective RAM capacity can be upgraded much further, and quite cost-effectively. Some SingleStore customers have been held back from deploying applications with truly large rowstore tables, totaling 10TB and more, due to the cost and operational complexity of managing the number of servers that would be required. With Optane, these projects become quite feasible. These same dynamics apply to other workloads that use rowstore tables in SingleStore, and these are the workloads that will be moved to Optane-powered servers within cloud providers and in large data centers. It’s no accident that launch partners for Optane persistent memory include Google Cloud, as well as server makers like Cisco, Dell EMC, and Lenovo, plus consultancy Accenture. It’s also possible for Optane to be useful with columnstore tables, or in applications that use a mix of rowstore and columnstore. However, columnstore needs for large databases are already served effectively by SSDs, and it will take careful testing to establish where Optane will or won’t offer a clear price/performance benefit. What You Should Do Today If you are a current SingleStore customer, running very large tables (1TB-plus total data size) in rowstore today – or if you have a near-term need to do so – reach out to your SingleStore contacts soon. You may be able to save money on your current deployment and cost-effectively grow your application’s memory footprint and effectiveness. If you are not running workloads of this type, Optane may not have immediate benefit for you in the short term, at least for the database part of your applications. As applications – everything from operating systems, to business applications, to end user applications – are rewritten to take advantage of Optane, you will have the opportunity to leverage those improvements by including Optane in your server mix. If you are not a SingleStore customer, but are running (or contemplating) applications that will require high rates of ingest, high transaction performance, fast query response, and a high degree of concurrent usage, Optane helps make SingleStore an even more attractive option. We encourage you to try SingleStore for free today, or contact us to learn how SingleStore can help with even very ambitious data management goals.", "date": "2019-05-03"},
{"website": "Single-Store", "title": "introducing-the-memsql-kubernetes-operator", "author": ["Carl Sverre"], "link": "https://www.singlestore.com/blog/introducing-the-memsql-kubernetes-operator/", "abstract": "Kubernetes has taken the world by storm, transforming how applications are developed, deployed, and maintained. For a time, managing stateful services with Kubernetes was difficult, but that has changed dramatically with recent innovations in the community. Building on that work, SingleStore is pleased to announce the availability of our SingleStore Kubernetes Operator, and our certification by Red Hat to run on the popular OpenShift container management platform. Kubernetes has quickly become one of the top three most-loved platforms by developers. Now, with the SingleStore Kubernetes Operator, technology professionals have an easy way to deploy and manage an enterprise-grade operational database with just a few commands. Note: The SingleStore Kubernetes operator is currently experimental, and in beta. It will reach general availability in the coming months. The new beta Operator is certified by Red Hat to run SingleStore software on Red Hat OpenShift , or you can run it with any Kubernetes distribution you choose. Running SingleStore on Kubernetes gives data professionals the highest level of deployment flexibility across hybrid, multi-cloud, or on-premises environments. As Julio Tapia, director of the Cloud Platforms Partners Ecosystem for Red Hat, put it in our press release , services in a Kubernetes-native infrastructure “‘just work’ across any cloud where Kubernetes runs.” As a cloud-native database , SingleStore is a natural fit for Kubernetes. SingleStore is a fully distributed database , deploys and scales instantly, and is configured quickly and easily using the native SingleStore API. SingleStore customers have requested the Kubernetes Operator, and several participated in testing prior to this release. The majority of SingleStore customers today deploy SingleStore on one or more public cloud providers. Now, with the Kubernetes Operator, they can deploy on any public or private infrastructure more easily. How to Use The SingleStore Kubernetes Operator You use the SingleStore Kubernetes Operator like other standard Kubernetes tools. Use the Kubernetes command-line interface (CLI) and the Kubernetes API to interact with the Operator and manage the application. The task of managing the cluster is greatly simplified. DevOps and administration teams can also use the Operator to implement partial or full automation. The Operator enables you to create, read, update, and delete SingleStore clusters. Among the options you specify (see here for details): The cluster size . Cluster size is defined in units, where one unit of cluster size is equal to one leaf node. The memory and CPU assigned . This is defined as height, where one unit of height equals 8 vCPUs and 32GB of RAM. The redundancy level . Level 1 is no redundancy, level 2 is one redundant copy (recommended for production use). The storage size . How much disk space you want to reserve. Because Kubernetes is a declarative, rather than imperative , environment, you describe the state of the cluster that you want Kubernetes to create and maintain. Kubernetes then maintains that state for you. The commands and operations are the same across all the major public clouds, private clouds, and on-premises installations as well. The minimum cluster size you should specify is a single leaf unit with height 1, three aggregator units (automatically created, with height 0.5), and redundancy level 2. When you create the cluster, a DDL endpoint is returned to you. You connect to the cluster using the DDL endpoint. The SingleStore Kubernetes Operator does not currently include the ability to split and merge partitions. You will need to perform this function manually, outside of Kubernetes. We expect to include partition management in a future release. Next Steps If you’re already a SingleStore customer, you can begin using the Kubernetes Operator today. Access it here . If you are not already a customer, you’ll find SingleStore a great fit for a wide range of operational analytics use cases. Try SingleStore for free today or contact us to learn how we can help you.", "date": "2019-05-08"},
{"website": "Single-Store", "title": "case-study-customer-saves-60k-per-month-on-memsql", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/case-study-customer-saves-60k-per-month-on-memsql/", "abstract": "A SingleStore customer previously ran their business on two databases: the Amazon Web Services Relational Database Service ( AWS RDS ) for transactions and Druid.io for analytics, with a total bill which reached over $93,000 a month. They moved both functions to SingleStore, and their savings have been dramatic – about two-thirds of the total cost. The customer’s new monthly cost is about $31,000, for a savings of $62,000 a month, or 66%. In addition, the customer gains increased performance, greater concurrency, and easier database management. Future projects can also claim these benefits, giving the customer lower costs for adding new features to their services and greater strategic flexibility. A new SingleStore customer reduced their monthly AWS costs by 93%, and theirtotal monthly costs by two-thirds, with a simple move to SingleStore The chart above shows the reduction in AWS RDS costs only – from a total of roughly $68,000 per month with AWS RDS and Druid.io, to roughly $5,700 a month with SingleStore replacing both. Licensing costs for Druid.io were roughly $25,000/month, the same as for SingleStore. So total costs dropped from roughly $93,000, with AWS RDS and Druid.io, to roughly $31,000, with SingleStore running in AWS. In addition to the dramatic cost savings, strategic flexibility is enhanced by the fact that SingleStore can run in a variety of public and private clouds, on premises, in mixed on-premises and cloud environments, and in virtual machines as well as containers. “Cloud lock-in” becomes a problem of the past. Moving from AWS RDS to SingleStore AWS RDS is a flexible service that allows the use of multiple databases. In this case, the customer was using AWS Aurora. Aurora is a MySQL and PostgreSQL-compatible relational database offered by AWS as one of a wide range of database offerings. Because SingleStore is MySQL wire protocol-compatible , the move from Aurora to SingleStore was very easy. And, because SingleStore is much more flexible than any one AWS database offering, it can accomplish many more tasks in a single database. For instance, SingleStore has strong support for both in-memory rowstore and disk-based columnstore data formats, with data moving flexibly between them. In AWS, by contrast, you might need to use either AWS ElastiCache for Memcached or AWS ElastiCache for Redis for in-memory performance, then transfer data into AWS Redshift for disk-based storage and analytics, making the analytics data stale. SingleStore can also be used to augment, rather than replace, Aurora and other AWS offerings. Data stored in Aurora can be copied to SingleStore for analytics, for example. The Aurora database then runs faster because it no longer has to handle analytics inquiries; analytics queries and analytics apps run faster because they have a dedicated SingleStore database, and because of SingleStore’s faster query performance and greater concurrency support. Conclusion The customer’s ability to save so much money from such a simple change is somewhat ironic, as one of the claimed selling points of AWS Aurora is: “Performance and availability of commercial databases at 1/10th the cost.” Whereas this customer was able to save more than 66%, and gain better performance, greater concurrency, and strategic flexibility, through a simple and easy move from AWS Aurora and Druid.io to SingleStore. Contact SingleStore for more information or start using SingleStore for free today!", "date": "2019-05-03"},
{"website": "Single-Store", "title": "mapping-and-reducing-the-state-of-hadoop", "author": ["Jacky Liang"], "link": "https://www.singlestore.com/blog/mapping-and-reducing-the-state-of-hadoop/", "abstract": "In this blog post, part one of a two-part series, we look at the state of Hadoop from a macro perspective. In the second part of this series, we will look at how Hadoop and SingleStore can work together to solve many of the problems described here. 2008 was a big year for Apache Hadoop. It appeared organizations had finally found the panacea for working with exploding quantities of data with the rise of mobile and desktop web. Yahoo launched the world’s largest Apache Hadoop production application. They also won the “terabyte sort” benchmark, sorting a terabyte of data in 209 seconds. Apache Pig – a language that makes it easier to query Hadoop clusters – and Apache Hive – a SQL-ish language for Hadoop – were actively being developed, by Yahoo and Facebook respectively. Cloudera, now the biggest software and services company for Apache Hadoop, was also founded. Data sizes were exploding with the continued rise of web and mobile traffic, pushing existing data infrastructure to its absolute limits. As a result, the term “big data” was coined around this time too. Then came Hadoop, promising to all organizations to answer any questions you have with your data. The promise: You simply need to collect all your data in one location and run it on free Apache Hadoop software, using cheap scalable commodity hardware. Hadoop also introduced the concept of the Hadoop Distributed File System (HDFS), allowing data to be spanned over many disks and servers. Not only is the data stored, but it’s also replicated 2 – 3 times across servers, ensuring no data loss even when a server goes down. Another benefit to using Hadoop is that there is no limit to the sizes of files stored in HDFS, so you can continuously append data to the files, as in the case of server logs. Facebook claimed to have the largest Hadoop cluster in the world , at 21 petabytes of storage, in 2010. By 2017, more than half of the Fortune 50 companies were using Hadoop. Cloudera and Hortonworks became multi-billion dollar public companies. For an open source project that had only begun in 2006, Hadoop became a household name in the tech industry in the span of under a decade. The only direction is up for Hadoop, right? However, many industry veterans and experts are saying Hadoop perhaps isn’t the panacea for big data problems that it’s been hyped up to be. Just last year in 2018, Cloudera and Hortonworks announced their merger. The CEO of Cloudera announced an optimistic message about the future of Hadoop, but many in the industry disagree. “I can’t find any innovation benefits to customers in this merger,” said John Schroder, CEO and Chairman of the Board at MapR. “It is entirely about cost cutting and rationalization. This means their customers will suffer.” Ashish Thusoo, the CEO Of Qubole, also has a grim outlook on Hadoop in general — “the market is evolving away from the Hadoop vendors – who haven’t been able to fulfill their promise to customers”. While Hadoop promised the world a single data store for all of your data, in cheap and scalable commodity hardware, the reality of operationalizing that data was not so easy. Speaking with a number of data experts at SingleStore, reading articles by industry experts, and reviewing surveys from Gartner, we noticed a number of things that are slowing Hadoop growth and deployment within existing enterprises. The data shows that the rocketship growth of Hadoop had been partly driven by fear of being left behind, especially by technology executives – the ones who overwhelmingly initiate Hadoop adoption, with 68% of adoption initiated within the C-suite, according to Gartner. We will also explore limitations to Hadoop in various use cases especially in this ever-changing enterprise data industry. Let’s dive in. Hype In a Gartner survey released in 2015, the research firm says that an important point to look at with Hadoop adoption is the low number of Hadoop users in an organization , which gives indication that “Hadoop is failing to live up to its promise.” Gartner say that hype and market pressure were among the main reasons for interest in Hadoop. This is not a surprise to many, as it’s hard to avoid hearing Hadoop and big data in the same sentence. Gartner offers the following piece of advice for the C-suite interested in deploying Hadoop: “CEOs, CIOs and CTOs (either singularly or due to pressure from their boards) may feel they are being left behind by their competitors, based on press and hype about Hadoop or big data in general. Being fresh to their roles, the new chief of innovation and data may feel pressured into taking some form of action. Adopting Hadoop, arguably ‘the tallest tree in the big data forest’, provides the opportunity.” The survey warns to not adopt Hadoop because of the fear of being left behind — Hadoop adoption remains still at an early adopter phase, with skills and successes still rare. A concrete piece of advice from Gartner is to start with small projects backed by business stakeholders to see if Hadoop is helpful in addressing core business problems. Using small deployments initially will allow an organization to develop skills and develop a record of success before tackling larger projects. Skills Shortage When using Hadoop for analytics, you lose the familiar benefits of SQL. According to the same survey cited above, it appears that around 70% of organizations have relatively few Hadoop developers and users. The low number of Hadoop users per organization is attributed to Hadoop innately being unsuitable for large simultaneous numbers of users. This also indicates difficulty in hiring Hadoop developers attributed to skill shortage. Which leads to our next point — cost. Cost Two facts about Apache Hadoop: It’s free to use. Forever. You can use cheap commodity hardware. But Hadoop is still very expensive. Why? While Hadoop may have a cheap upfront cost in software use and hosting, everything after that is anything but cheap. As explained before, to make Hadoop work for more than just engineers, there need to be multiple abstraction layers on top. Having additional copies of the data for Hive, Presto, Spark, Impala, etc, means additional cost in hardware, maintenance, and operations. Adding layers on top also means requiring additional operations and engineering work to maintain the infrastructure. While Hadoop may seem cheap in terms of upfront cost, the costs for maintenance, hosting, storage, operations, and analysis make it anything but. Easy to Get Data In, but Very Tough to Get Data Out Getting data into Hadoop is very easy, but it turns out, getting data out and deriving meaningful insight to your data is very tough. A person working on data stored in Hadoop – usually an engineer, not an analyst – is expected to have at least some knowledge of HDFS, MapReduce, and Java. One also needs to learn how to set up the Hadoop infrastructure, which is another major project in itself. Speaking with relevant industry people that have formerly deployed Hadoop or work closely with organizations that use Hadoop, this is the biggest pain point of the technology — how hard it is to run analytics on Hadoop data. Many technologies have been built to tackle the complexities of Hadoop, such as Spark (data processing engine), Pig (data flow language), and Hive (a SQL-like extension on top of Hadoop). These extra layers add more complexity to an already-complex data infrastructure. This usually means more potential points of failure. Hiring Software Engineers is Expensive An assortment of software skills are needed to make Hadoop work. If it’s used with no abstraction layer, such as Hive or Impala, on top, querying Hadoop needs to be done in MapReduce, which is written in Java. Working in Java means hiring software engineers rather than being able to hire analysts which are proficient in SQL. Software engineers with Hadoop skills are expensive, with an average salary in the U.S. at $84,000 a year (not including bonuses, benefits, etc). In a survey by Gartner , it’s stated that “obtaining the necessary skills and capabilities [ is ] the largest challenge for Hadoop (57%).” Your engineering team is likely the most expensive, constrained, and tough-to-hire-for resource in your organization. When you adopt Hadoop, you then require engineers for a job that an analyst proficient in SQL could otherwise do. On top of the Hadoop infrastructure and abstraction layers you need to more easily get data out, you now need to account for the engineering resources needed. This is not cheap at all. Businesses Want Answers NOW As businesses are going international, and customers are demanding instant responsiveness around the clock, companies are pushed to become real-time enterprises. Whether this is to derive real-time insights into product usage , live identification of financial fraud , providing customer dashboards that show investment returns in milliseconds , not hours, or understanding ad spend results on an up-to-the-second basis, waiting for queries to Map and Reduce simply no longer serves the immediate business need. It remains true that Hadoop is incredible for crunching through large sets of data, as that is its core strength — in batch processing. There are ways to augment Hadoop’s real-time decision abilities, such as using Kafka streams. But in this case, what’s meant to be real-time processing slows down to micro batching. Spark streaming is another way to speed up Hadoop, but it has its own limitations. Finally, Apache projects like Storm are also micro-batching, so they are nowhere near real time. Another point to consider is that, the above technologies mentioned are another piece of complexity added to an already-complex data infrastructure. Adding multiple layers between Hadoop and SQL-based analytic tools also means slow response, multiplied cost, and additional maintenance required. In short, Hadoop is not optimized for real-time decision making. This means it may not be well-suited to the evolving information demands of businesses in the 21st century. In this, part one of this two-part series on Hadoop, we talked about the rise of Hadoop, why it looked like the solution to organizations’ big data problems, and where it fell short. In the next part of this series, we will explore why combining Hadoop with SingleStore may help businesses that are already invested in Hadoop.", "date": "2019-05-13"},
{"website": "Single-Store", "title": "memsql-tpc-benchmarks", "author": ["John Sherwood"], "link": "https://www.singlestore.com/blog/memsql-tpc-benchmarks/", "abstract": "Editor’s note: Running these benchmarks and documenting the results is truly a team effort. In addition to John Sherwood, Eric Hanson and Nick Kline authored this blog post. If you’re anything like us, you probably roll your eyes at “company benchmarks its own software, writes a post about it.” This of course raises a question: if we’re cynical industry veterans, numb from the constant deluge of benchmarketing … why are we writing this? Simple: we wanted to prove to ourselves that we’re the only modern, scalable database that can do a great job on the three well-known database benchmarks, TPC-C, TPC-H, and TPC-DS, which cover both OLTP and data warehousing. Now, we hope to get you to suspend your skepticism long enough for us to prove our capabilities to you. SingleStore’s primary focus is on what we call operational analytics (you might have heard this referred to as HTAP , translytical , or HOAP ), running analytical queries across large, constantly changing datasets with consistently high performance. This performance is provided through the use of scale-out, compilation of queries to machine code, vectorized query execution, and use of single instruction, multiple data (SIMD) instructions. Our operational analytics capability blurs OLTP and data warehouse (DW) functionality, and different use cases take advantage of different slices of our spectrum of capabilities. To test some of the new features in our upcoming 7.0 release, we used multiple TPC benchmarks to push SingleStore beyond what our normal everyday stress tests can reach. Our results show that we can do both transaction processing and data warehousing well, and we scale well as workload size increases. No other scale-out database running on industry-standard hardware can do this. Playing Fair TPC benchmarks have a certification process, and we did our best to follow the specifications, but we did not work with TPC to certify the benchmarks, so these are informal results. But we hope it goes without saying that we didn’t cheat. In fact, SingleStore does not have any “benchmark specials” (features designed just to make the benchmark result better, but that nobody would ever use for a real application) built into the code. The TL;DR This is a long post because we wanted to write an in-depth overview of how we ran the benchmarks so you can see how we achieved all these results. But if you want just a quick summary, here’s how we did on the benchmarks. TPC-C: SingleStore scaled performance nearly linearly over a scale factor of 50x. TPC-H: SingleStore’s performance against other databases varied on this benchmark, but was faster than multiple modern scale-out database products that only support data warehousing. TPC-DS: SingleStore’s performance ranged from similar, to as much as two times faster than other databases. Expressed as a geometric mean, as is often done for benchmarking results, our performance was excellent. What we accomplished was solid performance across three distinct benchmarks, establishing SingleStore as a top choice database for operational analytics and cloud-native applications . Now let’s move on to the details of the benchmarks. TPC-C: Scale-out OLTP On the massively transactional side of the spectrum, we have the TPC-C benchmark. To quote the 1997 SIGMOD presentation announcing it, a benchmark is a “distillation of the essential attributes of a workload,” and TPC-C distills the absolute hell out of a sharded transactional workflow. As the scale (defined by the number of warehouse facilities being emulated) increases, the potential parallelism increases as well. From our perspective, this is ideal for discovering any bottlenecks in our own code. Our TPC-C database design used single tables for the large data sets, allowing us to use an existing driver targeting MySQL. Unlike some other official results published by major database vendors, we did not artificially split the data into multiple separate tables to reduce locking and contention. The way we ran the benchmark is much simpler, and shows how our scale-out architecture can make application development easier. While experimenting with smaller datasets, we quickly discovered that driving significant load required us to nearly match the CPU of the aggregators with the leaves in the cluster. This configuration, strong all over (“very Schwarzenegger”, as one of our engineers put it) is quite unusual; it’s rare that customers require such a ratio of aggregators to leaves. In part, our internal benchmarking service colocates drivers on aggregators, which required us to size up boxes to have the extra CPU. Additionally, under the pure OLTP TPC-C workload, aggregators are primarily coordinating transaction state and are effectively concurrency bound. As we would recommend for a real workload of this nature, we had redundancy and synchronous replication enabled. This means that there are two copies of every data partition on two different nodes for high availability (HA), and transaction commits are not acknowledged to the client until both the primary and secondary replicas are updated. This of course requires twice the memory compared to running without HA, and adds to transaction overhead, since row updates must be applied on the replica node before the commit. This cost is to be expected if HA with fast failover is required. If you want HA, you need to pay. After experimenting with smaller datasets, we drove just over 800,000 transactions per minute (tpmC) against 10,000 warehouses, using a cluster with 18 leaves and 18 aggregators, all r3.8xlarge instances. This led us to scale up, moving onto TPC-C at the 50,000 scale. This required what we would term a formidable cluster; storing a single replica of the data (including indexes) required more than 9 TB of RAM. For HA, we stored two copies of the data, which, after some fairly straightforward math, took us to almost 20 TB. We decided to use r5.metal instances, using 2x the leaf instances, for a total of 6x the cores compared to the 10,000 warehouse cluster. Notably, the only change we made in running the benchmark itself was in tuning the partition count and hash index buckets; from the perspective of the open source benchmark driver we were using, originally written for MySQL, everything just worked. The last notable thing about our 50,000 warehouse run was that we ran out of hardware from AWS while trying to reach the limit of the cluster; as we had taken 36 hosts for leaves and 21 for aggregators, we suppose that’s pretty reasonable. With 1,728 physical cores on the leaf nodes for the megacluster (to use the scientific term), it’s hard to argue with calling this “throwing hardware at the problem.” However, as discussed above, we were able to easily meet the same per-core performance as at smaller scales. We feel that our overall results are a strong validation of the performance improvements to our new intra-cluster replication implementation in SingleStore DB 7.0. Figure 1 – TPC-C results for SingleStore. Scale vCPUs 1,000 48 10,000 576 50,000 3,456 Table 1 – Leaf Cores, normalized to vCPUs. This table only includes leaf vCPUs for SingleStore, as we also had to use aggregator CPU to run the benchmark driver instances. Full details are included in Appendix A. In Table 1, for SingleStore, we show both the number of leaf cores and the total number of cores, including aggregator cores. The way our benchmark kit is designed, it was easiest for us to run the benchmark with this configuration. In addition, our benchmark setup ran the application threads driving the transaction work on the aggregator machines. Normally that is not accounted for in the benchmark results. The aggregator nodes were not fully utilized, and significantly over-provisioned to drive this test, though we were not able to determine the minimum amount of equipment needed to obtain performance at the level shown in Figure 1. Despite the caveats, these results show the dramatic benefits of our distributed architecture, in-memory skiplist and hash storage structures, and compilation of queries to machine code. While the above data is a beautiful (in our eyes) illustration of our ability to consume an OLTP firehose, these results are only possible by violating the rate limiting requirement found in the TPC-C specification. TPC-C is intended to emulate a limited number of physical terminals being interacted with by humans, and so more faithful drivers are limited to only pushing a handful of tpmC per warehouse. In a great example of why the TPC and the concept of audited results exist, we explicitly state here that these results are not comparable to others on that basis. Per-Core Throughput for OLTP It’s comforting to know that you’re getting your money’s worth as you scale a cluster and workload to larger sizes. Figure 2 illustrates the ability to maintain throughput per core for a transaction processing workload as data and cluster size increase. This translates to roughly linear scalability in throughput as the size of the transaction processing workload increases. Yet you can still run analytical queries across all the data, unlike the common approach of manually sharding large data sets across multiple databases instances with a single-node DBMS, such as Postgres, MySQL, or SQL Server. The nature of SingleStore allows for running analytic-style queries on this data in the same instance that is also handling transaction processing. Figure 2 — Per vCPU transaction throughput (includes aggregator vCPUs) . When increasing the dataset size and the hardware to match, SingleStore TPC-C throughput per vCPU held within a 30% band. We had plenty of headroom on the leaves at the 50,000 scale, and could have driven more throughput, but AWS ran out of r5.metal hosts for us to use as benchmark drivers, which was a pretty cool consolation prize in our opinion. TPC-H and TPC-DS: DW Scalability and Then Some On the opposite end of the spectrum, we have TPC-H (H) and TPC-DS (DS), categorized by the TPC as “decision support” benchmarks. H and DS use similar datasets, and DS is effectively the next-generation version of H. While H generates fairly straightforward queries and tends to be shard-friendly, DS thrills in its use of advanced SQL features and functions and exhilarates in its lopsided filters. Running DS is notoriously, purposefully difficult, and SingleStore can run all 99 DS queries, while many decision support-oriented databases cannot. TPC-DS At scale factor 10,000, the largest TPC-DS table contains just shy of 29 billion rows, with some 24 billion others spread out across the rest of the tables. TPC-DS then runs a set of 99 queries using a  formidable array of operators and features against this large data set. Below, we show all published official TPC-DS benchmark results to date [ AL19 , AL19b , Cis19 ] , comparing the total query runtimes with our unofficial SingleStore run. In addition, we sought out one more informal result on a well-known cloud data warehouse product, which we refer to as “Cloud DW Product.” Although SingleStore’s core strength is operational analytics, these comparisons show that we also are also competitive with data warehouse and big data products on a very challenging pure data warehouse workload. Figure 3 — comparative TPC-DS runtimes at 10TB scale factor — lower is better. For TPC-DS, we performed a ‘power run,’ where the sequential runtime of each query is summed. Please see the differences in benchmark versions and hardware used, described below. Product Physical Cores (not VCPUs) TPC-DS Version Is Certified Benchmark Result Power Run Time (s) Geometric Mean (s) SingleStore 640 1.4.0 no 6,495 6.03 Cloud DW Product 32 nodes, unknown size 1.4.0 no 4,481 11.03 Alibaba AnalyticsDB 512 2.10.1 yes 3,935 9.81 Alibaba Hadoop 520 2.10.1 yes 16,207 81.96 Transwarp Data Hub 1024 2.7.0 yes 21,615 72.73 Table 2 – Hardware used for TPC-DS runs and results, Power Run times, and Geometric Mean times . We normalized to physical cores rather than vCPUs to match TPC usage. The values displayed in Figure 3 and Table 2 don’t imply a strict ranking of the systems because of the hardware and version differences. Nevertheless, we believe that the overall results illustrate important relative strengths of the different systems, because the hardware for all the systems is of the same order of magnitude in scale, and the queries are largely equivalent between versions. We’ve included the geometric mean of the results in Table 2. You can calculate the geometric mean as the nth root of the product of n numbers. (For instance, the 9th root of the product of 9 numbers.) The interesting aspect of a geometric mean, when used for benchmark results, is that it de-emphasizes outliers. As a result, many consider it the most useful single value when reporting results of this type. For the result shown in Figure 3, where we spent about 6,500 seconds to run the queries, we used a cluster consisting of 19 i3.16xlarge leaves and a single i3.16xlarge aggregator instance. With over 600 leaf CPUs, this was certainly a sizable (and, we have been informed by our finance team, expensive) cluster, but still similar in scale to other clusters that successfully completed the benchmark. TPC-H Figure 4 — TPC-H total runtimes comparing SingleStore and SQL Server — smaller is better. We performed the same ‘power run’ with TPC-H as for DS, where the result is the sum of the run times of the sequentially executed queries that constitute the benchmark. This figure compares SingleStore, SQL Server 2017 at scale factor 10,000, and SQL Server 2016 at scale factor 30,000 (10 TB and 30 TB). Compared to TPC-DS, TPC-H is a much friendlier (and older) benchmark. We ran H at the 10 TB scale factor on a cluster of 8 i3.8xlarge leaf nodes, and then scaled to run it at 30 TB using 24 i3.8xlarge leaf nodes. The 30 TB dataset contains a table with 180 billion rows, compressed to about 11 TB on disk in our columnstore, and took about three and a half hours at an average of 2.4 GB/s to load from S3 using our Pipelines feature. While researching the benchmark, we found that every run submitted to the TPC at these larger scales was for SQL Server, the best of which we’ve graphed next to our results above. As shown in Figure 4, at a lower scale, we are outperformed by a quad-socket host running SQL Server with 224 vCPUs versus our 256. But at a larger size, scale-out catches up to scale-up – and running on commodity hardware versus specially-crafted quad socket monsters is a virtue of its own, as far as data center operators are concerned. Details of the SQL Server results are given in benchmark results published by Microsoft [ MS16 , MS17 ] . Figure 5 – Data volume processed per second per core for TPC-H . This chart shows a normalized throughput measure (DB _ size _ MB/cores/sum _ of _ query _ runtimes _ in _ seconds) for each of TPC-H 10000 and TPC-H 30000. This is to illustrate how SingleStore scales for the benchmark as data size increases. What we’re most looking for here is validation of our scale-out architecture. A fantasy, “perfectly scalable” database would process 3x the data with 3x the hardware in the same amount of time; as illustrated in Figure 5, we took about 30% longer than that ideal. Given that queries often require a final aggregation step, some falloff is expected. Moreover, more hardware adds substantial marginal return and doesn’t hit a ceiling in these tests — per core throughput drops by 30% as total work done goes up by a factor of around 300%. Figure 6 — comparative TPC-H results at 30 TB scale factor. We again scoured the web for TPC-H results and found several for well-known cloud data warehouse products, including Amazon Redshift, Microsoft Azure SQL Data Warehouse, and Snowflake Data Warehouse in reports from GigaOm [ GO18 , GO19 ] , as well as certified single-box results on the TPC web site. As shown in Figure 6, SingleStore provides competitive results compared to these products. Cost and Value At this point you might think the results are pretty impressive, but you’re probably also thinking there is some special-purpose hardware helping make these benchmarks run smoothly. Well, there isn’t. Just as we didn’t add any code tricks to make the benchmark run better, we didn’t use anything other than industry-standard hardware. We really wanted to make these benchmarks replicable by anyone who wanted to test them, and you can. We explain how to do that below. The enterprise edition of SingleStore will cost you dramatically less than legacy vendors such as Oracle, plus we have a free version running on up to a 4 node cluster that supports all enterprise features. Beyond just price, we believe SingleStore offers new value to customers by enabling them to build operational analytics applications, mixing OLTP and analytics operations, in a way that has not been feasible before. This enables real-time dashboards and other analytics on just-born data, driving new business value. By showing results for TPC-C, TPC-H, and TPC-DS running on one scale-out-capable SQL DBMS, we believe we demonstrate that SingleStore delivers new value. In addition, given the breadth SingleStore covers, once a group of application developers in an organization are trained to use it, they can apply their skills to a broad range of applications, giving flexibility to them and their managers. This of course is also valuable to the business. Conclusion We’ve shown that SingleStore can deliver strong results for the TPC-H and TPC-DS analytical benchmarks and the TPC-C transaction processing benchmark. To our knowledge, we’re the only SQL DBMS supporting true shared-nothing scale-out that can do this. Our hardware needs are comparable to competitors when running these benchmarks at similar scale factors, and our software licensing costs are significantly less than those of the established database vendors. SingleStore skills can be used to build a variety of applications, including systems for transaction processing, operational analytics, operational data stores, and data warehousing, without scale limits. This will give you and your team the ability to solve multiple problems at once, in less time, with one powerful tool. This post was edited to remove a comparison to another company’s published data after we discovered that the comparison was technically inaccurate. Appendix A: More Detailed Results TPC-C The hardware configurations we used for TPC-C for 1,000, 10,000, and 50,000 warehouse scale factors are given in the following table. The node types given are for AWS. Warehouses Leaf Nodes Sum Leaf vCPUs Sum Leaf RAM Aggregator Nodes tpmC 1,000 3 x r3.4xlarge 48 366 GB 3 x r3.4xlarge 94,900 10,000 18 x r3.8xlarge 1,152 4.29 TB 18 x r3.8xlarge 800,400 50,000 36 x r5.metal 3,456 27 TB 21 x r5.metal 5,300,000 We used the open-source MySQL driver from Percona for TPC-C, with slight modifications to the DDL. Specifically, we made the item table into a reference table, the history table into a columnstore table, changed indexes where appropriate to be hashes instead of skiplists, and sized the bucket count appropriately for the size of the dataset. Our cluster was configured to use two replicas for data, replicated synchronously across paired hosts, with asynchronous writes to disk. Aggregator nodes ran the benchmark driver alongside the SingleStore aggregator instances, where the driver consumed roughly half of the CPU used. Although we used the same instance type for leaves and aggregators out of convenience, in the TPC-C workload aggregators also required very little RAM, and it should be easily possible to save on costs by running on lower-end hosts. We can’t speak highly enough of the driver from Percona and how useful it was in enabling us to run the benchmark. We generated our datasets using the utility from the same repo, albeit modified to generate pipe-separated value files. We then loaded these files in parallel with SingleStore Pipelines , instead of doing direct database inserts. TPC-H The hardware configurations we used on AWS for TPC-H are given in the following table. Scale Factor Leaf Nodes Sum Leaf Phys. Cores Sum Leaf RAM Aggregator Nodes Power Run (s) 10 TB 8 x i3.8xlarge 128 1.9 TB 1 x m4.10xlarge 1,789 30 TB 24 x i3.8xlarge 384 5.72 TB 1 x m4.10xlarge 2,371 We generated our datasets using the open-source dataset generator available from tpc.org. In calculating the power run, we performed a warmup query, then averaged the result of 1-5 executions. The total number of runs was sometimes limited due to the total runtime of all the queries and the time available to finish the runs. TPC-DS The hardware configuration we used on AWS for TPC-DS is shown in the table below. Scale Factor Leaf Nodes Sum Leaf Phys. Cores Sum Leaf RAM Aggregator Nodes Power Run (s) 10 TB 19 x i3.16xlarge 608 9.05 TB 1 x i3.16xlarge 6,495 We generated our datasets using the dataset generator available from tpc.org. In calculating the power run, we performed a warmup query, then averaged the result of 1-5 executions, depending on the runtime of query execution, in the same fashion as for our TPC-H runs. Appendix B: How to Play Along at Home If you’re interested in replicating our results, or experimenting with SingleStore and these benchmarks, we’ve included instructions on how we went about running C, H, and DS here. TPC-C To run the TPC-C benchmark, we relied heavily on Percona’s excellent work, which we’ve forked to github.com/memsql/tpcc-mysql. This driver allows both generating the dataset and driving load. As SingleStore is wire compatible with MySQL, we were able to use the driver binary in an effectively unaltered format; in fact, the only changes we made to the repository were as follows: Fixed a units bug that caused queries to be judged as failing if they took 5 milliseconds instead of 5 seconds. Changed the dataset generator to create pipe-separated value files rather than inserting directly into the database. Altered the schema to have shard keys and to take advantage of SingleStore features such as reference tables and hash indices. If you’d like to use this driver to verify our results, the README covers how to both generate data and run the benchmark. Cluster Setup For the purposes of the TPC-C benchmark, there are several configuration settings that will drastically impact your results. To match what we used, do the following: SingleStore DB 7.0: While we did recently make our latest and greatest 6.x release available, one of the major improvements in SingleStore DB 7.0 (download here ) is an entirely new system for handling intra-cluster replication more efficiently. While 6.x releases support replication, performance is dramatically improved in 7.0. Enable synchronous replication. Enable asynchronous durability. Once SingleStore has the correct configuration, you will need to create a database, per the Percona README. Benchmark Execution While running the benchmark is exceedingly simple, to fully stress a large SingleStore cluster you will need to spread the load across multiple aggregators. To get relevant results, the driver instances need to run in parallel, and only data from when all drivers are running is a valid representation of the cluster’s performance. We wrote a harness that used the database to coordinate across hosts; however, as it hooks in pretty deeply to our internal benchmarking system, we’re unable to meaningfully open source this extension. TPC-H & DS While the TPC-C specification is highly descriptive and gives a wide latitude in how results are achieved, TPC-H and DS are explicitly about executing SQL queries. As such, we were able to leverage the dataset and query generation tools available for free download from tpc.org: [ H ] , [ DS ] . At a high level, running both benchmarks on SingleStore consists of the following steps: Data generation Create the table schema using our script (so that you use properly sized SingleStore data types and our shard key definitions and indexes) using create-tables.sql Load the data, using SingleStore Pipelines or a tool of your choice After loading the data, create statistics and optimize the tables; the commands are found in after-load.sql Run the queries by executing queries.sql Sizing the SingleStore database cluster We ran the benchmarks on AWS. For the DS benchmark at the 10 TB size we used a 20 node cluster with 1 MA, and 19 leaves, in total 20 nodes; for the H benchmark at 30 TB we used a cluster with an m4.10xlarge aggregator and 24 i3.8xlarge leaves. Generating the data Use the standard data generator for each benchmark found at http://www.tpc.org/tpc _ documents _ current _ versions/current _ specifications.asp . To accomplish this, look under H & DS and download the tools for that benchmark. Register, download the latest version, compile the C-based generator. Data generation can take a long time, especially at a larger scale like the 10 & 30TB sizes we used. The data size of the csvs will unexpectedly be quite close to the promised value, and we highly recommend asynchronously compressing and uploading to cloud storage; we stored the generated data files in AWS s3. Creating the schema You can find the table schema and queries at the SingleStore github repo at memsql/benchmarks-tpc , looking under the ‘h’ or ‘ds’ subdirectories as appropriate. You can clone or copy them to your local machine by using the command git clone git@github.com:memsql/benchmarks-tpc.git In the benchmarks-tpc/tpcds and benchmarks-tpc/tpch  directories you can find the files we used to create and run the benchmark, respectively H and DS versions of create-tables.sql , after-load.sql and queries.sql . Two common ways to submit queries to SingleStore are to use the command line MySQL client or the SingleStore tools. The SingleStore Studio tools, including the SQL editor, are described at https://docs.singlestore.com/memsql-studio/latest/memsql-studio-overview/ . Loading the data Depending on the system your test cluster is running on, there are various alternatives for loading data. The SingleStore Pipelines feature loads data in parallel; see https://docs.singlestore.com/memsql-pipelines/v6.7/pipelines-overview/ . We used SingleStore Pipelines to load data from S3 for our tests. An example pipeline command follows. CREATE PIPELINE catalog_sales AS LOAD DATA S3 \"<memsqlpath\\>/sf_10000/catalog_sales.\" CONFIG '{\"region\":\"us-east-1\",\"disable_gunzip\":false}' CREDENTIALS '{ \"aws_access_key_id\": \"<key\\>\", \"aws_secret_access_key\": \"<secret\\>\"}' INTO TABLE `catalog_sales` FIELDS TERMINATED BY '|' LINES TERMINATED BY '|\\n'; After loading The background flusher in SingleStore automatically and asynchronously reorganizes columnstore segment files (the internal files used to store columnstore tables) for compression and to create a total ordering across all segment files in a table. To make query execution faster and avoid asynchronous processes during query execution, we manually ran the command “optimize table”, as in OPTIMIZE TABLE `call_center` ; see https://docs.singlestore.com/sql-reference/v6.7/optimize-table/ SingleStore has manually created histogram statistics. Manual statistics can be created by running “analyze table” on each table, such as: ANALYZE TABLE `call_center` COLUMNS ALL ENABLE We did this before running the queries. These two commands are provided in after-load.sql . Running the queries We ran the queries multiple times and use the average run time, not counting the initial run where the query was compiled. We used columnstore tables for this benchmark, which are stored on disk, not in memory. The difference between the warm-up run and subsequent runs is just the time to compile a query, which was generally less than a second, but might be as long as a couple of seconds for very large and complex queries. The average run time varies a little on multiple runs of a query in a shared system like AWS, but we never saw that much variation. Many TPC benchmarks, including TPC-DS, have templatized queries that replace “placeholders” in an incomplete query text with specific values, such as numeric or string constants. We didn’t find much variation with different parameters, so we used a fixed parameterization for the ease of tracking performance across time. We include the query text with the parameters we used when running the queries in queries.sql . Appendix C: TPC-H Raw Results As described above, when running TPC-H and TPC-DS, there was an initial “cold” or warm-up run where SingleStore compiles (optimize and code-gen) the queries in addition to running them. (See our documentation for the compilation pipeline.)The warm run reuses that same query plan, but re-executes the entire query re-reading all data. The cold run time is a single run, while the final result is an average of warm runs as reported in Appendix A. Generally the difference between final and cold times comes down to the compilation time as described above. However, as some results show, there can be other factors coming from the use of shared infrastructure. Query 10T (final) 10T (cold) 30T (final) 30T (cold) 1 18.916 19.815 24.238 26.117 2 17.073 18.38 94.10 101.062 3 76.483 76.655 97.520 103.450 4 28.236 29.84 34.818 36.77 5 48.442 49.520 57.897 61.024 6 9.852 10.42 12.712 13.536 7 45.125 46.410 85.488 84.513 8 27.961 28.751 65.812 66.735 9 154.404 159.037 266.970 266.544 10 62.519 64.069 73.248 75.532 11 30.524 32.183 82.781 83.560 12 16.454 18.083 20.369 22.320 13 183.827 183.469 201.577 193.112 14 106.647 107.013 125.455 127.872 15 44.927 46.704 67.685 70.124 16 35.947 37.478 39.534 40.943 17 15.431 16.307 34.820 34.690 18 428.70 425.285 499.514 501.920 19 27.400 31.504 29.340 31.137 20 268.29 271.174 221.616 230.66 21 83.632 84.015 118.185 118.438 22 58.341 61.310 120.527 122.270 Appendix D: TPC-DS Raw Results Refer to Appendix C for information on how to interpret these results. Query 10T (final) 10T (cold) 1 7.933 9.232 2 16.367 17.324 3 0.774 1.633 4 105.199 106.541 5 23.133 27.435 6 149.987 150.346 7 2.033 3.293 8 2.624 3.558 9 9.375 17.654 10 8.231 11.261 11 46.006 45.239 12 0.836 3.395 13 2.327 4.435 14 321.497 329.853 15 7.681 5.129 16 60.115 58.161 17 2.421 6.716 18 10.289 13.983 19 2.059 4.071 20 0.955 3.061 21 1.292 2.353 22 1.509 3.311 23 590.080 598.040 24 1435.269 1530.016 25 2.390 7.031 26 1.510 2.930 27 2.245 4.688 28 27.215 34.892 29 120.356 124.340 30 8.656 10.561 31 18.996 19.429 32 0.541 2.212 33 7.459 9.128 34 2.730 4.494 35 21.369 25.379 36 1.017 2.043 37 10.826 12.178 38 79.474 81.756 39 0.466 1.686 40 1.526 4.418 41 0.946 1.544 42 0.385 0.972 43 3.816 5.118 44 3.320 5.727 45 5.890 7.475 46 4.012 5.758 47 10.873 12.565 48 1.554 3.780 49 3.285 9.361 50 1.942 4.884 51 3.006 5.080 52 0.894 1.508 53 0.741 1.818 54 4.990 8.529 55 0.724 1.403 56 5.036 7.167 57 6.109 8.491 58 3.430 4.521 59 26.509 27.612 60 7.257 9.501 61 3.056 6.141 62 2.779 4.038 63 1.879 4.765 64 11.704 28.918 65 3.920 5.362 66 1.629 8.578 67 1382.295 1379.958 68 3.045 5.198 69 7.111 10.689 70 5.992 6.805 71 3.560 6.693 72 38.113 39.607 73 1.799 2.840 74 32.279 33.930 75 21.565 29.151 76 1.550 4.514 77 3.917 8.667 78 1392.938 1431.461 79 18.727 21.594 80 6.433 13.057 81 9.889 11.282 82 28.270 29.588 83 4.068 5.807 84 1.807 3.201 85 2.433 5.647 86 0.684 1.599 87 63.438 64.373 88 24.798 27.863 89 0.927 1.940 90 0.648 2.492 91 1.976 3.140 92 0.414 2.424 93 0.372 1.183 94 32.121 35.912 95 152.090 152.829 96 0.765 1.766 97 8.628 9.905 98 1.443 3.888 99 6.189 7.854 References [ AL19 ] TPC Benchmark DS Full Disclosure Report, Scale Factor 10,000, Alibaba, http://www.tpc.org/results/fdr/tpcds/alibaba~tpcds~alibaba _ cloud _ e-mapreduce~fdr~2019-03-19~v01.pdf , March 19, 2019. [ AL19b ] TPC Benchmark DS Full Disclosure Report, Scale Factor 10,000, Alibaba Cloud AnalyticDB, http://www.tpc.org/results/individual _ results/alibaba/alibaba~tpcds~alibaba _ cloud _ analyticdb~es~2019-04-26~v01.pdf , April 26, 2019. [ DB17 ] J. Sompolski and R. Xin, Databricks, Benchmarking Big Data SQL Platforms in the Cloud, https://databricks.com/blog/2017/07/12/benchmarking-big-data-sql-platforms-in-the-cloud.html , 2017. [ Cis19 ] TPC Benchmark DS Full Disclosure Report, Scale Factor 10,000, Cisco, http://www.tpc.org/results/fdr/tpcds/cisco~tpcds~cisco _ ucs _ integrated _ infrastructure _ for _ big _ data _ ~fdr~2018-03-05~v01.pdf , March 5, 2018. [ Clo17 ] Apache Impala Leads Traditional Analytic Database, Cloudera, https://blog.cloudera.com/blog/2017/04/apache-impala-leads-traditional-analytic-database/ , April 25, 2017. [ GO18 ] Data Warehouse in the Cloud Benchmark, https://gigaom.com/report/data-warehouse-in-the-cloud-benchmark/ , 2018. [ GO19 ] Data Warehouse Cloud Benchmark, GigaOm, https://gigaom.com/report/data-warehouse-cloud-benchmark/ , 2019. [ MS16 ] SQL Server 30T TPC-H Result, [https://lenovopress.com/lp0502-x3950-x6-tpch-30tb-benchmark-result-2016-05-02] ( https://lenovopress.com/lp0502-x3950-x6-tpch-30tb-benchmark-result-2016-05-02 , 2016. [ MS17 ] SQL Server 10T TPC-H Result, http://c970058.r58.cf2.rackcdn.com/individual_results/Lenovo/lenovo~tpch~10000~lenovo_thinksystem_sr950~es~2017-07-09~v01.pdf , 2017.", "date": "2019-05-14"},
{"website": "Single-Store", "title": "68-query-performance", "author": ["Eric Hanson"], "link": "https://www.singlestore.com/blog/68-query-performance/", "abstract": "In this blog post, I’ll focus on new query processing capabilities in SingleStore DB 6.8. The marquee query feature is just-in-time (JIT) compilation, which speeds up query runtimes on the first run of a query – now turned on by default. We have also improved performance of certain right and left outer joins and related operations, and Rollup and Cube. In addition, we add convenience features, including sub-select without an alias, and extended Oracle compatibility for date and time handling functions. Finally, new array functions for splitting strings and converting JSON data are added. Other improvements in 6.8 are covered elsewhere . These include: secured HDFS pipelines improved pipelines performance LOAD DATA null column handling extensions information schema and management views enhancements Now, let’s examine how just in time queries can work in a database. Speeding up First Query Runtimes SingleStore compiles queries to machine code, which allows us to get amazing performance, particularly when querying our in-memory rowstore tables. By spending a bit more time compiling than most databases – which interpret all queries, not compiling them – we get high performance during execution. This works great for repetitive query workloads, such as real-time dashboards with a fixed set of queries and transactional applications. But our customers have been asking for better performance the first time a query is run, which is especially applicable for ad hoc queries – when slower performance can be especially noticeable. In SingleStore DB 6.7, we first documented a JIT feature for SQL queries, enabled by running ‘set interpreter mode = interpret_first’. Under this setting, SingleStore starts out interpreting a query, compiles its operators in the background, then dynamically switches from interpretation to execution of compiled code for the query _ as the query runs the first time . The interpret_first setting was classified as experimental in 6.7, and was off by default. In 6.8, we’re pleased to say that interpret_first is now fully supported and is on by default. This setting can greatly improve the user’s experience running ad hoc queries, or when using any application that causes a lot of new SQL statements to be run, as when a user explores data through a graphical interface. The interpret_first setting can speed up the first run of a large and complex query – say, a query with more than seven joins – several times by reducing compile overhead, with no loss of performance on longer-running queries for their first run. Rollup and Cube Performance Improvements Cube and Rollup operator performance has been improved in SingleStore DB 6.8 by pushing more work to the leaf nodes. In prior releases, Cube and Rollup were done on the aggregator, requiring more data to be gathered from the leaves to the aggregator, which can take more time. For example, consider the following query from the Cube and Rollup documentation : SELECT state, product_id, SUM(quantity)\nFROM sales\nGROUP BY CUBE(state, product_id)\nORDER BY state, product_id; The graphical query plan for this in 6.8, obtained using SingleStore Studio , is the following: Notice the Grouping Set operator, third from the bottom, which is used for the Cube calculation. The Grouping Set operator is below the Gather operator, which means it is done on the leaves in this case. This enhancement speeds up up several queries in the TPC-DS benchmark. In particular, query 67, which contains a large Rollup, improved by 5.5X compared with SingleStore DB 6.7. Right Semi/Anti/Outer Join Support SingleStore DB 6.8 introduces a new approach to executing certain outer-, semi-, and anti-joins. This does not add any new functional surface area to our SQL implementation; rather, it speeds up execution of some queries. A true right join operator is now supported, and certain kinds of left joins can be rewritten to right joins to enable them to run faster. For example, consider two tables, S and L, where S is a small table and L is a large table. Suppose this query or subquery is encountered: S left join L This can be rewritten and executed as L right join S Here, the hash build side is for S. Then L is scanned and the L rows are used to probe the hash table for S. Since L is large and S is small, this is a good strategy for this query, since it results in a smaller hash table that can more easily fit in cache. This enhancement can substantially speed up certain queries. For example, query 21 of the TPC-H benchmark speeded up about 4.3X using this approach from SingleStore DB 6.7 to SingleStore DB 6.8. Subselect Without Alias SingleStore DB 6.8 now allows you to use a subquery without an alias (name) for it, when leaving off the alias will not be ambiguous. For example, you can now say this: -- find average of the top 3 quantities\nSELECT AVG(quantity)\nFROM (SELECT * FROM sales ORDER BY quantity DESC LIMIT 3); Rather than this: SELECT AVG(quantity)\nFROM (SELECT * FROM sales ORDER BY quantity DESC LIMIT 3) as x; Resource Governor Extensions SingleStore DB 6.8 includes several extensions to the resource governor, which ensure that resources for more operations are governed under the desired pools. These extensions are: LOAD DATA operations now run in the pool for the current connection where the load operation is running. Stored procedures run in the resource pool of the current connection from where they are called. Query optimization work always runs in the pool of the current connection. Pipelines can be run under a pool you specify when you create the pipeline, using a new clause: [ RESOURCE POOL pool_name ] . New Built-In Functions Two new built-in functions related to arrays are provided: SPLIT() and JSON_TO_ARRAY(). SPLIT() The SPLIT() function has the following prototype: split(s text [, separator text NOT NULL])returns array(text) It splits a string into an array, using any amount of whitespace as a delimiter if no delimiter is specified. Or, if a delimiter is specified, it splits at that delimiter. For example, the query SELECT array_length(split('a b c') :> array(text)); returns 3. Normally you would use SPLIT in a stored procedure (SP) or a user-defined function (UDF). JSON_TO_ARRAY() The JSON_TO_ARRAY() function takes a JSON array and returns a SingleStore array. If your applications use JSON data that contains arrays, you can use JSON_TO_ARRAY when processing it in SPs or UDFs. For example: JSON_TO_ARRAY('[ \"foo\", 1, {\"k1\" : \"v1\", \"k2\" : \"v2\"} ]') would produce a result array of JSON elements (we’ll call it r) of length 3 like this: r[0] = \"foo\"\nr[1] = 1\nr[2] = '{\"k1\":\"v1\",\"k2\":\"v2\"}' Expression and Built-In Function Changes A number of small changes have been made to new expression capabilities. First, in SingleStore DB 6.7, we introduced a number of functions to make it easier to port Oracle applications and queries to SingleStore, and easier for experienced Oracle developers to use SingleStore. These include: NVL(), TO_DATE(), TO_TIMESTAMP(). TO_CHAR(), DECODE() REGEXP_REPLACE(), REGEXP_INSTR() We’ve improved the compatibility of these functions as follows: Enable TO_DATE() to support format strings with time-related format options (“HH”, “SS”, etc.) Enable TO_DATE() to support the “DD” format option Enable TO_TIMESTAMP() to support “YY” and “FF” format options Enable TO_TIMESTAMP(), TO_DATE(), and TO_CHAR() to support the “D” format option Enable TO_TIMESTAMP() and TO_DATE() to support using different punctuation as a separator Enable TO_TIMESTAMP() and TO_DATE() to raise an error, instead of returning NULL, for certain error cases Enable TO_CHAR() to support “AM” and “PM” Modify how TO_TIMESTAMP() parses “12” when using the “HH” format option In addition, we increased the precedence of || as concat (under sql_mode = ‘PIPES_AS_CONCAT’) to be compatible with MySQL and Postgres. Summary This post has covered the query processing extensions in SingleStore DB 6.8. We hope you’ll especially enjoy the new JIT compilation feature that improves first-run query times by default. Try it for your ad hoc workloads. Please download SingleStore DB 6.8 today!", "date": "2019-05-14"},
{"website": "Single-Store", "title": "memsql68-benchmarks", "author": ["Mike Boyarski"], "link": "https://www.singlestore.com/blog/memsql68-benchmarks/", "abstract": "SingleStore has spent the last six years on a single mission: developing the fastest, most capable database on the market for a new generation of workloads. Today’s businesses are beginning to win or lose on their ability to use data to create competitive advantage, and technology teams at these companies need data infrastructure that can meet an increasingly broad set of requirements, perform well at scale, and fit easily with existing processes and tools. SingleStore DB 6.8 is the latest release of the database that meets this challenge. Whether accelerating and simplifying the use of data to improve customer experiences, or accelerating analytics to drive better decision-making and optimize operations, both legacy databases and the current sprawl of specialty tools are failing for many data professionals. Today, we are proud to announce that we have reached an amazing milestone. SingleStore is the first database with true shared-nothing scalability, enabling essentially unlimited scale, to provide outstanding results on performance tests derived from three leading industry benchmarks : TPC-C, TPC-H, and TPC-DS . These results are now possible with our latest software releases. We are announcing the general availability of SingleStore DB 6.8 and the first beta release of SingleStore DB 7.0 . SingleStore DB 6.8 offers even faster analytics performance and advanced security for Hadoop environments . The SingleStore DB 7.0 beta previews new, mission-critical transactional capabilities for system of record applications and even more dramatic query performance improvements. Proving Database Scale and Performance for Both Transactions and Analytics We believe that the newest analytical systems and data-intensive applications, which include streaming, real-time decisions, predictive decisions, and dynamic, personalized experiences, represent a new set of workloads, which we call operational analytics . Operational analytics involves a specific combination of key database features, notably fast ingest through transaction processing, fast processing, and low-latency queries for reports and dashboards. Current industry benchmarks were designed to individually highlight the capabilities of different kinds of databases, since no one database, until recently, could run both transactional and analytics workloads at any scale. Yet that is exactly what today’s real-time and near-real-time operational and analytical data management environments require. In order to demonstrate what SingleStore can do for these use cases, we performed unofficial runs of different benchmarks from the TPC family. This allows us to showcase that SingleStore has the broad capabilities, performance across use cases, and scalability required today. The results were astounding. Using a single database product on standard cloud hardware, SingleStore was able to meet or beat the results of databases designed for only doing either transaction processing or analytics – not both. SingleStore is the only modern database that can successfully perform, scale, and deliver the full breadth of capabilities required to support today’s demanding analytics and operational applications. If you would like to learn more about these benchmarks, we’ve documented our infrastructure configuration, the detailed results, and comparisons to other database products in our benchmark blog post . But Wait, There’s More … Product Improvements in SingleStore DB 6.8 We are also excited to highlight the new capabilities in SingleStore DB 6.8. Our drive to build a fast, easy to use, enterprise-capable database means ongoing work to both optimize query performance and to ensure comprehensive security capabilities. In SingleStore DB 6.8, we introduced two key improvements: improved query performance and advanced security for Hadoop environments. Improved Query Performance We optimized our query compilation feature set to deliver what we call interpret-first query compilation. This innovative feature automatically speeds up first-run queries commonly used in ad hoc analytics environments, or when one-off queries are required. We see a 5x speedup of CUBE and ROLLUP queries with the new optimizations. ROLLUP queries let you calculate subtotals and grand totals for a set of columns, while CUBE queries let you calculate subtotals and grand totals for all permutations of the columns specified in the query. We also saw a nearly 5x performance improvement for a range of JOIN queries. You can learn more about our query performance improvements on our blog post here . Advanced Security for Hadoop Environments SingleStore has had pipeline support for HDFS since SingleStore DB 6.5. HDFS pipeline support allows SingleStore to quickly and easily ingest data from Hadoop environments, with SingleStore providing faster ANSI SQL query response, leveraging its power as a fully distributed database. Now, with SingleStore DB 6.8, we have added Kerberos support for HDFS pipelines, along with wire encryption for over-the-wire data delivery, to provide support for security standards which are commonly used in Hadoop deployments. You can read more about our HDFS security improvements at our blog post here . To learn more about SingleStore and our improvements with SingleStore DB 6.8, please join our upcoming live webinar . You can also sign up for our benchmarking webinar . Or, you can get started with SingleStore for free today.", "date": "2019-05-14"},
{"website": "Single-Store", "title": "68-secure-hdfs", "author": ["Jacky Liang"], "link": "https://www.singlestore.com/blog/68-secure-hdfs/", "abstract": "Many companies, including our customers, have invested heavily in Hadoop infrastructure. In a recent blog post , we explored the topic of hype when it came to enterprises deploying Hadoop across their organizations, and ultimately where Hadoop falls short for certain use cases. Using SingleStore, many of our customers have been able to augment Hadoop using our HDFS pipelines feature, allowing them to quickly ingest from the Hadoop Distributed File System (HDFS) and perform analysis of their data in real time. With SingleStore DB 6.8, we are happy to announce our support for Kerberos and wire encryption for HDFS pipelines. Kerberos is a widely used method for authenticating users, including users of Hadoop clusters. Similarly, wire encryption protects data as it moves through Hadoop. Combining Kerberos and wire encryption in Hadoop is the standard in enterprises demanding the highest level of security. In SingleStore DB 6.8, with the release of Kerberos and wire encryption for HDFS, customers now get comprehensive security through full standards-based authentication and over-the-wire data delivery.", "date": "2019-05-14"},
{"website": "Single-Store", "title": "case-study-kurtosys-why-would-i-store-my-data-in-more-than-one-database", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/case-study-kurtosys-why-would-i-store-my-data-in-more-than-one-database/", "abstract": "One of SingleStore’s strengths is speeding up analytics, often replacing NoSQL databases to provide faster performance. Kurtosys, a market-leading, digital experience platform for the financial services industry, uses SingleStore exclusively, gaining far faster performance and easier management across transactions and analytics. Kurtosys is a leader in the digital experience category, with the first truly SaaS platform for the financial services industry. In pursuing its goals, Kurtosys became an early adopter of SingleStore. Today, SingleStore is helping to power Kurtosys’ growth. Stephen Perry, head of data at Kurtosys, summed up the first round of efforts in a blog post several years ago, titled \"Why Would I Store My Data In More Than One Database?\" (Among his accomplishments, Steve is one of the first SingleStore-certified developers .) In the following blog post, we describe how usage of SingleStore has progressed at Kurtosys. In the first round, Kurtosys had difficulties with its original platform using Couchbase. The company moved to SingleStore, achieving numerous benefits. Further customer requests, and the emergence of new features in SingleStore, opened the door for Kurtosys to create a new platform, which is used by Kurtosys’ customers to revolutionize the way they deliver outstanding digital and document experiences to their sales teams and to their external communities of clients and prospects. In this new platform, SingleStore is the database of record. At Kurtosys, Infrastructure Powers Growth Kurtosys has taken on a challenging task: hosting complex financial data, documents, websites, and content for the financial services industry. Kurtosys’ customers use the Kurtosys platform for their own customer data, as well as for their sales and marketing efforts. The customer list for Kurtosys features many top tier firms, including Bank of America, the Bank of Montreal, Generali Investments, and T. Rowe Price. Kurtosys’ customers require high performance and high levels of security. Customer focus on security is greater in financial services than in most other business segments. A single breach – even a potential breach that is reported, but never actually exploited – can cause severe financial and reputation damage to a company. So customers hold technology suppliers such as Kurtosys to very high standards. Alongside security, performance is another critical element. Financial services companies claim performance advantages to gain new customers, so suppliers have to deliver reliably and at top speed. Since financial services companies also differentiate themselves on customer service, they require suppliers to provide excellent customer service in turn. (Like Kurtosys, SingleStore is well-versed in these challenges. Financial services is one of our leading market segment , with half of the top 10 North America financial services firms being SingleStore customers.) With all of these strict requirements for financial services companies to trust an external provider to host their content – including such crucial content as customer financial data – it is a major step. Yet, Kurtosys has met the challenge and is growing quickly. “Our unique selling proposition is based around the creative use of new and unique technology,” says Steve. “We’ve progressed so far that our original internal platform with SingleStore, which we launched four years ago, is now a legacy product. Our current platform employs a very modern approach to storing data. We are using SingleStore as the primary database for the Kurtosys platform.” Kurtosys Chooses Infrastructure for Growth Kurtosys is adept at innovating its infrastructure to power services for demanding customers. For instance, several years ago, Kurtosys used SQL Server to execute transactions and Couchbase as a high-performance, scalable, read-only cache for analytics. Initially, the combination made sense. Customers of Kurtosys wanted to see the company executing transactions on a database that’s among a handful of well-established transactional databases. SQL Server fit the bill. However, like other traditional relational databases, SQL Server is, at its core, limited by its dependence on a single core update process . This dependency prevents SQL Server, and other traditional relational databases, from being able to scale out across multiple, affordable servers. This means that the single machine running SQL Server is usually fully occupied with transaction processing and would struggle to meet Kurtosys’ requirements, such as the need for ad hoc queries against both structured and semi-structured data. That left Kurtosys needing to copy data to another system (initially Couchbase) and run analytics off that – the usual logic for purchasing a data warehouse or an operational analytics database . Couchbase seemed to be a logical choice. It’s considered a leading NoSQL database, and is often compared to other well-known NoSQL offerings, such as Apache Cassandra, Apache HBase, CouchDB, MongoDB, and Redis. Couchbase tells its target audience that it offers developers the opportunity to “build brilliant customer experiences.” NoSQL databases have the ability to scale out that traditional relational databases lack. However, NoSQL databases face fundamental limitations in delivering on promises such as those made by Couchbase. NoSQL databases favor unstructured or less-structured data. As the name implies, they don’t support SQL. Users of these databases don’t benefit from decades of research and experience in performing complex operations on structured and, increasingly, semi-structured data using SQL. With no SQL support , Couchbase can be difficult to work with, and requires people to learn new skills. Running against unstructured data and semi-structured JSON data, and without the benefit of SQL, Kurtosys found it challenging to come up with an efficient query pattern that worked across different data sets. Kurtosys Moves to SingleStore to Power Fast Analytics As a big data database, Couchbase works well for data scientists running analytics projects. However, for day in and day out analytics use, Kurtosys had difficulty with writing queries, and query performance was subpar. Couchbase was not as well suited for the workloads and high degree of concurrency – that is, large numbers of simultaneous users – required for Kurtosys’ internal user and customer analytics support, including ad hoc SQL queries, business intelligence tools, and app support. At the same time, Kurtosys needed to stay on SQL Server for transactions. Kurtosys had invested a lot in SQL Server-specific stored procedures. Its customers also liked the fact that Kurtosys uses one of the top few best-known relational databases for transactions. So, after much research, Kurtosys selected a fully distributed database which, at the time, ran in-memory: SingleStore. Because SingleStore is also a true relational database, and supports the MySQL wire protocol, Kurtosys was able to use the change data capture (CDC) process built into SQL Server to keep SingleStore’s copy of the data up to date. SingleStore received updates a few seconds after each transaction completed in SQL Server. Queries then ran against SingleStore, allowing both updates and queries to run fast against the respective databases. In the original platform, updates ran against SQL Server.CDC moved updates to SingleStore, which supported queries. SQL Server was now fully dedicated to transaction support, with the CDC process imposing little overhead on processing. And, because of SingleStore’s speed, the database was able to easily keep up with the large and growing transaction volume going through the Kurtosys platform. Kurtosys summed up its approach at the time in a slide deck that’s available within a Kurtosys blog post. The key summary slide is below. SingleStore-Based Platform Powers New Applications Kurtosys has now created a new internal platform. One of the key capabilities in the new platform is support for JSON data. In a recent SingleStore release, SingleStore DB 6.7, JSON data support is a core feature. In fact, comparing JSON data to fully structured data, “performance is about the same, which is a testament to SingleStore,” says Steve. With this capability, Kurtosys can keep many of the same data structures that it had previously used in Couchbase, but with outstanding performance. Also, when Kurtosys first adopted SingleStore, several years ago, SingleStore was largely used as an in-memory database. This gave truly breakthrough performance, but with accompanying higher costs. Today, SingleStore flexibly supports both rowstore tables in memory and disk-based columnstore. “Performance,” says Steve, “is almost too good to believe.” The new platform runs SingleStore for both transactions and queries. In the new platform, there’s no longer a need for CDC. Kurtosys runs SingleStore as a transactional database, handling both transactions and analytics. In the new platform, updates and queries all run SingleStore. The new internal platform powers Kurtosys applications with thousands of concurrent users, accessing hundreds of gigabytes of data, and with a database growing by several gigabytes of data a day. Kurtosys is looking forward to using the new features of SingleStore to power the growth of its platform. As Steve Perry said in a separate blog post, “What they do, they do right… we use SingleStore to improve the performance of query response.” Stepping Ahead with SingleStore SingleStore is a fundamental component of the key value proposition that Kurtosys offers its customers – and cutting-edge platforms, such as the one being developed at Kurtosys today, will continue to push SingleStore forward. To see the benefits of SingleStore for yourself, you can try SingleStore today for free. Or, contact us to speak with a technical professional who can describe how SingleStore can help you achieve your goals.", "date": "2019-05-30"},
{"website": "Single-Store", "title": "memsqls-columnstore-customers-competitive", "author": ["Jacky Liang"], "link": "https://www.singlestore.com/blog/memsqls-columnstore-customers-competitive/", "abstract": "A columnstore database takes all the values in a given column – the zip code column in a customer database, for instance – and stores all the zip code values in a single row, with the column number as the first entry. So the start of a columnstore database’s ZIP code record might look like this: 5, 94063, 20474, 38654… The “5” at the beginning means that the ZIP code data is stored in the fifth column in the rowstore database of customer names and addresses that the original data comes from. Columnstore databases make it fast and easy to execute reporting and querying functions. For instance, you can easily count how many customers you have living in each US zip code – or combine your customer data with a zip code marketing database. SingleStore combines rowstore and columnstore data tables in a single, scalable, powerful database that features native SQL support. (See our blog post comparing rowstore and columnstore .) And, in addition to its fast-growing, paid enterprise offering, SingleStore also has a highly capable free option. You can use SingleStore for free , with community support from our busy message board . SingleStore is free up to the point where you reach four nodes, or four separate server instances, with up to 32GB of RAM each – 128GB of RAM total. This large, free capacity is particularly useful for columnstore tables, where 128GB of RAM is likely to be enough to support a terabyte or so of data on disk, with excellent performance. We have several existing customers doing important work using SingleStore for free. And when you need more nodes, or paid support, simply contact SingleStore to move to an enterprise license. Why Use Columnstore? The columnstore is used primarily for analytical applications where the queries mainly involve aggregations over datasets that are too large to fit in memory. In these cases, the columnstore performs much better than the rowstore. A column-oriented store, or “columnstore,” treats each column as a unit and stores segments of data for each column together in the same physical location. This enables two important capabilities. The first is to scan each column individually – in essence, being able to scan only the columns needed for the query, with good cache locality during the scan. These features of columnstore get you excellent performance and low resource utilization – an important factor in the cloud, particularly, where every additional operational step adds to your cloud services bill. The other capability is that columnstores lend themselves well to compression. For example, repeating and similar values can easily be compressed together. SingleStore compresses data up to about 90% in many cases, with very fast compression and decompression as needed. As with the data design of columnstore tables, compression delivers cache locality, excellent performance, low resource utilization, and cost savings. In summary, you should use a columnstore database if you need great analytics performance. It also helps that SingleStore, as a scalable SQL database with built-in support for the MySQL wire protocol , natively supports popular analytic tools like Tableau, Looker, and Zoomdata. A big advantage with SingleStore is that you get both rowstore and columnstore tables in a single database, with built-in SQL support. This gives you a number of advantages: If you need to have rowstore table data duplicated and, in many cases, augmented in one or more columnstore tables, you can do this in a single database. You can run queries that join, or otherwise operate on, data spread across multiple rowstore and columnstore tables. You can make “game-time” price-performance decisions between storing your data in super-fast, in-memory rowstore tables vs. large, disk-based columnstore tables, then modify your decision as your business needs change. The training and experience you gain in using SingleStore for one use case extends automatically to many other use cases, whether rowstore or columnstore. For some of our customers, SingleStore is the last new database they’ll ever need – and ends up replacing one or more competing database options. You can also read more about the difference between rowstore and columnstore in our documentation. Some Current Columnstore Options There are several columnstore options out in the market. Here are a few of the more popular ones that we see. Note . Most of these options are not fully comparable to SingleStore because they don’t support both rowstore and columnstore, in-memory and disk-based tables, as SingleStore does. (See the list of benefits to this converged capability above.) However, you should consider a range of options before choosing any database provider, including SingleStore. ClickHouse ClickHouse is an open source columnstore database developed by Yandex specifically for online analytical processing (OLAP). ClickHouse allows for parallel processing of queries using multiple cores and very fast scanning of rows, while offering good data compression. However, there are disadvantages to using ClickHouse. There is no real DELETE/UPDATE support and no support for transactions. ClickHouse also uses its own query protocol, which means limited SQL support. This also means your favorite SQL tools may not be supported if you choose to use ClickHouse. Also, if you are migrating from a SQL database, you will likely have to re-write all your queries which have joins – a common operation in SQL. MariaDB Columnstore MariaDB is an open source fork of MySQL. This fork was done by Michael “Monty” Widenius, the author of MySQL, after Oracle purchased Sun Microsystems. MariaDB supports an open and vibrant community that has frequent updates and excellent community support. Additionally, MariaDB maintains high compatibility with MySQL, so it can be used as a drop-in replacement which supports library binary parity and exact matching with MySQL APIs. MariaDB also offers a columnstore engine for analytical use cases. However, since MariaDB only supports storing data on disk, if query speed and latency is a priority, then you may not be too happy with the performance. Additionally, MariaDB’s columnstore product is also still quite new, so there is likely to still have work to be done. Pivotal Greenplum Greenplum is a columnar data warehouse based on PostgreSQL. Greenplum uses massively parallel processing (MPP) techniques, with each database cluster containing different nodes, such as the master node and the segment node. This allows for parallel processing of queries and storage of data. Greenplum is also fully SQL-compliant and ACID-compliant. Finally, unlike most columnstore databases – but like SingleStore – Greenplum also supports both row and columnstore data storage. However, customers sometimes complain about the performance and usability of Greenplum. Many customers found the product to be difficult to tune, as Greenplum tends to use all the available system resources for every single query, which can lead to performance degradation when multiple queries are executed at the same time. Also, under high write load conditions, Greenplum would cause something called a Linux journaling error. Errors of this type may require rebuilding the entire database, which might take many hours to complete. SAP HANA HANA is an entirely in-memory columnstore database developed by SAP. A major strength with SAP HANA is that it’s built as a data platform — there are multiple “engines” that sit inside HANA columnstore. There are specialty engines built for calculations, spatial use cases, predictive algorithms, and more, allowing users to pick and choose the right engine for their specific use case without having to use materialized views. However, common complaints among SAP HANA users are the specialized skills one may need to work with the product. Furthermore, since SAP HANA is entirely in-memory-only with no disk component, it can get fairly expensive just for the RAM to contain all your data. Finally, the licensing costs of SAP HANA can get fairly high as well. Where SingleStore Shines In November 2018, we launched SingleStore DB 6.7 . With SingleStore DB 6.7, as well as later SingleStore releases, you can use SingleStore for free , within fairly robust limits. When using SingleStore for free, you can create clusters with up to four nodes, with no limit on the amount of data stored on disk. You also receive community support via online forums, rather than direct, paid support. Since launching SingleStore DB 6.7, we have been listening to how people using our software for free use SingleStore. Consistently in these conversations, our users – including those that run production workloads on the free tier – have consistently praised our columnstore. Purcado helps people get the best deals, on the best shoes,from the best retailers, quickly and easily. What makes it so good? You can use it for free up to four nodes with unlimited disk. But that’s not all — it also consistently outperforms other free, open source, and even enterprise-grade columnstore databases. This conclusion came from a number of people that have tested many rival databases – sometimes, even ten or more – before finally arriving at SingleStore. SingleStore has built-in support for ANSI SQL , so the query language is very familiar. We also support the MySQL wire protocol, meaning we support a wide range of tools in the data ecosystem. SingleStore offers incredible compression in disk-space columnstore, allowing you to store more data and save precious storage space at the same time. Real customers like Pandora are able to reliably achieve 85–90% on-disk compression for columnar data. SingleStore’s fully distributed nature means you can simply add affordable commodity hardware to increase query performance, concurrency, and ingest speed, and reduce data size . Finally, unique to SingleStore, the ability to combine rowstore and columnstore data in one query means you get the benefits of real-time and historical data unified in one query ! This means simplicity for your data engineering stack, lower maintenance costs, and improved performance, as SingleStore can, in many cases, be the one database to rule them all. Did we mention you can use all this for free ? What People Are Saying We can tout the benefits of SingleStore all we want, but we think it’s even better to let people who are using SingleStore for free do the talking for us. These are testimonials we have received directly from software developers and data engineers, company founders, and others using SingleStore to run their applications, answer their queries, and drive their businesses forward. Paul Moss, E-commerce Startup in the United Kingdom “I use SingleStore primarily for its columnstore. Your columnstore blows all of the free / open source columnstores in the market currently out of the water — it’s just so fast. PostgreSQL and CitusDB are inferior to your product. It’s not even close, especially since I’m running SingleStore on a single CentOS workstation machine. Additionally, as a business owner, you want the simplest engineering stack possible. SingleStore is one database to rule them all, replacing three to four different databases. It does it all well.” Hajime Sano, Nikkei in Tokyo, Japan “The performance of SingleStore free tier is just as good as the enterprise version, which means performance for each query is really fast, the fastest in the columnstore databases out there. That is the greatest thing. SingleStore also supports both rowstore and columnstore in one query. We’re now able to balance real-time query performance (in rowstore) with lower hardware cost (in columnstore). 24/7 operational data goes in in-memory, while archival data goes to disk.” Nikkei Asian Review, with scores of bureaus and more than 1000 journalists throughoutAsia, delivers both business-focused and general coverage across the region. Software Developer in Publishing Company in Germany “The incredible compression and speed of the columnstore engine really is something, querying gigabytes of data in seconds was amazing to see. Also the possibility of combining rowstore and columnstore in one query is a very nice feature.” Elad Levy, Entrepreneur in the Mobile Games Industry “SingleStore in particular has columnstore, which is free, and it’s amazing. If you want to analyze data and get business insight, just go with SingleStore’s columnstore. You also get the ability to mix and match transactions (OLTP) and analytics (OLAP) in a single query, which saves us from deploying and querying another database. It’s a 2-in-1 solution.” Peter Baylies, Purcado in Durham, NC “I appreciate SingleStore’s speed even on modest, single-box hardware, as well as its storage efficiency on disk.” Next Steps Don’t take our word for it — you can find out yourselves why our customers are saying such positive things about our columnstore and choose to run their businesses on SingleStore, both for free and paid, with support. We have a tutorial on loading data into SingleStore and a webinar for building an analytics app using SingleStore’s columnstore. These resources show just how fast and easy it is to set up and use SingleStore. To sum up, when using SingleStore for free, you can: Use up to 4 nodes, with no specific limit on disk storage Get rich community support at forums.singlestore.com Deploy to production Not face any time limits Want 24/7 support and even more nodes? You can contact us to begin the conversation.", "date": "2019-06-06"},
{"website": "Single-Store", "title": "video-modernizing-data-infrastructure-for-ai-and-machine-learning", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/video-modernizing-data-infrastructure-for-ai-and-machine-learning/", "abstract": "The AI Data Science Summit 2019 featured a keynote by SingleStore’s CEO, Nikita Shamgunov, where he was hosted by SingleStore partner Twingo . Nikita, a co-founder of SingleStore and the technical lead from the beginning, has shepherded SingleStore’s development toward a world where cloud , AI, and machine learning are leading trends in information technology. Now that these trends are becoming predominant, SingleStore is playing an increasing role, as Nikita discussed in his keynote. What follows is an abbreviated version of his presentation, which you can view in full here . – Ed. Today I want to talk about the demands of AI and machine learning data infrastructure. Certainly the promise is very big, right? I couldn’t be more excited about all the innovation that’s coming in retail, in health care, in transport and logistics. Investment into AI is very, very strong, with predictive analytics and customer analytics – areas where SingleStore is experiencing rapid and widespread adoption – as two of the top three planned uses for AI technology. However, the data challenges remain. Only 15% of the organizations have the right architecture and the right data infrastructure for AI, and only 8% of all systems are accessible to AI workflows. And we see this all the time. You walk into a major organization, data is siloed, it’s locked into databases, SaaS services, data warehouses, and more. As a data scientist, data management becomes kind of one of the first challenges that you need to solve, because your AI programs and your AI technology are only as good as the data that is flowing in. Databricks says the majority of AI projects have challenges moving from concept into production. What causes those delays? Typically, AI workflow is a multi-step workflow. SingleStore can simplify and accelerate a lot of the steps in the AI life cycle. SingleStore plugs into modern applications and plugs into modern workflows, such as AI workflows, a lot better than old school technology. It allows you to close the loop and automate the loop and remove a person looking at dashboards from the workflow and make the system completely automatic. And that’s what an operational system allows you to do, so you can go from analytics to pixels, from analytics into an app, almost instantaneously, with an automatic workflow. We are currently testing technology internally that reliably allows us to get responses to a specific set of query types in 2ms. We showed this to one customer, and they had a truly interesting response. Our customer said: “First, we actually don’t believe that you can do this. But if you can do it, we want it first.” We’re working with top U.S. banks on fraud prevention, which is another very, very typical example of using SingleStore. And fraud needs to be detected these days in real time. You swipe a credit card and you want to reject that credit card transaction in the transaction. And so, for that, you needed to have a very performant, very efficient data backbone. SingleStore is particularly well-suited for use in speeding up the workflows that are needed for all kinds of AI and machine learning applications. We jokingly call this a “markitecture diagram” – it shows the many ways that SingleStore brings together all the different strands of input and output, providing a fast, scalable, SQL database, which can ingest and store nearly any kind of data, for AI and machine learning programs to work against. We hope to work with many of you on AI and machine learning applications going forward. You can see my conference presentation here . For more information, please reach out. You can download and use SingleStore for free , up to certain fairly generous limits, with community support from the SingleStore Forums . For more ambitious uses of SingleStore, or to subscribe to our excellent paid support plans, contact SingleStore today .", "date": "2019-06-07"},
{"website": "Single-Store", "title": "managing-memsql-with-kubernetes", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/managing-memsql-with-kubernetes/", "abstract": "With the arrival of the cloud, organizations face new opportunities – and new challenges. Chief among them is how to take the greatest advantage of public and private cloud resources without being locked into a specific cloud or being barred from access to existing infrastructure. Container solutions such as Docker offer part of the solution, making it much easier to develop, deploy, and manage software. In our webinar , product manager Micah Bahti describes how to take advantage of the next step: Using Kubernetes, and the beta SingleStore Kubernetes Operator, to manage containers across public clouds and existing infrastructure. Until recently, however, Kubernetes didn’t manage stateful services. Recently, that support has been added, and SingleStore has stepped into a leading position. Ahead of other widely used database platforms, SingleStore has developed and made available a beta Kubernetes Operator. The Operator was announced at Red Hat Summit early in May. You can easily get the Operator, for either Open Shift or open source Kubernetes. Note: The SingleStore Kubernetes operator is currently experimental, and in beta. It will reach general availability in the coming months. You can also use the beta Operator with SingleStore on small deployments, including free instances of SingleStore . It scales smoothly to large databases as well; SingleStore scales to databases in the petabytes. Deploying and Installing Kubernetes for SingleStore Deploying and installing Kubernetes for SingleStore is very similar to using Kubernetes with other, stateless software. First, find the needed components. They’re available in the OpenShift container catalog and on Docker Hub . To start deployment, load the image of the SingleStore Kubernetes Operator and the configuration files into your Kubernetes cluster. Then, edit the YAML file, memsql-cluster.yaml, to define the attributes of your cluster. The most important is the size of the cluster, in gigabytes. One of the advantages of Kubernetes is that it’s very easy to change this later, quickly and at no cost. For other attributes, the minimum configuration for production should be: 1 leaf unit @ height 1 3 aggregator units @ height 0.5 redundancyLevel = 2 Note : You can’t downsize a cluster below the amount of data in it. For instance, if you create a 2GB cluster, then put 1.1GB of data in it, telling Kubernetes to downsize the cluster to 1GB will result in an error message. Finally, create the cluster, and manage it using kubectl. You can connect to the cluster with mysql. You will find support for all this in the SingleStore documentation and the SingleStore Forums . Reach out to us in the Kubernetes forum or by email at team@singlestore.com . Benefits of Using SingleStore with Kubernetes Because SingleStore offers fast, scalable SQL, the combination of the SingleStore database and the Kubernetes Operator gives you the ability to use a single relational database for transactions and analytics, without the need to move data. SingleStore easily ingests data from a range of sources and supports analytics platforms such as Looker, PowerBI, and Tableau. When you use SingleStore with Kubernetes, you get complete freedom to deploy or redeploy across physical or cloud infrastructure, as needed. Installation and deployment take minutes, not days or weeks; scaling is elastic; upgrades happen smoothly online. You can upgrade supporting hardware or software, with no effect on your SingleStore cluster. For more details about the Operator, see our initial announcement . And, if you are not yet a SingleStore user, you can try SingleStore for free today, or contact us to learn how we can help you.", "date": "2019-05-16"},
{"website": "Single-Store", "title": "webinar-benchmark-breakthrough", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/webinar-benchmark-breakthrough/", "abstract": "SingleStore has reached a benchmarking breakthrough: the ability to run three very different database benchmarks, fast, on a single, scalable database. The leading transactions benchmark, TPC-C, and analytics benchmarks, TPC-H and TPC-DS, don’t usually run on the same scale-out database at all. But SingleStore runs transactional and analytical workloads simultaneously, on the same data, and with excellent performance. As we describe in this webinar write-up, our benchmarking breakthrough demonstrates this unusual, and valuable, set of capabilities. You can also read a detailed description of the benchmarks and view the recorded webinar . SingleStore stands out because it is a relational database, with native SQL support – like legacy relational databases – but also fully distributed, horizontally scalable simply by adding additional servers, like NoSQL databases. This kind of capability – called NewSQL, translytical, HTAP, or HOAP – is becoming more and more highly valued for its power and flexibility. It’s especially useful for a new category of workloads called operational analytics , where live, up-to-date data is streamed into a data store to drive real-time decision-making. The webinar was presented by two experienced SingleStore pros: Eric Hanson, principal product manager, and Nick Kline, director of engineering. Both were directly involved in the benchmarking effort. SingleStore and Transaction Performance – TPC-C The first section of the webinar was delivered by Eric Hanson. The first benchmark we tested was TPC-C, which tests transaction throughput against various data sets. This benchmark uses two newer SingleStore capabilities: SELECT FOR UPDATE, added in our SingleStore DB 6.7 release . Fast synchronous replication and durability for fast synchronous operations, part of our upcoming SingleStore DB 7.0 release. (The relevant SingleStore DB 7.0 beta is available.) To demonstrate what SingleStore can do in production, we disabled rate limiting and used asynchronous durability. This gives a realistic aspect to the results, but it means that they can’t be compared directly to certified TPC-C results. These results showed high sync replication performance, with excellent transaction rates, and near-linear scaling of performance as additional servers are added. For transaction processing, SingleStore delivers speed, scalability, simplicity, and both serializability and high availability (HA) to whatever extent needed. SingleStore and Analytics Performance – TPC-H and TPC-DS The second section of the webinar was delivered by Nick Kline. Data warehousing benchmarks use a scale factor of 10TB of data at a time. SingleStore is very unusual in being able to handle both fast transactions, as shown by the TPC-C results, and fast analytics, as shown by these TCP-H and TPC-DS results – on the same data, at the same time. SingleStore is now being optimized, release to release, in both areas at once. Query optimization is an ongoing effort, with increasingly positive results. Nick described, in some detail, how two queries from the TPC-H benchmark get processed through the query optimizer and executed. The breakdown for one query, TPC-H Query 3, is shown here. The TPC-DS benchmark is somewhat of an updated and more complex version of the TPC-H benchmark alluded to above. In fact, it’s so challenging that many databases – even those optimized for analytics, can’t run it effectively, or can’t run some of the queries. SingleStore can run all the queries for both TPC-H and TPC-DS, as well as for TPC-C, and all with good results. For TPC-H, smaller numbers are better. SingleStore was able to achieve excellent results on TPC-H with a relatively moderate hardware budget. Results for TPC-DS were also very good. Because queries on TPC-DS vary greatly in their complexity, query results vary between very short and very long result times. As a result, the geometric mean is commonly used to express the results. We compared SingleStore to several existing published results. Smaller is better. Q&As for the SingleStore Benchmarks Webinar The Q&A was shared between Eric and Nick. Also, these Q&As are paraphrased; for the more detailed, verbatim version, view the recorded webinar . Both speakers also referred to our detailed benchmarking blog post . Q. Does SingleStore get used for these purposes in production? A. (Hanson) Yes. One example is a wealth management application at a top 10 US bank, running in real-time. Other examples include gaming consoles and IoT implementations in the energy industry. Q. Should we use SingleStore for data warehousing applications, operational database needs, or both? A. (Hanson) Our benchmarking results show that SingleStore is excellent across a range of applications. However, SingleStore is truly exceptional for operational analytics, which combines aspects of both. So we find that many of our customers begin their usage of SingleStore in this area, then extend it to aspects of data warehousing on the one hand, transactions on the other, and merged operations. Q. How do we decide whether to use rowstore or columnstore? A. (Kline) Rowstore tables fit entirely in memory and are best suited to transactions, though they get used for analytics as well. For rowstore, you have to spec the implementation so it has enough memory for the entire application. Columnstore also does transactions, somewhat more slowly, and is disk-based, though SingleStore still does much of its work in memory. And columnstore is the default choice for analytics at scale. (Also, see our rowstore vs. columnstore blog post . – Ed.) Q. How do you get the performance you do? A. (Hanson) There’s a lot to say here, but I can mention a few highlights. Our in-memory data tables are very fast. We compile queries to machine code, and we also work against compressed data, without the need to decompress it first – this can cut out 90% of the time that would otherwise be needed to, for instance, scan a record. We have super high performance for both transactions and analytics against rowstore. For columnstore, we use vectorized query execution. Since the early 2000s, there’s a new approach, in which you process not single rows, but thousands of rows at a time. So for filtering a column, as an example, we do it 4000 rows at a time, in tight loops. Finally, we use single instruction, multiple data (SIMD) instructions as part of parallelizing operations. Conclusion To learn more about SingleStore and the improvements in SingleStore DB 6.8, view the recorded webinar . You can also read the benchmarking blog post and view the benchmarking webinar . Also, you can get started with SingleStore for free today.", "date": "2019-05-30"},
{"website": "Single-Store", "title": "webinar-delivering-operational-analytics", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/webinar-delivering-operational-analytics/", "abstract": "SingleStore product marketing leader Mike Boyarski led the webinar, describing how SingleStore is well-positioned to power operational analytics . Operational analytics is the ongoing use of live and historical data to drive decision-making by both people and programs, including predictive analytics, machine learning, and AI. To do operational analytics you need to quickly ingest, analyze, and act on both incoming and existing data – all of which is in the wheelhouse of SingleStore. To view the webinar, click here . Traditionally, companies have their most important operational data stuck in silos. They can’t access it quickly or easily, and they can’t meet service level agreements (SLAs) for data delivery and data access. To solve these problems, businesses that need to move fast use SingleStore. This includes half of the top 19 banks, two of the top three telcos, tech leaders – from Akamai to Uber – and many others. Why Operational Analytics is Vital The demands that organizations make on their data are growing. Data volume and complexity are rising; business expectations are growing, and analytics are evolving to keep pace. Whereas reports and occasional queries from experts were once considered enough, today, businesses want to use real-time data to power predictive analytics, machine learning, and AI. Today’s systems either struggle to keep up, or don’t even try. Their responsiveness from event to insight may not meet SLAs – and often, the SLAs themselves are not enough to keep pace with new competitors. Costs and complexity continue to increase, and demands for access – from people with SQL queries, from SQL-compatible business intelligence (BI) programs, from management dashboards, and from programs that power predictive analytics, machine learning, and AI, are all rising. These requirements are now table stakes for organizations to be competitive, as digital native companies win more and more slices of the economic pie. In order to power analytics, organizations need to ingest, analyze, and act on blended real-time and historical data. SingleStore’s capabilities make it capable of meeting this challenge, where legacy databases fall short. A New Architecture – with SingleStore at the Core SingleStore has the relational database capabilities needed to handle structured data for both transactions and analytics, the scalability to grow to meet demand for ingest, analysis, concurrency, and action, and the flexibility to handle semi-structured JSON data and full-text search for unstructured data. SingleStore also runs on-premises and in multiple clouds, in containers and virtual machines, and with a new Kubernetes Operator for open source or Red Hat Open Shift Kubernetes distributions, making for a truly cloud-native option that lives where you need it to. SingleStore can ingest from a wide range of sources, including change data capture (CDC), at very high rates of speed; works with relational data, key-value data, JSON semi-structured data, geospatial data, and time series data . SingleStore handles data from systems of engagement (SOEs), such as social media, internet of things (IoT) data, and mobile phone data, including the full range of supported formats,with excellent performance. On the other side of the data store, analytics demands include lookups, aggregates, ad hoc queries, machine learning (ML), and artificial intelligence (AI). This wide range of demands, including many more users wanting direct query access, drives a strong need for increased concurrency. SingleStore can live right at the core of a modern data infrastructure, handling both transactions and analytics. You can augment existing systems or replace them at each stage of your infrastructure. SingleStore Meets Operational Requirements Companies from Comcast and Uber find SingleStore vital to meeting their operational analytics requirements. SingleStore excels vs. competitors in meeting the needs of operational data workloads. The leading operational analytics competitors include: MongoDB . Mongo is fully modern in its structure, and flexible in its deployment options. But performance for ingest, transactions, and analytics fall far behind SingleStore, and even lag other competitors. Oracle Exadata . Oracle’s Exadata database machine is legacy, rather than modern; inflexible; hard to manage and keep available; and very expensive. Amazon Aurora . Aurora is not modern, making it hard to match SingleStore or competitors in key areas of performance and flexibility. Q&A Mike took questions from the audience, including: Q. How do you write custom code to apply to ingested data? A. You can create stored procedures to use all kinds of code as part of the SingleStore Pipelines feature, Pipelines to stored procedures. Q. Are you an in-memory database? A. SingleStore started out as an in-memory database, using rowstore only. We’ve now added a robust columnstore capability that is used by many of our customers. Though columnstore is disk-based, many customers have been pleasantly surprised by its performance and functionality. Q. Do you support time series data? A. SingleStore has strong support for many time series capabilities. However, SingleStore does not have all of the functionality of a specialized time series database out of the box. We are doing work internally on this, and a number of SingleStore customers are using us for time series data today. Please contact us to find out how we handle time series workloads. Q. Can SingleStore be deployed on AWS, GCP, and Azure? A. Yes! We have customers on each of these platforms. Also, SingleStore’s Kubernetes Operator makes it easy to manage SingleStore on these platforms, as well as on-premises. Q. Has SingleStore been used to replace Exadata? What about Oracle’s version of SQL? A. Yes, a number of well-known customers have made this move. We give you all the performance you need on a modern infrastructure, at a much lower TCO – often 3x or less than Oracle. For Oracle’s PLSQL, we have a migration path and partners to help move up to thousands of stored procedures to SingleStore’s own language. (Which includes many PLSQL-friendly features.) Conclusion To learn more about SingleStore and how it can help you deliver operational analytics, view the recorded webinar . You can also get started with SingleStore for free today.", "date": "2019-06-26"},
{"website": "Single-Store", "title": "memsql-forums-announces-first-community-star", "author": ["Jacky Liang"], "link": "https://www.singlestore.com/blog/memsql-forums-announces-first-community-star/", "abstract": "SingleStore Forums are available anytime you have a question about SingleStore, want to know more about SingleStore, or are willing and able to help others. And now the SingleStore Forums , coming up on their one-year anniversary, have their first Community Star : Ziv Meidav, a big data architect who is serving as senior principal engineer at cybersecurity innovators Palo Alto Networks. The community of SingleStore users, developers, and employees gathers at the SingleStore Forums to ask questions, offer support, and discuss strategies for leveraging SingleStore. Got a nasty error message? Post it on a Forum. Want to suggest a feature? Same thing. What the SingleStore Forums Do The SingleStore Forums are organized into topics. Topics include: Announcements . All the latest from SingleStore and partners. Feature Requests . What you want to see in SingleStore going forward. Cluster Operations . Managing your SingleStore clusters. Documentation Feedback . Suggest improvements to SingleStore’s well-regarded documentation . SingleStore Studio . Get your questions answered about SingleStore’s cluster monitoring and debugging tool. SingleStore Management Tools . Ask about tools in the SingleStore Toolbox package and SingleStore Ops. Third-Party Integrations . Using SingleStore with third-party tools and frameworks that work with SingleStore, such as Kafka and Spark. SingleStore Development . If you’re developing against SingleStore, put your questions and comments here. Uncategorized . A catchall category. Site Feedback . Tell us how to improve the Forums. The great thing about the Forums is the mix of people aboard. SingleStore engineers, experienced SingleStore users from organizations large and small, new users, and students all come on, asking and answering questions, from the simplest to the most arcane. Everyone pitches in to help. The Forums are particularly valuable to users of SingleStore who don’t have an Enterprise license, but instead run SingleStore for free . This works on-premises as well as in the cloud, up to certain limitations as to nodes and resources used. For such users, the Forums are the only source of support. And users of SingleStore’s free level are regular contributors to the Forums. One of the things that makes it fun to create and use SingleStore is that it is, fairly literally, a computer science project. Building and using “the next great database,” which is our goal, touches on all sorts of problems. They range from the momentary and entirely practical – “my cluster just crashed” – to the architectural, mathematical, and sometimes almost philosophical. It all gets hashed out on the Forums. Ziv to the Rescue Like many SingleStore users and fans, Ziv asks penetrating questions on the SingleStore Forums. Ziv’s Forums activity shows comments on columnstore performance for JSON data, a question on SingleStore Studio, and a group of comments on a performance issue – which turns out to be a fairly deep topic. It all started with a SELECT statement that had varying execution speeds, ranging from 40ms to 500ms, when run on the SingleStore DB 7.0 beta . Ziv posted extensive details, and SingleStore’s Haoran Xu chimed in with an initial response. This was the beginning of a long exchange that eventually totaled 17 pages over 5 days. It ended with Haoran recognizing a bug in the beta, described in detail in his final message in the exchange. SingleStore found and fixed the cause of the bug immediately, and the new and improved code will appear in SingleStore DB 7.0 soon. Ziv Meidav’s focused, specific question helped uncover a bug in the SingleStore DB 7.0 beta. Another strong contribution is a question about optimizing a specific query, also from Ziv. The exchange generated by this question yields considerable insight into how SingleStore handles many different kinds of queries and how to optimize them. As you see here, the Forums, with questions like Zivs’, help SingleStore people as well as customers. Through the Forums, we learn about bugs, documentation issues, feature requests, and more. Software work can be very individual; through the Forums, SingleStore people and customers from around the world can work together to make SingleStore better. Conclusion The SingleStore Forums are a great resource, community, and unofficial school for SingleStore users and employees to teach and learn from each other, while solving problems and minimizing downtime. They’re available to anyone who has a free account from the SingleStore Customer Portal or signs up with a valid email. And if you participate in the forums, perhaps you’ll get chosen next month as our next Community Star. There is, of course, an easy way to get started with SingleStore. Download SingleStore for free and start using the SingleStore Forums today!", "date": "2019-07-13"},
{"website": "Single-Store", "title": "case-study-katoni-migrates-from-elasticsearch-to-memsql", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/case-study-katoni-migrates-from-elasticsearch-to-memsql/", "abstract": "Since SingleStore became free to use last November – for up to four nodes, and with community support – new, creative uses of SingleStore have abounded. One of the most impressive new implementations is from Katoni , an ecommerce hub that offers SEO tools via software as a service (SaaS) to mostly Scandinavian clients. Katoni has replaced Elasticsearch with SingleStore for SingleStore’s ability to handle complex queries with speed, scalability, and native SQL support. SingleStore is currently the primary database powering Katoni’s SaaS suite of SEO tools. SingleStore runs alongside PostgreSQL, which serves as a secondary database for specific tasks such as projects, users, and billing. Katoni runs a popular e-commerce portal and an SEO offering delivered as SaaS. Moving to SingleStore Like many companies, Katomi was originally running on other technologies, then moved to SingleStore as problems such as slow query performance dogged their efforts. According to Martin Skovvang, software engineer at Katoni, “We started out with Elasticsearch, but soon needed a replacement as our queries became more complex. Especially, the JOINs, subselects, and HAVING features provided the major benefit of moving to SingleStore, along with the transaction siupport, scalability, and ease of use.” Katoni uses a combination of rowstore and columnstore tables . They are excited about some upcoming SingleStore features that promise to combine the best of both. Getting SingleStore for free, while they grow their SaaS business, is crucial to Katomi’s success. As they hit their business milestones in SaaS, they expect to move to a paid subscription. “Impressive Stability” Katomi runs SingleStore in Google Cloud Platform, using three VMs. They collect millions of rows of data a day into tables with tens of millions of rows in rowstore, using several tens of gigabytes of memory, and more than a billion rows in columnstore, consuming disk storage of an additional tens of gigabytes. Katomi describes SingleStore’s stability as “impressive.” According to Skovvang, “What usually worries me most about managing databases? Two things: backups and crash recovery.” With other databases, crashes required involving the vendor’s support team, costing Katoni hours of downtime. With SingleStore, the recovery process is almost fully automatic; they never need to involve support. Katomi does have a short wishlist for SingleStore. The interpret_first feature, which was offered as an option in SingleStore DB 6.7, then turned on by default in SingleStore DB 6.8, met one wish. Katomi are also looking for a native UUID data type, unique indexes in columnstore, and enhanced support for foreign keys. Schema design options will open up if SingleStore can shard on non-PRIMARY keys. Enhanced multi-language support for certain full text indexes will help as well. Most of the features Katoni is looking for are either already planned, or under active discussion for SingleStore’s development road map. Conclusion Many SingleStore customers get traction running SingleStore for free, with the option of moving to a paid subscription as their needs grow. You can ask about SingleStore on the SingleStore Forums , download and run SingleStore for free , or contact SingleStore Sales today.", "date": "2019-07-14"},
{"website": "Single-Store", "title": "forrester-finds-millions-in-savings-and-new-opportunities-in-digital-transformation-with-memsql", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/forrester-finds-millions-in-savings-and-new-opportunities-in-digital-transformation-with-memsql/", "abstract": "A new wave of digital transformation is in progress, and this new wave is powered by the exponential growth in the volume and complexity of data. To make data valuable it must be collected, stored, analyzed, and operationalized so as to drive value. Forrester has conducted a Total Economic Impact (TEI) analysis showing the savings and opportunities made possible for organizations moving to SingleStore. In order to put savings and benefits into context, Forrester conducted case studies with four SingleStore customers. These customers face many of the data infrastructure problems that prevent organizations from using their data effectively, including: Data in multiple silos Stale data Complex data architectures Scalability limited, with expensive and fragile efforts to manage scalability Poor performance for specific functions and across the board The result? Brittle, overly complex data processing systems which, in many cases, are starting to come crashing down. The Forrester TEI Methodology The four customers that Forrester studies all upgraded to SingleStore to solve specific problems, as described below. The four customers were in online services; professional services’ utilities; and online security services. Each customer has from one to several use cases for SingleStore running during the study period. Forrester then used their trademark TEI methodology , scaling the results to a representative composite organization with 15,000 employees and $3B in revenues. The results were impressive – $15M in cost savings and new benefits across several initiatives and new opportunities generated by improved flexibility, all within a three-year period. Forrester found $15M in cost savings and benefits in three years with SingleStore. SingleStore is The No-Limits Database™, offering a database solution that emphasizes three things: speed, with accelerated time to insight; scale, the ability to grow data management, and company operations, at low and stable costs; and SQL, support for the lingua franca that has powered business solutions for decades. Taking advantage of these capabilities, the specific companies that were studied, and the composite company that Forrester created for the analysis, experienced the benefits that SingleStore promises: No more missed SLAs. No more fragmented data architecture. No more “we can’t do that.” Customer Benefits with SingleStore The benefits achieved in the composite company include: Reducing legacy database costs . Lower software license fees and reduced hardware costs for running the software, saved $4.1M over three years. Avoiding fraud-related costs . The cost of detecting fraud dropped, and success in detecting fraud improved, with a net benefit of $2.2M over three years. Reducing hardware issues . The composite organization avoided 25 business-critical failures over three years for a savings over that time period of $5.4M. Improved employee productivity in analytics . Reporting and data analysis time dropped from many hours to minutes, for productivity gains of $2.4M over three years. Better decision-making . All across the business, managers take advantage of opportunities more quickly, with direct revenue benefits of nearly $1m in three years. Improved product and services quality . Employees have more and better data at hand for helping colleagues, partners, and customers, with benefits compounding over time. A composite organization, by taking steps based on Forrester’s case study analysis of four actual SingleStore customers, would experience benefits of $15M against costs of roughly $3.7M, for a net present value (NPV) of $11.3M and an ROI of nearly 300%. How Customers Grow SingleStore’s impact In working with customers, we find that the benefits of SingleStore compound in another, powerful way. Customers tend to start by adopting SingleStore, as a new database in their arsenal, for a limited, specific use case where the cost/benefit ratio is hugely favorable and crystal clear. Once they get hands-on experience with SingleStore, however, they come up with new ideas for how to use it more broadly. Fanatics, for instance, dramatically scaled their ambitions, scaling SingleStore up to use it as the core engine for their entire companywide, worldwide transaction capability. Similarly, a financial services company made a two-step move away from Oracle . They now run operational analytics on a combination of Kafka streaming and the SingleStore database. See the Benefits for Yourself To see the benefits for yourself, download the Forrester TEI Report . And for hands-on experience, download and run SingleStore for free .", "date": "2019-06-18"},
{"website": "Single-Store", "title": "memsql-customers-speak-up-on-reviews-site-g2-crowd", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/memsql-customers-speak-up-on-reviews-site-g2-crowd/", "abstract": "SingleStore is featured on business solutions review site G2 Crowd . Customers cite advantages like MySQL-friendliness, data compression of 60-80%, strong training, and excellent support – and that’s just in a single review. What G2 Crowd Does for Users G2 Crowd features frank reviews of business solutions – software and services – across a wide range of categories. For software, categories include: Sales . CRM software, sales acceleration, and a whole range of other sales solutions. Marketing . Tools for A/B testing, content marketing, demand generation, lead management, social media marketing, and more. Analytics . Business intelligence, predictive analytics, and statistical software are among the categories featured. AI . Platform software, chatbots, deep learning, and image recognition are key artificial intelligence (AI) categories. AR/VR . In augmented reality (AR) and virtual reality (VR), CAD software, content management, and game engines are among the offerings. B2B marketplace platforms . Merchants, catering, and on-demand delivery for grocery stores and restaurants are included. CAD & PLM . For computer-aided design (CAD) and product lifecycle management (PLM), categories include 3D design, computer-aided manufacturing, and geographic information systems (GIS). IT infrastructure . Cryptocurrency software, relational databases (such as SingleStore), NoSQL databases, key-value stores, and many other types of infrastructure software are covered. Additional categories are collaboration & productivity; content management; customer service; (software) development; digital advertising; e-commerce; ERP (enterprise resource planning); governance, risk & compliance; (content and software) hosting; HR; IoT; IT management; (general) office; security; supply chain & logistics; and vertical industry software. G2 Crowd has features designed to help. You can easily link from a product to its overarching category (such as Relational Databases Software , for SingleStore), making comparisons easy. You can interact with a G2 Advisor who will help you find the products that are worth your time and attention. It’s easy to share comments on social media. And you can, in many cases, ask a question of, or request a demo from the vendor. Using G2 Crowd for research can deliver a lot of benefits: Validation . To begin with, you can quickly see that a provider is “real,” with validated and verified users who speak to the plusses and minuses of a product. Strengths and weaknesses . Reviews tell you what a product has done for people, and where it has fallen short. Sales call preparation . If you’re going to interact with a vendor’s salespeople, G2 Crowd can give you a running start on what to ask about. Research completeness . You can find competitors to a product you’re considering and get a quick read on relative strengths and weaknesses. This can be very helpful in creating a shortlist for serious consideration, for example. Posting a review on G2 Crowd also helps you, as a current user of a product. By posting, you encourage the good features of the products you review to get developed further – and the bad features to get fixed. Your review is likely to elicit responses and comments, amplifying your feedback and possibly helping you work around concerns. And letting vendors know that you post on G2 Crowd may give your feedback more weight with them as they develop their products further. What the G2 Crowd Says About SingleStore So far, so good – at this writing, SingleStore has a star rating of 4.5 out of a possible 5 points on G2 Crowd. Among the key positive comments: Speed . Queries are reported to be very fast, for example. SQL and MySQL compatibility . SingleStore uses standard ANSI SQL and is compatible with MySQL wire protocol, making it very easy to drop into existing workflows and development efforts. Distributed database . SingleStore is fully distributed, which is unusual for a relational database and means you can scale SingleStore to meet a very wide range of needs. Specific use cases . Geolocation queries, high rates of compression for columnstore tables, and online transaction processing (OLTP) at high rates of throughput are mentioned. Affordable . Several users describe the licensing costs as reasonable, even “cheap.” Not all the reviewers on G2 Crowd write in perfect English, as the site attracts users from all over the world. But the enthusiasm behind the many positive comments comes through: “We mostly go after greater performance and scalability”; “Very scalable, fabulous, and very easy to use”; “You can seamlessly join across both row and columnar based tables.” One comment seems to sum up many of the positives holistically: “We’re now able to keep pace with increasing data volume and provide faster insights for our customers.” This SingleStore review includes positives like “fast queries\" that appear in other reviews as well. There are some recommendations for the company as well – an easier deployment process is recommended; better tools; built-in performance monitoring; making the product easier for less-savvy users. Here at SingleStore – as is likely to be true at other vendors who see concerns listed – these issues have been noted, and efforts to address the concerns are well underway. And there are tips. One financial services administrator recommends taking the time to get to know the product in some depth, and using Prometheus and Grafana for monitoring. Next Steps If you’re already a SingleStore user – whether you have an Enterprise license, or are using SingleStore for free – consider posting a review today. (And remember that you can get questions answered on the SingleStore Forums as well.) Your efforts will benefit the community as a whole. If you haven’t yet tried SingleStore, take a look at the reviews on G2 Crowd . Post questions as comments there, or on the SingleStore Forums . And consider trying SingleStore for free today.", "date": "2019-07-18"},
{"website": "Single-Store", "title": "case-study-scalable-sql-database-uber", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/case-study-scalable-sql-database-uber/", "abstract": "Uber is driven by analytics. The fastest-growing company ever, by some accounts, Uber uses up-to-date information to entice customers, direct drivers, and run their business on a database that provides answers with sub-second response times. In this presentation, James Burkhart of Uber explains how SingleStore has helped solve critical latency issues for the company at a critical point in its growth. We encourage you to review the key points listed here, then view the presentation from Strata + Hadoop World 2017 and review the slides on Slideshare. Four Challenges for Uber Analytics What is real-time analytics ? Real-time analytics is a special case of operational analytics . Real-time analytics means providing fast answers – usually in a second or less – to queries, and including recent information in the response. Real-time analytics often requires streaming data, as a fast way to get new data into the analytics system, and a highly responsive database, which combines new information with existing information and provides results very quickly. Uber needs to connect drivers and passengers in real time. It needs to add or remove surge pricing smoothly as demand changes throughout the day. Uber’s entire business model is based on real-time data. SingleStore helps Uber make real-world decisions based on analytical results that the company uses to take action on the go. Uber wanted to overcome four challenges: Business Intelligence Real-time analytics Time series aggregates Geospatial data Uber was looking at metrics for just about every important aspect of their business: riders, drivers, trips, financials, etc. So business intelligence is the first challenge. An example of an actionable metric could be something like a surge of business in a contained geographic area. The action to take in response is to physically reposition supply in the real world. They wanted this to be in real time, which is the next challenge is. Before using SingleStore, Uber was facing a p99 ingest latency somewhere between 1.5 seconds on the lower end of sources, and 3 minutes on the higher end, depending on the ingestion source. They needed consistent responsiveness in fractions of a second, to a few seconds at the maximum, for all sources. Another area of interest is aggregation of time series data. In their real-time analytics system, Uber does not store the entire history of all time series data; there are other systems to do this. In this system, they want business metrics, which have strong seasonality components and are looked at in that context. The real-time analytics system is not a source of truth, but a source for aggregated data. For example, a morning commute has very different marketplace dynamics than a Friday night out on the town. An example query for this purpose would be something like, “Hourly count of trips in San Francisco.” The whole system is designed and optimized for aggregated data, not individual records. Geospatial data is the final key point. Uber’s business is operating across the physical world, and they need to be able to provide granular geospatial slicing and dicing of data to help analysts understand various marketplace properties of some geo-temporal segment of data. Knowing that, somewhere in San Francisco, that the unfulfilled rate of requests went up at a particular point in time is not useful, as compared to understanding that, for example, near AT&T Stadium, when a San Francisco Giants game let out, there was a spike representing an increase in demand. The slide (available in this Uber presentation on Slideshare ) shows an example of all of these issues coming together. It shows aggregated demand for Uber rides on New Year’s Eve 2016 (orange dots) vs. the early hours of New Year’s Day 2017 (green dots). You can see where demand is and the hour of the day, all relative to the terminator – the line between one day to another, which sweeps across the globe every 24 hours. In the slide, the terminator is moving across the Arabian peninsula and eastern Europe, where it’s midnight, and heading toward very dense concentrations of Uber demand, as 2016 is ending and 2017 approaching, across Europe and key cities in Africa, such as Cairo, Egypt. Uber demand peaks in key cities on New Year’s Eve. Why Uber Needs A SQL Database for Analytics Apollo is Uber’s internal platform for real-time analytics. Uber stores only recent data, that’s about seven weeks’ worth. They have low latency on ingestion, usually measured in seconds, between data being logged and making that data available for query. Apollo supports ad hoc exploration of data, arbitrary drill down including geospatial filtering and geospatial dimensioning of data. Another key property is deduplication. Use of Kafka is pretty heavy at Uber. With Kafka deployment, it ends up getting an at-least-once delivery guarantee. But one of the flaws in the existing system, the one that Uber has replaced with Apollo, was that it would double count in a lot of scenarios involving hardware or software failures. They needed to be able to assert that a uniquely identifiable record exists exactly once , and would not be double-counted with this system. And they have to do all of this with low latency to the end user. SingleStore is at the core of the Apollo analytics system at Uber. So SingleStore is where Uber stores the data. They investigated some alternatives during the research and planning phase, but found that SingleStore serves their needs well. They find it to be super fast, that it supports ingestion at a rate significantly beyond their requirements, and that it meets their reliability needs. Another feature that they started using, somewhat after the initial implementation, is SingleStore’s columnstore , alongside the in-memory rowstore. They started moving some of the older data into columnstore periodically, which can reduce storage costs by 90% plus in some cases. (Editor’s note: This is partly due to strong data compression in columnstore, in addition to the lower cost of storage on disk – as used for columnstore – vs. storage in memory, as used in rowstore.) Conclusion In summary, James described how SingleStore provided a fast, responsive, scalable solution that allows Uber to run important parts of their business using SingleStore as a standard SQL database . In the conclusion to the presentation, James goes on to talk about many specifics of the Apollo system, including Apollo Query Language (AQL), their query layer on top of SQL. He shows how they optimized many aspects of the system to support AQL while delivering high performance. To learn more about how Uber maximized their use of SingleStore, view the recorded presentation and review the slides . Also, you can get started with SingleStore for free today.", "date": "2019-07-26"},
{"website": "Single-Store", "title": "webinar-machine-learning-ai-memsql", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/webinar-machine-learning-ai-memsql/", "abstract": "Predictive analytics, machine learning, and AI are being used to power interactive queries and outstanding customer experiences in real time, changing how companies do business. SingleStore is widely used to help power these advanced applications, which require fast access to recent data, fast processing to combine new and existing information, and fast query response. In this webinar, Eric Hanson, Principal Product Manager at SingleStore, shows how SingleStore customers are using this fast, scalable SQL database in cutting-edge applications. You can read this summary – then, to get the whole story, read the transcript, view the webinar , and access the slides . Mapping SingleStore to Development and Deployment Eric describes how SingleStore helps at each step as you build and run machine learning models: Training models . You can use SingleStore as a fast, scalable source for training data, accessing the data via standard SQL. With SingleStore, you can complete many more training runs in less time. Integration with machine learning and AI tools . A wide range of machine learning and AI tools use the MySQL wire protocol, which is supported directly in SingleStore. So they connect directly to SingleStore as well. This includes the Python data analysis library pandas, the scikit-learn Python library, NumPy, the R language, SAS analytics, and the TensorFlow machine learning platform. Fast ingest . SingleStore accepts streaming data, for real-time ingest, or bulk uploads from a very wide range of sources. Scoring on load . As you load data through a SingleStore Pipeline, you can transform data with a Python script or any executable code that you create. This allows you to, for example, compute a “score” column from existing, input columns very fast, during the load process. Using these features together supports high productivity during development and very fast execution in production. Developers use their accustomed, purpose-built machine learning and AI tools, and are able to build and test models separately from the production pipeline. In production, SingleStore’s scalability, very fast ingest, and fast execution of pre-written, pre-tested Python scripts and pre-compiled executable code support very high performance at scale. Systems that have feedback loops benefit greatly as the benefits of fast ingestion and fast execution compound exponentially with time. Mapping SingleStore Features to Machine Learning and AI Whereas the overall philosophy of SingleStore is to integrate well with existing tools, there are a few SingleStore capabilities that further boost machine learning and AI use cases. Eric describes them: Scalabilit y. Operationalizing machine learning and AI programs is difficult enough, given the ongoing focus on research and development, over implementation, in these areas. It’s even more challenging when your shiny new ML pipeline can’t scale to match demand. Because SingleStore is fully distributed – for ingest, transactions, and analytics – it serves as the solution for a wide range of scalability problems, including this one. Vector functions . SingleStore has a few functions that are especially useful for vector similarity matching, producing amazingly fast execution of complex operations at scale. DOT_PRODUCT for two vectors and EUCLIDEAN_DISTANCE for two vectors are highly useful, specific functions. JSON_ARRAY_PACK for a floating point number and an array, and VECTOR_SUB for two vectors, are helpful supportive functions. ANSI SQL support . AI is often seen as a scientific function, isolated from business concerns. The full ANSI SQL support in SingleStore is highly useful operationally and can also serve as a bridge between the AI group and the business side. Approving Credit Card Swipes in 50ms of Processing Time Despite years of hype, it’s still early days for real-world implementations of machine learning models and AI programs in production applications. As deployments increase, SingleStore is being used for an ever-widening range of applications. (Streaming technologies such as Kafka and Spark are often deployed for the same purpose, despite the degree of change to data flows, skills, and vendor relationships required to take advantage of them.) One example is a credit card fraud detection application developed by a major US bank. Fraud detection used to be a batch process that ran at night, allowing many illicit purchases to be made on a stolen card or card data. But this bank is implementing fraud detection “on the swipe,” with approval decided within one second of the swipe – most of which is taken up by data transmission time. Only with SingleStore have then been able to assemble, process, and make a decision on their 70-feature model in real time. Conclusion For research and development, SingleStore is free to use for workloads up to 4 nodes (typically, 128GB of RAM), with much larger on-disk data sizes possible. When running SingleStore without paying, you get community support through the SingleStore Forums. To run larger workloads, and to receive dedicated support (as many organizations require for production), contact SingleStore for an enterprise license. SingleStore’s connectivity, capabilities, and speed make it a solid choice for machine learning and AI development and deployment. For more information you can read the transcript, view the webinar , and access the slides . Or, download and run SingleStore for free today!", "date": "2019-07-27"},
{"website": "Single-Store", "title": "ideal-stack-real-time-analytics", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/ideal-stack-real-time-analytics/", "abstract": "Real-time analytics is necessary to enable real-time decision making and to deliver enhanced customer experiences ( download real-time whitepaper ). Building a real-time application starts with connecting the pieces of your data pipeline. To make fast and informed decisions, organizations need to rapidly ingest application data, transform it into a digestible format, store it, and make it easily accessible. All at sub-second speed. In this video, we show how an earlier version of SingleStore was able to serve as the core of a real-time analytics stack. How SingleStore Supports Analytics Let’s briefly review SingleStore’s analytical approach. SingleStore is a database that is a real-time database , the fastest database for operational analytics. SingleStore is scalable, so you can scale to as many nodes as you want to have in your system. And SingleStore uses the proven SQL syntax; it is a relational, SQL database at its core. SingleStore has been ranked by analysts as the number one database for operational analytics . The mission of SingleStore is to make every company a real-time enterprise . We mean to enable every company to build a real-time data pipeline and to have real-time analytics on that pipeline. So we very much care, as a company, about accelerating the way you get analytics from your data as it comes in. (Editor’s note: This includes support for business analytics (BI) dashboards, as supported by major BI tools.) The purpose of real-time analytics is, you want to gain insights that provide meaning to your business from data as it comes in. So you don’t want to do a batch job first, then do the analytics later, or the analysis later. You want to do the analytics as your data is piping into the system. Analytics is used for building a business intelligence dashboard that’s real-time, for improving the customer experience, for increasing efficiency and for creating new revenue opportunities. A typical real-time data pipeline is architected as follows: Application data is ingested through a distributed messaging system to capture and publish feeds. A transformation tier is called to distill information, enrich data, and deliver the right formats. Data is stored in an operational (real-time) data warehouse for persistence, easy application development, and analytics. From there, data can be queried with SQL to power real-time BI dashboards. Let’s dive into understanding the ideal real-time stack. To create real-time analytics, to sell the idea of real-time analytics in the business, you need a BI dashboard. For all the customers we’ve worked with, there’s some sort of visualization. It’s either home-grown or it’s built using one of the third-party platforms: Tableau, Zoomdata, Looker, MicroStrategy, Qlik, you name it. Now, the common element of all these business intelligence tools that support business dashboards like this is that they only provide the interface. They usually don’t provide the backing data store. You need a place that stores your data, that’s really fast, to be able to come up with the real-time insight. And so, to be able to make your visualization real-time, you need a real-time backing store. Moreover, these sorts of visualizations only attach to backing stores that speak a certain language, meaning they have to be compliant with certain syntax, the most prevalent of which out there is SQL. In the image above, the one in the very right is real-time visualization. Next to it is this data persistence piece. Following are four characteristics of a data persistence element for real-time analytics: In-Memory and Solid State . It needs to leverage in-memory and solid state storage. If you have a database that doesn’t have the ability to use in-memory and solid state storage these days, it’s probably not fast enough for your purposes. Distributed Architecture . The fastest databases are distributed, because you want to be storing your data in many nodes and processing all of that in parallel. You want massively parallel processing to be able to retrieve your data. Data Source Connectivity . You need a data persistent store that can connect to various sources. It’s not important only to support pure SQL coming in, but the data store needs to be able to go and grab data from other sources. Flexible Deployment . You need to deploy it on the cloud and on-premises. More and more workloads that we see are moving to the cloud, and the cloud is an increasingly strong player in this space. So in other words, SingleStore is actually all the above four points. We are a scalable, SQL database. So, when you think about SingleStore, think about it as you would think about MySQL or PostgreSQL or Oracle, SQL databases that are compliant with the ANSI SQL standard The difference is that SingleStore has the ability to scale. The Pinterest Use Case When you use Pinterest, all the pins and re-pins that you do from your mobile devices or from the desktop are pumped into Kafka. And from Kafka, they are then enriched in Spark. They actually add location information and other information to it, and then store it into SingleStore. From SingleStore, they can then perform queries, ad hoc queries, to test. Usually they do A/B tests for ad-targeting purposes. And the big sort of innovation, or the big technical benefit here, is that they are doing one gigabyte per second of ingest, so that is around 72 terabytes of ingest, and they’re getting real-time analytics from all that data streaming into the system. An Energy Use Case This energy company has created a real-time analytics pipeline to determine the current state of their drills. This is an oil and gas company drilling for oil. If they drill in a certain direction, and the drill hits bedrock, that is very bad. Each drill bit costs them millions of dollars. So, what they need to do is determine, in real-time, whether or not the drill is going the right way. So, they have real-time inputs coming from their sensors. They pump all that information into Kafka – all the sensor information from all their drill bits. Once there, the data is sent through Spark, and they run a predictive analytics model on it. The model is developed in SAS, and they use Spark to run it – to execute the machine model, or to do the model scoring. Once there, they can then put the results in SingleStore to be able to decide whether or not they should take certain proactive actions with their drill bits. So, this is another example of a real-time data pipeline that provides real-time analytics. And finally, just to bring it all home. Refer to the demo below, and you can view the presentation . Wind Turbines Demo The image here shows you data from 2 million sensors that are on wind turbines located all over the world. This demo is a simulation. The locations of the wind turbines are real. This is a real data set, but the activity of the sensors is what’s simulated here. You’ll notice here that I can move around the globe and see where all my wind farms and wind turbines are. I’m specifically going into Europe, because I know that there are lots of wind farms and wind turbines in eastern Europe. The pipeline here is as follows. All the information from all of these is very similar to the energy company use case that I was just mentioning. All the data from all these wind turbines, each of them has sensors, and that data all goes into Kafka. From Kafka, it then goes into Spark, and there, we perform a simple regression analysis model to determine where or how we expect the wind turbine to behave. So, the ones that we see as yellow or green or red, essentially is our prediction of how those turbines will behave over time. So the ones that are red, we expect that you should probably replace it soon. The ones that are yellow are getting there. And you’ll notice that the red ones eventually turn to green, because of the expectation is that it’s a model where things are slowly decaying over time or degrading over time, and then they get repaired. And they’re degrading and then they get repaired. So you can see here a visualization of all that in real-time. This is a view of SingleStore Ops, which is our graphical dashboard for looking at the SingleStore clusters. (Editor’s note: SingleStore now also offers SingleStore Studio , a more advanced graphical interface.) As I described earlier, SingleStore is a distributed database, which means it exists or you install the same software across many systems, or across many machines. So here you see it has 16 machines, and all those machines are humming. If you see those green, it means that the CPU is going with SingleStore. So, why is it going? It’s because in real-time, the data is coming from Kafka to Spark to SingleStore, and rendered here in this dashboard. So, you can notice that sort of every second, it’s refreshing, showing users new insights based on real-time data that’s coming into the pipeline. Conclusion To learn more about how SingleStore supports generating analytics in real time, view the recorded presentation and review the slides . Also, you can read more in our resource center .", "date": "2019-08-06"},
{"website": "Single-Store", "title": "webinar-wealth-management-with-memsql", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/webinar-wealth-management-with-memsql/", "abstract": "Wealth management is an intensely competitive offering for banks and other financial services institutions. It requires high concurrency – the ability to serve many users, fast – low latency, and the ability to access vast amounts of current and historical data in real time. Institutions have used in-memory databases and streaming data to try to meet the demand. In this webinar, SingleStore’s Sourabh Mehta shows how SingleStore improves the wealth management experience for users and gives institutions the ability to stand out in this competitive area. You can view the wealth management webinar here . In the webinar, Sourabh describes the “before” and “after” architecture for a bank that provides wealth management dashboards to clients, whether individuals or family offices. By replacing a Hadoop/HDFS data store with SingleStore, the bank was able to deliver much more responsive updates to users, with query responsiveness in the tens of milliseconds; support tens of thousands of simultaneous users, and add more users without additional engineering work or expense; analyze five times as much historical data to provide better answers to user queries; and to avoid the processing and responsiveness delays that had previously occurred when important market news hit and usage surged. This webinar was originally presented as part of our webinar series, How Data Innovation is Transforming Banking (click the link to access the entire series of webinars and slides). This series includes several webinars, described in these three blog posts: Real-Time Fraud Detection for an Improved Customer Experience Providing Better Wealth Management with Real-Time Data (this webinar) Modernizing Portfolio Analytics for Reduced Risk and Better Performance Also included are these two case studies: Replacing Exadata with SingleStore to Power Portfolio Analytics Machine Learning and Fraud Detection “On the Swipe” For Major US Bank You can also read about SingleStore’s work in financial services – including use cases and reference architectures that are applicable across industries – in SingleStore’s Financial Services Solutions Guide . If you’d like to request a printed and bound copy, contact SingleStore . The Importance of Digital Transformation and Wealth Management According to a Gartner survey, digital transformation is the top priority for banks, with almost double the interest of any other priority. It’s also a factor in all the other priorities they describe – revenue and business growth, operational excellence, customer experience, cost optimization and reduction, and data and analytics. Wealth management is a critical business area for banks, and a top target for digital transformation: Deloitte says that 80% of retail bank profits are generated by high net worth individuals. Aite says that US brokerages and registered investment advisers manage a total of $24.2 trillion in assets – a few trillion dollars more than the size of the US economy . Assets controlled by these individuals are expected to rise by 25% in the current five-year period to 2021. So wealth management is large, fast-growing, and strategic – perhaps the #1 strategic focus, in many cases – for banks and other financial institutions. Digital transformation of wealth management offerings is a top priority. New Data Initiatives for Banks Speeding up the delivery of data to the wealth management dashboard allows the user to get the information they need to make decisions without any visible delay; “we don’t want them to see a spinning wheel,” is how banks often describe the desired experience. What banks want to eliminate, for their users, is called “event to insight latency” – that is, the waiting time between a request for information, or new information arriving from data sources, and the appearance of the information onscreen. This allows the user to interact smoothly with financial information, take action, see the response, and move on. In order to deliver a positive experience for customers, companies set up service level agreements (SLAs) across their digital delivery infrastructure. Meeting these SLAs is crucial, but difficult to manage as the number of users increases, the complexity of queries increases at the same time, and with news events that affect financial markets driving sudden surges in usage. Case Study – Wealth Management Dashboards A large US bank was struggling with their wealth management solution. Based on a Hadoop/HDFS solution, the solution had many problems, but the worst was batch data loading. Data didn’t stream into the solution; instead, it was pooled for an hour, then a batch upload was run. (Batch uploads are the default for Hadoop/HDFS.) During the batch uploads, queries were locked out, which users found unacceptable. In addition, queries were slow, ranging from tenths of a second into seconds. And concurrency support was poor; when critical market events occurred, and users moved onto the system en masse, response times slowed to a crawl. The SingleStore Solution The bank augmented the Hadoop/HDFS database with SingleStore, using SingleStore to power the wealth management solution and leaving Hadoop/HDFS as a data lake for long-term data storage. The results have been excellent: Data is streamed into SingleStore in real-time; no waiting for data to be batched. Ingest and query processing run lock-free, simultaneously; no query downtime during batch updates. Five years’ history instead of one; applications and user-driven queries can draw on five times as much data at hand for deeper analysis. Fast responsiveness; queries are answered in 10s of milliseconds, with no spinning wheel. High concurrency; 40,000 users are supported with no contention, even when market events cause spikes in usage. Conclusion SingleStore is great for augmenting Hadoop/HDFS and other existing systems that suffer from slow responsiveness, batch update timeouts, and concurrency issues. You can view the wealth management webinar , download and run SingleStore for free , or contact SingleStore today .", "date": "2019-08-08"},
{"website": "Single-Store", "title": "web-workers-client-side-react-redux", "author": ["David Gomes"], "link": "https://www.singlestore.com/blog/web-workers-client-side-react-redux/", "abstract": "If you’ve ever had a web application freeze while it was calculating something, chances are that performing that computation in a JavaScript Web Worker would help. In this blog post, SingleStore’s David Gomes explains how to use JavaScript Web Workers, together with React and Redux, to create fully-client side web applications. Introduction In this article, we’re going to explore how we leverage Web Workers , together with React & Redux, to build a fully client-side web application here at SingleStore. My goal with this article is to highlight a specific use case of Web Workers, as well as detail how we were able to build on top of the relatively low-level Web Workers API to make our code more organized and easier to iterate on. Web Workers are one of the most underrated features of JavaScript. Despite having been around for 10 years, they’re relatively unknown, and are not used very often in web applications. Most desktop GUI applications take advantage of multithreading to make sure their UIs are responsive while the application does other background work. Historically, web applications haven’t been able to apply the same strategy, but that’s where Web Workers come in. As an example, if you’re building a CodePen - like application and want to parse the code in the editor, and add syntax highlighting to it, a Web Worker is a great idea, since you can perform the work in parallel, without incurring the large network cost of sending the entire code to a web server. So, what is a Web Worker? A Web Worker is a feature of JavaScript that enables parallel execution of code in the browser. In other words, it allows for the execution of JavaScript in the background. The main use case of Web Workers is performing expensive computations in the browser without blocking the main thread, where the DOM is rendered. If you’ve ever had a web application freeze while it was calculating something, chances are that performing that computation in a Web Worker would help. In this blog post, we’re going to dive into our usage of Web Workers in a specific application. That application is SingleStore Studio , a visual user interface that allows our customers to easily monitor, debug, and manage their SingleStore clusters. (There are also use cases for business intelligence tools. Studio, because it’s so tightly integrated with SingleStore, is better able to give you SingleStore-specific and schema and monitoring information.) SingleStore Studio is implemented as a fully client-side web application that runs in the browser. It connects to SingleStore and runs queries on behalf of the user in order to show all kinds of information about the state of the cluster. Additionally, this tool also allows users to run arbitrary queries against their cluster via an embedded SQL development environment. Integrating React & Redux with Web Workers The frontend of SingleStore Studio is implemented using React for the view layer and Redux for the application state layer. The “backend” of the application runs in the browser inside a Web Worker. This allows us to perform all the expensive work of connecting to SingleStore, running queries, and parsing the results in the background. This is convenient, since the queries Studio runs against SingleStore may return millions of rows. As such, we want to parse and clean up the outputs from these queries without blocking the main thread. We used JavaScript Web Workers to move the database interaction to the background and create a smoother user experience. So far, all of this sounds wonderful. However, there’s one issue that I haven’t mentioned yet. How should the main thread and the Web Worker communicate? Historically, the Web Worker API offered a shared memory protocol for coordination between the main thread and worker threads. Unfortunately, due to the Spectre Vulnerability , most browsers disabled the API. Because of this, SingleStore Studio leverages the transfer protocol to communicate. This is what the transfer protocol API looks like: main-thread.js expensiveComputationWorker.postMessage({ n: 8 });\n\nexpensiveComputationWorker.onmessage = (msg) => {\n  console.log(\"received message from my web worker\", msg);\n}; worker-thread.js onmessage = (msg) => {\n  console.log(\"received message from main thread\", msg);\n\n  postMessage(getNthPrime(msg.n));\n}; Since the backend of our application runs inside the Web Worker, this API is too low level for our application. We need something more high level, which allows our components to easily request the data that they need from the backend. The first thing that comes to mind is GraphQL , a query language that allows clients to declaratively state which pieces of data they need from an agreed-upon schema. So, we gave it a spin and built a GraphQL server that lives inside the Web Worker. Then, we built resolvers for each piece of data that our client could possibly need, so our components could simply tell a GraphQL client (we used Apollo ) what they needed. After a while, this approach became cumbersome, since we now had two type systems that we had to keep in sync: TypeScript [ 1 ] The GraphQL Type System Having to write all type definitions twice slowed us down significantly [ 2 ] . Moreover, we were not taking advantage of the GraphQL query language at all. Most of the pages in our application request all the information about all the records of a given record type (e.g., all the databases in the cluster, all the nodes in the cluster, etc.). For this type of query, GraphQL is not very helpful, since we’re not taking advantage of the powerful query language at all. It makes our architecture more complex without giving us any real benefits. I actually gave a talk just about this entire experiment at React Fest last year . So, once we decided to drop GraphQL, we explored other options. This is the flow that we wanted – and ended up achieving: View Layer asks for data using some custom API Worker computes data Redux is populated with data View is subscribed to Redux updates and eventually displays data This general pattern is very standard for React+Redux applications. The interesting bit here is how to populate the Redux state (which lives on the main thread) from the worker thread. Here’s what we came up with. First, the view layer: page-databases.tsx import { queryDatabases } from \"worker/api/schema\";\n\nclass DatabasesPage {\n  componentDidMount() {\n    this.props.dispatch(queryDatabases());\n  }\n} We can see that it’s very easy for this React component to ask for the data it needs by dispatching a Redux action. But how does it work? How is the worker thread notified of this action, and how is the Redux state populated? We implemented this using Redux middleware. (If you are not familiar with Redux middleware, I recommend the official documentation .) We created middleware in Redux that listens to all the dispatched actions. Whenever it finds that a “worker action” was dispatched, it passes it to the web worker. A “worker action” is a specific type of Redux action object that the Redux reducers don’t listen to; instead, a “worker action” represents a specific API call on the worker thread. The middleware looks for a specific object structure to distinguish “worker actions” from regular Redux actions. So, our Redux middleware calls the worker thread (using a custom postMessage wrapper) and the worker thread then parses the “worker action” object to figure out which API call it should run. worker/api/schema.tsx export const queryStructure = makeActionCreator({\n  name: \"queryStructure\",\n\n  handle: (ctx: HandlerContext): Observable => {\n    ...\n  }\n}; The exported function from “worker/api/schema.tsx” generates a “worker action,” which the “page-databases.tsx” file dispatches. However, the function that the middleware will cause to run on the worker is the handle() function, which performs the actual work of connecting to SingleStore and returning the list of databases. Since the actual API call returns an Observable of plain Redux actions, each such action will be sent back to the main thread, where our middleware will dispatch them, allowing the reducers to listen to them. This completes the cycle that I mentioned earlier: (main thread → worker thread → Redux (main thread)). Observables as the output of API endpoints are extremely powerful, since they allow an API endpoint to emit multiple times. This makes the following patterns (and others) trivial: Emit the output of a query in batches, for a smoother experience Emit loading and finished (success, error) states individually, so that the Redux store contains the current state for a request (which will be shown in the view) One final thing to note is that all of our communication between the worker thread and the main thread is JSONified. We do this so that we can easily serialize and deserialize class instances using JSON revivers . If you’re curious about the performance consequences of this, you can check out this article . (It’s equivalent to the performance of native postMessage, which uses the structured clone algorithm.) Conclusion Web Workers are very powerful and their simple API allows one to easily build an abstraction layer on top of them. In our case, we figured out how to integrate React, Redux, and Web Workers in a way that works very well for us. We’ve found that this framework allows us to iterate quickly while achieving our main goal of running the heavy computation work without blocking the UI. If you are interested in an open source version of this solution, please reach out to us at david@singlestore.com . The SingleStore database is renowned for its performance, so it only makes sense that our UIs follow suit. For this reason, our frontend engineering team leverages the best web framework technologies to ensure our customers are guaranteed a stellar experience. If you are an Application Engineer with a similar passion for quality, we are hiring in Portugal and San Francisco . [ 1 ] : SingleStore Studio is written using TypeScript . [ 2 ] : There are some type definition generators that can help with this process. However, we found that this didn’t work well for us (we were using Flow at the time). Thanks to Brian Chen and Carl Sverre for editing help.", "date": "2019-08-14"},
{"website": "Single-Store", "title": "webinar-real-time-fraud-detection", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/webinar-real-time-fraud-detection/", "abstract": "In this webinar, which you can view here , SingleStore’s Mike Boyarski describes how real-time fraud detection represents a major digital initiative for banks. He shows how SingleStore gives fraud detection services, along with other real-time analytics, an edge with faster ingest, real-time scoring, and rapid response to a broader set of events. He also describes a major US bank’s implementation of this approach, which is described separately in our case study about fraud detection on the swipe. This webinar was originally presented as part of our webinar series, How Data Innovation is Transforming Banking (click the link to access the entire series of webinars and slides). This series includes several webinars, described in these three blog posts: Real-Time Fraud Detection for an Improved Customer Experience (this webinar) Providing Better Wealth Management with Real-Time Data Modernizing Portfolio Analytics for Reduced Risk and Better Performance Also included are these two case studies: Replacing Exadata with SingleStore to Power Portfolio Analytics Machine Learning and Fraud Detection “On the Swipe” For Major US Bank You can also read about SingleStore’s work in financial services – including use cases and reference architectures that are applicable across industries – in SingleStore’s Financial Services Solutions Guide . If you’d like to request a printed and bound copy, contact SingleStore . New Data Initiatives for Banks There’s a great deal of digital transformation occurring around data initiative for banks. Banks are finding a great deal of value in moving to a digital experience for their customers. Digitizing operations allows banks to deliver new services and products to the market, creating new sources of revenue. These initiatives create pressure on existing data infrastructure, which tends to have a great deal of latency between events in the real world and insights that banks can use to drive new applications and make better decisions. Banks, like other organizations, are seeking to enable more continuous data-driven actions and decision-making. They need a data infrastructure that can adapt to those changing conditions. More and more queries need to fit into a service level agreement window. SingleStore has customers that need all of their queries to run within a 200 millisecond requirement. And that’s ultimately to deliver the best experience for their own clients. We also see customers that want to innovate, but want to do it using the existing operations and skills. This means augmenting or supplementing what’s already in place to keep compatibility with existing tools, existing skills – SQL and other standard technology, relational technology – and also provide a path for cloud adoption. So cloud-native technologies that work in multi cloud or hybrid cloud configurations are the technologies that we find banks are investing in. Fraud Detection Challenges and Opportunities The market for fraud detection is significant. It’s moving from, in this study, $14 billion in 2018 to $34 billion in 2024. This represents a recognition that fraud is becoming very challenging and it can be addressed and augmented with technology.RSA reports a 600% increase in mobile fraud over the last three years. So obviously as more transactions are occurring on mobile channels, the reality of fraud occurring on that sort of engagement path is going up. Fraud is also very broad. Fraud applies to online payments, detecting insider trading, building or creating new accounts, and the synthetic identity issue, which is fake accounts using a blend of different information from hacked information. This creates a very complex hard problem for banks to mitigate, but also they want to provide the best possible experience to their customers to compete. Banks have to figure out that balance between enabling a frictionless digital experience but also protecting the assets of the business. Fraud detection is very much powered by analytics, the ability to apply data to detection through anomaly detection, or predictive classification, or clustering. Where the battle really gets fought is identifying the appropriate model and analytic functions to identify something to block or approve. Having the sort of advanced analytics powered by big data to find the best fit, to find the most accurate models, that’s where a lot of the art and science of fraud detection exists. You can see more about this in our case study of a bank’s fraud detection application, where they have moved from overnight fraud processing to checking for card fraud in real time, on the user’s card swipe. SingleStore Overview What makes SingleStore a great solution for something like fraud detection? SingleStore describes itself as the “no-limits database” because of the software’s architecture. Ultimately we have a distributed scale-out system, so our ability to support growing workloads and the growth in data is baked in, because it’s just a node-based scale-out system. SingleStore also has an innovative lock-free architecture, supporting the continuous ingestion and continuous data movement that are part of what we call operational analytics applications. SingleStore is an operational database that can do very fast analytics. Machine learning (ML) and AI can bring a great deal of value to the business, so we see a lot of customers taking advantage of real-time ML with SingleStore . And we see a lot of customers making a transition from their legacy, on-premises architecture to the cloud, and doing it in a flexible, adaptable way. You can run SingleStore and deploy that on any cloud and/or on your own on-premises infrastructure; it’s all about flexibility. SingleStore’s claim to fame is around delivering speed, scale, and SQL all in one package. Think of our system as being able to efficiently take data into the platform and then run queries on that data as fast as any data warehouse product in the market, including both legacy platforms like Teradata and Vertica, and some of the newer cloud-based data warehouses. SingleStore came into existence about five or six years ago. So we were built in the cloud. We took advantage of distributed processing, all the sort of cloud-native functions that you would expect, like Kubernetes and containers, making SingleStore a really good choice for those modern cloud-based platforms. Q&A How does SingleStore compare to Cassandra? Cassandra is used pretty heavily in the fraud detection market, and it’s largely because of the ingest … The ability to ingest data into Cassandra is very solid. The challenge with Cassandra, though, is if you want to do any sort of advanced additional query logic, we have found that customers really have a hard time getting insights out of Cassandra. So it’s okay for running a well-defined analytic function, but if you want to change that analytic function or iterate it quickly, that’s where the limitations come into play. Cassandra is a NoSQL system . That means their support for joins, and their ad hoc query support, and their ability to run additional sort of query functions and analytic functions, it’s not standard, right? At SingleStore, we are on par, if not better, with the ingest requirement. And then what’s the best about SingleStore is you’re getting a relational SQL environment, whereas with Cassandra, you’re getting their own custom implementation of SQL, which means you have to learn their language. So you’re getting a little bit more complexity around doing more continuous improvement of the analytics. So the takeaway is, you get the same ingest performance as Cassandra, but then you get the power of SQL using the SingleStore platform. Can you explain how SingleStore works with Kafka? Yes. In terms of ingest, SingleStore does have support for a number of sort of ingest techniques, whether it’s file systems, S3 – and SingleStore has a built-in connector to Kafka. So what that means is, you can, in a very simple way, using one command line function called “start pipeline,” it essentially automatically configures and identifies the connection point from your Kafka topic into SingleStore. It’s very easy to set up; there’s not a lot of hand-wringing and custom coding required to connect a Kafka environment to SingleStore. And then out of the box, you get exactly-once semantics, you get extremely high throughput, and then you can do really sort of fun things like put stored procedure logic onto the Kafka stream process so that you can do scoring on the stream, for example, if you’d like. You can also do pretty advanced things around doing logic on the data movement of Kafka into SingleStore, for example to identify where to land the data, particular on a node basis or into a particular type of table. So there’s a lot of power that you can get with Kafka and SingleStore. It’s probably the reason why, I think, it’s probably the number one ingest technology that’s used in conjunction with SingleStore. It’s a top-notch feature, and it’s really well regarded within our community. Can SingleStore be used with Hadoop? And how do customers typically deploy in that kind of environment? So Hadoop we see more and more being used as an archive storage layer. So we can do a couple things with Hadoop. One is you can stream data into both SingleStore and Hadoop concurrently. ( As shown in this case study – Ed. ) It’s your classic Lambda-style architecture. And so that means you can use SingleStore to do your analytics on the most current active data, and then your Hadoop cluster can be used for other types of analytics, more data science oriented or just archival style analytics. We do see some customers also landing data first into Hadoop, and then they use the HDFS connector to pull data from HDFS into SingleStore. And you can do that in a continuous fashion. So there’s an ability to stream data from Hadoop directly into SingleStore, and that allows for sort of, land once and then pull into SingleStore for spot queries, or maybe a segment of queries, or a segment of data. And then when that period of time ends or that query sort of project goes away, you can flush the data of SingleStore and keep all your data in HDFS. So there are a lot of different ways that people use SingleStore with Hadoop. And it has a lot to do with the application and the query requirements that you have. But I guess the net of it all is our HDFS pipeline is super robust and very commonly used to accelerate query performance of Hadoop. You get the nice relational SQL structure, all that interactive query response that you want. Conclusion You can take a free download of SingleStore. It’s a no-time-bomb version of our product. You can deploy it to production. Of course, it is limited by its scale and the number of nodes deployed, but you can do a lot with what’s available there. And then you can get support from the SingleStore Forums , which are community-driven, but also supported by some folks here at SingleStore.", "date": "2019-08-22"},
{"website": "Single-Store", "title": "replicating-postgresql-into-memsqls-columnstore", "author": ["Oryan Moshe"], "link": "https://www.singlestore.com/blog/replicating-postgresql-into-memsqls-columnstore/", "abstract": "Thanks to Oryan Moshe for this awesome blog post, which originally appeared on DEV Community. In the blog post, Oryan describes how to achieve the high performance of SingleStore’s columnstore for queries while keeping transaction data in PostgreSQL for updates – Ed. Making the impossible hard I… don’t think this is how it should look like. So it’s this time of the year again, we need to upgrade our SingleStore cluster and expand our contract to fit the new cluster topology. We really outdid ourselves this time. Expanding to a 1TB cluster is impressive, especially when it’s completely not justified. The background Wait. A 1TB Cluster? Yeah yeah, call us spoiled, but querying on PostgreSQL (PG from now on) is just not the same. Sure, you can get ok speeds if you’re using the correct indexes and optimize your queries, but it’s not even comparable to the performance you get from the memory based rowstore in SingleStore (Mem from now on), or the insanely fast aggregations of the columnstore. A Short (Short) Summary of Mem’s Different Storage Types So we basically have 2 types of storage in Mem, rowstore and columnstore. The rowstore is stored pretty much like any other database, but in the memory instead of the disk (crazy fast). This means each row is stored together with all of its columns. The columnstore is sort of a transposed rowstore. Instead of storing rows, we store columns ( thank you Captain Obvious ), which allows us to make aggregations stupid fast. (Think about it; instead of going to each row and summing the “cost” column, we can just go to the “cost” column and sum it up.) The columnstore is stored on the disk. ( The SingleStore blog has an article on making the most of both rowstore and columnstore tables . – Ed. ) The issue is SingleStore’s license costs more as we have more memory in our cluster, not to mention the cost of the machines themselves (1TB of memory isn’t exactly cheap). “So why not store everything in the columnstore? It’s cheaper both license and infrastructure wise, and it’s stupid fast!,” you might ask (if you talk to yourself while reading tech articles). So here’s the catch – the way the data is stored in a columnstore makes it incredibly fast in aggregated queries, and allows amazing compression, but updating a row is slow. ( Some people here at SingleStore are thinking creatively about a possible solution to the problem of slow updates to columnstore tables that Oryan mentions here; stay tuned. – Ed. ) How slow? If we need to update some columns for rows in a specific day, it’s faster for us to delete the data from this day and re-insert the updated one instead of updating the existing rows. So, How Do We Store Our Data? Well, in my team we use 7 flavors of databases (might be more, can’t really keep track these days) but the main ones are PostgreSQL, hosted and managed by AWS RDS (for transactional processing) and SingleStore, hosted on EC2 and managed by yours truly (for analytical processing – including, but not limited to, analytics and transactions.). Instinctively, most of our data is stored in PG (excluding some large columnstore tables containing North of 8B records). The problem is, once you go Mem you never go back, so we created a replication service that can replicate a row from PG to Mem’s rowstore in real-time. This allows us to enrich our columnstore-only tables, create ETLs, and most importantly, speed up queries. If you’re here, you either use Mem and thus know its performance, or just like to go around dev.to, reading random articles about niche DBs. If you’re the latter, let me hit you with some numbers. A completely reasonable query, consisting of 6 joins, took 30 minutes to run on PG. After optimizing it for 2–3 hours, adding indexes, banging my head against the wall and praying for a swift ending, I was able to cut it down to 3 minutes. Taking exactly the original query (the 30 minutes one) and running it on Mem, it took 1.87 seconds . The Real Deal Problems Definition, AKA What’s Making Me Lose Sleep So Mem is expensive, we’re almost at our new license limit (after more than doubling it) and there’s no way we can go back to querying exclusively on PG. The solution seems simple: move big tables to the columnstore, free up some memory so you don’t have to increase your license, and upgrade your machines. For this article I’ll use our table touch_points as an example, it’s our largest (both in memory and row count) table stored in a rowstore - it has over 180M rows, and weighs more than 190GB. Why is it in our rowstore? First, cause we replicate it from PG, and so far our service only supports replicating to rowstore tables. But, more importantly, it needs to be updated. Out of 30 columns, 2 might get updated  - visitor_id and cost . Solutions The First Solution So this was the “correct” solution, design-wise. In short, using ActiveRecord callbacks, I kept 2 tables up to date. One is the touch_points table in the columnstore, containing all columns that exist presently on touch _ points except the 2 that get updated. Other than touch_points , I created a table called touch_points_extra_data in the rowstore, containing the 2 missing columns and 1 ID column that allows me to connect the 2 tables. As I said, this was the correct solution design-wise. The problem is that so much could go wrong. With so many moving parts, all dependent on Tails hooks, we were sure to get out of sync sometime . Not to mention the fact that we’ll have to edit all of our queries from touch_points to add that extra JOIN. The Second Solution, AKA “The Bruteforce” So we realized our top priority is to keep the data correct, and we were willing to make some compromises ( foreshadowing ). I decided to replicate the whole table, as is, from PG once in a while. This way we can make sure that (up to the moment of replicating) our data will be identical in both DBs. The compromise is that we are used to having this data updated in real time, and now it’ll be outdated until the next replication. This is a compromise I’m willing to take. The Technical Part Easier Said Than Done So apparently replicating a whole table from one DB to another isn’t as straightforward as you would think. Especially when the two DBs run on different engines entirely. The first thing I tried is using pg_dump , with the plain file format (which essentially creates a file with loads of INSERT statements) and then convert it to MySQL syntax and load to Mem. Sounds great, right? I started the pg_dump , and 5 hours later it wasn’t even close to finishing, while the dump file was already at 60GB. pg_dump with the plain option is the most inefficient way to store data. 5 hours delay in replication is unacceptable. If at First You Don’t Succeed… Fail Again The next thing I tried was using the COPY command of PG, this command can copy (duh) a table, or a query into a FILE, a PROGRAM, or STDOUT. First I tried using the STDOUT option (the simplest one, and it doesn’t create a footprint of a huge dump file). psql -U read_user -h very-cool-hostname.rds.amazonaws.com -p 5432 -d very_cool_db -c\\\n\"\\COPY (SELECT * FROM touch_points) TO STDOUT\\\nWITH(DELIMITER ',', FORMAT CSV, NULL 'NULL', QUOTE '\\\"');\" > touch_points.csv And it worked! I got a “dump” file from PG containing our whole touch_points table, in just under 20 minutes. Now we just need to import it to Mem, but why do I need the file? I can just pipe the result right from PG straight into Mem! So I needed to create the part where Mem receives this csv-like table and loads it into the db. Luckily Mem is MySQL-compatible and provides us with the LOAD DATA clause! LOAD DATA LOCAL INFILE '/dev/stdin'\n  SKIP DUPLICATE KEY ERRORS\n  INTO TABLE touch_points_columnstore\n  FIELDS\n    TERMINATED BY ','\n    ENCLOSED BY '\"'\n    ESCAPED BY ''\n  LINES\n    TERMINATED BY '\\n'\n  MAX_ERRORS 1000000; Now, as I said we want to pipe that data right into Mem, so we need to create a connection to our DB: mysql -h memsql.very-cool-hostname.com -u write_user -P 3306 -D very_cool_db\\\n-p'4m4z1nglyS3cur3P455w0rd' -A --local-infile --default-auth=mysql_native_password -e\\\n\"LOAD DATA LOCAL INFILE '/dev/stdin' SKIP DUPLICATE KEY ERRORS\\\nINTO TABLE touch_points_columnstore FIELDS TERMINATED BY ','\\\nENCLOSED BY '\\\\\\\"' ESCAPED BY '' LINES TERMINATED BY '\\\\n' MAX_ERRORS 1000000;\" And then just pipe the data from PG to that connection! psql -U read_user -h very-cool-hostname.rds.amazonaws.com -p 5432 -d very_cool_db -c\\\n\"\\COPY (SELECT * FROM touch_points) TO STDOUT\\\nWITH(DELIMITER ',', FORMAT CSV, NULL 'NULL', QUOTE '\\\"');\" |\\\nmysql -h memsql.very-cool-hostname.com -u write_user -P 3306 -D very_cool_db\\\n-p'4m4z1nglyS3cur3P455w0rd' -A --local-infile --default-auth=mysql_native_password -e\\\n\"LOAD DATA LOCAL INFILE '/dev/stdin' SKIP DUPLICATE KEY ERRORS\\\nINTO TABLE touch_points_columnstore FIELDS TERMINATED BY ','\\\nENCLOSED BY '\\\\\\\"' ESCAPED BY '' LINES TERMINATED BY '\\\\n' MAX_ERRORS 1000000;\" And… It worked! But it took 2 hours to complete. I’m sure we can do better than that. Compression is Your Friend So two cool things important to understand about loading data into Mem are: When inserting a data file into Mem, it copies the file locally to the aggregator and splits the file between the nodes of the cluster, speeding up the data load significantly. Mem supports receiving gzip-compressed data files. Combining these two pieces of information made me understand that creating the file in the middle maybe isn’t as bad as I thought. I can compress that file, making storage a non-issue. It’ll also speed up the transfer of the file to the aggregator (before splitting) by cutting out most of the network related latency, and it’ll allow Mem to split the data between the nodes. Let’s do it! First of all I need to modify the PG part so instead of piping the content to STDIN, it pipes it to a PROGRAM, and in our case, gzip. psql -U read_user -h very-cool-hostname.rds.amazonaws.com -p 5432 -d very_cool_db -c\\\n\"\\COPY (SELECT * FROM touch_points) TO PROGRAM 'gzip > /data/tmp/replication/touch_points_columnstore.gz'\\\nWITH(DELIMITER ',', FORMAT CSV, NULL 'NULL', QUOTE '\\\"');\" After we created this tmp file we need to load it. Luckily the only thing we have to do is to change the source of the input file! Our finished script looks like this: psql -U read_user -h very-cool-hostname.rds.amazonaws.com -p 5432 -d very_cool_db -c\\\n\"\\COPY (SELECT * FROM touch_points) TO PROGRAM 'gzip > /data/tmp/replication/touch_points_columnstore.gz'\\\nWITH(DELIMITER ',', FORMAT CSV, NULL 'NULL', QUOTE '\\\"');\" &&\\\nmysql -h memsql.very-cool-hostname.com -u write_user -P 3306 -D very_cool_db\\\n-p'4m4z1nglyS3cur3P455w0rd' -A --local-infile --default-auth=mysql_native_password -e\\\n\"LOAD DATA LOCAL INFILE '/data/tmp/replication/touch_points_columnstore.gz' SKIP DUPLICATE KEY ERRORS\\\nINTO TABLE touch_points_columnstore FIELDS TERMINATED BY ','\\\nENCLOSED BY '\\\\\\\"' ESCAPED BY '' LINES TERMINATED BY '\\\\n' MAX_ERRORS 1000000;\" And that’s it! The created file weighs 7GB, and the whole process takes less than 20 minutes, so we can run it once an hour and have semi-realtime data! Obviously this wasn’t the end, I wrapped it up in a nice Rails module that allows me to replicate any query from PG to Mem easily, including truncating the old data and using 2 tables to minimize the downtime during replication. Feel free to contact me with any questions! (Twitter: @oryanmoshe. Github: oryanmoshe.)", "date": "2019-08-23"},
{"website": "Single-Store", "title": "case-study-replacing-exadata-with-memsql-portfolio-analytics-ml", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/case-study-replacing-exadata-with-memsql-portfolio-analytics-ml/", "abstract": "This case study was presented as part of a webinar session by Rick Negrin, VP of Product Management at SingleStore. In the webinar, which you can view here , and access the slides here , Rick demonstrates how a major financial services company replaced Oracle Exadata with SingleStore to power portfolio analytics, with greatly increased responsiveness for users and the ability to easily incorporate machine learning models into their applications. In this case study, we’ll emphasize the bank’s digital infrastructure using Exadata, then present their implementation of SingleStore as a reference architecture that you can consider for your own organization’s needs. This case study was originally presented as part of our webinar series, How Data Innovation is Transforming Banking (click the link to access the entire series of webinars and slides). This series includes several webinars, described in these three blog posts: Real-Time Fraud Detection for an Improved Customer Experience Providing Better Wealth Management with Real-Time Data Modernizing Portfolio Analytics for Reduced Risk and Better Performance Also included are these two case studies: Replacing Exadata with SingleStore to Power Portfolio Analytics (this case study) Machine Learning and Fraud Detection “On the Swipe” For Major US Bank You can also read about SingleStore’s work in financial services – including use cases and reference architectures that are applicable across industries – in SingleStore’s Financial Services Solutions Guide . If you’d like to request a printed and bound copy, contact SingleStore . Case Study, Before: The Previous Architecture, with ETL, Exadata, and RAC This case study describes an asset management company. It’s a fairly large asset management company, with about a thousand employees and probably just under a half a trillion in assets under management. They have been in business for several decades. They’ve invested heavily in a lot of different technologies, primarily in some legacy database technologies. Things were working pretty well for awhile, but as new requirements started to come in and more users are using the system, they’ve started running into trouble. So this is what their architecture looked like. And it should be fairly familiar to most of you. They have a variety of data sources, obviously their own internal operational systems, mostly legacy databases. Combined with some external third-party data and partner data that they would bring in. As well as behavioral data from how their users are using the system, both on the web and with mobile. And all of that data was moved, via standard extract, transform, and load (ETL) processes, into a traditional data warehouse. And then that data was accessed by a variety of different users. So you had business users who are using custom business applications, that are doing data exploration and doing data prep on that data. As well as business users using a combination of Tableau and Excel to do analysis. And some data scientists using SAS to do some data science and data exploration of the data. It’s trying to move the models forward. Now this resulted in a number of problems. So one is they were stuck with batch ETL. Which initially was okay, but as they’ve been trying to move to a more streaming system and more real time, this was becoming a bottleneck. And the existing database technology and the ETL technology they had, was just not sufficient. They couldn’t make it work go more often than nightly refreshes and hourly updates. This basically resulted in the system being offline whenever they would ingest large amounts of data. On top of that, the data models that were in use by their data scientists were aging, they were somewhat limited. It didn’t allow continuous development, so it was tough to evolve them as they learned new things and then got new data. And probably the most painful thing is that as more and more users who are trying to use the system, the queries were getting slower and slower. As concurrency ratcheted up, the queries would slow down. On top of that, people wanted to be able to use the data all the time, not just nine to five. And so they want to be able to use this system even when the data’s loading constantly. They tried to meet these new challenges by leveraging newer hardware, or appliances like Oracle RAC and Exadata. And those are extremely expensive, given the kind of hardware neeed to try to solve the problem. Case Study, After: The New Architecture, with Kafka, Spark, and SingleStore To solve these problems, they replaced the old architecture with something that looks like this. Basically with the combination of SingleStore and Kafka and Spark. So the first step was to replace all the ETL technologies with the Kafka queue. For those who aren’t familiar, Kafka is an in-memory, distributed queue. It’s fairly easy to set up and scale and manage. And it’s a great landing place for data that’s waiting to be processed. So they changed the older data sources to funnel into a single Kafka queue. And then from there they fork the data into a couple of SingleStore instances, as well as into a data lake for long-term storage. On top of that, they would then leverage a combination of the data that was security data and their data science sandbox SingleStore instance, as well as some data from the data lake, and pull that into a Spark cluster. So, leveraging the native integration that SingleStore has with Spark so they could train their machine learning models with the newest market data all the time, driving a continuous evolution of their machine learning algorithms. At the same time, the could continue to run queries into Tableau and Excel, continue to use SAS, and continue to run their business applications without having to disturb those approaches too much. And lastly, they were able to get much better performance that they were getting before. They got significantly faster queries. They also, because of the more efficient use of storage and cost effectiveness, they’re able to store the required five years of history versus the three they were able to store in Oracle. And they did all this while still being three times cheaper than the cost of the Oracle solution. To kind of summarize the benefits: the combination of Kafka, Spark, and SingleStore enabled them to do continuous trade and risk analysis using live market data, moving from batch into real time. They reduced their overall spend by 3x while still improving performance. And they have a new data platform for driving their ML and operational analytics delivery, making them much more agile and moving faster. Conclusion You can take a free download of SingleStore. It’s a no-time-bomb version of our product. You can deploy it to production. Of course, it is limited by its scale and the number of nodes deployed, but you can do a lot with what’s available there. And then you can get support from the SingleStore Forums , which are community-driven, but also supported by some folks here at SingleStore.", "date": "2019-08-24"},
{"website": "Single-Store", "title": "webinar-modernizing-portfolio-analytics-for-reduced-risk-and-better-performance", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/webinar-modernizing-portfolio-analytics-for-reduced-risk-and-better-performance/", "abstract": "In this webinar Rick Negrin, Product Management VP at SingleStore, describes the importance of portfolio analytics, enhanced by machine learning models, to financial services institutions – helping them to meet customer needs and edge out competitors. He shows how SingleStore speeds up portfolio analytics at scale, with unmatched support for large numbers of simultaneous users – whether connecting via ad hoc SQL queries, business intelligence tools, apps, or machine learning models. You can view the recorded webinar and download the slides . He also describes how a major US financial services institutions implemented Kafka, Spark, and SingleStore, replacing Oracle and widespread use of cumbersome extract, transform, and load (ETL) routines, in this separate case study . The business problem Rick discusses is the need to modernize portfolio analytics for reduced risk and better performance – both for the customer managing their portfolio, and for the institution offering portfolio management tools to customers. Institutional investors want smarter portfolio management services that deliver optimal returns while reducing their exposure to any one industry, currency, or other specific source of risk. Portfolio managers want guided insights to help them avoid sudden or dramatic rebalancing of funds that can drive up costs and reduce confidence and customer loyalty. SingleStore powers a number of portfolio dashboards and what-if analysis, leveraging live market data for the most up-to-date view of the market. The separate case study shows how a major financial services company used SingleStore to solve these problems, supporting their leadership position in the market. This webinar was originally presented as part of our webinar series, How Data Innovation is Transforming Banking (click the link to access the entire series of webinars and slides). This series includes several webinars, described in these three blog posts: Real-Time Fraud Detection for an Improved Customer Experience Providing Better Wealth Management with Real-Time Data Modernizing Portfolio Analytics for Reduced Risk and Better Performance (this webinar) Also included are these two case studies: Replacing Exadata with SingleStore to Power Portfolio Analytics Machine Learning and Fraud Detection “On the Swipe” For Major US Bank You can also read about SingleStore’s work in financial services – including use cases and reference architectures that are applicable across industries – in SingleStore’s Financial Services Solutions Guide . If you’d like to request a printed and bound copy, contact SingleStore . The Role of the Database in Digital Transformation Digital transformation remains the top priority by far for banks. This is confirmed by a Gartner study from 2019, but we at SingleStore hear it anecdotally in all the conversations that we have with financial institutions. This is because of the opportunity that digital transformation provides. When you to take advantage of new technologies, you can create new sources of revenue, and you can drive down your costs with new operating models, allowing you to deliver digital products and services that just weren’t possible before. To make this happen, you need to have an architecture and operating platform that supports a new set of requirements. One need is to drive down latency: the time from when a new piece of information is born to the time you’re able to gain insight and take action on it. The effort is to get that as close to zero as possible. When you do that, you can make faster data-driven actions in your business. So when something’s going on in the financial markets, the customer wants to understand it, to know what’s going on, as quickly as possible. And to be able to take action on it, in order to either reduce risk in a portfolio or perhaps take advantage of some new opportunity that’s come up. You also need adaptable analytics. The days of getting a static report once a week to your desk and then using that as information are far in the past. You need to be able to have an interactive experience with the data that’s flowing in. To be able to slice and dice it, looking at it across many different dimensions, to find the key insight that’s going to allow you to take advantage of what’s going on. And you also want to be able to apply historical data in order to take the best advantage of what’s happening in real time in the market. This is especially important in the context of the machine learning algorithms that are being developed. Using the historical data to understand the patterns just so you can identify, given what you’re seeing in the market right now, what’s likely occurring, and how best to take advantage of it. The second pillar is around service level agreements (SLAs), and particularly around analytics. So you are moving the systems from the back end, where you have maybe a couple of backend analysts who are working with the data, to the front end, where the people working with the data are the end users or the portfolio managers or even the end customers. The bar for the experience goes up dramatically. As does the need for concurrency – the need to support many simultaneous users, including those backend analysts but also BI tools and apps, at the same time. You want interactive experiences that are snappy and responsive and allow you to get answers as quickly as possible. But to make that happen, you have to have SLAs on all the different dimensions of usage within the system, how fast the data is ingested into the system, how quickly you can query it out, how fast the storage is growing. You need SLAs across all those dimensions in order to guarantee a positive customer experience. And you need you to do that not just during the average load time, but you need to maintain those SLAs even at peak times. Think of when some momentous event, or series of events, happens in the financial markets. You know, think 2008, or even 2000, and everybody’s coming in to use the system and you’ve got ten times more users concurrently trying to run their queries and trying to hit the storage system, the database system. You want to maintain those facilities even in the face of that – perhaps especially in the face of that. And then to do that, you need a system that can scale depending on the load. And last, if you want a system that supports your operational standards, it should plug in with your existing tools so you can leverage all of the tool sets. This means robust support for ANSI SQL. And then of course, also the experience that your users have with those tools. So you don’t have to retrain all your users on how to operate and manage and optimize the system. The more you can leverage the tools that you have, the easier it is to plug it into the overall ecosystem. And it’s got to be a system that’s not just built for today’s problems, but also for where everyone’s headed. And the place people are headed these days is all into cloud, into the public cloud systems. So it can’t be a legacy system, especially those that are tied to legacy hardware, because they won’t go where you’re headed. And you want something that is highly available and scalable and available to meet these requirements. The Rise and Rise of Portfolio Analytics Portfolio analytics is a huge market. It’s $23 billion today, expected to grow to nearly double that in the next five years. What’s driving that is the combination of compliance, digitalization, and the drive to automate the backend processes. And as that happens it basically allows new opportunities in the market. It’s all coming from technological innovations that are happening in the fintech industry and providing a number of opportunities to go and take advantage of those technologies. Now more concretely, what are the problems that financial services companies are facing? One is the need to combat passive investment vehicles. Those could become the default that people gravitate to because they’re easier and lower cost. You’re also seeing more competition among the large asset managers; it’s said that the number of asset managers has gone up by something like 10x over the last 20 years. There are more people trying to do asset management, and they are all using similar kinds of tools to do it. And then, because the passive investment vehicles have come to be so dominant, it’s driving down the fee structures, which means financial services companies need to be more cost effective and more cost efficient in how they operate. (For more on how one large financial services company met these requirements by moving from Oracle Exadata to SingleStore, see our case study – Ed.) How SingleStore Powers Portfolio Analytics (and Digital Transformation Overall) Now let’s go into why is SingleStore, why is it so good for portfolio analytics? Why did we build SingleStore, and how does it serve the market? SingleStore is a cloud-native operational database built for speed and scale. But what we do is what we call operational analytics. So there are two common patterns in the database interchange. Two workloads that were the dominant patterns for a very long time. OLTP online transaction processing. Which is all about serving applications and being reliable and distributed and transactional and durable. And there’s OLAP – online analytical processing, also known as data warehousing. OLAP is all about serving analytical queries, complex joins, and aggregations. Separating the two is ETL – the extract, transform, and load process, to move data out of OLTP, remix it, and store it in OLAP. This means the OLAP world lived on stale data, with reporting and analytics mostly done, in an offline manner (despite the name), for people to make business decisions and for future plans. What operational analytics is, is it’s a third workflow that combines the requirements of the other two. So it has all the analytical requirements in terms of the complexity of the queries, and the need for joins and aggregations, and time series and window functions and all the stuff you’d expect from a data technology. Combined with the requirements around maintaining an SLA, so it needs the reliability and availability and scalability of the operational systems. And when you have requirements that are part of both those things, SingleStore is the best database hands down. So we boil this down into kind of a pithy statement of, when you need analytics with an SLA, SingleStore is the best fit. And this is the bread and butter of what we do with SingleStore and the kind of problems that we solve. And we’re also seeing more and more people who are moving into doing more predictive ML and AI use cases. And it turns out what you need to solve those problems is a system that can operationalize your model at scale with a combination of historical and real time data. And it turns out SingleStore is a great fit for that. And so we see people doing more and more work in this space and it’s really just sort of the evolution of operational analytics. A good example of this is we have a large bank in North America that’s built their credit card fraud system on top of SingleStore. And they chose it because the system they had before was too slow. And so they could catch the fraud, but only after it had happened – or even after it had happened several times. And then they would refund the money to the customer, but they would lose out. What they wanted was a system that could identify and stop the fraud in its tracks before it happened. And to do that, you need to be able to load data continuously and in real time. Leverage historical data, for example, the user’s past purchasing history on the credit card. And then combine that with what happened with the credit card right now. And then make an instant decision around whether or not they’ll let the charge go through. And by leveraging SingleStore, they are able to do that. And using a custom model they built, they were able to implement that and achieve their objectives. And the third pillar of what we do is around helping customers move from cloud and replace their legacy systems. Bust as customers, as everybody is moving to the public cloud, there’s a need to replace the legacy systems that won’t make it there. Either because they are using hardware that’s just not possible to take to the cloud. Or because they have legacy algorithms and technology and intellectual property that was built for slow-spinning disk 20 years ago and just are not applicable, or don’t make as much sense and don’t work as well in a cloud environment. And SingleStore has the advantage of being a modern system built with modern data structures that works very well running in cloud environments. And giving you the availability and reliability and scalability of a cloud system. Combined with a front end that is familiar and easy to use, because it looks just like a regular relational database. So really giving you the best of both worlds, making it easy to move from legacy systems to something more modern that will run in the environment so you need to run. So that’s what we do. And now who we do it for? As you may have surmised, finance is a top industry for us. Over half the top 10 banks in North America make use of SingleStore, and they do it not just for portfolio analytics, but for a number of other use cases like fraud that I mentioned, risk management and trade analytics and pretty much anything that fits that operational analytics workload. There are more and more use cases popping up all the time. We have a number of SingleStore customers in the media space, Comcast and Pandora are just a couple of them. As well as in the telco space, like Verizon. Or people are doing things like tracking the quality of video or audio streaming. As well as doing user behavior analysis in order to implement things like personalization. As well as setting ad tracking and delivering analytics to partners like your advertisers, for how the ads are performing. And the third key vertical for us is really in the high tech space. Everything from companies as big as Akamai and Uber making major investments to leverage SingleStore, to fast growing startups that need systems that can help maintain their growth as they’re building their customer-facing SaaS products. So the key problems that we solve are around speed and scale on SQL. So when you need speed, meaning bringing data in fast, pouring data out fast. Or performance matters, coupled with scale, or you need to be able to scale the system up as your usage and number of customers are growing. But you want to do it with a familiar interface that works with existing tools and technology and skillset of your people. When you have all three of those requirements, then SingleStore is the absolute best fit for you. Speed, scale, SQL, is how we describe our differentiation, the key pain points that we solve. And that’s what’s driving our business. Now in terms of how we fit into the overall ecosystem, you can think of us like a regular relational database, and that data comes in from a data source. That data source can be standard operational legacy databases, could be streaming data sources like Spark and Kafka as I mentioned. Or bulk loading from big data systems. And we have technologies that you bring data in from all those different sources regardless of the type or how you’re bringing it in. We have things like a CDC technology for bringing in data from operational systems. We have a feature called pipelines, so let’s you bring data in easily from Kafka and from data stores. And we have the ability to do transformations on the data so they can easily get it into the right shape before you put into the database. Now when you put it in the database, we have two different storage engines. We have in-memory rowstore and a disk-based columnstore. We find customers tend to use a mix of the two. The rowstore tables are particularly good for hot data that needs to have a strict SLA, specifically when you do transactions or seeks. And the columnstore is much better for analytics and aggregation and the more analytical queries. And usually customers have the combination of the two on their system. We support a number of different data types. So, whatever type of data you want to store. We can support it in SingleStore, obviously we handle the relational data, but we also have a native geospatial type. So you can easily index that and put it alongside of your relational data. We store a native JSON column type so you can store data in JSON or project out properties and easily index them and reference them within your SQL query. We support time series and key value patterns as well. And then we also support a full text index, so you can pull text index elements of the database and reference that in your queries. And whether you’re using third-party analytical tools, like Tableau or Looker, Zoomdata or MicroStrategy, SingleStore supports all those. But many of our customers tend to use third party tools and many tend to do custom applications, or a mix, based on their needs and how they’re operating. And of course in terms of how we run, you can run us on-premises, on bare metal. You can leverage VMs, or more recently, you can now leverage Kubernetes – we now have a Kubernetes operator . If you want to run us inside of a Kubernetes cluster on-prem or in the cloud, you can leverage our operator to easily deploy and manage SingleStore. And we run it across all the different cloud vendors – AWS, Google and Azure. We have customers who run us self-managed in all three. We also offer a managed service. If you don’t want to manage SingleStore yourself, we’ll do it for you. How do we do all this? We do this through a set of features around having scalable SQL, so we have full ACID support. Meaning we support transactions and rollbacks combined with full ANSI SQL. So pretty much any SQL functions you would want supported in that SQL. And, as I mentioned, we have the full set of data types. Whether you’re storing documents or JSON, geospatial or full tech search, it’s all natively supported within the system. We have fast ingest, and we have standard bulk load API, supporting the MySQL wire protocol. You can load data using files in CSV or other data types, other formats, bulk data is easily bulk loaded into the system. We have a native parallel stream ingest feature called Pipelines. It’s a first class object in the system. So you can easily load data in with the simple statement, CREATE PIPELINE. You point it at the Kafka queue or an S3 bucket or any of your favorite storage mechanisms and it immediately starts loading the data in a parallel. And we’re able to do this loading and queries simultaneously, because of the lock-free semantics within our system. Like I mentioned, we do use a different data structure under the covers that allows us to do this in a way that the legacy databases cannot. Core Technologies that Power SingleStore Include MVCC, Skiplists Pretty much all legacy databases were built on top of a technology called a b-tree. This was great in the days of slow spinning discs, when we needed to bring data back in chunks. But it came with certain locking semantics that made it very difficult to run a data query at the same time. It wasn’t a requirement in the original days or the early database. SingleStore is built with a data structure called a skiplist . And with a skiplist, you can build them in a lock-free way. And that, combined with multi-version concurrency control (MVCC), our concurrency control system, allows us to be able to load data and to be able to query it simultaneously while it’s being loaded. This allows you to be streaming data in constantly, without blocking the system and preventing you from running queries. And this was one of the key underlying mechanisms that allows us to do what we do in a way that no one else can. Under the covers, we’re a distributed, shared-nothing, massively parallel system. So when you put your data in the system it’s transparently charted across a number of modes. If you need more power, you can just add more nodes to the system. The system will automatically rebalanced the data, transparent to the developer or to the application. We do all this on commodity hardware. We don’t require any special hardware, you just need any machine that meets the minimum number of cores in memory and supports a modern version of Linux, you can be up and running with SingleStore. We also provide high availability (HA), so it’s highly available, with transparent failover. So we’ll keep two copies of your data on two different machines at any given time. So if one of the machines dies, we transparently fail over and the application doesn’t even really notice. And we’re-cloud native. We sell software that you can deploy on-premises, and you can deploy us on any of the major cloud vendors. As I mentioned, we now have a Kubernetes operator, so that you can a run us in a Kubernetes cluster. So wherever you need to deploy we can make that work for you. And that’s it. So at this point that’s the over view of SingleStore. Hope you learned something interesting. I’m happy to take questions. Q&A Q. What are some of the other use cases SingleStore is solving in financial services? Great question. I think I may have touched on this earlier, but the portfolio analytics is definitely one of the primary ones. We also do things like trade analytics. So we have a large investment bank that wants to track each step in a trade process and identify any bottlenecks. So if there’s any bottlenecks they can immediately respond and go fix them, to make sure the trades are flowing as quickly as possible. So it’s something that they modeled within SingleStore, and they actually tried a number of different technologies. SingleStore is the only one that was able to meet their requirements for the number of updates per second they had to do. We’ve had a couple of other customers who’ve also implemented trade analytics that way. Risk management is another use case where you want to keep track of how much risk you’ve taken on during the day. In the past, often larger, especially investment banks can only tell how much risk they had taken on by doing a report at the end of the day. They try to leave quite a bit of buffer in order to make sure they didn’t take on too much risk and then violate the FTC rules. And so by having that available to them in real time, they can be much more specific and precise about how much risk they’re taking on. And not have to leave any opportunities on the table because they’re in fear of possibly going over a line that they might not be actually going over. And then one of the other use cases I mentioned was fraud. We actually have a fair amount of fraud use cases where people are tracking down either credit card fraud or other forms of fraud. But yeah, we have more use cases popping up all the time. Q. Can we get some help if we want to do a proof of concept with SingleStore? Absolutely. We have a number of highly trained sales engineers. We’ll be happy to help you get up and running and give you sort of the best practices. And help make sure that your system is optimally tuned to achieve the performance and scale you require. Q. What’s coming in the future for SingleStore? Great question. So we have a lot of good stuff, but there’s a lot more to go build. So we’re making additional investments and more on the transactional side. Particularly on recovery from disaster. Today we support backups and the backups are now online operations. So you can run that and it won’t block your system. There are full backups. So we’ve had a number of requests, like can we do incremental backups. Which allow you to run backup more often, reduce your RPO. So that’s a feature going to be coming soon. As well as things like point in time restore. So you can restore back to a particular point in time, again to try to reduce that RPO to as little as possible. We’re making efforts around simplifying the choices people have to make. So I mentioned we have a rowstore and a columnstore. And so having both of those in one system, with SQL queries able to span the two types of tables, is a huge innovation, and hugely valuable to customers. But it then presents a decision point when you’re designing your app around, “Hey, how do I decide whether I should put this data in rows or in columns? Make this table rowstore or columnstore?” And so we’re working on merging the rowstore and columnstore technologies together in something we call Universal Storage, to allow you to just have one table type, say CREATE TABLE, and you don’t have to think about how the data is stored underneath. SingleStore takes care of that and organizes it for you. And then the third pillar of investment is around the managed service, which I mentioned is currently a private preview. But we’ll be taking that into public preview soon. And we’re making investments to automate the management to the system to make it easy for you to be able to just spin up a cluster and you use it in SingleStore. Without having to worry about the physical management and troubleshooting characteristic. We invite you to learn more about SingleStore at www.singlestore.com , or give us a try for free it singlestore.com/free .", "date": "2019-08-24"},
{"website": "Single-Store", "title": "case-study-fraud-detection-on-the-swipe", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/case-study-fraud-detection-on-the-swipe/", "abstract": "This case study was originally presented as part of a webinar session by Mike Boyarski, Sr. Director of Product Marketing at SingleStore. It’s been updated to include additional information and references. In the webinar, which you can view here , and access the slides here , Mike describes the challenges facing financial services institutions which have decades’ worth of accumulated technology solutions – and which need to evolve their infrastructure immediately to meet today’s needs. In this case study, which was also described in the webinar, Mike shows how a major US bank created a new streaming data architecture with SingleStore at its core. Using SingleStore enabled them to move from overnight, batch fraud detection to fraud detection “on the swipe,” applying machine learning models in real time. He presents a reference architecture that can be used for similar use cases, in financial services and beyond. This case study presents a reference architecture that can be used by leading retail banks and credit card issuers to fight fraud in real time, or adapted for many other real-time analytics use cases as well. In addition, it describes how SingleStore gives fraud detection services an edge by delivering a high-performing data platform that enables faster ingest, real-time scoring, and rapid response to a broader set of events. A similar architecture is being used by other SingleStore customers in financial services, as described in our Areeba case study . This case study was originally presented as part of our webinar series, How Data Innovation is Transforming Banking (click the link to access the entire series of webinars and slides). This series includes several webinars, described in these three blog posts: Real-Time Fraud Detection for an Improved Customer Experience Providing Better Wealth Management with Real-Time Data Modernizing Portfolio Analytics for Reduced Risk and Better Performance Also included are these two case studies: Replacing Exadata with SingleStore to Power Portfolio Analytics Machine Learning and Fraud Detection “On the Swipe” For Major US Bank (this case study) You can also read about SingleStore’s work in financial services – including use cases and reference architectures that are applicable across industries – in SingleStore’s Financial Services Solutions Guide . If you’d like to request a printed and bound copy, contact SingleStore . Real-Time Fraud Case Study This application is a credit card solution. The SingleStore customer was looking to deliver a high-performance, agile fraud detection platform using standard SQL, with challenging performance requirements. And so I’ll talk about what that means around agility and some of the sort of performance demands they have. The customer has a time budget of one second from the time the card is swiped to the approval or refusal. There’s a very sort of sophisticated set of queries that need to be run in a very short window of time. They have about a 50 millisecond budget to work with to run a number of queries. In this application they are looking at about a 70-value feature record. And so we’ll spend a little bit of time on how that looks. Processing starts with a request, which is a transaction at a terminal or a point of sale system. The request hits, and that event is collected into the bank’s online transaction processing (OLTP) application. That’s a transactional operational database that is collecting that information. And that request is then converted by that OLTP app into a number of disparate queries. There are various models that they have to identify this event and match it against a number of other activities that may have occurred over time. And in this application, again, it’s roughly 70 queries that are being run. And so it’s running events like trying to identify engagement between this customer and that vendor over the past days, months, and years. They’re trying to identify a trend around that customer and that merchant. They’re also looking at other activity, like geolocation event information about prior sort of transactions and the location that the event is taking place in. Without listing every single query, there is a distinct set of queries that are being run all in parallel against a reference store, which is on SingleStore. SingleStore is the real-time operational data store. And so this is all about delivering a check against a fairly sophisticated model to do a score. And so the data is analyzed in SingleStore. We’ll score against the 70 odd queries and provide essentially, at the end of the day, a yes or no against that most recent transaction. And so that scoring service can occur within the timeframe that they required, which is around a 50 millisecond window. And what, ultimately, this particular customer was looking to do was get to more agility so that they can add even more feature extraction queries over time, so that can continue to optimize their model using continuous insights from their data. Think about this as a continuously moving model that needs to adjust to the insights that they’re gaining on the different and new fraud prevention techniques that some of their customers are taking advantage of. And so when you compare and contrast the previous solution that they were using before SingleStore, they were taking advantage of nightly batch jobs and accumulating these feature records for each customer, doing analysis overnight and trying to identify if the score in fact was correct or not. As any of you know who’s ever lost their credit card or debit card details, a fraudster can accumulate multiple charges in a day, if the check doesn’t occur until nighttime. So fraud events were getting through the system, resulting in lost revenue. And also, the other challenge that they had was they couldn’t easily change their fraud model. So their iterating of features and adding new queries was very slow in their sort of update process. When they could identify a profile or fingerprint of fraudulent activity, it took them, in some cases, many weeks to make an update to the system in order to catch that source or type of fraud on an ongoing basis or a go-forward basis. SingleStore’s Advantages for Real-Time Fraud Detection As a result, they had a number of reasons for moving to SingleStore. One was that they wanted to get to a fairly sophisticated number of queries running concurrently, all within this 50 millisecond window. That was something that was not possible for them with the previous system. That system was inconsistent, so they weren’t getting reliable performance. What they got from SingleStore that they are really excited about is the ability to add new features to their model scores using standard SQL, and do that an a more iterative basis. So this was giving them the flexibility to do further updates to their platform without having to re-engineer the system or wait for a lengthy sort of change management process that was part of their prior system. And a lot of the issues with the previous system had to do with the fact that they were using some technologies that were non-standard, meaning non-SQL or non-relational based. And so, ultimately what they are able to model out was that this continuous improvement and this real-time sort of refinement was going to save them literally tens to potentially hundreds of millions of dollars and sort of lost fraud events. So for them, it’s all about getting more agile with their fraud detection platform using standard SQL, getting the great performance so that their customers don’t notice any disruption in their experience and service, and then of course saving money and making money from that more advanced service. SingleStore Overview SingleStore is used by a lot of banks, because a lot of banks like the performance of SingleStore. They like the familiar relational SQL. And we typically see us beating out the competition on a price/performance basis on a regular basis. This diagram sums up SingleStore’s features. We jokingly call it a “markitecture” diagram, because it sums up our selling points in a form that relates to a lot of the reference architectures we derive from customer implementations of SingleStore. Our claim to fame is around delivering speed, scale, and SQL, all in one package. So of course most databases will say they’re fast, but I would argue that SingleStore is probably the world’s fastest database, because of our ability to really optimize the entire pipeline of data to the platform. So that includes ingestion. We have a lock-free architecture, so that means we can handle streaming events from Kafka or custom application logic and/or change data capture (CDC) logic. So just think of our system as being able to efficiently take data into the platform and then run queries on that data as fast as any data warehouse product in the market. That includes the great legacy platforms like Teradata and Vertica and others, and also some of the newer cloud-based data warehouses. We are fast, we have a strong columnstore engine that’s disk-based, along with a strong rowstore engine that runs in memory. I’ll talk a little bit about our underpinnings and our architecture in a moment, but it’s all about speed, getting data in, and then once the data has landed, running those queries as quickly as possible. I mentioned earlier about SingleStore’s scale, which is powered by our distributed scale-out architecture. It’s a node-based, shared-nothing architecture. That’s what makes SingleStore really, really fast. And we believe strongly in relational SQL, because we think that’s the easiest way to get data out of your system. It works really well with existing tools. But also, more importantly, it works with the skill set that already exists inside most organizations. In terms of our architecture and ecosystem support, as you can see on the left, we can ingest data from a variety of sources: Kafka, Hadoop/HDFS, AWS S3, Spark, and custom file systems. So our ingestion technology is top notch, and that’s another reason why a lot of customers, mostly banks, really like our platform. It works very well for those types of high-ingest environments. And then once data lands into SingleStore, we have two different storage engines. You have an in-memory based rowstore that’s really fast for point look-ups, transactions, and is fully ACID compliant. And of course we also have a columnstore engine that looks and feels like a traditional data warehouse. So great compression, it can query and do fast aggregate queries. We did a performance test on a trillion row scan in a second. So it’s very, very fast for data warehousing type jobs as well. And SingleStore has all the flexible data types that you would expect of a modern database, whether it’s JSON, whether it’s relational structure, key-value, time series, etc. Our deployment flexibility is mostly based on our Kubernetes and container support. You can run us on anything, whether that’s in the cloud or on your own, on-premises infrastructure. Of course, if you want to run it on bare metal or any other Linux environment, you can do that as well. Lastly, getting into a little bit more depth on our architecture, we are fully ACID compliant. That means that transactions can be committed, guaranteed, and logged to a disc. We treat every event as its own entity. SingleStore is fully ANSI SQL-compliant, and we have all of the flexible data type support that you would expect of a modern database. SingleStore’s ingestion is, again, world-class. We can do parallel stream ingest, we can ingest directly to rowstore, or columnstore, or both. It depends on your application needs. And that’s ultimately why customers really like the platform, is it gives you the flexibility to determine what’s the best outcome, the best process flow to get the SLA result that you need. We are a shared-nothing, massively parallel, highly concurrent database. Which means that, if you’ve got lots of users accessing your system or lots of data ingestion points coming into the system, concurrency for our platform, unlike most other platforms, is really not a challenge. That wraps up the story of how SingleStore is being applied to real-time fraud detection. While there’s a lot of depth that can be gone into around the types of functions and queries that are required of a fraud detection application, our goal here today was to give you a starting point to understand how we fit for this real-time fraud application. We invite you to learn more about SingleStore at SingleStore.com , or give us a try for free at SingleStore.com/free .", "date": "2019-08-25"},
{"website": "Single-Store", "title": "memsql-data-backbone-machine-learning-and-ai", "author": ["Nikita Shamgunov"], "link": "https://www.singlestore.com/blog/memsql-data-backbone-machine-learning-and-ai/", "abstract": "SingleStore co-founder and co-CEO Nikita Shamgunov gave the keynote address and a session talk at the AI Data Science Summit in Jerusalem earlier this summer. The session talk, presented here, describes the use of SingleStore as a data backbone for machine learning and AI. In this talk, he gives a demonstration of the power of SingleStore for AI, describes how SingleStore relates to a standard ML and AI development workflow, and answers audience questions. This blog post is adapted from the video for Nikita’s talk, titled Powering Real-Time AI Applications with SingleStore . – Ed. Demonstrating the Speed of SingleStore for AI Applications SingleStore allows you to deal with large datasets. And here, on my laptop, running Linux, I am going to show you a table here using this wonderful tool which we call SingleStore Studio. I’m going to show you a table that has 1.7 billion records. So I’m storing 1.7 billion records on my laptop. ( Note 1761516864 highlighted in SingleStore Studio – Ed. ) How many of you know what SQL is, and how to use SQL? ( Most of the audience raises their hands. – Ed. ) That’s very good. That’s very convenient, and I always ask this question because usually I get a very positive response. So people in general know SQL, and in a data science world, people often start with SQL to pull data into their own tools or frameworks because SQL is so convenient to slice and dice data before you move it somewhere else. So again, to level set of what SingleStore is before we go into the actual application, the system is a relational database. It stores data in an incredibly compressed format. So here I’m storing 1.7 billion records on my laptop. The whole thing takes 11 gigabytes. The kind of data here is somewhat typical, so data I collected from the telemetry from my own laptop. So I used a tool called Sysdig, and it starts telling all that information … telemetry that is happening in my own laptop. All the system calls, temperature, all the various telemetry and data points and events that are happening on my laptop. So obviously, you can run machine learning models to start predicting what’s going to happen next and there are startups in San Francisco that are doing exactly that for monitoring very large data centers, for monitoring Kubernetes containers and whatnot. So I captured that data and I captured that data in SingleStore, loaded it. And the trick is that you can load that data in real time, so you can connect to anything that emits data. You can connect to stream processing systems like Kafka, and the data will just start flowing into SingleStore. And you can assume that SingleStore is completely limitless, right? So you can land as much data in SingleStore as you want. But the data is not just stored in some sort of cold storage. If you want to get an individual data point out of the data, you can retrieve that data in an incredibly fast way. So those who work with S3, Hadoop… If you want to take one individual record stored on HDFS in a Parquet file and you want to get it back, some sort of big gears need to shift before you get this record back. Well, in this case, let’s just try to fetch a record here, out of SingleStore, out of my laptop. I can put limit one to get one record and stuff comes back in two milliseconds. And what that means is you can actually … once you deploy those models, right? And then when you deploy models into production, usually it’s the combination of intelligence that’s delivered through a model and some source that serves data to the application, you put them together and you can achieve incredibly low latencies eventually delivered to what people recently called smart apps. So that’s what SingleStore is. Typically, in data science there are multiple parts of the workflow, as you put systems into production, when you need to extract individual data points out of the system, you can do it with incredibly low latency. People do it for fraud detection all the time. You swipe a credit card, you need to understand if it’s a fraudulent transaction, or you’re walking into a building, you’re doing face recognition. That’s the thing … it has to be a real time system. Or you want to extract data from the system and slice and dice it. And I’ll give you an example of that as well. In this query I’m going to scan the whole 1.7 billion records and understand some basic information. So group, group, group those 1.7 billion records by this field that’s called GID. And that is actually happening also very, very quickly, in 1.2 seconds. In 1.2 seconds we processed a billion records here. So now, why this matters is because the system is incredibly convenient in becoming the backbone of where you want to land data, where you want to do feature engineering, where you want to expose that data set to your team. It’s very lightweight. It can run on your laptop and it’s free to run on your laptop . And then you can open up your laptop and put it on the server, put it in the cloud, scale it, and then go from analytics and data science to pixels that are displayed on people’s apps on the website in an incredibly efficient way. Q. What is SingleStore focused on here? A. SingleStore is focused on both fetching data and updating data. So here let’s go and update something. There’s an update statement. I can run it and it’s instant. And the idea is you can build apps. It’s a transactional database. And usually when you have apps you want to insert, update, delete, aggregate, produce reports, deliver analytics back to the application. And we give you basically infinite compute. We have this mechanism of locking your record. And in fact, we only lock on writes, so you will only have conflict on writes, and reads never take locks. So that makes it incredibly convenient to build apps because what people hate is when apps start to spin. There’s a spinning, like a wheel, on the app. Very often, when we win business from large companies, the definition of success is no spin. Q. What’s the drawback? A. What’s the drawback? Well, humbly, there are no drawbacks. So I think the only drawback is that it is in fact a distributed system so you need to use a distributed system versus sometimes the workloads are such that they fit on a small system and that they fit on a small system, it’s no better. So you’re kind of wasting that ability to scale. ( The advantages of SingleStore are not evident at small scale – Ed. ) Using SingleStore for Machine Learning and AI Workflow Q. What about the workflow for AI and ML? ( This refers to the ML and AI workflow Nikita showed during his keynote address on data infrastructure for ML and AI at the same conference. – Ed. ) A. So the question is … So what’s the workflow? And when do you build a model? So typically, this is kind of a very high level definition of the workflow. You define machine learning use cases. You define ML use cases, you do some data explorations. So you don’t need SingleStore to define ML use cases, but when you want to do data exploration, typically, people want to play with the data. And once you push data into SingleStore, it’s very, very efficient. You can explore data yourself by running SQL statements directly against the database. You can attach a business intelligence (BI) tool. You can visualize it, you can attach a notebook. Now you’ll be pulling data into Python or Spark, and SingleStore integrates and works with Spark incredibly well. So you can play with the data. Q. Where can I get the connector? A. The connector, it’s on GitHub, so you download the SingleStore connector, and what we do is we give you very fast data exchange between SingleStore and Spark data frames. And that’s what people use all the time, where SingleStore gives you basically storage and compute with SQL, and Spark gives you training and ad hoc analysis, ATL, feature engineering. And what people do is they create a data frame and they call “save to SingleStore” on a data frame and the data drops in there and it’s reliable, transactional, all those good things. Data pipeline. ( Refers to step 4., Data pipeline and feature engineering, in the workflow diagram. – Ed. ) So everywhere I go into modern organizations, there are data pipelines. You take data, extract it from multiple systems, and oftentimes people use Spark. Sophisticated customers and some of them are here in Israel, use change data capture (CDC) tools when they pull data from relational databases, MySQL, Oracle, massage the data and push it into SingleStore. So we spent years and put a lot of work into making that process very, very simple. And one of the most common patterns with data pipelines … usually there’s Kafka somewhere and once data is in Kafka, you can put in data into SingleStore at the click of a button. You say, “Create pipeline,” point at Kafka, data’s flowing in. Build ML model. ( Refers to step 5., Build ML model, in the workflow diagram. – Ed. ) So this is what you’re going to do using Python packages, and then people iterate, right? Obviously, there’s a lot going on when you develop a model and you think about a million things, but once you’re kind of out and proud with a model, you want to present results and you want to deploy that model. And typically, you deploy a model and then you run in some sort of parallel environment to make sure that you don’t screw up. And really depending on the use case, right? In some cases, the bar for quality is low and we have some customers that perform fraud detection on electricity IoT data such that, you know, “This household spent more on electricity than last month.” Okay, we want to look at that. Well, that’s not very sophisticated. And then anything you do there will improve their quality of fraud detection dramatically. And then we have customers that do financial analysis, risk analysis, understanding risk of an individual loan. That’s where the bar is very, very high because you improve the model by a little bit and then you’re saving millions of dollars. Then you plan for deployment and then you operationalize the model. ( Refers to step 8., Plan for deployment, and step 9., Operationalize model, in the workflow diagram. – Ed. ) And so that’s where some people deconstruct that model, and let’s say they do image recognition … And I showed you that video in the keynote . Maybe I’ll show that again here. So in this video, you can point a smartphone at anything and it will go and find that that’s an item in the catalog. So it does it in a very, very fluid way and it allows people to compare prices, efficiently shop. Or you’re talking to a friend and you want to buy the same thing that the friend has and whatnot. So in this case, they took a model and they deconstructed that model and they expressed that model in feature vectors and used basic operations that we offer in SingleStore, such as vector dot products, Euclidean distance, to run a very simple query. These are the primitive operations that we offer in SingleStore, so if you store feature vectors in a table, now, using a scale-out kind of limitless compute, you can run a query that will scan all the records in that table, compute dot product against the feature vector which you got from an image with all the feature vectors already there. Well, I’m not going to explain you what a dot product is, but basically running that query where all the feature vectors are greater than 0.9, that’s your similarity search. SingleStore’s Future for Machine Learning and AI Now, the advantage of doing this in a database is that the actual model … in this case, incredibly primitive … but co-locating the model and the data opens up all the possibilities that you can do. And now, what we’re working on now is the ability to push TensorFlow and Caffe and PyTorch models directly into SingleStore. And with that, you’re able to run those models in production, inside the database, right next to the data and deliver great user experiences by building smart apps. Final Q&A and Conclusion Q. How do transactions work for Spark? Very good question. So the way transactions work in SingleStore, as a transactional system, everything that’s between begin transaction and commit is atomic. So with Spark, it’s no different. If you take a data frame and you save to SingleStore, this data frame drops into SingleStore in a transactional way. So until every record makes it from the data frame, makes it into SingleStore, nobody else sees that data. There are transaction boundaries around “Save to SingleStore.” In the case of Kafka, we go micro-batch to micro-batch and the transaction boundaries are around the micro-batch. And in fact, if there is a failure, then if a micro-batch … So we will never persist half of a micro-batch. So each micro batch is going to be delivered to SingleStore as a whole, which gives you exactly-once semantics from Kafka. Q. Do I use SingleStore for training my model? Yeah. Training is done outside of SingleStore, so we do not support training, but we can be the data backbone for training, and if training needs data and queries, SingleStore periodically – that’s a perfect low-latency solution for that. All right. Thank you. You can download and run SingleStore for free or contact SingleStore – Ed.", "date": "2019-08-31"},
{"website": "Single-Store", "title": "why-do-banks-need-real-time-transaction-processing", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/why-do-banks-need-real-time-transaction-processing/", "abstract": "A new report from RT Insights describes the benefits of real-time transaction processing in banking and financial services and shows how traditional database architectures interfere with real-time data movement. In order to get the benefits of real-time transaction processing, such as improved portfolio management, fast credit card fraud and acceptance checks, and others, banks and other financial services institutions need to use a translytical database, combining the best of transaction and analytical data processing capabilities in a single, fast, scalable system. A Real-Time Database for Banking What is a real-time database? And why would you need one for banking and financial services companies ? A real-time database is a database that can support real-time processing. According to Wikipedia (as of the publication date), “ Real-time processing means that a transaction is processed fast enough for the result to come back and be acted on right away.” That certainly sounds like something banks could use – when you go to the ATM machine, or use a credit card, or apply for a home loan, you certainly want the systems you’re using to return the right answers, right away. (These functions are also good examples of the use of machine learning in financial services, another SingleStore specialty.) Indeed, accounting and banking are two of the areas where real-time databases are said to be most useful. The RTi report cites many important applications for “faster and more intelligent decision-making”: fraud monitoring; dynamic portfolio analysis; regulatory compliance; and protection from cyberthreats. What Kind of Database Can Be Real-Time? Traditional data processing depends on databases that seem designed, not to enable, but to keep data from being real-time. These databases are not scalable, so they’re limited to the capabilities of a single machine. In order to make the most of what one machine can do, transactions are handled on specific database type, called online transaction processing (OLTP). Then, the OLTP system is tied up for a while so a specialized process, extract, transform, and load (ETL), can copy data off it. The data is then remixed with other in-house and outside data, reformatted for faster analytics performance, and moved to a different kind of database for analytics, broadly called online analytics processing (OLAP). https://www.predictiveanalyticstoday.com/top-free-extract-transform-load-etl-software/ Different kinds of analytics databases exist; data warehouses have specialized tools for slicing and dicing data, while operational analytics databases are better suited for supporting applications, such as a mapping or ride hailing app on your phone. Even NoSQL gets in the act, with data lakes used for data science queries and even for business intelligence (BI) tools, though the fit there is not very strong. The movement of data from ingest, to OLTP, through ETL, to OLAP can take many hours and even days – far from real-time. So the RTI report puts forward translytical databases , which combine transactional and analytical capabilities, as the right place to look for an answer. A translytical database is both fast and scalable – when a workload is too much for a single server to support, a second server can be added, extending the processing power, RAM, and disk space available for the stored data. By combining both functions into a single database, eliminating the intermediate steps inherent in the OLTP/ETL/OLAP split, the translytical database can serve as a real-time database, supporting crucial applications in banking and financial services. Additional Benefits of a Real-Time Database for Financial Services Organizations are so used to traditional, siloed data structures that they don’t see some of the hidden costs involved – costs that are removed when slow-moving data becomes real-time data. Here are some of the benefits that banks and other financial services organizations receive when they move to real-time transaction processing: Improved customer experience. When real-time data becomes the expectation, improvements appear in all the different ways that a customer interacts with a financial services institution, from using a credit card to managing an investment portfolio. Better risk management. For regulatory reasons, financial services organizations must measure, and manage, the riskiness of customer portfolios, requiring a move to real-time transaction processing for a large part of what a bank does. It’s then relatively easy for the organization to offer real-time interactivity to customers. Creation of a “data culture.” By making real-time data the default, organizations can always be managing risks, always be in compliance with regulations, and always be offering the best possible services and support to their customers. To fully achieve this, an organization must move to having a “data culture” right across all functions and all levels, spurring even closer involvement with live data. Get Your Copy of the Report You can download and read the RTi report at no charge, giving you the chance to learn about real-time transaction processing and its benefits for banking and financial services. Get your copy today!", "date": "2019-08-31"},
{"website": "Single-Store", "title": "a-technical-introduction-to-memsql", "author": ["John Sherwood"], "link": "https://www.singlestore.com/blog/a-technical-introduction-to-memsql/", "abstract": "John Sherwood, a senior engineer at SingleStore on the query optimizer team, presented at SingleStore’s Engineering Open House in our Seattle offices last month. He gave a technical introduction to the SingleStore database, including its support for in-memory rowstore tables and disk-backed columnstore tables, its SQL support and MySQL wire protocol compatibility, and how aggregator and leaf nodes interact to store data and answer queries simultaneously, scalably, and with low latencies. He also went into detail about code generation for queries and query execution. Following is a lightly edited transcript of John’s talk. – Ed. This is a brief technical backgrounder on SingleStore, our features, our architecture, and so on. SingleStore: we exist. Very important first point. We have about 50 engineers scattered across our San Francisco and Seattle offices for the most part, but also a various set of offices across the rest of the country and the world. With any company, and especially a database company, there is the question of why do we specifically exist? There’s absolutely no shortage of database products out there, as probably many of you could attest from your own companies. Scale-out is of course a bare minimum these days, but the primary feature of SingleStore has traditionally been the in-memory rowstore which allows us to circumvent many of the issues that arise with disk-based databases. Along the way, we’ve added columnstore, with several of its own unique features, and of course you’re presented all this functionality through a MySQL wire protocol-compatible interface. The rowstore requires that all the data can fit in main memory. By completely avoiding disk IO, we were able to make use of a variety of techniques to speed up the execution, with minimal principal latencies. The columnstore is able to leverage coding techniques that – with code generation and modern hardware – allow for incredibly fast scans. The general market we find ourselves in is: companies who have large, shifting datasets, who are looking for very fast answers, ideally with minimal changes in latency, as well as those who have large historical data sets, who want very quick, efficient queries. So, from 20,000 feet as mentioned, we scale out as well as up. At the very highest level, our cluster is made up of two kinds of nodes, leaves and aggregators. Leaves actually store data, while aggregators coordinate the data manipulation language (DML). There’s a single aggregator which we call the master aggregator – actually, in our codebase, we call it the Supreme Leader – which is actually responsible for coordinating the data definition language (DDL) and is the closest thing we have to Hadoop-style named namenode, et cetera that actually runs our cluster. As mentioned, the interface at SingleStore is MySQL compatible with extensions and our basic idiom remains the same: database, tables, rows. The most immediate nuance is that our underlying system will automatically break a logical database into multiple physical partitions, each of which is visible on the actual leaf. While we are provisionally willing to shard data without regard to what the user gives us, we much prefer it if you actually use a shard key which allows us to set up convenient joins, et cetera, for actual exploration of data. The aggregator then is responsible for formulating query plans, bridging out across leaves as necessary to service the DML. Of particular note is that the engine that we use is able to have leaves perform computations with the same full amount of functionality that the aggregator itself can perform, which allows us to perform many worthwhile optimizations across the cluster. A quick, more visual example will better show what I’m talking about. Here we have an example cluster. We have a single master aggregator and three leaf nodes. A user has given us the very imaginatively named database “db” which we’re supposed to create. Immediately the aggregator’s job is to stripe this into multiple sub-databases, here shown as db _ 0 through db _ 2. In practice, we find that a database per physical core on the host works best, it allows parallelization and so on, but drawing out 48 of these boxes per would probably be a little bit much. So beyond just creating the database, as mentioned, we have a job as a database to persist data. And as running on a single host does not get you very far in the modern world. And so, we have replication. We do this by database partition, replicating data from each leaf to a chosen replica. So as you can see here, we’ve created a cluster such that there is no single point of failure. If a node goes down, such as this leaf mastering db _ 2, the other leaf that currently masters db _ 0 will be promoted, step up, and start new serving data. I’d also note that while I’m kind of hand waving a lot of things, all this does take place under a very heavy, two phase commit sort of thing. Such that we do handle failures properly, but for hopefully obvious reasons, I’m not going to go there. So in a very basic example, let’s say a user is actually querying this cluster. As mentioned, they talked to the master aggregator that’s shown as the logical database, db as mentioned, which they treat as just any other data. The master aggregator in this case is going to have to fan out across all the leaves, query them individually and merge the results. One thing that I will note here, is that I mentioned that we can actually perform computations on the leaves, in a way that allows us not to do so on the master. Here we have an order-by clause, which we actually push down to each leaf. Perhaps there was actually an index on A that we take advantage of. Here the master aggregator will simply combine, merge, stream the results back. We can easily imagine that even for this trivial example, if each leaf is using its full storage for this table, the master aggregator (on homogenous hardware at least) will not be able to do a full quick sort, whatever you want to use, and actually sort all the data without spooling. And so even this trivial example shows how our distributed architecture allows faster speeds. Before I move on, here’s an example of inserts. Here, as with point lookups and so on in the DML, we’re able to say the exact leaf that owns this row across its object. So here we talk to a single leaf end up transparently without the master aggregator necessarily knowing about it. Replicates that down to db _ 1’s replica on the other host. Allowing us to have durability, replication, all that good stuff. Again, as a database, we are actually persisting everything in the data that has been entrusted to us. We kind of nuance between durability to the actual persistence of a single host versus replication across multiple hosts. Like many databases, the strategy that we use for this is a streaming write-ahead-log which allows us to rephrase the problem from, “How do I stream transactions across the cluster?” to simply, “How do I actually replicate pages in an ordered log across multiple hosts?” As mentioned, this works at the database level, which means that there’s no actual concept of a schema, of the actual transactions themselves, or the row data. All that happens is that this storage layer is responsible for replicating these pages, the contents of which it is entirely agnostic to. The other large feature of SingleStore is its code generation. Essentially the classic way for a database to work is injecting in what we would call in the C++ world, virtual functions. The idea that in the common case, you might have an operator comparing a field of a row to a constant value. In a normal database you might inject an operator class that has a constant value, do a virtual function lookup to actually check that, and we go on with our lives. The nuance here is in a couple of ways this is suboptimal. First being that if we’re using a function pointer, a function call, we’re not in-line. And the second is simply that in making a function call, we’re having to dynamically look it up. Code generation on the other hand allows us to make those decisions beforehand, well before anything actually executes. This allows us both to make these basic optimizations where we could say, “this common case any engine would have – just optimize around it,” but also allows us to do very complex things outside of queries in a kind of precognitive way. An impressive thing for most when they look through our code base is is just the amount of metadata we collect. We have huge amounts of data on various columns, on the tables and databases, and everything else. And at runtime if we were to attempt to read this, look at it, make decisions on it, we would be hopelessly slow. But instead, by using code generation, we’re able to make all the decisions up front, efficiently generate code and go on with our lives without having runtime costs. A huge lever for us is the fact that we use an LLVM toolchain underneath the hood, such that by generating IR – intermediate representation – LLVM, we can actually take advantage of the entire tool chain they’ve built up. In fact the same toolchain that we all love – or we would love if we actually used it here for our main code base – to use in our day to day lives. We get all those advantages: function inlining, loop unrolling vectorization, and so on. And so between those two features we have the ability to build a next generation, amazing, streaming database.", "date": "2019-08-31"},
{"website": "Single-Store", "title": "create-pipeline-kafka", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/create-pipeline-kafka/", "abstract": "In this presentation, recorded shortly after SingleStore introduced SingleStore Pipelines, two SingleStore engineers describe SingleStore’s underlying architecture and how it matches up perfectly to Kafka, including in the areas of scalability and exactly-once updates. The discussion includes specific SQL commands used to interface SingleStore to Kafka, unleashing a great deal of processing power from both technologies. In the video , the SingleStore people go on to describe how to try this on your own laptop, with free-to-use SingleStore software. Introduction to SingleStore: Carl Sverre I want to start with a question. What is SingleStore? It’s really important to understand the underpinnings of what makes Pipeline so great, which is our SingleStore distributed SQL engine. There are three main areas of SingleStore that I want to talk about really briefly. The first area of SingleStore is that we’re a scalable SQL database. So if you’re familiar with MySQL, Postgres, Oracle, SQL Server, a lot of our really awesome competitors, we are really similar. If you’re used to their syntax, you can get up and running with SingleStore really easily, especially if you use MySQL. We actually have followed their syntax very similarly, and so if you already used MySQL, you can pretty much drop in SingleStore in place, and it just works. So, familiar syntax, scalable SQL database. What makes us scalable, really briefly? Well, we’re a distributed system. We scale out on commodity hardware, which means you can run us in your favorite cloud provider, you can run us on-premises and it generally just works. It’s super, super fast, as you can see – really, really fast and it’s just fun. So without further ado, I want to get into a little bit of the technical details that are behind what makes us such a great database. In SingleStore, we have two primary roles in the cluster. So if you think about a collection of Linux machines, we have some aggregators and some leaves. Aggregators are essentially responsible for the metadata-level concepts in the cluster. So we’re talking about the data definition layer – DDL, for people who are familiar SQL. We’re responsible for CREATE TABLE statements, for CREATE PIPELINE statements, which we’re going to get right into. In addition, master aggregators and child aggregators collectively handle things like failover, high availability, cluster management, and really importantly, query distribution. So when a SELECT query comes in, or an INSERT command comes in, you want to get some data, you want to insert some data. What we do is we take those queries and we shard those queries down onto leaf nodes. So leaf nodes are never connected to directly by your app. Instead you connect to the aggregators and we shard down those queries to the leaf nodes. And what are the leaf nodes? Well, leaf nodes satisfy a couple of really, really powerful features inside the engine. One feature is storage. If you have leaf nodes, you can store data. If you have more leaf nodes, you can store more data. That’s the general concept here. In addition, it handles pre-execution. So the more leaf nodes you have, generally the faster your database goes. That’s a great property to have in a distributed system. You want to go faster, add more leaf nodes. It generally scales up and we see some really amazing workloads that are satisfiable by simply increasing the size of the cluster. And that’s exciting. Finally you get a sort of natural parallelism. Because we shard these queries to all the leaf nodes, you can sort of take advantage of the fact that by scaling out, you are taking advantage of many, many cores like real, real solid Linux; everything you want performance-wise just works. And that’s a simple system to get behind, and I’m really excited to always talk about it because I’m a performance geek. So that’s the general idea of SingleStore. This Meetup is going to be really focused on Pipelines, so I just wanted to give you some basic ideas at the SingleStore level. Introduction to SingleStore Pipelines: John Bowler Pipelines are SingleStore’s solution to real-time streaming. For those of you who are familiar with what an analytics pipeline looks like, you might have your production database going into a Kafka stream or writing to S3 or writing to a Hadoop data lake. You might have a computation framework like Spark or Storm, and then you might have an analytics database farther downstream, such as RedShift, for example. And you might have some business intelligence (BI) tools that are hitting those analytics databases. So SingleStore Pipelines are our attempt at taking a step back and solving the core problem, which is: how do you easily and robustly and scalably create this sort of streaming analytics workload, end to end? And we are able to leverage some unique properties of our existing system. For example, since SingleStore is already an ACID-compliant SQL database, we get things like exactly-once semantics out of the box. Every micro-batch that you’re streaming through your system happens within a transaction. So you’re not going to duplicate micro-batches and you’re not going to drop them. You also have these pipelines, these streaming workloads are automatically distributed using exactly the same underlying machinery that we use to distribute our tables. In our database, they’re just automatically sharded across your entire cluster. And finally, for those of you who have made these sort of analytics workloads, there’s always going to be some sort of computation step, whether it’s Spark or any similar frameworks. We offer the ability to perform this computation, this transformation, written in whatever language you want, using whatever framework or whatever libraries you want. (This happens within the Pipeline, very fast. Accomplishing the same thing as an ETL step, but within a real-time streaming context. – Ed.) And we’ll explain in more detail how that works. So this is how you create a pipeline – or, this is one of the ways that you create a pipeline. You’ll notice that it is very similar to a CREATE TABLE statement and you can also alter a pipeline and drop a pipeline. The fact is that pipelines exist as first-class entities within our database engine. And underneath this CREATE PIPELINE line is a LOAD DATA statement that is familiar to anyone who’s used a SQL database, except instead of loading data from a file, you’re loading data from Kafka, specifically from this, a host name and the tweets topic. And then the destination of this stream is the tweets table. So in this three lines of SQL, you can declaratively describe the source of your stream and the sync of your stream and everything related to managing it is automatically handled by the SingleStore engine. This is sort of a diagram of how it works. Kafka, for those of you who are unfamiliar, is a distributed message queue. When you’re building analytics pipelines, you very commonly have lots of bits of production code that are emitting events or emitting clicks or emitting sensor data and they all have to get aggregated into some sort of buffer somewhere for whatever part of your analytics system is consuming them. Kafka is one of those very commonly used types, and it’s one that we support. So you have all different parts of your production system emitting events. They arrive in Kafka, maybe a few days worth of buffer. And when you create your pipeline in SingleStore, it automatically streams data in. Now, Kafka is a distributed system. You have data sharded across your entire cluster. SingleStore is also a distributed system. You have data sharded across your entire set of leaf nodes. So when you create this Kafka consumer, it happens in parallel, automatically. Now, this is sufficient if you have data in Kafka and you just want to load it straight into a table. If you additionally want to run some sort of transform, or MapReduce, or RDD-like operation on it, then you can have a transform. And the transform is just a binary or just a program. You can write it in whatever language you want. All it does is it reads records from stdin and it writes records to stdout, which means that you could write it as a Python script. You could write it as a Bash script. You could write it as a C program if you want. Amazing performance. You can do any sort of machine learning or data science work. You can even hit an external server if you want to. So every record that gets received from Kafka passes through this transform and is loaded into the leaf, and all of this happens automatically in parallel. You create the code of this transform and SingleStore takes care of automatically deploying it for you across this entire cluster. Conclusion The video shows all of the above, plus a demonstration you can try yourself, using SingleStore to process tweet streams. You can download and run SingleStore for free or contact SingleStore .", "date": "2019-09-01"},
{"website": "Single-Store", "title": "community-stars-light-up-memsql-forums", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/community-stars-light-up-memsql-forums/", "abstract": "The SingleStore Forums are seeing more and more community contributions – not just timely and incisive questions, but answers from the community, and valuable content contributions as well. We have two new community stars for the summer, and some valuable Q&As flying back and forth. What Makes an Online Community Work? Participation in any online community is mostly optional. People always have a lot of ways they can spend their time, so for an online community to take off, it has to offer a lot to people. So it’s notable that the SingleStore Forums are seeing more and more answered questions, and solid contributions, from customers and users – while SingleStore employees continue to help out as well. The Forums are also important to a key SingleStore initiative, offering free use of SingleStore for small and, in our humble opinion, even medium-sized deployments. Only Enterprise users have direct access to SingleStore’s highly regarded, responsive support team, so the Forums are a crucial source of help. Summer Community Stars The first Community Star, named in June, and featured in a previous blog post, was Ziv Meidav . Now we have a second, and a third Community Star. The July Community Star, Brandon Vincent , has all the tools needed to play the game, as they say in baseball. Not only does he dig in to help other users with complex technical questions; he recently posted an important piece of documentation, the excellent Columnstore Key Guidelines. Covering both shard keys and columnstore table keys, the Guidelines have a lot of important answers – and a couple of in-depth questions as well. The August Community Star, Mani Gandhi , is part of the broader SingleStore community, out beyond the Community Forums. Mani has amassed more than 14,000 karma on Hacker News, a good chunk of that while making insightful comments about SingleStore. (Do go read his posts, but don’t upvote him because of this reference, as Hacker News frowns on that.) Fun on the Forums There’s a lot of serious stuff on the Forums, of course – discussions of potential errors in a SingleStore Pipeline , a question about GROUP _ CONCAT sorting (coming soon, in SingleStore DB 7.0), and a question and answer about unwanted increases in memory usage . However, one has to wonder if the question and answers about using SingleStore as a plagiarism detector are entirely serious. Probably, but someone should double-check the question and answers to see if they’re all entirely original. School’s Back In As school goes back into session, and summer winds down, we expect things to get more business-like again – and to see an uptick of activity on the SingleStore Forums. See you there .", "date": "2019-09-01"},
{"website": "Single-Store", "title": "studio-release-1-8-1-first-year", "author": ["David Gomes"], "link": "https://www.singlestore.com/blog/studio-release-1-8-1-first-year/", "abstract": "Today is the one-year anniversary of SingleStore Studio and sees the release of SingleStore Studio 1.8.1. To celebrate, we have put together a brief summary of where we’ve taken SingleStore Studio in its first year, a few thoughts on how we will extend the product going forward, and an invitation to you to contribute your feedback. Just over a year ago , we launched the first release, SingleStore Studio 1.0.1. While the release notes detail everything that’s happened since, we want to reflect on the last year of development and give some additional context to these notes. How Studio Started The SingleStore Studio project started in early 2018 with the goal of creating a visual tool that our customers could use to manage and monitor their SingleStore clusters. During the first few months of development, we worked toward implementing a front-end architecture that would allow us to iterate on new features as quickly as possible. Upon completing our first release, we began tweaking and improving the product as we got feedback from our customers and from the community. One comment, from a Fortune 100 company in financial services, sums up the enthusiasm: “I will keep digging into SingleStore Studio, and bringing up more feature requests, as I see this to be a gold mine.” Our first public release was on August 6th, 2018. In this first release, Studio had only a handful of features: SQL Editor Resource Usage Profiler Schema Explorer Pipelines Table Nodes Table Screenshot of SingleStore Studio 1.0.1. What We’ve Added to Studio A month later, we added a new feature called Visual Explain . This had been one of the most-requested features from our customers. At its core, it allowed anybody to understand where a query is spending its time, in order to illustrate how to optimize its running time. The Visual Explain feature in Studio. Over time, we’ve been adding other new features such as: Real-time resource usage monitoring (disk, RAM, CPU, and network) Logical monitoring of the SingleStore topology Real-time monitoring of running queries Real-time resource usage monitoring. Over the past year we have significantly expanded the initial set of features. The Schema Explorer now contains much more information about the clusters’ schemas. Moreover, the SQL Editor has been completely reimagined to give users a more IDE-like experience: Multiple result tabs for easy query result comparison Schema tree search Loading and exporting of SQL files, and exporting results to CSV Easy database selection Persistent SQL Editor buffer Find/Replace and other text editor features Performance improvements Resizable panes Visual improvements Today, we’re shipping SingleStore Studio 1.8.1. The release notes for Studio 1.8.1 are: Adds result tabs to the SQL Editor’s output pane Disables the bottom panel from appearing when queries fail in the SQL Editor Visual refactor to the table page in the Schema Explorer Fixes a bug where tables with a date column would crash the Sample Data tab on the table page Active Processes in SingleStore Studio 1.8.1. Where SingleStore Studio’s Going We are continually receiving feedback from our customers, and Studio updates planned for later in 2019 will include a smoother onboarding experience, performance improvements for large clusters, and several other as-yet-unannounced features. As usual, all feedback is welcome. The best place to provide us with feedback is on the SingleStore Forums . Make sure to select the “SingleStore Studio” category when creating a new topic. If you’re interested in doing full stack engineering work on products like SingleStore Studio, make sure to check our currently open positions . We’re currently hiring in San Francisco, California and Lisbon, Portugal.", "date": "2019-09-06"},
{"website": "Single-Store", "title": "fiserv-on-machine-learning-real-time-analytics-at-financial-institutions", "author": ["Manish Pandey"], "link": "https://www.singlestore.com/blog/fiserv-on-machine-learning-real-time-analytics-at-financial-institutions/", "abstract": "In this webinar, which you can view here , Manish Pandey tells us about the transformative effect real-time data, machine learning, and artificial intelligence will make in helping financial services institutions meet increasing demands from consumers. (You can also see the slides .) Pandey, who is Sr. Director of Business Development and Digital Strategy at SingleStore partner Fiserv, highlights the power of SingleStore, and the smarts that Fiserv can bring to helping companies make the most out if it. – Editor As you know, financial services has always been an exciting world. And to add to it, it’s an interesting time as well for several different reasons. Data analytics and the speed at which we access, analyze and make sense of data, has become such a critical part of our business today. It’s not that data and speed were not important in the past, but the game is different today because our consumers understand what it means to have a profound experience. In fact, just to share a couple of data points to set the context, in a recent article by The Financial Brand , they talked about four D’s of consumer attrition, four D’s of consumer attrition are dissatisfaction, debt, displacement and divorce. Actually, no prizes for guessing, but more than 50% of consumers leave their bank because of dissatisfaction. Dissatisfaction has several different aspects of it, but the key part is attrition. The consumers are leaving the bank because they are not happy. According to a recent survey from TD Bank, the risk of payment fraud is the number one concern for 44% of financial industry professionals this year. That’s a 14% increase in just 12 months. So, on one hand we have consumers who are demanding the seamless, real-time experience. On the other hand, we have to deal with all these threats, which means friction. How do we balance it? I will touch upon three aspects of our world and discuss why it’s important to have real-time data computing capability. Also, why leverage analytics, machine learning, AI in combination to drive business forward and create delighted consumers. Digital-Native Companies are Setting the Pace With that, there are many reasons why these two are so successful, but one common aspect, they both know their customers very well. Also, they know how to engage with them without having a customer care person talk to them. In fact, I don’t know about you guys, but in fact, for me in last several, several years of my experience with these guys, I’ve never had to call them for any issues, which is just profound customer experience that I talked earlier on when we were setting the topic. How they do it, they know their customers like no one does. It’s not rocket science but, as I like to call it, it’s data science. They know how to use the data, not just data, but how to use the data in real-time and make sense of the data. Use the data to monetize as well. Look at what Google or Uber or Facebook are doing with that. They are pivoting to several related services and offerings and expanding their business, because of the way they use the data. Can we say the same thing for financial institutions? I have my different opinion on that. These innovations have changed the game in the industry. What’s happening close to our home in the financial services? If you look at financial services, I end up talking to four or five financial services companies, with the banks or insurance companies or credit unions, in a week. And it’s tons of engagement on so many different conversations and topics, but one of the key aspect that’s coming up pretty much in every conversation that either these financial institutions are trying to become a digital business or are going through a digital transformation, or they’re looking at digital business transformation. When we talk about digital transformation, it simply means that they’re trying to empower their customers. They’re empowering their customers on all different channels. It doesn’t matter how they are interacting with the banks, whether they are going online using tablet or mobile or simple desktop or they are calling a customer care rep and talking, or they’re walking to the branch. It’s about helping your customers do a lot of things themselves and that gives them freedom to express themselves. At the same time, with this transformation, FIs are trying to drive revenue and optimize the revenue either by selling new products and services in that space or driving the wallet share. When you think about the business transformation, there’s a lot happening in financial services. At the same time, when no one thought that any innovation is possible in insurance, companies like Lemonade came with a very little investment and funding, and they are threatening bigger insurance giants because of the simple user experience they have created for their consumers, and the way they have reduced the processing time using technologies like machine learning and artificial intelligence. They are bringing the innovation coming from non-banking industry and threatening our banking industry and financial services companies because, simply they have been able to help the customers, empower the customers and change the business models. It’s the time that, when we are seeing these conversations that are happening in banking and financial services too, when banks are looking to become platform companies. When there are talks around banking as a service being offered to non-banking companies. Consumer Demands are Changing There are discussions around Apple working with Goldman Sachs and talking about new cards in the market. With that, I want to shift our focus on the consumer side. So far we’ve talked about what’s happening at the more organization level in the financial services industry, but now if you look closely on the customer side, customers are telling us every day, they are giving us feedback. The reason we need to understand what they are giving us as a feedback is because we got understand what’s shifting in that paradigm. I mean, there’s a lot of noise, at the same time, there’s a lot of good feedback coming from our interaction with consumers. That’s the reason we need to understand the data. We need to access the data in real time and deliver the technologies that we talked about, analytics, machine learning, and AI, and it has gained much significance now. Consumers today have all different kinds of experiences in retail, online and entertainment. They want to do what they want to, and they want to do it now. I will say this one more time, they want to do what they want to do, and they want to do it now. We can’t disagree with that because, we are a consumer, and we also see that we are getting different experiences when we interact with the different touch-points. What you see on the screen is a representation of the same. They no longer see banks they used to see earlier. They’re demanding experiences consistent with what they see with other touch points. Consumers are saying, “Know me. You have all the data.” No one has more consumer data, in my opinion, than banks. We have all kinds of consumer data. They spend most of their time with us, and they spend most of their emotional relations with us. Money is not just money anymore. Money is very much emotional. So what do we do with that data? What do we do with that information? I’ll give you two real-life experiences. Banks who I deal with, if they are careful, they will understand that I’m not a phone-banking guy. I’m not a snail mail kind of guy. I’m more like an online guy. I love to chat with my bank. I love to interact with the chatbots and clarify my questions, but I see those banks interacting with me, sending those offers in my mailbox every week. Tons and tons of paper. I have no clue why they do it. Clearly, it means they don’t understand me. They don’t understand my preferences. Another example, one of the leading banks in this country, I went to them four, five years back. I just wanted to explore options of refinancing, because I thought I will take benefit of some lower interest rates. They were nice enough to run an analysis on me and look at my interest rate. They came back and said that, no, I have a very good deal, and I should not look at refinancing. It won’t be cost effective for me, and I listened to that. Now, from the same bank, I started getting mailers on refinancing. One, they are sending those mailers in my mailbox, again, every day. And now they advised me not to go for refinance, and they’re asking me to refinance, and they are sending me offers on refinancing. That’s so ridiculous. I mean, I’ve no idea why they will do this to me, or any consumer. That clearly tells us that we are doing not much with the data, or we are not doing much with the information that’s coming from the consumers. That’s been the biggest difference that we are seeing in our industry as compared to some other industries. If we go further and if we talk a little bit more about consumer expectations, so consumers, obviously, are expecting hassle-free interactions no matter how they do business with any institution. To give you some data points, these consumers are becoming increasingly comfortable with different ways to do any transaction, for that matter, paying as well. Whether it’s the older generation embracing digital channels or more people saying they see a digital wallet as secure. It is clear consumer perceptions are evolving. Several types of online transactions are showing modest increase over last years, including banking and financial transactions. We are also seeing boomers and seniors often showing largest increase and, though they are still trailing behind the other generations, but they are spending a lot of time online. And they are doing a lot of transactions online, whether they are doing financial transactions, or they’re doing non-financial transactions, or they are on social media. Those experiences are different for them and, obviously, that’s why there is a conversation as to why I’m not getting a consistent experience. Although many consumers also remain at least somewhat concerned about the security of receiving or paying or doing financial transactions, but they are embracing the new technology and experiences more and more every passing year. With that, we also look at, consumers are also increasingly getting very open to using non-banking companies, which is extremely important as you as a consumer. I mean, and your consumers you see that, when they are becoming more comfortable with non-banking players, especially when it comes to paying bails or taking a loan or managing money, tracking your budget, transferring money or anything, something financial, it’s not a good sign. It’s not a good sign because, we always thought that consumers will be attached with the banks because of several different reasons but, now because of experiences that they are getting from non-banking companies in these areas, they are all ready to switch. Just to let you know a data point, in 2017, 40% of consumers stated that they would be comfortable using a technology company to pay bills compared to 55% of them in 2018 survey, which is a big jump. So, today, consumers demand convenience, ease of use, faster services an enhanced user experience and interface. Not to spend too much time on this one, but we are also seeing, comfort is increasing with automation and online activities are increasing day by day. Not just among the younger generation, but baby boomers and seniors are getting very comfortable as well, and that’s the point that we are trying to convey. So, it’s cutting across different generations now. It’s cutting across different customer segments. It’s cutting across within the segments as well. As we go along, we’ll talk about how a bank, in fact, not just their data customer segmentation, but they developed like 15,000 micro customer segments depending on several different parameters. Now we are seeing the trend happening across different segments and within those segments and it’s just mind-blowing. How Financial Services Companies are Responding Now we are moving to the third dimension. I mentioned in the beginning of the webinar that I will touch upon three different dimensions of our world in this context in terms of access to real-time data and analytics and machine learning. We talked about what’s happening at the broader financial institution level. We talked about consumer preferences. Now, we’re going into the risk and fraud area. The risk and fraud has gained a lot of attention. It’s always been a very sticky topic, but now with the exposure that we have, with the technology, I mean, we are also seeing those cases of fraud increasing year on year. And there are millions and billions of dollars lost. It’s not just the amount of money lost, at the same time we’re talking about its impact on your brand and consumers. That’s the key reason that we should be looking at real-time data computing and combining this with analytics and other technologies that we’ve been talking about and not just detecting fraud but preventing fraud. That’s going to help us not just in terms of cutting our losses but, at the same time, it’s going to help us from the customer satisfaction point of view also. (See the YouTube video about PayPal and how they accomplish these goals.) Because, while the customers are talking about a seamless experience, they are not going to compromise on their data and security of data. So, it’s a balancing act that we have to play and in that context we have to make sure that we are preventing the fraud using real-time analytics and the power of real-time data computing. That’s not a surprise, that several senior executives in the financial services are using AI technologies in various areas such as personalization, in terms of how they deal with consumers, or how they think about their relationship with consumers. Productivity, fraud, wealth management, advisory services, and the list is ever growing. Where are you using this? That’s a question that we should be asking ourselves when we are looking at our strategy in terms of how we are looking to do our business. Why this is so all complex. Why this needs various areas of strategic focus rather than a siloed approach. Well, to start with, there are several challenges of legacy data, but it’s too big, too disparate, and too slow. Just to give you some data points. 3.5 billion Google searches are conducted, 300 million photos are uploaded to Facebook, and 2.5 quintillion bytes of data are created. IDC predicts global data will grow tenfold between 2016 and 2025 to a whooping 163 zettabytes. I don’t even know how to spell this, and I don’t even know how many zeros there are in zettabytes. It’s just a mind-blowing number. I mean, it’s just mind-blowing when we talk to banks, and when I hear the stories, they don’t even know where all the data is sitting and how old the data is. They are living with the data, which, probably, they never want to use it. In terms of velocity, data needs to move faster than legacy systems can handle. Even a 10-seconds lag in data delivery can pose a threat if you are dealing with, say, hyper-critical data. Again, IDC estimates 10% of all data will be hyper-critical in nature by 2025, so we are dealing with, not just the data, huge volume of data, at the same time, we are dealing with very sensitive data. So, why haven’t we moved faster and why now? Great question. Well, we have not moved faster because several different reasons, we talked earlier on and in the last slide as well. A strong data infrastructure and capability is really, really needed, which can help you solve real-time use cases. That will allow you to predict, optimize and forecast. It could also help you stream data from multiple legacy systems or sources and that’s where we are talking about how a more strategic approach to data capability, and not a siloed approach, can help you. At the same time, we’re talking about a much more robust data infrastructure and that’s where we were looking about this whole in-memory data computing, and SingleStore is playing a much bigger role with dealing with clients like you. The challenges are part of our business and there are many, and the way we have evolved the challenges have compounded. Then, of course, there is always, always pressure on total cost of ownership. There is always pressure on our bottom line. So, we have to move because of these reasons, and many other reasons that we have talked about. I think it’s fair to conclude here on touching upon those three areas and concluding with this quote that only things advancing faster than technology is consumer expectations. The fact is, the pace of change is accelerating. Your customers and consumers want what they want and when they want it. The name of the game is speed, ease and convenience. How Fiserv and SingleStore Help Real-time data for sure moves your business forward. Now, just to talk about some of those success stories that we have seen, not just working with our clients but, I also looked at some broader audience. I want to talk about just a few of them and then we will talk about what Fiserv and SingleStore are doing in this space and then we will take up some questions. The first one that you are seeing on the personalization, this was a story of a U.S. bank, which used machine learning to a study that discounts its private bankers were offering to customers. The private bankers claim that they offered these discounts to very important, valuable customers, but the analysis showed a different outcome. So the bank used analytics to determine that who should be given those offers. And they made a campaign around a personalized offering based on analytics and real-time processing of the data on the consumers. They saw an 8% increase in the revenue in just a few months, so that was a very profound ROI that we noticed in terms of driving your revenue using the power of real-time data and making a personalized offer using analytics and related technologies. When we talk about the next use case here, this was a story of a European Bank. They tried a number of things to forecast consumer attrition. Many of those, they did not give them the desired outcomes, so they turned to machine learning, and they build a predictive algorithm on customers who were still active, but they were likely to reduce their business with the bank. The new understanding gave them rise to a targeted campaign that reduced churn by 15%, which was, again, a very significant result that they noticed using machine learning and analytics technology. Then, the last one that I have, it’s an example of a bank in Asia, a leading bank. It used advanced analytics with the data streaming from so many different systems. Some of them, data points, were coming … like customer demographics or key characteristics of the customers, the product held, the credit card statements, the transactions and the point-of-sale data, online and mobile transfers and payments and credit bureau data. And they looked at completely 360-degree profile of the customers, and the bank discovered that they were not just looking at four or five or 10 different customer segments, but they were looking at like 15,000 micro-customer segments. That helped them build a mixed product to-buy model that increased the likelihood to buy three times over. That was a really profound learning for them and that helped them grow their business significantly. What Fiserv is doing, in collaboration with SingleStore, is also working with its clients on several different use cases, very, very similar to what we talked in the last slide. Some of them are listed here, but that’s not all, but we are also looking at how we can help our clients understand their consumers looking at their structured data, unstructured data and the social media data. Basically, ingesting data using SingleStore technology and then using the power of Fiserv data analytics and developing a complete customer crisis degree profiling and making a determination on how to engage with those customers. And that leads to the next best offer or next best action that we can take with those consumers. I mean, they could be attrition candidates, they may be dissatisfied, they may be a good candidate for causes opportunity. They may be moving, or they may be buying a new house, and we should be always there in front of them, going back to our team that we should know them, because our customers are expecting us to know them. At the same time, we got to also look at fraud alerts. That’s where we have combined together, and we are working on the power of real-time data and analytics to make sure that we are putting use cases which can help us predict or alert, fraud possibilities. Not to discount the point that we have also developed various dashboards and related portals, which can help you aggregate data in real-time and make some significant determinations as you go on. With that, I’m going to sum it up, and I’m going to say that, now, if you are interested in learning how the combination of real-time data computing offered by SingleStore and Fiserv’s power of analytics can help you grow your business or drive customer delight by offering real-time personalized services offers. Or, simply, identifying real-time data-related use cases suitable to your needs. We are very, very happy to talk one-on-one, and we are always willing to learn and share. So, reach out to my colleague, Lois, or myself, and we will set up some time and learn from your experiences and share ours. Questions and Answers Q. You talk with customers every day. What have you seen as the biggest changes in terms of market dynamics or priorities? I will answer this from two perspectives, one from the consumer point of view and one from the financial institutions’ perspective. When we look at the consumer, we talked about that. I mean, consumers are, of course, expecting a seamless experience when they are dealing with Netflix, Amazon, and the other touch points. They expect us to provide a similar experience when they deal with their banks. At the same time, they are being very loud and clear and they are saying that, “Hey, you’ve got to know me.” When I was engaged in one of the conversation with customers who were saying that, “Hey, when I’m filling a form on Facebook or other non-banking financial portals, I generally see that they fetch data from other sources about me.” And, hopefully, some data or type-ahead kind of thing. It’s happening in our world too, which is very profound. It goes back to the point that knowing them is really important. At the same time, they’re saying that, “Know me now.” So the real-time has a huge significance. There is no real opportunity for us to have any lag when we deal with our customers. So that’s from the customer point of view, but from the financial services’ perspective, I mean, the average span of human attention is five to seven seconds, which is less than a goldfish attention span. So if you’re talking about that kind of limited opportunity for us to engage with our consumers, and since they’re interacting mostly with digital channels, we have to be pretty solid in terms of how we are communicating with them and how we are not losing them in those five to seven seconds. That’s the kind of significant change that we are seeing and it goes back to one of the things that I mentioned that, how do you make your offers or interactions personalized and not make it generic? Not make it broader group-wise interaction? And there is a pattern evolving around these needs that we need to understand if you have to interact with those customers better and get their attention. So that’s the kind of changes I’m seeing right now from both perspective. Q. How should we start the journey if we want to move towards real-time data computing and analytics? Sometimes, it’s overwhelming because just the kind of challenges our ecosystem presents to us. We talked about legacy data and data sitting in silos. Data always been, again, it’s my personal opinion, data always has been one of the low-priority items in the financial services industry. Not that we didn’t value data, just that the challenges were so big that we never put so much attention on that. However, because of the changing dynamics, it has changed a lot. Now, my suggestion in that context always been to my clients that, let’s not try to solve so many use cases at one shot. You need to pick up a use case which is aligned with your strategic priority, whether it’s the use case around consumer or fraud or just accessing the data for various internal consumptions, which can help executives make better decisions. So it could be any of those areas, which are aligned with your key goals but, at the same time, the data strategy has to be much broader. And the infrastructure that you are looking at should be thought of from the perspective that you’re going to scale up and address many use cases. So start with one use case, but think about a much bigger data infrastructure game plan and a much better strategic analytics strategy and that’s where you can handle one use case, show the value proposition, show the ROI to your leadership team and then move on to the next one, but at the same time you have the infrastructure ready for that. Q. Can Fiserv and SingleStore help us understand our current landscape and help solve some use cases? Absolutely. That’s one of the reasons we’re partnered together because we want to provide end-to-end services and solutions to our clients across the entire journey. Fiserv and SingleStore can really look at, from the very beginning of your journey and walk the entire path. When I say that, what I mean that we can come and look at your current state of your data, data infrastructure, what kind of business priorities you have. And then, from there on, we can really build a strategy for you and then we can execute them back together. That’s what I mean by walking the path together. Fiserv and SingleStore, very, very happy to engage with you guys wherever you are in your journey. Not just solve use cases, but we can help you identify the right use cases, align with your business priorities and solve them for you. The biggest roadblock is to getting projects to move forward or to reinvigorate stalled opportunities. How have your customers and partners overcome these roadblocks? The biggest one always being around, like I was sharing, it’s about data access. It’s been, because the way this industry has evolved, there are so many legacy systems. Those systems, probably, were not required to talk to each other in earlier days. We are required to look at data the way we need to look at data because of the experience consumers are getting outside banking and financial services world. It has forced all of us. So, that’s been the biggest challenge – how do we access data? How do we make sense of the data, and then, how do we monetize the data? Again, we are working with our clients to really navigate this path and not get really bogged down in this whole journey by putting the right strategy in place. At the same time, guiding them towards the right tools and technologies that should be leveraged to continue on this journey. Again, the key point here is that, identifying the right use case. Whether it’s related to consumers or related to fraud and risk management or related to compliance or is related to helping your executive leadership make real-time decisions. It could be any of those areas, then we go backward and understand, how do we navigate through the data infrastructure, database challenges, how do we facilitate the realtime computing and then, at the same time, how do we enable you to address future use cases? So, I think the silos are the biggest opportunity here. Conclusion We invite you to learn more about SingleStore at singlestore.com or give us a try for free at singlestore.com/free .", "date": "2019-09-10"},
{"website": "Single-Store", "title": "memsql-singlestore-then-there-was-one", "author": ["Eric Hanson"], "link": "https://www.singlestore.com/blog/memsql-singlestore-then-there-was-one/", "abstract": "SingleStore Universal Storage is a new vision for how databases can work – first blurring and then, for most use cases, erasing any apparent difference between today’s rowstore and columnstore tables. In SingleStore Universal Storage™ Phase 1, shipping as part of SingleStore DB 7.0 ( currently in beta ), rowstore tables get null compression, lowering TCO in many cases by 50%. Columnstore tables get seekable columnstores, which support fast seeks and updates, giving columnstore tables many of the performance and usability features of rowstore tables. The hard choices developers have faced up to now between rowstore and columnstore tables – or between separate rowstore and columnstore database software offerings – are significantly reduced, cutting costs and improving performance. With the new system of record improvements , also offered in SingleStore DB 7.0, our vision of “one database to rule them all” begins to be realized. Introducing SingleStore Universal Storage SingleStore Universal Storage is a breakthrough in database storage architecture to allow operational and analytical workloads to be processed using a single table type. This will simplify the developer’s job while providing tremendous scalability and performance, and minimizing costs. As a significant first step, in SingleStore DB 7.0, Universal Storage Phase 1 allows OLTP applications to use columnstore tables to run operational transactions on data much bigger than RAM. This is supported via new hash indexes and related speed and concurrency improvements for disk-based columnstore tables, delivering seeks and updates at in-memory speeds. Universal Storage Phase 1 also now supports transactional applications on larger data sets more economically, via in-memory compression for null values in fast, memory-based rowstore tables, with memory savings of roughly 50% for many use cases. Together, these improvements give SingleStore customers better performance at lower cost, the flexibility to get the most from computing resources, and the ability to tackle the largest data management problems economically. Our Vision for the Ultimate Table Format SingleStore supports two types of data tables in the same database: in-memory rowstores, which are ideal for online transaction processing (OLTP) and hybrid transactional/analytical (HTAP) applications, and disk-based columnstores, which are the best choice for purely analytical applications. Customers love the speed and predictability of rowstores for OLTP and HTAP. They also love the truly incredible analytical performance of columnstores, plus their ability to store far more data than will fit in RAM economically. But customers have been asking for us to improve the total cost of ownership (TCO) of rowstores, because they have to provision servers with large amounts of RAM when tables get big, which can be costly. They’ve also asked for us to add OLTP-like features to columnstores, such as fast UPSERTS and unique constraints . In response, we’ve developed a vision of the future in which one table type can be used for OLTP, HTAP, and analytics on arbitrarily large data sets, much bigger than the available RAM, all with optimal performance and TCO. We call this SingleStore Universal Storage . Our ultimate goal for Universal Storage is that performance for OLTP and HTAP is the same as for a rowstore table, if the amount of RAM that would have been required for an explicitly defined rowstore table is available, and that OLTP performance degrades gracefully if less RAM than that is available. For analytics, utilizing large scans, joins, aggregates, etc., the goal is to provide performance similar to that of a columnstore table. The old-fashioned way to support OLTP on tables bigger than RAM would be to use a legacy storage structure like a B-tree. But that could lead to big performance losses; we need to do better. In the Universal Storage vision, we preserve the performance and predictability of our current storage structures and compiled , vectorized query execution capability, and even improve on it. All while reducing the complexity and cost of database design, development, and operations by putting more capability into the database software. In the 7.0 release, we are driving toward solving the customer requirements outlined above, and ultimately realizing our vision for a “Universal Storage” in two different ways. One is to allow sparse rowstores to store much more data in the same amount of RAM, thus improving TCO while maintaining great performance, with low variance, for seeks. We do this through sparse in-memory compression. The other is to support seekable columnstores that allow highly concurrent read/write access. Yes, you read that right, seekable columnstores . It’s not an oxymoron. We achieve this using hash indexes and a new row-level locking scheme for columnstores, plus subsegment access , a method for reading small parts of columnstore columns independently and efficiently. In what follows, we’ll explain how we achieve a critical first step in achieving our vision for a Universal Storage in SingleStore DB 7.0. And this is just the beginning. Sparse Rowstore Compression To address the TCO concerns of our customers with wide tables that have a high proportion of NULL values – a situation often found in the financial sector – we’ve developed a method of compressing in-memory rowstore data. This relies on a bitmap to indicate which fields are NULL. But we’ve put our own twist on this well-established method of storing less data for NULL values by maintaining a portion of the record as a structure of fixed-width fields. This allows our compiled query execution and index seeking to continue to perform at the highest levels. We essentially split the record into two parts: a fixed-width portion containing non-sparse fields and index keys, and a variable-width portion containing the fields which have been designated as sparse. Our NULL bitmap makes use of four bits per field instead of one, to enable room for future growth. We’ve created a pathway where we can add the ability to compress out default values like blanks and zeros, and also store normally fixed fields as variable-width fields – e.g., storing small integers in a few bytes, rather than 8 bytes, if they are declared as bigints. The following figure illustrates how sparse compression works for a table with four columns, the last three of which are designated as SPARSE. (Assume that the first one is a unique NOT NULL column, so is not designated as SPARSE). The fact that the first column is not sparse and the last three columns may be sparse is recorded in table-level metadata. Setting the SPARSE flag causes appropriate fields to have null values indicated by flags, rather than by a null value taking up the normal field width. In the table, the width of the wide fields represents 32 bits and the width of the narrow fields represents 4 bits. Actual space usage also depends on the presence of indexes and storage allocation structures and can’t be easily summarized, but this illustration is a useful visual representation of space savings for sparse fields that can be NULL. The sweet spot for SPARSE compression is a wide rowstore table with more than half NULL values. Here’s an example: CREATE TABLE t (\n  c1 double,\n  c2 double,\n  …\n  c300 double) compression = sparse; Specifying compression = sparse at the end of CREATE TABLE causes SingleStore to use sparse encoding for nullable structured fields, including numbers, dates, datetimes, timestamps, times, and varchars. Using this table schema with SingleStore DB 7.0, loaded with 1.05 million rows, of which two-thirds are NULL, we observe the following memory usage with no compression vs. roughly half the memory usage with sparse compression: Compression Setting Memory Use Savings (percent) NONE 2.62GB N/A SPARSE 1.23GB 53% So, for this wide table with two-thirds NULL values, you can store more than twice the data in the same amount of RAM. This of course can lead to big TCO savings, or enable you to tackle bigger problems to generate more business value with SingleStore. The MPSQL code used to do this experiment is given in Appendix A. Note . A handy query for calculating table memory use is: select database_name, table_name, format(sum(memory_use),0) m\nfrom information_schema.table_statistics\ngroup by 1, 2; This gives the actual memory used for each rowstore table. Seekable Columnstores with Support for High Concurrency To increase the scope of operational analytics and OLTP applications that can be handled with best-in-class performance and TCO with SingleStore, the sparse rowstore compression method described in the previous section can take you a long way. But it still requires all data to be kept in RAM, leading to a TCO and scale limit for some potential users. To drive to truly low TCO per row, while enabling operational applications, we’ve also enhanced our columnstore. (SingleStore’s columnstore is disk-based, with portions cached in memory for better performance.) This enhancement, the second prong of Universal Storage Phase 1, is to make columnstores seekable. How, you ask, is that possible? Aren’t columnstores designed for fast scanning, with no thought given to making OLTP-style seeks fast enough? Based on other columnstore implementations available on the market, you might think this is the case. But SingleStore DB 7.0 introduces new technology to make columnstores seekable, and updatable at fine grain with high concurrency. First, here’s a little background on our columnstore implementation to help understand what we’ve changed. SingleStore columnstores tables are broken into one-million-row chunks called segments. Within each segment, columns are stored independently – in contiguous parts of a file, or in a file by themselves. These stored column chunks are called column segments. Prior to 7.0, accessing a field of a single row would require scanning the entire, million-row column segment for that field. Subsegment Access SingleStore DB 7.0 speeds up access to a row in a columnstore by allowing the system to calculate the location of the data for that row, and then read only the portion of a column segment that is needed to materialize that row. This is called subsegment access. This may involve reading data for up to a few thousand rows, but it is nowhere near the full million rows of the segment. Once the offset of a row in a segment is known, only portions of that row and adjacent rows need to be retrieved, in the typical case. In some other cases, such as a run-length-encoded column, only a small number of bytes of data may need to be retrieved to materialize a column, simply due to the nature of the compression strategy. This also allows efficient seeking. The figure below illustrates how just a portion of the file data for a segment needs to be read to find one or a few rows at a specific position in a segment. Illustration of small portion of column segment files read to perform a seek using sub-segment access in a columnstore segment. Subsegment access allows rows to be materialized efficiently via seek-style access once the row position in a segment is known. So the question is, how can we come to know the row position, and do that efficiently? One way is to scan a single column to apply a filter, and record the row numbers that match the filter. While this can be extremely fast in SingleStore, based on the use of operations on encoded data and vectorization, it can ultimately require time proportional to the number of rows. Columnstore Hash Indexes To run selective queries even faster, we need indexes. Hence, SingleStore DB 7.0 introduces hash indexes on columnstores. You can create a hash index on any individual column in a columnstore table. Filters on these columns can thus be solved using the index. Seeks of a hash index to solve selective filters can identify the positions of the qualifying rows at a speed that’s orders of magnitude faster than a scan. Then, once the row positions are known, the new subsegment access capability is used to seek into each column referenced by the query to retrieve the data for the qualifying records. Multi-column filters can also be solved with multiple hash indexes via index intersection, i.e. intersecting the rowid lists from multiple indexes to produce a final set of qualifying rowids. Fine-Grain Locking for Columnstores Now that highly-selective, OLTP-style queries can be processed fast on columnstores, what else stands in the way of highly efficient read/write access for hundreds or even thousands of transactions per second? Anything that could make these fine-grain read and write transactions wait, of course. What might they wait for? Each other. Waiting in SingleStore DB 6.8 and earlier may happen for columstore updates in conflict with other updates, based on locking at the granularity of a million-row segment. Since this granularity is somewhat coarse, that can limit total concurrency. We’ve dramatically improved concurrency in 7.0 via row-level locking for columnstores. Performance Gains To measure the performance of columnstore seeks through hash indexes, I created a table with 1.074 billion (1024 x 1024 x 1024) rows, which has two columns, and a hash index on each column. The two columns are in completely different orders, and each value in the column is unique, or close to it. The schema of this table is as follows: create table f(a bigint, b bigint,\n shard key(a), key(a) using clustered columnstore, \n key(a) using hash, key(b) using hash\n); I created the same table on a 7.0.4 cluster and a 6.8.9 cluster, except that on the 6.8.9 cluster, the hash keys were omitted. I wrote a stored procedure to seek N times into the index and measure the average time to run the query (ignoring client-to-server communication time). This gave the following performance results. Column being seeked Table size (millions of rows) Runtime (ms), 6.8, no hash indexes Runtime (ms), 7.0, w/hash indexes Speedup (times) a 1,074 6.70 2.46 2.72X b 1,074 271 2.54 107X Notice the dramatic 107X speedup for seeking on column b, which was not ordered by the columnstore key (a). This shows the combined benefit of hash indexes and subsegment access. The key point is that the seek time on column b has gone from being too slow for a highly concurrent OLTP application (271ms) in Version 6.8, to fast enough for OLTP (2.54ms) in Version 7.0, with hash indexes. This greatly expands the kinds of workloads you can run on columnstore tables. The MPSQL code used for these tests is given in Appendix B. You may wish to try variations of the test, such as adding more columns to the table. Of course, adding more columns will slow down the seek time, since each column will have to be accessed. But even with dozens of columns, seek time can be in single-digit milliseconds, depending on your hardware — good enough to no longer be a bottleneck for many concurrent operational applications. Moreover, even for very wide tables, say with hundreds of columns, if the query selects from one to a few dozen columns, subsegment access and hash indexes can provide single-digit millisecond row lookup times. Design and Operational Improvements SingleStore offers both rowstore and columnstore tables in a single database software offering, with JOINs and other operations working smoothly across the two table types. (And more so in SingleStore DB 7.0.) This makes database design and operations simpler than with some competing solutions. However, even with SingleStore, there is still complexity in choosing which table type, or set of table types, to use to solve various problems. And operations work becomes more complex as more separate tables and different table types are used. For many use cases, Universal Storage simplifies the database design process, and therefore the level of operations work needed on the resulting implementation. These improvements are significantly realized by the improvements offered in Universal Storage Phase 1 / SingleStore DB 7.0, with room to grow as Universal Storage is more fully realized in future releases. The specifics of how these improvements affect our customers depend, of course, on the specifics of what they need to accomplish. In general, improvements include: Rowstore covers more requirements . Customers doing OLTP and HTAP work are sometimes forced to use columnstore for part or all of their needs because the amount of RAM required is too expensive, or impractically large. The null compression in Universal Storage Phase 1 helps considerably – as we’ve mentioned, by a factor of 2 for many of our customers. Future improvements in rowstore compression will extend these gains. ( Intel’s Optane memory can help too.) Columnstore covers more requirements . Customers who have rowstore-type problems to solve, but are forced to use columnstore for cost or practicality reasons – or for whom columnstore is a good fit, except for a few impracticably slow operations – will find columnstore performance much improved. This improvement is quite strong in the initial implementation of Universal Storage, and also has more room to grow in future versions. Fewer two-headed beasts . Customers often have to put new, active data in rowstore for speed, then move it to columnstore as it ages for cost reasons, despite the complexity this adds to their applications. Or they put a subset of data in a logical table in rowstore for some operations, and the full data set in columnstore for broad analytics purposes, duplicating substantial amounts of data across table types. With Universal Storage, the need to use multiple table types often goes away. Future versions will reduce this need further, as SingleStore absorbs the former need for complex table architectures into enhancements in the database software. Summary and Where We Go From Here SingleStore DB 7.0 introduces two major categories of enhancements that allow more data to be processed efficiently for OLTP, HTAP, and analytics workloads: Sparse rowstore compression to improve TCO for OLTP-style applications that need very fast and predictable row lookup times on multiple access paths, and Subsegment access, hash indexes, and improved concurrency for columnstore tables, to enable more OLTP-style work to function efficiently with columnstore tables that are larger than the available RAM. The Universal Storage features we’ve built for 7.0 are a down payment on additional capabilities we envision in future releases, including: Hybrid row/columnstore tables that are an evolution of our existing columnstores, allowing you to tune the amount of RAM used for the updatable, in-memory rowstore segment, and have more granular index control over all parts of the data. The goal here is that customers won’t have to try to store the data in two different ways, under application control, thus simplifying application development Unique hash indexes and unique constraints on columnstores Multi-column hash indexes on columnstores Ordered secondary indexes on columnstores Rowstores to cache results of columnstore seeks Automatic adaptation of the size of the updatable rowstore segment of a columnstore A columnstore buffer pool managed directly by SingleStore, rather than simply relying on the file system buffer pool Rowstore compression for zeros and blanks, and variable width encoding of small values As you can see, the SingleStore DB 7.0 release delivers a big part of the Universal Storage vision. And if you get on board with SingleStore now, you can expect the speed, power, and simplicity of SingleStore to continue to grow and improve. SingleStore Universal Storage will truly become a store to rule them all! Appendix A: Data Generation SPs for SPARSE Compression Measurement set sql_mode = PIPES_AS_CONCAT;\n\n-- load t using SPARSE compression\ncall buildTbl(300, 0.666, 1000*1000, \"compression = sparse\");\n-- load t with no compression\ncall buildTbl(300, 0.666, 1000*1000, \"compression = none\");\n\ndelimiter //\ncreate or replace procedure buildTbl(numCols int, sparsePercent float, nRows bigint,\n  compression text)\nas\nbegin\n  drop table if exists t;\n  call createTbl(numCols, compression);\n  call loadTbl(numCols, sparsePercent, nRows);\nend //\n\ncreate or replace procedure createTbl(numCols int, compression text) as \ndeclare stmt text;\nbegin\n  stmt = \"create table t(\";\n  for i in 1..numCols - 1 loop\n    stmt = stmt || \"c\" || i || \" double, \";\n  end loop;\n  stmt = stmt || \"c\" || numCols || \" double) \" || compression || \";\";\n  execute immediate stmt;\nend //\n\ndelimiter //\ncreate or replace procedure loadTbl(numCols int, sparseFraction float,\n  nRows bigint) as\ndeclare stmt text;\ndeclare q query(c bigint) = select count(*) from t;\ndeclare n int;\nbegin\n  stmt = \"insert into t values(\";\n  for i in 1..ceil(sparseFraction * numCols) loop\n    stmt = stmt || \"NULL,\";\n  end loop;\n  for i in (ceil(sparseFraction * numCols) + 1)..numCols - 1 loop\n    stmt = stmt || \"1,\"; \n  end loop;\n  stmt = stmt || \"1);\";\n  execute immediate stmt;\n  n = scalar(q);\n  -- Double table size repeatedly until we exceed desired number \n  -- of rows.\n  while n < nRows loop\n    insert into t select * from t;\n    n = scalar(q);\n  end loop;\nend //\ndelimiter ; Appendix B: Exercising Columnstore Hash Indexes create database if not exists db1;\nuse db1;\ndrop table if exists f;\n \n-- hash indexes on columnstores\ncreate table f(a bigint, b bigint,\n shard key(a), key(a) using clustered columnstore, key(a) using hash,\n key(b) using hash\n);\n/*\n-- Create table without hash indexes, for comparison\ncreate table f(a bigint, b bigint,\n shard key(a), key(a) using clustered columnstore\n);\n*/\n \n-- This will keep increasing the size of f until it has a t least n rows.\n-- the b column is a hash function of the a column and will \n-- be unique for ascending\n-- a values 1, 2, 3, ... up until 941083987-1. The data in the b column will\n-- be in a different order than the a column. So you can seek on a value of\n-- a and a value of b separately to show the benefit of hash indexes without\n-- worrying about whether the sort key on a is skewing performance on seeks\n-- on column b.\ndelimiter //\ncreate or replace procedure inflate_data(n bigint) as\ndeclare q query(c bigint) = select count(*) as c from f;\ndeclare tbl_size bigint;\nbegin\ntbl_size = scalar(q);\nif tbl_size = 0 then\n insert f values(1, 1);\nend if;\nwhile (tbl_size < n) loop\n insert into f\n  -- use of two prime numbers in this formula for b\n  -- guarantees unique b for a=1..941083987-1\n select a + (select max(a) from f), ((a + (select max(a) from f)) * 1500000001) % 941083987\n from f;\n tbl_size = scalar(q);\nend loop;\noptimize table f flush;\necho select format(count(*), 0) as total_rows from f;\nend //\ndelimiter ;\n \n-- load the data\ncall inflate_data(1024*1024*1024);\n \n-- try some seeks - should take single-digit milliseconds in 7.0\nselect * from f where b = 937719351;\nselect * from f where a = 2206889;\n \n-- show non-sortedness of column b when ordered by a.\nselect * from f order by a limit 100;\nselect * from f order by b limit 100;\n \ndelimiter //\ncreate or replace procedure measure_q(stmt text, n int)\nas\ndeclare\n q query(a bigint, b bigint) = to_query(stmt);\n a array(record(a bigint, b bigint));\n end_time datetime(6);\n start_time datetime(6);\n d bigint;\nbegin\n start_time = now(6);\n for i in 1..n loop \n   a = collect(q);\n end loop;\n end_time = now(6);\n d = timestampdiff(MICROSECOND, start_time, end_time) as diff_us;\n echo select format(d/n, 0) as avg_time_us;\nend //\ndelimiter ;\n \n-- measure seek time by seeking 100 times and taking the average,\n-- seeking on b, then a columns independently\ncall measure_q(\"select * from f where b = 937719351\", 100);\ncall measure_q(\"select * from f where a = 2206889\", 100);", "date": "2019-09-23"},
{"website": "Single-Store", "title": "replication-system-of-record-memsql-7-0", "author": ["Nate Horan"], "link": "https://www.singlestore.com/blog/replication-system-of-record-memsql-7-0/", "abstract": "System of record capability is the holy grail for transactional databases. Companies need to run their most trusted workloads on a database that has many ways to ensure that transactions are completed and to back up completed transactions, with fast and efficient restore capability. SingleStore DB 7.0 includes new features that deliver very fast synchronous replication – including a second copy in the initial write operation, atomically – and incremental backup, which offers increased flexibility and reliability. With these features, SingleStore DB 7.0 offers a viable alternative for Tier 1 workloads that require system of record capability. When combined with SingleStore Universal Storage , and SingleStore’s long-standing ability to combine transactions and analytics on the same database software, SingleStore DB 7.0 now offers unprecedented design and operational simplicity, lower costs, and higher performance for a wide range of workloads. The Importance of System of Record Capability The ability to handle system of record (SoR) transactional workloads is an important characteristic for a database. When a database serves as a system of record, it should never lose a transaction that it has told the user it has received. In providing system of record capability, there’s always some degree of trade-off between the speed of a transaction and the degree of safety that the system provides against losing data. In SingleStore DB 7.0, two new capabilities move SingleStore much further into SoR territory: fast synchronous replication and incremental backups. Synchronous replication means that a transaction is not acknowledged as complete – “committed” – until it’s written to primary storage, called the master, and also to a replica, called the replica. In SingleStore DB 7.0, synchronous replication can be turned on with a negligible performance impact. Synchronous durability – requiring transactions to be persisted to disk before a commit – is an additional data safety tool. It does take time, but writing to disk on the master happens in parallel with sending the transaction to the replica; there is an additional wait while the transaction is written to disk on the second system. The performance penalty is, of course, greater than for synchronous replication alone. Fast sync replication in SingleStore DB 7.0 makes it possible to run high availability with a small performance hit. In addition to synchronous replication and synchronous durability capabilities, a system of record database needs flexible restore options. In SingleStore DB 7.0, we add incremental backups, greatly increasing backup flexibility. Incremental backups allow a user to run backup far more often, without additional impact on the system. An incremental backup means only the data changed since the last backup needs to be stored. So the amount of time it takes to do the backup (and the resources required to implement the backup) are significantly reduced. This means a shorter RPO (Recovery Point Objective), which in turn means less data is lost in the event of an error that requires restoring a backup. The rest of this blog post focuses on synchronous replication, a breakthrough feature in SingleStore DB 7.0. Sync Replication in Action Synchronous replication in pre-SingleStore DB 7.0 release was very deliberate, and quite slow. Data was replicated as it was committed. So if there were lots of small commits, you would pay the overhead of sending the data network many separate transactions with small amounts of data. In addition, data sent to the replica partition would be replayed into memory on that system, and then acknowledged by the replica to the master – and, finally, acknowledged in turn to the user. This was slow enough to restrict throughput in workloads that did many writes. In SingleStore DB 7.0, we completely revamped how replication works. Commits are now grouped to amortize the cost of sending data on the network. The replication is also done lock-free, as it is with SingleStore’s use of skiplists . Lastly, the master doesn’t have to wait for the replica to replay the changes. As soon as the replica receives the data, an acknowledgement is sent back to the master, who then sends back success to the user. Because SingleStore is a distributed database, it can implement a highly available system by keeping multiple copies of the data, and then failing over to another copy in the event that it detects a machine has failed. The following steps demonstrate why a single failure – of a network partition, of a node reboot, of a node that runs out of memory, or of a node that runs out of disk space – can’t cause data to be lost. In the next section, we’ll describe how this failure-resistant implementation is also made fast. To provide failure resistance, here are the steps that are followed: A CREATE DATABASE command is received. The command specifies Sync Replication and Async Durability. SingleStore creates partitions on the three leaves, calling the partitions db _ 0, db _ 1, and db _ 2. (In an actual SingleStore database, there would be many partitions per leaf, but for this example we use one partition per leaf to make it simpler.) For redundancy 2 – that is, high availability (HA), with a master and replica copy of all data – the partitions are each copied to another leaf. Replication is then started, so that all changes on the master partition are sent to the replica partition. An insert hits db _ 1. The update is written to memory on the master, then copied to memory on the replica. The replica receives the page and acknowledges it to the master. The master database acknowledges the write to the master aggregator, which finally acknowledges it to the user. The write is considered committed. This interaction between the master partition and its replica makes transactions failure-resistant. If either machine were to fail, the system still has an up-to-date copy of the data. It’s fast because of the asynchronous nature of log reply on the replica system: the acknowledgement to the primary system takes place after the log page is received, but before it’s replayed in the replica. Making Log Page Allocation Distributed and Lock-Free There’s still a danger to this speedy performance. Even if the number of transactions is large, if the transactions are all relatively small, they can be distributed smoothly across leaves, and fast performance is maintained. However, occasional large transactions – for instance, loading a large block of data – can potentially prevent any smaller transactions from occurring until the large operation is complete. The bottleneck doesn’t occur on actual data updating, as this can be distributed. It occurs on the allocation of log pages. So, to make synchronous replication fast on SingleStore, we made log reservation and replication lock-free, reducing blocking. The largest difficulty in making our new sync replication was the allocation of log pages distributed and lock-free. There are several pieces that work together to prevent locking. The first part to understand is the replication log. Transactions that interact with the replication log are as follows: Reserve, Write out log record(s), Commit. The replication log is structured as an ordered sequence of 4KB pages, each of which may contain several transactions (if transactions are small), parts of different transactions, or just part of a transaction (if a transaction is > 4KB in size). Each 4KB page serves as a unit of group commit, reducing network traffic – full pages are sent, rather than individual transactions – and simplifying the code needed, as it operates mostly on standard-size pages rather than on variable-sized individual transactions. To manage pages, each one is identified by a Log Sequence Number (LSN), a unique ID which begins with the first page numbered zero, then increments by one with each subsequent page. Each page has a page header, a 48 byte structure. The header contains two LSNs: the LSN of the page itself, and the committed LSN – the LSN up to which all pages had been successfully committed at the time the page in question was created. So a page could have LSN number 53, and also record the fact that the committed LSN at the point this page was created was 48 – all of the first 48 pages have been committed, but page 49 (and possibly also other, higher-numbered pages) has not been. When a transaction wants to log something that it is doing to the log, there is an API which gives it logical space in the log and enough physical resources that it can be guaranteed not to fail, barring the node itself crashing. Next the transaction writes out into the log all the data that it wants within the log. Finally it calls the commit API, which is basically a signal to the log that the data is ready to be shipped over to the replica machine or to disk, or both. With this background, we can look at how the log works internally. We have a 128-bit structure called the anchor in the log, which we use in order to implement a lock-free protocol for the log reservations. The anchor consists of two 64-bit numbers. One is the LSN of the current page in the log, and the other is the pointer into the page where the next payload of data can be written. And all threads operate on the anchor using the compare-and-swap instruction, a CPU primitive which allows you to check that a particular location in memory has not changed, and then change it atomically, in one structure. It is very useful for lock-free operations, as we will see in a moment. SingleStore DB 7.0 Sync Replication Demonstration Let’s say we have four threads, and this diagram shows the current state of the anchor. And just for simplicity I’m not going to show the second part of the anchor, only the LSN. With all compare and swaps, the threads working on trying to write to the log start by loading the most recent LSN, which has the value 1000. Each thread reserves the number of pages it needs for the operation it’s trying to commit. In this case, Thread 1 is only reserving part of a page, so it wants to change the most recent LSN to 1001, while Thread 2 is reserving a large number of pages, and trying to change it to 2000. Both threads attempt to compare and swap (CAS) at the same time. In this example, Thread 2 gets there first and expects the LSN to be 1000, which it is. It performs the swap, replacing the anchor – the committed LSN – with 2000. It owns this broad swathe of pages and can stay busy with it for a long time. Then Thread 1 reads the anchor expecting it to be 1000. Seeing that it’s a different number, 2000, the compare fails. Thread 1 tries again, loading the new value of 2000 into its memory. It then goes on to succeed. It’s important to note that the CAS operations are fast. Once a thread is successful, it starts doing a large amount of work to put its page together, write the log to memory, and send it. The CAS operation, by comparison, is much faster. Also, when it does fail, it’s because another thread’s CAS operation succeeded – there’s always work getting done. A thread can fail many times without a noticeable performance hit, for the thread or the system as a whole. By contrast, in the previous method that SingleStore used, it was as if there were a large mutex (lock) around the LSN value. All the threads were forced to wait, instead of getting access and forming their pages in parallel. Compared to the new method, the older method was very slow. On failovers, the master data store fails, and the replica is promoted to master. The new master now replays all the updates it has received. It is possible that the old master received a page that was not also forwarded to the replica, because that’s the point at which the primary failed. However, with synchronous replication this is no problem – the page that only got to the master would not have been acknowledged to the user. The user will then retry, and the new primary will perform the update, send it to the new replica, receive an acknowledgement of successful receipt, and acknowledge to the user that the update succeeded. Performance Impact In the best case, there’s one round trip required per transaction, from user to master to replica, and back from replica to master to user. This is a low enough communication overhead that it is mostly amortized across other transactions doing work. As we mentioned above, the cost of turning on synchronous replication is single digit percentage impact on TPC-C, a high-concurrency OLTP benchmark. This makes the performance hit of adding a much better data consistency story effectively free for most users! The steps above show highlights, but there are many other interesting pieces that make the new synchronous replication work well. Just to name them, these features include async replication; multi-replica replication; chained replication, for higher degrees of HA; blog replication; garbage collection on blobs; divergence detection; and durability, which we’ve mentioned. Combined, all of these new features keep the impact of turning sync replication on very low, and give both the user and the system multiple ways to accomplish shared goals. Conclusion Synchronous replication without compromising SingleStore’s very fast performance opens up many new use cases that require system of record (SoR) capability for use with SingleStore. Also, the incremental backup capability, also new in SingleStore DB 7.0, further supports SoR workloads. We are assuming here that these will be performed using SingleStore’s rowstore tables, which are kept in memory. Both rowstore and columnstore tables support different kinds of fast analytics. So SingleStore can now be used for many more hybrid use cases in which SingleStore database software combines transactions and analytics, including joins and similar operations across multiple tables and different table types. These hybrid use cases may get specific benefits from other SingleStore features in this release, such as SingleStore Universal Storage . Our current customers are already actively exploring the potential for using these new capabilities with us. If you’re interested in finding out more about what SingleStore can do for you, download the SingleStore DB 7.0 Beta or contact SingleStore today .", "date": "2019-09-23"},
{"website": "Single-Store", "title": "case-study-ssimwave-memsql-scalability-performance", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/case-study-ssimwave-memsql-scalability-performance/", "abstract": "SSIMWAVE customers – from film producers to network engineers to media business executives – work to some of the highest standards in the world. They demand to work with the best. SSIMWAVE also works at that level, as the company’s 2015 Emmy award for engineering achievement demonstrates. They also ask the same high standards of their technology vendors/partners. For SSIMWAVE’s rather comprehensive analytics needs, only one database makes the grade: SingleStore. SSIMWAVE has unique technology and unique analytics needs. SSIMWAVE mimics the human visual system, enabling the software to quantify the quality of video streams, as perceived by viewers, into a single viewer score. Video delivery systems can then be architected, engineered, and configured to manage against this score. This score correlates strongly to what actual human beings would perceive the video quality to be. This allows SSIMWAVE users to make informed trade-offs among resources and perceived quality, automatically or manually, and all in real time. SSIMWAVE Cracks the Code According to Cisco , video data accounted for 73 percent of Internet traffic in 2017, a share that is projected to grow to 82 percent by 2022. Maximizing the quality of this video content, with the least bandwidth usage and at the lowest cost possible, is one of the most important engineering, business, and user experience issues in the online world. The barrier to balancing video quality against compression has been that only human beings could accurately assess the quality of a given video segment when it was compressed, then displayed on different devices. Further complicating the picture (no pun intended) is the fact that people, when asked to rate video quality, give different answers with varying levels of consistency over time. This has meant that a panel of several people was needed to render a useful assessment. As a result, a software engineer or operations person wanting to process and deliver data within acceptable levels didn’t have a reliable, affordable method for knowing how much was just enough, without serious compromise to the viewer’s experience. The SSIMWAVE website demonstrates what the company’s breakthrough algorithm and technology can enable for the media & entertainment industry. SSIMWAVE appears to have cracked the code on this problem with its proprietary SSIMPLUS® algorithm, described on their website , which provides capabilities not found elsewhere. The company’s technology assesses video quality with a single, composite number that achieves a correlation greater than 90 percent between machine assessment and subjective human opinion scores. With this technology, video professionals can make much more efficient use of network resources, while consistently maintaining the desired level of quality. SSIMWAVE users are able achieve significant bandwidth savings by configuring to deliver on a viewer score. The company’s customers include the largest IPTV providers in the US and Canada. Their platform is affecting the streams of tens of millions of subscribers in North America. SingleStore already has a strong position in media and communications solutions , including having Comcast as a customer , and it was natural for SSIMWAVE to consider SingleStore for its own analytics needs. SSIMWAVE’s Need for State-of-the-Art Analytics SSIMWAVE’s business is, in the end, all about numbers. For the company to deliver a complete and reliable service, it needs a high-performance database that can store very large quantities of data and respond very quickly to ad hoc analytics queries. SSIMWAVE has ambitious analytics goals. In addition to comprehensive internal requirements, it needs to offer state-of-the-art analytics capabilities to customers. SSIMWAVE needs both up-to-the-moment reporting, on data volumes that will increase exponentially as new data streams in, and the ability to retain all that data to meet customer service level agreements (SLAs). SSIMWAVE Chooses SingleStore SSIMWAVE was ready for an innovative solution. It compared three technologies that seemed most likely to meet its requirements: Apache Druid . Druid is a new analytics database, written in Java , which recently reached version 0.13.0. Druid partitions data by timestamp . It does not support JOINs and does not have a permissions model. MariaDB AX . MariaDB AX is a version of MariaDB – which itself is a MySQL fork – optimized for analytics. MariaDB AX is a columnar storage engine that supports ANSI SQL. It scales both up and out, and is optimized for use with leading data streaming tools. SingleStore . As a leading NewSQL database , SingleStore scales both up and out. Unlike many others, including MariaDB AX, it fully supports both rowstore and columnstore , at high levels of performance . SingleStore is widely used for real-time analytics and predictive analytics . The database assessment was led by Peter Olijnyk, Director of Technology at SSIMWAVE. Peter has 20 years experience as a software developer, architect, and engineering leader, along with a passion for playing guitar in his rock band. Olijnyk and his team at SSIMWAVE found the choice relatively easy, and decided on SingleStore. Among the key considerations were: Scalability . SSIMWAVE needs a seamlessly scalable database, as its business needs may drive it to arbitrarily large scale requirements. SingleStore’s distributed architecture fits the bill. Performance . SSIMWAVE needs high performance for its own internal needs, but also for its customers, who will be using the SSIMWAVE data architecture. Ease of setup . SSIMWAVE was able to use SingleStore’s documentation to get its first cluster running easily, in a matter of hours. This ease of setup and comprehensibility will extend to SSIMWAVE customers. Direct SQL queries . SSIMWAVE needs a tool with integrations to third party tools, allowing for direct SQL queries which are fast and responsive. Rowstore and columnstore support . Although its current use case is “99 percent columnstore,” SSIMWAVE likes having the door open to rowstore use cases with SingleStore. Data streaming architecture support . SingleStore works smoothly with leading stream-processing software platforms, including support for exactly-once updates. The benefit of SingleStore is its ability to scale out, enabling very high levels of performance. Wide range of integrations . SingleStore supports a wide range of integrations, including the MySQL wire protocol and other standard interfaces. “We use the ODBC interface in a standard way,” said Olijnyk. “We have found SingleStore’s ODBC interface to be customizable and flexible.” “The main thing that tipped the scales was the ease of use and out-of-box experience,” according to Olijnyk. “We went from reading about SingleStore to having clusters running in a matter of hours.” “We implement real-time data streaming and SingleStore for ingest and query response,” he reports. “Also, we recently needed a way to share state across our architecture. We considered ZooKeeper and Redis, but we ended up using SingleStore rowstore, because it gives us such high performance.” The move to this architecture for SSIMWAVE was never far from Olijnyk’s mind. “We prioritize ease of use and ease of installation. We have to concern ourselves with this approach; otherwise, costs and support effort would rise quickly. The fewer technicians we have to manage to support our customers, the better.” SSIMWAVE Chooses Managed Service SSIMWAVE was able to move quickly and smoothly into production to provide its service at scale to OTT companies. A few months after deployment, SSIMWAVE moved to SingleStore Managed Service , the new, high-performance, elastic cloud database service. With SingleStore Managed Service, SSIMWAVE gets the same high performance as before, and includes cloud services and software, but with much less operations effort. “Our focus is to make sure each video stream delivered makes its way to a happy customer. SSIMWAVE tunes video content quality to balance feasibility with the best experience possible.We moved to SingleStore Managed Service, as soon as it was available, because it helps us maintain that focus,” according to Olijnyk. He had cited ease of use and the out-of-the-box experience as drivers in the original move to SingleStore. With SingleStore Managed Service, both ease of use and the out-of-the-box experience are improved further. To see the benefits of SingleStore for yourself, you can try SingleStore today for free . Or, contact us to speak with a technical professional who can describe how SingleStore can help you achieve your goals.", "date": "2019-09-24"},
{"website": "Single-Store", "title": "the-beauty-of-a-shared-nothing-sql-dbms-for-skewed-database-sizes", "author": ["Eric Hanson"], "link": "https://www.singlestore.com/blog/the-beauty-of-a-shared-nothing-sql-dbms-for-skewed-database-sizes/", "abstract": "The limitations of a typical, traditional relational database management system (RDBMS) have forced all sorts of compromises on data processing systems: from limitations on database size, to the separation of transaction processing from analytics. One such compromise has been the “sharding” of various customer data sets into separate database instances, partly so each customer could fit on a single computer server – but, in a typical power law, or Zipf, distribution, the larger database don’t fit. In response, database application developers have had to implement semi-custom sharding schemes. Here, we describe these schemes, discuss their limitations, and show how an alternative, SingleStore, makes them unnecessary. What follows are tales of two different database application architects who face the same problem—high skew of database size for different customer data sets, meaning a few are much larger than others—and address this problem in two different ways. One tries to deal with it via a legacy single-box database and through the use of “clever” application software. The other uses a scalable database that can handle both transactions and analytics—SingleStore. Judge for yourself who’s really the clever one. The Story of the Hero Database Application Architect Once there was a database application architect. His company managed a separate database for each customer. They had thousands of customers. They came up with what seemed like a great idea. Each customer’s data would be placed in its own database. Then they would allocate one or more databases to a single-node database server. Each server would handle operational queries and the occasional big analytical query. When a server filled up, they’d just allocate additional databases to a different server. Everything was going great during development. The application code only had to be written once, for one scenario — all data for a customer fitting one DBMS server. If a database was big, no problem, just provision a larger server and put that database alone on that server. Easy. Then they went into production. Everything was good. Success brought in bigger customers with more data. Data grew over time. The big customer data grew and grew. The biggest one would barely fit on a server. The architect started losing sleep. He kept the Xanax his doctor prescribed for anxiety in the top drawer and found himself dipping into it too often. Then it happened. The biggest customer’s data would not fit on one machine anymore. A production outage happened. The architect proposed trimming the data to have less history, but the customers screamed. They needed 13 months minimum or else. He bought time by trimming to exactly 13 months. They only had two months of runway before they hit the wall again. He got his top six developers together for an emergency meeting. They’d solve this problem by sharding the data for the biggest customer across several DBMS servers. Most queries in the app could be directed to one of the servers. The app developers would figure out where to connect and send the query. Not too hard. They could do it. But some of the queries had aggregations over all the data. They could deal with this. They’d just send the query to every server, bring it back to the app, and combine the data in the app layer. His best developers actually thought this was super cool. It was way more fun than writing application software. They started to feel really proud of what they’d built. Then they started having performance problems. Moving data from one machine to the other was hard. There were several ways they could do things. Which way should they do it? Then someone had the great idea to write an optimizer that would figure out how to run the query. This was so fun. Around this time, the VP from the business side called the architect. She said the pace of application changes had slowed way down. What was going on? He proudly but at the same time sheepishly said that his top six app developers had now made the leap to be database systems software developers. Somehow, she did not care. She left his office, but it was clear she was not ready to let this lie. He checked the bug count. Could it be this high? What were his people doing? He’d have to fix some of the bugs himself. He started to sweat. A nervous lump formed in the pit of his stomach. The clock struck 7. His wife called and said dinner was ready. The kids wanted to see him. He said he’d leave by 8. The Story of the Disciplined Database Application Architect Once there was a database application architect. His company managed a separate database for each customer. They had thousands of customers. They at first considered what seemed like a great idea. Each customer’s data would be placed in its own database on a single-node database server. But, asked the architect, what happens when there’s more data than will fit on one machine? One of the devs on his team said he’d heard of this scale-out database called SingleStore that runs standard SQL and can do both operational and analytical workloads on the same system. If you run out of capacity, you can add more nodes and spread the data across them. The system handles it all automatically. The dev had actually tried the free version of SingleStore for a temporary data mart and it worked great. It was really fast. And running it took half the work of running their old single-box DBMS. All their tools could connect to it too. They decided to run just a couple of SingleStore clusters and put each customer’s data in one database on one cluster. They got into production. Things were going great; business was booming. Their biggest customer got really big really fast. It started to crowd out work for other customers on the same cluster. They could see a problem coming. How could they head it off? They had planned for this. They just added a few nodes to the cluster and rebalanced the biggest database. It was all done online. It took an hour, running in the background. No downtime. The VP from the business side walked in. She had a new business use case that would make millions if they could pull it off before the holidays. He called a meeting the next day with the business team and a few of his top developers. They rolled up their sleeves and sketched out the application requirements. Yeah, they could do this. Annual review time came around. His boss showed him his numbers. Wow, that is a good bonus. He felt like he hadn’t worked too hard this year, but he kept it to himself. His golf score was down, and his pants still fit just like in college. He left the office at 5:30. His kids welcomed him at the door. The Issue of Skewed Database Sizes The architects in our stories are facing a common issue. They are building services for many clients, where each client’s data is to be kept in a separate database for simplicity, performance and security reasons. The database sizes needed by different customers vary dramatically, following what’s known as a Zipf distribution [ Ada02 ] . In this distribution, the largest databases have orders of magnitude more data than the average ones, and there is a long tail of average and smaller-sized databases. In a Zipf distribution of database sizes, the size y of a database follows a pattern like size(r) = C * r^(-b) with b close to one, where r is the rank, and C is a constant, with the largest database having rank one, the second-largest rank two, and so on. The following figure shows a hypothetical, yet realistic Zipf distribution of database size for b = 1.3 and C = 10 terabytes (TB). Because of the strong variation among database sizes, the distribution is considered highly skewed. If your database platform doesn’t support scaleout, then it may be impossible to handle, say, the largest four customer databases when database size is distributed this way–unless you make tortuous changes to your application code, and maintain them indefinitely. I have seen this kind of situation in real life more than once. For example, the method of creating an application layer to do distributed query processing over sharded databases across single-node database servers, alluded to in the “hero” story above, was tried by a well-known advertising platform. They had one database per customer, and the database sizes were Zipf-distributed. The largest customers’ data had to be split over multiple nodes. They had to create application logic to aggregate data over multiple nodes, and use different queries and code paths to handle the same query logic for the single-box and multi-box cases. Their top developers literally had to become database systems software developers. This took them away from application development and slowed the pace of application changes. Slower application changes took money off the table. An Up-to-Date Solution for Skewed Database Sizes Writing a distributed query processor is hard. It’s best left to the professionals. And anyway, isn’t the application software what really produces value for database users? Today’s application developers don’t have to go the route of application-defined sharding and suffer the pain of building and managing it. There’s a better way. SingleStore supports transactions and analytics on the same database, on a single platform. It handles sharding and distributed query processing automatically. It can scale elastically via addition of nodes and online rebalancing of data partitions. Some of our customers are handling this multi-database, Zipf-distributed size scenario by creating a database per customer and placing databases on one or more clusters. They get a “warm fuzzy feeling” knowing that they will never hit a scale wall, even though most of their databases fit on one machine. The biggest ones don’t always fit. And they know that, when a database grows, they can easily expand their hardware to handle it. They only have to write and maintain the app logic one way, one time, for all of their customers. No need to keep Xanax in the top drawer. SingleStore doesn’t require performance compromises for transactions or analytics [ She19 ] . Quite the contrary, SingleStore delivers phenomenal transaction rates and crazy analytics performance [ She19, Han18 ] via: in-memory rowstore structures [ Mem19a ] , multi-version concurrency control [ Mem19c ] , compilation of queries to machine code rather than interpretation [ Mem19e ] , and a highly-compressed, disk-based columnstore [ Mem19b ] with vectorized query execution and use of single-instruction-multiple-data (SIMD) instructions [ Mem19d ] . Moreover, it supports strong data integrity, high availability, and disaster recovery via: transaction support intra-cluster replication of each data partition to an up-to-date replica (a.k.a. redundancy 2) cluster-to-cluster replication online upgrades. Your developers will love it too, since it supports popular language interfaces (via MySQL compatibility) as well as ANSI SQL, views, stored procedures, and user-defined functions. And it now supports delivery as a true platform as a service, Managed Service. Managed Service lets you focus even more energy on the application rather than running–let alone creating and maintaining–the database platform. Isn’t that where you’d rather be? References [ Ada02 ] Lada A. Adamic, Zipf, Power-laws, and Pareto – a ranking tutorial, HP Labs, https://www.hpl.hp.com/research/idl/papers/ranking/ranking.html , 2002. [ Han18 ] Eric Hanson, Shattering the Trillion-Rows-Per-Second Barrier With SingleStore, https://www.singlestore.com/blog/memsql-processing-shatters-trillion-rows-per-second-barrier/ , 2018. [ Mem19a ] Rowstore, SingleStore Documentation, https://docs.singlestore.com/v6.8/concepts/rowstore/ , 2019. [ Mem19b ] Columnstore, SingleStore Documentation, https://docs.singlestore.com/v6.8/concepts/columnstore/ , 2019. [ Mem19c ] SingleStore Architecture, https://docs.singlestore.com/latest/concepts/distributed-architecture/ , 2019. [ Mem19d ] , Understanding Operations on Encoded Data, SingleStore Documentation, https://docs.singlestore.com/v6.8/concepts/understanding-ops-on-encoded-data/ , 2019. [ Mem19e ] , Code Generation, SingleStore Documentation, https://docs.singlestore.com/v6.8/concepts/code-generation/ , 2019. [ She19 ] John Sherwood et al., We Spent a Bunch of Money on AWS And All We Got Was a Bunch of Experience and Some Great Benchmark Results, https://www.singlestore.com/blog/memsql-tpc-benchmarks/ , 2019.", "date": "2019-10-14"},
{"website": "Single-Store", "title": "helios-release-singlestore-system-of-record", "author": ["Peter Guagenti"], "link": "https://www.singlestore.com/blog/helios-release-singlestore-system-of-record/", "abstract": "SingleStore extends our operational data platform with an on-demand, elastic cloud service, and new features to support Tier 1 workloads. SingleStore is proud to announce two exciting new product releases today: SingleStore Managed Service , our on-demand, elastic cloud database-as-a-service, and SingleStore DB 7.0 Beta 2, the next major release of our database engine, featuring SingleStore Universal Storage ™ – a breakthrough new way of managing data – and new features to fully support Tier 1, system of record workloads. With SingleStore Managed Service, you get instant, effortless access to the world’s fastest, most scalable data platform for operational analytics, machine learning, and AI, with platform operations handled by SingleStore. Managed Service is a fully managed cloud service that provides you with instant access to our best-in-class operational database — on demand and at elastic scale — in public cloud environments around the world. SingleStore DB 7.0, which will be generally available in the coming months, brings two exciting new advance advances: SingleStore Universal Storage and key features to make SingleStore “system of record” capable for Tier 1 workloads. Data platform spending is seeing explosive growth as new applications are built to support real-time decisions, predictive analytics, and automation leveraging machine learning and AI. With this growth, and the desire for greater flexibility and cost control, more and more data workloads are moving to the cloud; see Gartner’s perspective in this recent analyst research . The vast majority of SingleStore’s customers are already deploying our database in the cloud, typically using deployment automation or container infrastructure. Many have chosen SingleStore as the cloud migration path from older, on-premises operational database implementations. The availability of SingleStore Managed Service, along with the imminent release of Universal Storage and the new system of record capabilities, make SingleStore the ideal choice for both new applications and to move legacy operational databases to the cloud. Announcing SingleStore Managed Service SingleStore Managed Service has the best-in-class speed, scale, and robust features you have come to expect from SingleStore, but now available on demand. Customers can focus on using their data to build breakthrough applications and analytical systems instead of mundane infrastructure management and tuning. With SingleStore Managed Service, there’s no software to deploy or manage. The most important steps in getting started are choosing an initial deployment size from a pull-down menu, then clicking a button to initiate deployment. Compute and storage resources are automatically assigned, the necessary software is installed and configured, and clusters are created, ready to store data — all in a few minutes. High availability is built in, with SingleStore handling data backup and, if needed, restore operations. SingleStore Managed Service handles infrastructure monitoring, cluster configuration, management, maintenance, and support. Benefits of SingleStore Managed Service include: Effortless deployment and management . As we have all come to expect from cloud services, deployment and upgrades are built in. With SingleStore Managed Service you get the full benefits and capabilities of the SingleStore data platform without having to worry about deployment, management, or maintenance. There’s no need to rack servers, script deployments, or manage VMs. Avoid cloud lock-in through multi-cloud flexibility . Managed Service is available today on Amazon Web Services and Google Cloud Platform. SingleStore operates exactly the same whether deployed on-premises on bare metal, across on-premises or cloud infrastructure, using the SingleStore Kubernetes Operator , or within the Managed Service service. You can use SingleStore to support a broad set of operational and analytical use cases, allowing for a simple, single platform across applications, analytical systems, and cloud deployments. Superior TCO . Compared to both legacy databases and proprietary databases from the cloud service providers, SingleStore Managed Service offers superior total cost of ownership (TCO). SingleStore offers high performance, scalability, ANSI SQL support, and the ability to replace traditional databases like Oracle Exadata and SAP Hana, at a fraction of the cost. When compared to the proprietary databases offered on Amazon Web Services and Google Cloud Platform, SingleStore’s unique architecture and high-performance query engine mean that many operational analytics workloads run with far less resource consumption, offering significant cost savings. SingleStore Managed Service is available in limited public preview today. The service is secure, stable, and ready for your production workloads. There is a time-limited trial open to all users; however, Managed Service is initially available only to a limited number of customers to purchase. Try it for yourself now, or request a technical deep dive with a product specialist. If you are interested in purchasing SingleStore Managed Service immediately, please contact us to request an invitation. Also, see our SingleStore Intro playlist on YouTube. Introducing Universal Storage: Breakthrough Data Management from SingleStore With legacy databases, customers had their data in silos: transaction data in rowstore tables, analytics and data warehousing systems using columnstore tables, and extract, transform, and load (ETL) operations to bridge the gap. SingleStore brings rowstore and columnstore tables together in a single database; ETL is eliminated, and SQL queries can combine data from both types of tables. With Universal Storage, in future versions of SingleStore, we will eliminate the need to choose one table type or another. The system will optimize storage and data access for you. SingleStore is seeking to eliminate data duplication, reduce complexity, and cut total cost of ownership with the launch of SingleStore Universal Storage , a new breakthrough in data architecture. In the future, Universal Storage will offer the fastest possible performance, at the lowest possible cost, for every kind of workload – transactional, analytical, and hybrid – by storing all data in a single table type. In SingleStore DB 7.0, Universal Storage is delivered through improvements to both rowstore and columnstore tables, allowing each to handle workloads that previously only worked well on the other. Rowstore tables – used for transactions, for seeking a few rows of data, and for analytics on rapidly-changing data – get null elimination via the new sparse compression feature. In null elimination, fields that are subject to sparse compression are labeled as SPARSE when the table is created. For each sparse field, a flag is created, half a byte wide. When the field’s value is null, the flag is set by the database software, and no value is stored for that column, saving that amount of storage – often 4, 8, or 16 bytes per null. Null elimination reduces the memory footprint of many rowstore tables by 50% or more, cutting memory usage and costs by half or more. Setting the SPARSE flag causes appropriate fields to have null values indicated by flags, rather than by a null value taking up the normal field width. Also in SingleStore DB 7.0, columnstore tables – highly useful for most analytics purposes, but much harder to update efficiently than rowstore tables – get multiple secondary indexes, allowing fast seeks on multiple access paths, and locking at row level, which enables higher concurrency of updates and deletes. With the initial implementation of SingleStore Universal Storage delivered in SingleStore DB 7.0, you can use columnstore for more workloads than you could before – workloads that previously required rowstore. Using columnstore for these additional workloads means lower total cost of ownership (TCO), while still meeting service level agreements (SLAs) and user expectations. In addition, for the workloads that still require the updating capabilities and even faster performance of rowstore, compression lowers TCO for those workloads as well. New Features Enable “System of Record” Capability SingleStore DB 7.0 also introduces two key availability features which combine to enable Tier 1, system of record workloads: much faster sync replication , turned on by default, and incremental backup. Fast sync replication introduces a high-availability replication option — the ability to always keep a replicated, live copy of your database as it’s receiving updates. Together with the availability of sync durability (copy to disk required for an update to be logged), long supported in SingleStore, sync replication enables the level of data safety required for Tier 1, system of record workloads. Fast sync replication in SingleStore DB 7.0 makes it possible to run high availability with a small performance hit. Incremental backup extends the software’s capabilities beyond the ability to do full backups, to include regular backups of changed data from select time frames, and the ability to restore reliably from the previous full backup plus incremental backups. Together, the addition of the ability to regularly backup recently changed data, and the ability to synchronously replicate your data, mean that SingleStore can now be trusted for critical data workloads. Get Started Today Managed Service is now available in limited public preview. You can try it now for free , or contact us to purchase Managed Service – availability is limited – or speak to a product specialist for details. SingleStore Managed Service is secure, stable, and ready for production immediately. The service is priced based on consumption, with options for both on-demand purchase and discounted prepaid subscriptions. Not all deployment regions are supported initially. Existing SingleStore customers can download SingleStore DB 7.0 Beta 2 to try out our new system of record features and Universal Storage. If you are interested in evaluating SingleStore DB 7.0, please contact us to request access. SingleStore DB 7.0 will be generally available in the cloud and for download later this year. Sign up for our upcoming Managed Service webinar to learn more. Also, see our SingleStore Intro playlist on YouTube.", "date": "2019-09-24"},
{"website": "Single-Store", "title": "case-study-medaxion-analytics-medtech", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/case-study-medaxion-analytics-medtech/", "abstract": "Medaxion has found a solution to the analytics problems that plague companies across the health care sector and beyond. John Toups, CTO of Medaxion, puts it plainly. “By combining the ease of presentation and abstraction of Looker for analytics, with the technical prowess of SingleStore as the database behind Looker, we now have simply the best analytics in healthcare.” Medaxion has changed the working lives of anesthesiologists, its core customers. These highly skilled professionals must deliver absolutely critical medical care, day after day, while also meeting demanding business requirements. These men and women, who once generated a blizzard of paper at every turn, now “record their anesthetics in Medaxion Pulse,” according to Toups, “and through Looker and SingleStore, that data is now actionable.” Today, surgical teams and patients who sometimes waited hours for an anesthesiologist to arrive are now helped much faster, with the aid of predictive analytics. “If it’s cold and snowy, some elective surgeries might get cancelled because it’s hard to get around,” Toups shared. “At the same time, ER admissions might spike because of hazards on the road and the outdoors. We can monitor the data, minute by minute, and help put anesthesiologists where they’re needed most” – getting care to more people, faster. Looker Exposes a Slow Database Looker is an analytics tool that’s designed to make analytics fast and easy. Looker integrates deeply with multiple data sources, rapidly converting user requests to live results. Looker also provides an abstraction layer, LookML, a modeling language that keeps users from having to learn SQL or other code. With Looker’s speed, and the ease of use provided by LookML, demand for analytics at Medaxion rose sharply. But increased demand meant more queries coming into Looker. And, while Looker itself could handle the volume, MySQL – Medaxion’s previous underlying database – couldn’t keep up. SingleStore Solves the Problem Toups and Medaxion had a problem. How did they solve it? Simple: by leaving MySQL and moving to SingleStore. “Much of what my customers want from analytics is real-time operational information, and there is enormous interest in monitoring and improving quality of care,” said Toups. For instance, anesthesiologists need to be near patients who will be needing surgery. It’s a dispatching problem that is, in a way, similar to the issue faced by Lyft and Uber: have the right person, providing the right service, in the right place, at the right time. But for Medaxion’s anesthesiologist clients, and the patients who need them, the stakes are higher. With Looker running on MySQL, Medaxion experienced significant problems. “At first, our analytical reporting was incredibly slow,” said Toups. “When we first started implementing Looker, a couple of years ago, we did a traditional ETL ‘lift and load’ into a MySQL reporting warehouse,” reported Toups. “This resulted in about 600GB of data. Now, on SingleStore, I use columnstore with compression and SingleStore’s native sharding.” “The underlying data includes measurements such as systolic and diastolic blood pressure, along with heartbeat and respiration and a variety of medical history information, all of which are strongly correlated to one another,” Toups continued. “I use associative techniques to compress that 600GB down to a 20GB dataset. Simply unprecedented compression. We always had similarity on our data, but we couldn’t take advantage of it in the MySQL environment.” “It was taking 30 or 40 minutes to generate retrospective quality data on the MySQL platform,” Toups said. “Because we couldn’t easily cache the entire data set in memory on MySQL, it was constant disk thrashing. But on SingleStore, the same analysis runs in less than a minute, and most queries return in under a second.” Toups summed up Medaxion’s progress: “We replaced the middle of the analytics chain with SingleStore. Looker is the body of the rocket ship that carries the information my customers – and their patients – need. SingleStore is the engine of that rocket ship.” Medaxion saw dramatic performance improvements with the move to SingleStore. Re-Plumbing the SingleStore+Looker Solution Solving complicated data flow problems is not always easy. It took a certain amount of expertise and effort to find the right solution to specific problems. Medaxion wanted a solution that would be easy to implement. It wanted as little change as possible in its pre-existing processing architecture. It didn’t want to have architects working for months, nor did it want to end up with a big team of operations people to monitor and manage a complicated solution. SingleStore frees Medaxion from a lot of operational overhead, while empowering the anesthesiologist users. The technical people at Medaxion also appreciate the architectural purity of SingleStore. “Scale can be difficult,” said Toups. “But scale done well is a joy.” New Architecture Delivers Results With SingleStore and Looker, each day sees new operational efficiencies. “Installing and maintaining SingleStore has been a no-effort proposition,” commented Toups. “And, because the previous solution was so slow, we used to do a lot of efficiency programming, indexing, and query planning. With SingleStore, I’ll pre-aggregate when there’s an obvious need. But I can also just let our customers do as they wish with the data.” The results are impressive. “We’ve worked to reduce the elapsed time to less than 30 seconds between a data event and a reportable fact,” said Toups. “This is an enormous improvement, and we achieved it without much change to the underlying architecture.” Medaxion Deploys SingleStore Managed Service Medaxion has continued to innovate in its use of SingleStore, serving as an early adopter of SingleStore Managed Service , SingleStore’s elastic cloud service. Medaxion served as a design partner for the new offering, contributing feedback and ideas as they stood their instance up on Managed Service and moved to production. “We get all of the advantages of SingleStore, with no increase in cost relative to hosting it ourselves in the public cloud, but tremendous advantages in operational savings and simplicity for the team. We save time and effort on managing our deployment, and SingleStore’s people can often fix issues before we’re even aware there might be a problem. We’ve helped make their product better, and they’ve helped make us even savvier users of their product.” Looking Forward It’s only been six months since Medaxion became a SingleStore customer. Medaxion’s rapid progress has fundamentally altered Medaxion’s future and the future of its anesthesiologist customers. SIS And Medaxion is just getting started. By deploying SingleStore and Looker together, Medaxion has enabled a data culture amongst its customers. Now, the anesthesiologists can do their own data mining, with results that are already amazing. Medaxion is building up a huge base of data to be mined going forward. Predictive analytics, machine learning, and AI can help Medaxion, and its empowered customers, to improve both business practice and medical practice around the crucial discipline of anesthesiology. Third, there may still be room for improvement in Medaxion’s data architecture. Toups and others at Medaxion are actively investigating the potential for far-reaching change in how Medaxion operates. SingleStore has helped Medaxion to save time, save money, and dramatically improve outcomes in the short term. At the same time, Medaxion is opening the door to even bigger, and better, changes in the future.", "date": "2019-09-24"},
{"website": "Single-Store", "title": "memsql-helios-7-0-overview-part-3", "author": ["Rick Negrin"], "link": "https://www.singlestore.com/blog/memsql-helios-7-0-overview-part-3/", "abstract": "SingleStore VP of Product Rick Negrin describes the upcoming SingleStore DB 7. 0 in depth. He describes crucial features such as fast synchronous replication, for transaction resilience, and Universal Storage™, which leads to lower total cost of ownership (TCO) and more operational flexibility, at both the architectural and operational levels. Negrin also describes new tools for deploying and managing SingleStore and new features in SingleStore Studio for monitoring, investigating active processes, and editing SQL. This blog post focuses on SingleStore DB 7.0 and the new features in SingleStore Studio. It’s based on the third part of our recent webinar, SingleStore Managed Service and SingleStore DB 7.0 Overview . (Part 1 is here ; Part 2 is here .) This blog post includes our most detailed description yet of the upcoming SingleStore DB 7.0, and new features in SingleStore Studio. Let’s dig in and see what’s new in SingleStore DB 7.0 and SingleStore Studio. So before we do that, I just want to set a little context for how we think about database workloads that are out there, and what’s driving the features that are built into them. If you look at the history of databases, there have been two common workloads that people use a database for. One is around analytics, also known as either Online Analytical Processing (OLAP), or data warehousing. That typically is comprised of requirements around needing queries to be very fast, particularly large and complex queries that are doing aggregations, or group-bys, or a large set of filters. Now you often have a large data size these days measured in terabytes, or hundreds of terabytes, sometimes even petabytes. Usually a company has large, infrequent batch loads of data, with large data loads coming in periodically. And then a need to resource-govern the different workloads that are making use of the system, because you often have different users running different types of queries. Now the other side is the transactional applications – the Online Transaction Processing (OLTP) workload. That has a difference in its requirements. So in that case, the reads are coming in from the application, and the writes are coming in from the application, as opposed to being different. The reason why is that it’s coming from different sources and the types of queries tend to be less complex queries, and more focused on things like fast record lookup or small, narrow-range queries. But there are much stronger requirements around the service level agreements (SLAs), around concurrency and the availability and resiliency of the system. The more mission-critical, the more there’s less tolerance for downtime. Whereas on the data warehouse side, the OLAP or Online Analytics Processing side, you’re often running things at night. It’s often offline at night. If an analyst is forced offline for an hour, they go and get coffee and maybe are unhappy but, you know, it’s not the end of the world. Whereas with the transactional side, often it’s an application, and sometimes it’s customer-facing, or internal-facing to many users within your organization. If it’s down, it can be very bad to catastrophic for the business. And so the SLAs around durability, resilience, extensibility are pretty critical. And what we’re seeing with the new, modern workloads is that often they have a combination of needs around the transactionality and the analytics, that they need the fast query and aggregations and the large data size, but there’s a change in one of the key requirements here. It’s not just large data load but they need fast data load. They need the data to come in within a certain SLA, but there’s an SLA not just on how fast the query is, but also on the ingest, how quickly the data gets into the system in order to meet that near-real-time requirement that people need. And then combine that with some use cases where, in addition to needing the aggregations, they also need the fast record lookups and all the availability and durability and resiliency that we expect from an operational system. And so it’s the combination of the same requirements that you have for the data warehouse as well, plus the operational ones, all combined in one system. There really aren’t many systems that can do that. SingleStore has made solid progress pretty much across all of these requirements. At this point, we can do a vast majority of the workloads out there. But that’s not enough for us. We’re not going to be happy or settled until we can do all the workloads, which means being able to be as available and as resilient as the most mission-critical, complicated, highest-tier enterprise applications that are out there. And to do that, we need to get even more investments in things like resiliency. That’s why we focused on the two key features of 7.0, which are around fast synchronous replication and incremental backup. So up until this current version, we’ve had synchronous and asynchronous replication. The difference being is when we mentioned we have high availability (HA), in which we keep two copies of the data. With async replication, you return back success to the user as soon as at least one copy of the data’s written. Asynchronous replication, you wait until both copies are written before you return back success of the user. This guarantees the user that if there’s a problem or a failover, you’re guaranteed that no data will get lost because you have the data in both copies. Now, we’ve always offered both mechanisms and the customers will choose, which one worked best for them, making a trade-off between performance and durability. And in some cases, customers made one choice versus the other but it was always unfortunate that they had to make that trade-off. Trade-offs are hard and nobody wants to have to choose between those two things they wanted. And so with 7.0, we revamped how we do synchronous replication, so it’s so close to the speed of async that there’s really just a negligible difference between them. We’ve enabled synchronous replication as the default so that everybody gets the durability guarantees without having to trade out performance. And this enables you to basically survive any one machine failure without having to worry about any loss of data. And additionally, we move from having full backups only, to also offering incremental backups. Full backups are great. They allow you to make sure that you have a copy of your data off the cluster in the event of a total cluster or major disaster. But having to do full backups only … Even though our backups are online operations that don’t stop you from running your existing workload, they do take up resources within the cluster. Moving to a model with incremental backup allows you to run the backups more often, reducing your recovery point objective (RPO) – the age of the files that you have to recover, before you can return to normal operations – and reducing the amount of load that you have in the cluster, so you don’t need as much capacity in order to maintain the SLA that you need without being impacted by running back-ups and restores. So really driving down the overall total cost of ownership (TCO) of the system and making it more resilient and driving down the RPO. Now, there’s a lot more stuff in 7.0, but synchronous replication and incremental backups are the two marquee features that we’ve delivered that will make a huge difference to customers at that upper tier of enterprise workloads. SingleStore Universal Storage in (Some) Depth Now, the other big investment we made was around the storage mechanisms that we have in the SingleStore. So again, if you look at SingleStore DB 6.8, which is still the current version, we have what we call Dual Store. We allow you to have a row-oriented table or a column-oriented table within your database and you can choose for every table that you create. So you create which one you want. Most of our customers end up choosing a mix of the two, because they get a certain set of trade-offs and advantages from a rowstore versus a columnstore table. Row-oriented tables to tend to be good for more OLTP-type applications where you need fine-grain aggregation or seeks, for updates or deletes on a set of rows. But the downside is that you get a higher TCO because rowstore is all stored in RAM, and memory can get expensive when you get to large data sizes. The column-oriented tables are much better for big data aggregation scanning billions of rows. You get much better compression on the column-oriented table, but you don’t get very good seek time if you need to seek just one or two rows, or a small number of rows. You don’t get secondary indexes. And so this put customers in an unfortunate position where they’ve had to choose between row-oriented and column-oriented tables. And if you need, for example, the big data aggregation because you’re doing table scans sometimes, but then doing seeks other times, you’re kind of stuck. You have to give up one or the other when you choose the solution for your application. Coming up, with SingleStore DB 7.0, we’ve made investments to make those trade-offs less harsh, investing in compression within the rowstore to drive down the TCO and implementing fast seeks and secondary hash indexes so that users who need those can just use columnstore data. Here’s the comparison, for rowstore tables (transactional): Benefits: Fine-grain aggregation: seek, update, and delete up to millions of rows, fast; secondary indexes (regular & spatial) Former detriment (pre-SingleStore DB 7.0): High TCO Now (with SingleStore DB 7.0 and Universal Storage): Average 50% storage compression, with new null compression feature And here’s a similar comparison, for columnstore tables (analytical): Benefits: Big data aggregation; scan billions of rows, very fast; full-text indexes; 5x-10x compression Former detriments (pre-SingleStore DB 7.0): Slow seeks; no secondary indexes Now (with SingleStore DB 7.0 and Universal Storage): Fast seeks; secondary hash indexes We’re not going to stop there. Long term, what we want is to have a single table that has all those capabilities. Under the covers, we autonomously and automatically will use the right rowstore or columnstore format and make use of memory and disk so that you don’t have to make the choice at design time. We make the choice for you, in operational time, based on how your workload is working. Progress in that direction is what you’re going to see over the next several versions of SingleStore. Here are the benefits for Universal Storage tables (transactional + analytical) , with their original source – either rowstore or columnstore: Fine-grain aggregation (from rowstore) and big data aggregation (from columnstore); seek, update, delete up to millions of rows, fast (rowstore) and scan billions of rows (columnstore); secondary indexes, regular + spatial + full-text (rowstore) and 5x-10x compression (columnstore). And last, but not least, of course, is this: in order to manage and make use of a distributed database, you need to have the right tools for both deploying and managing it. And so we have a number of new capabilities within our tool chain to make it easier to set up your cluster, if you’re using the self-managed software (rather than SingleStore Managed Service, where SingleStore does those things for you). Tools that also allow you to do things like online upgrade and do monitoring of your data over time, so you can do capacity management and troubleshoot problems that are intermittent. And then, on the visual side, the SingleStore Studio tool, which I briefly showed you during the demo, allows you to do things like logical monitoring to visualize the component states of the nodes within a cluster, to make sure there’s no hotspots or data skew or other problems that need attention. Physical monitoring of the actual hosts, so you can see if any one of them is using more resources whether it’s CPU or memory or disk or I/O than it should be using, and take action if needed. (Of course, the physical monitoring is only something that you can do when you’re self-managed; when using Managed Service, the physical monitoring is taken care of by SingleStore.) We also let you have tools to let you look for long-running queries, so you can troubleshoot if a query has problems. Perhaps there’s been a plan change, and you now have less optimal plan, using too much capacity or too much resources. So you can find the query, figure out what the problem is, and kill it if needed. And of course, the SQL Editor, which you saw in the demo, that allows you to write queries and experiment with the system as well as manage it. And that concludes our whirlwind tour of SingleStore Managed Service and the upcoming SingleStore DB 7.0. Of course, you don’t have to believe anything I say. You can try it for you today yourself. You can access SingleStore Managed Service. We made our trial available to you, which is available at SingleStore.com/free , or you can get started with the SingleStore DB 7.0 beta at SingleStore.com/7-release-candidate . If you’re not a customer of SingleStore, you can still access the beta for free, or download our free tier to use SingleStore. Thank you very much. SingleStore Q&A Q. Is the software the same for SingleStore Managed Service, your elastic managed service in the cloud, as in the self-managed software? ( Which you can deploy yourself, in the cloud or on-premises. Ed. ) Are there any differences in capability? A. Self-managed SingleStore the exact same engine used in the Managed Service. It’s what we ship on-prem. As we upgrade software, those versions show up in the cloud, in SingleStore Managed Service. In fact, they’re likely to show up on the managed cloud first, before they show up in the self-managed software that we ship. There are some slight differences. They’re mostly temporary. There are features that are disabled, that will be in the future. So for example, the transforms feature, where you can put in arbitrary code that will enable transformation of the data as it’s going to the pipeline. We don’t enable that on Managed Service because we’re not yet ready to support that. We only take arbitrary code and run it inside of our manage service. However, in time, we’ll figure out how to do that safely, and we’ll enable that feature. There are a few features like that aren’t supported yet but we’ll be supporting in the future. All this is documented in our documentation. There are also some features that are more around managing the physical aspects of the cluster – for example, the ability to just take a node and add it to the cluster or move it towards a cluster. All these are managed for you by the system, with SingleStore Managed Service. Q. Does SingleStore support high availability (HA), and how does it work? A. HA is built into the system. With the self-managed software, you can choose whether or not you want HA at all, or if you want one or two copies. With Managed Service, HA is always turned on. You can’t turn it off and it’s turned on automatically, so you won’t have to do anything. It just happens, transparently. Q. Is there a plan to allow for real-time web based replication for MySQL to SingleStore? If not, do we have current workarounds that can get this done for us, such as MySQL to Kafka to SingleStore, or another solution that is already proven? A. Yes. You can use existing replication tools, such as Attunity and other replication tools that support MySQL. (You can use these tools because SingleStore supports the MySQL wire protocol – Ed.) You can also stage data to Kafka or S3, and use SingleStore Pipelines to move it into SingleStore. In addition, SingleStore will have a solution that will move data from MySQL, Oracle, SQL Server, and other relational databases to SingleStore later this year. Q. I was told common table expression (CTE) materialization is coming in SingleStore DB 7.0. Is this true? A. Recursive CTE virtualization is not coming in SingleStore DB 7.0, but it’s on the roadmap for the future. Q. When is SingleStore DB 7.0 reaching general availability (GA)? A. We’re in Beta 2 right now, we’ll have a release candidate (RC) in the next month or two, and then the GA should be up soon after that. Q. Can you explain how you do fast synchronous replication in SingleStore DB 7.0, without any penalties? A. This blog post talks about our sync replication design: https://www.singlestore.com/blog/replication-system-of-record-memsql-7-0/ . Q. For SingleStore Managed Service, do you provide an SLA, and if so, what is it? When will SingleStore DB 7.0 be available on Managed Service? Will EU regions be available soon? And also, when will SingleStore Managed Service be on Azure? A. SingleStore Managed Service provides an SLA of 99.9% availability. SingleStore DB 7.0 will be available on SingleStore Managed Service when SingleStore DB 7.0 goes GA – in fact, SingleStore DB 7.0 is likely to be available on SingleStore Managed Service first. EU regions will be available in 2019. And SingleStore Managed Service will be available on Azure in the first half of 2020. Q. Does SingleStore provide data masking on personally identifiable information (PII) columns? A. SingleStore does not have masking as a built-in feature, but you can use views on top of tables to accomplish the same effect. Q. Are RabbitMQ pipelines in the works? A. These are on the roadmap, but not in the near-term. Q. Is there a cost calculator for Managed Service? A. Cost is # of units * hours used * price per unit. For more information on pricing, please contact SingleStore Sales . If you have any questions, send an email to team@SingleStoreDB.com . We invite you to learn more about SingleStore at SingleStore.com and give us a try for free at SingleStore.com/free . Again, thank you for participating, and have a great remainder of the day.", "date": "2019-10-22"},
{"website": "Single-Store", "title": "memsql-helios-7-0-overview-part-2", "author": ["Rick Negrin"], "link": "https://www.singlestore.com/blog/memsql-helios-7-0-overview-part-2/", "abstract": "This blog post shares the second part of our recent webinar, SingleStore Managed Service and SingleStore DB 7.0 Overview . (Part 1 is here .) In this part, Rick delivers a deep dive into SingleStore Managed Service, the new, elastic, on-demand cloud service from SingleStore. He describes the features of SingleStore Managed Service in depth, and delivers a demo. In the last part, Rick describes highlights of the upcoming SingleStore DB 7.0. Managed Service is our fully managed on-demand elastic version of SingleStore. Up to now, you could only download SingleStore software and run it yourself. Now, with Managed Service, SingleStore will now take care of things like provisioning, deployment, upgrades, and management alerting, and all of that can be offloaded from you to the SingleStore team. And you’re responsible for the logical management of the data. Meaning, creating your database, writing your queries, tuning your queries, creating your indexes, creating and granting permissions for users. So you handle all of the logical aspects that are more around what you need in order to build your application and you leave the management of the system to SingleStore, allowing you to be more free and move faster. This gives you effortless deployment. You can spin up a cluster in just a couple of minutes, and you get elastic scale, able to grow and shrink the cluster as your requirements demand. You get a much superior total cost of ownership (TCO), particularly versus legacy databases. The cost of SingleStore, especially the cost per query in SingleStore, is far more efficient in terms of how you use its hardware. And the fact that it runs on commodity hardware makes it much more cost efficient than the databases you’ve typically been using. And we’re multi-cloud and hybrid cloud. Meaning, you can run us, of course, on-premises using SingleStore software or you can run us in any cloud. Today we support AWS and GCP for Managed Service, with Azure coming early next year. And of course, all of this leads to better agility for you. Making it faster and easier for you to build your applications and get up and running as quickly as possible, getting that superior scale and performance. Now, how do we do it? It turns out that it was much easier than we expected, actually. When we started building Managed Service, we looked around at different technologies that can help us do the orchestration of spinning up the cluster and all those deployments and upgrades and pieces. And we settled on using Kubernetes. Now, there was some trepidation around whether Kubernetes was ready for a stateful service like a distributed database, because it had been primarily used for non-stateful application level systems. But we found that there had been enough investment by the community that it worked quite well for us – and in fact, a couple of people, over about six months, were able to get the system up and running quite easily. Kubernetes has capabilities like auto-healing if there’s a node failure. So if the node fails in the system, it automatically spins up a VM, brings it into the cluster, attaches to the cluster and gets it up and running. It allows us to do auto-scaling, easily growing the cluster as needed, and things like rolling upgrades to make sure that we can upgrade the software without having any impact on the end user. We use containers to actually pin your compute, so your cores and memory are dedicated to you via containers running on the host machines and we use block storage in the background. So, for example, on AWS, we use the elastic block store for the storage, which then we can easily detach and attach to the containers as needed. When it comes to things like the instance type and the node configuration, all that’s handled by SingleStore. We look at your workload and pick the optimal configuration that works best for you and you don’t have to worry about it. It’s all transparent. As well as delivering high availability (HA) transparently. So we always keep two copies of the data within the cluster. If there’s any problem with a node or if there’s any failures, it will automatically failover to the other copy, and then either fix the node or grow a new one, so that you get back to having two copies safely. And all this happens without any interruption, invisible to the user. And of course, security is top of mind for many people as they come to cloud, and there are all these typical security options enabled by default. So you get encryption on the wire and on disk, and we support the authentication mechanisms that people expect. And full support for a role-based access control as well. But enough talk; let me show you how it works. So this is our customer portal. When you signup for an account , this is what you’ll see. You click on this Clusters link on the left-hand outside, and from here you can create a cluster. So I’m going to say, Create cluster. I’m going to give it a name, Demo2. I can choose a cluster type. In this case, I’m going to choose development but this helps you keep track, which clusters are staging your production development systems. We support four regions today with more coming in the near future. So GCP Virginia, AWS Virginia, AWS Oregon and GCP Mumbai. (For the demo, see the recorded webinar, SingleStore Managed Service and SingleStore DB 7.0 Overview . For an animation, see our SingleStore Managed Service page . Example screenshots are shown here. To try SingleStore Studio, including the built-in demo described here, download SingleStore for free. – Ed) I’m going to choose AWS Virginia. I can choose the number of units. So I can choose four units. Unit is the amount of capacity that you specify for the size of the cluster. And again, whatever you choose, you can always change it later. But I’m going to choose a four-unit cluster, which gives me 32 vCPUs, 256 gigs of RAM, and 4 terabytes across the cluster total. Click Next. I put in my password, and then you can specify, within this an IP range, in order to lock the system down, so it’s only accessible by the machines you want to be accessible to. In my case, I’m just going to allow access from anywhere, because this is a demo. Then it’s Create cluster. So really, with just a handful of clicks, and a few minutes work, I’ve now spun up a four-node cluster within SingleStore. Now, it’ll take a handful of minutes to spin up, so while we’re waiting for that to spin up, I’ve got another cluster over here that I’m going to use to show you a little bit more about what it looks like. So what you’ll see is the cluster properties. So this information is the region and the size and the information about when it was created and what version I’m on. I can change the password over here, or edit the IP addresses that can connect to it. And then I’m given the endpoints here, that let me connect in order to run commands from your application. Now, you can either use obviously a SQL driver, a MySQL or ADB driver, to connect from your application, or you can also use Studio. And Studio is our tool for monitoring and managing SingleStore. So I’m going to click this link here to load data with SingleStore studio, and put in my password. Now, I’m in Studio, which gives me overall dashboard that tells me the number of nodes that I have, gives me some information about database usage and gives me a number of options here on the left. One of the things we introduced recently was a tutorial to help people understand how to make use of SingleStore. So I’m going to walk you through the first couple steps to that tutorial. Let’s say the first thing I want to do is I want to run the tutorial and I’m going to load some sample data. So I click this link here to load sample data. I’m going to load the sample stock data set and so the first thing you have to do before you load a new data, of course, is you have to create a database, which means you have to come up with the schema, what are the tables I want, what are the columns I want. And so I’m going to click paste queries here and this jumps me down to the SQL editor, which allows me to create and edit queries and run them and what we see is it’s creating a database called Trades and it creates a table called Trade with a number of different columns here. The first one being a columnstore table, and another table called Company that’s a rowstore table. So I’m just going to run that. If we go over to the databases link here, you can see that there’s a new database called Trades, and it’s got two tables. One that’s a rowstore and one a columnstore. Now, you’ll notice these tables are empty that’s because we haven’t put any data in them. So the next thing we’re going to do is we’re going to load some data. I’m going to paste the load data thing. Now, what this is doing is using that feature I mentioned earlier, SingleStore Pipelines. And the Pipelines feature, what it does is it lets you create an object in the system that will load data from some source location. As I mentioned, it supports both Kafka as well as Linux file systems, and the cloud vendor blob storage. In this case, we’re using AWS, so we’re going to load the data from S3. And it’s as simple as saying Create Pipeline, give it a name, load data from S3, and then you give it a bucket and then a bit of configuration and away you go. So now the data started loading and of course, if you want to see the pipeline in action, you can click the pipeline’s UI down here. You can see that the pipeline is already finished. So now if I go back to the databases and to this Trades database and I refresh my screen, you can see that there have been over 3,000 rows now loaded into the company table. And so if I want to, in fact, I can go down here and I can see that there are, in fact, 3,288 rows loaded in the table. If I want to get some idea of what they are, I can do a simple query that gets me the first 10 rows of the table. And I can investigate and kind of see what the data is from here. Now I’m going to stop here, even though there’s a lot more to this tutorial, just in the interest of time. But you’ll see at the end, you can actually run this tutorial yourself and actually go through all the way. We’ll generate more data and actually have you run queries over millions of rows to show you the power of SingleStore, but this gives you an idea of how easy and simply you can now get up and running with loading data and getting the value of SingleStore as quickly as possible. So next let’s talk about pricing. So how much does it cost? The way we do pricing, it’s very similar to Amazon. If you’re familiar with Amazon or any of the big cloud vendors, we have two billing models. We have both an on-demand model, where you pay for what you use on an hour-to-hour basis, and then that hourly usage is added up and billed to you monthly. So basically the number of units is the way you buy capacity. A unit is 8 vCPU and 64 gigs of RAM and one terabyte of storage. You specify how many units you want per cluster that you spin up. We add up all those units, times the price per unit, and then that’s your bill at the end of the month. Now if you’re running clusters 24/7 and you know you’re not going to be spinning them up and down, then you can get a discount by paying upfront for the usage for either a one-year or a three-year term. You see the pricing here for Reserved as well. And you can mix the reserved and on-demand pricing – again, similar to what you can do with the existing cloud vendors. Keep in mind please that this pricing is actually the North Virginia pricing and that pricing does vary by region and by cloud provider. And if you want more details on the pricing for other regions or the cloud providers, you can contact our Sales team . And last, let’s finish up with how does SingleStore compare with the other players in the market. So if you look at the other operational database players, one, you can see the SingleStore as a modern highly available architecture compared to the legacy architectures from some of the other players out there. SingleStore does very well. It’s really top in its class when it comes to performance, both how fast we bring data in, how fast you run queries, and the kind of concurrency you can scale. Doing much better than NoSQL systems like MongoDB or single-box systems like Amazon Aurora. And even Oracle Cloud systems, where they have scalable systems, but the price-performance ratio is not very good, where it costs a lot to get the scale you need. And particularly, in analytical query performance, SingleStore shines and really shows up better than any of the other competitors, particularly the single-box systems like Amazon Aurora, that will tap out once it hits the largest box size you can get. But of course, the fact that we can deploy both on-prem and in the cloud, and on any cloud that you want, gives you the flexibility that almost none of the legacy players have. It’s certainly not the big cloud providers like Amazon who will only run their stuff in Amazon and you can’t run it anywhere else. And then, as people move into the higher level of maturity, AI and ML integrations are a key important feature, and SingleStore shines there as well. So that finishes up the discussion around Managed Service.", "date": "2019-10-22"},
{"website": "Single-Store", "title": "from-big-data-to-fast-data-with-sme-solutions-group-and-memsql", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/from-big-data-to-fast-data-with-sme-solutions-group-and-memsql/", "abstract": "SME Solutions Group helps institutions manage risks and improve operations, and they’ve chosen SingleStore as a database partner . Their premier services include data analytics and business intelligence (BI) tools integration, areas where SingleStore adds tremendous value. In this webinar , SME Solutions Group describes how they use SingleStore to solve a wide range of problems, including big data and fast data needs for utilities. How SingleStore Fits SME Mike Czabator, a systems engineer with SingleStore, describes SingleStore as a fully relational, fully distributed, memory-first database. It loads data live; scales out to support arbitrarily large numbers of ad hoc SQL queries, BI tools users, application users, machine learning (ML) models and AI algorithms. There is no delay between data ingest and data availability for queries, which can span historical and current data at terabyte and petabyte scale. SingleStore supports streaming ingest, rowstore tables that run in memory – traditionally used for transactional (OLTP) applications – and disk-based columnstore tables, widely used in analytical (OLAP) applications. With SingleStore, queries can scan rowstore and columnstore tables, and there is no time-consuming, fraught extract, transform, and load (ETL) process; instead, SingleStore Pipelines move data around. SingleStore is MySQL wire protocol-compatible and connects to Kafka, Spark, and many other data sources and destinations. SingleStore runs on-premises, in the cloud, in containers, with SingleStore’s Kubernetes Operator , and as a managed service. One of the critical applications for SingleStore is speeding up dashboards created by BI tools, or custom-built; analytics-driven applications, such as Uber ride hailing ; and ML and AI applications, such as Epigen , which works with government and business and government clients. This is a perfect fit for the analytics and BI tools integration work done by SME Solutions Group. Building the Ideal Data Ecosystem Ron Katzman is Director of Strategy & Operations at SME Solutions Group. He describes current industry trends that the company helps customers align themselves with: data-driven decision making; the need for predictive analytics rather than reactive reports; and the need for speed, agility, and flexibility. In the energy industry, key needs include energy storage, cybersecurity, outage management, and distributed, rather than centralized, energy resources. These areas are ripe for IoT, machine learning and AI, and other emerging technologies. But utility companies still need help with strategy and implementation. Traditional data warehouses are simple but slow; the “event to insight cycle time” is long. Today, architectures are complex. The use of NoSQL solutions , as described in SingleStore’s Hadoop/HDFS case study , adds some capability, but also adds both complexity and cost. SingleStore offers a converged solution. It has nearly unlimited capability, fast ingest, and fast queries. Unlike Hadoop/HDFS and the whole NoSQL world, it has native support for ANSI SQL, leveraging existing developer skills. SME describes SingleStore as the heart of a modern digital transformation ecosystem, offering 10 times the performance of competing solutions at one-third the cost. Utility Case Studies There are several existing case studies for SingleStore client solutions in the energy industry. A Top 10 US utility streamed data through SingleStore for real-time analytics. They started identifying theft within 10 days of going live with SingleStore. Processing time for one legacy job dropped from 22 hours to less than 20 seconds. Ironically, the use of SingleStore increased the lifespan of existing platforms, since emerging, complex tasks could be offloaded to SingleStore. Another top energy company used SingleStore for data integration across billions of data points, with fast, efficient analytics running on top. Quality of service has improved, leading to happier customers. And a leading US energy company mitigates more than $1M a day in drilling costs using machine learning, taking out SAP HANA for a much faster, lower-cost solution based on SingleStore. SME describes SingleStore as “the fastest thing on the planet”; simple, high-performance, low-cost, and very flexible. Q&A, and More Two questions were answered at the end of the webinar. SME has seen time to value as short as two weeks with SingleStore. And, a user asked about the comparison of SingleStore to Snowflake. SME describes Snowflake as a fine database, but limited to the cloud, which is not always the favorite for utilities. SingleStore is unmatched on ingest capability, among other attributes, and gets the job done at a lower price point than other solutions. The webinar also describes a detailed case study of a SingleStore implementation that supports a new meter deployment with much greater data ingest and processing requirements. We’ll share a deep dive into this case study in a future blog post. In the meantime, you can schedule a demo with the SME Solutions Group; download and run SingleStore for free ; or contact SingleStore .", "date": "2019-11-01"},
{"website": "Single-Store", "title": "memsql-helios-7-0-overview-part-1", "author": ["Rick Negrin"], "link": "https://www.singlestore.com/blog/memsql-helios-7-0-overview-part-1/", "abstract": "This blog post shares the initial section of our recent webinar, SingleStore Managed Service and SingleStore DB 7.0 Overview . In this first part, Rick draws the big picture as to the need for a new approach to data processing, and shows how SingleStore fits the bill. In the second and third parts, Rick introduces SingleStore Managed Service, the new, elastic, on-demand cloud service from SingleStore, and describes highlights of the upcoming SingleStore DB 7.0. SingleStore Managed Service is brand new, and SingleStore DB 7.0 is the best version of our product we’ve done yet. It’s a great opportunity to be able to talk about it. I’m first going to give a SingleStore overview, describing what we built and why we built it. Describes the problems that businesses, especially large, successful business, face in meeting today’s demands, given their outdated infrastructure. Then we’ll go into the details around our launch of Managed Service, our managed service, and describe the new features coming in SingleStore DB 7.0. And we’ll finish up with some questions at the end. Today’s Successful Businesses – Old and New – Use Operational Data at Scale If you look at the most successful businesses across industries, there’s one thing that they all have in common, and that’s that they’re powered by live, operational data. They take advantage of the operational data they have to greatly enhance the customer experience and the opportunities and the way they run their business. This includes everything from Uber using the information they have about how people are calling cars to deploy more drivers when they’re needed, to figuring out how to price it, to financial companies like Goldman Sachs who are trying to figure out how to deliver the optimal experiences around portfolio analytics, online banking risk models, and such, to media companies that are delivering streaming media, and need to be able to keep track of exactly what the quality level is at any time, so they can guarantee that people are getting the best experience possible. Truly, across companies in pretty much every industry, data is the key to their success and to the differentiation that they have, becoming the best in their business. This isn’t something that just those companies wanted. It’s something that every company wants. But, it’s hard. And it’s hard because the requirements are much higher and more complex than they’ve ever been. So data volumes are rising, which means the amount of data you have to store and manage and work on is increasing at frightening speeds. And then the complexity around that data – both because of the variety of data sources, where the data is coming from, the formats that you have to process, and the demands on what you have to deal with that, are all rising as well. On top of that, consumers of the data have higher expectations than ever. This is true in the consumer side, where you think about things like people using the airline apps to know whether their flight is delayed or not. They expect instant notifications. Or banking applications – 15 years ago, or maybe even 10 years ago, your way of getting updates was a paper statement you got in the mail once a month. And now, if you swipe your credit card and don’t instantly see it on your app, you’re unhappy with the bank and feel like they’re failing. So expectations have risen, but not just across consumer; this also applies to enterprises as well. The users within the enterprise expect to see the data that they need in real time. They make decisions day-to-day. It’s not just a couple analysts in the back room who have access to the data. Everyone wants access to data, and they expect it to be up-to-date and easy to understand. And so the expectations from the users have been growing tremendously as well. And lastly, we’ve already moved from a historical view of the world, from where we’re looking back in time to try to understand what happened, to looking at what’s happening currently, what’s happening in real time. That’s not where people want to stop. They want to move on from there to even get to predictive. What’s going to happen in the future? And how can I take advantage of that? Already, today, the app that shows you a movie to rent, a book to buy, or when the car you’re booking is likely to appear, is using predictive analytics. For internal use, people want to know about pricing opportunities, what deals are likely to be successful, what inventory will be needed where? Then, they can pro-actively move it around. The final step is to take humans off of the front line. The system can start ordering new supplies, moving inventory, and so on, automatically, then tell the humans what it’s done. So every enterprise is on this journey, moving through that maturity cycle, and they’re all trying to get to the end as fast as possible. But they’re all struggling to keep up. The data infrastructure that we use today, that worked well enough against the previous requirements, from the last couple of decades are not keeping up with these demands, these new requirements. You’re trying to get a tighter time to insight, you’re trying to drive that time from between when data’s born to when you can get insight or take action on it, you’re trying to drive that down to zero. And the infrastructure is your bottleneck, and it’s not allowing you to do that. It’s taking minutes or hours or sometimes even days to move the data through the system, and get it to the end, when it’s too late, and you’re unable to meet the SLA that you’re trying to hit. On top of that, with the rise of the size of the volume of data and the complexity of the data, also comes rising costs. As you try to scale the systems that you have, that weren’t designed for that level of scale – they either hit a bottleneck, or they just hit a ceiling. The cake can get bigger, so you need the system to grow, but the costs of growing to that size are so astronomical, they’re not practical. And last, as you expose your data to, say, all of your employees versus just the incremental list, you’re moving from tens of people to hundreds, thousands, or even tens of thousands – depending on the size of your organization – all trying to get access to that data. And then they extrapolate even further as you expose it out to your customers or your partners in ways you haven’t tried to do before. The number of simultaneous users – that is, concurrency demands – just grows by orders of magnitude. And again, the data infrastructure of the past wasn’t designed to deal with that level of concurrency and still maintain the SLAs that you’re trying to hit. How SingleStore Solves Operational Problems All these new demands, and these trends in the industry, have created new requirements that didn’t exist before. That’s why we built SingleStore. It’s a cloud native operational database built for speed and scale. “Cloud-native” meaning it’s distributed and easy to scale on existing hardware. It’s a relational database, meaning it supports a relational model and a standard SQL interface, but with a modern architecture under the covers, to deliver the speed and the scale requirements that your new, modern applications need. SingleStore supports the workload we call operational analytics, where you’re doing primarily an analytics workload, or it’s aggregations and group-bys, table scans of large sets of data, and all where you need to meet an SLA. The database has to be resilient and available and durable in a way that the existing data warehouse technologies are not. And, when the intersection of those requirements is where the existing architecture is failing, SingleStore does things an order of magnitude better. An example is portfolio analytics, where bank customers want to look at their portfolios and have up-to-date market data in real time. They’re able to quickly drill in and understand how their portfolio is performing, and do what-if analysis to see how it would shift if they make a change. And they want to be able to do that even when the market’s busy, and hundreds or thousands of users all connect at the same time. The system scales smoothly, meeting its SLAs at every step, to deliver what customers need. And predictive analytics is just one of many applications that SingleStore is used for. As an organization moves through the maturity curve, starting to do more predictive ML and AI, where you need a highly scalable system that’s familiar and easy to use, but still delivers on the speed and scale of the type of mathematical operations you need to support an ML model. SingleStore supports that very well. What’s driving people to consider new architectures – more so now than any time before – is the move to cloud and the need to replace the legacy architectures. Often, the legacy architecture is dependent on custom hardware, particularly if you’re using an appliance-based technology. And as we move to cloud, there’s a realization you just can’t take that architecture, let alone that appliance, to the cloud. And so data architects and enterprises are being forced to rethink, “Hey, how should I make this work in the cloud?” And therefore, they’re open to looking at newer, more modern architectures, which is opening up opportunities for technologies like SingleStore to replace the legacy systems. But you don’t have to believe me. You can look at who our customers are. We have half of the top 10 banks in North America who are using us, like I said, for analytics fraud, real-time fraud analysis, and risk analytics. We have telcos who are using us for doing things like managing the amount of data coming in, as they move from 3G to 5G, with the size and complexity of the data growing tremendously, and the existing systems can’t keep up. Large media companies like Comcast, Hulu, Pandora are using us to track and evaluate the quality of their streaming media to make sure that their customers are having the best experience. (SSIMWAVE, which specializes in this, is using SingleStore to power not only internal workloads, but customer-facing workloads as well. – Ed.) And really, right across pretty much all industries, we find this pattern showing up more and more, as customers are trying to meet the requirements in these new model workloads, and they need an infrastructure that can support the speed and scale, in order for them to realize and meet their requirements. So how does SingleStore fit into your architecture? You can think of it as an operational database that supports both analytics and custom applications. So – whether you’re doing dashboards or ad hoc queries, and running third-party BI tools like Tableau or Looker, or building custom dashboards or custom applications to do real-time decisioning or Internet of Things (IOT), SingleStore sits as the database, or the data storage layer, underneath any of those applications or tools. And when you need to bring data in via native connectivity, things like Kafka, as well as NoSQL back-end storage systems like HDFS, and any of the cloud blob storage like S3 or Azure blob or Google blob storage, as well as being able to bring data in easily from relational database systems such as Oracle or SQL Server or MySQL or PostgreSQL. And programmatic systems like Spark – there’s a native connector to bring data in from Spark. No matter how you’re bringing the data in or how it’s coming into the system, SingleStore can easily connect to it. And internally, SingleStore can store any kind of data type, whether it’s relational standard data in tables, geospatial data, JSON data, you can have a native column of type JSON, so you can easily store JSON information but also project out properties and index them so you can have fast query access to that data. And SingleStore does very well with time series data, also. And all of this can be run on the infrastructure that works best for your business, whether it’s you running your software on-premises, either bare-metal or on VMs, or whether you want to self-host, or manage the software in the cloud, or you can run it using Kubernetes on-premises. And lastly, if you don’t want to run it yourself, you can use Managed Service. We will manage the underlying infrastructure for you, and you can focus on just building up your database. Now, there are a lot of different database companies out there, all claiming to be the thing you need. So what makes SingleStore unique? What makes us different than all the other players out there? So one is that we are built from the beginning as a distributed cloud-native architecture. It’s a shared-nothing distributed system that can run on industry-standard hardware. When you install SingleStore, and if you need more capacity, you simply add more nodes to the system. You can do that as an online operation and grow the cluster as large as you need it, or shrink it down if you don’t need the capacity. Streaming ingest has been a key focus for the company from day one, with our Pipelines feature in particular, that allows you to bring data in from other parallel systems like Kafka or the blob storage technologies in the cloud. So you can massively bring it in, in parallel, with exactly-once semantics. But underneath the covers, the thing that makes SingleStore truly unique is the architecture. Most legacy databases were built on a data structure called the B-tree. B-trees were built around getting data off spinning disks efficiently and very fast. But the world has moved on, and technology has moved on, and spinning disks are no longer the standard mechanism. There’s no reason to stay tied down to that data structure. So SingleStore, because it was focused more on newer hardware, and places where memory is much more accessible, uses a data structure called a skip list. And skip lists are much more efficient, especially if you want to build them lock-free, so they don’t have the same locking semantics. Whereas, B-trees are a lot more prone to locking. That’s the underlying mechanism that makes it possible for SingleStore to stream data on ingest. It will continue to let you run queries at the same time. This gives SingleStore the scale and the concurrency and the ability to get the performance that you need. Coupled with innovations like query compilation that make our queries run faster. And making use of SIMD instructions within the processor to do vectorization and a host of other innovations at the query compilation space. These are what give us our speed advantage. And we couple that with an interface that’s familiar, and it’s compatible with what your people already know how to use. It’s ANSI SQL compliant. We support pretty much all the standard SQL functions capabilities in the ANSI SQL standard and even further, we’re wire-level protocol compatible with MySQL. So the existing ecosystem of tools, whether it’s BI tools or ETL tools or programmatic tools, all work with SingleStore right out of the box. And, as I mentioned, SingleStore supports all the different data sources that you might want to use, giving you the flexibility to store the data in whatever format and shape that make the most sense for your application. It’s the combination of these that make SingleStore so powerful to use across your different applications and use cases.", "date": "2019-10-21"},
{"website": "Single-Store", "title": "helios-database-as-a-service-kubernetes", "author": ["Micah Bhakti"], "link": "https://www.singlestore.com/blog/helios-database-as-a-service-kubernetes/", "abstract": "Our new database-as-a-service offering, SingleStore Managed Service , was relatively easy to create – and will be easier to maintain – thanks to Kubernetes. The cloud-native container management software has been updated to more fully support stateful applications. This has made it particularly useful for creating and deploying SingleStore Managed Service, as we describe here. From Cloud-Native to Cloud Service SingleStore is a distributed, cloud-native SQL database that provides in-memory rowstore and on-disk columnstore to meet the needs of transactional and analytic workloads. SingleStore was designed to be run in the cloud from the start. More than half of our customers run SingleStore on major cloud providers, including Amazon Web Services (AWS), Google Cloud Platform (GCP), and Microsoft Azure. Even with the relative simplicity of deploying infrastructure in the cloud, more and more of our customers are looking for SingleStore to handle infrastructure monitoring, cluster configuration, management, maintenance, and support, freeing customers to focus on application development and accelerating their overall pace of innovation. SingleStore Managed Service delivers such a “managed service” for the SingleStore database. Thanks to the power of Kubernetes and the advancements made in that community, we were able to build an enterprise database platform-as-a-service with a very small team in just six months, a fraction of the time it would have taken previously. Making SingleStore Managed Service Portable Many of the members of the SingleStore team have built SaaS offerings on other platforms, and one of the key things we’ve learned is that applications developed on one cloud platform are not inherently portable to another platform. If you want to be able to move workloads from one platform to another, you have to make careful design choices. Each cloud provider builds unique features, services, and methods of operation into their offerings to reflect their own ideas as to what users need and to gain competitive advantage. These differences make it harder for customers to move resources – code, data, and operational infrastructure – from one cloud to another. This stickiness, which is often very strong indeed, benefits the cloud provider. Switching becomes expensive. Additionally, developers and operations people become expert on one platform, and have a steep learning curve if they want to move to another. In response, many companies now follow a “multi-cloud” strategy, where they deploy their IT assets across 2 or more providers. By developing a cloud-agnostic offering, we sought to empower SingleStore customers to deploy their database on the infrastructure of their choice, so that it works the same way across clouds. With cloud provider-specific services like AWS Aurora, or Microsoft SQL Database on Azure, this easy portability disappears. Achieving True Portability with Kubernetes Kubernetes allows application containers to be run on multiple platforms, thus reducing the development cost needed to be infrastructure agnostic, and it’s proven at large scale – for example, Netflix serves 139 million customers from their Kubernetes-based platform. And, with Kubernetes 1.5, a new capability called StatefulSets was introduced. StatefulSets give devops staffers resources for dealing with stateful containers, including both ephemeral and persistent storage volumes. When we began developing our managed service, we actually began by using the Google Kubernetes Engine (GKE). What we discovered was that while Amazon provides Elastic Kubernetes Service (EKS), and Microsoft provides Azure Kubernetes Service (AKS), each of these offerings runs different versions of Kubernetes. Figure 1. The first option SingleStore considered was to use three distinct, cloud provider-specific versions of Kubernetes – EKS, GKS, and AKS. In some cases, the Kubernetes version on offer is significantly outdated. Also, each is implemented in such a way as to make it hard to migrate applications and services between them. Providing true platform portability was incredibly important to us, so we made the decision not to use EKS, GKE, or AKS. Instead, we chose to deploy our own Kubernetes stack on each of the cloud platforms. We needed a way to repeatedly deploy infrastructure on each of the clouds in each of the regions we wanted to support. There are currently 16 AWS regions, 15 GCP regions, and 54 (!) Azure regions. That’s an unreasonable amount of infrastructure to manually deploy. Enter Kubernetes Operations (KOPS). KOPS is an open-source tool for creating, destroying, upgrading, and maintaining Kubernetes clusters. KOPS provides a way for kubernetes and kubectl to interact with our Docker containers. By using KOPS we are able to programmatically deploy Kubernetes clusters to each of the regions we want to support, and then tie the deployments into our back-end infrastructure to create SingleStore clusters. Creating a Kubernetes Operator In the past, SingleStore was managed using a stateful ops tool that ran individual clients on each of the SingleStore nodes. This type of architecture is problematic when the master and client get out of sync, or if the client processes crash, or if they fail to communicate with the SingleStore engine. In light of this, last year we built a new set of stateless tools that interact directly with SingleStore via an engine interface called memsqlctl. Because the memsqlctl interface is built into the engine, users don’t have to worry about the version getting out of sync, or about the client thinking it’s in a different state than the engine expects. SingleStorectl seemed like the perfect way to manage SingleStore nodes in a Kubernetes cluster, but we needed a way for Kubernetes to communicate with memsqlctl directly. In order to allow Kubernetes to manage SingleStore operations, such as adding nodes or rebalancing the cluster, we created a Kubernetes Operator. In Kubernetes, an Operator is a process that allows Kubernetes to interface with Custom Resources like SingleStore. Both the ability and the need to create Operators was introduced, along with StatefulSets, in Kubernetes 1.5, as mentioned above. Figure 2. The option we chose was to create our own portable Kubernetes stack and a toolset based on KOPS and our Operator. Custom Resources for the Kubernetes Operator We began by creating a Custom Resource Definition (CRD) – a pre-defined structure, for use by Kubernetes Operators – for memsql. Our CRD looks like this: memsql-cluster-crd.yaml apiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: memsqlclusters.singlestore.com\nspec:\n  group: singlestore.com\n  names:\n    kind: SingleStoreCluster\n    listKind: SingleStoreClusterList\n    plural: memsqlclusters\n    singular: memsqlcluster\n    shortNames:\n      - memsql\n  scope: Namespaced\n  version: v1alpha1\n  subresources:\n    status: {}\n  additionalPrinterColumns:\n  - name: Aggregators\n    type: integer\n    description: Number of SingleStore Aggregators\n    JSONPath: .spec.aggregatorSpec.count\n  - name: Leaves\n    type: integer\n    description: Number of SingleStore Leaves (per availability group)\n    JSONPath: .spec.leafSpec.count\n  - name: Redundancy Level\n    type: integer\n    description: Redundancy level of SingleStore Cluster\n    JSONPath: .spec.redundancyLevel\n  - name: Age\n    type: date\n    JSONPath: .metadata.creationTimestamp Then we create a Custom Resource (CR) from that CRD. memsql-cluster.yaml apiVersion: singlestore.com/v1alpha1\nkind: SingleStoreCluster\nmetadata:\n  name: memsql-cluster\nspec:\n  license: \"memsql_license\"\n  releaseID: 722ce44d-6f95-4855-b093-9802a9ae7cc9\n  redundancyLevel: 1\n\n  aggregatorSpec:\n    count: 3\n    height: 0.5\n    storageGB: 256\n    storageClass: standard\n\n  leafSpec:\n    count: 1\n    height: 1\n    storageGB: 1024\n    storageClass: standard The beta SingleStore Operator running in Kubernetes understands that the memsql-cluster.yaml specifies the attributes of a SingleStore cluster, and it creates nodes based on the releaseid and aggregator and leaf node specs listed in the custom resource. Benefits of Kubernetes and Managed Service Infrastructure Our original goal was to get SingleStore running in containers managed by Kubernetes for portability and ease of management. It turns out that there are a number of other benefits that we can take advantage of by building on the Kubernetes architecture. Online Upgrades The SingleStore architecture is composed of master aggregators, child aggregators, and leaf nodes that run in highly-available pairs. Each of our nodes is running in a container, and we have created independent availability groups for the nodes. This means that when we want to perform an upgrade of SingleStore, we can simply launch containers with the updated memsql process. By replacing the leaf containers one availability group at a time, then the child aggregators, and then the master aggregator, we can perform an online upgrade of the entire cluster, with no downtime for data manipulation language (DML) operations. Declarative Configuration Kubernetes uses a declarative configuration to specify cluster resources. This means that it monitors the configuration yaml files and, if the contents of the files change, Kubernetes automatically re-configures the cluster to match. So cluster configuration can be changed at any time; and, because Kubernetes and the SingleStore Operator understand how to handle SingleStore operations, the cluster configuration can change seamlessly, initiated by nothing more than a configuration file update. Recovering from Failure Kubernetes is designed to monitor all the containers currently running and, if a host fails or disappears, Kubernetes creates a replacement node from the appropriate container image automatically. Because SingleStore is a distributed and fault-tolerant database, this means that not only is the database workload unaffected by the failure; Kubernetes resolves the issue automatically, the database recovers the replaced node, and no user input is required. This capability works well in the cloud, because you can easily add nodes on an as-needed basis – only paying for what you’re using, while you’re using it. So Kubernetes’ ability to scale, and to support auto-scaling, only works well in the cloud, or in a cloud-like on-premises environment. Scalability – Scale Up/Scale Down By the same mechanism used to replace failed instances, Kubernetes can add new instances to, or remove instances from, a cluster, in order to handle scale-up and scale-down operators. The Operator is also designed to trigger rebalances, meaning that the database information is automatically redistributed within the system when the cluster grows or shrinks. In this initial release of SingleStore Managed Service, the customer requests increases or decreases in the cluster size from SingleStore, which is much more convenient than making the changes themselves. Internally, this changes a state file that causes the Operator to implement the change. In the future, the Operator gives us a growth path to add a frequently requested feature: auto-resizing of clusters as capacity requirements change. Parting Thoughts Using Kubernetes allowed us to accomplish a tremendous amount with a small team, in a few months of work. We didn’t have to write a lot of new code – and don’t have a ton of code to maintain – because we can leverage so much of the Kubernetes infrastructure. Our code will also benefit from improvements made to that infrastructure over time. Integrating SingleStore with Kubernetes allowed us to build a truly cloud-agnostic deployment platform for the SingleStore database, but it also provided a platform for us to provide new features and increased flexibility over traditional deployment architectures. Because of the declarative nature of Kubernetes, and because we built a custom SingleStore Operator for Kubernetes, we can make it easier to create repeatable and proven processes for all types of SingleStore operations. As a result, we were able to build this with just a couple of experienced people over a period of roughly six months. Now that we have a flexible and scalable architecture and infrastructure, we can continue to build capabilities on top of the platform. We are already considering features such as region-to-region disaster recovery, expanded operational simplicity – with cluster-level APIs for creating, terminating, or resizing clusters – and building out our customer portal with telemetry and data management tools to let our customers better leverage their data. This is just the beginning..", "date": "2019-11-06"},
{"website": "Single-Store", "title": "spin-up-a-memsql-cluster-on-kubernetes-in-10-minutes", "author": ["Rob Richardson"], "link": "https://www.singlestore.com/blog/spin-up-a-memsql-cluster-on-kubernetes-in-10-minutes/", "abstract": "Even though SingleStore is a distributed system, you can run a minimal version of SingleStore on your laptop in Kubernetes. We tell you how in this blog post. The combination of free access, and being able to run SingleStore on your laptop, can be extremely convenient for demos, software testing, developer productivity, and general fooling around. In this post we’ll quickly build a single-instance SingleStore cluster running inside Kubernetes on Docker Desktop, on a laptop computer, for free. You’ll need a machine with at least 8GB RAM and four CPUs. This is ideal for quickly provisioning a system to understand the capabilities of the SQL engine. Everything we build today will be running on your machine, and with the magic of containers, we’ll not need to install or configure much of SingleStore to get it running. The steps here are: get a free SingleStore license; install Docker Desktop; create a Kubernetes yaml file; start the SingleStore cluster through Kubernetes; and browse to SingleStore Studio. You’ll have a bare-bones SingleStore cluster running on your laptop machine in no time. Why Kubernetes, and Why a Single Container? Containers are a great way to run software in a protected sandbox and easy-to-manage environment, with less overhead than a virtual machine (VM) – and much less than a dedicated server. You can use containers to spin up applications and systems to try out software, or to quickly spin up a database to support local application development. We’ll use Docker Desktop in Kubernetes mode to provision and spin up a free SingleStore cluster, and just as easily, destroy it when we’re done. Using Kubernetes makes it much easier to run a small SingleStore cluster without interfering with other software running on the machine. The cluster-in-a-box container image that we’ll use here includes an aggregator node, a leaf node, and SingleStore Studio (our browser-based SQL editor and database maintenance tool), all running in one place, all pre-configured to work together. The minimal hardware footprint wouldn’t be nearly enough for production workloads, but it allows us to quickly spin up a cluster, connect it to our project, and try things out. You also have the option of using Docker containers without Kubernetes. We believe that having Kubernetes in the mix makes it easier to manage your cluster and introduces you to a powerful modus operandi for running SingleStore. However, if you don’t already have Kubernetes in your production environment, nor much experience for running SingleStore, you may want to consider running SingleStore in Docker containers, without Kubernetes. The steps to do that are very similar to the steps described in this blog post, and you can view them here. We could also use a Virtual Machine (VM), but containers are lighter-weight than a virtual machine (VM). Like virtual machines, containers provide a sandbox between processes. But unlike VMs, containers virtualize the operating system instead of the hardware, and the configuration-as-code mindset shared by Docker and Kubernetes ensures that we can quickly provision a complete virtual system from a small text file stored in Git. With the single-container SingleStore cluster described here, you can craft the simplest of tables all the way up to running a complex app, a dashboard, a machine learning model, streaming ingest from Kafka or Spark, or anything else you can think of against SingleStore. You’ll quickly understand the methodology and features of SingleStore, and can plan accordingly. The SingleStore cluster-in-a-box container has minimum hardware specs disabled, but you’ll still want a machine with at least 8 GB RAM and four CPUs. With specs well below SingleStore’s limits , you’ll see poor performance, so this system is definitely not the right setup for a proof of concept (PoC). But you can use this setup to experience the full features of SingleStore, and understand how it applies to your business problems. Once you know how SingleStore works, you can take these experiments and use what you learn to help you achieve your service-level agreements (SLAs) on distributed clusters that meets SingleStore’s minimum requirements and your high-availability needs. That’s when you can really open up the throttle, learn how your data performs on SingleStore, and dial in system performance for your production workloads. Cluster-in-a-box Multi-node Cluster Hardware Laptop computer Many hefty servers Best use-case * Try out SingleStore * Test SingleStore capabilities * Prototyping * Proof of concept (PoC) * Production workloads * High availability * High availability Cost Free up to four nodes with 32GB RAM each, and with community support Free up to four nodes with 32GB RAM each, and with community support Sign Up For SingleStore To get a free license for SingleStore, register at singlestore.com/free-software/ and click the link in the confirmation email. Then go to the SingleStore customer portal and login. Click “Licenses” and you’ll see your license for running SingleStore for free. This license never expires, and is good for clusters up to four machines and up to 128GB of combined RAM. This is not the license you’ll want for a production cluster, but it’s great for these “kick the tires” scenarios. Note this license key. We’ll need to copy/paste it into place next. Install Docker Desktop The first step in getting our SingleStore cluster running in Kubernetes (k8s) is to get Docker Desktop installed. We’ll use Docker Desktop’s Kubernetes mode as the simplest way to a Kubernetes cluster. Though beyond the scope of this article, you can also use another K8s cluster such as MiniKube, K3s, MicroK8s, or kind. Docker’s install requirements are quite specific, though most modern mid-range systems will do. Docker Desktop for Windows runs a Linux VM in Hyper-V, and Hyper-V requires Windows 10 Pro or Enterprise. Docker Desktop for Mac runs a Linux VM in xhyve, and requires a 2010 or newer model with macOS 10.13 or better. To install Docker Desktop, go to Docker Hub and choose the Docker Desktop version for your operating system. The download will require you to create a free account. Run the downloaded installer and accept all the defaults. Note for Windows users : If you are doing a fresh install, ensure you choose “Linux Containers” mode. If you installed Docker previously, ensure you’re running in Linux containers mode. Right-click on the Docker whale in the system tray (bottom-right by the clock), and choose “Switch to Linux Containers”. If it says “Switch to Windows Containers”, you’re already in the right place – that is, in Linux Containers mode. Note – adding more RAM : Though not required, SingleStore will definitely behave better when Docker Desktop has more capacity. Click on the Docker whale, choose “Settings…” on Windows or “Preferences…” on Mac, and click on the “Advanced” tab. If your machine has more than 8 GB RAM, set this to 8192. If your machine has 8 GB RAM or less, set it as high as you can. Then change the CPU count from 2 to 4. To turn on Kubernetes, open the Docker whale, choose “Settings…” on Windows or “Preferences…” on Mac, click the Kubernetes tab, and check “Enable Kubernetes”. If you don’t see this option, ensure you’re running in Linux containers mode or upgrade Docker Desktop. The first time you enable Kubernetes mode, it’ll take quite a while to download all the K8s control plane containers and start the cluster. Next time you start Docker, it’ll start much faster. Kubernetes Configuration Files Kubernetes stores configuration details in yaml files. (A yaml file is a text file that’s great for capturing our architecture setup.) Typically each yaml file contains a single resource. For simplicity, we’ll create one yaml file that includes both a deployment and a service. We’ll connect to the service, the service will proxy to the pod, and the pod will route the request into the container. We’ll use the memsql/cluster-in-a-box image built by SingleStore and available on Docker Hub . This image comes with the SingleStore database engine and SingleStore Studio preinstalled. The minimum system requirements are disabled in this “cluster-in-a-box” configuration. Create an empty directory, and create a file named kubernetes-memsql.yaml inside. Open this file in your favorite code editor and paste in this content. As with Python source code, white space is significant. Yaml uses two spaces, not tabs. Double-check the yaml file to ensure each section is indented with exactly two spaces. If you have more or fewer spaces, or if you’re using tabs, you’ll get an error on startup. Loading Gist. Please Wait... Here are the sections in the kubernetes-memsql.yaml file: — designates the break between the two resources. The content above it is the deployment, the content below it is the service. The deployment manages and restarts pods on failure, and the service load-balances traffic across all matching pods. A pod is a Kubernetes wrapper around one or more containers. Deployment: replicas: 1 notes that we only want one pod (container) to spin up. The metadata section is a list of key/value pairs. There’s nothing magic about these the names and values here, but they must match between both references in the deployment and the service. In the containers list, we list only one container. We pull the memsql/cluster-in-a-box image built by SingleStore, and name the container memsql . In the ports section, we identify inbound traffic that’ll route into the container. Open port 3306 for the database engine and port 8080 for SingleStore Studio. The first environment variable, START_AFTER_INIT , exists for legacy reasons. Without this environment variable, the container will spin up, initialize the SingleStore cluster, and immediately stop. This is great for debugging, but not the behavior we want here. The second environment variable, LICENSE_KEY , holds the SingleStore license. In production scenarios, we’d pull this value from a k8s secret , but for this demo, paste your license key from t he customer portal into place in this file. The next environment variable, ROOT_PASSWORD , stores the password for the admin account. Like the license, in production scenarios we’d move this to a k8s secret. For this trial cluster, set this password to something interesting. You’ll use this password when you login with your apps and the SingleStore Studio browser app below. Service: The selector section matches the metadata from the deployment. This is how the service knows which pods to use to load-balance incoming traffic. Ports of type NodePort are exposed between 30,000 and 32,767, so we adjust the port numbers into this range. In the service, we route database traffic into k8s from port 30306 to the container on port 3306 , and we route SQL Studio traffic to k8s from port 30080 to the container on port 8080 . Through the magic of Kubernetes, only traffic on these two ports routes from the WAN side of the k8s router to the LAN side of the container. All other traffic is blocked. If either 30306 or 30080 is in use on your machine, change these to an open port between 30,000 and 32,767. It’s also possible to get Kubernetes to randomly assign a port, though that’s out of the scope for this article. Save the file, and we’re ready to launch the resources in Kubernetes. Starting the SingleStore Cluster Open a new terminal window in the same directory with the kubernetes-memsql.yaml file. This could be Powershell, a command prompt, or a regular terminal. Type this in the shell: kubectl apply -f kubernetes-memsql.yaml This tells Kubernetes to create (or adjust) the service and deployment definitions, and to startup the container. The output from the container isn’t streamed to the console. To see the status of the pod as it starts up, type: kubectl get all The results look like this: If the pod status doesn’t say Ready then we need to look at the container’s logs. Grab the pod name. (In this case it’s pod/memsql-6cfd48586b-8b2fj .) Then type: kubectl logs pod/memsql-YOUR_POD_HERE Substitute your pod name into place. If you get an error starting the cluster, double-check that the license key is correct and from the Docker whale icon, ensure that both Docker and Kubernetes mode are running. If you get an image pull failure, ensure your network connection is working as expected. To relaunch the Kubernetes content, type: kubectl delete -f kubernetes-memsql.yaml\nkubectl apply -f kubernetes-memsql.yaml Congratulations! We’ve launched a SingleStore cluster. Let’s dive in and start using it. Start SingleStore Studio Now that SingleStore is running in Kubernetes, let’s dive in and start using it. Open the browser to http://localhost:30080 to launch SingleStore Studio. Click on local cluster, enter username of root , use the password you set above, and login. Note: Using an alternate kubernetes environment? : With slight modifications, these instructions also work with Minikube, Microk8s, k3s, and other Kubernetes runtimes. Rather than browsing to http://localhost:30080, browse to the cluster’s IP address. For Minikube, open http://minikube:30080. Outside the scope of this article, you may need to switch the service to type LoadBalancer or adjust firewall rules if the cluster is not running locally. There are security implications to these changes. On this main dashboard screen, we can see the health of the cluster. Note that this is a two-node cluster. Clicking on Nodes on the left, we see one node is a leaf node, one is an aggregator node, and they’re both running in the same container. In production, we’d want more machines running together to support production-level workloads and to provide high availability. Click on the SQL Editor page, and we see the query window. In the query window, type each command, select the line, then push the execute button on the top-right. Loading Gist. Please Wait... For more details on SingleStore Studio, check out the docs or watch the SingleStore Studio tour video. Where Can We Go From Here? SingleStore is now ready for all the “kick the tires” tasks we need. You could: Hook up your analytics dashboard to SingleStore, connecting to localhost:30306 . Start your application and connect it to SingleStore using any MySQL connector. Create an ingest pipeline from Kafka, S3, or other data source. Being able to run these tasks from such a simple setup is a real time-saver. However, don’t expect the same robustness or performance as you would have with a full install on a full hardware configuration. Cleanup We’ve finished our experiment today. To stop the database, run this command: kubectl delete -f kubernetes-memsql.yaml This will delete the service and the deployment, which will delete the pod and stop the container. If you’re done experimenting with SingleStore, delete the image by running docker image rm memsql/cluster-in-a-box in the terminal. Or better yet, leave this image in place to quickly start up your next experiment. Run docker system prune to remove any stored data or dangling containers left in Docker Desktop and free up this space on your hard disk. Conclusion With the SingleStore cluster-in-a-box container and Kubernetes, we quickly provisioned a “kick the tires” SingleStore cluster. Aside from Docker itself, there was nothing to install, and thus cleanup is a breeze. We saw how easy it is to spin up a cluster, connect to it with SingleStore Studio, and start being productive. Now go build great things!", "date": "2019-12-01"},
{"website": "Single-Store", "title": "leveraging-aws-sagemaker-and-memsql-for-real-time-streaming-analytics", "author": ["Mark Lochbihler"], "link": "https://www.singlestore.com/blog/leveraging-aws-sagemaker-and-memsql-for-real-time-streaming-analytics/", "abstract": "On Tuesday, November 12th, SingleStore will be presenting a workshop at AWS Partner Developer Day. ( Click here to join, or here for the Presentation and Student Guide on Github.) The workshop will be held at the AWS office at 350 West Broadway in New York City. The workshop will be focused on teaching customers how to enable real-time, data-driven insights. In the workshop, Amazon Sagemaker, the AWS managed service for deploying machine learning models quickly, will be shown working with SingleStore, a highly performant, cloud-native database. SingleStore supports streaming data analytics and the ability to run operational analytics workloads on current data, blended in real time with historical data, not yesterday’s closed data from a nightly batch update. Putting Machine Learning into Production Productionalizing a machine learning (ML) model has three phases. The lifecycle of a model begins with a build phase, then a training phase. After that, a model is selected to be used by the business. Some of the selected champion models are deployed into production. These are the models that have the potential to drive significant, immediate business value. Many organizations today are leveraging Amazon SageMaker’s highly scalable algorithms and distributed, managed data science and machine learning platform to develop, train and deploy their models. In the workshop, we will begin by leveraging a Sagemaker Notebook to build and train an ML model. What SingleStore Adds to ML and AI SingleStore adds a great deal to machine learning and AI, and a large share of SingleStore customers are running machine learning models and AI programs in production using SingleStore. SingleStore helps modernize existing data infrastructure. The rapid flow of data through the infrastructure is vital to machine learning and AI. New tools, such as Python programs or the use of SparkML for running models, can be integrated with SingleStore directly. Processing is fast, running at production speeds. Big data systems based on Hadoop were sold and installed with the promise of supporting machine learning and AI. These systems do indeed gather large amounts of data together, where it’s used for the data science work that build machine learning models. However, when it comes time to run the models, HDFS – the file system for Hadoop – is often too slow. Data is not stored in SQL format, and custom queries are needed to retrieve key information. The queries, however, run too slowly for production. So data is transferred from the data lake to SingleStore for production. This has three key benefits: the data is now accessible to SQL queries; the queries run much faster; and the performance achieved is scalable, simply by adding more servers. SingleStore integrates with a wide range of tools popular for use in developing and implementing machine learning models – many of which use Python as the programming language. Tools include pandas, NumPy, TensorFlow, scikit-learn, and named connectors in Microsoft SQL Server Analysis Services (SSAS), an analytics processing and data mining tool for Microsoft SQL Server. SSAS can pull in information from a wide range of different sources. The R programming language is also used, due to its facility with statistical computing and its widespread use for data mining. All of these tools can be used along with SingleStore Pipelines. Data can be “scored on load” against a machine learning model. As data is transferred from a source – AWS S3, the Hadoop HDFS database, Kafka pipelines, or the computer’s file system – it is operated on by Python or executable code, scored against the model. This flexible capability allows models to be operationalized rapidly and for scoring to run at high speed. With SingleStore, user code can be implemented as user-defined functions (UDFs), stored procedures – the ability to run stored procedures on ingest is called Pipelines to Stored Procedures – user-defined aggregate functions, and more. Because SingleStore is relational, with ANSI SQL support, common SQL functions such as joins and filters, and SQL-compatible tools (such as most business intelligence [ BI ] programs) are not only easy to use, but run fast, with scalability. These capabilities allow SingleStore to work well as a partner environment for SageMaker. What Happens in the Workshop Once we have developed the ML model, it will be deployed in production, enabling potential significant value add to a real-time business process. This deployment step is also referred to as “operationalizing the model.” With Sagemaker, our deployed model will be exposed as an inference API endpoint. At this point, we are ready to consume the ML model in a streaming data ingestion pipeline. With SingleStore, both real time and historical data are available to both your training and operationalization environments, helping organizations greatly reduce time to value and operational costs, while at the same time improving the customer experience. As more data is made available to your model, it can be optimized over time, and dynamically redeployed and leveraged within SingleStore on AWS. Participants in this half-day, hands-on workshop will be taught how to jointly deploy SingleStore real-time streaming ingest pipelines with Sagemaker inference ML endpoints. If you are a Data Engineer, Architect, or Developer, this session is designed for you. We hope you can join us on November 12th in New York City, or at one of our future sessions. For further information, email Mark Lochbihler , Director of Technical Alliances at SingleStore, or Chaitanya Hazarey , ML Specialist Solutions Architect at AWS.", "date": "2019-11-08"},
{"website": "Single-Store", "title": "case-study-thorn-frees-up-resources-with-memsql-helios-to-identify-trafficked-children-faster", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/case-study-thorn-frees-up-resources-with-memsql-helios-to-identify-trafficked-children-faster/", "abstract": "Thorn’s child sex trafficking investigations tool, Spotlight , gathers information from escort sites to provide law enforcement with a tool to help find trafficked children, fast. (A Special Agent for the Wisconsin Human Trafficking Task Force describes Spotlight this way: “ It is the greatest tool we have in the fight against human trafficking. “) And using SingleStore is one of the ways they do it. SingleStore is a powerful solution that meets Thorn’s requirements, including SQL support; fast query response time; support for machine learning and AI; a large and scalable number of simultaneous users; and horizontal scale-out. Also, SingleStore runs just about anywhere, notably including on-premises installations and all the major public clouds . Still, Thorn had a business problem. As a tech non-profit, they are highly skilled at identifying and making tradeoffs that will allow their small team to deliver the biggest impact. In a constantly shifting digital environment, they know they need to focus on keeping Spotlight agile, to help find victims faster. So they need to keep the operation and maintenance of Spotlight as simple and easy to manage as possible. In support of this strategy, Thorn is moving to SingleStore Managed Service , the fully managed, on-demand, and elastic cloud database from SingleStore. Where SingleStore DB 7.0 meets Thorn’s database needs, SingleStore Managed Service meets Thorn’s operational needs – removing work from Thorn’s development and operations personnel, and leaving it in the hands of SingleStore. Peter Parente, data engineer at Thorn, puts it well: “We want to focus our time on building the application for our mission, rather than managing every detail of exactly how the data is going to be stored.” Now, Thorn can focus on growing Spotlight to meet the needs of its users and fulfill its mission: to build technology to defend children from sexual abuse. What Thorn Delivers As Thorn describes it, new technologies can be used by abusers to facilitate abuse – and, thankfully, the same new technologies can be leveraged to stop this abuse. Thorn leverages data to find trafficked children faster, building technology to create a world where every child can be safe, curious, and happy. There are more than 150,000 escort ads posted daily across the US, totaling in the millions of ads a year – and, somewhere in that mountain of data, children are being sold for sex. Thorn’s research shows that 63% of child sex trafficking survivors were advertised online at some point. Harnessing that data, Spotlight is offered for free to users who are involved in actively investigating child sex trafficking cases. When Thorn started several years ago, they only focused on a few problematic sites and online sources. Now, the number of sites with child sex trafficking content is increasing, and the user base for Spotlight has grown. Thorn is a strong example of the need that so many organizations have for nearly limitless scalability and concurrent access. “As time passes, we have greater data complexity. More data to store and more users that need to analyze that data,” says Parente. “There are more sites, and some of the sites have added features that increase the data flowing in from them as well.” But, even as the demands increase, so does Thorn’s effectiveness. Thorn has huge impact . Spotlight has been very successful, helping to identify over 10,000 trafficked children. On average, eight children a day are identified with it. And Thorn is proud of having sped up law enforcement investigation time , by as much as 63% – that is, they’ve cut time for investigations by nearly two-thirds. (Thorn also educates people on these topics; more than 3.5 million teens have learned to identify and prevent sextortion – extortion focused on nude images of the victim – through Thorn projects.) To work effectively, such a system needs to meet a number of technical requirements: Fast ingest and fast processing . Processing a site of interest quickly; finding matches in minutes, not hours; and synthesizing results to users for easier analysis. Fully scalable . Thorn needs to be able to speed up or extend the system by adding capacity in a horizontal, linear fashion. Fast query response time . As with finding matches and reporting, query response time must be fast – seconds, not minutes or hours. High concurrency . Thorn needs to be able to support an ever-increasing number of signed-in recipients and interrogators of its data from a small computing footprint, with full scalability to meet new demands. In addition, Thorn identified two business-oriented requirements, to allow them to fulfill their specific mission most effectively: Low-maintenance . Thorn needs to spend as much of their engineering time as possible improving Spotlight by expanding its feature set. Building a reliable, flexible, data pipeline to support their solution needs to be as hassle-free and worry-free as possible. No one but Thorn can do this work. By making the core system as low-maintenance as possible, Thorn frees up their technical talent for this vital work. Stateless . Thorn quickly identified Kubernetes as a core element of any solution. Kubernetes is very good for managing stateless components; stateful support has recently been added, but it’s still somewhat of a work in progress. (And will always be more complex than managing the stateless parts.) So Thorn sought to keep its solution stateless in as many components as possible, if not all of them. How SingleStore Managed Service Helps Thorn Succeed Thorn built a tool that meets all their requirements: Thorn finds new or updated content on targeted websites. The content is placed in an Amazon Simple Storage Service (S3) bucket. A scalable, Python data pipeline using the Dramatiq library ( similar to Celery ) receives notifications of new text and media content in S3 via Amazon’s Simple Query Service (SQS) and processes it. The data pipeline stores the processed, transformed data in SingleStore Managed Service for exploration in the Spotlight application. Trained investigators look for key details that indicate a child trafficking victim, to build their case, and to locate the most vulnerable victims. SingleStore Managed Service sits at the heart of the system. “It’s currently our primary data store,” according to Parente. Using Machine Learning and AI to Facilitate Identifications Thorn uses SingleStore’s Euclidean distance function for computing image similarity, resulting in very high throughput rates for image comparisons. The process is described in detail in this blog post from SingleStore co-CEO Nikita Shamgunov: SingleStore as a Data Backbone for Machine Learning and AI . The slide below shows the use of this function. Thorn has previously worked with SingleStore on advances in machine learning for image recognition . Using Amazon SQS as a Data Pipeline Thorn uses Amazon S3 and SQS as the input source for their data pipeline. Many other SingleStore customers have used Kafka in similar situations. (We recently published a case study featuring the Kafka-plus-SingleStore architecture from a major technology services company .) But Thorn finds Amazon SQS easier to maintain and manage. According to Parente, “Our data is not currently delivered to S3 in a streaming fashion. It’s more a set of micro batches. We don’t currently have a need for the streaming support that you typically see associated with Kafka.” “We rely on SQS to provide us with the notifications we need, as data is delivered into our S3 buckets,” continues Parente. “When we receive a notification, our data pipeline runs a set of machine learning models and natural language processing annotators before storing the results in SingleStore for use by our application.” SingleStore Managed Service Helps Thorn Achieve Statelessness Why has Thorn chosen SingleStore Managed Service, rather than self-managed SingleStore software, which they could install and run on AWS themselves? The main reason is to focus their technical resources on other areas. Every hour saved in database administration is an hour freed up for work that will speed up an investigator’s process, providing timely insights and aggregating information across time and space to find child victims faster. The features of SingleStore Managed Service lend themselves to Thorn’s needs. Thorn has designed their system in such a way as to offload software maintenance and management to the greatest degree possible, using Kubernetes as their management tool for most of the system, and SingleStore Managed Service – which is built on Kubernetes, and managed using it – as their core database. Kubernetes was originally developed for stateless services, and Thorn built their data pipeline (above) to be as stateless as possible. Parente says, “The pipeline workers are all stateless. If we fail processing some input data, the pipeline simply retries the input from S3 at some point in the future. Our processing is idempotent .” More recently, Kubernetes has added features for managing stateful software. To make these features work, stateful software such as SingleStore (or any database) requires a Kubernetes Operator, which serves as an interface between the database and Kubernetes. SingleStore has created a Kubernetes Operator and uses it for managing SingleStore Managed Service . SingleStore customers are also using this Operator in their own development efforts. Thorn could have used the SingleStore Operator to integrate self-managed SingleStore software into their Kubernetes management framework. Instead, they chose SingleStore Managed Service. “So in some way,” Parente continues, “the indirect answer to the question, ‘Are we depending on the stateful features of Kubernetes?,’ is ‘Yes – but indirectly, through Managed Service.’” Thorn maintains their stateless management framework by leaving the management of stateful software – their SingleStore database – to SingleStore, the company, through Managed Service. “One of the reasons we’re using SingleStore Managed Service is to offload having to manage that stateful data store,” continued Parente. “If we weren’t using Managed Service, and instead hosting our own database, we would be responsible for scaling it on Kubernetes, making sure data is retained after nodes restart, repartitioning data to take advantage of new nodes, and so on.” Thorn defers to other industry-leading experts for its other data store. “For S3, Amazon is managing the complexity,” says Parente. “The files are written, and then we assume that S3 works as advertised.” The same questions arise for both technologies: “Are we sure it’s backed up? Is it going to scale? We want to offload that onto other vendors, including AWS and SingleStore. That’s time better spent for our mission-oriented work. We focus more on how we build out our system, or surface the processed information to our users in the best available fashion.” This approach allows Thorn to work more closely with their users, improve the system to meet user needs, and get data out to them in the way they need it, in the formats and with the timeliness they need to prioritize the identification of child sex trafficking victims. Conclusion As Julie Cordua, CEO of Thorn, has said : “SingleStore is delivering a real impact for our organization by making real-time decisions and predictive analytics easier. And, because it easily scales to support our machine learning and AI needs, SingleStore helps us continually build better tools to find victims of trafficking and sexual abuse, faster. It is a true case of technology being applied in a way that will make a real difference in people’s lives.” You too can take advantage of the ease of use, ease of management, and reliability of SingleStore Managed Service. Use SingleStore for free or contact SingleStore today .", "date": "2019-12-05"},
{"website": "Single-Store", "title": "case-study-moving-to-kafka-and-ai-at-a-major-technology-services-company", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/case-study-moving-to-kafka-and-ai-at-a-major-technology-services-company/", "abstract": "How does a major technology services company equip itself to compete in the digital age – and provide services so outstanding that they can significantly advance the business prospects of their customers? All while reducing complexity, cutting costs, and tightening SLAs – in some cases, by 10x or more? For one such company, the solution is to deliver real-time, operational analytics with Kafka and SingleStore. In this company, data flowed through several data stores, and from a relational, SQL database, into NoSQL data stores for batch query processing, and back into SQL for BI, apps, and ad hoc queries. Now, data flows in a straight line, through Kafka and into SingleStore. Airflow provides orchestration. Before SingleStore: Custom Code, PostgreSQL, HDFS, Hive, Impala, and SQL Server At this technology services company, analytics is absolutely crucial to the business. The company needs analytics insights to deliver services and run their business. And they use their platform to provide reports and data visualizations to their customers. (We’re leaving the company unidentified so they can speak more freely about their process and their technology decisions.) The company’s original data processing platform was developed several years ago, and is still in use today – soon to be replaced by Kafka and SingleStore. Like so many companies at that time, they chose a NoSQL approach at the core of their analytics infrastructure. Data flowed through the analytics core in steps: A custom workflow engine brings in data and schedules jobs. The engine was written in Python to maximize flexibility in collecting data and scheduling data pipelines. The data is normalized and stored in PostgreSQL, one of the leading relational databases. Data then moves into HBase, the data store for Hadoop – a NoSQL system that provides the ability to version data at an atomic (columnar) level. In the next step, data moves to Apache Hive, the data warehousing solution for Hadoop. Then new, updated Parquet tables are created on Cloudera’s version of the Apache Impala Hadoop-to-SQL query engine. Data then moves to SQL Server, another leading relational database, where it can be accessed by traditional, SQL-based business intelligence (BI) tools. The previous architecture had data going from SQL to NoSQL, then back to SQL. This system has worked well for batch-type analytics and other batch-oriented use cases, which is still most of what the company does with data. And, at different stages, data was available through either a traditional SQL interface, or through the ecosystem that has developed around Hadoop/HDFS (ie Impala). Costs were reasonable, due to use of a combination of in-house, open source, and licensed software. And, because the data was in relational format, both before and after storage in HDFS, it was well-understood and orderly, compared to much of the data that is often stored in NoSQL systems. However, the company is moving into real-time operational analytics, machine learning (ML), and AI. Looking at ML and AI highlighted many of the issues with the analytics processing core at the company: Stale data . Data is batch-processed several times as it moves through the system. At each step, analytics are being run on older and older data. Yet, as the prospect of implementing AI showed, it’s the newest data that’s often the most valuable. Loss of information . As data moves into a relational database (PostgreSQL), then into a NoSQL storage engine (HDFS), then into a cache-like query system (Cloudera Impala), and finally to another relational database (Microsoft SQL Server), the level of detail in the data that can be pulled through at each step is compromised. Clumsy processes . All of the steps together have considerable operational overhead, and specific steps have their own awkward aspects. For instance, Cloudera Impala works by taking in entire files of data from Hive and making them available for fast queries. Updating data means generating entire new files and sending them to Impala. Operational complexity . The company has had to develop and maintain considerable technical expertise dedicated just to keeping things going. This ties up people who could otherwise be building new solutions. Not future-ready . As the company found when it wanted to move to ML and AI, their complex infrastructure prevented the embrace of new technology. Moving to Kafka and SingleStore The previous architecture used by the technical services company had been blocking their path to the future. So they’re moving to a new, simpler architecture, featuring streaming data and processing through SingleStore, with support for ML and AI. The company will even be driving robotics processes. They will track and log changes to the data. This will allow them to have a sort of “time travel,” as they refer to it. The company has described what they need in the new platform: Simplicity . Eliminate Hadoop to reduce complexity and delays. Eliminate Impala to cut out 40-50 minute waits to load big tables on updates. Data sources . Oracle, Salesforce, Sharepoint, SQL Server, Postres (100-plus sources). Concurrent processing . Scale from 20-25 concurrent jobs, maximum, to 200 or more. Query types . Simple, complex, hierarchical (analytics), aggregates, time-series; ad hoc. Query speed . Cut ETL from 12 hours to 1 hour or less. SQL support . Most of the company’s engineers are strong SQL developers – but they need a scalable platform, not a traditional, single-process relational database. Business benefits . Take processing past thresholds that are currently being reached; reduce worker wait times (more jobs/day, more data/day, distributed container support). What platform could do the job? As one of the project leads puts it, “We were awesomely pleased when we saw SingleStore. We can enter SQL queries and get the result in no time. This, we thought, can solve a lot of problems.” The company quickly knew that Kafka would be the best way to ingest data streaming in at high volume. So, when they investigated SingleStore, they were especially happy to find the Kafka pipeline capability , including exactly-once updating , in line with Kafka. This helped them move Kafka to a larger role in their planning – from only being used to feed AI, to a streaming pipeline for all their analytics data. The company is still in the design phase. Kafka and SingleStore have handled all the use cases they’ve thrown at the combination so far. They can replicate the same environment in the cloud and on-premises, then move workloads wherever it’s cost-effective and convenient. The company can also mix and match rowstore and columnstore tables. For instance, data can flow into a rowstore table, where they will perform concurrent transformations on it. They then aggregate new and existing data in a columnstore table, eventually totaling petabytes of information. Unlike most databases – such as with the Impala solution and Parquet files, in the current solution – SingleStore can update columnstore tables without having to re-build and re-load them. (This capability is enhanced in the upcoming SingleStore DB 7.0 , with faster seeks and, as a result, faster updates.) This matches one of the company’s biggest use cases: support for lab queries. The labs need nearly infinite disk space for ongoing storage of data. They have columnstore tables with hundreds of columns, and their old solution had trouble pulling them through the Hive metastore. SingleStore gives them the ability to support thousands of discrete columns. Since SingleStore also has built-in support for JSON, as the project lead puts it, “there really are no limits.” They can then move selected data into rowstore tables for intensive processing and analysis. The company stopped looking at alternatives after it found SingleStore. For instance, Citus for Postgres provides a distributed query engine, Citus, on top of a very mature database, PostgreSQL. “However,” says the project lead, “it has limitations, and requires a lot of up-front planning to make it work.” Design and Implementation: Airflow, Kafka, and SingleStore, plus Spark There are many databases that can do some of these things in a pilot project, with limited amounts of data – or, in particular, with limited numbers of users. But, as the project lead says, “The beauty of SingleStore is, if you get more users, you just add more nodes.” The technical services company will use Airflow as a scheduler. Airflow sends commands to remote systems to send their data. Commands such as Create Pipeline get data flowing. Airflow sends the data to be ingested into a Kafka topic, then drops the connection to the remote system. Airflow doesn’t hold data itself; it’s simply there for orchestration. A straight flow from Kafka to SingleStore, with processing against SingleStore, speeds operations with fresh data. The company then uses a MySQL interface – SingleStore is MySQL wire protocol-compatible – to ingest the data into SingleStore. With the data in SingleStore, they have almost unlimited options. They can run Python code against database data, using SingleStore transforms and Pipelines to stored procedures . “Our only cost,” says the team lead, “is team members’ time to develop it. This works for us, even in cases where we have a one-man design team. Data engineers leverage the framework that we’ve built, keeping costs low.” The former analytics processing framework, complex as it was, included only open source code. “This is the first time we’ve introduced subscription-based licensing” into their analytics core, says the project lead. “We want to show the value. In the long run, it saves money.” They will be exploring how best to use Spark with SingleStore – a solution many SingleStore users have pioneered. For instance, they are intrigued by the wind turbine use case that SingleStore co-CEO Nikita Shamgunov demonstrated at Spark Summit. This is a real, live use case for the company. They currently have a high-performance cluster with petabytes of storage. They will be scaling up their solution with a combination of SingleStore and Spark, running against a huge dataset, kept up to date in real time. “The core database, by itself, is the most valuable piece for us,” says the project lead. “And the native Kafka integration is awesome. Once we also get the integration with Spark optimized – we’ve got nothing else that compares to it, that we’ve looked at so far. The company runs Docker containers for nearly everything they do and is building up use of Kubernetes. They have been pleased to learn that SingleStore Managed Service , SingleStore’s new, elastic cloud database, is also built on Kubernetes. It gives the project lead a sense of what they can do with SingleStore and Kubernetes, going forward. “That’s the point of containerization,” says the project lead. “Within a minute or two, you have a full cluster.” “We’ve gone from a screen full of logos,” says the project lead, “in our old architecture, to three logos: Airflow, Kafka, and SingleStore. We only want to deal with one database.” And SingleStore will more than do the job.", "date": "2019-11-16"},
{"website": "Single-Store", "title": "7-0-release-time-series-functionality", "author": ["Eric Hanson"], "link": "https://www.singlestore.com/blog/7-0-release-time-series-functionality/", "abstract": "SingleStore is uniquely suited to real-time analytics, where data is being ingested, updated, and queried concurrently with aggregate queries. Real-time analytics use cases often are based on event data, where each separate event has a timestamp. It’s natural to interpret such a sequence of events as a time series. Prior to the 7.0 release, SingleStore delivered many capabilities that make it well-suited to time-series data management [ Han19 ] . These include: a scaled-out, shared-nothing architecture that supports transactional and analytical workloads with a standard SQL interface, fast query execution via compilation and vectorization, combined with scale out, ability to load data phenomenally fast using the Pipelines feature , which supports distributed, parallel ingestion, non-blocking concurrency control so readers and writers never make each other wait, window functions for ranking, moving averages, and so on, a highly-compressed columnstore data format suitable for large historical data sets. Hence, many of our customers are using SingleStore to manage time series data today. For the SingleStore DB 7.0 release, we decided to build some special-purpose features to make it even easier to manage time-series data. These include FIRST(), LAST(), TIME _ BUCKET(), and the ability to designate a table column as the SERIES TIMESTAMP [ Mem19a-d ] . Taken together, these allow specification of queries to summarize time series data with far fewer lines of code and fewer complex concepts. This makes expert SQL developers more productive, and opens up the ability to query time series data to less expert developers. We were motivated to add special time series capability in SingleStore DB 7.0 for the following reasons: Many customers were using SingleStore for time series data already, as described above. Customers were asking for additional time series capability. Bucketing by time, a common time series operation, was not trivial to do. Use of window functions, while powerful for time-based operations, can be complex and verbose. We’ve seen brief syntax for time bucketing in event-logging data management platforms like Splunk [ Mil14 ] and Azure Data Explorer (Kusto) [ Kus19 ] be enthusiastically used by developers. We believe we can provide better overall data management support for customers who manage time series data than the time series-specific database vendors can. We offer time series-specific capability and also outstanding performance, scalability, reliability, SQL support, extensibility, rich data type support, and so much more. Designating a Time Attribute in Metadata To enable simple, brief SQL operations on time series data, we recognized that all our new time series functions would have a time argument. Normally, a table has a single, well-known time attribute. Why not make this attribute explicit in metadata, and an implicit argument of time-based functions, so you don’t have to reference it in every query expression related to time? So, in SingleStore DB 7.0 we introduced a special column designation, SERIES TIMESTAMP, that indicates a default time column of a table. This column is then used as an implicit attribute in time series functions. For example, consider this table definition: CREATE TABLE tick(\n  ts datetime(6) **series timestamp**,\n  symbol varchar(5),\n  price numeric(18,4)); It defines a table, tick , containing hypothetical stock trade data. The ts column has been designated as the series timestamp. In examples to follow, we’ll show how you can use it to make queries shorter and easier to write. The Old Way of Querying Time Series Before we show the new way to write queries briefly using time series functions and the SERIES TIMESTAMP designation in 7.0, consider an example of how SingleStore could process time series data before 7.0. We’ll use the following data for examples: INSERT INTO tick VALUES\n ('2020-02-18 10:55:36.179760', 'ABC', 100.00),\n ('2020-02-18 10:57:26.179761', 'ABC', 101.00),\n ('2020-02-18 10:59:16.178763', 'ABC', 102.50),\n ('2020-02-18 11:00:56.179769', 'ABC', 102.00),\n ('2020-02-18 11:01:37.179769', 'ABC', 103.00),\n ('2020-02-18 11:02:46.179769', 'ABC', 103.00),\n ('2020-02-18 11:02:59.179769', 'ABC', 102.60),\n ('2020-02-18 11:02:46.179769', 'XYZ', 103.00),\n ('2020-02-18 11:02:59.179769', 'XYZ', 102.60),\n ('2020-02-18 11:03:59.179769', 'XYZ', 102.50); The following query works in SingleStore DB 6.8 and earlier. As output, it produces a separate row, for each stock, for each hour it was traded at least once. (So if a stock is traded ten or more times, in ten separate hours, ten rows are produced for that stock. A row will contain either a single trade, if only one trade occurred in that hour, or a summary of the trades – two or more – that occurred during the hour.) Each row shows the time bucket, stock symbol, and the high, low, open, and close for the bucket period. (If only one trade occurred in that hour, the high, low, open, and close will all be the same – the price the stock traded at in that hour.) WITH ranked AS\n(SELECT symbol,\n    RANK() OVER w as r,\n    MIN(price) OVER w as min_pr,\n    MAX(price) OVER w as max_pr,\n    FIRST_VALUE(price) OVER w as first,\n    LAST_VALUE(price) OVER w as last,\n    from_unixtime(unix_timestamp(ts) div (60*60) * (60*60)) as ts\n    FROM tick\n    WINDOW w AS (PARTITION BY symbol, \n               from_unixtime(unix_timestamp(ts) div (60*60) * (60*60)) \n               ORDER BY ts\n               ROWS BETWEEN UNBOUNDED PRECEDING\n               AND UNBOUNDED FOLLOWING))\n \nSELECT ts, symbol, min_pr, max_pr, first, last\nFROM ranked\nWHERE r = 1\nORDER BY symbol, ts; This query produces the following output, which can be used to render a candlestick chart [ Inv19 ] , a common type of stock chart. +---------------------+--------+----------+----------+----------+----------+\n| ts                  | symbol | min_pr   | max_pr   | first    | last     |\n+---------------------+--------+----------+----------+----------+----------+\n| 2020-02-18 10:00:00 | ABC    | 100.0000 | 102.5000 | 100.0000 | 102.5000 |\n| 2020-02-18 11:00:00 | ABC    | 102.0000 | 103.0000 | 102.0000 | 102.6000 |\n| 2020-02-18 11:00:00 | XYZ    | 102.5000 | 103.0000 | 103.0000 | 102.5000 |\n+---------------------+--------+----------+----------+----------+----------+ The query text, while understandable, is challenging to write because it uses a common table expression (CTE), window functions with a non-trivial window definition, a subtle use of ranking to pick one row per group, and a non-obvious divide/multiply trick to group time to a 60 * 60 second bucket. New Time-Series Functions in SingleStore DB 7.0 Here I’ll introduce the new time series functions, and then show an example where we write an equivalent query to the “candlestick” query above using the new functions. I think you’ll be impressed by how concise it is! Also see the latest documentation for analyzing time series data and for the new time series functions . FIRST() The FIRST() function is an aggregate function that takes two arguments, as follows: FIRST (value[, time]); Given a set of input rows, it returns the value for the smallest associated time. The second argument is optional. If it is not specified, it is implicitly the SERIES TIMESTAMP column of the table being queried. It’s an error if there is no SERIES TIMESTAMP available, or if there is more than one available in the context of the query where FIRST is used; in that case, you should specify the time explicitly. For example, this query gives the symbol of the first stock traded among all stocks in the tick table: SELECT first(symbol) FROM tick; The result is ABC, which you can see is the first one traded at 10:55:36.179760 in the rows inserted above. LAST() LAST is just like FIRST except it gives the value associated with the latest time. TIME _ BUCKET() TIME _ BUCKET takes a time value and buckets it to a specified width. You can use very brief descriptions of bucket width, like ‘1d’ for one day, ‘5m’ for five minutes, and so on. The function takes these arguments: TIME_BUCKET (bucket_width [, time [,origin]]) The only required argument is bucket _ width. As with FIRST and LAST, the time argument is inferred to be the SERIES TIMESTAMP if it is not specified. The origin argument is used if you want your buckets to start at a non-standard boundary – say, if you want day buckets that begin at 8am every day. Putting It All Together Now that we’ve seen FIRST, LAST, TIME _ BUCKET, and SERIES TIMESTAMP, let’s see how to use all of them to write the candlestick chart query from above. A new version of the same query is simply: SELECT time_bucket('1h') as ts, symbol, min(price) as min_pr,\n    max(price) as max_pr, first(price) as first, last(price) as last\nFROM tick\ngroup by 2, 1\norder by 2, 1; The new version of the query produces this output, which is essentially the same as the output of the original query. +----------------------------+--------+----------+----------+----------+----------+\n| ts                         | symbol | min_pr   | max_pr   | first    | last     |\n+----------------------------+--------+----------+----------+----------+----------+\n| 2020-02-18 10:00:00.000000 | ABC    | 100.0000 | 102.5000 | 100.0000 | 102.5000 |\n| 2020-02-18 11:00:00.000000 | ABC    | 102.0000 | 103.0000 | 102.0000 | 102.6000 |\n| 2020-02-18 11:00:00.000000 | XYZ    | 102.5000 | 103.0000 | 103.0000 | 102.5000 |\n+----------------------------+--------+----------+----------+----------+----------+ Look how short this query is! It is 5 lines long vs. 18 lines for the previous version. Moreover, it doesn’t use window functions or CTEs, nor require the divide/multiply trick to bucket time. It just uses standard aggregate functions and scalar functions. Conclusion SingleStore DB 7.0 makes it much simpler to specify many time-series queries using special functions and the SERIES TIMESTAMP column designation. For a realistic example, we reduced lines of code by more than three-fold, and eliminated the need to use some more advanced SQL concepts. Given the high performance, unlimited scalability, and full SQL support of SingleStore, it was a strong platform for time series data in earlier releases. Now, in SingleStore DB 7.0 , we’ve taken that power and added greater simplicity with these new built-in capabilities. How can you apply SingleStore DB 7.0 to your time-oriented data? References [ Han19 ] Eric Hanson, What SingleStore Can Do For Time Series Applications, https://www.singlestore.com/blog/what-memsql-can-do-for-time-series-applications/ , March 2019. [ Inv19 ] Understanding Basic Candlestick Charts, Investopedia, https://www.investopedia.com/trading/candlestick-charting-what-is-it/ , 2019. [ Kus19 ] Summarize By Scalar Values, Azure Data Explorer Documentation, https://docs.microsoft.com/en-us/azure/kusto/query/tutorial#summarize-by-scalar-values , 2019. [ Mem19a ] FIRST, SingleStore Documentation, https://docs.singlestore.com/v7.0/reference/sql-reference/time-series-functions/first/ , 2019. [ Mem19b ] LAST, SingleStore Documentation, https://docs.singlestore.com/v7.0/reference/sql-reference/time-series-functions/last/ , 2019. [ Mem19c ] TIME _ BUCKET, SingleStore Documentation, https://docs.singlestore.com/v7.0/reference/sql-reference/time-series-functions/time _ bucket/ , 2019. [ Mem19d ] CREATE TABLE Topic, SERIES TIMESTAMP, https://docs.singlestore.com/v7.0/reference/sql-reference/data-definition-language-ddl/create-table/ , 2019. [ Mil14 ] James Miller, Splunk Bucketing, Mastering Splunk, O’Reilly, https://www.oreilly.com/library/view/mastering-splunk/9781782173830/ch03s02.html , 2014.", "date": "2019-12-05"},
{"website": "Single-Store", "title": "7-0-release", "author": ["Peter Guagenti"], "link": "https://www.singlestore.com/blog/7-0-release/", "abstract": "SingleStore’s breakthrough new Universal Storage™ data management, enhanced system of record capabilities, time series enhancements, and more, are now available instantly and on demand with SingleStore Managed Service. You can try SingleStore DB 7.0 and Managed Service for free , instantly. SingleStore is proud to announce the general availability of SingleStore DB 7.0, available first on SingleStore Managed Service, the company’s elastic cloud database available on public cloud providers around the globe. Fully managed and available on demand, SingleStore Managed Service delivers instant, effortless access to the world’s fastest, most scalable data platform for operational analytics, machine learning and AI. SingleStore DB 7.0 delivers new, fast resilience features; the first iteration of SingleStore Universal Storage, delivering table type convergence; new time series features; and other new features, described below. With the public availability of SingleStore DB 7.0 — available now on SingleStore Managed Service and for download on December 10th— SingleStore further cements itself as a powerful fit for a company’s innovative and most critical operational workloads. SingleStore Managed Service delivers enhanced ease of use and reduced management complexity, lower total cost of ownership (TCO) compared to both on-premises and cloud provider offerings, and the flexibility to run your data workloads in multiple cloud providers and hybrid deployments. Also see the AWS-SingleStore press release for SingleStore Managed Service and the SingleStore DB 7.0 press release . What’s New in SingleStore DB 7.0 We previously described two key sets of features in SingleStore DB 7.0: Resilience features . Resilience features allow database content to survive server and software failures. Resilience features in SingleStore DB 7.0 include much faster synchronous replication and synchronous durability, and incremental backup, which captures only recently changed data. SingleStore Universal Storage features . SingleStore Universal Storage will eventually see data stored in a single table type whose size is not limited by the available memory, yet still provides the highest level of performance for both transactional and analytical operations. SingleStore DB 7.0 features include rowstore table data compression and fast seeks in columnstore tables . Fast sync replication and sync durability in SingleStore DB 7.0 make it possibleto run in high availability mode with only a small performance hit. SingleStore DB 7.0 also includes a wide range of additional features . Highlights include: Improvements in SingleStore Studio . SingleStore Studio features available now include logical monitoring of nodes; physical monitoring of cluster resource usage; the ability to find, understand, and kill long-running queries; and the ability to separate out SQL Editor results into tabs, then export them into a CSV file for analysis. Improvements in SingleStore Tools . SingleStore tools enable installation and control of self-managed SingleStore clusters, on-premises and in the cloud. SingleStore Tools include an improved cluster setup utility and easier migration from the original SingleStore Ops to the new tools. Tools also perform upgrades from SingleStore DB 6.7 and SingleStore DB 6.8 to SingleStore DB 7.0. Automatic gathering of SingleStore statistics . All types of statistics, including range and cardinality, are now automatically gathered, on all types of tables. This is a big ease-of-use improvement and dovetails with the benefits of SingleStore Managed Service and the move to SingleStore Universal Storage. Time Series functions . SingleStore DB 7.0 includes the new FIRST, LAST, and TIME_BUCKET functions, and the ability to designate a SERIES TIMESTAMP column in metadata, greatly simplifying code for time series processing with SingleStore . Additional improvements . These include improvements to query execution, query optimization, data storage, data loading, and data backup, as well as cross-database views. SingleStore Managed Service and SingleStore DB 7.0 continue to include robust free options . SingleStore Managed Service has a free 8-hour trial; the self-managed SingleStore software can be used indefinitely for free , within limits on the number of nodes used, and with community support only. The updated Visual Explain feature in SingleStore Studio. Benefits of SingleStore Managed Service With the initial release of SingleStore DB 7.0 occurring through SingleStore Managed Service, on AWS and other cloud platforms, the combined advantages of SingleStore Managed Service and SingleStore DB 7.0 become available on AWS. SingleStore Managed Service advantages include: Effortless deployment and elastic scale . With SingleStore Managed Service, you get the full capabilities of one-click deployment and easy cloud scalability. Users skip all the initial stages that are usually required for database availability: hardware procurement; racking servers; operating system and libraries installation and testing; database software deployment and configuration; and management of VMs or containers and their uptime. With SingleStore Managed Service, customers simply choose the number of nodes they wish. Managed Service starts them up and keeps them running. It handles data backup and restore, and managing and maintaining the software, using the cloud-native SingleStore Kubernetes Operator and the SingleStore Kubernetes stack . Enhanced ease of use and reduced TCO . Automatic setup and automated resizing of SingleStore clusters greatly increases ease of use for customers. (Resizing is handled on request, and will be further automated in future versions of SingleStore Managed Service.) In Managed Service, SingleStore handles much of the work, and many of the issues, that were formerly left to customer DevOps staff – either alone, or in cooperation with SingleStore Support. Staff are freed up as SingleStore Managed Service helps eliminate tedious, time-consuming tasks. As a cloud resource, SingleStore Managed Service ensures that the organization pays only for what it needs. (These aspects of SingleStore Managed Service are greatly enhanced by the new features in SingleStore DB 7.0, below.) Greatly increased flexibility . SingleStore combines the capabilities of OLTP databases and online analytical processing (OLAP) databases into a single database. The same database manages streaming data on ingest and processing, and replaces complex extract, transform, and load (ETL) processes with SingleStore Pipelines. It also supports both high concurrency of query volume and high query complexity. This eliminates data sprawl – the need for multiple copies of the same data – and reduced the number of data tools you need to support a given application. You get less complexity and up-to-date data for your applications. A converged database, managed by SingleStore as the provider, with SQL support and full scalability – that is, SingleStore Managed Service, powered by SingleStore DB 7.0 – offers a breakthrough capability to power rapid growth for organizations large and small. Next Steps SingleStore Managed Service, powered by SingleStore DB 7.0, is available now. You can get started instantly with SingleStore today for free or contact Sales .", "date": "2019-12-05"},
{"website": "Single-Store", "title": "spin-up-a-memsql-cluster-on-docker-desktop-in-10-minutes", "author": ["Rob Richardson"], "link": "https://www.singlestore.com/blog/spin-up-a-memsql-cluster-on-docker-desktop-in-10-minutes/", "abstract": "Even though SingleStore is a distributed system, you can run a minimal version of SingleStore on your laptop in Docker. We tell you how in this blog post. The combination of free use of the software, and being able to run SingleStore on your laptop, can be extremely convenient for demos, software testing, developer productivity, and general fooling around. In this post we’ll quickly build a single-instance SingleStore cluster running on Docker Desktop, on a laptop computer, for free. You’ll need a machine with at least 8GB RAM and four CPUs. This is ideal for quickly provisioning a system to understand the capabilities of the SQL engine. Everything we build today will be running on your machine, and with the magic of Docker containers, we’ll not need to install or configure much of SingleStore to get it running. The steps here are: get a free SingleStore license; install Docker Desktop; create a Docker Compose file; start the SingleStore cluster through Docker; and browse to SingleStore Studio. You’ll have a bare-bones SingleStore cluster running on your laptop machine in no time. Why Docker, and Why a Single Docker Container? Docker is a great way to run software in a protected sandbox and easy-to-manage environment, with less overhead than a virtual machine (VM) – and much less than a dedicated server. You can use it to spin up applications, systems, and virtual hardware to try out software or to quickly spin up a database to support local application development. We’ll use Docker here to provision and spin up a free SingleStore cluster, and just as easily, destroy it when we’re done. Using Docker makes it much easier to run a small SingleStore cluster without interfering with other software running on the machine. Pre-installed in the cluster-in-a-box container image is an aggregator node, a leaf node, and SingleStore Studio, which is a browser-based SQL editor and database maintenance tool, all running in one place, all pre-configured to work together. The minimal hardware footprint wouldn’t be nearly enough for production workloads, but it allows us to quickly spin up a cluster, connect it to our project, and try things out. We could use a Virtual Machine (VM), but Docker containers are lighter-weight than a virtual machine (VM). Like virtual machines, Docker provides a sandbox between processes. But unlike VMs, containers virtualize the operating system instead of the hardware, and Docker’s configuration-as-code mindset ensures we can quickly provision a complete virtual system from a small text file stored in Git. With the single-container SingleStore cluster described here, you can craft the simplest of tables all the way up to running a complex app, a dashboard, a machine learning model, streaming ingest from Kafka or Spark, or anything else you can think of against SingleStore. You’ll quickly understand the methodology and features of SingleStore, and can plan accordingly. The SingleStore cluster-in-a-box container has minimum hardware specs disabled, but you’ll still want a machine with at least 8 GB RAM and four CPUs. With specs well below SingleStore’s limits , you’ll see poor performance, so this system is definitely not the right setup for a proof of concept (PoC). But you can use this setup to experience the full features of SingleStore, and understand how it applies to your business problems. Once you know how SingleStore works, you can take these experiments and use what you learn to help you achieve your service-level agreements (SLAs) on distributed clusters that meets SingleStore’s minimum requirements and your high-availability needs. That’s when you can really open up the throttle, learn how your data performs on SingleStore, and dial in system performance for your production workloads. Cluster-in-a-box Multi-node Cluster Hardware Laptop computer Many hefty servers Best use-case * Try out SingleStore * Test SingleStore capabilities * Prototyping * Proof of concept (PoC) * Production workloads * High availability * High availability Cost Free up to four nodes with 32GB RAM each, and with community support Free up to four nodes with 32GB RAM each, and with community support Sign Up for SingleStore To get a free license for SingleStore, register at singlestore.com/free and click the link in the confirmation email. Then go to the SingleStore customer portal and login. Click “Licenses” and you’ll see your license for running SingleStore for free. This license never expires, and is good for clusters up to four machines and up to 128GB of combined RAM. This is definitely not the license you’ll want for a production cluster, but it’s great for these “kick the tires” scenarios. Note this license key. We’ll need to copy/paste it into place next. Install Docker Desktop The first step in getting our SingleStore cluster running in Docker Desktop is to get Docker Desktop installed. If you already have a recent version of Docker Desktop, you need only ensure you’re in Linux containers mode. Docker’s install requirements are quite specific, though most modern mid-range systems will do. Docker Desktop for Windows runs a Linux VM in Hyper-V, and Hyper-V requires Windows 10 Pro or Enterprise. Docker Desktop for Mac runs a Linux VM in xhyve, and requires a 2010 or newer model with macOS 10.13 or better. To install Docker Desktop, go to Docker Hub and choose the Docker Desktop version for your operating system. The download will require you to create a free account. Run the downloaded installer and accept all the defaults. Note for Windows users : If you are doing a fresh install, ensure you choose “Linux Containers” mode. If you installed Docker previously, ensure you’re running in Linux containers mode. Right-click on the Docker whale in the system tray (bottom-right by the clock), and choose “Switch to Linux Containers”. If it says “Switch to Windows Containers”, you’re already in the right place – that is, in Linux Containers mode. Note: Adding more RAM : Though not required, SingleStore will definitely behave better when Docker Desktop has more capacity. Click on the Docker whale, choose “Settings…” on Windows or “Preferences…” on Mac, click on the “Advanced” tab. If your machine has more than 8 GB RAM, set this to 8192. If your machine has 8 GB RAM or less, set it as high as you can. Then change the CPU count from 2 to 4. Create a Docker Compose File A docker-compose.yaml file gives Docker Desktop instructions to spin up one or more containers together. It’s a great way to capture all the docker pull , docker build , and docker run details. This file doesn’t replace Dockerfile but rather makes it much easier to use them. We’ll use the memsql/cluster-in-a-box image built by SingleStore and available on Docker Hub . Pre-installed in this image is the SingleStore database engine and SingleStore Studio. The minimum system requirements are disabled in this “cluster-in-a-box” configuration. Create an empty directory and create a file named docker-compose.yaml inside. Open this file in your favorite code editor and paste in this content: Loading Gist. Please Wait... A yaml file is a text file that’s great for capturing our architecture setup. As with Python source code, white space is significant. Yaml uses two spaces, not tabs. Double-check the yaml file to ensure each section is indented with exactly two spaces. If you have more or fewer spaces, or if you’re using tabs, you’ll get an error on startup. Here are the sections in the docker-compose.yaml file: We’re using the Docker Compose syntax Version 2. The services array lists all the containers that should start up together. We’ve only defined a single container here, named memsql , which uses the memsql/cluster-in-a-box image built by SingleStore. In the Ports section, we identify inbound traffic that’ll route into the container. Open port 3306 for the database engine and port 8080 for SingleStore Studio. If either of these ports are in use on your machine, change only the left port. For example, to connect from outside Docker to the database on port 3307 use 3307:3306 . The first environment variable, START_AFTER_INIT , exists for legacy reasons. Without this environment variable, the container will spin up, initialize the SingleStore cluster, and immediately stop. This is great for debugging, but not the behavior we want here. The second environment variable, LICENSE_KEY , is the placeholder for the license we got from the customer portal. Don’t copy the license key into place here — you’ll accidentally leak secrets into source control. Instead, this syntax notes that we’ll reference an environment variable set in the terminal. In time, we could easily add our application, business intelligence (BI) dashboard, and other resources to this file. If you have an existing docker-compose.yaml file, you can copy that content into place here too. Save the file, and we’re ready to launch Docker. Starting the SingleStore Cluster Open a new terminal window in the same directory as the docker-compose.yaml file. This could be Powershell, a command prompt, or a regular terminal. First, we’ll set the license key as an environment variable. Copy the license from the customer portal , in the Licenses tab, and create an environment variable in the terminal: Command prompt:\\ set LICENSE_KEY=paste_license_key_here \\\nPowershell:\\ $env:LICENSE_KEY = 'paste_license_key_here' \\\nMac/Linux/Git Bash:\\ export LICENSE_KEY=paste_license_key_here Paste your actual license key in place of paste_license_key_here. It’s really long and probably ends with == . Next, type this in the shell: docker-compose up This tells Docker to pull or build all the images, start up all the containers in our docker-compose.yaml file, and stream the console output from each container to our terminal. I find it fascinating to watch each application spew their innards here. If you get an error starting the cluster, double-check that the license key is correct and that Docker is running. If you get an image pull failure, ensure your network connection is working as expected. To retry, type Cntrl-C in the terminal, then type: docker-compose down \\ docker-compose up Congratulations! We’ve launched a SingleStore cluster. Let’s dive in and start using it. Start SingleStore Studio Now that SingleStore is running in Docker, let’s dive in and start using it. Open the browser to http://localhost:8080 to launch SingleStore Studio. Click on the local cluster, enter username of root , use the password you used above, and login. On this main dashboard screen, we can see the health of the cluster. Note that this is a two-node cluster. Clicking on Nodes on the left, we see one node is a leaf node, one is an aggregator node, and they’re both running in the same container. In production, we’d want more machines running together to support production-level workloads and to provide high availability. Click on the SQL Editor page, and we see the query window. In the query window, type each command, select the line, then push the execute button on the top-right: Loading Gist. Please Wait... For more details on SingleStore Studio, check out the docs or watch the SingleStore Studio tour video. Where Can We Go From Here? SingleStore is now ready for all the “kick the tires” tasks we need. You can: Hook up your analytics dashboard to SingleStore. Add your application to the docker-compose.yaml file and connect it to SingleStore using any MySQL connector . Create an ingest pipeline from Kafka, S3, Spark, or other data source. Being able to run these tasks from such a simple setup is a real time-saver. However, don’t expect the same robustness or performance as you would have with a full install on a full hardware configuration . Cleanup We’ve finished our experiment for today. To stop the database, push Ctrl-C in the terminal, and type: docker-compose down This will stop all the containers in the docker-compose.yaml file. If you’re done experimenting with SingleStore, delete the image by running docker image rm memsql/cluster-in-a-box in the terminal. Or better yet, leave this image in place to quickly start up your next experiment. Run docker system prune to remove any stored data or dangling containers left in Docker Desktop and free up this space on your hard disk. Conclusion With the SingleStore cluster-in-a-box container and Docker Desktop, we quickly provisioned a “kick the tires” SingleStore cluster. Aside from Docker itself, there was nothing to install, and thus cleanup is a breeze. We saw how easy it is to spin up a cluster, connect to it with SingleStore Studio, and start being productive. Now get a copy of SingleStore for free and go build great things!", "date": "2019-11-27"},
{"website": "Single-Store", "title": "webinar-time-series-memsql-7-0", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/webinar-time-series-memsql-7-0/", "abstract": "With the SingleStore DB 7. 0 release, SingleStore has added more special-purpose features, making it even easier to manage time series data within our best-of-breed operational database. These new features allow you to structure queries on time series data with far fewer lines of code and with less complexity. With time series features in SingleStore, we make it easier for any SQL user, or any tool that uses SQL, to work with time series data, while making expert users even more productive. In a recent webinar ( view the recording here ), Eric Hanson described the new features and how to use them. The webinar begins with an overview of SingleStore, then describes how customers have been using SingleStore for time series data for years, prior to the SingleStore DB 7.0 release. Then there’s a description of the time series features that SingleStore has added, making it easier to query and manage time series data, and a Q&A section at the end. Introducing SingleStore SingleStore is a very high-performance scalable SQL relational database system. It’s really good for scalable operations, both for transaction processing and analytics on tabular data. Typically, it can be as much as 10 times faster, and three times more cost-effective, than legacy database providers for large volumes under high concurrency. We like to call SingleStore the No-Limits Database because of its amazing scalability. It’s the cloud-native operational database that’s built for speed and scale. We have capabilities to support operational analytics. So, operational analytics is when you have to deliver very high analytical performance in an operational database environment where you may have concurrent updates and queries running at an intensive, demanding level. Some people like to say that it’s when you need “Analytics with an SLA.” Now, I know that everybody thinks they have an SLA when they have an analytical database, but when you have a really demanding SLA like requiring interactive, very consistent response time in an analytical database environment, under fast ingest, and with high concurrency, that’s when SingleStore really shines. We also support predictive ML and AI capabilities. For example, we’ve got some built-in functions for vector similarity matching. Some of our customers were using SingleStore in a deep learning environment to do things like face and image matching and customers are prototyping applications based on deep learning like fuzzy text matching. The built-in dot product and Euclidean distance functions we have can help you make those applications run with very high performance. (Nonprofit Thorn is one organization that uses these ML and AI-related capabilities at the core of their app, Spotlight, which helps law enforcement identify trafficked children. – Ed.) Also, people are using SingleStore when they need to move to cloud or replace legacy relational database systems. When they reach some sort of inflection point, like they know they need to move to cloud, they want to take advantage of the scalability of the cloud, they want to consider a truly scalable product, and so they’ll look at SingleStore. Also, when it comes time to re-architect the legacy application – if, say, the scale of data has grown tremendously, or is expected to change in the near future, people really may decide they need to find a more scalable and economical platform for their relational data, and that may prompt them to move to SingleStore. Here are examples of the kinds of workloads and customers we support: Half of the top 10 banks banks in North America, two of the top three telecommunications companies in North America, over 160 million streaming media users, 12 of the Fortune 50 largest companies in the United States, and technology leaders from Akamai to Uber. If you want to think about SingleStore and how it’s different from other database products, you can think of it as a very modern, high-performance, scalable SQL relational database. We have all three: speed, scale, and SQL. We get our speed because we compile queries to machine code. We also have in-memory data structures for operational applications, an in-memory rowstore structure, and a disk-based columnstore structure. We compile queries to machine code and we use vectorized query execution on our columnar data structure. That gives us tremendous speed on a per-core basis. We’re also extremely scalable. We’re built for the cloud. SingleStore is a cloud-native platform that can gang together multiple computers to handle the work for a single database, in a very elegant and high-performance fashion. There’s no real practical limit to scale when using SingleStore. Finally, we support SQL. There are some very scalable database products out there in the NoSQL world that are fast for certain operations, like put and get-type operations that can scale. But if you try to use these for sophisticated query processing, you end up having to host a lot of the query processing logic in the application, even to do simple things like joins. It can make your application large and complex and brittle – hard to evolve. So SQL, the relational data model, was invented by EF Codd (PDF) – back around 1970 – for a reason. To separate your query logic from the physical data structures in your database, and to provide a non-procedural query language that makes it easier to find the data that you want from your data set. The benefits that were put forth when the relational model was invented are still true today. We’re firmly committed to relational database processing and non-procedural query languages with SQL. There’s tremendous benefits to that, and you can have the best of both. You can have speed, and you can have scale, along with SQL. That’s what we provide. How does SingleStore fit into the rest of your data management environment? SingleStore provides tremendous support for analytics , application systems like dashboards, ad-hoc queries, and machine learning. Also other types of applications like real-time decision-making apps, Internet of Things apps, dynamic user experiences. The kind of database technology that was available before couldn’t provide the real-time analytics that are necessary to give the truly dynamic user experience people are looking for today; we can provide that. We also provide tremendous capabilities for fast ingest and change data capture (CDC). We have the ability to stream data into SingleStore from multiple sources like file systems and Kafka. We have a feature called Pipelines, which is very popular, to automatically load data from file folders, AWS S3, Kafka. You can transform data as it’s flowing into SingleStore, with very little coding. We support a very high performance and scalable bulk load system. We have support for a large variety of data types including relational data, standard structured data types, key-value, JSON, geospatial, time-oriented data, and more. We run everywhere. You can run SingleStore on-premises, you can run it in the cloud as a managed database platform, or as a service in our new Managed Service system , which just was delivered in September. We also allow people to self-host in the cloud. If they want full control over how their system is managed, they can self-host on all the major cloud providers and also run in containers; so, wherever you need to run, we are available. I mentioned scalability earlier and I wanted to drill into that a little bit to illustrate the, how our platform is organized. SingleStore provides an image to the database client application as just, it’s just a database. You have a connection string, you connect, you set your connection to use us as a database, and you can start submitting SQL statements. It’s a single system image. The application doesn’t really know that SingleStore is distributed – but, underneath the sheets, it’s organized as you see in this diagram. There are one or more aggregator nodes, which are front-end nodes that the client application connects to. Then, there can be multiple back-end nodes. We call them leaf nodes. The data is horizontally partitioned across the leaf nodes – some people call this sharding. Each leaf node has one or more partitions of data. Those partitions are defined based on some data definition language (DDL); when you create your table, you define how to shard the data across nodes. SingleStore’s query processor knows how to take a SQL statement and divide it up into smaller units of work across the leaf nodes, and final assembly results is done by the aggregator node. Then, the results are sent back for the client. As you need to scale, you can add additional leaf nodes and rebalance your data, so that it’s easy to scale the system up and down as needed. How Customers Have Used SingleStore for Time Series Data So with that background on SingleStore, let’s talk about using SingleStore for time series data. First of all, for those of you who are not really familiar with time series, a time series is simply a time-ordered sequence of events of some kind. Typically, each time series entry has, at least, a time value and some sort of data value that’s taken at that time. Here’s an example time series of pricing of a stock over time, over like an hour and a half or so period. You can see that the data moves up and down as you advance in time. Typically, data at any point in time is closely correlated to the immediately previous point in time. Here’s another example, of flow rate. People are using SingleStore for energy production, for example, in utilities. They may be storing and managing data representing flow rates. Here’s another example, a long-term time series of some health-oriented data from the US government, from the Centers for Disease Control, about chronic kidney disease over time. These are just three examples of time series data. Virtually every application that’s collecting business events of any kind has a time element to it. In some sense, almost all applications have a time series aspect to them. Let’s talk about time series database use cases. It’s necessary, when you’re managing time-oriented data, to store new time series events or entries, to retrieve the data, to modify time series data – to delete or append or truncate the data, or in some cases, you may even update the data to correct an error. Or you may be doing some sort of updating operation where you are, say, accumulating data for a minute or so. Then, once the data has sort of solidified or been finalized, you will no longer update it. There are many different modification scenarios for time series data. Another common operation on time series data is to do things like convert an irregular time series to a regular time series. For example, data may arrive with a random sort of arrival process, and the spacing between events may not be equal, but you may want to convert that to a regular time series. Like maybe data arrives every 1 to 10 seconds, kind of at random. You may want to create a time series which has exactly 1 data point every 15 seconds. That’s an example of converting from an irregular to a regular time series. Another kind of operation on time series is to downsample. That means you may have a time series with one tick every second, maybe you want to have one tick every one minute. That’s downsampling. Another common operation is smoothing. So you may have some simple smoothing capability, like a five-second moving average of a time series, where you average together like the previous five seconds worth of data from the series, or a more complex kind of smoothing – say, where you fit a curve through the data to smooth it , such as a spline curve. There are many, many more kind of time series use cases. A little history about how SingleStore has been used for time series is important to give, for context. Customers already use SingleStore for time series event data extensively, using our previously shipped releases, before the recent shipment of SingleStore DB 7.0 and its time series-specific features. Lots of our customers store business events with some sort of time element. We have quite a few customers in the financial sector that are storing financial transactions in SingleStore. Of course, each of these has a time element to it, recording when the transaction occurred. Also, lots of our customers have been using us for Internet of Things (IoT) events. For example, in utilities, in energy production, media and communications, and web and application development. For example, advertising applications. As I mentioned before, SingleStore is really tremendous for fast and easy streaming. With our pipelines capability, it’s fast and easy to use load data, and just very high-performance insert data manipulation language (DML). You can do millions of inserts per second on a SingleStore cluster. We have a columnstore storage mechanism which has tremendous compression – typically, in the range of 5x to 10x, compared to raw data. It’s easy to store a very large volume of historical data in a columnstore table in SingleStore. Because of the capabilities that SingleStore provides for high scalability, high-performance SQL, fast, and easy ingest, and high compression with columnar data storage. All those things have made SingleStore really attractive destination for people that are managing time series data. New Time Series Features in SingleStore DB 7.0 (For more on what’s in SingleStore DB 7.0, see our release blog post , our deep dive into resiliency features , and our deep dive into SingleStore Universal Storage . We also have a blog post on our time series features . – Ed.) Close to half of our customers are using time series in some form, or they look at the data they have as time series. What we wanted to do for the 7.0 release was to make time series querying easier. We looked at some of our customers’ applications, and some internal applications we had built on SingleStore for historical monitoring. We saw that, while the query language is very powerful and capable, it looked like some of the queries could be made much easier. We wanted to provide a very brief syntax to let people write common types of queries – to do things like downsampling, or converting irregular time series to regular time series. You want to make that really easy. We wanted to let the more typical developers do things they couldn’t do before with SQL because it was just too hard. Let experts do more, and do it faster ,so they could spend more time on other parts of their application rather than writing tricky queries to extract information from time series. So that said, we were not trying to be the ultimate time series specialty package. For example, if you need curve fitting, or very complex kinds of smoothing ,or you need to add together two different time series, for example. We’re not really trying to enable those use cases to be as easy and fast as they can be. We’re looking at sort of a conventional ability to manage large volumes of time series data, ingest the time series fast, and be able to do typical and common query use cases through SQL easily. That’s what we want to provide. If you need some of these specialty capabilities, you probably want to consider a more specialized time series product like KBB+ or something similar to that. Throughout the rest of the talk, I’m going to be referring a few times to an example based on candlestick charts. A candlestick chart is a typical kind of chart used in the financial sector to show high, low, open, and close data for a security, during some period of time – like an entire trading day, or by minute, or by hour, et cetera. This graphic shows a candlestick chart with high, low, open, close graphic so that the little lines at the top and bottom show the high and low respectively. Then, the box shows the open and close. Just to start off with, I wanted to show a query using SingleStore DB 6.8 to calculate information that is required to render a candlestick chart like you see here. On the left side, this is a query that works in SingleStore DB 6.8 and earlier to produce a candlestick chart from a simple series of financial trade or transaction events. On the right-hand side, that’s how you write the exact same query in SingleStore DB 7.0. Wow. Look at that. It’s about one third as many characters as you see on the left, and also it’s much less complex. On the left, you see you’ve got a common table expression with a nested select statement that’s using window functions, sort of a relatively complex window function, and several aggregate functions. It’s using rank, and then using a trick to pick out the top-ranked value at the bottom. Anyway, that’s a challenging query to write. That’s an expert-level query, and even experts struggle a little bit with that. You might have to refer back to the documentation. I’ll go over this again in a little more detail, but just please remember this picture. Look how easy it is to manage time series data to produce a simple candlestick chart on the right compared to what was required previously. How did we enable this? We provide some new time series functions and capabilities in SingleStore DB 7.0 that allowed us to write that query more easily. We provide three new built-in functions: FIRST(), LAST(), and TIME _ BUCKET(). FIRST() and LAST() are aggregate functions that provide the first or last value in a time window or group, based on some time period that defines an ordering. I’ll say more about those in a few minutes. TIME _ BUCKET() is a function that maps a timestamp to a one-minute or five-minute or one-hour window, or one-day window, et cetera. It allows you to do it in a very easy way with a very brief syntax, that’s fairly easy to learn and remember. Finally, we’ve added a new designation called the SERIES TIMESTAMP column designation, which allows you to mark one of your columns as the time column for your time series. That allows some shorthand notations that I’ll talk about more. Here’s a very simple example table that holds time series data for financial transactions. We’ve got a ts column, that’s a datetime 6 marked as the series timestamp. The data type is datetime 6, which is, it’s standard datetime with six places to the right of the decimal point. It’s accurate down to the microsecond. Symbol is like a stock symbol, a character string up to five characters. Price is a decimal, with up to 18 digits in 4 places to the right of the decimal point. So very simple time series table for financial information. Some examples that are going to follow, I’m going to use this simple data set. Now, we’ve got two stocks, made-up stocks, ABC and XYZ that have some data that’s arrived in a single day, February 18th of next year, in a period of a few minutes. We’ll use that data and some examples set in the future. Let’s look in more detail at the old way of querying time series data with SingleStore using window functions. I want to, for each symbol, for each hour, produce high, low, open, and close. This uses a window function that partitions by a time bucket. The symbol and time bucket ordered by timestamp, and the rows are between unbounded preceding and unbounded following. “Unbounded” means that any aggregates we calculate over this window will be over the entire window. Then, we compute the rank, which is the serial number based on the sort order like 1, 2, 3, 4, 5. One is first, two is second, so forth. Then, the minimum and maximum over the window, and first value and last value over the window. First value and last value are the very original value and the very final value in the window, based on the sort order of the window. Then, you see that from Unix time, Unix timestamp, ts divided by 60 times 60, times 60 times 60. This is a trick that people who manage time series data with SQL have learned. Basically, you can multiply, you can divide a timestamp by a window, and then multiply by the window again, and that will chunk up a fine-grain timestamp into a coarser grain that is bound at a window boundary. In this case, it’s 60 times 60. Then, finally, the select block at the end, you’ve got, you’re selecting the time series, the timestamp from above the symbol, min price, max price, first, last, but above that produced an entry for every single point in the series, so we really only want one. We pick out the top-ranked one. Anyway, this is tricky. I mean, this is the kind of thing that will take an expert user from several minutes, to many minutes, to write, and with references back to the documentation. Can we do better than this? How can we do better? We introduced first and last as regular aggregate functions, in order to enable this kind of use case, with less code. We’ve got a very basic example. Now, select first, price, ts from tick, but the second argument to the first aggregate is a timestamp, but it’s optional. If it’s not present, then we infer that you meant to use the series timestamp column of the table that you’re querying. The top one is the full notation, but in the bottom query, you say select first price, last price from tick. That first price and last price from tick implicitly use the series timestamp column ts as the time argument, the second argument to those aggregate functions. It just makes the query easier to write. You don’t have to remember to explicitly put in the series time value in the right place when you use those functions. Next, we have a new function for time bucketing. You don’t have to write that tricky divide, and then that multiply kind of expression that I showed you before. Much, much easier to use, more intuitive. Time bucket takes a bucket width, and that’s a character string like 5m, for five minutes, 1h for one hour, and so forth. Then, two optional arguments – the time and the origin. The time is optional just like before. If you don’t use it, if you don’t specify it, then we implicitly add the series timestamp column from the table or table, from the table that you’re querying. Then, origin allows you to provide an offset. For example, if you want to do time bucketing but start at 8:000 AM every day, you want a bucket by day but start your day at 8AM instead of midnight, then you can put in an origin argument. Again, this is far easier than the tricky math expression that we used for that candlestick query before. Here’s some example of uses of origin with an 8AM origin. For example, we’ve got this table T with that, and ts is the series timestamp ,and v is a value that’s a double-precision float. You see the query there in the middle: select time bucket 1d ts, and then you pick a date near the timestamps that you’re working with, and provide… That’s your origin. It’s an 8AM origin. Then, some of these. You can see down below that the days, the day bucket boundaries are starting at 8AM. Normally, you’re not going to need to use an origin, but if you do have that need to have an offset you can do that. Again, let’s look at the new way of answering, providing the candlestick chart query. This uses, we say select time bucket 1h, which is a one hour bucket. Then, the symbol, the minimum price, the maximum price, the first price, and the last price. Notice that in first and last and time bucket, we don’t even have to refer to the timestamp column in the original data set, because it’s implicit. Some of you may have worked with specialty products for managing web events like Splunk or Azure Kusto, and so this concept of using a time bucket function or a bucket function with an easy notation like this, you may be familiar with that from those kind of systems. One of the reason people like those products so much for the use cases that they’re designed for is that it’s really easy to query the data. The queries are very brief. We try to bring that brevity for time series data to SQL with this new capability, with the series timestamp that’s an implicit argument to these functions. Then, just group by 2, 1, which is the time bucket and the symbol and order by 2, 1. So, very simple query expression. Just to recap, SingleStore for several years has been great for time series ingest and storage. People loved it for that. We have very fast ingest, powerful SQL capability, with time-oriented functions as part of our window function capability. High-performance query processing based on compilation to machine code and vectorization, as well as scalability through scale-out and also the ability to support high concurrency, where you’ve got lots of writers and readers concurrently working on the same data set. And not to mention, we provide transactions, support, easy manageability, and we’re built for the cloud. Now, given all the capabilities we already had, we’re making it even easier to query time series data with this new brief syntax, these new functions, first, last, and time bucket in the series timestamp concept, that allows you to write queries very briefly, without having to reference, repeatedly and redundantly, to the time column in your table. This lets non-expert users do more than they could before, things they just weren’t capable of before with time series data, and it makes experts users more productive. I’d like to invite you to try SingleStore for free today, or contact Sales . Try it for free by using our free version, or go on Managed Service and do an eight-hour free trial. Either way, you can try SingleStore for no charge. Thank you. Q&A: SingleStore and Time Series Q. What’s the best way to age out old data from a table storing time series data? A. The life cycle management of time series data is really important in any kind of time series application. One of the things you need to do is eliminate or purge old data. It’s really pretty easy to do that in SingleStore. All you have to do is run a delete statement periodically to delete the old data. Some other database products have time-oriented partitioning capabilities, and their delete is really slow, so they require you to, for instance, swap out an old partition once a month or so to purge old data from a large table. In SingleStore, you don’t really need to do that, because our delete is really, really fast. We can just run a delete statement to delete data prior to a certain time, whenever you need to remove old data. Q. Can you have more than one time series column in a table? A. You can only designate one column in a table as the series timestamp. However, you can have multiple time columns in a table and if you want to use different columns, you can use those columns explicitly with our new built-in time functions – FIRST(), LAST(), and TIME _ BUCKET().There’s an optional time argument, so if you want to have like a secondary time on a table that’s not your primary series time stamp, but you want to use it for some of those functions, you can do it. You just have to name the time column explicitly in the FIRST(), LAST(), and TIME _ BUCKET() functions. Q. Does it support multi-tenancy? A. Does it support multi-tenancy? Sure. SingleStore supports any number of concurrent users, up a very high number of concurrent queries. You can have multiple databases on a single cluster, and each application can have its own database if you want to, to have multi-tenant applications running on the same cluster. Q. Does SingleStore keep a local copy of the data ingested or does it only keep references? If SingleStore keeps a local copy, how is it kept in sync with external sources? A. SingleStore is a database system. You create tables, you insert data in the tables, you query data in the tables, you can update the data, delete it. So when you add a record to SingleStore it, that record, a copy of the information and that record, the record itself is kept in SingleStore. It doesn’t store data by reference, it stores copies of the data. If you want to keep it in sync with external sources, you need to, as the external values change, you’ll need to update the record that represents that information in SingleStore. Q. How can you compute a moving average on a time series in SingleStore? A. Sure. You can compute a moving average; it depends on how you want to do it. If you just want to average the data in each time bucket, you can just use average to do that. If you want to do a moving average, you can use window functions for that, and you can do an average over a window as it moves. You can average over a window from three preceding rows, to the current row, to average the last four values. Q. Did you mention anything about Python interoperability? In any event, what Python interface capabilities do you offer? A. We do have Python interoperability – in that, you can let client applications that connect to SingleStore and insert data, query data, and so forth in just about any popular query language. We support connectivity to applications through drivers that are MySQL wire protocol-compatible. Essentially, any application software that can connect to the MySQL database and insert data, update data, and so forth, can also connect to SingleStore. We have drivers for Python that allow you to write a Python application and connect it to SingleStore. In addition, in our Pipeline capability, we support what are called transforms. Those are programs or scripts that can be applied to transform batches of information that are flowing into SingleStore through the Pipeline. You can write transforms in Python as well. Q. Do I need to add indexes to be able to run fast select queries on time series data, with aggregations? A. So, depending on the nature of the queries and how much data you have, how much hardware you have, you may or may not need to use indexes to make certain queries run fast. I mean, it really depends on your data and your queries. If you have very large data sets and high-selectivity queries and a lot of concurrency, you’re probably going to want to use indexes. We support indexes on our rowstore table type, both ordered indexes and hash indexes. Then, our columnstore table type, we have a sort key, a primary sort key, which is like an index in some ways, as well as support for secondary hash indexes. However, the ability to share your data across multiple nodes in a large cluster and use columnstore, data storage structures that with very fast vectorized query execution makes it possible to run queries with response times of a fraction of a second, on very large data sets, without an index. That can make it easier as an application developer, you can let the power of your computing cluster and database software just make it easier for you and not have to be so clever about defining your indexes. Again, it really depends on the application. Q. Can you please also talk about encryption and data access roles, management for SingleStore? A. With respect to encryption, for those customers that want to encrypt their data at rest, we recommend that they use Linux file system capabilities or cloud storage platform capabilities to do that, to encrypt the data through the storage layer underneath the database system. Then, with respect to access control, SingleStore has a comprehensive set of data access capabilities. You can grant permission to access tables and views to different users or groups. We support single sign-on through a number of different mechanisms. We have a pretty comprehensive set of access control policies. We also support row-level security. Q. What of row locking will I struggle kind with by using many transactions, selects, updates, deletes at once? SingleStore has multi-version concurrency control , so readers don’t block writers and vice versa. Write-Write conflicts usually happen at row-level lock granularity. Q. How expensive is it to reindex a table? A. CREATE INDEX is typically fast. I have not heard customers have problems with it. Q. Your reply on moving averages seem to pertain to simple moving averages, but how would you do exponential moving averages or weighted moving averages where a windows function may not be appropriate? A. For that you’d have to do it in the client application or in a stored procedure. Or consider using a different time series tool. Q. Are there any utilities available for time series data migration to / from an existing datastores like Informix, A. For straight relational table migration, yes. But you’d have to probably do some custom work to move data from a time series DataBlade in Informix to regular tables in SingleStore. Q. Does series timestamp accept integer data type or it has to be datetime data type? A. The data type must be time or datetime or timestamp. Timestamp is not recommended because it has implied update behavior. Q. Any plans to support additional aggregate functions with the time series functions? (e.g. we would have liked to get percentiles like first/last without the use of CTEs) A. Percentile _ cont and percentile _ disc work in SingleStore DB 7.0 as regular aggs. If you want other aggs, let us know. Q. Where can I find more info on AI (ML & DL) in SingleStore? A. See the documentation for dot _ product and euclidean _ distance functions. And see webinar recordings about this from the past. And see blog: https://www.singlestore.com/blog/memsql-data-backbone-machine-learning-and-ai/ Q. Can time series data be associated with asset context and queried in asset context. (Like a tank, with temperature, pressure, etc., within the asset context of the tank name.) A. A time series record can have one timestamp and multiple fields. So I think you could use regular string table fields for context and numeric fields for metrics to plot and aggregate. Q. Guessing the standard role based security model exists to restrict access to time series data. A. Yes. (End of Q&A) We invite you to learn more about SingleStore at https://www.singlestore.com , or give us a try for free at https://www.singlestore.com/free .", "date": "2019-12-18"},
{"website": "Single-Store", "title": "case-study-memsql-replaces-hadoop-for-2-million-meters", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/case-study-memsql-replaces-hadoop-for-2-million-meters/", "abstract": "SME Solutions Group is a SingleStore partner. An SME customer, a utility company, was installing a new meter network, comprising 2 million meters, generating far more data than the old meters. The volume of data coming in, and reporting needs, were set to overwhelm their existing, complex, Hadoop-based solution. The answer: replacing 10 different data processing components with a single SingleStore cluster. The result: outstanding performance, scalability for future requirements, the ability to use standard business intelligence tools via SQL, and low costs. SME Solutions Group ( LinkedIn page here ) helps institutions manage risks and improve operations, through services such as data analytics and business intelligence (BI) tools integration. SingleStore is an SME Solutions Group database partner . George Barrett, Solutions Engineer at SME, says: “SingleStore is like a Swiss Army knife – able to handle operational analytics, data warehouse, and data lake requirements in a single database.” You can learn more about how the two companies work together in this webinar and in our previous blog post . Introduction A utility company had installed a complex data infrastructure. Data came in from all the company’s systems of record: eCommerce, finance, customer relationship management, logistics, and more. The new meter network was going to blow up ingest requirements to 100,000 rows per second, with future expansion planned. The existing architecture was insufficient, and it lacked the ability to scale up quickly. It featured ten components: HDFS, Hive, and Druid . The core database was made up of the Hadoop Distributed File System (HDFS); Hive, for ETL and data warehousing; and Druid, an online analytics processing (OLAP) database, all frequently used together for big data / data lake implementations. ODS, EDW, and Spark . An operational data store (ODS) and an electronic data warehouse (EDW) fed Spark, which ran analytics and machine learning model. Key-value store, MongoDB (document-based), Cassandra (columnstore data), and ElasticSearch (semi-structured data) . Four different types of NoSQL databases stored data for different kinds of reporting and queries. The Old Solution Fails to Meet New Requirements This mix of different components was complex, hard to manage, and hard to scale. Worse, it was simply not up to the task of handling the anticipated ingest requirements, even if a lot of effort and investment were expended to try to make it work. Core requirements would have hit different parts of this complex system: Ingest data as fast as it was getting streamed . HDFS, for example, is best used for batch processing. Though it can handle micro-batches, it’s not really adapted for true streaming, as required for the new meter system. Aggregate data on three levels . The utility needed to aggregate data continuously, at five second intervals per meter, per day, and per month, with the ability to add aggregations going forward. The interaction between HDFS and Hive could not run fast enough to provide the needed aggregations without resorting to multi-second, or longer, response times. Bucket selected reads into high alarms and low alarms . Some reads needed to be marked off as alarms due to a value being too high or too low, with the alarm levels changed as needed by the utility. (SingleStore’s new time series features can help here.) The HDFS/Druid pairing could not handle this requirement flexibly and with the needed performance. Query data throughout process . The utility needed to allow operations personnel, analysts, and management to interactively query data throughout the process, and to be able to add applications and machine learning models in the future. The 10-component stack had a variety of interfaces, with different levels of responsiveness, altogether too limited and too complex to support query needs from the business. Maintain performance . Performance that could meet or exceed tight service level agreements (SLAs) was needed throughout the system. The variety of interfaces and the low performance levels of several components almost guaranteed that performance would be inadequate initially, and unable to scale up. SingleStore Meets and Exceeds Requirements The utility built and tested a new solution, using Kafka to stream data into SingleStore. The streaming solution, with SingleStore at its core, pulled all the functionality together into a single database, instead of 10 different components, as previously. And it more than met all the requirements. Ingest data as fast as it’s getting streamed . The combination of Kafka and SingleStore handles ingest smoothly, with Kafka briefly holding data when SingleStore is performing the most complex operations, such as monthly aggregations. Aggregate data on three levels . SingleStore handles the three aggregations needed – per meter, meter/day, and meter/month – with room for more. Bucket selected reads into high alarms and low alarms . SingleStore’s ability to run comparisons fast, on live data, makes it easy to bucket reads into alarm and non-alarm categories as needed. Query data throughout process . SingleStore supports SQL at every step, as well as the MySQL wire protocol, making it easy to interface SingleStore to any needed tool or application. Maintain performance . SingleStore is so fast that a modestly sized cluster handles the entire operation. If data volumes, query volumes, or other business needs require it, SingleStore scales linearly to handle the increased demands. There are also obvious operational advantages to using a single database, which supports the SQL standard, to ten disparate components which don’t. Machine learning and AI are now also much easier to implement. With a single data store for all kinds of data, live data and historical data can be kept in separate tables in the same overall database. Standard SQL operations such as JOINs can unify the data for comparison, queries, and more complex operations, with maximum efficiency. The Future with SingleStore With SingleStore at the core, SME’s customer is able to run analytics and reporting across their entire data-set using a wide variety of tools and ad-hoc processes. Although the original use case was 140 million rows of historical meter read data, they are easily able to scale their environment as their data grows to billions and even trillions of rows. George and others are also excited about the new Universal Storage capability, launched in SingleStore DB 7.0. In this initial implementation of Universal Storage, rowstore tables have compression, and columnstore tables have fast seeks. The tables are more alike, and the need to use multiple tables, of two different types, to solve problems is greatly reduced. Over time, more and more problems will be solved in one table type, further simplifying the already much-improved operations workload. You can learn more about how the two companies work together in this webinar and in our previous blog post . To get in touch with the SME Solutions Group, you can schedule a demo or visit their Linkedin page . To try SingleStore, you can run SingleStore for free ; or contact SingleStore .", "date": "2019-12-19"},
{"website": "Single-Store", "title": "why-nosql-databases-wrong-tool-for-modern-application", "author": ["Rick Negrin"], "link": "https://www.singlestore.com/blog/why-nosql-databases-wrong-tool-for-modern-application/", "abstract": "It’s time for us to admit what we have all known is true for a long time: NoSQL is the wrong tool for many of the modern application use cases, and it’s time that we move on. NoSQL came into existence because the databases at the time couldn’t handle the scale required. The rise of this new generation of data services solved many of the problems of web scale and rapidly growing data sets when it was created more than a decade ago. NoSQL also offered a new, cost-effective method for cold storage/occasional batch access for petabyte scale data. However, in the rush to solve for the challenges of big data and large numbers of concurrent users, NoSQL abandoned some of the core features of databases that make them highly performant and easy to use. Dealing with these trade-offs might be NoSQL’s greatest contribution to the database world. It forced an evolution, combining the best of the big data capabilities with the structure and flexibility of the proven relational model to produce a scalable relational database. Relational databases evolved to create an entirely new generation of systems that can handle nearly all of the workloads, with the scalability, reliability, and availability requirements that modern applications demand. From traditional workloads such as transactional applications and business analytics, to newer workloads such as multi-tenant services and operational analytics. The rise of new databases such as Google Spanner, Azure Data Warehouse, and our eponymous database, SingleStore, have proven that, for the majority of use cases, relational databases are easier to use and generally perform better than the NoSQL systems. I know this might be controversial. I also know that you might quickly dismiss my perspective as biased. But let me break down the history, architecture, and applications of these databases, then judge for yourself. This blog post, originally published in July 2018, has been updated with references to newer SingleStore releases. – Ed. How NoSQL Rose in Popularity NoSQL came full force onto the scene in the late 2000s, although it started much further back. It was developed largely to address the scale problems of existing database systems. It was clear that scale out was a more cost-effective model for building large systems. For the largest systems such as email and search built by Google, Facebook, Microsoft, and Yahoo, it was the only way to scale. The value of scale-out first clicked for me personally when I read James Hamilton’s paper on designing and deploying Internet Scale Services in 2007 . Scaling the application tier came first because it was easier to scale a stateless system. The storage layer was another story. By definition, databases are stateful, and maintaining the guarantees (i.e. ACID ) on that state across a distributed system is really hard. So layers were built on top of the existing database systems (MySQL, SQL Server, etc.) to create a distributed storage layer. I ran into a couple of examples of this while working as a Product Manager in the SQL Server team at Microsoft. The first example was internal at Microsoft where the company built Webstore, which was a sharding layer on top of SQL Server used by Hotmail and the associated services. In fact, Webstore was the motivation to build what eventually became today’s Azure SQL Database. Webstore was clunky and lacked a lot of core functionality but it worked and gave Microsoft an ability both to scale to the size of data it needed and achieve high availability. But Webstore required an entire team of engineers to build and maintain. In the mid-2000s, MySpace had a large number of SQL Servers to manage the rapidly growing site. The company was adding users so fast that new SQL Server boxes needed to be added every day. Running all those SQL Servers and querying across them was a massively complicated endeavor that took a large number of engineers to maintain. The same patterns were repeated at Facebook and others, because all of the burgeoning tech giants struggled with scale. It became clear that, with their massive usage and growth, these new digital services demanded a new solution for ingesting, managing, and surfacing data. Ideally, we needed something that could natively present a single interface but scale out over many machines with built-in high availability. Eventually, the large scale cloud services (Google, Facebook, Yahoo, Microsoft, and others) all built their own custom systems to handle scale demand. Those systems were all different but the basic ideas were shared, either directly or through academia. Eventually open source systems started popping up using those same ideas, and the NoSQL movement was born. To solve for web scale, NoSQL departed from the traditional databases in a few critical ways, So let’s look at why these choices were made. ACID and BASE: Pros and Cons of Eventual Consistency There are two models for storage systems, ACID and BASE . ACID stands for Atomic, Consistent, Isolation, and Durable. It covers the guarantees you get with most relational databases. ACID guarantees that writes have to wait for the data to hit disk before returning success to the client. Further, if you are really paranoid about durability (i.e. not losing data) you configured the database to wait until the write traveled over the network to some other machine and hit the disk on that side as well. So there is a guarantee that the data is always what you wrote but you give up some performance in write speed. BASE, which is typical for NoSQL systems, stands for Basically Available, Soft State, and Eventually Consistent. Eventual consistency is faster on writes because the application doesn’t have to wait to see if the write persisted. As soon as the data store captured the write, but before it is persisted to disk or to another machine, it could tell the application the write was successful and the application could move on to the next operation. So you gain a performance advantage but at the risk of not seeing the data you just wrote, or the data might be lost altogether in an error condition. Eventual consistency is a reasonable trade-off of durability risk versus availability. Developers continue to have to make these trade-offs as documented in this report . If your business is consumer engagement and latency has a direct impact on your income (which is true for all content, community, and commerce applications), you want the most responsive UI you can get. If you have to scale to millions of concurrent users you can’t tolerate any bottlenecks. What you trade-off by adopting eventual consistency in your database architecture is occasionally losing someone’s post or a comment, which is an acceptable risk for these types of applications. The other side of the spectrum of durability versus risk are things such as financial applications. You don’t want your bank using eventual consistency to store the result of your ATM transactions or your stock sales. In these cases, you still have users demanding little to no latency but are unwilling to accept a transaction not getting written to disk. There is a place for eventual consistency, but it is not always the only answer. Architects and developers of data systems should be able to choose what level of consistency they want. That choice should be at the use case level, not a platform level. Why Schema Is Important It’s not clear why schema was lost as part of the NoSQL movement. Yes, it was hard in the early days to build a distributed metadata manager to maintain schemas across a distributed system to support operations, such as adding a column. So it was unsurprising for schema to have been left out of the early designs. But instead of finding a way to add it in later, schema was simply eliminated altogether. It’s also understandable why folks make the argument that it makes you less agile. Good schema design is hard and requires careful upfront thinking. When things are changing rapidly (as they were then and continue to be now) you don’t want to be locked into a schema. But this is a fallacy. It is true that lack of schema increases agility for the engineer who owns putting data into the system. However, it kicks the problem down to the readers of the data, who are usually an order of magnitude greater in number and often don’t have the context about the state of the data when it was written. These users are usually the ones who are generating value from that data and should have as few roadblocks as possible. To give an analogy, imagine libraries saying they are doing away with the Dewey Decimal System and just throwing the books into a big hole in the ground and declaring it a better system because it is way less work for the librarians. There is a time and place for semi-structured data, because sometimes you don’t know the shape of some of the data ahead of time or it is sparsely populated. But if you truly don’t understand any of the data coming or what it will look like, then what good is it? The truth is that there is always schema. The data always makes sense to someone. That someone should take the time to encode that knowledge into the platform so it is usable by the next people. If it is a mix of data that is understood and some that is changing rapidly, put the latter into a semi-structured column in a database, then figure out what columns to project out of it later. SQL Server and Oracle could do this with XML 15 years ago. SingleStore, and a number of other modern databases, can do it now with JSON data. Document data storage (and key/value) should be a feature of a modern database, not the sole capability of a product. Non-SQL Syntax for Query This decision in the design of NoSQL databases follows going schema-less. If you don’t have schema, then ditching SQL syntax kind of makes sense. In addition, query processors are hard to build for a single box, and building a distributed one is much harder. Most notably, if you are a developer trying to get a new app up and running, this kind of system feels easier. MongoDB perfected the art of a simple installation and first-use experience. But it turns out the relational model is pretty powerful. Just having get and put functions is great if you never want to answer any question other than “fetch object with id 2”.  But the majority of the applications out there end up needing more than that. If you want to read a great article by someone (who does not work on a data store product) who came to this conclusion after working on two separate projects with MongoDB read this . It is an excellent example of where document databases fall short. In anything but the most trivial systems you always end up wanting to query the data a different way then you stored it. Ironically, the relational model was invented in the 1960s to solve this exact problem with the data stores of the time (IMS and Codasyl). A relational model with the ability to join is the only reasonable way to get data out. Yes, it is harder up front, but it is way easier than pulling all the data up into your app and building the join yourself. I have seen customers try to do this over and over again with NoSQL databases, and it always leads to madness. Many of these NoSQL systems achieved their primary goal. They provided a data store with a single interface that could scale out over many machines with built-in high availability. While there has certainly been some success, NoSQL adoption has run into blockers. There are couple of different reasons. Performance is a key one, in particular doing analytic queries with any sort of SLA. Manageability is another, because distributed systems are notoriously hard to manage. But the thing preventing traction with NoSQL more than anything, is the need to retrain people. There are a lot of people trained and educated in the relational world. NoSQL has been trying to convert the world for the last 10 years but it has only made a small dent. The NoSQL companies all together make up just a few percent of the $50 billion in the database market. While software engineers seemed to love NoSQL, data people (DBAs, data architects, analysts) went reluctantly into the NoSQL world as seemingly the only way to achieve the scale necessary. But it meant they had to relearn new APIs, tools, and an ecosystem, throwing out years of successful approaches, patterns, and assets. They wanted to do things using a proven model, but still get the scale without compromising the durability, availability, and reliability of the system. NewSQL – Best for Performance and Scale When we built SingleStore, we started with the premise that customers liked the capabilities of relational databases but wanted the availability and reliability of a scale out systems. Our goal was to allow a customer to have the best of both worlds. SingleStore is a distributed relational database that supports transactions and analytics and scales out on commodity hardware. You get the familiar relational model, SQL query syntax, and a giant ecosystem of tools coupled with the scalability and availability of modern, cloud-native systems. Let’s revisit this in the context of the core differences of a NoSQL system. SingleStore – Best for Consistency and Performance SingleStore has knobs that let you tune how much you want to trade-off between consistency and performance. The trade-off will always be there, but now you don’t have to choose between these things at the platform level. You can make the choice for each use case that makes sense. Consistency versus performance is not some hardcore philosophical choice, it is a choice of what is better for your application and requirements. SingleStore has two settings that let you tune for this. The first lets you decide whether to wait for disk persistence or not . There is an in-memory buffer that stores the transactions before they are persisted to disk. You can either have success return as soon as it hits the buffer or when it hits the disk. If you return when it hits the buffer there is a chance that a machine failure or reboot could happen before it is persisted and the data will be lost. On the other hand, waiting for it to persist to disk will take longer. In addition, with HA you have two modes of replication , sync and async, that ensures a second copy of the data on another machine. If you set the replication to synchronous mode you wait until the transaction is received on the secondary before returning success to the client. If async mode for replication is on, then the transaction returns success before the data is replicated to the secondary. This gives you the ability to tune the trade-off consistency and durability for performance for what fits your risk/performance profile. SingleStore DB 7.0 includes fast sync replication and sync durability. SingleStore – Schema (JSON, Spatial Types, Full-Text Indexes) SingleStore implements schema by storing the metadata in small internal database and synchronously replicating the metadata to all the nodes when it is changed. It uses a two-phase commit to ensure that DDL changes propagate properly through the cluster and are built in a way so that they do not block select queries. SingleStore supports more than just relational though. You can type a column as JSON and store a JSON document in it. If you decide there are some columns you want to query later, you can project the properties as columns and index them. SingleStore also supports Spatial types and Full-Text indexes as well. We understand that customers need a mix of data types in a system that is familiar and where all the types of data can co-exist naturally. SingleStore – Lingua Franca SingleStore has solved for using SQL syntax across distributed databases at scale. A distributed query processor allows you to express your query using standard SQL syntax, and the system takes care of distributing your query across the nodes in the cluster and aggregating the results back for you. SingleStore supports all of the common ANSI SQL operators and functions which gives you a powerful model for expressing just about any query. SingleStore does this by having two node types in the system, aggregators and leaves. Aggregators handle the metadata of the distributed system, route queries, and aggregate results. Leaves store the data and do the heavy lifting of executing the query on the partitions. Where it can, SingleStore will execute joins locally, which is why schema design is pretty important. If it can’t, SingleStore will shuffle the data as needed. So customers can use the SQL language without knowing how the data is partitioned underneath. If you would like to learn more, SingleStore docs have more detail on how this is built. SingleStore distributes data across aggregator and leaf nodes . What this means is that you are able to use the skills, investments, and tools you already have in your company with SingleStore, or people can use SingleStore the way they use any other relational database and don’t have to be retrained. In addition, because SingleStore supports the MySQL wire protocol, the existing massive ecosystem of BI, ETL, and other middleware tools just work with SingleStore. You don’t have to hire new staff, learn a bunch of new tools, or bring in new software. It just works. Why You Shouldn’t Use NoSQL NoSQL came along to handle the scale requirements as web apps and multi-tenant services were taking off. Given how hard the problems were to solve, it is understandable that these early attempts at dealing with scaling at the storage layer forced customers into a difficult set of trade-offs. But relational databases have evolved. They can can handle nearly all of the workloads, with the scalability, reliability, and availability requirements that modern applications demand. Workloads such as operational analytics . As all companies realize the value of being data driven they want to enable all their employees with up-to-date data. To do this requires a new breed of analytics systems that can scale to hundreds of concurrent queries, deliver fast queries without pre-aggregation, and ingest data as it is created. On top of that, they want to expose data to customers and partners, requiring an operational SLA, security capabilities, performance, and scale not possible with current data stores. This is just one of several new workloads that are driving demand for new capabilities beyond what the legacy databases and NoSQL systems can offer. The relational model has stood the test of time. It continues to add new innovations, such as SingleStore Universal Storage . In addition, it has absorbed the new data types (search, spatial, semi-structured, etc.) and consistency models so they can coexist in one system. There is no inherent scalability challenges with the relational model or the SQL query syntax. It just needed a different storage implementation to take advantage of a scale-out architecture. The new databases such as SingleStore have proven that, for the majority of use cases, relational databases are easier to use and generally perform better than the NoSQL systems. Thank you NoSQL. You put pressure on the database community to force it to solve the challenges of the cloud-scale world. It worked. Relational databases have evolved to meet those requirements. We got it from here. SingleStore Managed Service and SingleStore DB 7.0 are available now. You can get started instantly with SingleStore today for free or contact Sales .", "date": "2020-01-07"},
{"website": "Single-Store", "title": "what-is-skiplist-why-skiplist-index-for-memsql", "author": ["Adam Prout"], "link": "https://www.singlestore.com/blog/what-is-skiplist-why-skiplist-index-for-memsql/", "abstract": "This blog post was originally published in January 2014, and it has long been the first blog post on the SingleStore blog – and one of the best. In this blog post, SingleStore co-founding engineer Adam Prout explains one of the key technical features that distinguishes SingleStore: its use of skiplist indexes over Btrees and similar structures. Adam has now revised and updated this blog post to include the recently released SingleStore DB 7.0 and SingleStore Universal Storage™. The most popular data structure used for indexing in relational databases is the Btree (or its variant, the B+tree). Btrees rose to popularity because they do fewer disk I/O operations to run a lookup compared to other balanced trees. To the best of my knowledge, SingleStore is the first commercial relational database in production today to use a skiplist, not a Btree, as its primary index backing data structure for in-memory data. SingleStore, founded in 2011, began as an in-memory, rowstore database. SingleStore’s storage design evolved, in the years following, to support on-disk data in columnstore format. We then added more intelligence around when rows are stored in memory vs. on disk, and in rowstore or columnstore format, with the Universal Storage project . Through all this, the skiplist has remained the index of choice for in-memory rowstore data. A lot of research and prototyping went into the decision to use a skiplist. I hope to provide some of the rationale for this choice, and to demonstrate the power and relative simplicity of SingleStore’s skiplist implementation. I’ll show some very simple single-threaded table scans that run more than eight times faster on SingleStore, compared to MySQL, as a very basic demonstration. (SingleStore performs even better than this on more aggressive and complex workloads). This article will stick to high-level design choices and leaves most of the nitty-gritty implementation details for other posts. What is a Skiplist? Btrees made a lot of sense for databases when the data lived most of its life on disk and was only pulled into memory and cached as needed to run queries. On the other hand, Btrees do extra work to reduce disk I/O that is needless overhead if your data fits into memory. As memory sizes have increased, it is feasible today to support indexes that only function well for in-memory data, and are freed from the constraints of having to index data on disk. A skiplist is well suited for this type of indexing. Skiplists are a relatively recent invention. The seminal skiplist paper was published in 1990 by William Pugh: Skiplists: a probabilistic alternative to balanced trees . This makes the skiplist about 20 years younger than the Btree, which was first proposed in the 1970s. A skiplist is an ordered data structure providing expected O(Log(n)) lookup, insertion, and deletion complexity. It provides this level of efficiency without the need for complex tree balancing or page splitting like that required by Btrees, redblack trees, or AVL trees. As a result, it’s a much simpler and more concise data structure to implement. Lock-free skiplist implementations have recently been developed; see this paper, Lock-Free Linked Lists and Skiplists , published in 2004 by Mikhail Fomitchev and Eric Ruppert. These implementations provide thread safety with better parallelism under a concurrent read/write workload than thread-safe balanced trees that require locking. I won’t dig into the details of how to implement a lock-free skiplist here, but to get an idea of how it might be done, see this blog post about common pitfalls in writing lock-free algorithms . A skiplist is made up of elements attached to towers. Each tower in a skiplist is linked at each level of the tower to the next tower at the same height, forming a group of linked lists, one for each level of the skiplist. When an element is inserted into the skiplist, its tower height is determined randomly via successive coin flips (a tower with height n occurs once in 2^n times). The element is linked into the linked lists at each level of the skiplist, once its height has been determined. The towers support binary searching by starting at the highest tower and working towards the bottom, using the tower links to check when one should move forward in the list or down the tower to a lower level. Why a Skiplist Index for SingleStore There are many reasons why skiplists are best for SingleStore, primarily: they’re memory-optimized, simple (including the need for many fewer lines of code to implement them), much easier to implement in a lock-free fashion, fast, and flexible. 1) Memory-Optimized SingleStore supports both in-memory rowstore and on-disk columnstore table storage. The rowstore is designed for fast, high-throughput access to data stored in memory. The columnstore is designed for scanning and aggregating large amounts of data quickly. The columnstore is disk-backed but keeps recently written data in-memory in rowstore layout before flushing it to disk in columnstore layout. The columnstore also stores all metadata about files in memory, in internal rowstore metadata tables – i.e, metadata such as maximum and minimum values of each column, a bitmap of deleted rows, etc. This data is needed in memory for fast and easy access by query execution for eliminating entire files from a filter. See our documentation for more information about our columnstore . These two storage types are being converged by the SingleStore Universal Storage project to create a storage design with most of the benefits of both table types, while eliminating the need to think about the details of storage layouts when designing and managing an application. Thus, both of SingleStore’s table types have a need for a memory-optimized rowstore index, as does our future ideal design of a Universal Storage table type. (You can see a deep dive on Universal Storage from Eric Hanson, and some useful information about our current and future implementation of Universal Storage in Rick Negrin’s webinar.) Being memory-optimized means indexes are free to use pointers to rows directly, without the need for indirection. In a traditional database, rows need to be addressable by some other means than a pointer to memory, as their primary storage location is on disk. This indirection usually takes the form of a cache of memory-resident pages (often called a buffer pool) that is consulted in order to find a particular row’s in-memory address, or to read it into memory from disk if needed. This indirection is expensive and usually done at the page level (e.g., 8K at a time in SQL Server). SingleStore doesn’t have to worry about this overhead. This makes data structures that refer to rows arbitrarily by pointer, like a skiplist does, feasible. Dereferencing a pointer is much less expensive than looking up a page in the buffer pool. 2) Simple SingleStore’s skiplist implementation is about 1500 lines of code, including comments. Having recently spent some time in both SQL Server’s and Innodb’s Btree implementations, I can tell you they are both close to 50 times larger in terms of lines of code, and both have many more moving parts. For example, a Btree has to deal with page splitting and page compaction, while a skiplist has no equivalent operations. The first generally available build of SingleStore took a little over a year to build and stabilize. This feat wouldn’t have been possible with a more complex indexing data structure. 3) Lock-Free A lock-free or non-blocking algorithm is one in which some thread is always able to make progress, no matter how all the threads’ executions are interleaved by the OS. SingleStore is designed to support highly concurrent workloads running on hardware with many cores. These goals makes lock-free algorithms desirable for SingleStore. (See our original blog post on lock-free algorithms and our description of sync replication , by Nate Horan.) The algorithms for writing a thread-safe, lock-free skiplist are now a solved problem in academia. A number of papers have been published on the subject in the past decade. It’s much harder to make a lock-free skiplist perform well when there is low contention (ie., a single thread iterating over the entire skiplist, with no other concurrent operations executing). Optimizing this case is a more active area of research. Our approach to solving this particular problem is a topic for another time. Btrees, on the other hand, have historically needed to use a complex locking scheme to achieve thread safety. Some newer lock-free, Btree-like data structures such as the BWtree have recently been proposed that avoid this problem. Again, the complexity of the BWTree data structure far outpaces that of a skiplist or even a traditional Btree. (The BWTree requires more complex compaction algorithms then a Btree, and depends on a log-structured storage system to persist its pages). The simplicity of the skiplist is what makes it well suited for a lock-free implementation. 4) Fast The speed of a skiplist comes mostly from its simplicity. SingleStore is executing fewer instructions to insert, delete, search, or iterate compared to other databases. 5) Flexible Skiplists also support some extra operations that are useful for query processing and that aren’t readily implementable in a balanced tree. For example, a skiplist is able to estimate the number of elements between two elements in the list in logarithmic time. The general idea is to use the towers to estimate how many rows are between two elements linked together at the same height in the tree. If we know the tower height at which the nodes are linked, we can estimate how many elements are expected to be between these elements, because we know the expected distribution of towers at that height. Knowing how many elements are expected to be in an arbitrary range of the list is very useful for query optimization, when calculating how selective different filters in a select statement are. Traditional databases need to build separate histograms to support this type of estimation in the query optimizer. Addressing Some Common Concerns About Skiplists SingleStore has addressed several common concerns about skiplists: memory overhead, CPU cache efficiency, and reverse iteration. Memory Overhead The best-known disadvantage of a skiplist is its memory overhead. The skiplist towers require storing a pointer at each level of each tower. On average, each element will have a tower height of 2 (we flip a coin in succession to determine the tower height, such that a tower of height n occurs 1 in 2^n times). This means, on average, each element will have 16 bytes of overhead for the skiplist towers. The significance of this overhead depends on the size of the elements being stored in the list. In SingleStore, the elements stored are rows of some user’s table. The average row size in a relational database tends to be hundreds of bytes in size, dwarfing the skiplist’s memory overhead. BTrees have their own memory overhead issues that make them hard to compare directly to skiplists. After a BTree does a page split, both split pages are usually only 50% full. (Some databases have other heuristics, but the result of a split is pages with empty space on them). Depending on a workload’s write patterns, BTrees can end up with fragmented pages all over the BTree, due to this splitting. Compaction algorithms to reclaim this wasted space are required, but they often need to be triggered manually by the user. A fully compacted BTree, however, will be more memory-efficient than a skiplist. Another way SingleStore is able to improve memory use compared to a traditional database is in how it implements secondary indexes. Secondary indexes need only contain pointers to the primary key row. There is no need to duplicate the data in key columns like secondary Btree indexes do. CPU Cache efficiency Skiplists do not provide very good memory locality because traversing pointers during a search results in execution jumping somewhat randomly around memory. The impact of this effect is very workload-specific and hard to accurately measure. For most queries, the cost of executing the rest of a query (sorting, executing expressions, protocol overhead to return the queries result) tends to dominate the cost of traversing the tower pointers during a search. The memory locality problem can also be mostly overcome by using prefetch instructions ( mm_prefetch on Intel processors). The skiplist towers can be used to read ahead of a table scan operation and load rows into CPU caches, so they can be quickly accessed by the scan when it arrives. Reverse Iteration Most skiplist implementations use backwards pointers (double linking of list elements) to support iterating backwards in the list. The backwards pointers add extra memory overhead and extra implementation complexity; lock-free, doubly-linked lists are difficult to implement. SingleStore’s skiplist employs a novel reverse iterator that uses the towers to iterate backwards without the need for reverse links. The idea is to track the last tower link that is visited at each level of the skiplist while seeking to the end, or to a particular node, of the skiplist. These links can be used to find the element behind each successive element, because each time the reverse iterator moves backwards, it updates the links at each level that are used. This iterator saves the memory required for backwards pointers, but does result in higher reverse-iteration execution cost. Reverse iteration is important for a SQL database because it allows ORDER BY queries to run without sorting, even if the ORDER BY wants the opposite sort order (ie, ascending instead of descending) of the order the index provides. Quick Performance Comparison Performance benchmarking of databases and data structures is very difficult. I’m not going to provide a comprehensive benchmark here. Instead, I’ll show a very simple demonstration of our skiplist’s single-threaded scan performance compared to innodb’s Btree. I’m going to run SELECT SUM(score) FROM users over a 50 million-row users table. The test is set up to, if anything, favor MySQL. There is no concurrent write workload in this demonstration (which is where SingleStore really shines), and SingleStore is running with query parallelism disabled; both MySQL and SingleStore are scanning using only a single thread. Innodb is running with a big enough buffer pool to fit the entire table in memory, so there is no disk I/O going on. CREATE TABLE `users` (\n`user_id` bigint(20) NOT NULL AUTO_INCREMENT,\n`first_name` varchar(100) CHARACTER SET utf8,\n`last_name` varchar(100) CHARACTER SET utf8,\n`install_date` datetime,\n`comment` varchar(500),\n`score` bigint(20),\nPRIMARY KEY (`user_id`)\n) SingleStore’s single-threaded scan performance is 5 times faster in the first case and 8 times faster in the second case. There is no black magic involved here. SingleStore needs to run far fewer CPU instructions to read a row, for the reasons discussed above. SingleStore’s advantages really come to the fore when there are concurrent writes and many concurrent users. Conclusion SingleStore takes advantage of the simplicity and performance of lock-free skiplists for in-memory rowstore indexing. These indexes are used to back rowstore tables, to buffer rows in memory for columnstore tables, and to store metadata about columnstore blob files on disk. The result is a more modern indexing design, based on recent developments in data structure and lock-free/non-blocking algorithms research. The simplicity of the skiplist is the source of a lot of SingleStore’s speed and scalability .", "date": "2019-12-18"},
{"website": "Single-Store", "title": "the-g2-crowd-has-much-more-to-say-about-memsql", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/the-g2-crowd-has-much-more-to-say-about-memsql/", "abstract": "SingleStore is garnering a lot of positive attention on business solutions review site G2.com (formerly known as G2 Crowd). As Walt P, a data engineer at an enterprise company, puts it: “We have loaded data in volumes that have historically stopped other database technologies in their tracks.” Since our look at the site last July , dozens of new reviews have been posted, citing SingleStore’s speed, high capacity, and SQL compatibility, among other features. However, the most recent review was in December; now, some lucky SingleStore user has the chance to register the first comment of the new year . (Also, as most would say, of the new decade, though – as xkcd points out – this is a matter of some controversy. ) G2.com features information and frank user reviews of business solutions – software and services – across a wide range of categories. Software categories include CRM software, demand generation software, BI tools, AI development and delivery tools, marketplace platforms, CAD/CAM tools, and many more. Services categories include lawyers, tax people, sales training, graphic design, website design, staffing services, channel partners, and much more. It’s easy to link from a product to its overarching category (such as Relational Databases Software , for SingleStore and related software, or to interact with a G2 Advisor , to help you narrow your search with help from a live person. (Not a bot – at least, not last time we checked.) You can quickly share your comments on social media, ask a vendor question , or request a demo from a vendor. Using G2.com will help you see whether a product is a strong player in the market; assess its strengths and weaknesses; look at the competition; and get ready to answer intelligent questions in a vendor call. Your review comments encourage the vendors of products you use to keep up the good work and to fix any problems. Note : Be sure to hit the Show More link on G2.com reviews. Otherwise, you might miss comments about recommendations to others, the problems being solved, benefits being realized, and other valuable information. What SingleStore Users on G2.com Say About SingleStore The recent comments on G2.com have yielded a star rating of 4 out of a possible 5 points. The major positive areas cited in include speed, capacity, and ease of implementation. (Some of the comments have been lightly edited for spelling, syntax, spelling out acronyms, and so on – the original comments are on the G2 site .) Speed The major focus of many comments, and a common thread through nearly all of them, is SingleStore’s speed. SingleStore achieves its high speed through a distributed, lock-free architecture . New features in SingleStore Universal Storage improve speed further, with smaller data size (equals faster loading times) for rowstore tables, and faster seeks and faster updating, using secondary hash indexes, for columnstore tables. Comments include: “We saw a sub-second response on a test case with 1000 concurrent heavy API calls (scanning billions of rows across many tables + window functions etc) along with applying role-based RBAC functionalities on the fly.” (SingleStore has processed over a trillion rows per second.) “Incredibly fast performance for dashboards!” This was the heading of a five-star review from a VP of IT Operations. They went on, “We have had excellent luck with ingest rates and the ability to do lightning fast counts and other general math on time-based data sets.” “We’ve been able to pump through many millions of records in seconds, where in other database systems we were having our queries time out.” “I… recommend it for businesses who have issues with data retrieval slowness and are looking to improve data retrieval speeds. We solved slowness in our grid reports by using SingleStore. This reduced the time clients waited for reports to be generated and made their experience much more pleasant.” “The best thing about SingleStore is the speed at which queries are run. Add Kafka pipelines for your ETL process and everything data-related becomes much easier.” “We handle extremely large data set analysis. SingleStore provides us with the capability to do so at incredible speed!” “We, a Big 4 accounting firm, use SingleStore for rapid creation and destruction of clusters. SingleStore’s speed and reliability are key for our use case, since speed is our priority. Not having to keep clusters active saves time and money. SingleStore’s speed enables that competitive advantage.” Problem being solved: “ Slow queries in MySQL.” Benefits realized: “SingleStore allows us to run all queries, with no limitation, at every scale of data.” “Currently the teams are using SingleStore to parse extremely large datasets in times that used to take days.” Capacity – and “Solving too Many Problems” What’s truly remarkable about SingleStore is its linear scalability – the ability to maintain high performance, for both transactions and analytics, at high volumes. This includes high ingest volumes, processing large volumes of data, high volumes of queries, and high concurrency – many queries coming from many sources, including ad hoc queries, business intelligence tools, application demands, machine learning models, and more. When people refer to SingleStore’s speed, as in the previous section, they often really mean its ability to maintain very high speeds with very large volumes of data. A number of the more recent comments on G2.com address this core SingleStore capability: “Our primary BI tool is Tableau. Before we implemented SingleStore, we were connecting Tableau to Microsoft SQL Server and running reports off there. However, the performance was extremely slow, especially when querying large chunks of data. With SingleStore, we can query as much data as we need, with little to no impact on performance time.” “We replaced the databases of our core operational systems with SingleStore. We mostly go after greater performance and scalability.” “Very scalable, fabulous, and very easy to use. I recommend it for the management of the organization’s data.” “Great for extreme data processing. For extremely large data mining, SingleStore stands above the rest. The ability to process an extreme amount of data expediently is by far the greatest part of SingleStore.” “(We implemented a) data lake with SingleStore. Performance increased multi-fold and scaled for a large volume of data.” “We are solving too many problems with the help of SingleStore. Now we can analyze large amounts of data with efficiency.” Two Sides to Ease of Use, and More The only major aspect of SingleStore’s capabilities that receives mixed reviews on G2 Crowd is ease of use. Some commenters describe it as very easy to use, and very easy to implement; this is especially true for those who have MySQL experience. (Not only do both databases use SQL, but SingleStore directly supports the MySQL wire protocol for connectivity.) Other commenters describe SingleStore as complicated or taking time to master; “It has some learning curve,” says one. The commenters are, indeed, talking about the same database. The point here is that, when dealing with large volumes of data, SingleStore eliminates the need to shard your own database , or to buy in a complex and expensive solution such as Oracle Exadata , when capacity demands outstrip what can be handled on a single server. So SingleStore is easy to use, compared to the big-data alternatives. But it’s harder to understand how it works than with some single-server relational databases. SingleStore is also unusual in its ability to handle both transactions and analytics, while replacing slow and complex extract, transform, and load (ETL) processes with easy-to-use SingleStore Pipelines . Several comments recognize this; “it has potential to be used for both for transaction processing as well analytical queries.” But a couple of comments seem to reflect a view of SingleStore as an in-memory database (only); “Excellent in-memory option for high-powered data analysis,” says one. SingleStore rowstore tables, used for transactions, do run in memory; but columnstore tables, used for most analytics operations, are disk-based, compressed 5-10x, and very efficient. Most of the comments recognize both sets of capabilities. This reflects an unusual strength: the ability of SingleStore to perform both transactions and analytics in a single database system. “The same DB system can be used for OLTP and OLAP purpose as well,” says one commenter. And another: “You can seamlessly join across both row and columnar based tables.” A weakness mentioned for SingleStore in the comments is around centralized performance monitoring. SingleStore does provide SingleStore Studio, which is great for monitoring the health of your cluster and for optimizing queries, but not focused on performance monitoring as such. SingleStore offers an historical monitoring tool, which is currently in beta, for customers with an Enterprise license. Internally, and like many of our customers, we also use Prometheus – an exporter is mentioned here – and Grafana, which are just two of the many monitoring tools that work with SingleStore. Finally, SingleStore DB 7.0 includes the first iteration of SingleStore Universal Storage , which will eventually unify rowstore and columnstore tables in a single table type, for most workloads. In SingleStore DB 7.0, Universal Storage allows for compression of in-memory rowstore tables of about 50%, saving a great deal of money. At the same time, columnstore tables get big performance improvements from first-generation Universal Storage features in the new release. Summing Up One comment sums up many of SingleStore’s benefits, well, elegantly. When asked, What benefits have you realized?,” this customer comments: “The problem of running streaming analytics on high-velocity, high volume data sets with sub-second API responses. This product elegantly solves the problem.” If you’re already a SingleStore user – whether you have an Enterprise license, or are using SingleStore for free – consider posting a review today. (And remember that you can get questions answered on the SingleStore Forums as well.) Your efforts will benefit the community as a whole. If you haven’t yet tried SingleStore, take a look at the reviews on G2.com . Post questions there, or on the SingleStore Forums . And consider trying SingleStore for free . SingleStore customers – free users, and Enterprise users: Add your review to G2.com today!", "date": "2020-01-09"},
{"website": "Single-Store", "title": "the-write-stuff", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/the-write-stuff/", "abstract": "Why did the world need SingleStore? In this blog post, updated from a few years ago, early SingleStore Product Manager Carlos Bueno explains why SingleStore works better, for a wide range of purposes, than a NoSQL setup. (Thanks, Carlos!) We’ve updated the blog post with new SingleStore product features, graphics, and relevant links. To wit, you should also see Rick Negrin’s famous blog post on NoSQL and our recent case study on replacing Hadoop with SingleStore . Tell us if this sounds familiar. Once upon a time a company ran its operations on The Database Server, a single machine that talked SQL. (Until the advent of NewSQL , most relational database systems – the ones that support SQL – ran on a single core machine at a time. – Ed.) It was tricked out with fast hard drives and cool blue lights. As the business grew, it became harder for The Database to keep up. So they bought an identical server as a hot spare and set up replication, at first only for backups and failover. That machine was too tempting to leave sitting idle, of course. The business analysts asked for access so they could run reports on live-ish data. Soon, the “hot spare” was just as busy – and just as mission-critical – as the master server. And each machine needed its own backup. The business grew some more. The cost of hardware to handle the load went way up. Caching reads only helped so much, and don’t get us started about maintaining cache consistency. It was beginning to look like it would be impossible for The Database Server to handle the volume of writes coming in. The operations people weren’t happy either. The latest semi-annual schema change had been so traumatic and caused so much downtime that they were still twitching. It was then that the company took a deep breath, catalogued all their troubles and heartache, and decided to ditch SQL altogether. It was not an easy choice, but these were desperate times. Six months later the company was humming along on a cluster of “NoSQL” machines acting in concert. It scaled horizontally. The schemas were fluid. Life was good. For a while, anyway. It turned out that when scaled up, the NoSQL cluster worked fine except for two minor things: reading data and writing data. Reading data (“finding documents”) could be sped up by adding indexes. But each new index slowed down write throughput . The business analysts weren’t about to learn how to program just to make their reports. That task fell back onto the engineers, who had to hire more engineers, just to keep up. They told themselves all this was just the price of graduating to Big Data. The business grew a little more, and the cracks suddenly widened. They discovered that “global write lock” essentially means “ good luck doing more than a few thousand writes per second.” A few thousand sounds like a lot, but there are only 86,400 seconds in a day, and the peak-hour of traffic is generally two or three times the average – because, people sleep. A limit of 3,000 writes per second translates to roughly 90 million writes a day. And let’s not talk about reads. Flirting with these limits became as painful as the database platform they’d just abandoned. Tell us if this sounds familiar. I’ve seen a lot of companies suddenly find themselves stuck up a tree like this. It’s not a fun place to be. Hiring performance experts to twiddle with the existing system may or may not help. Moving to a different platform may or may not help either. A startup you’ve definitely heard of runs four – count ‘em, four – separate NoSQL systems, because each one had some indispensable feature (eg, sharding or replication ) that the others didn’t. That way lies madness. Let’s look at the kinds of hardware running Hypothetical Corp’s business. 50 application servers (lots of CPU) 10 Memcached servers (lots of RAM) Four NoSQL servers (lots of disk) The interesting thing is that Hypothetical has several times more RAM in its fleet than the size of their database. If you ask them why, they’ll tell you “because accessing data from RAM is much faster than from disk.” This is, of course, absolutely true.  Accessing  a random piece of data in RAM is 100,000 times faster than a spinning hard disk, and 1,000 times faster than from SSDs. Here’s a crazy idea: instead of throwing a bunch of memory cache around a disk-based NoSQL database that has only half the features you want, what if you cut to the chase and used a database with in-memory rowstore tables, and disk-based columnstore tables, instead? One that talks SQL ? And has replication ? And sharding that actually works ? And high availability ? And massive write throughput via lock-free data structures ? And transactions – including transactions in stored procedures ? And flexible schemas with JSON & non-blocking ALTER TABLE support… …and one that’s steadily growing in capabilities and features. Since this blog post was written, SingleStore has added columnstore tables (see above), SingleStore Studio visual tool for managing clusters, SingleStore Managed Service – our elastic cloud database service, SingleStore Universal Storage , the ability to run SingleStore for free – on premises or in the cloud – and so much more. As mentioned above, please see Rick Negrin’s NoSQL blog post and our case study on replacing Hadoop with SingleStore . You can then get started with SingleStore for free or contact Sales . Notes: http://docs.mongodb.org/manual/core/write-performance/ “After every insert, update, or delete operation, MongoDB must update every index associated with the collection in addition to the data itself. Therefore, every index on a collection adds some amount of overhead for the performance of write operations.” https://tech.dropbox.com/2013/09/scaling-mongodb-at-mailbox/ “…one performance issue that impacted us was MongoDB’s database-level write lock. The amount of time Mailbox’s backends were waiting for the write lock was resulting in user-perceived latency.” http://redis.io/topics/partitioning “The partitioning granuliary [ sic ] is the key, so it is not possible to shard a dataset with a single huge key like a very big sorted set.”", "date": "2020-01-02"},
{"website": "Single-Store", "title": "what-memsql-can-do-for-time-series-applications", "author": ["Eric Hanson"], "link": "https://www.singlestore.com/blog/what-memsql-can-do-for-time-series-applications/", "abstract": "In earlier blog posts we described what time series data is and key characteristics of a time series database . In this blog post, which originally appeared in The New Stack, Eric Hanson, principal product manager at SingleStore, shows you how to use SingleStore for time series applications. At SingleStore we’ve seen strong interest in using our database for time series data . This is especially the case when an organization needs to accommodate the following: (1) a high rate of event ingestion, (2) low-latency queries, and (3) a high rate of concurrent queries. In what follows, I show how SingleStore can be used as a powerful time-series database and illustrate this with simple queries and user-defined functions (UDFs) that show how to do time series-frequency conversion, smoothing, and more. I also cover how to load time series-data points fast, with no scale limits. Note : This blog post was originally published in March 2019. It has been updated to reflect the new time series functions in SingleStore DB 7.0 . Please see the Addendum at the end of this article for specifics on using the information herein. – Ed. Manipulating Time Series with SQL Unlike most time series-specific databases, SingleStore supports standard SQL, including inner and outer joins, subqueries, common table expressions (CTEs), views, rich scalar functions for date and time manipulation, grouping, aggregation, and window functions. We support all the common SQL data types, including a datetime(6) type with microsecond accuracy that’s perfect as a time series timestamp. A common type of time-series analysis in financial trading systems is to manipulate stock ticks. Here’s a simple example of using standard SQL to do this kind of calculation. We use a table with a time series of ticks for multiple stocks, and produce high, low, open, and close for each stock: CREATE TABLE tick(ts datetime(6), symbol varchar(5),\n   price numeric(18,4));\nINSERT INTO tick VALUES\n  ('2019-02-18 10:55:36.179760', 'ABC', 100.00),\n  ('2019-02-18 10:57:26.179761', 'ABC', 101.00),\n  ('2019-02-18 10:59:16.178763', 'ABC', 102.50),\n  ('2019-02-18 11:00:56.179769', 'ABC', 102.00),\n  ('2019-02-18 11:01:37.179769', 'ABC', 103.00),\n  ('2019-02-18 11:02:46.179769', 'ABC', 103.00),\n  ('2019-02-18 11:02:59.179769', 'ABC', 102.60),\n  ('2019-02-18 11:02:46.179769', 'XYZ', 103.00),\n  ('2019-02-18 11:02:59.179769', 'XYZ', 102.60),\n  ('2019-02-18 11:03:59.179769', 'XYZ', 102.50); This query uses standard SQL window functions to produce high, low, open and close values for each symbol in the table, assuming that “ticks” contains data for the most recent trading day. WITH ranked AS\n(SELECT symbol,\n    RANK() OVER w as r,\n    MIN(price) OVER w as min_pr,\n    MAX(price) OVER w as max_pr,\n    FIRST_VALUE(price) OVER w as first,\n    LAST_VALUE(price) OVER w as last\n    FROM tick\n    WINDOW w AS (PARTITION BY symbol\n    ORDER BY ts\n        ROWS BETWEEN UNBOUNDED PRECEDING\n        AND UNBOUNDED FOLLOWING))\n \nSELECT symbol, min_pr, max_pr, first, last\nFROM ranked\nWHERE r = 1; Results: +--------+----------+----------+----------+----------+\n| symbol | min_pr   | max_pr   | first    | last     |   \n+--------+----------+----------+----------+----------+\n| XYZ    | 102.5000 | 103.0000 | 103.0000 | 102.5000 |\n| ABC    | 100.0000 | 103.0000 | 100.0000 | 102.6000 |\n+--------+----------+----------+----------+----------+ Similar queries can be used to create “candlestick charts,” a popular report style for financial time series that looks like the image below. A candlestick chart shows open, high, low, and close prices for a security over successive time intervals: For example, this query generates a table that can be directly converted to a candlestick chart over three-minute intervals: WITH ranked AS\n   (SELECT symbol, ts,\n    RANK() OVER w as r,\n    MIN(price) OVER w as min_pr,\n    MAX(price) OVER w as max_pr,\n    FIRST_VALUE(price) OVER w as first,\n    LAST_VALUE(price) OVER w as last\n \n   FROM tick\n   WINDOW w AS (PARTITION BY symbol, time_bucket('3 minute', ts)\n        ORDER BY ts\n        ROWS BETWEEN UNBOUNDED PRECEDING\n                AND UNBOUNDED FOLLOWING))\n \nSELECT symbol, time_bucket('3 minute', ts), min_pr, max_pr,\nfirst, last\nFROM ranked\nWHERE r = 1\nORDER BY 1, 2; Results: +--------+-----------------------------+----------+----------+----------+----------+\n| symbol | time_bucket('3 minute', ts) | min_pr   | max_pr   | first    | last     |\n+--------+-----------------------------+----------+----------+----------+----------+\n| ABC    | 2019-02-18 10:54:00.000000  | 100.0000 | 100.0000 | 100.0000 | 100.0000 |\n| ABC    | 2019-02-18 10:57:00.000000  | 101.0000 | 102.5000 | 101.0000 | 102.5000 |\n| ABC    | 2019-02-18 11:00:00.000000  | 102.0000 | 103.0000 | 102.0000 | 102.6000 |\n| XYZ    | 2019-02-18 11:00:00.000000  | 102.6000 | 103.0000 | 103.0000 | 102.6000 |\n| XYZ    | 2019-02-18 11:03:00.000000  | 102.5000 | 102.5000 | 102.5000 | 102.5000 |\n+--------+-----------------------------+----------+----------+----------+----------+ Smoothing is another common need in managing time series data. This query produces a smoothed sequence of prices for stock “ABC,” averaging the price over the last three ticks: SELECT symbol, ts, price,\nAVG(price) OVER (ORDER BY ts ROWS BETWEEN 3 PRECEDING AND CURRENT ROW) AS smoothed_price\nFROM tick\nWHERE symbol = 'ABC'; Results: +--------+----------------------------+----------+----------------+\n| symbol | ts                         | price    | smoothed_price |\n+--------+----------------------------+----------+----------------+\n| ABC    | 2019-02-18 10:55:36.179760 | 100.0000 |   100.00000000 |\n| ABC    | 2019-02-18 10:57:26.179761 | 101.0000 |   100.50000000 |\n| ABC    | 2019-02-18 10:59:16.178763 | 102.5000 |   101.16666667 |\n| ABC    | 2019-02-18 11:00:56.179769 | 102.0000 |   101.37500000 |\n| ABC    | 2019-02-18 11:01:37.179769 | 103.0000 |   102.12500000 |\n| ABC    | 2019-02-18 11:02:46.179769 | 103.0000 |   102.62500000 |\n| ABC    | 2019-02-18 11:02:59.179769 | 102.6000 |   102.65000000 |\n+--------+----------------------------+----------+----------------+ Using Extensibility to Increase the Power of SingleStore for Time Series SingleStore supports extensibility with user-defined functions and stored procedures. SingleStore compiles UDFs and stored procedures to machine code for high performance. I actually used SingleStore’s extensibility to create the time _ bucket() function, shown in the Supplemental Material section below, which appeared in the previous section as a UDF. This function provides equivalent capability to similar functions in time-series-specific products. You can easily create a function or expression to bucket by time intervals, such as second, minute, hour, or day. A common need with time-series data is to perform interpolation. For example, suppose you have a time series with points at random intervals that are 30 seconds apart on average. There may be some minutes with no data point. So, if you convert the raw (irregular) time-series data to a regular time series with a point a minute, there may be gaps. If you want to provide output for plotting with no gaps, you need to interpolate the values for the gaps from the values before and after the gaps. It’s straightforward to implement a stored procedure in SingleStore by taking a query result and outputting a row set, with the gaps interpolated, into a temporary table. This can then be sent back to the client application using the ECHO command . In addition, SingleStore supports user-defined aggregate functions. These functions can be used to implement useful time series operations, such as shorthand for getting the first and last values in a sequence without the need for specific window functions. Consider this query to get the first value for stock ABC in each three minutes of trading, based on a user-defined aggregate function (UDAF) called FIRST() : SELECT time_bucket('3 minute', ts), first(price, ts)\nFROM tick\nWHERE symbol = \"ABC\"\nGROUP BY 1\nORDER BY 1; Results: +-----------------------------+------------------+\n| time_bucket('3 minute', ts) | first(price, ts) |\n+-----------------------------+------------------+\n| 2019-02-18 10:54:00.000000  | 100.0000         |\n| 2019-02-18 10:57:00.000000  | 101.0000         |\n| 2019-02-18 11:00:00.000000  | 102.0000         |\n+-----------------------------+------------------+ The implementations of the FIRST() UDAF, and the analogous LAST() UDAF, are shown in the Supplemental Material section below. Time Series Compression and Life Cycle Management SingleStore is adept at handling both bursty insert traffic for time series events and historical time series information where space savings are important. For bursty insert traffic, you can use a SingleStore rowstore table to hold time series events. For larger and longer-lived sets of time series events, or older time series data sets that have aged and are unlikely to be updated anymore, the SingleStore columnstore is a great format. It compresses time-series data very effectively, with SingleStore supporting fast operations on compressed columnstore data. Moreover, columnstore data resides on disk, so main memory size is not a limit on how much data you can store. Scalable Time Series Ingestion When building a time series application, data can come at high rates from many sources. Sources include applications, file systems, AWS S3, Hadoop HDFS, Azure Blob stores, and Kafka queues. SingleStore can ingest data incredibly fast from all these sources. SingleStore Pipelines are purpose-built for fast and easy loading of data streams from these sources, requiring no procedural coding to establish a fast flow of events into SingleStore. SingleStore can ingest data at phenomenal data rates. In a recent test, I inserted 2,850,500 events per second directly from an application, with full transactional integrity and persistence, using a two-leaf SingleStore cluster. Each leaf ran on an Intel Xeon Platinum 28-core system. Comparable or even better rates can be had using direct loading or Kafka pipelines. If you have to scale higher, just add more nodes — there’s no practical limit. When General-Purpose SingleStore Is Right for Time Series We’ve seen the market for time-series data management bifurcate into special-purpose products for time series, with their own special-purpose languages, and extended SQL systems that can interoperate with standard reporting and business intelligence tools that use SQL. SingleStore is in this second category. SingleStore is right for time series applications that need rapid ingest, low-latency query, and high concurrency, without scale limits, and which benefit from SQL language features and SQL tool connectivity. Many time-series-specific products have shortcomings when it comes to data management. Some lack scale-out, capping the size of problems they can tackle, or forcing application developers to build tortuous sharding logic into their code to split data across multiple instances, which costs precious dollars for labor that could better be invested into application business logic. Other systems have interpreted query processors that can’t keep up with the latest query execution implementations as ours can. Some lack transaction processing integrity features common to SQL databases. SingleStore lets time series application developers move forward confidently, knowing they won’t hit a scale wall, and they can use all their familiar tools — anything that can connect to a SQL database. Summary SingleStore is a strong platform for managing time series data. It supports the ability to load streams of events fast and conveniently, with unlimited scale. It supports full SQL that enables sophisticated querying using all the standard capabilities of SQL 92, plus the more recently added window function extensions. SingleStore supports transactions, high rates of concurrent update and query, and high availability technologies that many developers need for all kinds of applications, including time series. And your favorite SQL-compatible tools, such as business intelligence (BI) tools, can connect to SingleStore. Users and developers – in areas such as real-time analytics, predictive analytics, machine learning, and AI – can use the SQL interfaces they’re familiar with, as described above. All of this and more makes SingleStore a strong platform for time series. Download and use SingleStore for free today and try it on your time series data! Addendum In SingleStore DB 7.0, we added TIME _ BUCKET(), FIRST(), and LAST() as built-in functions. So, if you run the scripts above to create those functions on SingleStore DB 7.0, you will get an error. We recommend that you simply use the built-in version of the functions in 7.0. See the documentation here: https://docs.singlestore.com/v7.0/reference/sql-reference/time-series-functions/time-series-functions/ If you’d like to experiment with the user-defined versions, edit the scripts to rename the functions to TIME _ BUCKET2(), FIRST2(), and LAST2(), or something similar, before creating them. Supplemental Material 1/2: Full Text of time _ bucket() Function -- Usage: time_bucket(interval_string, timestamp_value)\n-- Examples: time_bucket('1 day', ts), time_bucket('5 seconds', ts)\n\nDELIMITER //\nCREATE OR REPLACE FUNCTION time_bucket(\n  bucket_desc varchar(64) NOT NULL, \n  ts datetime(6)) RETURNS datetime(6) NULL AS\nDECLARE\n  num_periods bigint = -1; \n  second_part_offset int = -1;\n  unit varchar(255) = NULL;\n  num_str varchar(255) = NULL;\n  unix_ts bigint;\n  r datetime(6);\n  days_since_epoch bigint;\nBEGIN\n  num_str = substring_index(bucket_desc, ' ', 1);\n  num_periods = num_str :> bigint;\n  unit = substr(bucket_desc, length(num_str) + 2, length(bucket_desc));\n  IF unit = 'second' or unit = 'seconds' THEN\n    unit = 'second';\n  ELSIF unit = 'minute' or unit = 'minutes' THEN\n    unit = 'minute';\n  ELSIF unit = 'hour' or unit = 'hours' THEN\n    unit = 'hour';\n  ELSIF unit = 'day' or unit = 'days' THEN\n    unit = 'day';\n  ELSE\n    raise user_exception(concat(\"Unknown time unit: \", unit));\n  END IF;\n\n  unix_ts = unix_timestamp(ts);\n  \n  IF unit = 'second' THEN\n    r = from_unixtime(unix_ts - (unix_ts % num_periods));\n  ELSIF unit = 'minute' THEN \n    r = from_unixtime(unix_ts - (unix_ts % (num_periods * 60)));\n  ELSIF unit = 'hour' THEN \n    r = from_unixtime(unix_ts - (unix_ts % (num_periods * 60 * 60)));\n  ELSIF unit = 'day' THEN \n    unix_ts += 4 * 60 * 60; -- adjust to align day boundary\n    days_since_epoch = unix_ts / (24 * 60 * 60);\n    days_since_epoch = days_since_epoch - (days_since_epoch % num_periods);\n    r = (from_unixtime(days_since_epoch * (24 * 60 * 60))) :> date;\n  ELSE\n    raise user_exception(\"Internal error -- bad time unit\");\n  END IF;\n\n  RETURN r;\nEND;\n//\nDELIMITER ; Supplemental Material 2/2: Full Text of first() and last() Aggregate Functions The following UDAF returns the first value in a sequence, ordered by the second argument, a timestamp: -- Usage: first(value, timestamp_expr)\n-- Example: \n--   Get first value of x for each day from a time series in table \n--     t(x, ts)\n--   with timestamp ts.\n-- \n--   SELECT ts :> date, first(x, ts) FROM t GROUP BY 1 ORDER BY 1;\n\nDELIMITER //\nCREATE OR REPLACE FUNCTION first_init() RETURNS RECORD(v TEXT, d datetime(6)) AS\n  BEGIN\n    RETURN ROW(\"_empty_set_\", '9999-12-31 23:59:59.999999');\n  END //\nDELIMITER ;\n\nDELIMITER //\nCREATE OR REPLACE FUNCTION first_iter(state RECORD(v TEXT, d DATETIME(6)), \n   v TEXT, d DATETIME(6)) \n  RETURNS RECORD(v TEXT, d DATETIME(6)) AS\n  DECLARE \n    nv TEXT;\n    nd DATETIME(6);\n    nr RECORD(v TEXT, d DATETIME(6)); \n  BEGIN\n    -- if new timestamp is less than lowest before, update state\n    IF state.d > d THEN\n      nr.v = v;\n      nr.d = d;\n      RETURN nr;\n    END IF;\n    RETURN state; \n  END //\nDELIMITER ;\n\nDELIMITER //\nCREATE OR REPLACE FUNCTION first_merge(state1 RECORD(v TEXT, d DATETIME(6)), \n   state2 RECORD(v TEXT, d DATETIME(6))) RETURNS RECORD(v TEXT, d DATETIME(6)) AS\n  BEGIN\n    IF state1.d < state2.d THEN\n      RETURN state1;\n    END IF;\n    RETURN state2;\n  END //\nDELIMITER ;\n\nDELIMITER //\nCREATE OR REPLACE FUNCTION first_terminate(state RECORD(v TEXT, d DATETIME(6))) RETURNS TEXT AS\n  BEGIN\n    RETURN state.v;\n  END //\nDELIMITER ;\n\nCREATE AGGREGATE first(TEXT, DATETIME(6)) RETURNS TEXT\n  WITH STATE RECORD(v TEXT, d DATETIME(6))\n  INITIALIZE WITH first_init\n  ITERATE WITH first_iter\n  MERGE WITH first_merge\n  TERMINATE WITH first_terminate; A LAST() UDAF that is analogous to FIRST(), but returns the final value in a sequence ordered by timestamp, is as follows: -- Usage: last(value, timestamp_expr)\n-- Example: \n--   Get last value of x for each day from a time series in table t\n--     t(x, ts)\n--   with timestamp column ts.\n-- \n--   SELECT ts :> date, last(x, ts) FROM t GROUP BY 1 ORDER BY 1;\n\nDELIMITER //\nCREATE OR REPLACE FUNCTION last_init() RETURNS RECORD(v TEXT, d datetime(6)) AS\n  BEGIN\n    RETURN ROW(\"_empty_set_\", '1000-01-01 00:00:00.000000');\n  END //\nDELIMITER ;\n\nDELIMITER //\nCREATE OR REPLACE FUNCTION last_iter(state RECORD(v TEXT, d DATETIME(6)), \n   v TEXT, d DATETIME(6)) \n  RETURNS RECORD(v TEXT, d DATETIME(6)) AS\n  DECLARE \n    nv TEXT;\n    nd DATETIME(6);\n    nr RECORD(v TEXT, d DATETIME(6)); \n  BEGIN\n    -- if new timestamp is greater than largest before, update state\n    IF state.d < d THEN nr.v = v; nr.d = d; RETURN nr; END IF; RETURN state; END // DELIMITER ; DELIMITER // CREATE OR REPLACE FUNCTION last_merge(state1 RECORD(v TEXT, d DATETIME(6)), state2 RECORD(v TEXT, d DATETIME(6))) RETURNS RECORD(v TEXT, d DATETIME(6)) AS BEGIN IF state1.d > state2.d THEN\n      RETURN state1;\n    END IF;\n    RETURN state2;\n  END //\nDELIMITER ;\n\nDELIMITER //\nCREATE OR REPLACE FUNCTION last_terminate(state RECORD(v TEXT, d DATETIME(6))) RETURNS TEXT AS\n  BEGIN\n    RETURN state.v;\n  END //\nDELIMITER ;\n\nCREATE AGGREGATE last(TEXT, DATETIME(6)) RETURNS TEXT\n  WITH STATE RECORD(v TEXT, d DATETIME(6))\n  INITIALIZE WITH last_init\n  ITERATE WITH last_iter\n  MERGE WITH last_merge\n  TERMINATE WITH last_terminate;", "date": "2020-01-21"},
{"website": "Single-Store", "title": "stop-the-insanity-eliminating-data-infrastructure-sprawl", "author": ["Rick Negrin"], "link": "https://www.singlestore.com/blog/stop-the-insanity-eliminating-data-infrastructure-sprawl/", "abstract": "There is a trend in industry which says that modern applications need to be built on top of one or more special-purpose databases. That every application benefits from using the best-of-breed technology for each requirement. And that the plethora of special-purpose options available from certain cloud providers is reasonable to manage. That’s all BUNK. The reality is that navigating the choices, figuring out how to use them effectively, and dealing with the ETL and inevitable data sprawl, is so difficult that the pain far outweighs any technical advantage you might get. In the vast majority of use cases, a single modern, scalable, relational database can support all of an application’s needs, across cloud providers and on-premises. Over the past decade, applications have become more and more data-intensive. Dynamic data, analytics, and models are now at the core of any application that matters. In order to support these requirements, there is a commonly held belief that modern applications need to be built on top of a variety of special-purpose databases, each built for a specific workload. It is said that this allows you to pick the best ones to solve your application needs. This trend is apparent when you look at the plethora of open source data tools that have proliferated in recent years. Each one was built to scratch an itch; optimized for specific, narrow use cases seen in a smattering of projects. In response, some of the cloud vendors have packaged up these multiple database technologies for you to choose from, commonly forking from existing open source projects. You’re then meant to wire together several of these tools into the needed data solution for each application. On the surface, the argument seems logical. Why bother to build or use general-purpose technology across multiple problem domains, maybe having to work around limitations that come from being a general-purpose solution, when you can use multiple tools, purpose-built for each of the specific problems you are trying to solve? Andy Jassy, CEO of AWS, made this point in his keynote at the company’s Re:Invent conference recently. Saying: “In the past, customers primarily used relational databases, and the day for that has come and gone…. Customers have asked for, and demand, purpose-built databases.” The claim is that relational databases are too expensive; not performant; don’t scale. That is supposedly why Amazon offers eight operational databases. (It had been seven, but they announced another one at the conference: Amazon Managed Apache Cassandra Service .) This is not including the various analytic data warehouse technologies from AWS, such as Redshift, Athena, and Spectrum. Jassy goes on a rant about how anyone who tries to convince you otherwise is fooling you, and you should just ignore the person and walk away. (See the 1:18:00 mark in the keynote.) Well, I am that person – and I am not alone. This is not to say that there is no value in each of the special-purpose database offerings. There are certainly use cases where those special purpose databases shine, and are truly the best choice for the use case. These are cases where the requirements in one specific dimension are so extreme that you need something special-purpose to meet them. But the absolute requirement for specialty databases is mostly in outlier use cases, which are a tiny fraction of the total of workloads out there. In the vast majority of apps that people build, the requirements are such that they can be satisfied by a single, operational NewSQL database – a distributed relational database, supporting a mix of transactional and analytical workloads, multi-model, etc. – such as SingleStore. This is especially true when you find you need more than just a couple of special-purpose databases in your solution, or when your requirements are expected to change over time. The burden of choice has always been the dilemma of the software engineer. It used to be that the choice was whether to buy an existing component or to build it yourself. You had to make the trade-off between the dollar cost to purchase the component – and the risk it might not be as good as you hoped – vs. the cost, in time and engineering resources, to build and maintain a custom solution. Most experienced engineers would likely agree that, in most cases, it is better to buy an existing component if it can meet the requirements. The cost to build is always higher than you think, and the cost to work out issues and to maintain the solution over time often dwarfs the initial cost. In addition, having someone to call when something breaks is critical for a production system with real customers. But then things changed. How Choices Have Ballooned with Open Source and the Cloud The emergence of open source software has fundamentally changed the “build vs. buy” choice. Now, it is a choice of build, buy – or get for free. And people love free. Most engineers who use open source don’t really care about tinkering with the source code and submitting their changes back to the code base, or referring to the source code to debug problems. While that certainly does happen (and kudos to those who contribute), the vast majority are attracted to open source because it is free. The availability of the Internet and modern code repositories like Github have made the cost to build software low, and the cost to distribute software virtually nothing. This has given rise to new technology components at a faster rate than ever seen before. Github has seen massive growth in the number of new projects and the number of developers contributing, with 40 million contributors in 2019, 25% of whom are new, and 44 million repositories. On the face of it, this seems great. The more components that exist, the better the odds that the one component that exactly matches my requirements has already been built. And since they are all free, I can choose the best one. But this gives rise to a new problem. How do I find the right one(s) for my app? Too Many Options There are so many projects going on that navigating the tangle is pretty difficult. In the past, you generally had a few commercial options. Now, there might be tens or hundreds of options to choose from. You end up having to narrow it down to a few choices based on limited time and information. Database technology in particular has seen this problem mushroom in recent years. It used to be you had a small number of choices: Oracle, Microsoft SQL Server, and IBM DB2 as the proprietary choices, or MySQL if you wanted a free and open source choice. Then, two trends matured: NoSQL, and the rise of open source as a model. The number of choices grew tremendously. In addition, as cloud vendors are trying to differentiate, they have each added both NoSQL databases and their own flavors of relational (or SQL) databases. AWS has more than 10 database offerings; Azure and GCP each have more than five flavors. AWS offers a bewildering plethora of database choices . DBEngines (a site for tracking the popularity of database engines) has more than 300 databases on the list, with new ones getting added all the time. Even the definition of what is a “database” has evolved over time, with some simple data tools such as caches marketing themselves as databases. This is making it difficult to know, without a lot of research, whether a particular technology will match the requirements of your application. Fail to do enough research, and you can waste a lot of time building on a data technology, only to find it has some important gap that tanks your design. Choosing a Specialty Database Type There are many different flavors of databases on the list. Operational databases and data warehouses are the most common types, but there are several more. Each has a set of requirements which they solve. Database Types Requirements Operational Databases Oracle, SQL Server, Postgres, MySQL, MariaDB, AWS Aurora, GCP Spanner -   Fast Insert -   Fast Record Lookup -   High Concurrency -   High Availability -   High Resilience -   Relational Model -   Complex Query -   Extensibility Data Warehouses Teradata, Netezza, Vertica, Snowflake -   Fast Query -   Aggregations -   Large Data Size -   Large Data Load -   Resource Governance Key-Value Stores Redis, GridGain, Memcached -   Fast Insert -   Fast Record Lookup -   High Concurrency -   High Availability Document Stores MongoDB, AWS DocDB, AWS DynamoDB, Azure Cosmos DB, CouchDB -   Fast Record Lookup -   High Availability -   Flexible Schema Full-Text Search Engines Elasticsearch, AWS Elasticache, Solr -   Fuzzy Text Search -   Large Data Sets -   High Availability Time Series: InfluxDB, OpenTSDB, TimescaleDB, AWS Timestream -   Simple queries over time series data GraphDB: Neo4j, JanusGraph, TigerGraph, AWS Neptune -   Graph-Based Data Relationships -   Complex Queries Table 1. Fitting Your Prospective Application to Different Database Types Every database is slightly different in the scenario it excels at. And there are new specialty databases emerging all the time. If you’re building a new solution, you have to decide what data architecture you need. Even if you assume the requirements are clear and fixed – which is almost never the case – navigating the bewildering set of choices as to which database to use is pretty hard. You need to assess requirements across a broad set of dimensions – such as functionality, performance, security, and support options – to determine which ones meet your needs. AWS boils it down to a 54-slide deck to help you choose. If you have functionality that cuts across the different specialty databases, then you will likely need multiple of them. For example, you may want to store data using a standard relational model, but also need to do full text queries. You may also have data whose schema is changing relatively often, so you want to use a JSON document as part of your storage. The combination of databases you can use in your solution is pretty large. It’s hard to narrow that down by just scanning the marketing pages and the documentation for each potential solution. Websites cannot reliably tell you whether a database offering can meet your performance needs. Only prior experience, or a PoC, can do that effectively. How Do I Find the Right People? Once you have found the right set of technologies, who builds the application? You likely have a development team already, but the odds of them being proficient in programming applications on each specific, new database are low. This means a slower pace of development as they ramp up. Their work is also likely to be buggier as they learn how to use the system effectively. They also aren’t likely to know how to tune for optimal performance. This affects not just developers, but the admins who run, configure, and troubleshoot the system once it is in production. How Do I Manage and Support the Solution with So Many Technologies? Even after you pick the system and find the right people, running the solution is not easy. Most likely you had to pick several technologies to build the overall solution. Which means probably no one in your organization understands all the parts. Having multiple parts also means you have to figure out how to integrate all the pieces together. Those integration points are both the hardest to figure out, and the weakest point in the system. It is often where performance bottlenecks accumulate. It is also a source of bugs and brittleness, as the pieces are most likely not designed to work together. When the solution does break, problems are hard to debug. Even if you have paid for support for each technology – which defeats the purpose, if you’re using things which are free – the support folks for each technology are not likely to be helpful in figuring out the integration problems. (They are just as likely to blame each other as to help you solve your problem). The Takeaway Going with multiple specialty databases is going to cost you, in time, hassle, money and complexity: Investigation analysis . It takes a lot of energy and time to interrogate a new technology to see what it can do. The number of choices available is bewildering and overwhelming. Every minute you spend doing the investigation slows down your time to market. Many vendors . If you end up choosing multiple technologies, you are likely to have different vendors to work with. If the solution is open source, you are either buying support from a vendor, or figuring out how to support the solution yourself. Specialized engineers . It takes time and experience to truly learn how to use each new data technology. The more technology you incorporate into your solution, the harder it is to find the right talent to implement it correctly. Complicated integrations . The most brittle parts of an application are the seams between two different technologies. Transferring data between systems with slightly different semantics, protocols that differ, and connection technologies that have different scale points are the places where things break down (usually when the system is at its busiest). Performance bottlenecks . Meshing two different technologies is also where performance bottlenecks typically occur. With data technologies, it is often because of data movement. Troubleshooting integration problems . Tracking down and fixing these issues is problematic, as the people doing the tracking down are rarely experts in all the technologies. This leads to low availability, frustrated engineers, and unhappy customers. Considering SingleStore – a New Solution for a New Era Ideally, there would be a database infrastructure which is familiar; which has an interface that most existing developers know how to use and optimize; and which has functionality needed to handle 90% or more of the use cases that exist. It would need to be cloud-native – meaning it natively runs in any cloud environment, as well as in an on-premises environment, using cloud-friendly tools such as Kubernetes. This ideal technology would also be distributed, so that it scales easily, as required by the demands of the application. This database would be the default choice for the vast majority of applications, and developers would only need to look for other solutions if they hit an outlier use case. Using the same database technology for the vast majority of solutions means the engineers will be familiar with how to use it, and able to avoid the issues listed above. This is why we built SingleStore. Legacy databases like Oracle and SQL Server served this function for a long time. But the scale and complexity requirements of modern applications outgrew their capabilities. These needs gave rise to the plethora of NoSQL systems that emerged out of the need to solve for the scale problem. (I discuss this in my blog about NoSQL and relational databases .) But the NoSQL systems gave up a lot of the most useful functionality, such as structure for data and SQL query support, forcing users to choose between scale and functionality. NewSQL systems like SingleStore allow you to have the best of both worlds. You get a highly scalable cloud native system that is durable, available, secure, and resilient to failure – but with an interface that is familiar to developers and admins. It supports a broad set of functionality. It supports ANSI SQL. It supports all major data types – relational, semi-structured (native support for storing JSON and for ingesting JSON, AVRO and Parquet), native geo-spatial indexes, and Lucene-based full text indexes that allow Lucene queries to be embedded in relational queries. It has both rowstore and columnstore tables – currently merging into Universal Storage tables – supporting complex analytical workloads, as well as transactional and operational workloads. SingleStore has support for transactions. It supports stored procedures and user-defined functions (UDFs), for all your extensibility needs. It can ingest data natively from all the major data sources from legacy database systems, to blob stores like S3 and Azure Blob, as well as modern streaming technologies such as Kafka and Spark. The combination of a shared-nothing scale-out architecture and support for in-memory rowstore tables means there is no need for a caching layer for storing and retrieving key-value pairs. Because SingleStore is wire protocol-compatible with MySQL, it supports a huge ecosystem of third-party tools. SingleStore has a rich set of security features, such as multiple forms of authentication (username/password, Kerberos, SAML, and PAM). It supports role-based access control (RBAC) and row-level security for authorization. SingleStore supports encryption. You can use the audit feature to determine who accessed your data, and what was accessed. Lastly, SingleStore can be deployed using standard command-line tools, on Kubernetes via a native Kubernetes Operator, or managed by SingleStore, via our managed service, SingleStore Managed Service . SingleStore in Action Let’s walk through some examples of a few customers who ran into these problems while first trying to build their solution using a combination of technologies, and how they ultimately met their requirements with SingleStore. Going Live with Credit Card Transactions A leading financial services company ran into challenges with their credit and debit fraud detection service. The firm saw rising fraud costs and customer experience challenges that ultimately prompted re-building their own in-house solution. The goal was to build a new data platform that could provide a faster, more accurate service – one that could catch fraud before the transaction was complete, rather than after the fact, and be easier to manage. You can see in the diagram below that the customer was using ten distinct technology systems. Stitching these systems together, given the complexity of interactions, was very hard. Ultimately, the overall system did not perform as well as they hoped. The latency for the data to traverse all the systems was so high that they could only catch fraud after the transaction had gone through. (To stop a transaction in progress, you need to make a decision in tens or hundreds of milliseconds. Anything longer means an unacceptable customer experience.) It was also hard to find people with the right experience in each of the technologies to be sure the system was being used correctly. Lastly, it was hard to keep the system up and running, as it would often break at the connection points between the systems. They replaced the above system with the architecture below. They reduced the ten technologies down to two: Kafka and SingleStore. They use Kafka as the pipeline to flow all the incoming data from the upstream operational systems. All of that lands in SingleStore, where the analysis is done and surfaced to the application. The application then uses the result to decide whether to accept the credit card transaction or reject it. In addition, analysts use SingleStore to do historical analysis to see when and where fraud is coming from. They are now able to meet their service level agreements (SLAs) for running the machine learning algorithm and reporting the results back to the purchasing system, without impacting the user experience of the live credit card purchase. Making e-Commerce Fanatics Happy Another SingleStore customer, Fanatics, also used SingleStore to reduce the number of technologies they were using in their solution. Fanatics has its own supersite, Fanatics.com , but also runs the online stores of all major North American sports leagues, more than 200 professional and collegiate teams, and several of the world’s largest global football (soccer) franchises. Fanatics has been growing rapidly, which is great for them as a business – but which caused technical challenges. Fanatics’ workflows were very complex and difficult to manage during peak traffic events, such as a championship game. Business needs evolve frequently at Fanatics, meaning schemas had to change to match – but these updates were very difficult. Maintaining the different query platforms and the underlying analytics infrastructure cost Fanatics a lot of time to keep things running, and to try to meet SLAs. So the company decided on a new approach. SingleStore replaced the Lucene-based indexers. Spark and Flink jobs were converted to SQL-based processing, which allows for consistent, predictable development life cycles, and more predictability in meeting SLAs. The database has grown to billions of rows, yet users are still able to run ad hoc queries and scheduled reports, with excellent performance. The company ingests all its enterprise sources into SingleStore, integrates the data, and gains a comprehensive view of the current state of the business. Fanatics was also able to unify its users onto a single, SQL-based platform. This sharply lowers the barriers to entry, because SQL is so widely known. More details about the Fanatics use cases are in this blog post from SingleStore . The above are just two examples. SingleStore has helped many customers with simplification and performance of their application: SingleStore customer Thorn has built their application to find missing children using SingleStore Managed Service, making use of several features in SingleStore , vector matching for face matching, and full text search for text analysis. A major technology services company moved from a complex, multi-database solution to a simple solution, with Kafka for streaming, and SingleStore as the sole database. SME Solutions Group is building a business by moving their customers from complex data lake implementations to simpler, SingleStore-based solutions. Go Guardian, a fast-growing SaaS company, increased their scalability and simplified their infrastructure by moving from a legacy database + Druid to SingleStore . Conclusion Some cloud providers claim that you need to have eight different purpose-built databases, if not more, to build your application – and that it is impractical for one database to meet all of the requirements for an application. We at SingleStore respectfully disagree. While there are some outlier use cases that may require a specialty database, the vast majority of applications can have all their key requirements satisfied by a single NewSQL database, such as SingleStore. Even if you can build your solution using a number of special-purposes databases, the cost to investigate, build, optimize, implement, and manage those systems will outweigh any perceived benefit. Keep it simple; use the database that meets your requirements. Try SingleStore for free today, or contact us for a personalized demo.", "date": "2020-01-22"},
{"website": "Single-Store", "title": "gartner-peer-insights-applauds-memsql", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/gartner-peer-insights-applauds-memsql/", "abstract": "Gartner Peer Insights features technology product reviews from enterprise users, with more than 300,000 reviews covering nearly 6,000 products in more than 330 categories. Upholding their reputation as top industry analysts for enterprise technology, Gartner sees to it that the reviews are, in their words, “rigorously vetted,” with “no vendor bias.” SingleStore has nearly two dozen reviews , with an overall rating of 4.5 stars. Reviews cover  highlighting key points of what the software does for users. For those who want to know more about SingleStore, these reviews are a stellar resource. And, for those who are already SingleStore users, you can post a review today . Each rating includes dozens of areas for comment, in several distinctive areas: evaluation & contracting, such as reasons for purchase; integration & deployment, such as listing other platforms and products the software will be integrated with; service & support, such as the quality of support; product capabilities, such as the variety of data types supported; and additional context. Across the reviews, several key points come up: Speed . SingleStore is fast – “blazing fast aggregate queries,” says one user. “The queries perform really well on sharded data,” says another. A third liked “fast data ingest – running at a trillion rows per second.” A senior consultant with a large manufacturer described “10x to 100x performance improvements compared to Microsoft SQL Server.” Scalability . SingleStore is a scalable SQL database, a rare commodity, since legacy databases don’t scale easily, if at all. This opens up a range of capabilities; “creating pipelines within SQL is amazing.” MySQL compatibility . SingleStore supports ANSI SQL, and is wire protocol-compatible with MySQL, making it easy to integrate and easy to use. “The learning curve with our engineers was minimal,” said one user. “It’s friendly for a MySQL user, but faster,” said another. Flexibility . “SingleStore has a hybrid, rowstore and columnstore solution to resolve different use cases.” This flexible architecture was mentioned by many reviewers. Reviewers also shared a few things they wanted others to be aware of, in addition to the positives. “Make sure you understand the difference between the memory engine and the columnstore engine,” reads one review. Several said scaling to a larger or smaller footprint should be smoother. Reviewers integrated SingleStore with a wide range of widely used technologies, including: MySQL, S3, and PHP. Python, Databricks, and Tableau. Other business intelligence (BI) products, such as DundasBI. One user replaced SAP Sybase with SingleStore; two others replaced Microsoft SQL Server. Most deployments took from zero to three months. Several comments summarized users’ reactions. “If you could start over, what would your organization do differently?,” says one question. The answer: “Start using SingleStore earlier”; another echoed, “Pick SingleStore sooner.”  A highly satisfied user purchased SingleStore to “create internal/operational efficiencies” and “drive innovation.” Other comments described the “strong customer focus” of SingleStore, the way in which the product is “evolving fast,” and the “strong roadmap” for feature development. “An awesome team,” said one user in finance, who also said that their own “engineering teams are very happy with the product.” Much of the value of review sites like this one comes from the large number of distinctive voices that contribute. If you are already a SingleStore user, you can post a review today . If not, you can also read about the reviews on G2.com , and ask questions on G2.com, or on the SingleStore Forums . And you can try SingleStore for free today.", "date": "2020-01-27"},
{"website": "Single-Store", "title": "new-year-new-faqs", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/new-year-new-faqs/", "abstract": "As interest in SingleStore increases, we get many questions about how SingleStore works, how to get started with it, and more. Performance reviews website G2.com has a SingleStore Q&A section where potential customers can ask questions. Here are some of the questions we hear most often – with answers – lightly edited, for context and clarity. Q. What is the advantage of SingleStore over other distributed databases? A. Compared to relational databases – those which support SQL – we believe that SingleStore is the fastest, most efficient SQL database. SingleStore features full, linear scalability, unlike competitors. We also handle both transactions and analytics in one database, as described by customers in the reviews on G2.com . Compared to NoSQL, SingleStore is at least as scalable, far more efficient with machine resources, and of course, unlike NoSQL databases, we have full ANSI SQL support. SingleStore also supports data types, such as JSON and geospatial data, that may otherwise only be supported by NoSQL databases. Q. How to simplify scaling of a SingleStore cluster? We would like our microservices to use-in memory processing and storage for analytics purposes. A. This question does seem particularly pertinent to microservices, as you are more likely to have multiple data stores. There are several parts to the answer: This tutorial describes how to scale your cluster for optimal performance. You can use Kubernetes, specifically the SingleStore Kubernetes Operator , to scale clusters flexibly. With SingleStore Managed Service , our elastic managed service in the cloud, you simply send a request, and SingleStore will quickly rescale the cluster for you. For more specifics, please use the SingleStore Forums to give a more detailed description and get a more detailed answer – or file a support ticket, if you have an Enterprise license. Alternatively, contact SingleStore directly for more in-depth information. Q. Is SingleStore a row-based or column-based store? A. We are happy to report that the answer is: Yes. SingleStore supports both rowstore and columnstore tables in the same database (or separate databases), with the ability to efficiently run JOINs and other SQL operations across both table types. We also support new capabilities, under the umbrella of SingleStore Universal Storage , which will gradually unify the two table types for most workloads; see the description of Universal Storage-related changes in SingleStore DB 7.0, below. And see the SingleStore review comments on G2.com for more information about rowstore and columnstore tables, and also contact SingleStore directly . Q. What is the relationship between SingleStore and MySQL? A. SingleStore and MySQL are both ANSI SQL-compliant, so the same queries work on both – as is, or with minor changes. In addition, SingleStore directly supports the MySQL wire protocol for connectivity. Q. What versions of SingleStore are available? A. The current version, SingleStore DB 7.0, is the only version available. SingleStore DB 7.0 is available for (free) download and is also the version that powers SingleStore Managed Service , our on-demand, elastic, managed service in the cloud. (Managed Service is compared to other cloud databases in the table below.) Some existing SingleStore customers are still running older versions, as is typical with infrastructure software. Q. Where can I go for help if I do not have access to paid support? A. Please visit the SingleStore Forums . Q. How can I get a copy of SingleStore? A. You can use SingleStore for free . You can download a fully capable version of the SingleStore software for use on your on-premises servers or in the cloud. This comes with community support and has fairly liberal capacity constraints. Alternatively, you can get a free 8-hour trial of SingleStore Managed Service, our elastic managed service in the cloud. Or, contact SingleStore to discuss using SingleStore for a proof of concept enterprise.", "date": "2020-01-10"},
{"website": "Single-Store", "title": "a-balanced-approach-to-database-use-with-microservices", "author": ["Rob Richardson"], "link": "https://www.singlestore.com/blog/a-balanced-approach-to-database-use-with-microservices/", "abstract": "Microservices architectures have focused heavily on compute services, with data storage and retrieval – a crucial topic – sidelined, and left as an exercise for the developer. Crucially, Kubernetes did not include direct support for stateful services (ie, databases) initially. The Red Hat developers blog has suggested that data management is The Hardest Part About Microservices . In response, in this blog post, we suggest that a NewSQL database, such as SingleStore, can provide data services to multiple microservices implementations in a manageable way, simplifying the overall architecture and boosting performance. There are many ways to tackle data needs for a microservices architecture. Because microservices architectures have such a strong following in the open source community, and because Kubernetes was slow to support stateful services such as databases, many architects and developers seem to assume several predicates: Every service in a microservices architecture will have its own data store Across data stores, data will be eventually consistent – not, well, consistently consistent (ie, not meeting ACID guarantees) The application developer will be responsible for managing data across services and for ultimate consistency. But not every microservices thinker is ready to throw out the baby – the valuable role that can be played by a relational database – with the bathwater of a restriction to open source, and usually NoSQL, databases. Kubernetes Operators and Microservices Both the tools available for using databases with microservices, and the thinking that a developer can draw on when considering their options, are evolving. In the area of tools, Kubernetes has developed stateful services support. Additions such as PersistentVolume, PersistentVolumeClaim, and StatefulSet make these services workable. The emergence of Operators in the last few years makes it much, much easier to use Kubernetes for complex apps that include persistent data, as most apps do. (You can see the blog post that introduces, and explains, Operators here .) You can learn how to build an Operator from Red Hat’s OpenShift site . As an example, SingleStore has used Kubernetes to build and manage its, well, managed service, SingleStore Managed Service . After earlier attempts to develop such a service ran into difficulties, Kubernetes, and the development of a SingleStore Kubernetes Operator (in beta) by the team, made it possible for SingleStore to bring Managed Service to market with just a few months of work by a few individuals. With an elastic, on-demand, cloud database as the very definition of a stateful service, this is just one example that the Kubernetes Operator, as well as Kubernetes as a whole, are fully ready for prime time. New Thinking About the State (of Data) Some daring thinkers – in one particular case, at RedHat – have focused on reminding their fellow developers of some of the advantages of a single, central, relational database, long taken for granted: ACID transactions; one place to find and update data, a single thing to manage, and a long history of research and development. The authors then go on to develop a primer on how best to share a relational data store among multiple services. In their sample, they use MySQL as the relational data store. One microservices maven, Chris Richardson, offers both options. He gives robust descriptions of the use of both a database-per-service approach and a shared database approach in microservices development. Microservices maven Chris Richardson describes varied approaches to database access in microservices apps. But RedHat’s reference to MySQL, as a venerable and widely used relational database, incidentally highlights one of the primary objections to the use of legacy relational databases for microservices development: their lack of scalability. Scalability is one of the chief, if not perhaps even the single most important, attributes of a microservices architecture. It’s so important that many microservices developers restrict themselves to NoSQL architectures, which assume scalability as an attribute, simply to avoid having to deal with artificial constraints on things like database size or transaction volume. A Modest Proposal (for Microservices Data) We would like to suggest here that SingleStore is a solid candidate for use as a shared relational database for microservices applications. This choice is not restrictive; specific services can still use local databases, and they can be of any type needed. But for complex operations such as transactions, and even for many incidental operations such as logging users, a relational database which can be shared or sectioned as needed, and used on a database-per-service database when that’s required, and that works well alongside NoSQL data stores, might be a valuable asset. NewSQL databases in general, and SingleStore in particular , have the attributes needed to serve this role, including: Speed . SingleStore is very fast, for ingest, processing, and transaction responsiveness. Scalability . SingleStore retains its speed across arbitrarily large data sizes and concurrency demands, such as application queries. SQL support . Not only is the SQL standard ubiquitous, and therefore convenient, it’s also been long optimized for both speed and reliability. Multiple data types . SingleStore supports an unusually wide range of data types, for a relational, SQL database: relational data, JSON data, time series data, geospatial data, and can import in the AVRO format typically used in Kafka, as well as offering full-text search on data. Transactions plus analytics . SingleStore supports both transactions and analytics in a single database; you simply create rowstore tables for some data and columnstore tables for others. Also, with SingleStore DB 7.0 having reached GA last December, you can now use Universal Storage features to depend more often on just one table type or the other. Cloud-native . SingleStore is truly cloud-native software that runs unchanged on-premises, on public cloud providers, in virtual machines, in containers, and anyplace you can run Linux. SingleStore Managed Service, which is itself built on Kubernetes, offers a managed service, reducing operational cost and complexity. While SingleStore has other features that are beneficial in any context, these are the key features that stand out the most in a microservices environment. Getting Started SingleStore offers free use of the software for an unlimited time, within a generous footprint limit , and with community support, rather than a support restriction. You can typically run an entire proof of concept on SingleStore before you are likely to need to scale up enough to purchase an Enterprise license. So we suggest that you try SingleStore for free , and get started with your next big project, today.", "date": "2020-01-31"},
{"website": "Single-Store", "title": "webinar-recap-2-fast-distributed-synchronous-replication", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/webinar-recap-2-fast-distributed-synchronous-replication/", "abstract": "This is the second part of a two-part blog post; part one is here . The recent release of SingleStore 7.0 has fast replication as one of its major features. With this release, SingleStore offers high-throughput, synchronous replication that, in most cases, only slows SingleStore’s very fast performance by about 10%, compared with asynchronous replication. This is achieved in a very high-performing, distributed, relational database. In this talk, available on YouTube , Rodrigo Gomes describes the high points as to how SingleStore achieved these results. Rodrigo Gomes is a senior engineer in SingleStore’s San Francisco office, specializing in distributed systems, transaction processing, and replication. In this second part of the talk (part one is here ), Rodrigo looks at alternatives for replication, then describes how SingleStore carries it out. Considering Replication Alternatives First, we should define what the goals are. What are the objectives? We have a primary and secondary, as before, two nodes – and we want the secondary to be a logically equivalent copy of the primary. That just means that if I point my workload at the secondary, I should get the same responses as I would on the primary. What is highly desirable is performance. You don’t want replication to be taking a very long time out of your system, and you don’t really want to under-utilize any resources. So this goes hand in hand with performance. At some point you’re going to be bottlenecked on the pipe of the network or the pipe to disk, whichever one is smaller, and if you are under-utilizing those pipes, that means you leave performance on the table, and you can get more. So how would one go about doing this? Here’s what a naive solution would look like. Now the idea is, when your transaction finishes before you make its effect visible – and this is somewhat based on what SingleStore does so it’s not necessarily the only way to do this – but it is kind of a useful abstraction of how one would go about doing it. So you figure out how you’re going to serialize your transaction like the binary format, you write it to disk, you send it to secondaries, and you declare it committed. On the secondary, you’re just receiving from the primary, also persisting it to disk, because you should have your transaction log there and then applying it on the secondary as well. Does anyone know what is wrong with this? Audience: You didn’t wait for the secondary to get back to you. Exactly. So does anyone know why that is a problem? The answer was, you didn’t wait for the secondary to get back to you. So it might not be completely obvious, because you might think that the sends are blocking, but most systems would not do that. They would just say, I’m going to send and forget; right, it’s completely asynchronous. But there we’re not waiting for the secondary to tell us it’s received the transaction. What could go wrong with that? Audience: Because then, if your first one dies, you’ll have corrupted data. Yep. So here’s a helpful diagram I drew in my notebook earlier. So the user sends a transaction, the primary sends it to the secondary, but it just kind of gets lost halfway. The primary says, okay, transaction has committed to the user – and then it goes offline. We promote the secondary to primary so that the workload can keep going. The user asks about its transaction and the secondary just says, “I don’t know, never happened.” All right, so the way we fix it is we wait for acknowledgements. The secondary also has to send it back. Here’s a description of that. Now what is wrong with this one? This one is a little subtler. Audience: This is a typical two-phase commit approach. Right. This is a typical two-phase commit, but there is still something incorrect here. Audience: Now you have a problem with waiting for it – latency. So that is a great point. We can wait forever here. For this presentation, we’re not going to fix that. So we care that there’s a secondary that’s eligible for promotion, that has all the data. But if there is a failure in the secondary, you are never going to get an ACK because it’s gone. Audience: Can’t you just get the ACK after the transaction is stored? Well, but it’s stored in one place, you want it to be stored in two places, right? Audience: Can’t you go from the transaction log and still load the disk, even if it fails? Well, the log is on disk still, but we’re saying we want it on some different machine – because if your primary machine fails, you don’t want the workload to die, and you don’t want to lose data if your primary machine fails forever. What you could do for this is you can have some other system that notices the failure in the secondary – and, in a way, demotes it from being a promotable secondary, and that way you don’t need to wait for ACKs from that secondary anymore. You would lose one copy of the data, but the same system can try to guarantee there is a new copy before it allows that to happen. I’m always talking about a single secondary here, but you could also have multiple copies all ready. You don’t necessarily just have to have one other copy – and, in a way, that allows you to stay available. If you just have something detecting the failure saying, I don’t care about the secondary anymore, I have a copy somewhere else. Let’s keep going. What else is wrong with this? Audience: It looks slow. Can’t we send ACK right after sending to disk? That’s a good observation. Audience: Not before? Before sending to disk, we can’t, because then if the secondary dies right there, your data is gone. We could do it right after sending to disk if all you care about is that the data is persisted, but not necessarily visible – and in this case, we kind of care. But that means when you promote the secondary, you have to add an extra step waiting for the data to become visible so that you can use it. Otherwise you would get the same kind of problem. But at that point today it is persisted, so it will eventually become visible. Audience: Can you send to the secondaries first? I know that’s strange. There is nothing that stops you from doing that. Audience: Then you can have a protection on the other side when the first one fails. But it’s equivalent to having the transaction on the first one and then the other one failing. Right? You could do that. You could also just pretend the disk is one of your secondaries, and basically pretend it’s like a special machine, and now you just treat it the same way you would treat all other secondaries. It’s actually a very useful, simplifying way for coding it, because then you don’t have to replicate a lot of logic. But disks are special; that’s one of the things I’m going to oversimplify here. We’re not going into how disks are special, because it’s awful. They tell you they’ve persisted their data; they haven’t. They tell you they’ve persisted their data – you write something to them, they overwrite your data. You have to be very careful when dealing with disks. There’s something else wrong here, or there’s something… It’s wrong because it makes things very, very hard to code, but it’s weird. So imagine that you have many concurrent transactions running at the same time. How could that break things? Audience: Are the transactions getting read from the secondary, or they all are always persisted by the primary? Let’s assume they’re always processed by the primary. So if you have multiple transactions going at the same time, they can reorder when they send to disk, and when they send to the secondaries. Audience: Because if the first line takes too much, then the write … So you always have the guarantee that the secondary has all the data, but the transaction log is not necessarily the same. This is not necessarily incorrect, because the main goal we stated is that they are logically equivalent, and these two are logically equivalent. They have the same set of transactions in their transaction logs, but things become really tricky if you need to catch up a secondary. So imagine that the primary fails, and you failover to a secondary, and maybe there’s another secondary, and now this secondary also fails, and now you have three different transaction logs. This secondary comes back up, and at this point, you don’t necessarily want to send everything to it from scratch, because that could take a long time. The failures could be temporary, if your restarts are quick, or it could be a network issue that caused you to detect a failure – whereas the node was still online, and maybe the network issue just lasted a second. And now, if you have a lot of data, it could take maybe an hour or more to provision this secondary from scratch. But you know that the secondary at some point was synchronized with you – it had the same transactions; you just want to send the missing transactions. And now the transaction logs are different, though, so you don’t necessarily know what it’s missing compared to you. If the transaction logs were the same, then you know that there is an offset before which everything is the same. That offset is actually not too complicated to compute when things are the same, because you know that if something is committed, if you’ve got an ack for that transaction, then everything before that transaction is the same on every node, if the transaction logs are the same. And you can basically send that offset from the primary to the secondary saying, okay, everything before this has been committed – and you can either do it periodically, and in SingleStore we just do it kind of lazily. When we need to write something new, we just send that value if it’s changed, and so you can use that value as the offset, if everything is the same. If everything is not the same then you may have a different set of transactions before that offset on both. That offset just doesn’t make sense anymore. So how would we go about fixing this? Audience: Okay. Can you just send the last version of the transaction log from the first one to the second one. The second one just grabs that and applies it? Not sure I follow. Audience: Imagine you said you have the transactions happening on one, and you send that transaction, together with data, to the second one. The second one grabs that data and persists both the transaction and the data itself. Right, but- Audience: What if you sent the transaction log already computed – the transaction log – and the second one just… So you always send a valid copy of the transaction log. Oh, so I guess the idea is that, instead of sending when a transaction commits, you just send the log as you write to it. So you could do that. That becomes very slow, because you basically have to wait for the log to be written up to where you wrote to it. And that’s not necessarily going to be fast. You can imagine you have a transaction that’s going to take an hour to write to the log, and then another transaction that’s going to write to the log next, and that’s going to be a bit slow. In a way, it’s a reasonable solution; it’s sort of what this is doing. Basically, put a lock in there so that you send to disk and to secondaries at the same time, and you’re just going to send as you write to the log. The difference is the transactions can write to disk in that solution, all at the same time, and then only later does it send to the secondaries. But you still have to wait for that to happen. So I kind of spoiled it. One of the big issues here is that it’s going to be pretty slow. I drew a fun graphic here. So you can imagine you have a bunch of transactions all doing work, all doing progress at the same time, and then they’re all going to compete on the same lock. And this is not necessarily the exact way it’s going to happen, but you can imagine that if you read it just like here you’re going to send to disk first, and then send to the secondaries. So while you’re waiting on the network, your disk is empty, or it’s not doing anything. While you’re waiting on disk the network is not doing anything. You could do these in parallel so you could like push this back, but there’s always going to be one that’s slower, either disk or network; and while one is executing, the other one is waiting for it. That can be a pretty big performance bottleneck and that was actually something that could sort of happen on older SingleStore versions. So can anyone tell me what they would do to fix this? To avoid this bottleneck? Audience: Generate, like, sequence numbers, since it’s the primary that processes all and the transactions and generates the sequence number for each one. Then send without logging to the secondaries, and the secondaries need to always know what was the last one that they wrote, and check if what they receive is the next one. Or if they should wait for… If they are receiving what was one, they receive three, so they know that two should arrive. Audience: So basically moving the lock to the secondary. It’s the same performance? Not quite the same performance. Audience: The way it writes on the log. So the suggested idea is, each transaction has a sequence number, and you write them in that order to the log, but you have to do it locally as well – which can also become a bottleneck, but you don’t need to lock around the sending over the network. And the secondary just has to make sure that it writes them in the same order. That has the problem that it would still kind of lock on the disk, right? So you’d still be all waiting for this; each transaction has to wait on every other transaction to write to disk before it can send. So I think it’s still isn’t ideal, but it’s close, it’s very close. Audience: What about the difference between that and also using a random generated number. So something like three options that just seem like the random generator and ID like in one, adjusting the one the three, and then two. You know already they are representing the same transaction but… just get the order correctly at the end on the second node. I’m not sure I follow. So the idea is to add the random number generator- Audience: To represent the transaction- So each transaction has a unique ID basically. Audience: Basically, but the transactions also could be represented by, let’s say, the three actions, like write, write, write. So you don’t have to wait on the disk; you persist with the the serial… but an order could be 3-1-2 for instance. But when you’re collecting information from the transaction log, we correct… The persistence on this should be… So you persisted out of order but then you reorder them at the end? Audience: Yeah. That has the same issue as before, where you don’t necessarily have an offset to send things after. The problem from before was that computing where to start catching up, in a new secondary, is very hard, if things are ordered differently on different nodes. I think that would still have the same problem. Audience: Is this solution like sacrificing the secondary start-up time? No, you shouldn’t have to sacrifice the start up time. I’m going to count to five to see if anyone has any other ideas. One – Audience: I think you can afford to have some rollback when you have conflicts, and if you have at least one piece that provides you the commits, to have some kind of eventual consistency. On top of that, you can have different groups of primary backups. And you can at least attenuate or diminish a little bit that lock. It won’t prevent it, but you’ll at least reduce it. So, different groups of primary backups, according to your workload? You could do some kind of sharding also. You could do sharding, and it does help, because now you have some parallelism for different locks, but we still want it to be faster per shard. How the Revamped Replication System in SingleStore 7.0 Works So SingleStore does do sharding, and there were some debates we had over whether it’s a good idea to make replication much faster, because you still have parallelism across shards. We found that it was still a very good idea to do it, to make it really fast. All right, one, two, three, four, five. Okay. It’s very similar to the sequence number ideas suggested initially. So this is one of the optimizations we implemented in Replog. There are a few others, but this is the one I’m going to go into more detail in here. Where you still allow sending to secondaries out of order, but you don’t just send the transaction content, you also send the offset, and the log where it’s written, and the way we do that… So you have to be very careful when you do that, because you can’t just write to the disk – otherwise, if you have two concurrent writes, they might just write on the same offset. What we do is we have a counter for each shard that says where we’re at on the disk, and every time we want to write a new transaction to the log, we increment that counter by the size of the transaction, and the current value of the counter gives us the current offsets, and you can do the incrementing at an atomic manner without using locks. The hardware is sort of using locks but it’s pretty fast and then you have your offset and then you just have to… You know that no other thread, no other transaction, is going to touch your reserved portion of the disk, and so when you write to disk, there are no locks there. The socket still sort of has a lock, it doesn’t even need to be an explicit lock. Like when you call send on the socket, it will go to the OS, and the OS will just send everything serially. But at that point you just send the offset, and the transaction log content, and you’re able to effectively max out the network pipe at least. Audience: Don’t you have an empty disk space at this portion if you reserve the offset, but then it fails? That is a great observation. Yes. So one of the great challenges of doing something like this is that, if something fails, you have portions of the disk that are empty, and now you have to be able to distinguish those from non-empty portions. And you also have to deal with those when trying to figure out where to do the catch-up. For the catch-up part, it’s not actually very hard, we do basically the same thing as before. Because we have something in the background in our replication system that knows an underestimate of everything below this offset is already committed. And so you know that everything below that offset doesn’t have any gaps, and so it is safe to use that offset to start catching up. Above that offset, the challenging part is: suppose that your primary fails, and you have a secondary that has a hole, you have to know how to identify that hole. And what we do is – actually, there’s a lot more that we do that’s not here – but we basically have a checksum for each transaction block that allows us to distinguish between something that was written. So we just read the entire log and check against the checksum and something that’s a hole – or we call them, torn pages – and that allows you to kind of skip it. And we can basically overwrite other secondaries’ logs with that hole, and they know to skip it as well, if they need to recover. Audience: If you’re applying the log right after writing the log to disk – if you have a gap, you’re basically applying the transaction in the wrong order. Not necessarily. That’s actually a great question. Audience: Unless they are commutative. Right. So the question is, if you have a gap, and actually if you’re just replicating out order, and you’re applying right when you received the data, you might be applying things out of order – and that is true. You’re not applying things in the same order as in your primary necessarily, but they are going to necessarily be commutative, because you can’t have dependencies on a transaction that is going to commit by another transaction. If you have a hole, that means that everything in that hole or that gap is not committed yet necessarily, because you haven’t acknowledged it. So if you’re going to apply something after the gap that you’ve already received, that means that the two transactions are running concurrently, and are committing at the same time. And if that is true, then they will not conflict, because they both validated their commits at the same time. Audience: Oh, when you reach this point, you already validated… so they are commutative. So in SingleStore, we actually use pessimistic concurrency control, which means that they just have locks on everything they are going to write, which is how we can guarantee at that point they’re not going to conflict. You could have other mechanisms, with optimistic concurrency control, where you just scan your write set. If we did that at that point, we would already have validated. Audience: You forgot to mention what is the isolation level provided by SingleStore. So today it is pretty bad; it’s read committed. Read committed isolation level just guarantees that you’ll never read data that was never committed, but it’s pretty weak. Because you might read something in the middle of two transactions – for example, you might see half a transaction, and half another. That’s actually what I’m going to be working on at SingleStore for the next few months, increasing the isolation levels we support. So pretty exciting stuff. The main reason we don’t have higher isolation levels right now is that, due to the distributed nature of the system, everything is sharded. We don’t have a way to tell between two shards of the data, that two rows were committed in the same transaction, right now. And so within one shard we actually do support higher isolation levels, that you can do snapshot reads, which guarantees you that you don’t ever see half a transaction. And it’s fairly easy to build on top of that for snapshot isolation or even read committed when performance. But we are working, for the next major release of SingleStore in a few months, to have higher isolation levels, hopefully. We are all engineers here, we all know how planning can go. How am I doing on time? Audience: Good. We are okay. So does anyone have questions about this? Audience: The only kind of weird question I have is it feels weird to not write to disk continuously. I’m not sure if that’s actually a problem. When you say continuously, you mean like always appending, right? Audience: Yes, always appending, because it’s weird to go back in offsets and writing to disk. It may feel weird, but it’s actually not a problem, because you kind of still write continuously when you reserve here. So one way to think about it is when you’re writing to a file, if you’re appending at the end, you’re actually doing two operations. You are moving the size up so there’s a metadata change on the file system and you’re also writing all the pages to disk. And what we’re doing is moving the size change into the transaction. So these two operations together are sort of equivalent to writing at the end of the file in the file system because it’s still two operations. That’s actually a tricky thing, because if you’re doing synchronous writes to a file, and your file is not a fixed size, and you’re moving the size up, your writes might go through and tell you they’re persisted, but the metadata change could be independent. And so you have to either pay a very high cost to make sure that the metadata change goes through, and it’s persisted, or you can pre-size files, which is what we do. But that has the separate costs that now you have to keep track of the ends of your log separately, and manage that yourself. So you still need that metadata somewhere. We just thought we could do better. I think we do better. Any other questions? Right. That is it for the presentation. We are hiring in Lisbon if anyone’s interested in working on interesting problems like this one. We also have other teams that work on different types of problems – like front-end, back-end systems management. And if you have any questions about this presentation, the work we do, or if you’re interested, we have a bunch of people from SingleStore here – right now, and later. Thank you. Audience: Okay. Thank you Rodrigo. Conclusion This is the conclusion of the second, and final, part of this webinar recap. You can see part one of this talk; view the talk described in these two blog posts; detailed description of fast, synchronous replication in SingleStore 7.0 in another technical blog post ; and read this description of a technically oriented webinar on SingleStore 7.0. If you want to experience SingleStore yourself, please try SingleStore for free or contact our sales team .", "date": "2020-02-08"},
{"website": "Single-Store", "title": "webinar-recap-1-fast-distributed-synchronous-replication", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/webinar-recap-1-fast-distributed-synchronous-replication/", "abstract": "This is the first part of a two-part blog post; part two is here . The recent release of SingleStore DB 7.0 has fast replication as one of its major features. With this release, SingleStore offers high-throughput, synchronous replication that, in most cases, only slows SingleStore’s very fast performance by about 10%, compared with asynchronous replication. This is achieved in a very high-performing, distributed, relational database. In this talk, available on YouTube , Rodrigo Gomes describes the high points as to how SingleStore achieved these results. Rodrigo Gomes is a senior engineer in SingleStore’s San Francisco office, specializing in distributed systems, transaction processing, and replication. In this first part of the talk (part two is here ), Rodrigo describes SingleStore’s underlying architecture, then starts his discussion of replication in SingleStore. Introduction to SingleStore Clusters Okay. So before I start, I’m just going to give some context and a quick intro to SingleStore. This is a rough sketch of what a SingleStore cluster looks like. I’m not going to go into all of this. We’re going to explore a very small part of this; specifically, replication. But to motivate why we need it, basically we have a database here, and that database is sharded into multiple partitions, and each partition has redundancy. And we do that so that we can provide durability and availability to our partitions. So if there are failures, you don’t lose your data; your transactions don’t go away. And today I’m going to talk about redundancy and replication within a single shard ( of the database – Ed .). Focusing on Transactions So a quick intro, I’m going to talk about transactions. Transactions are what your application uses to talk with a database. So if you build an application, what it’s doing is it’s sending read transactions to the database to see what’s the status and also updates or writes or deletes to make modifications to that state. And you want a bunch of very nice properties from transactions so that writing applications is easy. For today, the one we care about is that transactions are durable. What that means is that when the system tells you your transaction has committed, if you wrote something to the database – if you made a change to the state – and something fails, that change is not magically going to go away. And the way you maintain durability, in most systems, is by doing two things: Maintaining a transaction log Replication The first thing most systems do is they have a transaction log. What the transaction log allows you to do is persist to disk the binary format of your transactions. So you can imagine that you’re making changes to the database. You say, “Write the number 10 on my database.” And the way it goes about doing this is it first will persist that to disk before telling you that it’s committed or necessarily even making those changes visible. And when the disk tells you that the number 10 is going to be committed, you tell the user that it’s committed. This is an oversimplification, and a lot of this presentation is going to have oversimplifications, because replication, transactions, and durability is a fairly hard topic. I’ll try to note what oversimplifications I use, so that if you have any questions during the coffee break, I am very happy to talk about it. But basically, transaction logging is what you use so that if you crash when you restart, you have a log of your transactions, and the user never hears that a transaction is committed before it’s persisted to the log. Some systems actually write the “how to undo the transaction” part first. SingleStore has one simplifying factor, which is all the mutable state actually exists in memory, so you never need to undo anything. Systems that write to disk need to undo the changes because they might crash in the middle of doing them, but in memory we just lose everything and we just replay the redo log. So the problem is just how to apply them. Replication and SingleStore DB 7.0 The other way we maintain durability is with replication. This doesn’t give you just durability, this also gives you availability. The idea is that you never really want your data to live only on one machine. You want it to live on multiple machines, because you might lose one machine – and, even if it’s temporary, then your system is completely offline. So if it lives on some other machine, you can make it the new go-to machine for this piece of data. Also, you might lose one machine forever. Imagine that – I don’t know – one of the racks in your data center suddenly went into a volcano, of its own will, and now it’s not coming back. But at least you always have another machine – hopefully, on another rack – that has all your data, and you just keep going. The type of replication I’m going to be talking about today is primary-secondary. (There are others, consensus being one of the other popular ones.) The idea with primary-secondary replication is that there’s one node that is the primary, and that’s what the user interacts with. It doesn’t necessarily interact with the primary directly. An example is, in the SingleStore cluster we have an indirection layer, because the data is sharded. So we have one node that’s responsible to know where things go. But, for this presentation, you can assume that it’s the user interacting directly. In SingleStore those would be the nodes we call aggregators, but they are kind of users of this copy. And then the primary node sends over the network, to the other nodes, the transaction log. There are other kinds of replications. You can do statement replication, where you’re sending the statements the user sends you. Which is still kind of a transaction log, but we use physical replication and physical durability, which means that we actually persist all the effects of a transaction onto disk. That allows us to actually do some more interesting things. Because with statements, the order of the statements matters a lot, whereas with transaction changes, the order doesn’t necessarily matter as much. So you just persist all the effects, and we just send over the network such that the secondary always has a logically equivalent copy. So I’ve been working on this for longer than I care to admit – or than anyone else should know, outside of SingleStore – but we just shipped a revamped replication system in SingleStore DB 7. I’m not going to describe everything we’ve done, there’s like 50,000 lines of code there. There’s not enough time in this day to describe everything that goes into it, but I will go through how you would build a replication system of your own. And I’m also going to go in some detail into one of the optimizations we made to make our new replication system very fast. Again, be warned there’s going to be a lot of oversimplification. I’m going to gloss over things like failures. I’m pretty sure about two-thirds of those lines are failure handling, so that things don’t blow up, but let’s go into it. Conclusion – Part 1 This is the end of Part 1 of this webinar recap. You can see Part 2 ; view the talk described in this blog post; read a detailed description of fast, synchronous replication in SingleStore DB 7.0 in another technical blog post ; and read this description of a technically oriented webinar on SingleStore DB 7.0. If you want to experience SingleStore yourself, please try SingleStore for free or contact our sales team .", "date": "2020-02-08"},
{"website": "Single-Store", "title": "find-and-fix-problems-fast-with-memsql-tools", "author": ["Roxanna Pourzand"], "link": "https://www.singlestore.com/blog/find-and-fix-problems-fast-with-memsql-tools/", "abstract": "SingleStore Tools is a new set of command line programs for managing the clusters of servers or instances that make up your database.  You can use SingleStore Tools to help you find and fix problems with SingleStore quickly and incisively. Our legacy management tool, SingleStore-Ops, generated cluster reports, with output logs and diagnostics per cluster. For efficiency, the SingleStore team developed an internal tool called ClusteRx, to parse the reports coming out of SingleStore-Ops. We are now sharing this functionality with our customers as two new commands in SingleStore Tools, memsql-report collect and memsql-report check . Read on to learn how these tools can help you find and fix problems fast. At SingleStore, we are continuously working to enhance the monitoring and troubleshooting tools for the database, and aiming to create an engine that is fully self-healing. While this is the end goal we are striving towards, it is not a small feat. Today, it is often beneficial to understand the point-in-time health of the SingleStore system (the host, the nodes, and the database itself). These types of health assessments are most useful when you are troubleshooting an ongoing issue that does not have a clear root cause. Specifically, these health checks can be useful in cases where a user may need some hints or indicators that will give them direction on where and how to start investigating a particular symptom. Perhaps you notice that some of your application queries are failing intermittently, but there is no obvious culprit. Where do you start troubleshooting the issue? You might go through a slew of system checks, but you aren’t sure if you’re simply chasing the white rabbit. How do you narrow down the problem? We’ll get back to this example shortly… Past Health Checks at SingleStore SingleStore has a legacy management tool called SingleStore-Ops , which runs in conjunction with the database. SingleStore-Ops has the ability to generate what we call a cluster report, which output logs and other diagnostics on a given cluster. It’s an informative report – but it can be challenging to navigate if you don’t know where to start your investigation. It’s a classic ‘finding a needle in a haystack’ problem. When a customer filed a customer support ticket, the SingleStore support team typically requested that they run this report, and the support team then used the report to help fix problems. (If you have filed a case with our teams, you are probably very familiar with how to collect a report). Over time, the SingleStore support team learned what critical data points in these dense cluster reports offer the most insight to nail down or troubleshoot an issue. This feedback loop led to an internal tool that was developed by the support team, called ‘ClusteRx’ (notice the ‘Rx’ health pun!). ClusteRx parses the cluster report and provides an output that notifies the support team of various pass/fail checks that can be used as indicators on where the root cause of a particular problem may lie. (We will provide a practical example a little later on in the article). Making the Internal Tool External This internal tool, developed by our support team, became so useful for helping to troubleshoot a cluster experiencing problems that we decided to make it available to our customers. This is very exciting, because making this feature available to our customers enables them to troubleshoot SingleStore without assistance, and it also ensures they are equipped with the information and tools they need to manage their SingleStore cluster successfully. Fast forward to the present day: We redesigned our entire management tool into a new framework called SingleStore Tools , which replaces SingleStore-Ops. We took the lessons learned from the internal ClusteRx tool for point-in-time health assessments that SingleStore support and engineering iterated on together, and we applied them to new functionality within SingleStore Tools, called memsql-report collect and memsql-report check. What the New Tool Does The newly redesigned version of this tool does the following: memsql-report collect gathers the diagnostic report from the given cluster. memsql-report check runs various pass/fail checkers on the report and outputs the result to the user, highlighting actionable tasks if a certain check failed. As of this blog post, memsql-report check has 55 different checkers , and we are continuously developing more. Below are a few examples of the checkers in memsql-report check: outOfMemory – reports on any recent out of memory failures. leavesNotOnline – provides information on SingleStore leaves that are offline in the cluster. userDatabaseRedundancy – confirms your data is properly replicated across the cluster. defaultVariables – checks certain SingleStore system variables and ensures they are set to recommended values. SingleStore documentation describes the 55 different checkers for memsql-report check . The Real-Life Problem Back to the real-life user example that we introduced at the beginning of this article… A user noticed that a subset of their application query workload was failing, but there were no leading indicators as to why. How can we use memsql-report check to help us? Some definitions, before we go any further: A SingleStore aggregator is a cluster-aware query router. It is responsible for sending all the queries to the nodes that have the data on them (SingleStore leaf nodes). The ready queue is a queue of all processes that are waiting to be scheduled on a core. The customer with this issue filed a ticket with support, and support instructed them to run memsql-report collect and memsql-report check on their cluster. Using the output of memsql-report check , the customer immediately detected that the ready queue was saturated on one of the aggregators. Each query that runs on a cluster – including internal queries used by the nodes to communicate with each other – requires exactly one thread on an aggregator; the ready queue saturation message means that the maximum number of connections allowed on that aggregator has been reached. Saturation of the ready queue typically means that queries will be queued on that aggregator, and depending on your application logic, it can lead to timeouts and failures. This explains why some, but not all, queries were failing in the customer’s application. Tying this back to SingleStore’s report-check functionality, the customer was able to identify that the ready queue was saturated by looking at one particular checker that stood out, the readyQueueSaturated checker. Here is the example output on the ready queue that piqued our interest: readyQueueSaturated [FAIL]\n\nThe ready queue has not decreased on 10.22.182.7 The customer shared the output of the checker with us, and we focused on the aggregator that exhibited this failure (10.22.182.7) and identified in the processlist that there were about 50 open connections on this aggregator. Hmm. This finding was puzzling to our team because SingleStore aggregators are typically meant to handle more than 50 concurrent connections at a time. So, why were queries failing? The Real-Life Solution It turns out memsql-report check clued the customer in on another issue, which they brought to our attention. The aggregator was configured to only have 50 connection threads ( max _ connection _ threads ) open at once. The max _ connection _ threads setting on an aggregator is essentially a limit on the number of queries – including internal SingleStore queries – the aggregator will run simultaneously. The value recommended for aggregator connection threads is 192, so this aggregator was configured to service almost four times fewer connections than it was supposed to! readyQueueSaturated [FAIL]\n\nThe ready queue has not decreased on node 10.22.182.7\n\nWarn: Value of max_connection_threads 50 doesn't follow the recommended value 192 for node 10.22.182.7 As soon as the customer updated the value for max _ connection _ threads to the recommended level, 192, the issue was resolved. Without report-check, it would have taken a lot of investigation time to get to the bottom of the issue. With this tool, you can find problems in minutes that could have taken hours otherwise. For example, in this case, the customer would have had to check the logs for every node to identify that the ready queue was saturated on a given aggregator. Furthermore, the user would have had to check each nodes’ setting for max _ connection _ threads to find the misconfiguration. Both of these could have taken a significant amount of time, especially with a large SingleStore cluster. Trying It Yourself This scenario is one of many examples of how useful memsql-report check can be for quickly identifying and mitigating issues and limiting break-to-fix times. Many of our customers also use this tool after they deploy SingleStore, before they do upgrades, maintenance, or other significant activities, as a sanity check to confirm their cluster is in good shape. If you haven’t used memsql-report check , you should check it out. We encourage our customers to use this to troubleshoot issues on their own! And, if you’re still stuck, reach out. Your efforts will help SingleStore Support to help you, faster and more effectively. (If you’re not yet a paying customer, your efforts will help you find help on the SingleStore Forums.) Looking ahead, we want to expand this tool so that customers can do system checks to validate their environment before they install SingleStore, including validating performance on their hardware. Additionally, we want to incorporate some of the applicable health checks into the database engine directly. If you have feedback on what we should add to the checker, please post in the SingleStore Forums . And see the SingleStore documentation for a full list of all the checkers . If you haven’t done so yet, you can try SingleStore for free , or contact SingleStore today. memsql-report check Example Below is an example of output for memsql-report check . I suggest that, when using it, you look at all the checks and review the ones that failed. For example, for the memory check failure, this user allocated a large percentage more memory to the database than is actually available on their machines. In this case, I would adjust maximum _ memory on all my nodes to ensure my cluster is within physical memory limits. ✘ maxMemorySettings ……………………….. [ FAIL ] FAIL total maximum _ memory of all nodes on 127.0.0.1 too high (180% of RAM) [ 7106/3947 ] bash-4.2$ memsql-report check –report-path /home/memsql/report-2019-12-20T012005.tar.gz ✓ explainRebalancePartitionsChecker …………. [ PASS ] ✓ queuedQueries …………………………… [ PASS ] ✘ defaultVariables ………………………… [ WARN ] WARN internal _ keepalive _ timeout is 90 on F91D002A777E0EB9A8C8622EC513DA6F0D359C4A (expected: 99) WARN internal _ keepalive _ timeout is 90 on 5E8C86A7D53EFE278FD70499683041D4968F3356 (expected: 99) ✓ memsqlVersions ………………………….. [ PASS ] NOTE version 6.8.13 running on all nodes ✘ vmOvercommit ……………………………. [ WARN ] WARN vm.overcommit _ memory = 1 on 127.0.0.1. The Linux kernel will always overcommit memory, and never check if enough memory is available. This increases the risk of out-of-memory situations ✘ maxMemorySettings ……………………….. [ FAIL ] FAIL total maximum _ memory of all nodes on 127.0.0.1 too high (180% of RAM) [ 7106/3947 ] ✓ leafAverageRoundtripLatency ………………. [ PASS ] ✘ detectCrashStackTraces …………………… [ WARN ] WARN data from SingleStoreStacks collector unavailable on host 127.0.0.1: /tmp/memsql-report656183805/127.0.0.1-MA-LEAF/memsqlStacks.files.json not found ✓ failedBackgroundThreadAllocations …………. [ PASS ] ✓ columnstoreSegmentRows …………………… [ PASS ] NOTE columnstore _ segment _ rows = 1024000 on all nodes ✓ tracelogOOD …………………………….. [ PASS ] ✓ runningBackup …………………………… [ PASS ] ✓ interpreterMode …………………………. [ PASS ] NOTE interpreter mode INTERPRET _ FIRST found on all nodes ✘ userDatabaseRedundancy …………………… [ WARN ] WARN this cluster is not configured for high availabililty ✘ maxOpenFiles ……………………………. [ WARN ] WARN fs.file-max = 524288 might be low on 127.0.0.1, recommended minimum is 1024000 WARN open files ulimit (1048576) is set higher than fs.file-max value (524288) on 127.0.0.1 ✓ offlineAggregators ………………………. [ PASS ] ✘ outOfMemory …………………………….. [ WARN ] WARN dmesg unavailable on host 127.0.0.1: error running command: ` ”/usr/bin/dmesg” ` : exit status 1 ✓ kernelVersions ………………………….. [ PASS ] NOTE 4.9 on all ✓ replicationPausedDatabases ……………….. [ PASS ] ✓ mallocActiveMemory ………………………. [ PASS ] ✓ missingClusterDb ………………………… [ PASS ] ✓ failedCodegen …………………………… [ PASS ] ✘ numaConfiguration ……………………….. [ WARN ] WARN NUMA nodes unavailable on host 127.0.0.1: exec: “numactl”: executable file not found in $PATH ✓ duplicatePartitionDatabase ……………….. [ PASS ] ✓ tracelogOOM …………………………….. [ PASS ] ✘ transparentHugepage ……………………… [ FAIL ] FAIL /sys/kernel/mm/transparent _ hugepage/enabled is [ madvise ] on 127.0.0.1 FAIL /sys/kernel/mm/transparent _ hugepage/defrag is [ madvise ] on 127.0.0.1 NOTE https://docs.singlestore.com/memsql-report-redir/transparent-hugepage ✓ defunctProcesses ………………………… [ PASS ] ✓ orphanDatabases …………………………. [ PASS ] ✓ pendingDatabases ………………………… [ PASS ] ✓ orphanTables ……………………………. [ PASS ] ✓ leafPairs ………………………………. [ PASS ] NOTE redundancy _ level = 1 ✓ unrecoverableDatabases …………………… [ PASS ] ✓ unkillableQueries ……………………….. [ PASS ] ✓ versionHashes …………………………… [ PASS ] ✓ filesystemType ………………………….. [ PASS ] ✓ runningAlterOrTruncate …………………… [ PASS ] ✓ leavesNotOnline …………………………. [ PASS ] ✘ maxMapCount …………………………….. [ FAIL ] FAIL vm.max _ map _ count = 262144 too low on 127.0.0.1 NOTE https://docs.singlestore.com/memsql-report-redir/configure-linux-vm-settings ✓ defaultWorkloadManagement ………………… [ PASS ] ✓ longRunningQueries ………………………. [ PASS ] ✓ diskUsage ………………………………. [ PASS ] ✓ validLicense ……………………………. [ PASS ] NOTE you are using 32.0 GB out of 320.0 GB cluster capacity (licensed: 320.0 GB) NOTE License expires on 2020-01-31 08:00:00 UTC ✓ cpuFeatures …………………………….. [ PASS ] ✓ unmappedMasterPartitions …………………. [ PASS ] ✓ blockedQueries ………………………….. [ PASS ] ✓ orchestratorProcesses ……………………. [ PASS ] ✓ delayedThreadLaunches ……………………. [ PASS ] ✓ disconnectedReplicationReplicas …………….. [ PASS ] ✓ minFreeKbytes …………………………… [ PASS ] ✓ cpuModel ……………………………….. [ PASS ] NOTE Intel(R) Core(TM) i7-6567U CPU @ 3.30GHz on all ✓ readyQueueSaturated ……………………… [ PASS ] ✓ failureDetectionOn ………………………. [ PASS ] ✓ collectionErrors ………………………… [ PASS ] ✓ secondaryDatabases ………………………. [ PASS ] Some checks failed: 44 PASS, 7 WARN, 3 FAIL", "date": "2020-01-29"},
{"website": "Single-Store", "title": "spin-up-a-memsql-cluster-on-vagrant-in-10-minutes", "author": ["Rob Richardson"], "link": "https://www.singlestore.com/blog/spin-up-a-memsql-cluster-on-vagrant-in-10-minutes/", "abstract": "This blog post is part of a series describing how to run an entire SingleStore cluster – which usually requires at least five servers or machine instances – on your laptop, in a Linux virtual machine (VM) provisioned by Vagrant, a tool for managing virtual machinres. That’s right – even though SingleStore is a distributed system, you can run a minimal version of SingleStore on your laptop, in a single Linux VM, with a tool you may already know how to use. We tell you how in this blog post. You also have the option of running SingleStore on your laptop in other environments – see our blog posts for Docker Desktop , Kubernetes , and Linux (without Vagrant). You should use the one you have more experience with, or is more compatible with your work environment. Whichever method you use, the combination of free access, and being able to run SingleStore on your laptop, can be extremely convenient for demos, software testing, developer productivity, and general fooling around. In this post we’ll quickly build a single-instance SingleStore cluster running on Linux, in a VM provisioned by Vagrant, on a laptop computer, for free. You’ll need a machine with at least 8GB RAM and four CPUs. This is ideal for quickly provisioning a system that will help you test and understand the capabilities of the SQL engine. Everything we build today will be running on your machine, and we’ll not need to install or configure much to get it running. The steps here are: get a free SingleStore license; install Vagrant and virtualization software; install SingleStore engine and tools; provision SingleStore as a cluster-in-a-box; browse to SingleStore Studio; and create a database. You’ll have a bare-bones SingleStore cluster running on your laptop machine in no time. Why Vagrant and Why a Single VM? Virtual Machines (VMs) are a great way to run software in a protected sandbox and easy-to-manage environment, with less overhead than a dedicated server, and less ceremony than containers. You can use VMs to spin up applications and systems to try out software — even from a Mac or Windows machine, or to quickly spin up a database to support local application development. We’ll use a Linux VM to provision and spin up a free SingleStore cluster, and just as easily, destroy it when we’re done. Using Vagrant makes it easy to provision a fresh VM provisioned exactly the way you want. A Vagrantfile specifies the exact details of the VM and the initialization scripts to run. When Vagrant finishes instantiating the machine, everything is provisioned exactly as you expect. We can pause (halt) and resume machines to continue work, or destroy and recreate VMs to ensure we have the latest versions running in the VM. In production we would likely use Terraform for this level of automation, but when running locally, Vagrant is a simpler, developer-friendly tool. SingleStore’s cluster-in-a-box configuration that we’ll use here includes an aggregator node and a leaf node running on a single machine. We’ll add SingleStore Studio (our browser-based SQL editor and database maintenance tool), all running in one place, all configured to work together. The minimal hardware footprint wouldn’t be nearly enough for production workloads, but it allows us to quickly spin up a cluster, connect it to our project, and try things out. There are other options for running SingleStore’s cluster-in-a-box setup. We could use containers running on Kubernetes or Docker Desktop . If your production cluster runs on Linux machines or VMs, taking the approach in this post helps add parity between production and development environments. If production is running in Kubernetes, you may prefer running SingleStore on Kubernetes . If you use containers without Kubernetes, you may find running the SingleStore container with a docker-compose.yaml file easier. You can also run SingleStore in a Linux virtual machine . With the single-machine SingleStore cluster described here, you can craft the simplest of tables all the way up to running a complex app, a dashboard, a machine learning model, streaming ingest from Kafka or Spark, or anything else you can think of against SingleStore. You’ll quickly understand the methodology and features of SingleStore, and you can plan accordingly for real-world deployments. In this post, we’ll disable SingleStore’s minimum hardware specs, but you’ll still want a machine with at least 8 GB RAM and four CPUs. With specs well below SingleStore’s limits , you’ll see poor performance, so this system is definitely not the right setup for a proof of concept (PoC). But you can use this setup to experience the full features of SingleStore, and understand how it applies to your business problems. Once you know how SingleStore works, you can take these experiments and use what you learn to help you achieve your service-level agreements (SLAs) on distributed clusters that meets SingleStore’s minimum requirements and your high-availability needs. That’s when you can really open up the throttle, learn how your data performs on SingleStore, and dial in system performance for your production workloads. Cluster-in-a-box Multi-node Cluster Hardware 💻 Laptop computer 🖥️🖥️🖥️ Many hefty machines Best use-case - Try out SingleStore -   Test SingleStore capabilities -   Prototypes - Proof of concept (PoC) -   Production workloads -   High availability -   Performance testing Cost Free up to four nodes with 32GB RAM each, and with community support Free up to four nodes with 32GB RAM each, and with community support Sign Up For SingleStore To get a free license for SingleStore, register at singlestore.com/free-software/ and click the link in the confirmation email. Then go to the SingleStore customer portal and login. Click “Licenses” and you’ll see your license for running SingleStore for free. This license never expires, and is good for clusters up to four machines and up to 128GB of combined RAM. This is not the license you’ll want for a production cluster, but it’s great for these “kick the tires” scenarios. Note this license key. We’ll need to copy/paste it into place later. Install Vagrant and Virtual Box The first step in getting our SingleStore cluster running is to get Vagrant to provision a Linux Virtual Machine. Vagrant uses VirtualBox by default, but can easily be configured to use other hypervisors as well. On Windows, Hyper-V conflicts with other virtualization technology, so use Hyper-V, and don’t install VirtualBox. Head to Vagrant’s downloads page and download the version of Vagrant for your system, and install it. If you’ve chosen a vm provider not included in the install, install the 3rd party provider too. If you’re on Mac or don’t have Hyper-V, go to https://www.virtualbox.org/wiki/Downloads and download VirtualBox for your platform. Open the installer and follow the prompts. If you’re on Windows and already have Hyper-V installed, you can skip this step. Build the Vagrantfile The Vagrantfile tells Vagrant how to provision the virtual machine. Though we could provision the VM and type all these commands, putting them in a Vagrantfile allows the tool to do this for us. Create an empty directory, and create a file named Vagrantfile (not Vagrantfile.txt) in the empty folder. Add this content to the file: In this file, we’re using the generic/ubuntu1904 box available from https://app.vagrantup.com/boxes/search I’ve chosen this box because there’s a version of this VM for many providers.  We’ve also configured the VM to use 4 CPUs and 4 gigs RAM. This definitely isn’t enough for a production cluster, but will be great for a developer setup. If you’re on Windows and using Hyper-V, change the provider section to this: # set the provider\n  config.vm.provider \"hyperv\"\n  # configure the provider\n  config.vm.provider \"hyperv\" do |v|\n    v.cpus = 4\n    v.memory = 4096\n    v.maxmemory = 4096\n    v.enable_virtualization_extensions = true # hyperv only\n  end The Vagrantfile references two additional configuration files that will do the heavy lifting of installing SingleStore and starting the cluster.  One is run as root, the other is run as a regular user. Let’s create these two files in the same folder as the Vagrantfile:\\ provision.sh: This file runs as root. It configures apt to connect to the SingleStore repository, then installs memsql-toolbox, memsql-client, and memsql-studio. Finally it starts memsql-studio as a service. We’ll come back to what’s in each of these packages. For the next file, we’ll need the license key. Go to the customer portal, switch to the license tab, and copy your license key. It’s really long, and likely ends in ==.\\ start.sh: Set your license key and admin password into place. Because we’ve embedded these secrets in this file, it’s not appropriate to check this file into source control. Though it is possible to pass parameters into Vagrant as you provision a VM, and then use the parameters in a Vagrantfile, in this developer-centric workflow, that’s unnecessary complexity. With Vagrant installed, and the Vagrantfile and supporting scripts in place, we’re ready to let Vagrant provision our VM. Open a terminal in the directory with all the files. On Windows, run the command prompt as Administrator. Then run this: vagrant up On Windows using Hyper-V, it will first prompt you for the network interface to use. Choose a network interface with internet access so that it can install the SingleStore packages. It’ll also ask for your Windows credentials so it can create a file share to get the provision.sh and start.sh files into the VM. The output will begin with details like this: Bringing machine 'default' up with 'virtualbox' provider...\n==> default: Importing a Virtualbox instance\n    default: Creating and registering the VM...\n    default: Successfully imported VM\n    default: Configuring the VM...\n==> default: Starting the machine...\n==> default: Waiting for the machine to report its IP address...\n    default: Timeout: 120 seconds\n    default: IP: 192.168.184.164\n==> default: Waiting for machine to boot. This may take a few minutes... If you get an error about a port already in use, open up Vagrantfile, find the port_forward line, and adjust the host port. Perhaps choose port 8081 instead of 8080 for example. The guest port will stay as is. This tells Vagrant’s router to NAT traffic differently. The first time this launches, it’ll download the VM from Hashicorp’s site, so it may look frozen for a time. Not to worry — it’s still working. From here you’ll see all the console output from each of the provisioning commands. I find it wonderfully fascinating to watch the console output roll by. In time, Vagrant will finish provisioning the machine, and return the command prompt to you. It’s running! Note : The username for all Vagrant machines is vagrant , and the password is also vagrant . Install SingleStore Components What did we install? Let’s look at the three tools and understand what each does. memsql-toolbox adds the admin tools for provisioning a cluster. We used memsql-deploy from this toolbox to start our cluster. memsql-client adds memsql, a lightweight client application that allows you to run SQL queries against your database from a terminal window. We won’t use it in this tutorial, but it’s really handy to have when you just need to pull some data real fast. memsql-studio is the browser-based cluster administration tool and SQL query executor. Vagrant ran this one as a service to keep it running in the background. In a production environment, we may choose to install only memsql-toolbox on our main machines, and leave the other tools for ansilatory machines. In this developer-focused single machine setup, we installed all three onto the same machine. Start SingleStore Studio As you installed the software, it started SingleStore Studio as a service, and launched the SingleStore database. Now let’s use them. SingleStore Studio runs on port 8080. Open your favorite browser on your laptop, and browse to http://localhost:8080/ . In the Vagrantfile, we forwarded this port to our local machine, so localhost will work. If you needed to change the port, browse to the new port here. Click “Add New Cluster” The host name is localhost because SingleStore Studio and the SingleStore cluster are both running on the same Vagrant machine. The port is 3306, the port of the master aggregator. The username is root , and the password is the password you put in the start.sh file. Mark the cluster as Development. Give the cluster a name and description such as “SingleStore dev cluster”. Click “Create Cluster”.\\ Note : this button is disabled until you’ve filled in all the required fields. We’re now directed to the main dashboard screen for our cluster. On this main dashboard screen, we can see the health of the cluster. Note that this is a two-node cluster. Clicking on Nodes on the left, we see one node is a leaf node, one is an aggregator node, and they’re both running in the same container. In production, we’d want more machines running together to support production-level workloads and to provide high availability. Click on the SQL Editor page, and we see the query window. In the query window, type each command, select the line, then push the execute button on the top-right. Loading Gist. Please Wait... For more details on SingleStore Studio, check out the docs or watch the SingleStore Studio tour video. Where Can We Go From Here? SingleStore is now ready for all the “kick the tires” tasks we need. You could: Hook up your analytics dashboard to SingleStore, connecting to the machine’s IP on port 3306. Start your application and connect it to SingleStore using any MySQL connector . Create an ingest pipeline from Kafka, S3, or other data source. Being able to run these tasks from such a simple setup is a real time-saver. However, don’t expect the same robustness or performance as you would have with a full install on a full hardware configuration . Cleanup We’ve finished our experiment today. To stop and delete the VM, run this command: vagrant delete This will delete the VM. If you’re done experimenting with SingleStore, you can delete the Ubuntu VM by running: vagrant box remove generic/ubuntu1904 Or better yet, leave this machine in place to quickly start up your next experiment. Conclusion With the SingleStore cluster-in-a-box configuration and a Vagrant-provisioned Linux virtual machine, we quickly stood up a “kick the tires” SingleStore cluster. Vagrant took all the work out of provisioning the cluster, and much like a container, handed us a fully running system. We saw how easy it is to spin up a cluster, connect to it with SingleStore Studio, and start being productive. Now go build great things!", "date": "2020-02-11"},
{"website": "Single-Store", "title": "spin-up-a-memsql-cluster-on-linux-in-10-minutes", "author": ["Rob Richardson"], "link": "https://www.singlestore.com/blog/spin-up-a-memsql-cluster-on-linux-in-10-minutes/", "abstract": "This blog post is part of a series describing how to run an entire SingleStore cluster – which usually requires at least five servers or machine instances – on your laptop, in a Linux virtual machine (VM). That’s right – even though SingleStore is a distributed system, you can run a minimal version of SingleStore on your laptop, in a single Linux VM. We tell you how in this blog post. You also have the option of running SingleStore on your laptop in other environments – see our blog posts for Docker Desktop , Kubernetes , and Vagrant . You should use the one you have more experience with, or is more compatible with your work environment. Whichever method you use, the combination of free access, and being able to run SingleStore on your laptop, can be extremely convenient for demos, software testing, developer productivity, and general fooling around. In this post we’ll quickly build a single-instance SingleStore cluster running on Linux, in a VM, on a laptop computer, for free. You’ll need a machine with at least 8GB RAM and four CPUs. This is ideal for quickly provisioning a system that will help you test and understand the capabilities of the SQL engine. Everything we build today will be running on your machine, and we’ll not need to install or configure much to get it running. The steps here are: get a free SingleStore license; install and boot a VM; install the SingleStore engine and tools; provision SingleStore as a cluster-in-a-box; browse to SingleStore Studio; and create a database. You’ll have a bare-bones SingleStore cluster running on your laptop machine in no time. Why a Single VM? Virtual Machines (VMs) are a great way to run software in a protected sandbox and easy-to-manage environment, with less overhead than a dedicated server, and less ceremony than with containers. You can use VMs to spin up applications and systems to try out software — even from a Mac or Windows machine, or to quickly spin up a database to support local application development. We’ll use a Linux VM to provision and spin up a free SingleStore cluster – and, just as easily, to destroy it when we’re done. Using a Linux terminal makes it much easier to explore the pieces in the SingleStore ecosystem and provision a small cluster. SingleStore’s cluster-in-a-box configuration that we’ll use here includes an aggregator node and a leaf node running on a single machine. We’ll add SingleStore Studio (our browser-based SQL editor and database maintenance tool), all running in one place, all configured to work together. The minimal hardware footprint wouldn’t be nearly enough for production workloads, but it allows us to quickly spin up a cluster, connect it to our project, and try things out. There are other options for running SingleStore’s cluster-in-a-box setup. We could use containers running on Kubernetes or Docker Desktop . If your production cluster runs on Linux machines or VMs, taking the approach described in this post helps add parity between production and development environments. If production is running in Kubernetes, you may prefer running SingleStore on Kubernetes . If you use containers without Kubernetes, you may find running the SingleStore container with a docker-compose.yaml file easier. You also have the option of using Vagrant – which, unlike the Docker containers used in the other two options, has its own separate instance of the operating system. With the single-machine SingleStore cluster described here, you can craft the simplest of tables all the way up to running a complex app, a dashboard, a machine learning model, streaming ingest from Kafka or Spark, or anything else you can think of, against SingleStore. You’ll quickly understand the methodology and features of SingleStore, and you can plan accordingly for real-world deployments. In this post, we’ll disable SingleStore’s minimum hardware specs, but you’ll still want a machine with at least 8GB RAM and four CPUs. With specs well below SingleStore’s limits , you’ll see poor performance, so this system is definitely not the right setup for a proof of concept (PoC). But you can use this setup to experience the full features of SingleStore, and understand how it applies to your business problems. Once you know how SingleStore works, you can take these experiments and use what you learn to help you achieve your service-level agreements (SLAs) on distributed clusters that meets SingleStore’s minimum requirements and your high-availability needs. That’s when you can really open up the throttle, learn how your data performs on SingleStore, and dial in system performance for your production workloads. Cluster-in-a-box Multi-node Cluster Hardware 💻 Laptop computer 🖥️🖥️🖥️ Many hefty machines Best use-case - Try out SingleStore -   Test SingleStore capabilities -   Prototypes - Proof of concept (PoC) -   Production workloads -   High availability -   Performance testing Cost Free up to four nodes with 32GB RAM each, and with community support Free up to four nodes with 32GB RAM each, and with community support Sign Up For SingleStore To get a free license for SingleStore, register at singlestore.com/free-software/ and click the link in the confirmation email. Then go to the SingleStore customer portal and login. Click “Licenses” and you’ll see your license for running SingleStore for free. This license never expires, and is good for clusters up to four machines and up to 128GB of combined RAM. This is not the license you’ll want for a production cluster, but it’s great for these “kick the tires” scenarios. Note this license key. We’ll need to copy/paste it into place later. Install VirtualBox The first step in getting our SingleStore cluster running is to get a Linux Virtual Machine. You can use VirtualBox, Xen, VMware, Hyper-V, or any other virtualization technology to craft a Linux VM. In this example we’ll use VirtualBox because it’s free and available on all platforms. If you’re already using virtualization technology, you can skip this step. Go to https://www.virtualbox.org/wiki/Downloads and download VirtualBox for your platform. Open the installer and follow the prompts. If you’re running on Windows, this will disable Hyper-V. Start a Linux VM Download a modern version of an Ubuntu VM from https://www.osboxes.org/ubuntu-server/ , or from https://virtualboxes.org/images/ubuntu/ , or from your corporate VM catalog. We’ll exclusively use the terminal, so choose a Server version if prompted. A server version removes the Desktop shell. You won’t have a mouse or VM tools installed in this configuration. The SingleStore database is tested to run on RHEL-based and Debian-based Linux Operating Systems, but with minor variation, you may be able to run this tutorial on other systems as well. Start up your virtualization software, and create a new virtual machine. In VirtualBox, the “New VM” button is on the toolbar. As you create the new VM, configure these settings: Set the VM to 64-bit mode. SingleStore doesn’t run on 32-bit systems . Set the memory to 8GB RAM. If your laptop doesn’t have 8GB of RAM free, set it as high as you can. Don’t create a new virtual hard drive. Instead, choose the Linux machine you downloaded. Ensure the VM has a network card. With the VM created, click Start, and boot the VM. Now let’s get to work installing SingleStore. Installing SingleStore We’re following the install steps for Debian-based systems to match the Ubuntu VM. If you chose a RedHat-based Linux OS, switch to the RedHat setup steps in the SingleStore docs for this section. In the Linux VM, open a terminal if necessary, and run these commands. We won’t be saving any files into the directory, so we don’t need to change directory into a specific folder. Each command in this section is run as root because we’re configuring the system here. The output of these commands is verbose and roughly amounts to console barf. As long as the commands return without error, we’re good. Become root. Though we could run each command as sudo, it’s easier to do it all at once. sudo su Allow installing Linux packages as https: apt update\napt install -y apt-transport-https Install SingleStore’s apt repository into Linux: wget -O - 'https://release.singlestore.com/release-aug2018.gpg' 2>/dev/null | apt-key add -\napt-key list\necho \"deb [arch=amd64] https://release.singlestore.com/production/debian memsql main\" | tee /etc/apt/sources.list.d/memsql.list Note that there are three commands here. If the lines wrap on your monitor, you may need to remove line-breaks as you type the commands. Install the latest version of SingleStore Toolbox, SingleStore command-line client, and SingleStore Studio: apt update\napt install -y memsql-toolbox memsql-client memsql-studio Start SingleStore Studio, the browser-based administration tool as a background service: systemctl start memsql-studio Stop being root exit SingleStore Components What did we install? Let’s look at the three tools and understand what each does. memsql-toolbox adds the admin tools for provisioning a cluster. We’ll use memsql-deploy from this toolbox to start our cluster. memsql-client adds memsql, a lightweight client application that allows you to run SQL queries against your database from a terminal window. We won’t use it in this tutorial, but it’s really handy to have when you just need to pull some data real fast. memsql-studio is the browser-based cluster administration tool and SQL query executor. We ran this one as a service to keep it running in the background. In a production environment, we may choose to install only memsql-toolbox on our main machines, and leave the other tools for ansilatory machines. In this developer-focused single machine setup, we installed all three onto the same machine. Starting the SingleStore Cluster We’ll start a “cluster-in-a-box” cluster. This cluster configuration has a single aggregator node and a single leaf node, both running on the same machine. In a production scenario, we’d want these on different machines, and many more than one. Go to the customer portal, switch to the license tab, and copy your license key. It’s really long, and likely ends in ==. Reusing the terminal from above, we run these commands not as root from any directory: memsql-deploy cluster-in-a-box --license \"YOUR_LICENSE_HERE\" ---password any_admin_password -y You can verify it’s running by running: ps -ef | grep memsql My output looks like this: root      3597     1  0 00:17 ?        00:00:03 /usr/bin/memsql-studio\nmemsql    3779     1  0 00:18 ?        00:00:00 /opt/memsql-server-7.0.11-df50c6ab30/memsqld_safe --defaults-file /var/lib/memsql/6c659cc1-2c1a-4c7c-95fb-f2b96c53d0d8/memsql.cnf --user 110 --auto-restart StagedEnable\nmemsql    3785  3779 12 00:18 ?        00:00:55 /opt/memsql-server-7.0.11-df50c6ab30/memsqld --defaults-file /var/lib/memsql/6c659cc1-2c1a-4c7c-95fb-f2b96c53d0d8/memsql.cnf --user 110\nmemsql    3787  3785  0 00:18 ?        00:00:00 /opt/memsql-server-7.0.11-df50c6ab30/memsqld --defaults-file /var/lib/memsql/6c659cc1-2c1a-4c7c-95fb-f2b96c53d0d8/memsql.cnf --user 110\nmemsql    4038     1  0 00:18 ?        00:00:00 /opt/memsql-server-7.0.11-df50c6ab30/memsqld_safe --defaults-file /var/lib/memsql/aeebcd10-7103-4b81-aef8-49f1308a0bde/memsql.cnf --user 110 --auto-restart StagedEnable\nmemsql    4044  4038 12 00:18 ?        00:00:53 /opt/memsql-server-7.0.11-df50c6ab30/memsqld --defaults-file /var/lib/memsql/aeebcd10-7103-4b81-aef8-49f1308a0bde/memsql.cnf --user 110\nmemsql    4046  4044  0 00:18 ?        00:00:00 /opt/memsql-server-7.0.11-df50c6ab30/memsqld --defaults-file /var/lib/memsql/aeebcd10-7103-4b81-aef8-49f1308a0bde/memsql.cnf --user 110\nrob       4399  4384  0 00:25 pts/0    00:00:00 grep memsql If your cluster doesn’t start correctly, you can run these additional commands as root to disable hardware capacity checks: sudo memsqlctl -yj update-config --all --key minimum_core_count --value 0\nsudo memsqlctl -yj update-config --all --key minimum_memory_mb --value 0 After running these commands, re-run the memsql-deploy command not as root. Congratulations! We’ve launched a SingleStore cluster. Let’s dive in and start using it. Start SingleStore Studio As we installed the software, we started SingleStore Studio as a service. Then we launched the SingleStore database. Now let’s use them. First we need to find the IP address of the Linux VM. In the terminal on the VM, run: ifconfig The results will look like this: lo: flags=73<UP,LOOPBACK,RUNNING> mtu 16384\n    inet 127.0.0.1 netmask 0xff000000 \n    inet6 ::1 prefixlen 128 \n    … snip ...\nen0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST> mtu 1500\n    inet 10.0.2.15 netmask 0xffffff00 broadcast 192.168.102.255\n    inet6 0102::0102:0102:0102:0102 prefixlen 64 scopeid 0x20 \n    ether 01:02:03:04:05:06 \n    … snip ... We’re looking for the IP address of the VM, so we’ll ignore the loopback interface (127.0.0.1). Look for the inet line, and grab the first set of numbers. My VM’s IP is 10.0.2.15. Next, let’s launch SingleStore Studio. It runs on port 8080. Open your favorite browser on your laptop, and browse to http://YOUR_IP:8080/ , substituting your VM’s IP. So I would browse to http://10.0.2.15:8080/ . Click “Add New Cluster” The host name is localhost because SingleStore Studio and the SingleStore cluster are both running on the same machine. The port is 3306, the port of the master aggregator. The username is root, and the password is the password you set above. Mark the cluster as Development. Give the cluster a name and description, such as “SingleStore dev cluster”. Click “Create Cluster”.\\ Note : this button is disabled until you’ve filled in all the required fields. We’re now directed to the main dashboard screen for our cluster. On this main dashboard screen, we can see the health of the cluster. Note that this is a two-node cluster. Clicking on Nodes on the left, we see one node is a leaf node, one is an aggregator node, and they’re both running in the same container. In production, we’d want more machines running together to support production-level workloads and to provide high availability. Click on the SQL Editor page, and we see the query window. In the query window, type each command, select the line, then push the execute button on the top-right. Loading Gist. Please Wait... For more details on SingleStore Studio, check out the docs or watch the SingleStore Studio tour video. Where Can We Go From Here? SingleStore is now ready for all the “kick the tires” tasks we need. You could: Hook up your analytics dashboard to SingleStore, connecting to the machine’s IP on port 3306. Start your application and connect it to SingleStore using any MySQL connector . Create an ingest pipeline from Kafka, S3, or other data source. Being able to run these tasks from such a simple setup is a real time-saver. However, don’t expect the same robustness or performance as you would have with a full install on a full hardware configuration . Cleanup We’ve finished our experiment today. To stop the VM, run this command: sudo shutdown This will stop the VM. As the VM powers down, so too will the SingleStore Studio service and the SingleStore nodes. If you’re done experimenting with SingleStore, delete the VM. Or better yet, leave this machine in place, to quickly start up your next experiment. Conclusion With the SingleStore cluster-in-a-box configuration and a Linux virtual machine, we quickly provisioned a “kick the tires” SingleStore cluster. We got to experience more of the components and configuration of a SingleStore cluster than we would have with a pre-provisioned system. We saw how easy it is to spin up a cluster, connect to it with SingleStore Studio, and start being productive. Now go build great things!", "date": "2020-02-11"},
{"website": "Single-Store", "title": "dresners-wisdom-of-crowds-report-puts-memsql-at-the-top", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/dresners-wisdom-of-crowds-report-puts-memsql-at-the-top/", "abstract": "A recent Dresner report, the Analytical Data Infrastructure Market Study , includes SingleStore for the first time. The report shows SingleStore as the leader among full-service database providers – ahead of Google, Amazon, Microsoft, IBM, SAP, and Oracle. (A few specialty analytics products also rank highly.) The Dresner report is part of their Wisdom of Crowds series, and rates SingleStore as an Overall Leader in both major categories of the report: Customer Experience and Vendor Credibility, with a perfect Recommend score. About the Dresner Study Dresner Advisory Services is well-known for their insightful market studies on business intelligence (BI) and analytics. Howard Dresner, the founder of Dresner Analytics, coined the term business intelligence (BI). Their marquee report series, the Wisdom of Crowds, asks real users to rate BI and analytics products on key performance indicators. The reports “contain feedback from real life implementers, IT directors, and contractors that actually are working with BI products in the field.” In these reports, Dresner asks actual users to rate their products on the key indicators that users themselves require. The Analytical Data Infrastructure report targets “technology components for integrating, modeling, managing, storing, and accessing the data sets that serve as sources for analytic/BI consumers, e.g., analytic/business applications, tools, and users.” The report focuses on databases, and SingleStore is one of more than a dozen vendors studied. SingleStore, as we’ll describe below, was the top-rated general purpose database (that is, not solely an analytics product). Use cases for the products studied include (from most-used to least-used): Reporting and dashboards Discovery and exploration by business users Data science, which includes the use of machine learning and AI for predictive and advanced analytics Embedded analytics within business applications (high volume, low latency) Users named performance, security, and scalability as their most-desired features. Performance was rated as especially desirable within the embedded analytics use case, where SingleStore shines, and in the largest companies – those with more than 10,000 employees (usually those with more than $1B in revenue). The top feature desired is scalability, which is core to SingleStore’s differentiation as a fast, scalable relational database with SQL support. Scalability is also strongly desired by tech companies, financial services, retail, and consumer services companies, as well as government. The largest and smallest companies are the most interested in scalability, while smaller and medium-sized companies put data life cycle management first. SQL support and columnar data support are seen as critical or very important by the most companies, especially for business uses. Among analytical features, aggregations lead the way, along with multi-dimensional/OLAP-type queries. SingleStore’s strong SQL support and ability to mix rowstore and columnstore data tables make it well-suited to meet thee requirements. Also highly desired are user-defined functions and machine learning support, both areas where SingleStore stands out. About SingleStore in This Report SingleStore ranks very strongly on both major axes that Dresner measures for customer experience: product/technology, where SingleStore is in the top five, near Google, and sales and service, where SingleStore is in a near-tie for second with Snowflake, which is exclusively an analytics provider. Taking the two measures together, SingleStore is the leader among full-service databases (that handle both transactions and analytics), ahead of Google, Amazon, Microsoft, IBM, SAP, and Oracle. (Among these companies, only Google ranked above average on both measures; Oracle, IBM, and SAP ranked worst overall.) In the Vendor Credibility model, SingleStore ranked very high on user confidence, and above average for value. According to Dresner, SingleStore stands out as “an Overall Leader in both the Customer Experience and Vendor Credibility models.” Further, SingleStore is best in class for technical support professionalism, for responsiveness, and for consulting professionalism. SingleStore’s recommend score is perfect. Conclusion Considered alongside Industry leaders, SingleStore rates very highly – and is #1 among full-service database providers in both Customer Experience and Vendor Credibility. You can download the report for free . You can also try SingleStore for free today, or contact us for a personalized demo.", "date": "2020-02-13"},
{"website": "Single-Store", "title": "capterra-captures-memsql-reviews", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/capterra-captures-memsql-reviews/", "abstract": "Capterra describes itself as “the leading online resource for business software buyers,” boasting more than one million verified software reviews across hundreds of software categories. Those one million reviews include many positive takes on SingleStore software, including this comment from a big data manager: “Best Distributed RDBMS Ever!” Capterra is growing fast, with more than five million monthly users , and the number of verified reviews on the site nearly doubling between 2018 and mid-2019. Capterra research asserts that reviews reduce purchase cycles by more than six months. More than four in ten small and medium business (SMB) users surveyed use reviews in their purchase process. And these users, by and large, love SingleStore. It’s a “great product” with “similarity to MySQL” and “more scalability.” It’s “very fast,” “easy to operate,” and “works perfect on Linux.” “If you are looking for something for real-time analytics/dashboard, this is the go-to option.” The Capterra site is focused on SMB users. For enterprise users, we have a roundup of comments and an update with newer comments from reviews site G2 Crowd, which is more focused on enterprise users. And we’ve captured highlights from reviews site Gartner Peer Insights , which also focuses on the enterprise. (Gartner owns both Gartner Peer Insights and Capterra, which it purchased for more than $200M in 2019.) Together, these review sites can give you a fair picture of SingleStore’s suitability for your needs – and, hopefully, shorten your purchase cycle, as Capterra does for many of its users. Most Helpful Reviews (Nearly) Say It All The most helpful reviews show at the top of the list for Capterra’s software reviews. Several of the most helpful reviews for SingleStore include an awful lot of the best features of SingleStore, as seen on Capterra, and across all three of the reviews sites we’ve described: “One solution for streaming analytics on big data,” says a senior manager for data engineering. He’s focused on machine learning and AI, and he describes the software as “super simple.” His shop runs “multi-petabyte S3” stores with “huge Kafka clusters.” They see “sub-second response on a test case with 1000 concurrent heavy API calls (scanning billions of rows).” SingleStore is “incredibly fast,” “fantastic partners” who offer “access to their core engineering team.” A director of IT infrastructure at a large oil and energy company has built a “simplified data lake system” around SingleStore. He sees “large amounts of IoT data (trillions of rows)” that “can be queried in milliseconds.” Processes that “took hours to run” are now “running sub-second on SingleStore.” The software offers “amazing performance” and is “highly and easily scalable.” A senior architect for IT and services at a large company calls SingleStore a “supersonic DB” that “aces every database” that he has worked with. SingleStore is “the database of the new generation” with “huge potential for both on-premises and cloud.” It features “high compatibility,” “resilience,” and “scalability.” SingleStore is “highly recommended to any organization wanting to get rid of old-fashioned databases.” Many of the comments offer real insight. One big data manager lists pros which include “JSON support and full-text search,” “drop-in replacement to the famous MySQL,” and “in-memory tables for high OLTP workloads and on-disk columnar storage for OLAP workloads.” Users are able to “ingest millions of documents every day and run sophisticated dashboards against them.” They achieve a “huge performance win,” see SingleStore as “easy to connect to Kafka” and “easy to set up on Kubernetes.” SingleStore is a “great replacement for Hadoop for a fraction of the cost,” with aggregation times dropping from over 2 hours to less than 20 minutes. And “You can seamlessly join both row and columnar tables and query across it.” A few more adjectives, from these and other reviews: “elegant”; “excellent”; “amazing”; “the go-to option” for real-time analytics and dashboards; “great support”; “blazing fast”; “good engineering principles”; “fast implementation” (in a one-day hackathon); “too easy to set up.” A senior data engineering manager for AI offers insightful comments in a five-star review. The Bottom Line One user sums it up, with words that many users of other solutions would like to be able to say: “We are within our SLA.” If you use SingleStore already, consider posting a review today. Your efforts will benefit the community as a whole. If you haven’t yet tried SingleStore, take a look at the reviews on the Capterra site . You can also post questions on the SingleStore Forums or try SingleStore for free today.", "date": "2020-02-12"},
{"website": "Single-Store", "title": "whats-after-the-mean-stack", "author": ["Rob Richardson"], "link": "https://www.singlestore.com/blog/whats-after-the-mean-stack/", "abstract": "The MEAN stack – MongoDB, Express.js, Angular.js, and Node.js – has served as a pattern for a wide variety of web development. But times have changed, and the components of the MEAN stack have failed to keep up with the times. Let’s take a look at how the MEAN stack superseded the previous stack, the LAMP stack, and at the options developers have now for delivering efficient Web applications. Introduction We reach for software stacks to simplify the endless sea of choices. The MEAN stack is one such simplification that worked very well in its time. Though the MEAN stack was great for the last generation, we need more; in particular, more scalability. The components of the MEAN stack haven’t aged well, and our appetites for cloud-native infrastructure require a more mature approach. We need an updated, cloud-native stack that can boundlessly scale as much as our users expect to deliver superior experiences. Stacks When we look at software, we can easily get overwhelmed by the complexity of architectures or the variety of choices. Should I base my system on Python?  Or is Go a better choice? Should I use the same tools as last time? Or should I experiment with the latest hipster toolchain? These questions and more stymie both seasoned and newbie developers and architects. Some patterns emerged early on that help developers quickly provision a web property to get started with known-good tools. One way to do this is to gather technologies that work well together in “stacks.” A “stack” is not a prescriptive validation metric, but rather a guideline for choosing and integrating components of a web property. The stack often identifies the OS, the database, the web server, and the server-side programming language. In the earliest days, the famous stacks were the “LAMP-stack” and the “Microsoft-stack”. The LAMP stack represents Linux, Apache, MySQL, and PHP or Python. LAMP is an acronym of these product names. All the components of the LAMP stack are open source (though some of the technologies have commercial versions), so one can use them completely for free. The only direct cost to the developer is the time to build the experiment. The “Microsoft stack” includes Windows Server, SQL Server, IIS (Internet Information Services), and ASP (90s) or ASP.NET (2000s+). All these products are tested and sold together. Stacks such as these help us get started quickly. They liberate us from decision fatigue, so we can focus instead on the dreams of our start-up, or the business problems before us, or the delivery needs of internal and external stakeholders. We choose a stack, such as LAMP or the Microsoft stack, to save time. In each of these two example legacy stacks, we’re producing web properties. So no matter what programming language we choose, the end result of a browser’s web request is HTML, JavaScript, and CSS delivered to the browser. HTML provides the content, CSS makes it pretty, and in the early days, JavaScript was the quick form-validation experience. On the server, we use the programming language to combine HTML templates with business data to produce rendered HTML delivered to the browser. We can think of this much like mail merge: take a Word document with replaceable fields like first and last name, add an excel file with columns for each field, and the engine produces a file for each row in the sheet. As browsers evolved and JavaScript engines were tuned, JavaScript became powerful enough to make real-time, thick-client interfaces in the browser. Early examples of this kind of web application are Facebook and Google Maps. These immersive experiences don’t require navigating to a fresh page on every button click. Instead, we could dynamically update the app as other users created content, or when the user clicks buttons in the browser. With these new capabilities, a new stack was born: the MEAN stack. What is the MEAN Stack? The MEAN stack was the first stack to acknowledge the browser-based thick client. Applications built on the MEAN stack primarily have user experience elements built in JavaScript and running continuously in the browser. We can navigate the experiences by opening and closing items, or by swiping or drilling into things. The old full-page refresh is gone. The MEAN stack includes MongoDB, Express.js, Angular.js, and Node.js. MEAN is the acronym of these products. The back-end application uses MongoDB to store its data as binary-encoded JavaScript Object Notation (JSON) documents. Node.js is the JavaScript runtime environment, allowing you to do backend, as well as frontend, programming in JavaScript. Express.js is the back-end web application framework running on top of Node.js. And Angular.js is the front-end web application framework, running your JavaScript code in the user’s browser. This allows your application UI to be fully dynamic. Unlike previous stacks, both the programming language and operating system aren’t specified, and for the first time, both the server framework and browser-based client framework are specified. In the MEAN stack, MongoDB is the data store. MongoDB is a NoSQL database, making a stark departure from the SQL-based systems in previous stacks. With a document database, there are no joins, no schema, no ACID compliance, and no transactions. What document databases offer is the ability to store data as JSON, which easily serializes from the business objects already used in the application. We no longer have to dissect the JSON objects into third normal form to persist the data, nor collect and rehydrate the objects from disparate tables to reproduce the view. The MEAN stack webserver is Node.js, a thin wrapper around Chrome’s V8 JavaScript engine that adds TCP sockets and file I/O. Unlike previous generations’ web servers, Node.js was designed in the age of multi-core processors and millions of requests. As a result, Node.js is asynchronous to a fault, easily handling intense, I/O-bound workloads. The programming API is a simple wrapper around a TCP socket. In the MEAN stack, JavaScript is the name of the game. Express.js is the server-side framework offering an MVC-like experience in JavaScript. Angular (now known as Angular.js or Angular 1) allows for simple data binding to HTML snippets. With JavaScript both on the server and on the client, there is less context switching when building features. Though the specific features of Express.js’s and Angular.js’s frameworks are quite different, one can be productive in each with little cross-training, and there are some ways to share code between the systems. The MEAN stack rallied a web generation of start-ups and hobbyists. Since all the products are free and open-source, one can get started for only the cost of one’s time. Since everything is based in JavaScript, there are fewer concepts to learn before one is productive. When the MEAN stack was introduced, these thick-client browser apps were fresh and new, and the back-end system was fast enough, for new applications, that database durability and database performance seemed less of a concern. The Fall of the MEAN Stack The MEAN stack was good for its time, but a lot has happened since. Here’s an overly brief history of the fall of the MEAN stack, one component at a time. Mongo got a real bad rap for data durability. In one Mongo meme , it was suggested that Mongo might implement the PLEASE keyword to improve the likelihood that data would be persisted correctly and durably. (A quick squint, and you can imagine the XKCD comic about “sudo make me a sandwich.”) Mongo also lacks native SQL support, making data retrieval slower and less efficient. Express is aging, but is still the defacto standard for Node web apps and apis. Much of the modern frameworks — both MVC-based and Sinatra-inspired — still build on top of Express. Express could do well to move from callbacks to promises, and better handle async and await, but sadly, Express 5 alpha hasn’t moved in more than a year. Angular.js (1.x) was rewritten from scratch as Angular (2+). Arguably, the two products are so dissimilar that they should have been named differently. In the confusion as the Angular reboot was taking shape, there was a very unfortunate presentation at an Angular conference. The talk was meant to be funny, but it was not taken that way. It showed headstones for many of the core Angular.js concepts, and sought to highlight how the presenters were designing a much easier system in the new Angular. Sadly, this message landed really wrong. Much like the community backlash to Visual Basic’s plans they termed Visual Fred , the community was outraged. The core tenets they trusted every day for building highly interactive and profitable apps were getting thrown away, and the new system wouldn’t be ready for a long time. Much of the community moved on to React, and now Angular is struggling to stay relevant. Arguably, Angular’s failure here was the biggest factor in React’s success — much more so than any React initiative or feature. Nowadays many languages’ frameworks have caught up to the lean, multi-core experience pioneered in Node and Express. ASP.NET Core brings a similarly light-weight experience, and was built on top of libuv , the OS-agnostic socket framework, the same way Node was. Flask has brought light-weight web apps to Python. Ruby on Rails is one way to get started quickly. Spring Boot brought similar microservices concepts to Java. These back-end frameworks aren’t JavaScript, so there is more context switching, but their performance is no longer a barrier, and strongly-typed languages are becoming more in vogue. As a further deterioration of the MEAN stack, there are now frameworks named “mean,” including mean.io and meanjs.org and others. These products seek to capitalize on the popularity of the “mean” term. Sometimes it offers more options on the original MEAN products, sometimes scaffolding around getting started faster, sometimes merely looking to cash in on the SEO value of the term. With MEAN losing its edge, many other stacks and methodologies have emerged. The JAM Stack The JAM stack is the next evolution of the MEAN stack. The JAM stack includes JavaScript, APIs, and Markup. In this stack, the back-end isn’t specified – neither the webserver, the back-end language, or the database. In the JAM stack we use JavaScript to build a thick client in the browser, it calls APIs, and mashes the data with Markup — likely the same HTML templates we would build in the MEAN stack. The JavaScript frameworks have evolved as well. The new top contenders are React, Vue.js, and Angular, with additional players from Svelte, Auralia, Ember, Meteor, and many others. The frameworks have mostly standardized on common concepts like virtual dom, 1-way data binding, and web components. Each framework then combines these concepts with the opinions and styles of the author. The JAM stack focuses exclusively on the thick-client browser environment, merely giving a nod to the APIs, as if magic happens behind there. This has given rise to backend-as-a-service products like Firebase, and API innovations beyond REST including gRPC and GraphQL. But, just as legacy stacks ignored the browser thick-client, the JAM stack marginalizes the backend, to our detriment. Maturing Application Architecture As the web and the cloud have matured, as system architects, we have also matured in our thoughts of how to design web properties. As technology has progressed, we’ve gotten much better at building highly scalable systems. Microservices offer a much different application model where simple pieces are arranged into a mesh. Containers offer ephemeral hardware that’s easy to spin up and replace, leading to utility computing. As consumers and business users of systems, we almost take for granted that a system will be always on and infinitely scalable. We don’t even consider the complexity of geo-replication of data or latency of trans-continental communication. If we need to wait more than a second or two, we move onto the next product or the next task. With these maturing tastes, we now take for granted that an application can handle near infinite load without degradation to users, and that features can be upgraded and replaced without downtime. Imagine the absurdity if Google Maps went down every day at 10 pm so they could upgrade the system, or if Facebook went down if a million people or more posted at the same time. We now take for granted that our applications can scale, and the naive LAMP and MEAN stacks are no longer relevant. Characteristics of the Modern Stack What does the modern stack look like?  What are the elements of a modern system?  I propose a modern system is cloud-native, utility-billed, infinite-scale, low-latency, user-relevant using machine learning, stores and processes disparate data types and sources, and delivers personalized results to each user. Let’s dig into these concepts. A modern system allows boundless scale. As a business user, I can’t handle if my system gets slow when we add more users. If the site goes viral, it needs to continue serving requests, and if the site is seasonally slow, we need to turn down the spend to match revenue. Utility billing and cloud-native scale offers this opportunity. Mounds of hardware are available for us to scale into immediately upon request. If we design stateless, distributed systems, additional load doesn’t produce latency issues. A modern system processes disparate data types and sources. Our systems produce logs of unstructured system behavior and failures. Events from sensors and user activity flood in as huge amounts of time-series events. Users produce transactions by placing orders or requesting services. And the product catalog or news feed is a library of documents that must be rendered completely and quickly. As users and stakeholders consume the system’s features, they don’t want or need to know how this data is stored or processed. They need only see that it’s available, searchable, and consumable. A modern system produces relevant information. In the world of big data, and even bigger compute capacity, it’s our task to give users relevant information from all sources. Machine learning models can identify trends in data, suggesting related activities or purchases, delivering relevant, real-time results to users. Just as easily, these models can detect outlier activities that suggest fraud. As we gain trust in the insights gained from these real-time analytics, we can empower the machines to make decisions that deliver real business value to our organization. SingleStore is the Modern Stack’s Database Whether you choose to build your web properties in Java or C#, in Python or Go, in Ruby or JavaScript, you need a data store that can elastically and boundlessly scale with your application. One that solves the problems that Mongo ran into – that scales effortlessly, and that meets ACID guarantees for data durability. We also need a database that supports the SQL standard for data retrieval. This brings two benefits: a SQL database “plays well with others,” supporting the vast number of tools out there that interface to SQL, as well as the vast number of developers and sophisticated end users who know SQL code. The decades of work that have gone into honing the efficiency of SQL implementations is also worth tapping into. These requirements have called forth a new class of databases, which go by a variety of names; we will use the term NewSQL here. A NewSQL database is distributed, like Mongo, but meets ACID guarantees, providing durability, along with support for SQL. CockroachDB and Google Spanner are examples of NewSQL databases. We believe that SingleStore brings the best SQL, distributed, and cloud-native story to the table. At the core of SingleStore is the distributed database. In the database’s control plane is a master node and other aggregator nodes responsible for splitting the query across leaf nodes, and combining the results into deterministic data sets. ACID-compliant transactions ensure each update is durably committed to the data partitions, and available for subsequent requests. In-memory skiplists speed up seeking and querying data, and completely avoid data locks. SingleStore Managed Service delivers the same boundless scale engine as a managed service in the cloud. No longer do you need to provision additional hardware or carve out VMs. Merely drag a slider up or down to ensure the capacity you need is available. SingleStore is able to ingest data from Kafka streams, from S3 buckets of data stored in JSON, CSV, and other formats, and deliver the data into place without interrupting real-time analytical queries. Native transforms allow shelling out into any process to transform or augment the data, such as calling into a Spark ML model. SingleStore stores relational data, stores document data in JSON columns, provides time-series windowing functions, allows for super-fast in-memory rowstore tables snapshotted to disk and disk-based columnstore data, heavily cached in memory. As we craft the modern app stack, include SingleStore as your durable, boundless cloud-native data store of choice. Conclusion Stacks have allowed us to simplify the sea of choices to a few packages known to work well together. The MEAN stack was one such toolchain that allowed developers to focus less on infrastructure choices and more on developing business value. Sadly, the MEAN stack hasn’t aged well. We’ve moved on to the JAM stack, but this ignores the back-end completely. As our tastes have matured, we assume more from our infrastructure. We need a cloud-native advocate that can boundlessly scale, as our users expect us to deliver superior experiences. Try SingleStore for free today, or contact us for a personalized demo.", "date": "2020-02-16"},
{"website": "Single-Store", "title": "kubernetes-101-architecture", "author": ["Rob Richardson"], "link": "https://www.singlestore.com/blog/kubernetes-101-architecture/", "abstract": "Kubernetes is widely discussed, and widely misunderstood. We here at SingleStore are fans of Kubernetes and the stateful services that have been added to it since its launch, and we’ve used them to build and manage SingleStore Managed Service. So we are taking the time to explain Kubernetes’ architecture, from the top down, to help you decide whether using Kubernetes is right for you. What is Kubernetes? Kubernetes is an open-source orchestrator originally built by Google, now owned and managed by the Cloud Native Computing Foundation ( CNCF ). Kubernetes is famous for extensibility and for no-downtime rolling updates. Most public and private cloud platforms have push-button mechanisms to spin up a Kubernetes cluster, pooling provisioned cloud resources together into a single platform that can be controlled together. Kubernetes has made a name for itself as the cloud-native platform for running containers. As a user unfamiliar with this platform, you can find this guide helpful in getting acquainted with the various components of Kubernetes, and getting started running your containers here. As your needs grow, from stateless services to stateful data stores or custom installations, Kubernetes offers extension points that allow you to replace pieces with community extensions or custom-built components. Kubernetes is quite the weird mouthful. As noted by Dan Richman in Geekwire , “Kubernetes (‘koo-burr-NET-eez’) is the no-doubt-mangled conventional pronunciation of a Greek word, κυβερνήτης, meaning ‘helmsman’ or ‘pilot.’ Get it? Helping you navigate the murky waters of cloud computing…”  Because it’s such a wonderfully long word, we often shorten it to “k8s,” which you can think of as “k followed by 8 letters and then s.” Kubernetes lives in the family of orchestrators, but it’s hardly the only one. Before diving into the specifics of this orchestrator, let’s look at what an orchestrator is in general. Note . We at SingleStore are great believers in Kubernetes. We use Kubernetes to manage SingleStore Managed Service , our always-on, elastic, managed SingleStore service in the cloud. (You may have noted how our description of SingleStore Managed Service resembles descriptions of the capabilities of Kubernetes.) We had to develop a beta Kubernetes Operator for SingleStore to do so. But, even with that effort, we were able to develop and release SingleStore Managed Service much more quickly, and with a much smaller team, than without it. What is an Orchestrator? Kubernetes is an example of an orchestrator — machinery for bringing a bunch of machines together to act as a single unit. Imagine we collect a dozen or a hundred machines. Though we could connect to each one remotely to configure it, we’d rather just tell the orchestrator an instruction and have it control each machine for us. I’ll give the orchestrator an instruction. “Run three copies of my web server, I don’t care how.”  (Granted we’ll likely specify many more details than this.) The orchestrator can take these instructions, find free resources on a machine (or machines) of its choosing, and launch the containers. We give the orchestrator our desired state, and let it choose how to ensure that desired state is met. Perhaps we give the orchestrator additional instructions to ensure the webserver is running across availability zones or in multiple regions. We don’t need to specify which machine does what, only that the work is done. Because the orchestrator is in control of the machines, the orchestrator can also monitor the health of the system. What if one webserver container crashes?  Or what if the machine it’s running on fails, or needs to upgrade? The orchestrator notices this failure, and takes corrective action. It’ll schedule a new copy of the webserver on other hardware, and if possible, only then take down the old system. … and we get to stay asleep. Which machine did it choose?  I don’t know … and I don’t need to know. I need only know that my webserver is running and that traffic is routing into it. Examples of Orchestrators There are many different brands of orchestrators, both open-source and proprietary. Examples include Kubernetes, Docker Swarm, Azure Service Fabric, Amazon Cluster Service, and Mesosphere DC/OS. All of these orchestrators can solve the “what to run where” problem. We give it an instruction like “Run three copies of my webserver, I don’t care how,” and the orchestrator will launch three containers and report their status. Most orchestrators will also restart a container when it fails, notifying the humans that it has done so. (This allows the humans to, for instance, take action to make failures less likely in the future.) Each orchestrator has strengths and weaknesses, and each is best for particular jobs. I’m not familiar with the needs of your organization, so I can’t presume to dictate which orchestrator is the right one for you. Walk into your IT department and say “Kubernetes? Docker Swarm?”, for example, and some bright-eyed engineer will talk your ear off for an hour explaining why one of these orchestrators is the right one for our organization. I completely agree — that is the right choice for you. But for the sake of this post, let’s assume we choose Kubernetes. 😀  Kubernetes isn’t novel because it can pool machines together into a single cloud OS, nor is it novel in keeping containers running. But it is a novel and well accepted platform. Let’s dig in and see more. Kubernetes Architecture Most introductory tutorials for Kubernetes show a graphic like this, step away, and say, “So now that I’ve explained it to you…”  <scratches head > . The first time I saw a diagram like this one, I too was incredibly confused. What are all the boxes and arrows? What does that mean to me and my app? The Kubernetes Architecture. (Source: VitalFlux) Note . The graphic above is from VitalFlux , which takes on the heavy lifting of explaining Kubernetes in more detail than we have provided here. I’ll explain each of these pieces, and unless you’re going to go deep into the weeds, this is likely the first and last time you’ll need to worry about Kubernetes in this way. In an exceptionally oversimplified explanation, here’s what this diagram shows: 1 . The Control Plane are all the machines that manage the cluster; All the green boxes. (If this were an organization chart, it would be labeled “management.”) We can think of this as all the plumbing in the system. The work of your web properties and data stores is not run here. You’ll generally want a few machines doing this work — three or five or nine. In most cloud-provided k8s clusters, these machines are free. 2 . The Worker Nodes are all the machines doing work for you and your business; All the blue boxes. These machines run your web properties, back-end services, scheduled jobs, and data stores. (You may choose to store the actual data elsewhere, but the engine that runs the data store will run here.) 3 . As a developer or ops engineer, you’ll likely use kubectl , the command-line utility for Kubernetes, to start, stop, and change content in the cluster. 4 . kubectl connects to the kubernetes API server to give it instructions. 5 . The API server stores data in etcd , the kubernetes data store. 6 . The Controller Manager polls against the API, and notices a change in etcd. 7 . The Controller Manager directs the Scheduler to make a change to the environment, and the Scheduler picks a Worker Node (one of the blue boxes) to do the work. 8 . The Scheduler tells the Kubelet to make the necessary change. The Kubelet is responsible for the node (machine). 9 . The Kubelet fires up a Pod , and runs Docker commands to run the container. cAdvisor watches the running pods, reporting events back to the API that get stored in etcd. 11 . As a user visiting a website, the traffic comes in through the Internet, and through a load balancer, which chooses one of the worker nodes (machines) to run the content. 12 . The traffic is forwarded to Kube-Proxy . 13 . Kube-Proxy identifies which Pod should receive the traffic and directs it there. 14 . The Pod wraps the container , which processes the request, and returns the response. 15 . The response flows back to the user across the Kube-Proxy and the Load Balancer. This is likely the first and last time you care about Kubernetes at this level. (The knee bone connected to the hip bone.)  But as you look at the graphic and the flow of data, one thing stands out: Kubernetes is great for microservices because Kubernetes is microservices. Each one of these pieces is a container running inside the Kubernetes cluster. Each can be replaced with alternate implementations, either from the community, or things you build. Kubernetes is a great place for you to host your services because Kubernetes hosts its own services this way as well. Conclusion Kubernetes is the orchestrator for the modern web. Whether you’re running stateless website code or running a stateful data store, such as SingleStore, Kubernetes can scale to match your needs. As the emerging industry standard platform, it’s extensible, allowing one to replace portions of the cloud OS with custom pieces, from either open-source players or custom-built components. Kubernetes will automatically monitor your crucial business web properties, and can restart necessary services without intervention. If you’re interested in trying some or all of this with SingleStore, you can try SingleStore software or SingleStore Managed Service for free today, or contact us to discuss SingleStore and SingleStore Managed Service in depth.", "date": "2020-02-17"},
{"website": "Single-Store", "title": "spin-up-a-cluster-on-memsql-helios-in-10-minutes", "author": ["Rob Richardson"], "link": "https://www.singlestore.com/blog/spin-up-a-cluster-on-memsql-helios-in-10-minutes/", "abstract": "You can run SingleStore Managed Service , with $500 of FREE credits. We tell you how in this blog post. The combination of free access, and not needing to worry about infrastructure maintenance, can be extremely convenient for demos, software testing, developer productivity, brief proof of concept (PoC) tests, and brief tests of production workloads. Many Ways to Run SingleStore for Free Because there are significant costs to SingleStore, free trials of SingleStore Managed Service in the cloud are limited to 48 hours of time – two full days. You can also download and run SingleStore software for free, in the cloud (you pay the cloud costs), or on your own services, without a time limit. The only restriction is a limit to four nodes (servers or server instances) with up to 32GB RAM, which is enough for many of our customers to do a great deal of meaningful work. And, if you wish to try running SingleStore for free on your laptop, rather than across multiple servers, you can easily do that too. We’ve developed instructions for running SingleStore for free on Docker Desktop (recommended if you don’t have specialized knowledge of the other methods listed here); on Kubernetes (which is great if you know, or want to use, Kubernetes); directly on Linux ; or on Vagrant . You should run your cluster on SingleStore Managed Service for the greatest ease of use and management if you’re sure that you will want to use SingleStore Managed Service, or if you want to compare the fully managed SingleStore Managed Service in the cloud to SingleStore software that you manage yourself, whether in the cloud of on-premises. In this post you’ll quickly build a 4-node SingleStore cluster running on SingleStore Managed Service, the managed, infinite scale database in the cloud, for free. There is no hardware to provision, no software to install, no fuss, no mess. The steps here are: get a free SingleStore license; login to the cloud portal; create the SingleStore Managed Service cluster; and browse to SingleStore Studio. You’ll have a solid 4-node, distributed, partitioned cluster running in the cloud in about ten minutes. Why Move to the Cloud? In traditional database projects, you first need to acquire hardware, then provision the hardware by installing the operating system, install the SingleStore software, and finally provision the database. In the event of hardware failure, you need to diagnose the failure, contain the failing parts, and replace or reconfigure to compensate for failure. When running in the cloud, all this infrastructure management is outsourced to SingleStore’s managed cloud platform. The only things you’ll need to specify are the size of our cluster and which public cloud you’d like to use. In non-trial configurations, the cost of SingleStore Managed Service includes both the software and hardware. Compare this to the cost of your legacy database and the cloud or on-prem machines you’ll need to manage. SingleStore Managed Service contains the costs, offering a single, easy-to-read bill that covers all your data storage and processing needs. And customers tell us that the cost of SingleStore Managed Service, which includes the cloud provider costs, is almost identical to the cost of an Enterprise license for SingleStore software, plus the cloud costs to run it. The only difference is that, with SingleStore Managed Service, the service is managed by SingleStore, rather than by you. You save the time, effort, and cost of managing your cluster. Your database management tasks and support requirements from SingleStore are simplified. With SingleStore Managed Service, once you know how SingleStore works, you can take these experiments and quickly scale up to assess SingleStore against your service-level agreements (SLAs) and your need for high availability needs. That’s when you can really open up the throttle, learn how your data performs on SingleStore, and dial in system performance for your production workloads. Cluster-in-a-Box Multi-Node Cluster Hardware 💻 Laptop computer 🖥️🖥️🖥️🖥️ Many hefty machines Best use case - Try out SingleStore -   Test SingleStore capabilities -   Prototyping - Proof of concept (PoC) -   Production workloads -   High availability -   Performance testing Cost Free, up to four nodes with 32GB RAM each Free, up to four nodes with 32GB RAM each Support Community support Community support Sign Up For SingleStore To get a free license for SingleStore, register at https://www.singlestore.com/managed-service-trial// and click the link in the confirmation email. Then go to the SingleStore customer portal at and login. On the Licenses page, you also see the free-tier license. This license never expires, and is good for clusters up to four machines and 128GB of combined RAM. This is not the license you’ll want for a production cluster, but it’s great for these “kick the tires” scenarios. You could use this license key to spin up a single-node cluster-in-a-box in Docker Desktop , on Kubernetes , or on a Linux VM . You could also use this to spin up a 4-node cluster with up to 32-gigs ram on a Linux VM, in containers, or using the Kubernetes operator. Spin Up a Cluster Spinning up a cloud-native trial database is easy. You’ll log in to the portal, click through a few dialogs, and in no time you’ll have a cluster of machines working to store and serve our data quickly. Signup for an account Click Clusters on the far left. Click Create a Managed Cluster. Set the cluster’s name.\\ Choose the cluster’s environment. This is merely a description and doesn’t affect the responsiveness or availability of the cluster. Choose the Cloud Provider and Region. Don’t see the region you want yet? Contact us , and let us know. The Cluster Size only gives you a single option. In non-trial environments, you could scale up, but like SingleStore’s free tier , in the SingleStore Managed Service trial, you’re limited to 4 nodes. Click “Next.”\\ Set the database admin password. This is the password you’ll use as you connect applications and dashboards to the data store. Set database access restrictions. Best practice is to limit the database to a few key IP addresses, so there’s no more attack surface than necessary. Click “Add My Current IP Address.” Click “Create Cluster.” With that, the cluster is provisioned. Within a few minutes, it’ll be ready to use. There was no software to install, no hardware to provision, no fuss, no mess. Start SingleStore Studio As soon as the cluster says it’s ready, let’s dive in. SingleStore Studio is the browser-based SQL editor and database administration tool. You get SingleStore Studio access for free as part of any SingleStore Managed Service cluster. Signup for an account. Click Clusters on the far left. Scroll down through the information page. Note that the username is “admin.” Two IP addresses are listed for the cluster. The first is the Admin connection for changing schema. The second is for querying and altering data. Click on “Open SingleStore Studio.” Login to the cluster using the username “admin” and the password you entered when creating the cluster. Down the left side we see all the available tools in SingleStore Studio including the SQL Editor, the list of databases, and the Query Profiler. On the main page, we see 7 healthy nodes. Wait a minute – we provisioned a four-node cluster. How did we get seven nodes? Click on the nodes tab on the left, and you’ll see the nodes in your cluster. You will have one master aggregator, two other aggregators, and four leaf nodes. Data is stored in partitions on the leaf nodes. Users and applications connect to any aggregator node and issue queries. The aggregator splits the query into pieces for each leaf and collects, assembles, and sorts the results into a deterministic result set. So you paid for four nodes, and you got seven nodes. You see that you only pay for the nodes that are storing your data. Excellent! Click on the SQL Editor page, and you see the query window. In the query window, type each command, select the line, then push the execute button on the top-right. Loading Gist. Please Wait... For more details on SingleStore Studio, check out the docs or watch the SingleStore Studio tour video. Note . There are some SingleStore Studio features that don’t make sense in a managed cloud scenario, so they’re absent when Studio is used with SingleStore Managed Service. This is only because those features apply to aspects of SingleStore that are managed for you in SingleStore Managed Service, but that you have to manage yourself (with the help of SingleStore Studio) in self-managed SingleStore software, whether you run it in the cloud or on-premises. This makes your work simpler. Where Can You Go From Here? SingleStore is now ready for all the “kick the tires” tasks you need. You could: Hook up your analytics dashboard to SingleStore, connecting to the cloud’s IP on port 3306. Start your application and connect it to SingleStore using any MySQL connector . (SingleStore has native support for the MySQL wire protocol, easing connectivity.) Create an ingest pipeline from Kafka, S3, or other data source. Being able to run these tasks from such a simple cloud-native setup is a real time-saver. It’s simple to scale this trial up to accomodate large data workloads. But whether it’s a SingleStore Managed Service trial, a Managed Service dev cluster, or a Managed Service production cluster, you get the same always-on, cloud-native experience. Cleanup You’ve finished your experiment today. To stop the database: Login to the customer portal. Click Clusters on the far left. Choose your cluster and click “Delete Cluster”. This will delete the cluster and all the data stored in it. When you’re ready for your next experiment, come here and create a new trial database. Conclusion With SingleStore Managed Service, you can spin up a fully distributed, cloud-native SQL database in minutes. There was no software to install, and no hardware to provision. You saw how easy it is to spin up a cluster, connect to it with SingleStore Studio, and start being productive. You can try SingleStore for free today, using SingleStore Managed Service or self-managed SingleStore software, or contact us for a personalized demo. Now go build great things!", "date": "2020-02-24"},
{"website": "Single-Store", "title": "webinar-recap-1-of-3-migration-strategy-for-moving-operational-databases-to-the-cloud", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/webinar-recap-1-of-3-migration-strategy-for-moving-operational-databases-to-the-cloud/", "abstract": "This webinar describes the benefits and risks of moving operational databases to the cloud. It’s the first webinar in a three part series focused on migrating operational databases to the cloud. Migrating to cloud based operational data infrastructure unlocks a number of key benefits, but it’s also not without risk or complexity. The first session uncovers the motivations and benefits of moving operational data to the cloud and describe the unique challenges of migrating operational databases to the cloud. ( Visit here to view all three webinars and download slides.) About This Webinar Series Today starts the first in a series of three webinars: In this webinar we’ll discuss in broad strokes, migration strategy, cloud migration, and how those strategies are influenced by larger IT transformation or digital transformation strategy. In our next webinar, we’ll go into the next level of details in terms of database migration best practices, where we’ll cover processes and techniques of database migration across any sort of database, really. In the final webinar, we’ll get specific to the technical nuts and bolts of how we do this in migrating to Managed Service, which is SingleStore’s database as a service. In this webinar, we’ll cover the journey to the cloud, a little bit about the current state of enterprise IT landscapes, and some of the challenges and business considerations that go into making a plan, making an assessment, and choosing what kind of workloads to support. Next we’ll get into the different types of data migrations that are typically performed. And some of the questions you need to start asking if you’re at the beginning of this kind of journey. And finally, we’ll get into some specific types of workloads along the way. Any sort of change to a functioning system can invoke fear and dread, especially when it comes to operational databases, which of course process the critical transactions for the business. After all, they’re the lifeblood of the business. And so, we’ll start to peel the onion and break that down a little bit. If you’re just starting your journey to the cloud, you’ve probably done some experimentation, and you’ve spun up some databases of different types in some of the popular cloud vendors. And these cloud providers give guidelines oriented towards the databases and database services that they support. There’s often case studies which relate to transformations or migrations from Web 2.0 companies, companies like Netflix, who famously have moved all of their infrastructure to AWS years ago. But in the enterprise space, there’s a different starting point. That starting point is many years, perhaps decades of lots of different heterogeneous technologies. In regards to databases themselves, a variety of different databases and versions over the years. Some that are mainframe-resident, some from the client-server era, older versions of Oracle and Microsoft SQL, IBM DB2, et cetera. And these databases perform various workloads and may have many application dependencies on them. So, unlike those web 2.0 companies, most enterprises have to start with a really sober inventory analysis to look at what their applications are. They have to look at that application portfolio, understand the interconnections and dependencies among the systems.In the last 10 to 15 years especially, we see the uptake of new varieties of data stores, particularly NoSQL data stores such as Cassandra or key-value stores or in-memory data grids, streaming systems, and the like. Note . See here for SingleStore’s very widely read take on NoSQL . Introduction In companies that have just been started in the last 15, 20 years, you could completely run that business without your own data center. And in that case, your starting point often is a SaaS application for payroll, human resources, et cetera. In addition to new custom apps that you will build, and of course, those will be on some infrastructure or platform as a service (PaaS) provider. So some of this is intentional, and that large enterprises may want to hedge their bet across different providers. And that’s consistent with a traditional IT strategy in the pre-cloud era, where I might have an IBM Unix machine, and then an HP Unix machine, or more recently Red Hat, Linux, and Windows and applications. But these days, it’s seen as the new platform where I want that choice is cloud platforms. Other parts of this are unintentional, like I said, with the lines of business, just adopting SaaS applications. And what you see here on the right, in the bar chart is that the hybrid cloud is growing. And to dig into that a little bit, to see just how much hybrid cloud has grown just from the year prior and 2018, it’s quite dramatic in the uptake of hybrid, and that speaks to the challenge that enterprise IT has, in that legacy systems don’t go away overnight. It’s not surprising that cloud spend is the first thing that sort of bites businesses. And it does have an advantage for experimentation with new applications, new go to markets, especially customer facing applications. Because it’s so easily scalable, you may not be able to predict how popular the mobile app may be, for instance, or your API, or your real-time visualization dashboard. So putting it in an elastic environment makes sense. But the cost may explode pretty quickly as other applications get there too. And with governance and security, I think those are obvious in that when you’re across a multi-cloud environment, you’ve got to either duplicate or integrate those security domains to ensure that you have the right control over your data and your users. There are regulatory things to be concerned about in terms of the privacy of the data, depending on the business, traffic protection of data in the U.S. and California, or in Europe with the general data protection regulation (GDPR). We’re now at a point in the adoption of cloud, that it’s not just sort of SaaS applications and ancillary supporting services around them, but it’s also the core data itself, like the databases service, in particular relational databases. And this might be a surprise given the popularity of NoSQL in recent years, you’ll see that NoSQL databases service are growing, but to lesser extent than relational. And what’s happening across relational data warehousing or OLTP, traditional OLAP, and NoSQL databases, is that there’s been a proliferation of all of these different types. But the power of relational still is what is most useful in many applications. Gartner’s view of this is that just in the next two years that 75% of all databases will be deployed or migrated to a cloud platform. So that’s a lot of growth. That number doesn’t necessarily mean the retirement of existing databases. I think it speaks to the growth of new databases going in the cloud, because launching those new systems is so convenient and so easy, and – for the right kinds of workload – affordable. So at this point, let’s pause and let’s have a question to the audience. So, who is your primary cloud service provider? You see the popular ones listed there. You may have more than one cloud service provider. But what’s your predominant or standard one is what we’re asking here. And we’ll wait for a few moments while responses come in. Okay, this result matches what we’ve seen from other industry reports in terms of the popularity of AWS and then second Azure. Given the time and the market, this isn’t such a surprise. In a year from now, we might see a very different mix with what’s happening with the adoption, uptake of Google and Azure in the different services. So let’s move on. So what are the challenges of database migrations? Within enterprise IT, the first thing that needs to be done is to understand what that application dependency is. And when it comes to a database, you need to understand particularly how the application is using the database. And so just some examples of those dependency points to look for, what are the data types that are going to be used there? Are there bar codes, integers? What’s the distribution of those stored procedures? Although there’s a common language on families of databases, often there are nuances to how what’s available in a stored procedure in terms of processing, so the migration of stored procedures takes some effort. Most traditional SQL databases will provide user-defined functions where a user can extend functions. And then the query language itself in terms of the data manipulation language (DML) for queries, create, update, delete, et cetera. And in terms of the definition of objects in the database, the Data Definition Language (DDL) concerning how tables are created, for instance, and the definition of triggers and stored procedures and constraints. There’s also a hardware dependency to look at for depending on the age of the application, that software might be tied to your particular processor or machine type. And the application itself may only be available on that platform combination. In my own experience, I’ve seen this many times in airlines where the systems for gate and boarding, systems for check in, systems for ground operations, they were written decades ago provided typically by an industry specific technology provider, and they suited the business processes of that airline for many years. But as the airline is looking to do more customer experience interactions and collect data about the customer’s experience from existing touch points like the check-in touch point, the kiosk, the mobile app, but they want to enhance this data. And they want to bring operational data, typically a lot of these operational data systems in logistics and create providers and airlines and other types of operations manufacturing, they don’t lend themselves well to do this. So migrating these applications can be more difficult. Often it’s going to be Agra modernization where you’re just moving off of that platform. Initially, you would integrate with these, and you may store the data that you event out in your targets, new database in the cloud. And finally, there is often a management mismatch of the application. In other words, the configuration of that application as database doesn’t quite fit the infrastructure model of the cloud model that you’re migrating to. The assets aren’t easily divided parametrized and put into your DevOps process and your CI/CD pipeline. Often it’s not easy to containerize. So these are some of the challenges that make it more difficult in enterprise IT context to migrate the applications which of course drag along the databases for these applications. Charlie Feld, a visionary in the area of IT transformation, has his Twelve Timeless Principles: No Blind Spot Outcomes: Business, Architecture, Productivity Zoom Out Progressive Elaboration & Decomposition Systems Thinking The WHO is Where All the Leverage Is 30 Game Changers Functional Excellence is Table Stakes Think Capabilities Architecture Matters Constant Modernization Beachhead  First, Then Accelerate So let’s talk about the phases of migration. So we’ll go into this more in the second webinar, where we talk about best practices, but I’ll summarize them here. Assessing applications and workloads for cloud readiness allows organizations to: Determine what applications and data can – and cannot – be readily moved to a cloud environment What delivery models (public, private, or hybrid) can be supported Which applications you do not want to move to cloud You’ve got to classify these different workloads. So you can look at them in terms of what’s more amenable to the move? How many concurrent users do I expect? Where are they geographically distributed? Can I replicate data across more easily in the cloud to provide that service or without interrupting that service? Do I have new applications and transactions coming online? Perhaps there are more, there are new sensors in IoT, sensors that I need to now bring that data to these applications. So you need to categorize these workloads in terms of the data size, the data frequency, the shape and structure of the data, and look at what kind of compute resources you’re going to need, because it’s going to be a little bit different. Of course, this will require some testing by workload. So at this point, I’d like to pause and ask Alicia have another polling question. So what types of workloads have you migrated to the cloud so far? Given the statistics we see from the surveys, most likely, most of you have done some sort of migration or you’re aware of one in your business and what you’ve done. And you might be embarking on new types of applications in terms of streaming IoT. So roughly a third have not been involved in migration so far. And another third, it’s been analytics and reporting. That result on analytic and reporting, I think is insightful, because when you think about the risks and rewards of migrating workloads, the offline historical reporting infrastructure is the least risky. If you have a business scenario where you’re providing weekly operational reports on revenue or customer churn or marketing effectiveness, and those reports don’t get reviewed perhaps until Monday morning, then you can do the weekly reporting generation over the weekend. If it takes two hours or 10 hours to process the data, it’s not such a big deal. Nobody’s going to look at it until Monday. So there’s a broader array of sort of fallbacks and safety measures. And it’s less time-critical. Those are sort of the easier ones. So 16% of you reported that transactional or operational databases you’re aware of, or you’ve been involved in moving this to the cloud. And that is really what’s happening right now, that we find at SingleStore as well, is that the first wave was this wave of analytical applications, and now recently, you see more of the operational transactions, which is the core part of the business. Here are criteria to choose the right workloads for data migration: Complexity of the application Impact to the business Transactional and application dependencies Benefits of ending support for legacy applications Presence or absence of sensitive data content Likelihood of taking advantage of the cloud’s elasticity What are the most suitable candidates for cloud migration? Here are a few keys: Applications and databases which already need to be modernized, enhanced, or improved to support new requirements or increased demands Consider apps having highly variable throughput Apps used by a broad base of consumers, where you do not know how many users will connect and when Apps that require rapid scaling of resources Development, testing and prototyping of application changes Q&A and Conclusion How do I migrate from Oracle to SingleStore? Well, we’ve done this for several customers. And we have a white paper available online that goes into quite a lot of detail on how to approach that, and have a plan for an Oracle to SingleStore migration. What makes SingleStore good for time series? That’s a whole subject in itself. We’ve got webinars and blog articles available on that. But essentially, I’ll give a few of them here and that SingleStore allows you to first of all ingest that data without blocking for writes; you can do that in parallel often. So if you’re reading from Kafka, for instance, which itself is deployed with multiple brokers and multiple partitions, SingleStore is a distributed database, and you can ingest that time series data in real time and in parallel. So that’s the first point is ingestion. Secondly, we provide time series-specific functions to query that data that allows it for easy convenience, so it’s not necessary to go to a separate, distinct, unique database. Again, SingleStore is a unified converged database that handles relational, analytical, key-value, document, time series, geospatial all in one place. And so it’s suitable to the new cloud native era, where you’re going to have these different data types and access patterns. What is the difference between SingleStore and Amazon Aurora? Yeah, so that question is probably coming because when you’re migrating to a cloud database, typically you’re looking at one of the major cloud providers, AWS or Google Cloud Platform or Microsoft Azure. And each of these providers provides various types of databases. Amazon Aurora is a database built on Postgres, and there’s a version also for MySQL, or at least compatibility in that way that allows you to do that. So it’s worth a look. But what you’ll find when you’re doing sort of high-performance application is that the system architecture of Aurora itself is the biggest Achilles’ heel there, which is it’s composed of the single-node databases of MySQL or Postgres, depending on the edition you’ve chosen, and it’s basically sharding that across multiple instances and providing a shard and middleware layer above that. And that has inefficiencies. It’s going to utilize more cloud resources. And in the beginning that might – at small volumes, that might not manifest into a problem. But when you’re doing this at scale across many applications, and on a bigger basis, those compute resources really add up in terms of the cost. So SingleStore is a much more efficient way, because it was written from the ground up, it’s not built out of some other single-node, traditional SQL database like Aurora. SingleStore’s built from the storage layer all the way up to take advantage of current cloud hardware as well as modern hardware in terms of AVX2 instruction sets and SIMD and, if that’s available, non volatile memory. Secondly, I’d say that Aurora differs in a major way and that it’s oriented to just the transactions, OLTP type processing. Whereas SingleStore does that, but not just that it also has a rowstore with a columnstore, which is what our traditional analytical database like Amazon Redshift has. So, in a way, you could say that with Amazon, you would need two databases to do what SingleStore can do with a single database. We invite you to learn more about SingleStore at singlestore.com or get started with your trial of SingleStore Managed Service .", "date": "2020-02-27"},
{"website": "Single-Store", "title": "webinar-recap-2-of-3-ensuring-a-successful-cloud-data-migration", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/webinar-recap-2-of-3-ensuring-a-successful-cloud-data-migration/", "abstract": "This webinar describes what’s needed to successfully migrate your data to the cloud. What does a good cloud data migration look like? What are the strategic decisions you have to make as to priorities, and what are the challenges you’ll face in carrying out your plans? We will describe a tried and tested approach you can use to get your first wave of applications successfully migrated to the cloud. About This Webinar Series This is the second part of a three-part series . Last week we covered migration strategies; broad-brush business considerations to think about, beyond just the technical lift and shift strategy or data migration. And the business decisions and business strategy to guide you as to picking what sorts of workloads you will migrate. What sorts of new application architectures you might take advantage of. In today’s webinar we’ll go one level deeper. Once those decisions are mapped out, what does a good cloud data migration look like? And in the final session, we’ll go a layer deeper, and we’ll get into migrating a particular source to target database. Challenges of Database Migrations So you’re looking at migrating an on-premises, so-called legacy database. You have particular IT responsibilities that span lots of different areas, lots of different infrastructure, lots of different layers. You’ve got the responsibility of all of these things. But a lot of this work doesn’t provide any differentiation for you in the marketplace or for your business. So when you look at what’s possible in moving to a cloud database, the main thing that you get to take advantage of is a lot of that infrastructure is taken care of for you. And so any cloud database is going to greatly reduce that cost of ownership in terms of the infrastructure management. So the general value proposition of a cloud database, or any SaaS service, is that it allows you to reduce all of this work and focus on the business differentiation of your application. There are still challenges that you have to address in moving to a cloud database. First is the dependencies of applications. So an application may use particular proprietary data types. There may be custom logic and stored procedures, custom functions, etc., and SQL extensions that you’ll need to look at in your initial assessment. But it’s not just the database itself that has to be considered when you’re doing a migration. There’s an ecosystem around the database, tooling and such, doing things such as replication. You may have real-time data integration in and out of your database through ETL, Kafka, middleware products, that sort of thing. You want to look for what visibility you have in terms of monitoring and management. And any sorts of practices or automation that you have around your existing visibility and monitoring. You have to look at that in terms of migration, or redo those processes and techniques and backup and recovery. And you have to discover and set your goals. Ask the right questions: Determine which applications and data can – and cannot – be readily moved to a cloud environment. Identify the workload types – maybe high-frequency transactions on an OLTP type of database, or needs further into the analytics spectrum. Determine which applications you do not want to move to cloud. There’s a data integration process – extract, transform, and load (ETL) – that provides significant latency between when data’s written to the original source and into an analytic store for queries. The trade-off and the cost of this is that you have latency introduced here in the queries to the right. So this might be fine if this is a weekly report. It’s not a report that has to run in a particular timeframe. But, as you move into real-time scenarios, this is what we at SingleStore call operational analytics. There’s no time for ETL etc. anymore. Thus, your reasons for moving to a cloud database. One is to scale further because your data is expected to grow. You also want to avoid the cost of all of that infrastructure. And you want a better consumption model for going forward. Things like Oracle tend to be very expensive. They perform, up to a point, but they’re expensive and difficult to scale. So what about the data itself? What do we think about what to look for here? And there’s four aspects essentially, when you’re looking at the migration from a particular database to a cloud database. And the first is the shape, which follows from what I just discussed in terms of data models. How the data is structured in the schema and what types are used will dictate that shape. if you’ve got row structure data along with JSON you may want to join … It would be easier in your application and simplify your application if you could join the array structure of JSON with rows. If you could do distributed joins across any of the distributed data. SingleStore allows this converged model across these different types of data shapes. So what is the total dataset size? Is it somewhat fixed, or is it something that is unbounded – that you expect to grow on a continual basis? You collect this data because it has consequences to your business operations. How efficient or how well that operation’s doing, and time matters. You want to make decisions in the moment. What’s the subset of the data that’s going to be queried? So it could be terabytes and terabytes, maybe petabytes. We can query that in a way that we can spread data across a distributed database, such as SingleStore, and get parallelism. The goal should be to serve the largest variety of workloads with the least amount of infrastructure. You can do that in a cloud native database like SingleStore Managed Service that handles all of these workloads. So that simplification is really important, especially in the cloud era, when infrastructure in the cloud is so easy to create. With just a push of a button – or not even that, an automated API call. So let’s take a poll of you, the people attending. So overwhelmingly the attendees, you’ve reported that a traditional relational database – Oracle; SQL Server; Aurora, which is either a Postgres or MySQL variant; and MySQL are your most common. And that’s not a surprise; MySQL is the most popular database in the world. Typically we also find that the analytic warehouses are the ones to move first to the cloud because there’s less risk. But what we’re seeing now, and it’s happening in a big way, is the operational database moving to the cloud and handling these cloud-scale, mixed workload things. One pattern that’s worked for SingleStore with our customers, is to take an incremental step. And to split some of the workload from maybe that transactional system that might be handling all of the ingestion of the transactions as well as analytics. If you can have a near-real-time, or with the smallest lag possible, to do analytics against that transaction and scale the concurrency. So that’s a good initial first step, to do a partial migration and replicate the data needed for analytics – provide it to the applications, the data scientists, the analysts. SingleStore, as a distributed database, allows you to handle highly concurrent reads. The nodes of the database for handling those inbound queries can be scaled independently of the data nodes themselves. And so that allows you to do this cost-effectively. If you’re using SingleStore Managed Service for this, it’s as simple as pushing a button and resizing your cluster. But if you have this on-prem, or self-managed SingleStore in the cloud, again you’re scaling the aggregators to allow this concurrency. So this pattern is a good initial first step that helps to minimize risk. Especially, it works when you’ve got really high read to write ratios on this data. In talking to some of our banking customers, sometimes if we’re talking about retail banking and it’s a mobile banking application, that sort of thing. Or the web application, it can be as much as nine-to-one or 10-to-one in terms of reads to writes. Data Migration In moving applications to the cloud, you’re dealing typically with really large amounts of data. And so how you handle the replication itself, for example, should be elastic. Just in the way that a modern database like SingleStore is distributed and SingleStore Managed Service is elastic. You often have massive amounts of data. It could be hundreds of terabytes, from tens to hundreds of databases. So doing it in a serial single fashion can take quite a lot of time. So you want to be able to distribute that migration and replication work and parallelize it as much as possible. SingleStore Replicate is a built in capability of the product. It’s not something separate. It allows you to do this replication from sources such as Oracle, Oracle RAC, SQL Server. And in the future we’ll be doing more with it. Today these are the sources. You’ll find more information about this at docs.singlestore.com . And it supports what I’ve just described earlier. The essential characteristics of reliable cloud data migration, where it comes to the data migration and replication itself. And then it supports the essential elasticity. It’s distributed, it can replicate in parallel. It can recover from checkpoints. So that when you’re moving massive datasets from your operational database, you can get all of that reliably into your target system. So at this point, I’ll pause for a polling question: what types of replication are you using today? Some of this might be dictated by the database ecosystem that your company currently uses. GoldenGate for instance, is part of the Oracle Suite. Informatica is independent. So what types of replication are you using today? That’s an interesting result. This space of data integration and ETL has grown quite a lot in recent years. Especially in the cloud context. There are other cloud native ways such as Google, Alooma to do integrations with their databases. Matillion is another one that comes to mind. So I think this result, to put it in perspective, indicates what we’ve seen in the large amount of options that have grown. And also the fact that there’s so many choices here that it shows how much data migration or database migration to the cloud is happening. I’ll leave you with these three takeaways, and that there’s lots to consider when migrating an operational database to the cloud. First and foremost, assessing the workload. It’s important to understand that you may find different shapes of data, different data types, different data models used. And that no longer are you restricted to having to move in a one-to-one fashion or a one-to-many fashion. In the way that Amazon has moved from Oracle databases and to five or more different types of databases. That’s a complex scenario. And it’s expanded their infrastructure in terms of the variety and types. That’s more complex to manage. So you have to consider, does your business have the staff and skills to manage a growing variety of database types. Or can you look to move to a cloud database that supports these multiple types of models in a converged fashion, as SingleStore Managed Service does. Secondly, migration of a database is not an all-or-none proposition. You can do this in a partial migration using a pattern that we’ve seen successful with our customers. If the workload ratio of reads to writes is very high, and needs high concurrency for reporting, web, mobile access for users, then consider just replicating the data to Managed Service to first provide the analytical workload in the cloud. And then secondly, come back and move the transactional workload. Thirdly, automate as much as possible in the migration process. Tools are a big part of the answer, but they’re not the only part. And as I said, no matter what source or target database you’re moving from or moving to, the stored procedures, procedural languages is where you should expect some manual work. Even if you’re moving from one version of MySQL to another version of MySQL or one version of PostgreSQL to another, you’ll have those problems. For the next session, the final session of the series, I’ll be talking about database migration best practices. And so I’ll get into one more level of technical specifics of how you do this with SingleStore and replicate, as I showed you. So you have something concrete in terms of seeing how this process is done. Q&A and Conclusion Have any SingleStore customers migrated from Oracle? Yes. I would say that’s the most common. Oracle has a very large footprint in the enterprise. And we’ve migrated from Oracle RAC and Oracle DB and Oracle Exadata. Not only Oracle though, we’ve also migrated from Microsoft SQL Server, SAP HANA. And then newer types of databases as well, such as migrations from Snowflake to SingleStore and SAP HANA to SingleStore. And in the Snowflake example, it’s often because there’s a need for lower latency for operational analytics, including greater concurrency. And the way that SingleStore was built allows for that low latency, real-time result – an HTAP, hybrid transactional/analytical process use case. Besides replicate, what are ways to get data into SingleStore? A real-time way to get data in is through SingleStore Pipelines. So that is an integration capability built into this product that allows you to subscribe to Kafka topics. You can do ingestion in parallel. If you’ve got multiple Kafka partitions, then those can map to the distributed partitions of SingleStore Managed Service, such that you can ingest in parallel. It’s a little bit different of an integration strategy, because you’re not getting all of the guarantees that I just described in replication through SingleStore Replicate. But it is an initial way to get data in. And then finally a bulk load of data from a flat file source, CSV, Hadoop, S3 buckets. What I would call data at rest, or static data. You can bulk load in that way. Can I manage SingleStore in my own cloud environment? Yes. So Managed Service provides SingleStore as a database as a service. But with SingleStore Managed Service, you’re not managing any of the infrastructure. You can also deploy SingleStore in a cloud environment and manage it yourself. We make that easy because we provide the Docker image and the Kubernetes Operator. Such that if that’s the fashion that you’re doing it in your VPC, it’s fairly easy to do. Or you can do it with just the native binaries and install it in your cloud environment yourself. You’ll also find it on the Amazon marketplace, and you can try out setting that up in Amazon. You can try SingleStore for free – either SingleStore Managed Service, or the SingleStore software that you manage yourself – or contact SingleStore today .", "date": "2020-02-28"},
{"website": "Single-Store", "title": "dump-oracle", "author": ["Peter Guagenti"], "link": "https://www.singlestore.com/blog/dump-oracle/", "abstract": "“Breakup Day” is February 21st. In the spirit of moving on from unhealthy relationships, we decided to write our very own “Dear John” letter to the vendor we hear our customers complain the most about… or in this case, a “Dear Larry” letter. We hope this gives anyone who’s feeling taken advantage of by their behemoth vendor the courage to sever ties. Cheers. Dear Larry, Let’s be honest – this relationship is not working. I’m ditching you for someone who treats me like I matter…  and who doesn’t take all of my money! You’ve been mistreating me for years, Larry. And to say that you’ve been painfully slow to react to my wishes, inflexible as a “partner”, and unwilling to evolve to meet my changing needs, is a huge understatement. There is no one who makes me feel as taken advantage of as you do. Now that there are clearly so many better options for me, I finally have the courage and conviction to leave. So why is all of my data packed up and ready to go? You deserve to know: You’re the definition of “legacy,” and you can’t easily support the kind of diversity I need. How difficult and expensive you make it to work with mixed workloads, mixed data types, and streaming systems in this day and age is a joke. You might think having lots of expensive, specialized tools makes you desirable, but trust me – it’s the opposite. I don’t want to have to somehow figure out how to stitch everything together and make it work. The things I’m trying to do are already a challenge, without you making everything more complicated. Then there’s your performance and scale limitations. You were fast, in your day, but software has evolved! I need speed and scale, without having to give up an arm and a leg for it. The new database companies give me the performance I need, without compromises. And they do it at a fraction of what you charge me. Not to mention I can run them anywhere! And, no, don’t try to sell me on Exadata being the answer to these problems. We both know that it’s too little, too late, too limited. Oh, and too expensive, of course. Most importantly, I need someone who supports me and behaves like a real partner. Who understands where the world is going, and appreciates what I’m going through. A vendor who is flexible to suit me, and doesn’t expect me to flex to do things their way. All that being said, I also want to thank you, Larry. Seriously. It’s really hard to make the first move, and you served us well in your time. You came along when there was pretty much no other game in town. It’s next to impossible to be first and get everything right from the get-go. So, props to you for putting yourself out there and using the best available resources at the time to just plain make things happen. We all thank you for what you helped us do for many years. Our relationship was an important chapter in my life, Larry, but I need to move on. I have new needs, and I must embrace the modern world we’re living in. I need to move fast – like, real-time fast – and I want someone who is there at my side and helps me try new and exciting things. I need real speed and elastic scale in my life, not to mention someone who is all-in on the cloud, and not a Johnny Come Lately (and Reluctantly and Cost-Prohibitively, I might add). Frankly, Larry, I need a partner who understands where I’m going, and who is heading there with me… I’m through waiting for you to change – beyond the superficial fixes which you claim are game-changing, but we both know they’re just hot air. Expensive hot air, at that. It’s time to face reality, Larry: the world is leaving you behind, and so am I. You had a million chances, and you blew them all. I’m taking up with the NewSQL crowd. They get me. And we’re about to make beautiful, data-driven music together. Good-bye, A soon-to-be-former-customer P.S. If you want to see what a “healthy” database vendor relationship looks like, with a more focused and committed partner, check out our Oracle vs. SingleStore comparison or contact us for a personalized demo!", "date": "2020-02-20"},
{"website": "Single-Store", "title": "webinar-recap-memsql-helios-technical-overview", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/webinar-recap-memsql-helios-technical-overview/", "abstract": "Solutions Engineer Mihir Bhojani presents a 20-minute technical overview of SingleStore Managed Service . In this webinar recap, we present the key points of the interview, and give you a chance to review them at your leisure. You can also view the SingleStore Managed Service Technical Overview . In this webinar, we’ll cover what exactly SingleStore Managed Service is and how it compares with self-managed SingleStore, which you download yourself, provision, and run in the cloud or on-premises. With SingleStore Managed Service, SingleStore provisions the hardware in the cloud and runs SingleStore itself; you just set up tables and manage your data. After describing Managed Service, I’ll have a hands-on demo that will show you the whole end-to-end process of getting started with Managed Service. Managed Service is basically the SingleStore product that you’ve known and you have used before, except that it’s a fully managed service. We offer it on an on-demand model so you can spin up clusters on the fly temporarily for a few hours, or keep them long-running. SingleStore Managed Service is also an elastic database, because you’ll be able to grow your cluster or shrink on the fly, and also on demand. All of this is done online, so there’s no downtime when you scale out or scale down. With Managed Service, we here at SingleStore take care of your cluster provisioning and software deployment. We’ll do biweekly maintenance upgrades on your cluster. And ongoing management and operations of your cluster will be handled by SingleStore experts. That leaves you, as the user, to be only responsible for logical management of your data, helping you to keep your focus on application development. With SingleStore Managed Service, you get effortless deployment and elastic scale, so you can have a cluster up and running in five minutes. We’re going to demonstrate that today. You have superior TCO when you compare to legacy databases, so you can do a lot more with Managed Service with a lot less hardware allocated. You have multi-cloud and hybrid flexibility, so you have your pick when it comes to which cloud provider and which region you want to deploy in. Currently SingleStore Managed Service is available on AWS and GCP; we have multiple global regions in both of those platforms, and we have Azure support coming in the next five to six months. Let’s talk about how Managed Service works under the hood, then we can jump into the demo. Managed Service is facilitated by, and built on, Kubernetes. ( Using the beta SingleStore Kubernetes Operator – Ed. ) This helps us enable unique features like auto-healing, handling node failures, and also lets us enable features like auto-scaling and to do rolling online upgrades. Because SingleStore is inherently a distributed system, it’s really important that there’s high availability at all times. For storage, SingleStore makes use of basically the block storage of your cloud provider. We choose optimal instance type and node configuration when you start creating the cluster. High availability (HA) is transparently built in. So you will have HA as soon as you spin up the cluster. The way HA works in SingleStore is that leaf nodes are basically paired up. Leaf nodes are basically data nodes, and you have data being duplicated from one leaf node to another and vice versa. So if one leaf node goes down, the other one can keep the cluster up and live. Security is also enabled by default. The way we do that is we have in-flight encryption, so we encrypt your connections using TLS in-flight. Then we also have at-rest encryption. We use your cloud provider’s encryption services to do encryption. Let’s see Managed Service in action. I want to outline what exactly we’ll be seeing today. The use case for today that we’re going to handle, and this is a real-life use case that we have companies using SingleStore for, is ad tech. So this data set is a digital advertising data example, which basically drills down and gets data from a traditional funnel. So we’re going to see multiple different advertisers and campaigns and we’re going to see what kind of engagements and events they are generating. We’re going to facilitate that by creating a cluster from scratch. We’ll create the database and tables from scratch as well. And then we’ll create a pipeline to start ingesting data from an existing Kafka topic. Then finally, we’ll access all of this data from Managed Service via a business intelligence (BI) dashboard. Let’s get started. So if you go to singlestore.com/managed-service-trial/, you’ll be able to spin up with $500 of free credits. That’s the amount of time that you get to spin up a cluster as a trial. So if you go to portal.singlestore.com and you go to the Clusters tab, you’ll see a create button and when you click on that, you’ll be brought to this screen. I’m just going to name this Managed Service-webinar. You’ll have your pick when it comes to which cloud provider and region you want to be in. And we’re always adding new regions as well. So right now I’ll leave it at AWS Virginia. Cluster size – so, because SingleStore is a distributed system in this case, when we refer to units, we’re really talking about leaf nodes. (Leaf nodes hold data. They are accompanied by one or more aggregator nodes, which hold schema and which dispatch queries to the leaves – Ed.) One unit in SingleStore Managed Service is actually eight VCPUs, 64GB of RAM and one terabyte storage. You can only purchase units in pairs, so in increments of two essentially. For now we’re going to leave this at four, so it’s a modest-sized cluster. We’ll generate a password and make sure to copy it. Then you can also configure cluster access restrictions. So if you want to white-list specific IP ranges, we absolutely recommend that you do that. For now, I’ll just hide my own IP address. Then, as an advanced setting, you can also configure if you want to automatically expire this cluster after a certain number of hours. Once we click on Create cluster, we’ll see that the cluster will start reconciling resources. What it means, it’s really trying to go to AWS and using Kubernetes, it’s going to spin up a cluster on the fly. This process is expected to take about five minutes. In the interest of saving time, I already have a cluster spun up before the webinar, so I’m just going to use that going forward for this webinar. It’s essentially the same size cluster as I created before, and once it’s up and running, you’ll see something like this on your screen, where on the right side you’ll have all the connection information. The SingleStore Studio that we’re going to tour just in a second here is basically the GUI interface for interacting with SingleStore Managed Service. But we also get these endpoints where if you want to connect from your own applications, your own BI tools using JDBC/ODBC, you can basically use these endpoints to connect to the SingleStore cluster. For now, let’s just go into SingleStore Studio. Here, we’ll enter the password that we generated earlier. As soon as you log in you’ll see a screen, something like this, which is basically a dashboard that shows you what current health and usage looks like on your cluster. As we can see, there’s currently no pipelines, no data, nothing. The reason why it says seven nodes is because when we chose four units, when spinning up the cluster, so we got four leaf nodes as expected; the rest of the three nodes are basically aggregator notes. So you get one master agg and two child agg, plus the leaf nodes. All of those are obviously load balanced. Let’s start creating some schemas. So here we have this sample use case basically. Here is a sample schema that we’re going to use. I’m just going to explain what’s happening here. So here we’re going to create a database and then two tables. The first table is the events table and this is basically the fact table, right? So this is where all the raw events from the Kafka topic are going to stream into. This is a columnstore table. So when you create tables in SingleStore, you have two options. You have rowstore and columnstore. The default table type in SingleStore is rowstore. So if you don’t specify, then by default you’ll get a rowstore table, which means all your data will go in memory. But if you do specify a key using cluster columnstore, then basically you get a columnstore table, where your data is actually on disk. We use memory to optimize performance, and we store indexes of metadata in memory. Columnstore is definitely recommended for OLAP (online analytical processing) workloads and rowstore is recommended for OLTP (online transaction processing) workloads. Then the second table here is campaigns, which is basically just a lookup table. This table is that we’re going to join when we need to look up specific campaigns. That’s why we’re creating it as a reference table. Reference tables actually store the entire copy of the table on every single leaf node, so it really helps in situations where you need to join this table often. You just need to be careful that this table is a good (small) size to be able to fit into every single node. Then we’re going to also populate the campaigns table with some static values. We have about 14 campaigns that we’re going to populate to begin with. Let’s run everything. This should take about eight to nine seconds. Now that we have everything running, we have the ad tech database now, and then we have two tables inside this database. The next step is, we have this Kafka topic here, it’s a Kafka cluster that we’re hosting in-house. How do we get that data into the SingleStore Managed Service cluster? That’s where the pipelines feature comes in. The pipelines feature is something that can connect to Kafka obviously, but it can also connect to other sources like AWS S3, Azure blobstore, HDFS. If you have data sitting in any of those places, then you can always use a pipeline to natively stream data directly to SingleStore. For now we’re going to use Kafka, as it’s one of our most-used technologies. You can also specify a batch interval, how often you want to batch data into SingleStore. So right now it’s at 2,500 milliseconds or 2.5 seconds. I’m just going to make this 100 milliseconds and see what kind of results we get. We’re going to obviously send all the data into the events table that we created earlier. Then here are just all the fields that we want to populate coming in from the topic. This pipeline is created. But if we go back to the dashboard, we’ll see that there’s no data being actually written yet because we haven’t started it. I just need to go back to the SQL editor and then alter the pipeline offsets so that we only get data basically from the latest offset. Then I’m going to finally start the pipeline. Now if I go back to the dashboard, we should see rows being written at a steady pace into SingleStore. Just to reiterate, this demo is not to show you any performance aspects of SingleStore, it’s basically just a functional demo. SingleStore Managed Service can handle very high throughputs when it comes to ingestion and also query performance. Now that we have data flowing in, let’s start running some queries. Here’s some prewritten SQL queries that we have written. Let’s explore what we have so far. Then like I mentioned, we’ll go over to a BI tool to see how this data looks in a dashboard format. For a simple question, how many events have we processed so far? That number right now is 51,000 and every time you run this, we expect it to go up a few thousands. So if I ran it again, it should go up. So right now it’s at 57,000 and if I ran it again, it’s at 60,000. What campaigns are we running? This is the lookup table that we populated earlier. These are all the 14 campaigns that we are running right now. Then here’s the traditional funnel query for analysts to see broken everything down by campaign, how many impressions, clicks and downstreams they’re getting per campaign. Let’s see how this data looks like in a Looker dashboard. Now Looker is a BI tool that we partner with and SingleStore works, integrates with almost all the BI tools out there. We are also MySQL compatible. If there’s a tool out there that does not have a SingleStore connector, you can always leverage the MySQL connector to connect to us. This is the dashboard that’s basically reading off of the Managed Service cluster that we just created. This number of ad events should match up with the number here. So yes, it’s 101. If I go back here, it should be close to that number. All right. This is basically just the raw number that’s coming into the SingleStore cluster right now. Here we just have some charts that are… By the way, all of them are updating in real time. So this is a dashboard that’s refreshing every one seconds. If I just click on edit and go to settings – yep, there it is. So all these charts are set to auto-refresh every one second. Now almost all BI tools have that option, but they’re almost never used, because it’s really important for the underlying database to be able to handle a number of queries coming in at a steady pace and be able to respond back to all those queries in time so that the dashboard doesn’t choke up. We can see here that Managed Service and SingleStore are able to do just that. Here we can see we have a number of ad events by top five advertisers. Here it’s broken down, ad events by region and obviously which companies. Then here is just the raw table of ad events as they are coming in. It’s really important in SingleStore that we emphasize the time that goes from an event to happen to insight, to someone to analyze it. It’s extremely short. Basically in the last 10 minutes or so, we basically created a cluster. We created databases. We even created a pipeline and then we started flowing the data in. And now we have a dashboard that we’re able to get insight from. And not only that, now that we have the insight, we can now take the appropriate action. Even though this use case focuses basically on a lot of the ad tech aspects, it’s really important to know that we have done this for multiple industries, whether that’s IoT, financial services… We work with a variety of customers out there. The real value of SingleStore here is that you basically get the best of all worlds when it comes to speed, scale, and SQL. With speed, that could mean the time it takes to actually spin up a cluster in a distributed system that’s complex to set up. Now we are able to get it up and running in less than five minutes. Or it could be the ultra-fast ingest and the high performance you get when it comes to querying your data. The scale-out mechanism of SingleStore is obviously really unique, the way that you can always expand your aggregator or leaf nodes. Then, in Managed Service, it’s even easier now, because all it takes is one click of a button if you want to expand your cluster or shrink down. And all of those things are facilitated using SQL. It’s really important to the companies that we work with and the analysts that we work with that they’re able to continue using SQL, because it is a powerful analytical language and you basically get the best of all worlds when it comes to using SingleStore. You still get to use SQL, but you get all the benefits of the speed and the scalability that you traditionally get only with NoSQL databases. (Because previously, those were the only databases that were scalable; now, NewSQL databases combine scalability and SQL. See our popular blog post on NoSQL . – Ed) Now we basically combine all three and give it to you in one package. There’s also another use case that I wanted to cover. Medaxion is a company that I also personally work with. Medaxion is basically a medical analytics platform. They provide a platform for anesthesiologists to analyze medical data. It’s really important for them to have a 24/7, always-on analytics solution so that they can help and change lives. Before SingleStore, they had some challenges – pretty simple challenges and fair challenges as well, that they weren’t able to get – their event to insight time was way too slow. Partly because of the legacy systems or single-box systems that are not able to keep up with the growth and demand that a lot of the startups face from time to time. In this case, MySQL couldn’t scale, and wasn’t able to fit their needs, and that was leading to a poor customer experience. As a result of that, Medaxion couldn’t help the anesthesiologists be at the right place at the right time, and also automate their work as much as possible. When Medaxion came to us, they had these three requirements – of many more, but these three were the main requirements. They wanted something that was fast and they wanted something that had SQL. They wanted a database that could work natively with the Looker platform that we just saw. That’s what Medaxion uses as well. And they wanted to be able to write ad hoc SQL queries. So they wanted something that could be scalable, that could have really high performance when it comes to ingest and querying, and also be accessible using SQL. They wanted a managed service. Now just like all other startups out there, there’s a lot of operational complexity when it comes to managing a distributed system. Of course when it’s a database that’s mission critical to your application, then it does have some complexity related. They were very clear that they wanted a managed service because they didn’t want to add additional personnel just to handle the operations of a system like SingleStore. They wanted something obviously that had reliable performance, so something that was running 24/7, and no downtime whatsoever. When they switched to SingleStore, these were the results that we got. We got really fast and scalable SQL, basically exactly what they wanted, and now they have dramatic performance gains and no learning curve. This boils down to the second point, we’re MySQL-compatible. There was a very low amount of work and risk involved when it comes to switching from a system like MySQL to SingleStore because we’re not only wire protocol-compliant, you’re already familiar with the language, and you’re already familiar with the tools that integrate with those technologies. With SingleStore Managed Service, now they can just eliminate any operational headaches. They don’t have to worry about upgrading every two weeks. They don’t have to worry about high configuring, high availability, encryption, load balancing – a lot of those things are now taken care of by us and SingleStore experts. That leaves you, and in this case Medaxion, to just focus on development tasks. As a result of that, now they have near-real-time results. And they have what the CTO of Medaxion calls, “The best analytics platform in the healthcare industry.” They said it’s facilitated because of SingleStore. I just want to thank you guys for your time today. If you do want to spin up your own Managed Service trial, please feel free to go to singlestore.com/managed-service . All our trials are 48 hours, giving you more time to ingest your data, more time to test your queries. Of course, if you have any questions about pricing and billing, please reach out to us at Managed Service@SingleStoreDB.com . Q&A What is the uptime SLA guaranteed on Managed Service? That’s a good question. Uptime SLA on Managed Service, we’re guaranteeing three nines of uptime, so 99.9%. If the underlying storage is in S3, then storage will be cheaper. That’s a good question. Right now you can’t have underlying storage be S3. You basically do have the ability to ingest data directly from S3, but the data does need to be within the SingleStore cluster. How about the Kinesis pipeline? Right now we only support Kafka pipelines, and Kinesis is something that would take some custom work. We do have some custom scripts available if you do want to ingest data from Kinesis, but right now natively we only connect to Kafka. How does backup/restore work on Managed Service? We will take daily backups on Managed Service for you and we’ll retain a copy of that backup for seven days. So you can always request a restore from a particular day in the last week or so. But you also have the ability to kick off your own backups whenever you desire. So you can always back up into your S3 bucket, or your Azure blobstore bucket, and then you can always restore from those technologies as well.", "date": "2020-03-05"},
{"website": "Single-Store", "title": "webinar-recap-3-of-3-best-practices-for-migrating-your-database-to-the-cloud", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/webinar-recap-3-of-3-best-practices-for-migrating-your-database-to-the-cloud/", "abstract": "This webinar concludes our three-part series on cloud data migration. In this session, Domenic Ravita actually breaks down the steps of actually doing the migration, including all the key things you have to do to prepare and guard against problems. Domenic then demonstrates part of the actual data migration process, using SingleStore tools to move data into SingleStore Managed Service. About This Webinar Series This is the third part of a three-part series. First, we had a session on migration strategies ; broad-brush business considerations to think about, beyond just the technical lift and shift strategy or data migration. And the business decisions and business strategy to guide you as to picking what sorts of workloads you will migrate, as well as the different kinds of new application architectures you might take advantage of. And then, last week, we got down to the next layer, talked about ensuring a successful migration to the cloud of apps and databases in general. In today’s webinar we’ll talk about more of the actual migration process itself. We’ll go into a little bit of more detail in terms of what to consider with the data definition language, queries, DML, that sort of thing. And then I’ll cover one aspect of that, which is the change data capture (CDC) or replication from Oracle to SingleStore, and show you what that looks like. Database Migration Best Practices I’ll talk about the process itself here in terms of what to look at, the basic steps that you are going to be performing, what are the key considerations in each of those steps. Then we’ll get into more specifics of what a migration to Managed Service looks like and then I’ll give some customer story examples to wrap up, and we’ll follow this with a Q and A. So the process that we covered in the last session has mainly these key steps, that we’re going to set the goals based on that business strategy in terms of timelines, which applications and databases are going to move, considering what types of deployment you’re going to have, and what’s the target environment. We do all this because it’s not just the database that’s being moved, it’s the whole ecosystem around the database. So connections for integration processes, the ETL processes from your operational database to your data warehouse or analytic stores. As well as where you’ve got multiple applications sharing a database, understanding what that is and what the new environment is going to look like. Whether that’s going to be a similar application use of the database or if you’re going to redefine or refactor or modernize that application, perhaps splitting a monolith application to microservices for instance. Then that’s going to have an effect on what your data management design is going to be. So just drilling into step three there, for migration, that’s the part we’ll cover in today’s session. Within that, you’re going to look at specific assessment of the workloads and you’re going to look, what sorts of datasets to return, what tables are hit, what’s the frequency, the concurrency of these? This will help in capacity sizing, it’ll also help in understanding which functions of the source database are being used in terms of features and capabilities, such as stored procedures. And once you do that assessment, and there are automated tools to do this, you’ll look at planning that schema migration of step two. And the schema migration involves the table definitions but also all sorts of other objects in the database that you’ll be prepared to adapt. Some will be one-to-one – it depends on your type of migration – and then the initial load, and then continuing to that replication with a CDC process. So, let’s take the first one here in terms of the assessment of the workloads, what you want to consider here. And when you think about best practices for this, you want to think about how are applications using the database? Are they using it in a shared manner? And then specifically, what are they using from the database? So for instance, you may need to determine what SQL queries, for instance, are executed by which application so that you can do the sequencing of the migration appropriately. So finding that dependency, first of which applications use the database, and then more fine-grained, what are the objects, use of stored procedures, etc., and tables. And then finally, what are the specific queries? So one strategy or tactic, I would say, that’s helpful in understanding that use by the application is to find a way to intercept the SQL queries that are being executed by those applications. So, if this is a job application, if you wrapped the connection object when the Java connection object is being created, and also the object for the dynamic SQL, then you can use this kind of wrapper to collect metrics and information and capture the query itself so that you have specific data on how applications use the database, which tables, which queries, how often they’re fired. And you could do this in other languages as long as you’re… it’s for that client library, whether it’s an ODBC, JDBC, et cetera. This technique helps to build a data set as you assess the workload to get really precise information about what the queries are executed and what objects. And then secondly, when you have that data, you’ll find that the next thing that you want to do is to look at the table dependencies. So again, if this is an operational database that you’re migrating, then it’s typical that you might have an ETL process that keys off of one-to-many tables to replicate that data into a historical store, a data warehouse, a data mart, etc. And so, understanding what those export and/or ETL processes are, on which tables they depend, is fairly key here. These are just two examples of the kinds of things that you want to look at for the workload. And of course with the first one, once you have the queries, and you can see what tables are involved, you can get runtime performance on that, you can have a baseline for what you want to see in the target system, once the application and the database and the supporting system pipelines have been migrated. So now let’s talk a little bit about schema migration. And there’s a lot involved in schema migration because we’re talking about all the different objects in the database, the tables, integrity constraints, indexes, et cetera. But we could sort of group this into a couple of broad areas, the first being the data definition language (DDL) and getting a mapping from your source database to your target. In previous sessions in this series we talked about the migration type or database type, whether it’s homogeneous or heterogeneous. Homogeneous is like for like, you’re migrating from a relational database source to the same version even, of a relational database, just in some other location – in a cloud environment or some other data center. That’s fairly straightforward and simple. Often the database itself provides out-of-the box tools for that sort of a migration and replication. When you’re moving from a relational database to another relational database, but from a different vendor, that’s when you’re going to have a more of an impedance mismatch of some of the implementations of DDL, for instance. You’ll find many of the same constructs because they’re both relational databases. But despite decades of so-called standards, there’s going to be variation… there are going to be some specific things for each vendor database. So for instance, if you’re migrating from Oracle to SingleStore, as far as data types, you’ll find a pretty close match from an Oracle varchar2 to a SingleStore varchar, from a nvarchar2 to SingleStore varchar, from an Oracle FLOAT to SingleStore decimal. Those are just some examples, and we have a white paper that describes this in detail and gives you those mappings, such that you can use automated tools to do much of this schema migration as far as the data types and the table definitions, etc. After the data types, the next thing that you would be looking at would be queries and the data manipulation language (DML). So, when you look at queries, you’ll be thinking, “Okay, what are the different sorts of query structures? What are the operators in the expression language of my source, and how do they map to the target?” So, how can I rewrite the queries from the source to be valid in the target source? Again, you’re going to look at the particular syntax around, for instance, outer join syntax, do I have recursive queries? Again, just using Oracle as an example, SingleStore has a fairly clear correspondence of those capabilities from relational data stores like Oracle, PostgreSQL, mySQL, etc., and SingleStore. If your source is a mySQL database, you’ll find that the client libraries can be used directly in SingleStore because our client bindings are SingleStore wire protocol compliant. So you can use basically any driver, any client driver, from SingleStore in the hundreds that are available throughout every programming language into SingleStore, so that simplifies a little bit of some of your testing in that particular case. The third thing I’d point out here is that while you may be migrating from a relational database to another relational database, and you may still consider this, or you should consider this, a heterogeneous move, because the architecture of the source databases often, almost always these days, a legacy single-node type of database. Meaning that it’s built on a disk-first architecture, it’s meant to scale vertically, meaning a single machine to get more performance, you scale up, you get a bigger hardware with more CPU processors. And when you’re coming to SingleStore, you can run it as a single node, but the power of SingleStore is that it’s a scale-out distributed database, such that you can grow the database to the size of your growing dataset with simply adding nodes to the SingleStore cluster. SingleStore is distributed by default, or distributed native you might say, and that’s what also is one of the attributes that makes it a good cloud-native database with Managed Service, and that allows us to elastically scale that cluster for Managed Service, scale up and down, and I’ll come back to that in a moment. But as part of that, when you think about the mapping, the structure of a source relational to a target like Managed Service, you’re mapping a single-node database to a distributed one. So there’s a few extra things to consider, like the sharding of data, or some people call this partitioning or distributing the data, across the cluster. The benefit of that is that you get resiliency, in the case of node failures you don’t lose data, but you also get to leverage the processing power of multiple machines in parallel. And this helps when you’re doing things like real-time raw data ingestion from Kafka pipelines and other sources like that. This is described in more detail in our white paper, which I’ll point out in just a moment. So once you’ve got those things mapped, and you may be using an automated tool to do some of that schema mapping, you’ll have to think about the initial load. And this, depending on your initial dataset, could take some amount of time, a significant amount of time, just when you consider the size of the source dataset, the network link across which data must move, what’s the bandwidth of that link? And so if you’re planning a migration cut-over, like over a weekend time, you’ll want to estimate based on those things. And what’s the initial load going to be, and by when will that initial data load complete, such that you can plan the new transactions start of the replicating of the new data. And also when you’re doing the load, what other sorts of data prep needs to happen in terms of ensuring that the integrity constraints and other things like that are working correctly. I’ll touch a little bit about how we address that through parallelism. So finally, once that initial load is done, then you’re looking to see how you can keep up with new transactions that are written to the source database. So you’re replicating, you’ve got a snapshot for the initial load, and now you’re replicating from a point in time, doing what’s called change data capture (CDC). As the data is written you want the minimal latency possible to move and replicate – copy – that data to the target system. And there are various tools on the market to do this. Generally you want this to have certain capabilities such as, you should expect some sort of failure. And so you need sort of some checkpoint in here so you don’t have to start from the very beginning. Again, this could be tens, hundreds of terabytes in size if this is an operational database, or an analytic database, it’s going to have more data if it’s been used over time. Or, if it’s multiple databases, each may be a small amount of data, but together you have got a lot in process at the same time. So you want to have your replication such that it can be done in parallel and you have checkpointing to restart from the point of failure rather than the very beginning. And then finally, data validation and repair. With these different objects in a source and a target, there’s room for error here, and you’ve got to have a way to automatically validate and run tests against the data that are valid, you want to think about automating that. And as much as possible in testing, doing your initial load, you want to validate data there before starting to replicate; as data’s replicating you’re going to have a series of ongoing validations to ensure that you’re not mismatching or your logic is not incorrect. Let’s go to our first polling question. You’re attending this webinar probably because you’ve got cloud database migration on your mind. Tell us when you are planning a database migration. Coming up in the next few weeks, in which case you would have done a lot of this planning and testing already. Or maybe in the next few months, and you’re trying to find the right target database. Or maybe it’s later in this year or next year, and maybe you’re still in the migration strategy, business planning effort. Okay. Most of you are not in the planning phase yet, so you’re looking to maybe see what’s possible. You might be looking to see what target databases are available, and what you might be able to do there. We hope you take a look at what you might do with Managed Service in the Cloud. We’ll talk about moving workloads to Managed Service. Managed Service is SingleStore’s database as a service. Managed Service is, at its essence, the same code base is as SingleStore, self-managed, as we call it, but it’s provided as a service such that you don’t have to do any of your own work on the infrastructure management. It takes all the non-differentiated heavy lifting away, such that you can focus just on the definition of your data. Like SingleStore self-managed, SingleStore Managed Service provides a way to do multiple workloads of analytics with transaction simultaneously on the same database or multiple databases in a Managed Service cluster. You can run Managed Service in all the major cloud providers, AWS, GCP, Azure is coming soon, in multiple regions. Looking at some of the key considerations in moving to Managed Service … I mentioned before this identification of the database type source and target. There’s multiple permutations of what to consider like for like with homogenous. Although it may be a relational to relational database, as the examples I just provided with, say, Oracle and SingleStore, there are still some details to be aware of. The white paper we provide gives a lot of that guidance on the mapping. There are things that you can take advantage of in Managed Service that are just not available or not as accessible in Oracle. Again, that’s things like the combination of large, transactional workloads simultaneously with the analytical workloads. Next thing is the application architecture. I mentioned this earlier. Are you moving? Is your application architecture going to stay the same? Most likely it’s going to change in some way, because when these migrations are done for a business, typically they’re making selections in the application portfolio for new replacement apps, SaaS applications often, to replace on-prem applications. A product life cycle management, a PLM system on prem, often is not carried on into the cloud environment. Some SaaS cloud provider is used, but you still have the integrations that need to be done. There could be analytical databases that need to pull from that PLM system, but now they’re going to be in the cloud environment. Looking at, what are the selections and the application portfolio or the application rationalization, as many people may think about it? As to what that means for the database. Then for any particular app, if it’s going to be refactored from a monolith to microservices-based, what does that mean for the database? Our view in terms of SingleStore for use in microservices architectures is that you can have a level of independence of the service to the database, yet keep the infrastructure as simple as possible. We live in an era where it’s really convenient to spin up lots of different databases really easily, but even when they’re in the cloud, those are more pieces of infrastructure that you now have to manage the the life cycle of. As much as possible you should try to minimize the amount of cloud infrastructure that you have to manage. Not just in the number of instances of database, but also the variety of types. Our view of purpose-built databases and microservices is that you can have the best of purpose-built, which is support for different data structures and data access methods, such as having a document store, and geospatial data, full-text search with relational, with transactions, analytics, all living together without having to have the complexity of your application to communicate with different types of databases, different instances, to get that work done. Previously in the industry, and part of the reason why purpose-built databases caught on, is that they provided a flexibility to start simply, such as document database, and then grow and expand quickly. Now we, as an industry, have gone to the extreme of that where there’s an explosion of different types of sources. To get a handle on that complexity, we’ve got to simplify and bring that back in. SingleStore provides all of those functions I just described in a single service, and Managed Service does that as well. You can still choose to segment by different instances of databases in the Managed Service cluster, yet you have the same database type, and you can handle these different types of workloads. For a microservices-based architecture, it’s giving you the best of both worlds; the best of the purpose-built polyglot persistence NoSQL sorts of capabilities and scale out, but with the benefits of robust ANSI SQL and relational joints. Finally, the third point here is optimizing the migration. As I said, with huge datasets, the business needs continuity during that cutover time. You’ve got to maintain service availability during the cutover. The data needs to be consistent, and the time itself needs to be minimized on that cutover. Let me give a run through some of the advantages of moving to Managed Service. As I said, it’s a fully managed cloud database as a service, and, as you would expect, you can elastically scale up a SingleStore cluster and scale it down. Scaling down is also maybe even perhaps the more important thing, because if you have a cyclical or seasonal type of business like retail, then there’ll be a peak towards the end of the year, typically Thanksgiving, Christmas, holiday seasons. That infrastructure, you’ll want to be able to match to the demand without having to have full peak load provisioned for the whole year. Of course cloud computing, this is one of the major benefits of it. But, your database has to be able to take advantage of that. Managed Service does that through, again, its distributed nature. If you’re interested in how this works exactly, go to the SingleStore YouTube channel. You’ll see quick tutorials on how to spin up a Managed Service cluster and resize it. The example there shows growing the cluster. Then once that’s done, it rebalances the data, but you can also size that cluster back down. As I mentioned, it eliminates a lot of infrastructure and operations management. It gives you some predictability in costs. With Managed Service, without going into the full pricing of Managed Service, basically our pricing is structured around units or nodes. Those nodes, or resources, are described by computing resources in terms of how many CPUs, how much RAM. Eight virtual CPUs, 64 gigabytes of RAM. It’s based on your data growth and your data usage patterns. That’s the only thing you need to be concerned about in terms of cost. That makes doing financial forecasts for applications a lot simpler. Again, since Managed Service is provided in multiple cloud providers like AWS, GCP, and soon Azure, in multiple regions, you can co-locate or have a network proximity of your chosen Managed Service cluster to your target application environment. Such that you can minimize any costs across in terms of data ingress and egress. When you bring data into Managed Service, you just get the one cost, so the Managed Service unit cost. From your own application, your cloud-hosted application or your datacenter-hosted application that’s bringing data into Amazon, or Azure, or GCP, you may incur some costs from those providers, but from us, it’s very simple. It’s just the per-unit, based on the node. Managed Service is reliable out of the box in that it’s a high availability (HA) deployment, such that if any one node fails, you’re not losing data. Data gets replicated to another leaf node. Leaf nodes in Managed Service are data nodes that store data. On every leaf node, there’s one-to-many partitions, so you’re guaranteed to have a copy of that data on an another machine. Most of this would be fairly under the covers for you. You should not be experiencing any sort of slowdown in your queries, provided that your data is distributed. Next, freedom without sprawl. What I’m talking about is, Managed Service allows you to, as I said earlier, combine multiple types of workloads, to do mixed workloads of transactions and analytics, and different types of data structures like a document. If you’re creating a product catalog and you’re querying that, or you have orders structured as documents, with Managed Service as well as SingleStore, you can store these documents, such as the order, in the raw JSON format, directly in SingleStore. We have an index into that such that you can query and make JSON queries part of your normal application logic. In that way, SingleStore can act as a document or key-value store in the same way that MongoDB or AWS DocumentDB or other types of document databases do. But, we’re more than that, in that you’re not just limited to that one kind of use case. You can add relational queries. A typical use case here is storing the raw JSON but then selecting particular parts of the nested array to put into relational or table columns, because those can be queried as a columnstore in SingleStore. That has the advantage of compression. There’s a lot of advantages in doing these together, relational with document, for instance, or relational with full-text search. Again, you can have this freedom of the different workloads, but without the sprawl of having to set up a document database and then separately a relational database to handle the same use case. Then, finally, I would say a major advantage of Managed Service is that it provides a career path for existing DBAs of legacy single-node databases. There’s a lot of similarity in the basic fundamental aspects of a database management system, but what’s different is that, with SingleStore, you get a lot of what was previously confined to the NoSQL types of data stores, key-value stores, and document stores, for instance. But, you’d get those capabilities in a distributed nature right in SingleStore. It’s, in some ways, the ideal path from a traditional single-node relational database career and experience into a cloud-native operational and distributed database like SingleStore. So what I would like to do at this point is show you a simple demonstration of how this works. I’d refer you to our whitepaper for more details about what I discussed earlier from migrating Oracle to Managed Service, and you’ll find it there by navigating from our homepage to resources, Whitepapers and Oracle and SingleStore migration. So with that, let me switch screens for just a moment. And what I’m going to do. So I’ve got a Telnet session here or SSH session into a machine running an Amazon where I’ve got an Oracle database, and I’m going to run those two steps I just described. Basically the initial data load, and then I’m going to start the replication process. Once that replication process, with SingleStore Replication, is running, then I’ll start inserting data, new data into the source of Oracle database. And you’re going to see that written to my target. And I’ll show a dashboard to make this easy to visualize and the data that’s being written. So the data here is billing data for a utility billing system. I’ve got new bills and payments and clearance notifications that come through that source database. I’ll show you the schema in just a moment. So what I’ll do is I’ll start my initial snapshot. I’ve got one more procedure to run here. Okay. So that’s complete and now I’ll start my application process. And so from my source system, Oracle, we’re writing to SingleStore Managed Service. And you see it’s written 100,000 rows to the BILLS table, 300,000 to CCB and 100,000 to PAYMENTS. So now let’s take a look at our view there, and we can take a look at the database. It’s writing to a database called UTILITY. And if I refresh this, I’ll see that I will have some data here in those three tables… it gave me the count there, but I can quickly count the rows, see what I got there. So I also have a dashboard, which I’ll show it here and that confirms that we’re going against the same database that I just showed you the query for. So at this point I’ve got my snapshot of the initial data for the bills, payments, and clearance notices. So what I’ll do now is start another process that’s going to write data into this source Oracle database. And we’ll see how quickly this happens. Again, I’m running from a machine image in Amazon US East. I’ve got my Managed Service cluster also on Amazon US East. And so let’s run this to insert into these three tables here. And as that runs, you’ll see SingleStore Replicate, which you’re seeing here, it’s giving us how many bytes per second are being written, how many rows are being inserted into each of these tables, and what’s our total run time in terms of the elapsed and the initial load time for that first snapshot data. So here you’ll see my dashboard’s refreshing. You start to see this data being written here into Managed Service. What we can do is use SingleStore Studio to view the data as it’s being written. So let’s first take a look at the dashboard and you can see we’re writing roughly anywhere from four to 10,000 rows per second against the database, which is a fairly small rate. We can get rates much higher than that in terms of tens of thousands or hundreds of thousands of rows written per second depending on the size. If they’re small sometimes it can be millions of rows in certain situations. And let’s take a look at the schema here. And you’ll see that this data is increasing in size. As I write that data and SingleStore gives you this studio view such that you can see diagnostics on the process on the table as it’s happening, as data’s being written. Now you may notice that these three tables are columnstore tables. Columnstores are used for analytic queries and they have really superior performance for analytic queries and they compress the data quite a lot. And our column stores use a combination of memory and disk. After some period of time this data and memory will persist, will write to disk, but even when it’s in memory, you’re guaranteed the durability and resilience. Again, because Managed Service provides high availability by default, which you can see that redundancy through the partitions of the database through this view. Case Studies I’ll close out with a few example case studies. First… Well I think we’re running a little bit short on time, so I’m going to go directly to a few of these case studies here. Managed Service was first launched back in fall of last year and since then we’ve had several migrations. It’s been one of the fastest launches in terms of uptake of new customers that we’ve seen in the company’s history. This is a migration from an existing SingleStore self-managed environment for a company called SSIMWAVE who provides video compression and acceleration for all sorts of online businesses, and their use case is around interactive analytics, ad hoc queries. And they want to be able to look at what are the analytics around how to optimally scale their video serving and their video compression. And so they are a real-time business and they need operational analytics on this data. Just to draw an analogy, if you’re watching Netflix and you have a jitter or a pause, or you’re on Prime Video and you have a pause for any of these online services, it’s an immediately customer-impacting kind of customer-facing scenario. And so this is a great example of a business that depends on Managed Service and the Cloud to provide this reliability to deliver analytics for a customer facing application. Sort of what we call analytics live in SLA. So they’d been on Managed Service now for several months and you see some of the quote here on why they’ve moved and the advantage of the time savings with Managed Service. A second example is Medaxion, and they were moving from… Initially they moved to SingleStore from MySQL instance, and then over to Managed Service. And their business is providing information to anesthesiologists, and for them, again, it’s a customer facing scenario for operational analytics. They’ve got to provide instantaneous analysis through Looker dashboards and ad hoc queries against this data. And Managed Service is able to perform in this environment for an online SAS application essentially where every second counts in terms of looking at what’s the status of the records that Medaxion handles. And then finally, I’ll close with this Thorn. They are a nonprofit that focuses on helping law enforcement agencies around the world identify trafficked children faster. And if there’s any example that that shows that time criticality and the importance of it in the moment operational analytics, I think this is it, because most of this data that law enforcement needs exists in various silos or various systems or different agencies systems and what Thorn does is to unify and bring all of this together but do it a convenient searchable way. So they’re taking the raw sources among which are posts online, which they feed through machine learning process to then land that processed data into Managed Service such that their Spotlight application can allow instant in the moment searches by law enforcement to identify based on image recognition and matching if a child has been involved… is in a dangerous situation and correlating these different law enforcement records. So those are three great examples of Managed Service in real-time operational analytics scenarios that we thought we’d share with you. And with that I’ll close, and we’ll move to questions. Q&A and Conclusion How do I learn more about migration with SingleStore? On our resources page you’ll find a couple of Whitepaper’s on migration. One about Oracle specifically and one more generally about migrating. Just navigate to the home page, go to Resources, Whitepapers. You’ll find that. Also there is a webinar we did back last year or before, the five reasons to switch – you can catch that recording. Of course you can also contact us directly and we’ll provide an email address here to share with you. Where do I find more about SingleStore Replicate? So that’s part of 7.0, so you’ll find all of our product documentation is online and SingleStore Replicate is part of the core product, so if you go to docs.singlestore.com, then you’ll find it there under the references. Is there a charge for moving my data into Managed Service? There’s no ingress charge that you incur using self-managed SingleStore or SingleStore Managed Service. Our pricing for Managed Service is purely based on the unit cost as we call it. And the unit again is the computing resources for a node and it’s just the leaf node, it’s just the data node. So eight vCPUs, 64GB of RAM, that is a Managed Service leaf node. All of that is just a unit. That’s the only charge. But you may incur data charges from depending on where your source is, your application or other system for the data leaving, or the egress from that environment, if it’s a cloud environment. So not from us per se, but you may from your cloud provider.", "date": "2020-03-01"},
{"website": "Single-Store", "title": "cloud-database-trend-report-from-dzone-features-memsql", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/cloud-database-trend-report-from-dzone-features-memsql/", "abstract": "A new Trend Report from DZone highlights the move to cloud databases. The report paints a bright picture for cloud database adoption, with roughly half of developers asserting that all of their organization’s data will be stored in the cloud in three years or fewer. You can get a copy of the report from SingleStore . DZone has issues a new trend report on cloud databases. In the report, leaders in the database space focus on specific use cases, calling out the factors that help you decide what you need in any database, especially one that’s in the cloud. The advantages of cloud databases include flexibility to scale up and scale back, easier backups of data, moving database infrastructure out of house, and offloading some database maintenance. SingleStore is a database that runs anywhere Linux does, on-premises and on all three major cloud providers – AWS, Google Cloud Platform (GCP), and Microsoft Azure. SingleStore Managed Service is a managed service with the SingleStore database at its core. Managed Service is available on AWS and GCP, with Azure support to follow soon. The SingleStore Kubernetes Operator gives you the flexibility to manage this cloud-native database with cloud-native tools. SingleStore is also a fast, scalable SQL database that includes many features that are normally claimed only by NoSQL databases: easy scalability, fast ingest, fast query response at volume, and support for a wide range of data types, especially JSON and time series data. Between self-managed SingleStore (the version you download and run on Linux), and MemSQ Managed Service, the advantages of cloud databases – scalability, easy and reliable backups, and moving both infrastructure and maintenance out-of-house – are readily available, on a solution that’s also identical on-premises. The report points out several interesting facts: Slightly more than half of organizations that have a cloud database solution in place have had one for two years or less. More than two-thirds of cloud database users either use multiple clouds (40%) or are seriously considering doing so (26%). Analytics is the #1 reason for moving databases to the cloud, with modernization of existing apps and becoming cloud native also ranking highly. The database as a service (DBaaS) model, represented by SingleStore Managed Service and many other options, has a slight lead over those who use a self-managed database. About half of respondents believe all of their data will be in the cloud in three years or fewer. The report goes on to interview Adam Ballai, CEO at RevOps, in depth about the research findings. You can access a copy of the report from SingleStore . This version includes a customer case study from Thorn, which seeks to eliminate child sexual abuse from the Internet, using machine learning, AI – and SingleStore. You can also try SingleStore for free .", "date": "2020-03-11"},
{"website": "Single-Store", "title": "ways-to-compute-pi-in-sql", "author": ["Nikita Shamgunov"], "link": "https://www.singlestore.com/blog/ways-to-compute-pi-in-sql/", "abstract": "In celebration of Pi Day , I asked SingleStore’s Engineering team to come up with clever ways to compute pi in SQL. Because that’s the kind of thing we engineers enjoy! And, given pi’s privileged status in mathematical history – and my own history of earning a bachelor’s degree in Mathematics – I just couldn’t resist. Truth be told, in the early days of SingleStore, my fellow math geeks and I had some fun with Pi Day – yes, including eating actual pies – and we even had a colleague with the nickname of Pieguy. Pie, not Pi, btw. So, if anyone was going to put together a blog post on calculating pi in honor of Pi Day, it would be me. How else is SingleStore Pi/Pie-forward? Here are a few fun facts: All of our conference rooms in San Francisco are named after mathematicians and their accomplishments: (Thomas) Bayes, (Emmy) Noether, (Maryam) Mirzakhani, (Alan) Turing, and (John) Venn. (Oh, and the conference rooms in our Portugal engineering office are named for pastries. And, if you squint hard enough, pastel de nata – the name of a conference room – is a mini-pie!) All of our product releases are named after pies. We used to eat pie for major releases. For instance, for icecreampie, we ate ice cream pie. But we haven’t eaten any fictional pies like KeyUsingClusteredColumnstorePie (lol). We’re on Kalm at the moment, which stands for keypie a la mode. I dare you to top that! That being said, we had fun putting this post together, and we hope you have a great #piday. Let’s Use Stored Procedures _Code is available [in Github](https://gist.github.com/carlsverre/08fcd301faa1bbb2f3fe7bc80da62d4a)._ Let’s Generate a Large(-ish) Dataset with Some Random Data Code is available Code is available in Github . From there, the conversation digressed into the quality of our RAND() implementation and our engineers started to compare it with loading data from /dev/urandom with data generated by calling RAND(). Code is available in Github . We Also Got an Alternative Solution Code is available in Github . This one outputs pie – the author says it is a pecan pie, but it’s hard to tell for sure. This is all fun and good, but of course you can just do: memsql> select PI();\n+-------------------+\n| PI()              |\n+-------------------+\n|          3.141593 |\n+-------------------+\n\n1 row in set (0.00 sec) Thank you all for reading! Now, time for some cherry pie. With thanks to Carl Sverre for graphics and code storage.", "date": "2020-03-14"},
{"website": "Single-Store", "title": "a-hoap-ful-new-perspective-451-research", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/a-hoap-ful-new-perspective-451-research/", "abstract": "Analyst firm 451 Research has come up with new research that sees a bright future for HOAP – hybrid operational and analytical processing. The report is titled 451 Perspective: A HOAP-ful future for hybrid operational and analytical processing. This new type of processing has received several different names, from different analyst firms and consultancies: HTAP (Gartner) Translytical processing (Forrester) HOAP (451 Research) Operational analytics (common amongst other analyst firms) By any of these names, this new style of data processing – which unifies transactions and operational analytics, including many data warehousing-type functions – is widely believed to have a bright future. And SingleStore is right in the middle of it. What HOAP Replaces In a previous report , 451 Research identified HOAP as an emerging category. Now, they see HOAP experiencing broad adoption. HOAP seeks to unify two formerly separate data processing categories, and to largely eliminate the need for a third: Online transaction processing (OLTP) . Transaction processing systems have various needs for reliability, with requirements which begin at strong – “nearly always works” – to absolute -“must work every time, across time zones and disparate data centers, safeguarding against serious financial and reputational consequences for data loss or significant downtime.” Online analytics processing (OLAP) . Analytics processing systems, which include data warehousing systems, data marts, and data shoeshine stands (just kidding), typically work on copies of existing data. They must be fast, reliable, scalable to multiple apps and users, and affordable, with SQL support. Extract, transform, and load (ETL) . Systems which transform and format data from ingest or OLTP systems to OLAP systems have become a separate category of their own. Using an ETL system reduces the requirements for the OLTP and OLAP systems that an ETL product connects to, but adding a third system to the mix increase cost and complexity, as well as ensuring significant end-to-end latency. One of the advantages that 451 Research cites for OLTP systems, and their use of rowstore tables, is the ability to handle complex queries with joins. And they also cite architectural advantages to the separation of transactions, given that these are well suited to rowstore tables, and analytics, which usually benefit from the use of columnstore tables. SingleStore, however, fuzzes over these distinctions – and, with Universal Storage, is on track to nearly eliminate them. SingleStore not only supports both table types in a single database; it supports joins, and other operations, for rowstore tables, columnstore tables, and across table types. A diagram from the previous 451 Research HOAP report shows HOAP revenues as making up roughly 20% of all database revenues by 2021 The Benefits of HOAP 451 Research describes HOAP databases as “taking a bite out of (formerly) pure OLTP workloads,” with more of this expected in the future. There are many benefits to HOAP, largely deriving from its ability to handle both transactional and analytics processing – without the need for ETL. Benefits include: Fewer systems to maintain . By eliminating ETL, and taking on many OLTP and OLAP workloads, HOAP systems reduce operational and maintenance complexity within customer organizations. Eliminating latencies . Latency due to ETL is eliminated, and the latencies adhering to OLTP and OLAP are reduced to a single overall latency, which may be measured in milliseconds – vs. many hours. or even several days, for the OLTP-ETL-OLAP combination. Real-time decision making . When properly exposed to apps, business intelligence (BI) programs, and ad hoc SQL queries, a HOAP database can take the same data which is used for transactions and also make it available for analytics, in near real time. Spreading HOAP within Organizations When 451 Research surveyed organizations about the type of workloads they run, HOAP showed up fifth among seven types of workloads, a bit ahead of data science workloads. And 451 sees HOAP revenue growing by more than a third in five years, rising from nearly $60B in 2018 to nearly $80B in 2022. 451 Research also sees HOAP nearly doubling its share of workloads, to more than a quarter of all workloads, in the same five-year period. Why the growing use of HOAP? Reasons include: Better new products and services Lower costs Higher product sales Competitive advantages 451 Research describes HOAP as playing an important role in companies becoming data-driven. Different Kinds of HOAP According to the report, there are different ways to deliver a HOAP system. 451 Research largely recasts the database wars of recent years through the lens of HOAP. So the HOAP market is made up of both distributed, relational systems, such as SingleStore, and NoSQL systems – which we here at SingleStore believe are overrated and overused . Cloud vendors also offer HOAP systems. They use their operational control in the cloud to hide system complexity. This allows them to meet customer needs while trading off resources, including human intervention in database operations, behind the scenes. At the same time, a cloud database that’s exclusive to a cloud provider is not, ironically, cloud-native. These systems typically run on just one cloud vendor and can’t be run on-premises, even using cloud-native tools such as Kubernetes. SingleStore, as fully cloud-native software, runs anyplace you can run Linux – on-premises, or in any cloud. SingleStore offers a Kubernetes Operator for easily managed, cloud-native operation in any environment. And SingleStore Managed Service is a database-as-a-service (DBaaS) offering, managed by SingleStore using Kubernetes. All of these capabilities fit 451’s desiderata, which include database-as-a-service across hybrid IT systems and vendors that offer a consistent experience across a broad portfolio of options. Bringing HOAP to Life in Your Organization To get a copy of this report, contact 451 Research. And, you can try it yourself : take advantage of a time-limited free trial of SingleStore Managed Service, or run SingleStore for free. Download SingleStore software or connect to Managed Service for free today . Or, if you want to know more before you proceed, contact SingleStore .", "date": "2020-04-02"},
{"website": "Single-Store", "title": "dzone-memsql-webinar-2-of-3-kubernetes-file-storage-configuration-and-secrets", "author": ["Rob Richardson"], "link": "https://www.singlestore.com/blog/dzone-memsql-webinar-2-of-3-kubernetes-file-storage-configuration-and-secrets/", "abstract": "This is the second blog post derived from the recent DZone webinar given by Rob Richardson, technical evangelist at SingleStore. (From the link, you can easily download the slides and stream the webinar.) Kubernetes was originally developed to manage containers running stateless workloads, such as computation-intensive processing tasks and web servers. It was then extended to allow you to manage stateful workloads, such as data tables and e-commerce sessions, but doing so is not easy. Rob is one of the relatively few people who have figured out how to do this well. The webinar was well-attended, with a solid Q&A session at the end. There’s so much going on that we’ve split the webinar into three blog posts: The first blog post, Kubernetes and State ; this blog post, which is the second in the series, Kubernetes File Storage, Configuration, and Secrets; and a third blog post, Kubernetes StatefulSets and Q&A . You can read through all three for education – or use them as a step-by-step road map for actual implementation. You can also see Rob’s Kubernetes 101 blog post for an introduction to Kubernetes. As Rob mentions during his talk, SingleStore database software is well-suited for containerized software development, managed by Kubernetes. SingleStore has developed a Kubernetes Operator , which helps you use SingleStore within a containerized environment managed by Kubernetes. Alternatively, if you want to use a Kubernetes-controlled, elastic database in the cloud, without having to set up and maintain it yourself, consider SingleStore Managed Service . Managed Service is available on AWS, Google Cloud Platform, and is coming soon on Microsoft Azure. Following is a rough transcript of the middle part of the talk, illustrated by slides from the accompanying presentation. This second blog post dives into file storage in Kubernetes, Kubernetes configuration, and the use of secrets in Kubernetes. Stateful Resources #1: Volumes Here’s our first stop in the suite of stateful resources, volumes. With a volume, we probably want to store these files so the data isn’t lost to a pod restart, or to share data files between pods. Now the cool part is this is not that unlike a network share, where we just have a drive often on the network where we can store things. And so that’s kind of a good analog, but we’re actually going to create a symbolic link into the pod, so a certain directory just happens to be stored elsewhere. Now, because it’s a network share, file locks don’t work the way we expect. So if I lock a file in one pod, and then try to read it or write it from the other pod, that operation will work just fine. So we need to architect our app slightly differently to not assume that files are locked. Perhaps each pod uses a different folder, or perhaps we’re only doing unique file names based on operations – but we need to be careful there, because file locks don’t work the way we expect. So here’s a YAML file that defines a pod referencing a volume. And so I’ve eliminated some of the normal things that we would see in a pod file – all of the pieces about resiliency and health checks. But we’ve defined an image. We’ve defined a name for our container. And then we have this volume mount section, and this volume mount section talks about the details of the volume. So here, we give the name of the volume. And then the mount path – this is the path inside the container. apiVersion: v1\nkind: Pod\nmetadata:\n  name: mypod\nspec:\n  containers:\n  - image: nginx:alpine\n    name: myapp\n    # ... snip ...\n    volumeMounts:\n    - name: the-volume\n      mountPath: /path/in/container/to/dir\n  volumes:\n  - name: the-volume\n    hostPath:\n      path: /mnt/host/path/to/dir So in /path/in/container/to/dir, if my app opens a file inside that directory, it’s as if it’s any other file running inside this container – it’s just a file on the file system. I can add more folders inside this directory. I can add more files to them. I can read and write those files, and to the application, it just believes that it’s part of the disk. But this volume here is defined here, the-volume, and I can specify various mechanisms for getting that volume to be more durable. So in this case, I chose to make it a host path, and so here’s that path to the directory on the host. I could choose to put this inside my NAS or on a network share, or in an S3 blob, or wherever it makes sense to hold this data. I can make that much more durable. Now, this is interesting, and it’s a great example of getting file storage off of my pod, into a more durable place. But it kind of mixes things. So the magic question is, well, who owns that storage spot – the developer, or Operations? Should the developer own it, as they’re crafting the rest of that YAML file, and identifying the version of the image it should use? Should they own that hard drive? Or, Operations really wants to ensure that that’s backed up correctly. So do they want to identify the correct location, and perhaps the naming convention inside the SAN? And so we really want this separation between developer and Operations, and that’s where we can get into a really cool abstraction in Kubernetes where we can talk about persistent volumes and persistent volume claims. Stateful Resources #2: Persistent Volumes and Persistent Volume Claims Now, this does get a little bit hairy, so I’ve kind of nicely drawn it so that we can follow it. We have a storage system, a hard drive, and we can carve that storage up into blocks. We’ll call these blocks persistent volumes. Each block represents one thing that a pod can check out. So we’ll have a persistent volume, and then we’ll have a persistent volume claim that claims one of them. So a pod can say, “I need a hard drive and I need it to be five gigs.” And so it’ll go find a five gig spot and it’ll pull it out and it will sim link that as a folder within the container. (This section of Rob’s talk, about persistent volumes and persistent volume claims, got kudos from a Kubernetes maintainer. Many organizations get partway through doing this work, give up, and hire an outsider to do it – but the explanation that Rob gives here should give you a fighting chance. – Ed.) We have storage. We have the persistent volume. We have the persistent volume claim. And we have the pod definition. Now, the cool thing is the persistent volume claim and the pod can be owned by the developer. The persistent volume and the storage can be owned by Operations. And so we can have this really cool clean separation of concerns here, where we can ensure that that storage is backed up correctly and that it’s allocated according to our provisioning needs. And the developer can also assume that they just need storage and it just magically works, connecting it to the right folder within their pod. And so that’s a really elegant way to have persistent volumes and persistent volume claims that can separate the dev and ops points of responsibility. So creating a persistent volume, here’s that ops part of it. We specify that host path, that path to where we want to actually store the directory. We can specify access modes, ReadWriteOnce in this case. (We can also say ReadWriteMany or ReadOnly .) And then we give it a storage size. In this case I said, “This path has a storage of 10 gigs. If you go over 10 gigs, then that drive is full.” Now, I may have more space on my hard drive, I just carve it into 10 gig pieces. apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: pv-volume\n  labels:\n    type: local\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 10Gi\n  accessModes:\n    - ReadWriteOnce\n  hostPath:\n    path: /mnt/host/path/to/dir Here’s that persistent volume, and to claim it, the developer will claim a persistent volume claim specifying the details that they need. So in this case, I said I need something that is read write once and I need it to be at least three gigs. Now, the cool part is, we specified this as 10 gigs. I said I want three gigs or more and so it says, “Okay, I’ll grab that one.” We don’t need any other details other than the ReadWriteOnce match and the storage amount. We can specify other details like availability zone, affinity, or storage, speeds, quality of service details. But those are the two that we definitely need to do, ReadWriteOnce (or one of the other options) and the storage. apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pv-claim\nspec:\n  storageClassName: manual\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 3Gi # at least this much So with that persistent volume claim, now here in my pod definition, rather than specifying where it’s actually stored, I’m going to reference that persistent volume claim’s name. So the pod will reference the persistent volume claim and all the rest of it will just work. Here in the volume mounts, I’m specifying the path and the container that I want it to connect to and everything just works. apiVersion: v1\nkind: Pod\nmetadata:\n  name: mypod\nspec:\n  containers:\n  - image: nginx:alpine\n    name: myapp\n    # ... snip ...\n    volumeMounts:\n    - name: the-volume\n      mountPath: /path/in/container/to/dir\n  volumes:\n    - name: the-volume\n      persistentVolumeClaim:\n        claimName: pv-claim So we have the storage. We have that persistent volume. A pod can claim one of those persistent volume blocks and attach it into its container and now we have persistent storage inside of our pod. And we have that nice separation between dev and ops. Stateful Resources #3: Configuration So that’s persistent volumes, and now we have file storage. We can upload profile pictures. We can save CSV files that we have uploaded. All of the content associated with files, we can now stick into our application. But how do we know where that location is? Perhaps we want other config details or maybe we want config details that are environment specific. Now, the cool part about configuration within Kubernetes is we can mount these in two ways: As files on disk As environment variables We’ll see examples of each. In Kubernetes, here’s our simplistic example where we just list the environment variables that we want to define here in our pod. Here is the environment variable. Here’s the value we want it to be set at. And then, in our application-specific way, we can read that environment variable and take action. apiVersion: v1\nkind: Pod\nmetadata:\n  name: mypod\nspec:\n  containers:\n  - image: nginx:alpine\n    name: myapp\n    # ... snip ...\n    env:\n    - name: THE_MESSAGE\n      value: \"Hello world\"\n    - name: FOO\n      value: \"bar\" Now, a similar mechanism. Who owns this configuration data? Again, we have to choose between dev vs. ops. Developer . Does the developer have to know all of the details in each environment? Operations . What if Operations really wants production secrets that are not exposed to developers? And so let’s look at a mechanism within Kubernetes where we can split that out as well, the ConfigMap, which identifies all of those details. So for example, here in this ConfigMap, I have some data. I have a database, SingleStore. Logging is set to true. My API URL is set to this URL, and these can be environment specific details stored by operations. As a developer, I then just reference that ConfigMap by name and it just works. apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: the-configmap\ndata:\n  database: memsql\n  logging: 'true'\n  api_url: https://example.com/api Here, instead of environment variables, I’m saying from ConfigMap ref and I just list that ConfigMap name. So as the developer, I can have one pod definition that will work in all environments. My application and my container can work identically through all environments. And then we just have environment-specific ConfigMaps that happen to be preloaded into our cluster so that we can reference the correct database. All those database connection details just get pulled right in and we’re able to connect to our database just fine. apiVersion: v1\nkind: Pod\nmetadata:\n  name: mypod\nspec:\n  containers:\n  - image: nginx:alpine\n    name: myapp\n    # ... snip ...\n    envFrom:\n    - configMapRef:\n        name: the-configmap # ConfigMap's name ConfigMap as Env Vars So, we can also mount this ConfigMap as files instead of as environment variables. And we may choose to do this so that it’s not available all the time, maybe a little bit less discoverable. So here we specify the mount path. That’s just some path in the container where I want to store all of my configuration details. Inside that folder, there’ll be a file for each of the ConfigMap keys. So inside the path and container to dir, I would have a database file and I would have a username file and I would have an API URL file. And my application could open that file, read the secret, and take appropriate action. apiVersion: v1\nkind: Pod\nmetadata:\n  name: mypod\nspec:\n  containers:\n  - image: nginx:alpine\n    name: myapp\n    # ... snip ...\n    volumeMounts:\n    - name: the-volume\n      mountPath: /path/in/container/to/dir\n  volumes:\n    - name: the-volume\n      configMap:\n        name: the-configmap # ConfigMap's name ConfigMap as Files (One File per Key in Target Folder) Stateful Resources #4: Secrets So some of these configuration details, we may want to keep in secret. Rather than specifying them globally and storing them in plain text, we may want to use these encrypted. So let’s dig into secrets. Secrets are stored encrypted at rest since Kubernetes 1.13. Arguably, that’s kind of late. Before this, it was stored as base 64 encoded. Oops. And because the secrets needed to be available on all hosts, because all hosts may run any pod, then if I compromise one of the hosts in my cluster, I could get access to all the secrets, just base 64 decode them, and get at them. That was solved in Kubernetes 1.13, and so now Kubernetes secrets are stored encrypted at rest. They are available on every host, so we do need to protect the host to ensure that those secrets are preserved, but they are encrypted at rest now. Alternatively, I may choose to put my secrets in a key vault like HashiCorp, and that’s a great solution as well. So if I create these secrets, I can do that in a YAML file like I do here. This YAML file specifies the secret name is db-connection, and then here’s my list of secrets that I would like in this secret container. My database username is devuser. My database password is password, and I can store these in string data, just in plain text. If I flip this string data with data, then these will be base 64 encoded. So base 64 encoding things is a smidge weird. They’re still not secret, but here’s that YAML definition. Now, I recommend not using this YAML definition, because it’s really easy to accidentally check this into source control – and once I’ve checked this into source control, then I’ve lost that secret data. I need to rotate my keys. I need to change my passwords. That data is now exposed. apiVersion: v1\nkind: Secret\nmetadata:\n  name: db-connection\ntype: Opaque\nstringData:\n  DATABASE_USERNAME: devuser\n  DATABASE_PASSWORD: password So rather than using a YAML file, I recommend using kubectl to create secrets. Now here, this is one long line. Each of these back ticks represents a continuation of that line. Here, as I’m creating the secret, I’m specifying the literal content for each thing. I can do this in a shell, where I can shortly thereafter kill my history, or I can pull that in from a file so I don’t even expose my passwords to the command line. But I can pull in those either as base 64 encoded content or, in this case, because I said from literal, I can pull that in from just plain text data. So I say here in my DB-connection piece, I have two pieces of data I’d like to store in the secret. One is named username and its value is devuser and one is named password and its value is password. So we usually don’t change secrets a whole lot within our cluster. We just need them to be present, and so I find this command line or an API call to be a whole lot easier to maintain and to keep secure, rather than a YAML definition. kubectl create secret generic db-connection \\\n     --from-literal=username=devuser \\\n     --from-literal=password='password' So now, as I pull them into my application, we can do a similar kind of abstraction. Ops will own creating those secrets, perhaps rotating those secrets. And developers can then mount those either as environment variables or as files. In this case, I’m choosing to mount it as a file, so I have a volume mount. Here’s my mount path. That’s the folder inside the container. I’m mounting it as read only as true because I really don’t want to change those secrets, not that I can, but I’m just making that explicit here. And then in my volume definition, previously we identified it from a ConfigMap, and then from a persistent volume claim. And here, instead of a persistent volume claim, we’re referencing it as a secret. apiVersion: v1\nkind: Pod\nmetadata:\n  name: mypod\nspec:\n  containers:\n  - image: nginx:alpine\n    name: myapp\n    # ... snip ...\n    volumeMounts:\n    - name: the-volume\n      mountPath: /path/in/container/to/dir\n      readOnly: true\n  volumes:\n    - name: the-volume\n      secret:\n        secretName: db-connection # The secret name Secrets as Files (One File per Key in Target Connection) So I have my secret name, DB-connection. And because I’ve specified that secret name, then in this directory I’ll have two files, a username file and a password file. Whenever my application needs to connect to the database, it can read those two files, grab those secrets, and continue on. I can also choose to mount these as environment variables. Down here at the bottom, I’m still referencing that secret key ref as DB-connection, but I can also specify the key. In this case that is password, and it’s going to set an environment variable called DB-PASSWORD. Same thing with DB-USERNAME. I’ll set that value from the secret, and that’ll set that environment variable within my container. Now I can have one pod definition that is universal across all environments, and it can just assume that those secrets exist and pull in the applicable data as it starts up the pod. So we’ve done some really, really cool things. We’ve got configuration details here, and we’ve got secrets so that we can store secret data. We can separate those details between developers and Operations to ensure that that content is preserved across environments, and secrets don’t leak into hands that don’t need them. We’ve got volumes where we can store our persistent data. And that may be enough with volumes and ConfigMaps and secrets to be able to get the majority of your application into Kubernetes. Conclusion This blog post, File Storage, Configuration, and Secrets , is the second in a series. The first, Kubernetes & State , appeared previously. The third, Kubernetes StatefulSets and Q&A , is coming up next. These are all part of a series covering Rob Richardson’s recent DZone webinar . You can try SingleStore for free today. If you download the SingleStore database software, and want to manage it with Kubernetes, download the SingleStore Kubernetes Operator as well. Alternatively, use SingleStore Managed Service , our elastic managed database service in the cloud. We at SingleStore manage Managed Service using our Kubernetes Operator, reducing complexity for you.", "date": "2020-04-18"},
{"website": "Single-Store", "title": "dzone-memsql-webinar-1-of-3-kubernetes-state", "author": ["Rob Richardson"], "link": "https://www.singlestore.com/blog/dzone-memsql-webinar-1-of-3-kubernetes-state/", "abstract": "Kubernetes was originally developed to manage containers running stateless workloads, such as computation-intensive processing tasks and web servers. It was then extended to allow you to manage stateful workloads, such as data tables and e-commerce sessions. However, doing so is not easy. One of the people who has figured out how to do this well is our own Rob Richardson, technical evangelist at SingleStore. In a recent DZone webinar , this three-part webinar series, Rob shows you how to build stateful workloads in Kubernetes. (From the link, you can easily download the slides and stream the webinar.) The webinar was well-attended, with a solid Q&A session at the end. There’s so much going on that we’ve split the webinar into three blog posts: This blog post, Introducing State in Kubernetes; a second blog post, File Storage, Configuration, and Secrets ; and a third blog post, Kubernetes StatefulSets and Q&A . You can read through all three for education – or use them as a step-by-step road map for actual implementation. You can also see Rob’s Kubernetes 101 blog post for an introduction to Kubernetes. As Rob mentions during his talk, SingleStore database software is well-suited for containerized software development, managed by Kubernetes. SingleStore has developed a Kubernetes Operator , which helps you use SingleStore within a containerized environment managed by Kubernetes. Alternatively, if you want to use a Kubernetes-controlled, elastic database in the cloud, without having to set up and maintain it yourself, consider SingleStore Managed Service . Managed Service is available on AWS, Google Cloud Platform, and is coming soon on Microsoft Azure. Following is a rough transcript of the talk, illustrated by slides from the accompanying presentation. This initial blog post covers the original, stateless functionality of Kubernetes; how stateful workloads were added; and how to go beyond kubectl get all to actually get all the things, not just some of them. Kubernetes as a Stateless Platform Kubernetes was originally designed to be a stateless platform, and so when we first look at Kubernetes, we probably build something like this. We’ve got our containers running inside of pods. We have a service in front of it. We may have an Ingress controller that routes traffic to the service. We got this. This is easy. (I asked Rob for more about Ingress controllers and Operators, and he elaborated: “To make it easier to work with new or existing stateless software work in a Kubernetes environment, you can create an Ingress controller for it” – an example is this one for NGINX web server software . “To make it easier to work with new or existing stateful software work in a Kubernetes environment, you can create an Operator for it” – an example is the SingleStore Kubernetes Operator , which Rob describes further on in this webinar. He concluded by saying, “Working with an existing Operator or Ingress controller is much, much easier than creating a new one.” – Ed.) Well yeah, it takes a little bit to get to this point, but this is really great. Users can come hit our Ingress controller. They can route to the service. The service goes to the pods, and all of that works great. However, our apps probably look something like the below instead. We have a little bit more complexity here. So we still have our containers and they’re running inside of pods. We still have our service round robining between them. But with our Ingress controller, we’re probably terminating SSL here. So we’ll have a certificate. We’ll have domain configuration that is probably different between environments. Additionally, we’ll likely have database content, or maybe file storage content, and we need to store configuration details to get to that content. And so we have a whole lot of state in our application that we didn’t have in that initial, simplistic example. So how do we get this state inside of Kubernetes? That’s what we’ll dig into today. Today’s the Day Two experience, where we talk about getting state into Kubernetes. (For the Day One experience – an introduction to Kubernetes – see Rob’s previous blog post, Kubernetes 101__. – Ed.) So what is state? State is anything that lives beyond the current lifetime. And so we can think of state as things living beyond the function call. That’s likely what we do when we’re writing software, is building variables that may live longer than a function call. As we talk about web requests, we may talk about things that live longer than a request and response cycle. So as I go save the data, I then need to go run another request to get that data back, and so that’s the state that I need to save. We also have state that talks about server restarts. So I may need to save files on the file system, and if the server restarts, I want those files to stay on that file system. Or perhaps I’m going across a load balancer and so I need that file system to be shared across all my web servers. That’s state as well. So these are the types of things that we’re going to talk about today – getting that state stored inside of Kubernetes: Configuration details Secrets Data stores File stores Singleton services Among these, singleton services are special. This is a service which is making business decisions, and it needs a holistic and complete view of all of the system. So we need to have that one just maintain all of the pieces. A database is a great example of a singleton service. We can’t have more than one copy of the database in our environment. We may have different machines as part of a single cluster, but ultimately, we need to have that one unique database. It’s not like each pod can connect to its own database and store its own data and then a different pod may get to a different database. We need to have that one single source of truth. So we talked about configuration, secrets, data stores, and file stores, and these singleton services. Let’s dig into each one and start to explore how those work inside Kubernetes. How to Really “Get All” for Stateful Resources Now, one of the big annoying things is that, when we say kubectl get all , we don’t actually get everything. The name suggests that it’s supposed to get everything, but it really doesn’t. What it gets is all of the stateless resources. The developers of Kubernetes made this choice deliberately, because for things that can’t be recreated easily, they don’t want to expose them prematurely. So that if we were to delete and recreate everything, maybe we couldn’t get that secret back, because we didn’t have the unencrypted data. Or if we destroyed and recreated a stateful service like a database, we couldn’t get that data back. So kubectl get all deliberately doesn’t show stateful resources. The bummer part then is, as we go start exploring stateful resources, we’ll probably need a different mechanism for getting them. Now, we can definitely say kubectl get , and each of the things, and just list them. But if we say kubectl get all , we need to know that maybe our resources aren’t all listed there. Luckily, there is a mechanism to get all the things. It’s massively complex. First, I go grab all of the resources, kubectl api-resources . That will list all of the resources in my cluster, including custom resources, perhaps added by third party packages. Given that list, I could run that through an awk script to go grab the name, and I could comma separate them, and then I could say kubectl get that big list. So that’s kubectl get , kubectl api-resources , pipe it to awk, pipe it to grep, pipe it to xargs, pipe it to sed. I have that as an alias in my shell so that I can get all the all the things, including all of the stateful resources. You may choose to download these slides and copy that into your bash profile, as well and have a similar alias for truly getting all of the things. Conclusion This blog post, Introducing State in Kubernetes, is the first in a series covering Rob Richardson’s recent DZone webinar . Coming soon: the second blog post, File Storage, Configuration, and Secrets ; and the third blog post, Kubernetes StatefulSets and Q&A . You can try SingleStore for free today. If you download the SingleStore database software, and want to manage it with Kubernetes, download the SingleStore Kubernetes Operator as well. Alternatively, use SingleStore Managed Service , our elastic managed database service in the cloud. We at SingleStore manage Managed Service using our Kubernetes Operator, reducing complexity for you. The webinar was well-attended, with a solid Q&A session at the end. There’s so much going on that we’ve split the webinar into three blog posts: This blog post, Introducing State in Kubernetes; a second blog post, File Storage, Configuration, and Secrets ; and a third blog post, Kubernetes StatefulSets and Q&A . You can read through all three for education – or use them as a step-by-step road map for actual implementation. You can also see Rob’s Kubernetes 101 blog post for an introduction to Kubernetes. As Rob mentions during his talk, SingleStore database software is well-suited for containerized software development, managed by Kubernetes. SingleStore has developed a Kubernetes Operator , which helps you use SingleStore within a containerized environment managed by Kubernetes. Alternatively, if you want to use a Kubernetes-controlled, elastic database in the cloud, without having to set up and maintain it yourself, consider SingleStore Managed Service . Managed Service is available on AWS, Google Cloud Platform, and is coming soon on Microsoft Azure. Following is a rough transcript of the talk, illustrated by slides from the accompanying presentation. This initial blog post covers the original, stateless functionality of Kubernetes; how stateful workloads were added; and how to go beyond kubectl get all to actually get all the things, not just some of them. Kubernetes as a Stateless Platform Kubernetes was originally designed to be a stateless platform, and so when we first look at Kubernetes, we probably build something like this. We’ve got our containers running inside of pods. We have a service in front of it. We may have an Ingress controller that routes traffic to the service. We got this. This is easy. (I asked Rob for more about Ingress controllers and Operators, and he elaborated: “To make it easier to work with new or existing stateless software work in a Kubernetes environment, you can create an Ingress controller for it” – an example is this one for NGINX web server software . “To make it easier to work with new or existing stateful software work in a Kubernetes environment, you can create an Operator for it” – an example is the SingleStore Kubernetes Operator , which Rob describes further on in this webinar. He concluded by saying, “Working with an existing Operator or Ingress controller is much, much easier than creating a new one.” – Ed.) Well yeah, it takes a little bit to get to this point, but this is really great. Users can come hit our Ingress controller. They can route to the service. The service goes to the pods, and all of that works great. However, our apps probably look something like the below instead. We have a little bit more complexity here. So we still have our containers and they’re running inside of pods. We still have our service round robining between them. But with our Ingress controller, we’re probably terminating SSL here. So we’ll have a certificate. We’ll have domain configuration that is probably different between environments. Additionally, we’ll likely have database content, or maybe file storage content, and we need to store configuration details to get to that content. And so we have a whole lot of state in our application that we didn’t have in that initial, simplistic example. So how do we get this state inside of Kubernetes? That’s what we’ll dig into today. Today’s the Day Two experience, where we talk about getting state into Kubernetes. (For the Day One experience – an introduction to Kubernetes – see Rob’s previous blog post, Kubernetes 101__. – Ed.) So what is state? State is anything that lives beyond the current lifetime. And so we can think of state as things living beyond the function call. That’s likely what we do when we’re writing software, is building variables that may live longer than a function call. As we talk about web requests, we may talk about things that live longer than a request and response cycle. So as I go save the data, I then need to go run another request to get that data back, and so that’s the state that I need to save. We also have state that talks about server restarts. So I may need to save files on the file system, and if the server restarts, I want those files to stay on that file system. Or perhaps I’m going across a load balancer and so I need that file system to be shared across all my web servers. That’s state as well. So these are the types of things that we’re going to talk about today – getting that state stored inside of Kubernetes: Configuration details Secrets Data stores File stores Singleton services Among these, singleton services are special. This is a service which is making business decisions, and it needs a holistic and complete view of all of the system. So we need to have that one just maintain all of the pieces. A database is a great example of a singleton service. We can’t have more than one copy of the database in our environment. We may have different machines as part of a single cluster, but ultimately, we need to have that one unique database. It’s not like each pod can connect to its own database and store its own data and then a different pod may get to a different database. We need to have that one single source of truth. So we talked about configuration, secrets, data stores, and file stores, and these singleton services. Let’s dig into each one and start to explore how those work inside Kubernetes. How to Really “Get All” for Stateful Resources Now, one of the big annoying things is that, when we say kubectl get all , we don’t actually get everything. The name suggests that it’s supposed to get everything, but it really doesn’t. What it gets is all of the stateless resources. The developers of Kubernetes made this choice deliberately, because for things that can’t be recreated easily, they don’t want to expose them prematurely. So that if we were to delete and recreate everything, maybe we couldn’t get that secret back, because we didn’t have the unencrypted data. Or if we destroyed and recreated a stateful service like a database, we couldn’t get that data back. So kubectl get all deliberately doesn’t show stateful resources. The bummer part then is, as we go start exploring stateful resources, we’ll probably need a different mechanism for getting them. Now, we can definitely say kubectl get , and each of the things, and just list them. But if we say kubectl get all , we need to know that maybe our resources aren’t all listed there. Luckily, there is a mechanism to get all the things. It’s massively complex. First, I go grab all of the resources, kubectl api-resources . That will list all of the resources in my cluster, including custom resources, perhaps added by third party packages. Given that list, I could run that through an awk script to go grab the name, and I could comma separate them, and then I could say kubectl get that big list. So that’s kubectl get , kubectl api-resources , pipe it to awk, pipe it to grep, pipe it to xargs, pipe it to sed. I have that as an alias in my shell so that I can get all the all the things, including all of the stateful resources. You may choose to download these slides and copy that into your bash profile, as well and have a similar alias for truly getting all of the things. Conclusion This blog post, Introducing State in Kubernetes, is the first in a series covering Rob Richardson’s recent DZone webinar . Coming soon: the second blog post, File Storage, Configuration, and Secrets ; and the third blog post, Kubernetes StatefulSets and Q&A . You can try SingleStore for free today. If you download the SingleStore database software, and want to manage it with Kubernetes, download the SingleStore Kubernetes Operator as well. Alternatively, use SingleStore Managed Service , our elastic managed database service in the cloud. We at SingleStore manage Managed Service using our Kubernetes Operator, reducing complexity for you.", "date": "2020-04-18"},
{"website": "Single-Store", "title": "the-future-is-bottomless", "author": ["Joseph Victor"], "link": "https://www.singlestore.com/blog/the-future-is-bottomless/", "abstract": "SingleStore is a state of the art, distributed, scale-out, transaction processing and query execution engine. In the same system, we can do kickass large-scale analytical queries and ultra-low-latency key-value workloads. With the release of SingleStore DB 7.0 at the end of 2019, our new replication protocol, which is something I am personally very proud of, has greatly improved the durability of data stored in SingleStore clusters. I don’t think there’s any other system in the world that can do what we can do, and you can check our TPC-H, TPC-DS and TPC-C benchmark results to prove it. Yet, there’s an awkwardness. Compared with some of our competitors, who couldn’t dream of tackling the workloads we do, SingleStore doesn’t “just work”. Yes, we can work amazingly at scale, but actually scaling is a delicate and manual process. Yes, our durability is rock-solid, but users still have to take and manage their own backups. Yes, our integrated-storage-and-compute gives incredible (and predictable) query performance, but rebalancing could become awkward when it would require moving hundreds of terabytes of data that might never be queried. So the question we are left with is this: can we solve all of these problems in a way that doesn’t sacrifice the things that make SingleStore SingleStore? Can we build it with tight integration with our existing best-in-the-world clustered storage, and use it to simultaneously make SingleStore more suitable for system of record, and also have the perfect out-of-the-box cloud experience? Can we draw inspiration from the best ideas from Aurora and Snowflake and Big Query, but do it the SingleStore way? And can we do it in a way that works on any cloud, or on-prem? This is what we hope to achieve with Bottomless. What is Bottomless? Bottomless is the separation of storage and compute for SingleStore. Basically, data files (snapshots, logs and blobs) are persisted to S3 (or comparable blob storage, or even NFS) asynchronously. The “blobs” I’m talking about are the compressed, encoded data structures backing our columnstore. We still maintain high availability within the SingleStore cluster for the most recent data, and for availability in case of a failure, but long-term storage moves to blob storage. In this world, blobs which aren’t being queried can be deleted from SingleStore node’s local disk, allowing the cluster to hold more data than available disk, thus making the cluster’s storage “bottomless”. In fact, new replicas don’t need to download all blob files in order to come online, so creating and moving partitions (which are slices of each table’s data) becomes very cheap. The extra durability coming from all data eventually landing in blob storage is game-changing, making the way we used to think about backups and disaster recovery obsolete. It’s essentially “continuous backup”. This will obviously help us address larger petabyte-sized datasets for historical analytics. But, we don’t intend to do this only to support analytic workloads. Since we’re SingleStore, we’ll bring Bottomless storage to durable transactional, low-latency workloads without compromising performance. Much More Flexible Clustering Not requiring all the blobs to be locally on disk means provisioning or moving a replica can be very fast, on the order of minutes. This can give you faster rebalancing, allowing for a few interesting use cases. Scaling up and down: If you need to burst your workload in the cloud, you could increase the number of leaves and rebalance onto them while the workload runs. When the burst is over, you can more densely pack the partitions and save on the CPUs and memory you aren’t using to run queries. Adding read replicas: You could quickly provision a set of read-only replicas to run a query workload against, and discard them when you’re done. Low recovery time objective (RTO): Restoring doesn’t involve blobs; a database can be restored and available for queries very quickly in the event of a disaster. Read only point-in-time recovery (PITR): It is possible, in a Bottomless storage world, to quickly spin up a new set of replicas for a fixed point-in-time in the past, and discard these extra replicas when you’re done. Bottomless is Backup-Theory-Of-Everything All the data eventually goes to blob storage, so this, by definition, is a continuous backup. Using a forthcoming Global Versioning scheme – it’s not released yet, but it’s similar to Clock-SI with some SingleStore specific innovations – we will have the ability to recover to a consistent point any time in the past (or at the tip of history). Taking explicit daily backups, then, isn’t really a thing: you are always taking backups. If you want an off-site backup, simply replicate your remote storage (or, if it’s S3, just accept they have 11 nines of durability and enjoy). So in this world: Backup Incremental backup Continuous backup Disaster recovery Point-in-time recovery (PITR) are all the same feature, and it’s always-on at virtually no additional overhead. Of course, if you really want to take an explicit backup, you can, but now it can be taken instantaneously: simply record the current Global Version. Query Processing A Bottomless experience means that data files can be deleted from local disk once they are persisted to blob storage, at least if they aren’t being queried. This means the size of your local disk doesn’t determine how much data you can store. The user can insert and insert and insert and never delete, or have a very long retention period. Since our data is stored in a columnstore format, this can happen on the per-column level: columns which aren’t being queried often need not be stored locally! And read replicas querying only a subset of the data need only store that subset. But the system has to be a little bit smart. A newly provisioned replica will be “queryable” in minutes, but those queries will be slow, so the new replica should probably start downloading “popular” blobs immediately in the background. Further, a replica that might be used to maintain availability (that is, one which could become a master at any moment), needs to keep its cache as close as possible to the master’s, so that a failover won’t cause a blip in the workload while the cache is warming up. For the same reason, rebalance operations (and other controlled failovers) should make sure the cache is sufficiently hot before proceeding. All this to say, blob caching gives query processing incredible flexibility, including the ability to store more data than there is disk. The Possibilities are Bottomless We can now start to think about absolutely incredible capabilities. Possibilities such as: Data sharing Give an org within your company a consistent, read-only copy of the database which doesn’t take resources from the main cluster, and spin them down when you’re done. These copies can either be synchronized with the main cluster or an unchanging snapshot of the database at a specific point in the past. Merger-as-a-service Today we merge layers in log structured merge trees for each table in each partition. We can turn it into a separate service. It just reads and writes to blob storage, and tells the master about the new metadata. This moves the computationally expensive parts of our architecture into dedicated services. This service would be inherently stateless. Ingest-pipelines-as-a-service For append-only workloads, there is no reason for ingest to be in the same cluster. It can just read from your existing pipeline (Kafka, S3, or whatever), upload blobs, and tell the primary cluster about the new metadata. Managing Unpredictability To close, I’ll note the dynamic macro environment we’re in today. It’s difficult enough to anticipate the workload and storage needs for a system or collection of systems as changing requirements drive unexpected business scenarios. With Bottomless, the user doesn’t have to worry about provisioning storage, nor managing backups and durability. And changing the cluster size is a fully online size-of-metadata operation, so it’s easy to scale-up the resources you need or share data within your organization. It just works. What we seek to provide is even greater operational efficiency and workload isolation for system-of-record, low-latency transactions, along with support for large complex analytics, all in the same system. In this sense, we’re building Bottomless to have synergy with our concept of Universal Storage. The benefit will be even greater flexibility to right-size your data infrastructure to the current need, while reducing the variety of data management layers on your architecture.", "date": "2020-04-22"},
{"website": "Single-Store", "title": "memsql-now-available-on-red-hat-marketplace", "author": ["Rick Negrin"], "link": "https://www.singlestore.com/blog/memsql-now-available-on-red-hat-marketplace/", "abstract": "We have exciting news: SingleStore is now certified and available through Red Hat Marketplace. This open cloud marketplace makes it easier for developers to discover and access certified software for container-based environments , across all clouds and on-premises. Through Red Hat Marketplace, customers can take advantage of responsive support, streamlined billing and contracting, simplified governance, and single-dashboard visibility across clouds. Red Hat Marketplace provides four key benefits that database customers are looking for: lower TCO; simplification in governance; the ability to unify data processing at scale; and faster speed of application delivery, supported by Red Hat-certified interoperability of the solutions. Partnership Benefits Simplification : Managing increasing IT complexity while rapidly delivering new business capabilities is an ongoing challenge for leaders. Red Hat Marketplace helps minimize complexity for infrastructure and cloud operations, in much the same way that SingleStore simplifies and converges data and processing styles for applications with its NewSQL approach. Total Cost of Ownership (TCO): SingleStore is delivered via a Kubernetes Operator, leveraging Red Hat OpenShift and helping to reduce TCO. This makes it easier for developers and customers to use SingleStore, with its speed, scale, and support for relational SQL, alongside other Red Hat-certified software. Speed: SingleStore is readily available as a part of OpenShift, which accelerates the entire delivery cycle of business capabilities. Since SingleStore combines diverse OLTP and OLAP workloads, deploying on OpenShift further speeds application delivery. Scale: Red Hat Marketplace also enables SingleStore to democratize unified data processing at scale. With the solution partners in the Marketplace all certified on OpenShift infrastructure, customers can quickly develop and deliver systems that bring their unique business capabilities to market. Avoids Lock-in. Runs Anywhere. Deep Cost Benefits. Built in partnership by Red Hat and IBM, the Red Hat Marketplace is designed to meet the unique needs of developers, procurement teams, and IT leaders through simplified and streamlined access to popular enterprise software. All solutions available through the marketplace have been tested and certified for the Red Hat OpenShift Container Platform , the industry’s most comprehensive enterprise Kubernetes platform, allowing them to operate anywhere OpenShift runs. A container-based approach helps ensure that applications can consistently run and be managed identically, regardless of the underlying cloud infrastructure. This gives companies the flexibility to run their workloads on-premises, or in any public or private cloud, with improved portability and confidence, freeing companies from vendor lock-in. Developers can use popular enterprise software that allows them to build once and deploy anywhere. Procurement teams save time with streamlined price negotiation and license management. Business leaders can evaluate software performance and spend, with comprehensive monitoring and usage tracking across clouds. For companies building cloud-native infrastructure and applications, Red Hat Marketplace is an essential destination to unlock the value of cloud investments. Try it out at marketplace.redhat.com .", "date": "2020-04-29"},
{"website": "Single-Store", "title": "virtual-hug-we-could-all-benefit-from-some-humanity-and-positivity-right-now", "author": ["Gina von Esmarch"], "link": "https://www.singlestore.com/blog/virtual-hug-we-could-all-benefit-from-some-humanity-and-positivity-right-now/", "abstract": "We are globally faced with a unique set of circumstances at the moment. This virus is keeping us physically apart, and related business results are impacting all of us. But we can still come together virtually to build a sense of hope and global community. This message is from SingleStore’s Co-CEOs, Nikita Shamgunov and Raj Verma. Building Community & Camaraderi e SingleStore is working to do that by reaching out to its employees and the community to put humans – and humanity – back into business. In an effort to create human connectedness in a time of physical distancing , we have created the ‘Virtual Hug’ movement to instill positivity for all of us. This movement is in response to our employees’ ideas to go beyond the walls of our business, to think and act in unison, by supporting one another globally with a simple act of goodwill. We believe that together we can contribute to a positive state of mind.  In fact, you just might find a Virtual Hug in one of our email footers or in a virtual sticker. This virtual program emphasizes the importance and value we place on our people and the people with whom we do business, partner, and intersect. As part of Virtual Hug, we have instituted an employee matching donations program to help fight COVID-19. Virtual Hug also empowers SingleStore employees to express their concern and care about SingleStore customers and partners by sending them a Virtual Hug email footer and/or digital sticker. In addition, Virtual Hug is reaching out to the larger community. Many universities, charities, and non-profits are being impacted by the pandemic. To help them at this pivotal time, we are making SingleStore database software available for free download to these organizations. They can use the donated software at any scale, and we are providing them with SingleStore Managed Service, our elastic managed service in the cloud, at cost. Creating a Movement to Focus on the Positive We all know that the last several weeks haven’t been easy. And while the challenge continues, and while we may not be able to connect in person, we are reaching out to individuals and businesses everywhere, because we believe that together we can contribute to a positive solution. Virtual Hug aims to deliver one by building hope and a sense of community. That calls for thinking – and acting – beyond our business to work in unity with each other globally. Virtual Hug is a movement to create positivity for our employees, and for them to share that positivity with others during a time when we all need it the most. We invite you to join us to be part of our Virtual Hug Campaign, to share a sticker with someone that might enjoy it, or share how we might be able to help your university or non-profit charity. We’re all in this together.", "date": "2020-04-22"},
{"website": "Single-Store", "title": "memsql-improves-financial-operations-for-a-major-oil-and-gas-company", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/memsql-improves-financial-operations-for-a-major-oil-and-gas-company/", "abstract": "A major oil and gas company works in a volatile market, where IoT data floods in at the rate of gigabytes per second – and where up-to-date financial projections, generated by a .NET application, serve as a major competitive advantage. After considering MongoDB and Oracle Exadata, they replaced two NoSQL databases at the core of their analytics architecture  – MapR and ElasticSearch – with SingleStore. After a fast implementation, they now run financial forecasts multiple times a day, with much greater performance, with many fewer servers, and at much lower cost than with competitors. The company now also uses SingleStore for a wide range of financial forecasting applications, land contracts analysis, ERP reporting, and more. Introduction Finding, extracting, and delivering oil and gas is one of the most challenging businesses in the world. And it is also, in normal times, one of the most lucrative businesses in the world. But the recent crash in the price of oil, along with a collapse in demand caused by the coronavirus epidemic, have exerted unprecedented pressures on this industry. But oil and gas has always been a tough business. One of the biggest challenges, and opportunities, is financial. Well-run companies use mountains of data to help them manage their reserves of available capital. With good access to capital, when things look good, these companies invest in turning on existing supply, and exploring for new reserves. When times are tough, they reduce production and hold tight until the next opportunity. Players which are poorly run, or which are too small to compete effectively, tend to miss out on some of the peaks and get caught short in the valleys, driving them to sell out, or go out of business. The assets of the weaker company are then purchased by one of their better-run competitors, which thereby grows larger, and – to the extent that the company is run well – more resilient. A large oil and gas company has built an extensive Internet of Things (IoT) deployment across its physical plant of wells, drilling rigs, and pipelines. They also use a wide range of external data feeds, and mix this internally generated and market information to make smart decisions. However, a few years ago, their IT infrastructure was not up to the task. They replaced the databases at the core of their infrastructure with the SingleStore database platform. The primary use case for this newly powerful engine is to constantly adjust the company’s financial forecasts. Management is in constant touch with everyone from crews in the field, to lenders and analysts in financial capitals, while scrutinizing roughly 100 major variables at a time. With all of this input, they are constantly creating and stress-testing potential new budgets. All of this forms a company-wide risk exercise, with outsize rewards for success, and severe business consequences for failure. In this case study, we’ll describe in detail the business problem caused by the previous architecture; what alternative solutions were considered; and how SingleStore helped solve the problem. Now, using SingleStore, the company stands out from competitors, stays ahead of the market, and grows, through the best and the worst of times. This is likely to be of interest to other oil and gas companies; other companies that use IoT in daily operations; companies that need financial analytics and budgeting flexibility, across large and changing data sets; and any company that needs to speed up the flow of information, to meet the current wave of business challenges. Company Profile Several years ago, the oil and gas company described here, which is based in Texas, approached SingleStore. This Fortune 500 oil and gas company is publicly held, with tens of billions of dollars in annual revenues and thousands of employees. They actively explore for hydrocarbons – oil and gas – predominantly from shale. Shale sources are notable for being on the expensive side to develop, but are also more easily turned off and on for production than traditional sources of oil and gas. This means they can be actively managed, differently than traditional sources – and elicits the need for constant flows of information to get the best use from the shale resources. The company uses the data from sensors across the drilling process for real-time well production analysis. They combine this with a wide range of live, always-on, internal and external data feeds to embed themselves in financial and informational data flows. The company can use these massive data flows for a wide range of purposes: To quickly repair problems with rigs and other infrastructure To predict when maintenance should occur To decide when to run which rigs, as prices and customer demand change To work with financiers to manage cash and access to capital The company uses these data flows to reduce non-productive time (NPT) and cut costs – and also to tap capital markets and take advantage of opportunities. But the company’s ability to bring in and process all of this data was severely handicapped by slow and limited data processing operation. Because of this, decisions that could have saved the company millions of dollars a week – or made the company even more money – went unmade, as the data was not available when and where it was most needed. Existing resources were weak at every point: Ingest capability could not handle the massive flows of internal and external data in a timely fashion. Processing capability could not integrate live, streaming data with historical data, costing the company the ability to make decisions using all of the data that it has access to. Analytics were slow; in a business where time is money, waiting for answers was intolerable. The company needs to be able to constantly generate and test new budgets, on a region-by-region basis; slow analytics drastically limited flexibility. Concurrency for analytics was poor; the company could not support all of its current and desired users at once, to include ad hoc queries, business intelligence (BI) tool queries, and API calls from apps, all hitting the database(s) at the same time. Adding more analytics or BI tool users, or a new app, slowed down results for all users. The business saw that its weakest point was financial forecasting. If they could integrate all this data to generate solid budgets, and test them against different scenarios so as to best manage risk, they could get ahead of aggressive competition. After careful study, a SingleStore database was chosen, to speed ingest, processing, and the delivery of results, with increased concurrency. The new system was scoped, deployed, and brought into production in just seven months. Costs are down; flexibility, manageability, and profitability are up. The Business Problem The company is constantly looking for new customers and negotiating with existing ones. Like any public company, they need to meet detailed financial reporting requirements, and to accurately forecast revenues, costs, and profits and losses – per region, and then roll these budgets up for the company as a whole. None of this is easy. Shale oil company operations are heavily dependent on prices of conventional oil and gas, as production costs must be below the commodity price for a well to be profitable. These costs include drilling and completion costs, delivery costs, operating expenses, taxes, and more. Companies use decline curves to look at the rate of production decline over the life of a well, and work to determine what the estimated ultimate recovery (EUR) will be for a given well. Getting the economics right – not quarter by quarter, but day by day – is a necessity for these companies, given the impact of changing commodity prices. Financial forecasting for an oil and gas company depends on a constantly fluctuating mix of factors. When the price of oil is high enough, a producing well can be quite lucrative. Today’s Digital Oilfield uses IoT-enabled devices, sensors, and systems to reduce costs, improve performance, enhance safety, minimize downtime, and drive operational efficiencies. Maintaining extremely complex and expensive equipment, using predictive analytics to do preventive equipment maintenance, and managing the supply chain are all areas that have been improved with IoT. As a result of the company’s IoT investment, the business is able to manage, and centralize the flow of information from, the myriad of sensors on the drilling rig. Instead of managing each sensor independently, the company can now look holistically at everything from flow rates to temperature and pressure to mud pulse telemetry signals. They are able to profile potential sites extensively and choose which ones to develop. And they can use real-time data to decide when to take rigs online and offline, and when to operate existing wells. The cost of money – borrowing to fund capital requirements – is another variable. Each region where the company operates wells has its own forecasts and budget. Modeling to support the forecasting and budgeting process for each region is a huge challenge, due to the inherent difficulties of the business, and the sheer number of variables involved (more than 1,000, in many decisions). Managing and budgeting resources is a critical business requirement. Having the right data infrastructure to power the necessary decision-making processes, and the right people to run the infrastructure, is just as important as having the right equipment and the right people in the field. With the oil and gas company’s previous infrastructure, they were seeing more than 2x swings in budgetary allocations. That is, the size of opportunities – and the need for funds to pursue them – could easily fluctuate by more than a factor of two during a quarter. The business lost money due to unoptimized decision making, and lost opportunities that they could not see coming up, since the business could not reallocate funds quickly enough. Further, the company could not predict earnings accurately. All of this hurt the company’s financial results. The Technical Challenge The company had a wide range of data sources, which overloaded their existing system; growing processing requirements, which were not being met; and ever-increasing analytics demands, both in volume of queries and in number of users. Desired service level agreements (SLAs) for response time were dropping, in the face of stiff competition; SLAs for concurrency were rising, as human analysts, BI tools, financial dashboards, applications, and machine learning models compete for analytics access. The company used a combination of MapR to accumulate incoming data and ElasticSearch to power analytics. Internal opinions of the system were poor. One member of the assessment team described MapR harshly: “It’s a hole in the ground. You put your stuff in it and light it on fire.” MapR Technologies, the company behind this product, was sold last year to Hewlett-Packard Enterprise (HPE). Before: MapR and ElasticSearch powered analytics and apps. ElasticSearch is “a distributed, open source search and analytics engine for all types of data, including textual, numerical, geospatial, structured, and unstructured,” built on Apache Lucene. ElasticSearch is valuable for use with mixed data, but it’s a NoSQL solution, about which we’ve expressed our opinion before . Relational databases that support SQL tend to be much faster for running analytics, per machine, as well as far more compatible. NewSQL solutions, such as SingleStore, take this efficiency and scale it smoothly across distributed clusters of generic servers. Faced with the poor results they were experiencing, the company considered four alternative solutions, in addition to SingleStore. One of the alternatives, Oracle, is a relational database solution; the other three are NoSQL solutions. The alternatives to SingleStore were quickly found wanting: Expanded use of Elasticsearch for analytics . Elasticsearch lacks ACID guarantees – basically, the ability to do updates reliably – and true SQL capabilities. Performance is not up to requirements, and scalability also falls short. Expanded use of MapR for processing . MapR has the same overall characteristics and concerns as Elasticsearch, as well as serious business problems at the company that sold and supported the software. MongoDB . A third NoSQL contender, MongoDB is more widely used for mainstream workloads than the other two. However, Mongo still suffers from a lack of ACID guarantees and true SQL capabilities. Performance falls short. Oracle on Exadata . Scaling Oracle is expensive and inefficient. Neither price nor performance of Exadata were appealing to the company. SingleStore . SingleStore, a leading NewSQL database, has the price, performance, and SQL support that the company needs, as described below. SingleStore scales much more smoothly, for instance, than Oracle, delivering ten times the performance at one-fourth the cost . Solving the Problem The oil and gas company realized they had a big problem, deserving of investment at the company level. So they started by setting out the requirements for a solution: CEO-level visibility Global availability – more than 300 in initial named users Massive data volumes, including bulk ingest at approx 8GB/sec Trickle feeds updating millions of rows (very wide tables) per second Total compute – 2 clusters, 3200 CPUs, 30TB of Memory Interactive analytics – less than 3 seconds to display charts Integrate with Oracle, Spark, and big data (MapR) Support for financial application written in .NET Fastest possible time to go live (just 7 months, for SingleStore) SingleStore was considered carefully, and was found to not only solve the core business problem, but several additional problems as well. Here are key technical characteristics of the SingleStore solution: Fast time to market . A target was set of seven months, from the start of the deployment effort to deploying the first version of the solutions. System performance . Performance requirements were met from day one. Also, unlike other vendors considered, there was no need for outside consulting services to optimize performance and meet the requirement. Easy integration . The SingleStore solution integrated with existing systems – Oracle, .NET, and their existing big data solution (MapR) – out of the box. Simplicity . SingleStore was used to replace Elasticsearch and MapR for database vendor and database product consolidation, simplicity, and performance. Easy scale-out . Growth from 20 systems to 40 systems was accomplished overnight. Easy re-use of existing skills . In-house Oracle skills were easily used to manage SingleStore systems. In contrast to the previous use of NoSQL solutions (Elasticsearch and MapR), SingleStore delivered the familiarity of SQL, the ACID capabilities inherent to a relational database management system (RDBMS), and the performance and scalability of a distributed system – an attribute of SingleStore that is generally found in NoSQL solutions, and not generally found in truly SQL-compliant systems, but a key part of what SingleStore delivers. Price-performance has proven to be excellent. The closest competitor to SingleStore, in terms of capabilities, was Oracle Exadata. But SingleStore consistently excels over Oracle on many attributes, especially price-performance. Many organizations achieve as much as ten times the performance of Oracle at roughly one-fourth the cost . After: SingleStore powers analytics and apps The business benefits that the company has gained with the move to SingleStore are substantial, and becoming even more significant with time: Each department and each region now has the ability to assess current business and oil production, enabling them to forecast budgets from their computers or mobile devices at any time of day. With this flexibility, forecasts and budgets are now a major competitive advantage for the company; they can leverage available capital and their physical plant more efficiently than others, and with greater responsiveness to market conditions. The volatility of oil and gas prices having only increased in recent years, this flexibility helps separate winners, like this company, from others. The move to SingleStore has contributed to strong earnings and profitability, as shown in the company’s market capitalization and share price – which consistently rises in good times, and stays ahead of the competition when times are tough. After the first successful IoT project, this leading oil company has greatly expanded the use of SingleStore across a wide range of applications: additional  financial forecasting use cases; land contracts analysis; ERP reporting; and more. Conclusion The adoption of SingleStore at this leading oil and gas company led to many breakthroughs; improved performance, a simpler system design, new capabilities, and a new data backbone for a wide range of applications. Today, SingleStore is being considered for ever-broader use within the company. If you are interested in finding out more about SingleStore’s applicability to your data processing and analytics challenges, try SingleStore for free today or contact SingleStore .", "date": "2020-05-02"},
{"website": "Single-Store", "title": "dzone-memsql-webinar-3-of-3-kubernetes-statefulsets-and-qa", "author": ["Rob Richardson"], "link": "https://www.singlestore.com/blog/dzone-memsql-webinar-3-of-3-kubernetes-statefulsets-and-qa/", "abstract": "This is the third blog post derived from the recent DZone webinar given by Rob Richardson, technical evangelist at SingleStore. (From the link, you can easily download the slides and stream the webinar.) Kubernetes was originally developed to manage containers running stateless workloads, such as computation-intensive processing tasks and web servers. It was then extended to allow you to manage stateful workloads, such as data tables and e-commerce sessions, but doing so is not easy. Rob is one of the relatively few people who have figured out how to do this well. The webinar was well-attended, with a solid Q&A session at the end. There’s so much going on that we’ve split the webinar into three blog posts: The first blog post, Kubernetes and State ; the second blog post, Kubernetes File Storage, Configuration, and Secrets ; and this blog post, the third and final one, which covers Kubernetes StatefulSets, and the Q&A at the end of the webinar. You can read through all three for education – or use them as a step-by-step road map for actual implementation. You can also see Rob’s Kubernetes 101 blog post for an introduction to Kubernetes. As Rob mentions during his talk, SingleStore database software is well-suited for containerized software development, managed by Kubernetes. SingleStore has developed a Kubernetes Operator , which helps you use SingleStore within a containerized environment managed by Kubernetes. Alternatively, if you want to use a Kubernetes-controlled, elastic database in the cloud, without having to set up and maintain it yourself, consider SingleStore Managed Service . Managed Service is available on AWS, Google Cloud Platform, and is coming soon on Microsoft Azure. Following is a rough transcript of the final part of the talk, plus the Q&A at the end, illustrated by slides from the accompanying presentation. This third blog post describes Kubernetes StatefulSets, plus a range of topics from the Q&A. Kubernetes StatefulSets At the end of the previous section, we had taken care of configuration details and secrets. We had discussed a couple of ways of separating those details between developers and Operations, so as to keep things orderly. We also created volumes in which to store our persistent data. Now let’s take on StatefulSets and kind of level up and see if we can get even more of our application into place. StatefulSets allow us to create mechanisms where we have machines that need to talk to each other and have predictable names. Perhaps hosting a cluster, such as a database cluster. So this is a bundle of machines. It creates pods with predictable names. And with that, we can then communicate between containers. It’s easy if I’m creating a cluster of machines, or if I’m trying to connect to a cluster of machines, where I have to list the machines. Now, if the pods are coming and going and each one gets a random name every time, how does my application keep track of those, and how does it go ask the system how to connect? We avoid all of that by creating a StatefulSet. We can create a StatefulSet that will have a known number of machines – by which, I mean containers – and each of them will have a known name. And if one of them goes down, then a new one will come back with that exact same name, and the same data, and now we don’t need to keep track of the list of machines/containers. We can think of a StatefulSet as a deployment with predictable names. A deployment is that mechanism where I can say, “I would like three copies of it,” and so it’ll always ensure that three pods are running. In this case, the StatefulSet is that deployment with three pods running, but we know what those machine names will be. They’ll be zero, one, and two. Kubernetes will ensure that two doesn’t start up until one is running as well. So, StatefulSet use cases. We might want to spin up a Kafka cluster or an ELK stack cluster or a database cluster, such as my SingleStore. SingleStore uses lots of machines to distribute our data across commodity hardware so that we don’t need one big, beefy machine to be able to store our database. And with that distributed nature of SingleStore, we do need to have lots of machines and they need to be able to coordinate together. So with a StatefulSet, we can put our SingleStore machines into our Kubernetes cluster and ensure all of them are playing well together. So here’s that mechanism of a StatefulSet. We have each pod with a predictable name. So here’s pod-0. Here’s pod-1. Here’s pod-2. Each one will have a container running in it and then we have this headless service that allows for communication between pods. If a container wants to reach out to a particular pod, it can through this headless service, given its known name. And then our inbound traffic can reach in through this headless service, or we could create another service that is specific for communicating to some or all of these StatefulSet containers. So here’s a YAML file that defines that StatefulSet. You’ll notice it looks not that unlike a template from a deployment definition. We have the number of replicas. We have a template that describes how we build our thing. We’ll give it a service name. That’s that headless service that we must create. Our match labels. All of this kind of mirrors a deployment. But in this case, as we specify our template, our template matches the same mechanism as well. The only real difference is that we’ve identified it as a StatefulSet, and we’re listing our headless service that we’re using as well. So here’s that headless service, and what makes it headless is this cluster IP of none. The cluster IP of none says it’s not going to create a cluster IP. I can only connect to it by name. And the cool part then is that if I’m connecting to it to connect to any of the machines, I can do it much like I would a normal service. Let me just curl that service name. But if I want to speak to a particular machine, then I can talk to that machine. So in this case, perhaps the db-0 needs to talk to the db-1. So it’ll just connect to the db-1.db-service, and now it can communicate directly with that second pod through this headless service. Most apps will connect to it this way, just as db-service, and at that point it’ll just round robin across all of them. So as I have my SingleStore database set up, I’ll create this headless service to connect to my aggregator nodes, and just call that the aggregator service. But then, as the aggregators need to connect to each other, or as the aggregators need to connect to the leaf nodes, then they can do that by referencing them by name here. Now That We’ve Achieved Statefulness… So here’s that stateful Kubernetes service. Now, in our application we had a whole lot more pieces. We had a certificate and a domain name. We have connection details off to a database cluster. We have paths to our file storage. We have storage details, things that need to stay more persistent. And as we look through each of these pieces, we can see that not only do we have all of our normal stateless services, our containers, our pods, our services, but we also have all of the stateful pieces. Here’s a ConfigMap that specifies the domain name. Here is a secret that is the certificate. Here’s a ConfigMap or a secret that specifies the connection details to get to our database cluster. We have persistent volumes and persistent volume claims that allow us to get to durable storage elsewhere. And we have a StatefulSet that allows us to define a database cluster, or another cluster that takes multiple machines of known machine names. All of these stateful pieces now allow us to move the rest of our application into our Kubernetes cluster and maintain business continuity across the complexities that become our application. Volumes, Secrets, ConfigMaps, and StatefulSets are that next level that get us to the “day two” experience in Kubernetes. (For the “day one” experience – an introduction to Kubernetes – see Rob’s Kubernetes 101 blog post . Ed.) Q&A Q: How do I version my container secrets? A: We looked at a YAML definition that defined those secrets and I talked about how it’s scary to check those in the source control. Perhaps we want to create a separate repository that is known to be private, and check in those YAML files into the separate repository that keeps it secret. That’s one way to version our secrets. Another way is to keep the secrets elsewhere, perhaps in a vault like a HashiCorp vault or Azure key vault, and that will allow you to version the secrets as well. Q: How do I get https certificates into the web server containers? A: Let’s look at the stateful Kubernetes diagram again (above) and see how we did that. Now, if we were to terminate https here at the pods, we could map in a secret or a ConfigMap to get that content into each pod. I would really recommend a secret because it is a certificate. And so I could just map that in as one of the secrets, exposed either as environment variable or as a file. In this case, the certificate may get big. So perhaps mapping it in as a file might make more sense. As we step back, probably we don’t want to terminate SSL here at the container. Probably, we want to terminate SSL at the firewall, at the gateway up here. And the cool part then is I don’t need to get the certificate into my container. I can get my certificate here into my Ingress controller, and I have a single ConfigMap or a single secret that points it to the Ingress controller, and the Ingress controller can terminate SSL. At that point, all of the traffic downstream is just http between pods, and that might be an easier way to solve that problem. If I do need certificates in this communication between Ingress and service, between service and pods, then that’s where I would reach for something like Istio or Linkerd , a service mesh. And that service mesh can ensure that all of the communication within the cluster is encrypted separately. I still terminate SSL at the ingress controller, the public SSL. Then Istio picks up and will encrypt traffic between the Ingress controller and services, or between services and the pods, using sidecar proxies. Q: Why would I choose containers over virtual machines like EC2? A: What makes containers really, really powerful is that they act as ephemeral, isomorphic hardware. We can spin one up at the drop of the hat, and it only contains the differences between what I need to run my application and the host machine that is running that container. So containers end up being a lot lighter-weight, a lot smaller. Where a virtual machine may be tens or hundreds of gigs, a container is usually tens or hundreds of megs. And that shrinkage in storage cost also translates into run-time efficiency as well. I can get a whole lot more containers running in a given cluster than I can virtual machines. Using containers does mean that I need to think more about how I deploy my application. I need to split it into pieces. I need to define my services that will load balance across my containers. I need to terminate my SSL at the ingress point, perhaps, or create other abstractions to be able to handle these containers, but it is a whole lot lighter-weight mechanism to run. And because it’s much lighter-weight, then you’ll get operational efficiencies straight out of the gate. Ultimately, if you can move your application into containers, you will probably make it faster and easier to run straight away. Q: How do I create a deployment or a StatefulSet with one pod on each node in a cluster? A: So I want to create a StatefulSet much like I would create this StatefulSet for a database cluster – but, instead of having a known number of pods, I want to have one on each machine in my cluster. Maybe I want to do monitoring or shipping logs into Prometheus or something like that. That is called a DaemonSet, rather than a StatefulSet, and we can think of it exactly like a deployment. A DaemonSet is a deployment with one pod on each machine in the cluster. So ultimately, I’m not specifying the number of replicas. I’m just specifying hey, go do it. And it’ll put one pod on each node in my cluster. Q: How do I get SingleStore running on Kubernetes? A: We talked about how to create StatefulSets. SingleStore has three different types of machines. We have the leaf nodes that actually store the data. We have aggregators where clients connect to that and that will push the data across into all the leaf nodes that need to participate in that. And then we have a master aggregator, whose job it is to control the health of the cluster. So ultimately, as we get SingleStore into Kubernetes, we’ll need three different StatefulSet s. One for all of the leaf nodes, one for the aggregator nodes, and one for the master aggregator. But you don’t need to build up that complexity. If you want to, you can use the Kubernetes Operator created by SingleStore. It includes a custom resource definition where I can specify here’s how many aggregator nodes I want; here’s how many leaf nodes I want. And the operator will take care of deploying all of the pods, keeping them running, and deploying all the StatefulSets to ensure that I have durability and availability. I can also specify across availability zones if I would like. And ultimately, that operator then hides all of that complexity from us, removes all of the need to understand all that complexity. So I can just deploy the operator and fire it up and it just works. If you want to take it to the next level of using Kubernetes and avoid all of that complexity, then SingleStore Managed Service is actually running on top of the Kubernetes operator. It is exactly that same database engine, but with SingleStore Managed Service, you just sign up. You provision an on-demand database and it’ll give you a database connection. At that point, enter those database connection details in your application, and you have database-on-demand. The really cool thing is that all of the database backup and always-on details and all of the border security pieces that you might have to run by yourself are managed completely by SingleStore Managed Service. SingleStore Managed Service is available in Google Cloud and in AWS, and coming soon in Azure as well. And so you can start up a Managed Service cluster in the public cloud and the region that you would like. Q: Who should own the Kubernetes YAML files, developers or Operations? A: When we were digging into volumes and persistent volumes, we started talking about this mechanism. If developers choose to own these, then developers should own the persistent volume claim and the pod definition. Probably also the service and the deployment. And operations should own the persistent volume and the storage. That’s a really elegant separation of concerns where all of the physical details are owned by Operations and all of the application specific details are owned by developers. Now, we may choose to take a little bit different approach as well. We may just say, developers just want to focus on their app, and they just want to deploy it to somewhere, at which point we could opt to make all of the YAML files an Operations-specific thing. Perhaps they’re just bundled in as part of the build, and developers just build their application and commit their source code and the build server, the DevOps pipeline, actually pulls in all of the YAML files and containerizes the application and does all the things. That is a way to kind of simplify those details away from developers so they don’t have to worry about them. The downside is it makes it more difficult for developers to really experience the same mechanism that applications run in when they’re in production. At that point, developers say, “Well, how do I SSH into my container to do the last little bit of configuration?” That’s not really a thing in containers (laughs). So if you can give the YAML files to developers, I think that will really help in leveling up their skills and helping them really experience the system as it runs in production. If developers really, really don’t want to deal with it, perhaps you have a separate DevOps team that handles the integration pieces that would kick off during the build, but that does create layers of hierarchy that can get in the way sometimes. Q: Why would I use containers over a platform-as-a-service, like Azure Web Apps or AWS Elastic Beanstalk? A: Elastic Beanstalk is great for hosting apps. I may want to put them even into Lambda as a function-as-a-service. And that is a great abstraction platform. Under the hood, they probably have this load balancer that goes across all of the instances. They may have an ingress mechanism where I’m specifying the certificate. And if their assumptions on my hardware and framework really work for me, then Elastic Beanstalk or Azure Web Apps may be perfect. As I get deeper into microservices, and I really want to take control over all of these containers and pods, and I want to create a mechanism where some containers are exposed publicly and some are only exposed to other services, as I want to get into authentication between services, then that’s where the platform-as-a-service mechanisms kind of fall down. They’re built more for that simpler case, where I have a website – arguably, a monolithic website – that I just want to get online. As I want to dig in deeper, I may want to take control of those pieces, and that’s when I want to go more towards containers and Kubernetes. If I’m just in that simpler case, maybe platform as a service or function as a service might even be a cheaper option for me. Q: Should I provision my volume storage in the Kubernetes’ worker nodes hard drives? As we were doing the volumes here, we were looking at – this one was mounted to the hostPath, which means written onto the hard drive of the Kubernetes node, that Kubernetes machine. And that is dangerous. We’re assuming that that node stays, that that’s persistent. I probably want to put this on my SAN (storage area network), or put this in an S3 bucket, perhaps an Azure Files storage where that data is more durable and available across all the nodes in the environment. If I just need a temporary folder to be able to dump stuff for a time and make sure that it’s available for the next request, perhaps the hostPath is sufficient, but it depends on the need. hostPath is definitely faster, and so we won’t have network latency associated with getting to that persistent store. But it is fragile. We’re assuming when we say hostPath, that we don’t have pods on different nodes in our cluster, and that our host stays up. If that EC2 instance that happens to be running our cluster reboots or dies, then that hostPath resource may be gone on the next request. hostPath is great for temporary things. It’s not great for super durable things. Q: Do I need an Ingress controller to run stateless content? To get web traffic into stateless k8s resources, you generally have two options.  Either you create an Ingress controller (the preferred way for http/https traffic) or you modify your service to be of type LoadBalancer (the preferred way for non-http traffic like tcp traffic). An operator is not necessary to make stateless content work.  An operator is a k8s mechanism to abstract away the details of an application that has many parts.  One could configure the operator to simply say “I’d like my cluster to have 4 nodes”, and the operator can create the Deployments, Services, ConfigMaps, PersistentVolumes, PersistentVolumeClaims, Roles, RoleBindings, ServiceAccounts, and CronJobs to make it work.  Without an operator, one would need to make all these things one’s self and manage their interrelated dependencies.  An operator still does all this complexity, but wraps it in a simple experience for the consumer. It is not correct that to do stateless one /must/ use Ingress, and it is also not correct that to do stateful one /must/ use an operator.  It is correct that using Ingress makes stateless easier, and that using an operator makes stateful and stateless management easier. Conclusion This blog post, File Storage, Configuration, and Secrets , is the third in a series. The first, Kubernetes & State , and the second, File Storage, Configuration, and Secrets , appeared previously. These are all part of a series covering Rob Richardson’s recent DZone webinar . You can try SingleStore for free today. If you download the SingleStore database software, and want to manage it with Kubernetes, download the SingleStore Kubernetes Operator as well. Alternatively, use SingleStore Managed Service , our elastic managed database service in the cloud. We at SingleStore manage Managed Service using our Kubernetes Operator, reducing complexity for you.", "date": "2020-05-04"},
{"website": "Single-Store", "title": "explainable-churn-analysis-with-memsql-and-fiddler", "author": ["Kalyan Chintalapati"], "link": "https://www.singlestore.com/blog/explainable-churn-analysis-with-memsql-and-fiddler/", "abstract": "SingleStore and Fiddler Labs are working together to offer the power of SingleStore to users of Fiddler’s toolset for explainable AI – and to offer Fiddler’s explainability tools to the many SingleStore customers who are already using, or moving to, operational AI. To this end, the two companies are offering new, efficient ways to connect SingleStore self-managed software, and the SingleStore Managed Service managed service in the cloud, to Fiddler’s toolset. SingleStore is very well-suited to the demands of operationalizing AI – that is, powering machine learning models and AI applications as they’re put into production. SingleStore processes relational data, JSON data, time series data, geospatial data, and more, with blazing fast ingest speeds, eye-popping transaction performance, unmatched query responsiveness, and high concurrency. There are many resources available which demonstrate this, but among the best is this webinar on machine learning and AI from our own Eric Hanson. Fiddler provides a vital need, as AI moves out of labs and into the real world: Explainable AI . With Fiddler, you can describe why your AI models reached a given conclusion – why did one person get a loan, and another get selected for a clinical trial of a new drug? You need to have answers to these kinds of questions, beyond “the model said so.” With Fiddler, business analytics and data science teams can build and deploy models that are inherently explainable, and provide explainability even for models that do not have it built in from the start. In this blog post, we show how the SingleStore database and Fiddler work together to solve a knotty business problem: reducing churn among services customers. Solving this single problem cost-effectively can go far toward improving profitability in your business. A version of this blog post also appears on the Fiddler Labs website . Reducing Churn In today’s turbulent economy, customer needs are changing swiftly, causing business disruptions. As a leader, it’s more important than ever to understand the ‘why’ behind customers’ actions, so you can empower your teams to build successful products and services. Having the right infrastructure and tools is critical to enable your teams to respond to these dynamic needs quickly. Analyzing, predicting and monitoring churn accurately is critical for every data science or business intelligence team, especially in times like these. By complementing SingleStore’s industry-leading capability of enabling fast access to data at scale – across both streaming and historical datasets – with Fiddler’s Explainable AI Platform, which provides visibility, insights, and actionable analytics, business intelligence and analytics teams are perfectly positioned to respond to shifting customer needs and to ensure that customers are well served. Challenges with Churn Analysis There are a common set of analytics challenges to address when monitoring customer churn: Descriptive analytics – Identifying possible reasons for customer churn Diagnostic analytics – Ascribing actual customer churn to specific reasons Predictive analytics – Predicting churn and the reasons for it Prescriptive analytics – Identifying potential actions to reduce future churn Solution To begin with, all customer data needs to be effectively organized in one place, to enable teams to leverage AI-powered technologies to model the churn. The database needs to be able to handle streaming data in real time, so analytics are performed on the “latest and greatest” data. SingleStore’s fast streaming database is an ideal platform for organizing this data. SingleStore provides unmatched processing capabilities for both transactions and queries, vital for operational analytics, machine learning, and AI. By streaming customer data into SingleStore, users get all the interactive query capabilities, along with the ability to keep the data up-to-date within milliseconds. We can then run churn analytics on this by connecting it with Fiddler,  an explainable AI platform that helps data scientists and analysts build trust with AI decisions inside their organizations. Cutting-edge explainability algorithms help business users make sense of AI, getting answers to cause-and-effect questions on the drivers behind a prediction. BI teams regularly iterate on multiple models to predict churn. Fiddler allows comparison of performance of multiple models to identify the most effective one for a given task. The Explainable AI Platform offers a lens to assess model performance and validate models. Since the precision of the churn model not only impacts performance but also decision-making, customers would like to iterate on the models, and monitor several versions of the model, to help in identifying problems and solutions. Integrating SingleStore with Fiddler – As Easy as 1-2-3! While many analytics tasks bring in data from a CSV file, data used in machine learning generally resides in a database like SingleStore. Bringing this data into Fiddler’s Explainable AI Platform as an ML dataset is the first step in the AI/ML development workflow. Explainable churn analysis with SingleStore and Fiddler. There are a few ways to bring data into Fiddler. It can be imported directly from any database which Fiddler supports, such as SingleStore; uploaded as a CSV file in the browser; or loaded directly from a file store such as AWS S3. 1 . Preparing Data in SingleStore For the purpose of this blog post, we used the popular Telco Churn Dataset from Kaggle as an example. Let’s assume this Telco company saves all customer data in SingleStore in a database named churn_example and a table named telco_customer_churn .  Here’s the DDL; you can also access the DDL on Github . The sample dataset is available for download from an S3 bucket . DROP DATABASE IF EXISTS churn_example;\n\nCREATE DATABASE churn_example;\n\nUSE churn_example;\n\nCREATE TABLE telco_customer_churn\n\n(\n\n    customerID TEXT,\n\n    gender TEXT,\n\n    SeniorCitizen BOOLEAN,\n\n    Partner TEXT,\n\n    Dependents TEXT,\n\n    tenure INT,\n\n    PhoneService TEXT,\n\n    MultipleLines TEXT,\n\n    InternetService TEXT,\n\n    OnlineSecurity TEXT,\n\n    OnlineBackup TEXT,\n\n    DeviceProtection TEXT,\n\n    TechSupport TEXT,\n\n    StreamingTV TEXT,\n\n    StreamingMovies TEXT,\n\n    Contract TEXT,\n\n    PaperlessBilling TEXT,\n\n    PaymentMethod TEXT,\n\n    MonthlyCharges DECIMAL(13, 4),\n\n    TotalCharges DECIMAL(13, 4),\n\n    Churn TEXT,\n\n    PRIMARY KEY (customerID)\n\n); For the purposes of this tutorial, we will populate telco_customer_churn with the information from the Kaggle Telco Churn dataset. This can be done by creating a SingleStore Pipeline to load the data from S3. CREATE or REPLACE PIPELINE `telco_customer_churn` AS\n\n    LOAD DATA S3 'download.singlestore.com/first-time/WA_Fn-UseC_-Telco-Customer-Churn.csv'\n\n    CONFIG '{\"region\": \"us-east-1\"}'\n\n    SKIP DUPLICATE KEY ERRORS\n\n    INTO TABLE `telco_customer_churn`\n\n    FIELDS\n\n        TERMINATED BY ','\n\n        OPTIONALLY ENCLOSED BY '\"'\n\n    IGNORE 1 LINES;\n\nSTART PIPELINE `telco_customer_churn` FOREGROUND; Once the data is in place, run SELECT * from telco_customer_churn LIMIT 10 to validate the data and the column names. 2 . Connecting SingleStore to Fiddler To add SingleStore as a data source, we need the authentication information to construct the database URI. We can add SingleStore as the type of database in Fiddler and furnish the rest of the details like the hostname, port, username, password, the database to connect to and add the connector. The settings are validated via a connection to the database. The ability to add and remove database connectors is an Administrator-privileged operation, whereas usage of data from the connectors themselves is a non-administrators operation. Importing data from SingleStore into Fiddler. Once the connector for SingleStore is in place, users can then import data using the connector into Fiddler. To do this, begin Fiddler’s dataset upload workflow, to add this data as a dataset for churn analysis. Select the data source that was just added, then enter the SQL query to select the data to be imported into Fiddler. In the background, a database connection is established, the SQL query is run, and its results are ingested into Fiddler. The data is then parsed and validated to infer the column names and data types, which are presented to the user for adjustment as needed. 3 . Analyzing Churn using Explainable AI Next, start analyzing the data. Glean more insights about the features like their mean, variance, and also look at the statistical covariance across all the features. Fiddler’s Explainable AI Platform allows us to analyze the features using feature distribution and mutual information charts to visualize their statistical dependencies, among other details. In order to leverage Explainable AI, Fiddler offers 2 options: Bring in a custom pre-trained model Build an interpretable model Data scientists can use option #1 to bring in their own ML models, built on open-source or custom ML platforms, and then use Fiddler to explain them. To do this, Fiddler offers a Python library that data scientists can use to upload a pre-trained model. Alternatively, they can follow option #2, and use Fiddler to build an interpretable model on the platform. Once the models are ingested, Fiddler uses sophisticated explainability algorithms to compute the causal drivers for model predictions. And the explanations are presented in a collection of dashboards, consumable by business users as well as data scientists. For example, an Account Manager in a Customer Success team can use the dashboard below to understand why this customer is likely to churn, with a probability of 75%. Top five reasons why this customer is likely to churn. As shown in the picture above, the top five reasons why this customer is likely to churn are: Short tenure (only 4 months) on the telecom service Lack of online security in her package Being on a month-to-month contract Not having tech support And paying high monthly charges of $76 Using this information, the Account Manager can then intervene and fiddle with the inputs, and examine what-if scenarios . For example, they can see what would happen if they tried a couple of actions. Offer the customer “TechSupport” Reduce her Monthly Charges from $76 to $60 Two actions would reduce the customer’s likelihood to churn from 75% to 40%. In addition to simple, business user-facing explanations, Fiddler also supports advanced slicing and explanation capabilities to go deeper into the data and the models – for instance, to understand why the cohort of, for example, low-tenure users are churning. Slicing to explain a cohort of high churn, low tenure users. 4 . Using SingleStore and Fiddler Together in Production Once a churn model is operationalized, Fiddler can be connected to a live SingleStore database to continuously monitor , predict, and explain the churn model. After the model is live, users can monitor the performance in production and close the feedback loop. Fiddler will connect with SingleStore to score the model in a continuous manner and monitor performance. That way our users can track business KPIs, performance metrics, and setup alerts when something goes out of the ordinary.  Fiddler’s Explainable Monitoring features help analysts and data scientists to keep track of the following: Feature Attributions: Outputs of explainability algorithms that allow further investigation, helping to understand which features are the most important causal drivers for model predictions within a given time frame. Data Drift: Track the data coming from SingleStore, so that analysts and data scientists can get visibility into any training-serving skew. Outliers : The prediction time series from the model outputs, and outliers that are automatically detected for egregious high-churn or low-churn predictions. Monitoring dashboard showing Outliers (orange dots) in Churn Prediction. SingleStore is well-suited to work in tandem with an Explainable AI Platform like Fiddler: Speed : the faster the database runs , the more up-to-date monitoring is, and more explanations can be tested against more data in a given time frame. This increases the functional value of Fiddler. Scale : SingleStore, as a fully distributed database, can easily be scaled out as needed. There’s no barrier to handling more data, or to speeding up processing of existing data volumes. SQL : SingleStore is a relational database with native ANSI SQL support. It fully preserves schema, which serves as a valuable input to Fiddler, while connecting to the wide range of BI tools that depend on SQL. (Also, see our take on NoSQL vs NewSQL .) Spark : In addition to supporting a wide range of monitoring and analytics tools, including Fiddler, SingleStore also boasts the SingleStore Spark Connector 3.0, speeding model processing via predicate pushdown. Kafka : Streaming data platforms such as Kafka are often used to bring the “latest and greatest” data to machine learning models, without delay. Kafka partitions map directly to SingleStore leaf node partitions, allowing very rapid, parallel ingest and exactly-once processing . Converged data : In addition to relational data, SingleStore handles semi-structured JSON data, full-text search for unstructured data, geospatial data, and time series data, with specific time series functionality . In addition to downloadable, self-managed software that runs on all platforms, from on-premises to any cloud, SingleStore offers SingleStore Managed Service , an elastic managed service in the cloud. With Managed Service, you can minimize the time to stand up, and the operational effort to run, an advanced, feature-rich database. Like SingleStore, Fiddler works with a wide range of BI tools. You simply export data from Fiddler to a range of tools. The data from Fiddler can then be integrated into dashboards, reports, and the answers to interactive queries. Interactive churn dashboard. Conclusion The seamless integration between SingleStore and Fiddler enables easy import of data from inside SingleStore to Fiddler’s Explainable AI Platform, for ML insights in minutes. Data science and analytics teams working on customer churn can upload their pre-trained models, or quickly generate interpretable models on Fiddler. Once the models are in place, Fiddler users can easily create interactive dashboards for users in the business. They can also export explanations into their favorite BI tool of choice. Business users, such as account managers, can then self-serve to understand why a customer is likely to churn, run what-if scenarios, and fiddle with data to see what actions they can take to save a customer from churning. You can try SingleStore for free or contact SingleStore for more information. Or contact Fiddler for a trial in the cloud.", "date": "2020-05-05"},
{"website": "Single-Store", "title": "case-study-true-digital-group-helps-to-flatten-the-curve-with-memsql", "author": ["Domenic Ravita"], "link": "https://www.singlestore.com/blog/case-study-true-digital-group-helps-to-flatten-the-curve-with-memsql/", "abstract": "True Digital Group is using SingleStore to power a contact tracing app, preventing the spread of COVID-19 in Thailand. (See our joint press release .) The app uses React and Web Workers in the frontend, with SingleStore Pipelines and time series functions processing fast ingestion of events, and geospatial functions used to plot data on the map in real-time. The first functional version of the app was built in two weeks. Our Global Lockdown Preventing the spread of the COVID-19 disease has been our collective priority in this pandemic. By avoiding non-essential, discretionary travel such as shopping trips and social gatherings, we keep ourselves safe and prevent ourselves from unknowingly being asymptomatic carriers to our most vulnerable members of society. The latest data from the hardest hit hotspots of the outbreak show that social distancing is starting to have the intended effect of flattening the curve. Millions of businesses have been forced to close with a disproportionate impact on the restaurant, entertainment, and travel and hospitality industries. Finding ways to safely reopen economies is urgently needed. How to Start Moving Again Shelter-in-place orders have been needed because carriers of COVID-19 can be asymptomatic for as much as two weeks and could unknowingly spread the virus. Once a person tests positive, the traditional approach is to then conduct contact tracing, which is done by interviewing each patient for 12 hours to determine with whom he or she has been in contact over the prior days and weeks before the diagnosis. Most people can’t recall all the individuals with whom they’ve been in contact, and they could be unaware of how their proximity to others in public places and stores may have spread the virus to those around them. These interviews require human contact tracers. To conduct interviews in the U.S., given its population, estimates for the number of human contact tracers needed range from 100,000 to 300,000. This makes it clear that to start moving and interacting face-to-face again we must be able to automate some aspects of this to perform “test and trace” at scale and in a timely fashion. The solution to this problem may be your phone. The True Solution True Digital Group’s Analytics team is rapidly moving on this idea in Thailand. Operating under True Digital Group, the digital arm of True Corporation, the leading telecommunication provider in the country, the Analytics team is collecting and aggregating anonymous mobile phone locations in real-time so they can assist the Thai Government authorities in providing a tool to help track the spread of COVID-19. The tracepulse app combines live and historical alerting for COVID-19 tracking. To be the most effective in preventing further spread, the data has to be low-latency, real-time data, which shows how these anonymous mobile phones are moving, and to immediately and proactively determine the location hubs of increasing population density. Large gatherings are especially worrisome during a pandemic like this. The key concept is to track how mass population moves from one part of the country to the other, then determine what they can deliver to the Thai Government authorities to help them, firstly, protect themselves and, secondly, obviate the need for testing for the virus. They would like to have a real-time visualization and alerting system which automatically determines where large gatherings are forming, via the geographic density of the mobile phone locations. There are several technical challenges that must be overcome to anonymously track and trace at scale. First of all, they have to perform event stream processing on 500,000 anonymous location events every second for over 30 million mobile phones. Second, the visualization has to be updated at least every 2 minutes. Third, the solution must support a moving time window of undetermined duration to support queries against real-time and recent data for contact tracing. Fourth, reference data and historical data must be available in some situations for further analysis and investigation – and this dataset is 2 petabytes. Fifth, the real-time visualization must support geoanalytic queries with ultra-low latency, on the order of milliseconds. And finally, the first functional version of this system must be built in only two weeks. Prior to this pandemic, True Digital Group’s Analytics team was already leading in the analytics space for telco, with the successful deployment of a platform able to crunch trillions of relevant customer data points such as location, browsing activity, etc. True Digital Group’s Analytics team successfully launched location services based on this, for example to optimize retail branch locations. The computation strategy was to land data in Hadoop, then run map-reduce operations on the data on a scheduled batch basis every 24 hours to create the aggregates and derived data. However, the tool was designed to operate in batch mode, and had no real-time capability. Once the pandemic struck, the need moved from “delayed-data-is-good-enough” to “in-the-moment, analytics with an SLA” being a necessity. The solution was to stream raw event data from the core telco network into SingleStore via Google Cloud Storage (GCS). Now, with this solution, they can see how many people might be crowding into a specific area. The first usage is to power the wall-sized dashboards in the crisis operations control center for the country. These dashboards allow users to zoom into one province, district, or sub-district of Thailand, and see in real time how many people are located there. This helps to anticipate where large gatherings may be forming. The second usage is for determining the optimal placement of resources, such as medical supplies. This enables dynamic reallocation of human resources and physical resources, which makes the disaster relief efforts more proactive and timely. Building a Modern App with SingleStore SingleStore is a distributed operational database that provides a scalable backend, combining the capabilities of transactional and analytical databases, while supporting high-speed parallel ingestion and relational SQL. It also provides specialized data types and functions supporting geospatial, time-series, document, and key-value use cases. The combination of these capabilities and the familiar ANSI SQL of SingleStore suited the requirements of this project, enabling True Digital to integrate and simplify its backend data platform. Anonymized location data is continuously written from the core telecom network to Google Cloud Storage (GCS) as a series of Parquet files. As these files appear, they are continuously ingested in parallel by SingleStore Pipelines and then processed by SingleStore stored procedures. While this is happening, the database is serving highly-concurrent queries invoked by an interactive geoanalytic dashboard. This drillable dashboard provides an in-the-moment view of the density of mobile phones in any given area, from the country level down to the city block. Users can visually explore this geographical information and zoom in for more granular, detailed views of clusters of anonymized mobile phones to spot potential emerging problems. Each of these clicks progressively query SingleStore to calculate the new statistics for each geographical region as users drill down. SingleStore’s geospatial functions support the ability to determine the spatial relationships across data, such as distance, containment, and intersection calculations. Geospatial objects are first-class datatypes in SingleStore, which means that you can use spatial relationships to join tables. For example, to find all of the businesses in a neighborhood, you join the businesses table with the neighborhoods table on whether the business location (a point) is contained by the neighborhood’s shape (a polygon). The True Digital solution is a progressive web application (PWA) which leverages React and NodeJS to expose APIs to backend microservices, ensuring the best possible interactive user experience. The use of React and Web Workers for the frontend implementation helps to ensure the immediate, interactive responsiveness of the app. For fast-changing data, the response time of the backend database is a crucial component in ensuring this fast user experience. SingleStore is uniquely suited to these types of workloads, which combine the scaling of new data being written while simultaneously handling highly-concurrent lookup queries as well as complex analytic queries. With SingleStore supporting more than a trillion rows per second on raw data, it comfortably handles the ingestion of 500,000 events per second and being the application database for this customer-facing application. The app supports queries by location over a recent history of data, allowing users to see trends over time. Data is defined as sequences of events, with each event labeled with a timestamp. Time series data can be stored in SingleStore using rowstore or columnstore tables. Each row has a time-valued attribute to hold the event time. Time series functions , such as time bucketing and smoothing, are available. Blazingly Fast Delivery Faced with such a fast-moving public health crisis, every minute that can be saved in getting an accurate, precise prevention system in place literally saves lives. In the frontlines of this global war against a virus, we see our healthcare and essential services workers as heroes. The data architects, data engineers, and frontend developers working 24/7 to get this life-saving system in production are helping prevent the spread, contain the virus, and flatten the curve. In just one week, the first operational version of the application was designed and built with SingleStore, showing that rapid development and quick, agile iterative DevOps cycles can be done with SingleStore. Since then, the functionality has been iteratively developed and extended in order to demonstrate new possibilities for analysis to the Thai Government authorities. The Light at the End of the Tunnel While many are working around the world to develop treatments and a vaccine, this will take quite a long time. In the US, the COVID-19 Healthcare Coalition is one example of such an effort. It is a private-sector led response that brings together healthcare organizations, technology firms, nonprofits, academia, and startups to preserve the healthcare delivery system and help protect U.S. populations. New crisis management systems such as True Digital’s will continue to be rapidly developed and expanded to meet the needs of the new normal. In the workplace, companies must consider how this new normal impacts business continuity planning and human resources. Efforts like True Digital’s to provide a means of identifying emerging breakdowns in social distancing help us to better manage in a crisis. Approaches like this may even provide a way to get the world moving again in a safe fashion and restart economies while we await a vaccine. SingleStore is proud to partner with True Digital for such an important cause. About True Digital Group True Digital Group is the digital arm of True Corporation, Thailand’s leading telecommunication company. With the goal of becoming the country’s leader in digital solutions, True Digital Group has built deep competences in cutting-edge technologies such as artificial intelligence (AI), big data, blockchain, cloud, Internet of Things (IoT), and robotics. With this, True Digital Group is able to build a unique ecosystem of digital platforms and solutions, addressing the digital needs of consumers, merchants, and enterprises. True Digital Group has launched regional operations across Southeast Asia, with Indonesia and the Philippines as the first two new markets. Visit the website at www.truedigital.com for more details.", "date": "2020-05-07"},
{"website": "Single-Store", "title": "new-environment-validation-checks-in-memsql-tools", "author": ["Roxanna Pourzand"], "link": "https://www.singlestore.com/blog/new-environment-validation-checks-in-memsql-tools/", "abstract": "You can now validate the hardware and software environment for SingleStore, before you install the software. As you may already know, the SingleStore-Report module within SingleStore Tools allows you to run health checks on your database. The new functionality in SingleStore-Report lets you run environment validation checks – before you install SingleStore. These checks ensure that machines are appropriately configured for the best performance of the database, and the least likelihood of future problems. SingleStore is a high-performance database; it ingests data rapidly, and supports workloads that require fast transaction processing, low query latency, and high concurrency. (Which includes lots of simultaneous users, including individual SQL queries, business intelligence tools, applications, and machine learning models.) You may have heard the database analogy that compares a fast database to a racecar. At SingleStore, we like this comparison. To confirm your race car is ready to perform, you need to ensure that the foundation of the car – the configuration of the environment that your database will run in – is in good shape. The Importance of Configuration for a Distributed Database The world of distributed databases includes a large number of databases that run best on a single machine, or that require very careful configuration and management to “scale out” in a limited fashion. There is also a small number of newer, relational databases that are distributed, which are referred to as NewSQL databases. SingleStore is a NewSQL database . There are also a wide, and growing, range of NoSQL databases. (We have our own take on NoSQL .) Any truly distributed database, whether NewSQL or NoSQL, depends on the cooperation of many separate nodes to function. For a distributed database, your performance is only as good as your slowest node – and, the more time each and every node is up and running, the faster and more reliable the whole database is. Since performance issues can often be tied directly to configuration – whether it be the operating system, network, or disk – it is important to have a foolproof way to check that your entire system is set up properly, down to the last node, so you can unleash the full power of the database. Configuring SingleStore What does this mean for the SingleStore database? Optimal configuration will lead to the best possible performance, with the fewest possible problems. This translates to obtaining more value out of your data faster, and spending less time on tuning and troubleshooting. We will review some examples below of configuration recommendations that can affect the database. Our system requirements documentation contains a full list of these items. At a high level, it is essential to confirm that your machines have enough resources to operate the database, and that your operating system is configured properly. Here are three examples: From a hardware perspective, we require a minimum of 4 cores, and 8 GB of RAM, per server. Some operating system configuration recommendations include checks for settings like ‘Transparent Huge Pages,’; if this setting is not disabled, you may experience inconsistent query performance. Configuring Non-Uniform Memory Access (NUMA), on your machines that can benefit from it, will improve your performance significantly given your workload. SingleStore recommends more than a dozen specific system configuration settings be checked and, where needed, changed, before you install the SingleStore database. It’s tedious to have to check/change each of them by hand, across every host in your cluster – and any tedious manual effort opens the door for potential errors. To avoid the manual effort that would otherwise be needed, use SingleStore-Report to do this work. SingleStore-Report summarizes all the information in one place, through an easy-to-use interface. Pre-Installation Validation in SingleStore Tools The SingleStore Report module collects a report on your cluster that covers a series of checks around the SingleStore cluster, databases within it, and the system hosting it. It also outputs a set of pass/fail checks on settings, based on SingleStore-recommended best practices. In the previous versions, the Report module expected that you had an existing SingleStore cluster when using it. Recently, we released a version of the SingleStore Report that adds the ability to run pre-environment checks, which only reports on components that are applicable to host machines without the SingleStore software installed on them . This feature allows you to confirm the validity of the environment before installing the database and loading data. Incorporating this pre-check functionality in SingleStore Tools means you have a clear-cut path to identify any problems, before you proceed with the installation process. How Does It Work? In the first step of SingleStore software installation, you download SingleStore Tools to manage the software. (Later in the process, Tools will deploy the database for you as well.) After you register the machines that you plan to install SingleStore on with Tools, don’t proceed immediately with SingleStore installation as the next step. Instead, run the following command to check your environment first: memsql-report collect --validate-env This collects a report with pre-installation environment checks, without installing anything. After the report has been collected, you can run: memsql-report check --validate-env --report-path </path/to/report> This outputs a list of all pre-environment checks in a pass/fail/warn manner, and alerts you to any potential configuration changes that you need to make before proceeding with the installation. Below is a sample output of this check. See below for the actions we recommend you take, if you get this report in your own environment. $ memsql-report check --validate-env --report-path report-2020-05-05T000204.tar.gz\n\n✘ minFreeKbytes ................................. [FAIL]\n\nFAIL vm.min_free_kbytes = 67584 too low on 172.31.68.57\n\nNOTE https://docs.singlestore.com/memsql-report-redir/configure-linux-vm-settings\n\n✓ validateSsd ................................... [PASS]\n\n✘ partitionsConsistency ......................... [WARN]\n\nWARN Some partitions start sector on nvme0n1 are inconsistent (should be a multiple of 4096): [nvme0n1p1]\n\n✓ diskUsage ..................................... [PASS]\n\n✓ chronydDisabled ............................... [PASS]\n\n✓ cpuHyperThreading ............................. [PASS]\n\n✓ cpuModel ...................................... [PASS]\n\nNOTE AMD EPYC 7571 on all\n\n✓ orchestratorProcesses ......................... [PASS]\n\n✓ cpuFeatures ................................... [PASS]\n\n✓ vmOvercommit .................................. [PASS]\n\n✓ defunctProcesses .............................. [PASS]\n\n✓ kernelVersions ................................ [PASS]\n\nNOTE 4.18 on all\n\n✓ cpuFreqPolicy ................................. [PASS]\n\n✘ maxMapCount ................................... [FAIL]\n\nFAIL vm.max_map_count = 65530 too low on 172.31.68.57\n\nNOTE https://docs.singlestore.com/memsql-report-redir/configure-linux-vm-settings\n\n✓ collectionErrors .............................. [PASS]\n\n✘ transparentHugepage ........................... [FAIL]\n\nFAIL /sys/kernel/mm/transparent_hugepage/enabled is [always] on 172.31.68.57\n\nFAIL /sys/kernel/mm/transparent_hugepage/defrag is [madvise] on 172.31.68.57\n\nNOTE https://docs.singlestore.com/memsql-report-redir/transparent-hugepage\n\nSome checks failed: 11 PASS, 2 WARN, 3 FAIL Seeing this report, as a user, I would do the following: Increase the vm setting, vm.max _ map _ count, to the specified value, which will decrease the risk of memory errors. Check on the consistency of disk partitions on this host, so that performance of disk operations across the cluster falls within a similar range. Disable Transparent Huge Pages to ensure that the system has consistent query performance times. For more information on these commands, please see the documentation on memsql-report collect and memsql-report check . Conclusion and What’s Next The ability to check your system, prior to installation, against a vetted set of SingleStore best practices ensures that your database is production-ready to serve your critical applications. Stay tuned for additional functionality around pre-install checks that we will provide in the future. This includes the ability for the tool to run performance benchmarks on your hardware. Also, we plan to incorporate the validation check directly in the installation process, so you won’t have to run it separately anymore. If you have a SingleStore cluster that’s managed by SingleStore tools, you can try this today! Check and see if your servers are configured appropriately. If you are not yet using SingleStore, you can try SingleStore for free or contact SingleStore .", "date": "2020-05-20"},
{"website": "Single-Store", "title": "memsql-fuels-growth-with-50m-hercules-capital-partnership", "author": ["Gina von Esmarch"], "link": "https://www.singlestore.com/blog/memsql-fuels-growth-with-50m-hercules-capital-partnership/", "abstract": "The following press release appeared on BusinessWire today , with the subhead: Non-Dilutive Financing Provides Tech Company with Added Fuel to Accelerate Growth. SAN FRANCISCO–( BUSINESS WIRE )–SingleStore, the No-Limits Database for operational analytics and cloud-native applications, has signed a debt facility that provides up to $50 million of new capital. Hercules Capital (NYSE: HTGC), the largest non-bank venture debt provider with more than $2.4 billion in total assets, served as underwriter for the financing. “This is a strong vote of confidence for SingleStore’s future. Our FY20 financial performance concluded with an impressive 70% growth in annual recurring revenue (ARR), and with a single-digit cash burn, our current ARR vs. cash outlay ratio is less than one,” remarked SingleStore co-CEO Raj Verma. “We are a well-run, well-capitalized business. As we move forward, our charter remains the same. SingleStore will always prioritize our customers’ success by delivering world-class products and services, increasing shareholder value, and contributing to our community in every way we can.” “This is a strong vote of confidence for SingleStore’s future. Our FY20 financial performance concluded with an impressive 70% growth in annual recurring revenue (ARR), and with a single-digit cash burn, our current ARR vs. cash outlay ratio is less than one.” Tweet this SingleStore delivers breakthroughs for modern data workloads spanning transactions, analytics, and AI for leading businesses undergoing digital transformation. SingleStore is a converged data platform leader with hundreds of enterprise customers, including many within the Fortune 500. In 2019, the company launched Managed Service, its cloud database available on AWS, GCP and Azure. SingleStore is proud to work with leading partners such as IBM/Red Hat, Fiserv, Infosys, Tata Consultancy Services and Virtusa to deliver innovative solutions to its customers. Headquartered in San Francisco, and with offices in Portland, Seattle, Sunnyvale, London, Lisbon, and Kyiv, SingleStore is backed by GV (formerly Google Ventures), Glynn Capital, Accel, Khosla Ventures, Caffeinated Capital, Data Collective, and IA Ventures. “This structured investment represents a significant commitment from Hercules and provides an example of the breadth of our platform and our ability to finance growth-orientated, institutionally-backed technology companies at various stages. We are impressed with the work that the SingleStore management team has accomplished operationally and excited to begin our partnership with one of the promising companies in the database market,” said Steve Kuo, senior managing director technology group head for Hercules. About SingleStore SingleStore is The No-Limits Database TM , powering modern applications and analytical systems with a cloud-native, massively scalable architecture for maximum ingest and query performance at the highest concurrency. SingleStore envisions a world where every business can make decisions in real time and every experience is optimized through data. Global enterprises use the SingleStore distributed database to easily ingest, process, analyze, and act on data in order to thrive in today’s insight-driven economy. SingleStore is optimized to run on any public cloud or on premises with commodity hardware. Visit www.singlestore.com or follow us @SingleStoreDB. About Hercules Capital, Inc. Hercules Capital, Inc. (NYSE: HTGC) is the leading and largest specialty finance company focused on providing senior secured venture growth loans to high-growth, innovative venture capital-backed companies in a broad variety of technology, life sciences and sustainable and renewable technology industries. Since inception (December 2003), Hercules has committed more than $10.2 billion to over 500 companies and is the lender of choice for entrepreneurs and venture capital firms seeking growth capital financing. Companies interested in learning more about financing opportunities should contact info@htgc.com , or call 650.289.3060.", "date": "2020-05-11"},
{"website": "Single-Store", "title": "memsql-welcomes-george-kadifa-as-a-business-growth-advisor", "author": ["Gina von Esmarch"], "link": "https://www.singlestore.com/blog/memsql-welcomes-george-kadifa-as-a-business-growth-advisor/", "abstract": "Tech and financial veteran George Kadifa brings deep experience to further accelerate SingleStore’s growth SAN FRANCISCO – May 27, 2020 – George Kadifa, co-founder and Managing Director of Sumeru Equity Partners, will join SingleStore, the Database of NOW™ for operational analytics and cloud-native applications, as an advisor. He will provide SingleStore with growth guidance, market insights, and relevant industry connections, as a proven seasoned leader with database industry experience. “With three decades of operating experience in the technology sector, George brings SingleStore a wealth of knowledge, having previously run large data management businesses at HP and Oracle. As we embark on this next stage of our hyper-growth, his sage council will be invaluable. I couldn’t have asked for a better person to join us on our advisory board,” said SingleStore co-CEO Raj Verma. Prior to co-founding Sumeru, George served as executive vice president at Hewlett-Packard (HP). He reported to HP’s CEO and was responsible for leading growth initiatives and alliance programs with its largest customers, key partners, and service providers. George also held the executive vice president position at HP Software, a $4 billion business including application delivery management, big data, enterprise security and IT/cloud operations management. “I’ve kept an eye on SingleStore for some time, and I am very excited about the emerging space it is in today, for a number of reasons. Globally we are at a time when the importance of data insights is paramount, as is the need for companies to move to the cloud with technology that removes latency and is easy to implement,” remarked Kadifa. “I look forward to drawing on my expertise with technology companies of various sizes to help further SingleStore’s growth trajectory as we hit our next stage of expansion.” Before his work at HP, George was an operating partner at Silver Lake, a global technology investment firm with more than $26 billion of assets under management. George was responsible for driving growth and operational improvement at a variety of enterprises within the firm’s large cap investment fund. George also has held leadership positions at IBM, Oracle, Corio Corp. (of which he was the chairman and CEO), Booz-Allen & Hamilton, and Xerox. About SingleStore SingleStore is The Database of NOW™, powering modern applications and analytical systems with a cloud-native, massively scalable architecture for maximum ingest and query performance at the highest concurrency. SingleStore envisions a world where every business can make decisions in real time and every experience is optimized through data. Global enterprises use the SingleStore distributed database to easily ingest, process, analyze, and act on data in order to thrive in today’s insight-driven economy. SingleStore is optimized to run on any public cloud, or on-premises with commodity hardware. Visit www.singlestore.com or follow us @SingleStoreDB . Contact Gabrielle Jasinski 708.732.3913 gabrielle@bospar.com", "date": "2020-05-27"},
{"website": "Single-Store", "title": "memsql-7-1", "author": ["Eric Hanson"], "link": "https://www.singlestore.com/blog/memsql-7-1/", "abstract": "SingleStore is proud to announce the general availability of SingleStore DB 7.1 for immediate download. SingleStore DB 7.1 is also available today on SingleStore Managed Service , the company’s elastic cloud database, available on public cloud providers around the world. With the availability of SingleStore DB 7.1, SingleStore further cements itself as the leading NewSQL platform for real-time analytics and augmented transaction processing [ Ron19 ] , delivering superb transactional and analytical performance in one system. The release also delivers improved resilience features to further strengthen SingleStore for mission-critical applications providing transaction processing, operational analytics, and more. With limitless scalability, high availability (HA) built in, disaster recovery (DR) support, and full SQL, SingleStore is a strong foundation for your digital transformation initiatives [ DT20 ] . With SingleStore, you can change the way you do business, and capture new opportunities without concern for speed and scale limits. At the same time, you can rely on existing skills and the SQL ecosystem to build groundbreaking new applications. This blog post describes the major features of SingleStore DB 7.1 self-managed software and SingleStore Managed Service in some detail. For even more detail, see the release notes . Also see our detailed updates on disaster recovery improvements in SingleStore DB 7.1 , improvements to SingleStore Tools , and our new SingleStore Training modules . Major Feature Areas in SingleStore DB 7.1 Along with incremental improvements across the range of database platform functionality, SingleStore DB 7.1 provides especially notable new features in the following areas. Universal Storage SingleStore’s Universal Storage technology is being delivered as an arc of new features over several releases. The aim of this technology is to support real-time analytics and online transaction processing (OLTP) on SingleStore, with great performance and low total cost of ownership (TCO). The Universal Storage story began with SingleStore DB 7.0 [ SS19 ] and continues in SingleStore DB 7.1. Universal Storage is an extension of our columnstore technology that excels at analytics and which also improves OLTP, including support for indexes, unique keys, seeks, and fast, highly selective, nested-loop-style joins. Because the data is highly compressed and doesn’t all have to fit in RAM, Universal Storage gives excellent TCO. See our upcoming SingleStore DB 7.1 Universal Storage blog for more details [ SS20 ] . Resilience Databases that support applications that people run their businesses on must be resilient to failure. SingleStore has supported transactions, persistence, high availability (HA), and disaster recovery (DR) for years. We’re continuing to build and strengthen these feature areas. In SingleStore DB 7.1, we’re delivering fast DR failback. We use a differential approach to reduce the amount of data that must be transmitted when you bring a previously-failed primary cluster back on line and make it the primary again. See our blog on DR failback for more information [ YW20 ] . SingleStore’s HA is based on a partitioned data model, keeping an original version of each partition and a copy on two separate leaf nodes, where SingleStore stores actual data; metadata is stored in aggregator nodes. (We’ll use the term “node” to mean “leaf node” in the following.) If a node fails, a replica of its data is readily available on another node, and all data remains accessible. In the previous SingleStore release, SingleStore DB 7.0, all partitions from one node are replicated on a single other node. So if one node fails, that second node can become a hot spot, subject to contention, since it has twice as much data as before – and thus, twice its normal share of query processing work. In SingleStore DB 7.1, we introduce a feature called load-balanced failover to solve this problem. With load-balanced failover, when a node fails, then the partitions on each node will be spread around to several other nodes, not just one. This avoids the creation of a hot spot after a failure, which would slow performance, while also keeping the probability low that loss of a node will cause unavailability of data. Programmability SingleStore supports the MPSQL language for programming internal extensions, including stored procedures and user-defined functions.  The 7.1 release enhances this capability, in particular for transaction processing application development and for handling JSON. Extensibility We’re making it easier than ever to build applications with SingleStore. For example, particularly in OLTP applications, it’s common to fetch a single row in a stored procedure, and fill a separate local variable with the value for each column of the row. This was possible in our MPSQL language before, but took more lines of code than was desirable. This same operation can now be performed with much-improved brevity, using syntax like this: SELECT first_name, last_name INTO var_first, var_last\n\n            FROM employees WHERE id = 2; In addition, this also works with dynamic SQL. For example: EXECUTE IMMEDIATE sql_string INTO var_first, var_last; This syntax is similar to that used in other database products, making it familiar and easier to learn, and simplifying application porting from other database offerings into SingleStore. In addition, access to fields of local variables or parameters that have a RECORD type is now supported in SQL statements in stored procedures. This can reduce the number of lines of code to perform cursor-like operations a lot. For example: create table t(a int, b int);\n\ninsert t values(1, 2),(3,4),(5,6),(7,8);\n\ncreate table t2(a int, b int);\n\ndelimiter //\n\ncreate or replace procedure p() as\n\ndeclare q query(a int, b int) = select a, b from t where a >= 5;\n\nbegin\n\n  for r in collect(q) loop\n\n    call echo_stuff(r);\n\n    insert t2 values(**r.a**, **r.b**); -- notice use of record fields inline\n\n  end loop;\n\nend //\n\ncreate or replace procedure echo_stuff(r record(a int, b int)) as\n\nbegin\n\n  echo select **r.a**, **r.b**; -- also using record fields here\n\nend //\n\ndelimiter ; Then we can call this procedure and see the results, as follows: memsql> call p();\n\n+------+------+\n\n| a    | b    |\n\n+------+------+\n\n|    5 |    6 |\n\n+------+------+\n\n1 row in set (0.25 sec)\n\n+------+------+\n\n| a    | b    |\n\n+------+------+\n\n|    7 |    8 |\n\n+------+------+\n\n1 row in set (0.26 sec)\n\nQuery OK, 0 rows affected (0.26 sec)\n\nmemsql> select * from t2;\n\n+------+------+\n\n| a    | b    |\n\n+------+------+\n\n|    7 |    8 |\n\n|    5 |    6 |\n\n+------+------+ Even though we’ve made this much easier, don’t forget that it’s often best to do things in a set-oriented way, with single SQL statements, rather than process a row at a time in a loop, where possible. That’s because it’s faster and can use fewer lines of code. Can you think of a way to do the same thing as the stored procedure (SP) above, without processing one record at a time? Manageability Backups are a key part of almost everyone’s system management and HA/DR processes. SingleStore DB 7.0 introduced incremental backup, which is convenient, and in some cases critical, since it allows a quick incremental backup to be done before important operations, such as rolling out a new application change. SingleStore DB 7.1 extends our incremental backup capability in the cloud by now supporting Amazon S3, Azure Blob Store, and GCP storage as targets for incremental backups. Upgrades are a big part of system management. SingleStore DB 7.1 supports online upgrade from 7.0 and earlier. This allows moving to the latest software version without downtime. A new management view, MV _ BLOCKED _ QUERIES, shows what queries are blocked and why they are blocked, to help troubleshoot query wait problems. Other Features Additional features available in SingleStore DB 7.1 include the following. TABLE() The new TABLE() function is a table-valued function that takes an array as an argument, and returns a set of rows, with one row per array element. It can be used in the FROM clause of a query. When used with the json_to_array() function, it’s a convenient way to explode JSON arrays into rowsets. For example: create table t(id int, json_col json);\n\ninsert into t values(1, '[1,2,3]');\n\ninsert into t values(2, '[4,5]');\n\nselect * from t join table(json_to_array(t.json_col)); Which outputs: +------+----------+-----------+\n\n| id   | json_col | table_col |\n\n+------+----------+-----------+\n\n|    1 | [1,2,3]  | 1         |\n\n|    1 | [1,2,3]  | 2         |\n\n|    1 | [1,2,3]  | 3         |\n\n|    2 | [4,5]    | 4         |\n\n|    2 | [4,5]    | 5         |\n\n+------+----------+-----------+ Partition Split During Backup SingleStore has great elasticity. You can add leaf nodes and rebalance data partitions across them online. A limit to our elasticity occurs once we get down to having one database partition per leaf node, and then you can’t subdivide further, so adding nodes and rebalancing is no longer effective. Prior to SingleStore DB 7.1, increasing the number of partitions in a database required creating a new database with more partitions, and then copying the data over to it from the original database. This could be labor-intensive. In SingleStore DB 7.1, we automate the partition split process. This is done through a new version of the backup command, which splits partitions in two as it creates the backup. When you restore the backup, you have the same tables, data, and other objects, but there are twice as many partitions. This saves a lot of labor and time compared to the old approach. Global Temporary Tables SingleStore has supported temporary tables for years. The standard temporary tables are local to a session. Two different sessions could use the same temporary table name, and there would be two different tables. Global temporary tables, on the other hand, which are new in SingleStore DB 7.1, can be shared across sessions. These are in-memory rowstore tables that are non-logged, so they are great for inserting bursts of data with no I/O impact. Depending on your hardware, but especially if it uses a regular hard disk drive for the log, the new global temporary tables may triple the speed of burst inserts. Of course, since global temporary tables are non-logged, and the data is present only in RAM, if a leaf node goes down, the table will go into an error state, and can’t be queried. If this happens, your application will have to drop the table and re-create it to begin again. This tradeoff between fault tolerance and speed may be fine for applications which are trying to maximize performance for bursts of inserts, or operations on intermediate scratch tables used for data transformation. If you need fault tolerance, simply use regular tables. Ingest We’ve expanded our ingest and backup support for Google Cloud Storage (GCS), continuing our integrations with the industry’s major cloud providers. We now support loading data through SingleStore pipelines from a GCS bucket. We now support backing up and restoring your database from a GCS bucket directly. We used to support this using the S3 interface, but we now interact with the GCS interface directly, eliminating any potential compatibility issues. Query Optimization Several query optimization improvements are provided in 7.1, including: Cardinality estimation for joins, with histograms A command to purge all plans from the plan cache, to enable reliable query compilation time testing and validation that changes to statistics are having the desired effect Enabling of additional query shapes involving nested subselects A NOPARAM() function to allow you to avoid parameterization of literals in filters. NOPARAM() can help with parameter-sensitive plans (e.g., when you have a query that needs a different plan when run for a narrow range of dates compared to a wide range of dates). You can also use NOPARAM() to force a query to recompile a new plan for new parameter values if it is very sensitive to its parameters. This can allow you to always get a good plan for those parameters, at the expense of spending more time to compile the query every time it is run with a new parameter. Industry-Standard Built-In Functions We have introduced a number of standard built-in functions to ease the development process and migration from other tools, including: TRUNC() – Allows you to truncate a date to a given granularity, or a number to a given precision. TO_NUMBER() – Allows you to convert a string or expression to a number data type, with an optional format specification REGEXP_SUBSTR – This allows you to return a substring that matches a given regular expression pattern. Security We now support a configurable password complexity policy that can control different aspects of passwords: the length, the number of alphanumeric and special characters, as well as the number of occurrences of sequences or repeated patterns in the password. We also now support configurable parameters that you can specify to lock a user’s account after a certain number of failed login attempts. You can specify the amount of time the account is locked after the logout attempt threshold is reached. This feature is to ensure your systems are protected from password attacks. These new features allow you to implement your organization’s password policies directly in SingleStore, without relying on a third-party tool or application program. This may be convenient for your developers, and may save you money and reduce complexity by not requiring an extra software purchase. Conclusion The SingleStore DB 7.1 release delivers a big advance in our ability to support system-of-record applications on SingleStore, including improvements to Universal Storage (unique keys on columnstores, fast selective joins on columnstores); DR failback; easier stored procedure programming for transactional applications; useful new built-in functions to ease application development and migration; and more. Are you seeking a data platform with unlimited scale and power, which your SQL-trained developers will readily be able to learn and use, to enable your digital transformation initiatives? Look no further than SingleStore DB 7.1, available for immediate download and via SingleStore Managed Service . References [ Ron19 ] Adam Ronthal, There is Only One DBMS Market!, https://blogs.gartner.com/adam-ronthal/2019/07/17/one-dbms-market/ , Gartner, 2019. [ DT20 ] What is Digital Transformation? The Enterprisers Project, https://enterprisersproject.com/what-is-digital-transformation , 2020. [ SS19 ] SingleStore Universal Storage – And Then There Was One, https://www.singlestore.com/blog/memsql-Universal Storage-then-there-was-one/ , September, 2019. [ SS20 ] SingleStore Universal Storage, Episode 2, <link-TK > , April, 2020. [ YW20 ] Yu-wang Wang, DR Failback in SingleStore DB 7.1, https://www.singlestore.com/blog/fast-disaster-recovery-failback-memsql-7-1 , May, 2020.", "date": "2020-05-28"},
{"website": "Single-Store", "title": "introducing-memsql-self-paced-training", "author": ["Lorrin Smith-Bates"], "link": "https://www.singlestore.com/blog/introducing-memsql-self-paced-training/", "abstract": "Jack Welch, former CEO of General Electric, once said, “An organization’s ability to learn, and translate that learning into action rapidly, is the ultimate competitive advantage.” SingleStore is designed to be as familiar as possible to SQL users, while bringing the advantages of full, linear scale-out to the relational database world. Our new, free training courses are designed to leverage whatever you already know, provide you with the new information you need, and help you to be successful with SingleStore right from day one. First, a word about the design of the training. SingleStore’s self-paced training is designed to be engaging, interactive, bite-sized, and sticky. It uses evidence-based methods to ensure that you will learn what you need to leverage SingleStore to its fullest potential, and make sure that the learning sticks. You’ll find captivating video lessons and demonstrations, knowledge checks, hands-on exercises, and quizzes. Rather than the hands-on exercises being “clickersizes”, or exercises in copying and pasting, they are designed to make you think about the problem and come up with your own answer. However, they also have hints and solutions in case you get stuck. We are working to build out a full library of courses. We’ve started by concentrating on developer content, so that developers can get started with SingleStore right from the start. Our First Eight Training Courses When you first access our training courses, you’ll need to create a log-in. Then you can dive right into our first tranche of eight free training courses, available on the SingleStore training page : SingleStore Overview . A brief introduction to SingleStore’s features. Getting Started with SingleStore Managed Service . An introduction to our managed service. Schema Design . When to use rowstores, columnstores, shard keys, indexes, etc. Data Ingest . Getting data into SingleStore, using SingleStore Pipelines etc. Query Tuning . Learning to build SingleStore queries in the most efficient way. Time Series Data Capture and Analysis . Focusing on time series features. SingleStore Procedural SQL . How to extend and customize SingleStore’s functionality. Working with JSON Data . Ingesting and accessing JSON data in SingleStore tables. In these courses, we take you through a journey with SingleStore. SingleStore Overview is for users of our self-managed, downloadable software that you deploy and administer yourself, as well as for users of SingleStore Managed Service. Getting Started with SingleStore Managed Service introduces you specifically to our managed service, and will help you get started with your Managed Service free trial. Be sure to check both of them out before starting your free trial. Use these training modules to get initial momentum as to how to get the most out of SingleStore. If you are still deciding whether to use self-managed SingleStore or the managed service, SingleStore Managed Service , you can use the two resources to help you choose. All the other courses apply equally to both. (Some of our Administrator courses, coming in the months ahead, will only apply to self-managed SingleStore.) In Schema Design , you’ll learn about SingleStore’s distributed architecture, including how SingleStore distributes data across leaf nodes. You’ll also learn what SingleStore columnstore, rowstore, and reference tables are, and what they are good for. With that knowledge you’ll learn how to design a database schema that is optimized for your data and workloads. You’ll learn when it is best to use a columnstore or a rowstore, and if it is better to use a reference table or a sharded table. You’ll learn about how sharding works, and which columns in your tables are the best to use as shard keys. You’ll also see how SingleStore indexes work, how to create them, and when it is best to use them. In Data Ingest , you’ll learn how to load data in batches. And you’ll go beyond that to learn how to take advantage of real-time data ingest, provided by SingleStore’s robust pipeline infrastructure and exactly-once pipeline semantics. You will learn how to ingest data from Kafka ( which also supports exactly-once updating ) and from AWS S3 buckets. Because our self-paced training is hands-on, you’ll have the opportunity to create Kafka and AWS S3 pipelines during the training. Query Tuning shows you how to use SingleStore tools like EXPLAIN and PROFILE to see how SingleStore runs your queries. You’ll be able to see where the expensive operations are when running your queries, and you’ll be able to tune those queries to make them more efficient. You’ll see what changes can be made in your schema design in order to optimize your most important queries. SingleStore’s capabilities include the ability to work with time series data. The module, Time Series Data Capture and Analysis , helps you understand how to use SingleStore for this purpose, including the use of some new SingleStore functions—FIRST(), LAST(), and TIME _ BUCKET()—that help in your analysis of time series data. SingleStore also has a robust procedural language, SingleStore Procedural SQL (MPSQL), that you will learn how to use. You can easily include business logic in SingleStore so that the computation that needs to take place on your data can take place where the data lives, instead of having your applications make expensive trips back and forth to the data store. You’ll create stored procedures, user defined functions (UDFs), table-valued functions (TVFs), and user-defined aggregate functions (UDAFs). All of these functions and procedures allow you to encapsulate custom logic in your databases, extending the functionality of SingleStore. In the Working with JSON module you’ll learn how to create tables that include columns to contain JSON data, how to access and update JSON data once it is stored in a SingleStore table. You’ll also learn how to create a persisted computed column from JSON data so that you can create indexes on JSON data and to make access to that data more efficient. What’s in the Works In the months ahead, there is more developer content coming. There will be new training on full-text indexes, geospatial data, query hints, benchmarking and tuning, and window functions. We are also working on administrator content. Soon there will be a course on deploying SingleStore, followed by training on security, upgrading, high availability, fail over, disaster recovery, monitoring, resource governance, troubleshooting, and SingleStore Managed Service administration. We will make it clear which administrator modules apply to both flavors of SingleStore, and which you only need with self-managed SingleStore—not with SingleStore Managed Service. By the time we’re done, we’ll cover SingleStore development and SingleStore administration from beginning to end. You can expect new courses, on a regular basis, covering all aspects of SingleStore. We will also update existing content to keep up with new and changed features in new releases of SingleStore. Be sure to come back to the training site as we will be updating it on a regular basis with new developer and administrator self-paced training. What to Do Next Have I mentioned that SingleStore self-paced training is free? You can complete the developer content in the free trial of either SingleStore Managed Service or self-managed SingleStore software, or the cluster-in-a-box Docker container. You will be able to complete the future Administrator content in a set of Docker containers that virtualizes a four-node cluster, or in AWS EC2 instances. We also want to get your feedback. When you take a SingleStore self-paced course, please rate it and send us a note at training@singlestore.com . As Jack Welch indicated in the quote that starts this post, knowledge is power. When you combine that power with the power of SingleStore, imagine what can happen!", "date": "2020-05-28"},
{"website": "Single-Store", "title": "fast-disaster-recovery-failback-memsql-7-1", "author": ["Yu-wang Wang"], "link": "https://www.singlestore.com/blog/fast-disaster-recovery-failback-memsql-7-1/", "abstract": "SingleStore is steadily improving its high availability (HA)  and disaster recovery (DR) features. The design of primary partitions and their corresponding secondary partitions on leaf nodes provided HA, while the replications configured on a remote cluster support DR. SingleStore DB 7.0 marked a major milestone with the introduction of fast synchronous replication . This makes it much more practical to use high availability (HA) with SingleStore, as you can easily keep a live copy of your database contents in the cluster. In this release, SingleStore DB 7.1, we are adding a much-requested feature, fast disaster recovery failback (also known as simply DR failback). Our next release, SingleStore DB 7.5, later in 2020, will add another much-requested feature, point-in-time restore. In this blog post I will describe how DR failback has worked in the past and the important improvements in this new release, SingleStore DB 7.1. We believe that you will find the new DR failback feature to be robust and highly useful. Improvements in Replication for Failback The point of DR is that you can quickly switch processing away from your primary cluster, if it fails or needs to be upgraded, and onto your secondary. In most scenarios, you will then want to fix your primary database, switch operations back to it, and return to using the copy that you had failed over to as your secondary: DR failback. In previous releases of SingleStore, switching back to the original primary required you to halt operations, restore the original primary fully from the now-live backup site – a slow process – and only then resume operations. The length of the process, and resulting downtime, was a big problem, even preventing some sites from failing back. Now, in SingleStore DB 7.1, you can bring the primary up-to-date with the backup by restoring only the changes that have occurred since the original failure (the “disaster” in the term “disaster recovery”). This is usually much, much faster than a full restore, and greatly improves the recovery time objective (RTO), sharply reducing downtime and minimizing the risk of additional problems. With fast DR failback you can reverse a replication relationship in minutes, instead of hours or even days. You may recognize that this improvement is also a significant step toward point-in-time restore (PITR), planned for the next major release of SingleStore. Using the REPLICATE Statement for Fast Failback The REPLICATE statement allows users to create a replica database from a primary database that resides in a different region. In previous releases of SingleStore, this replication was always a full replication. Just as it’s been with previous releases, the user can still pause and resume the replication as needed. But the full replication from scratch is not usually necessary, and takes a long time. We can do better. SingleStore DB 7.1 delivers a new capability to replicate only the differences between the secondary and the primary, instead of requiring full replication. This improves the customer’s recovery time objective (RTO) by a great deal. We will describe the commands here, and then walk you through the correct steps to use for a couple of real-life scenarios: restoring to the original database site after a failover, and handling upgrades to hardware and/or software, with the upgrades implemented one cluster at a time. The following commands are used: REPLICATE DATABASE db_name [WITH {SYNC | ASYNC} DURABILITY] FROM master_user[:'master_password']@master_host[:master_port][/master_db_name]; This is the version of replicate which SingleStore has had for a long time. In this version, you create a new replication relationship from the primary cluster/database to a secondary cluster/database. A new database, db _ name, is created, and the system replicates master _ db _ name from scratch, in full, to new _ db. REPLICATE DATABASE db_name WITH FORCE DIFFERENTIAL FROM master_user[:'master_password']@master_host[:master_port][/master_db_name]; This is the new version of REPLICATE which is now available in SingleStore DB 7.1. In this version, you create a new replication relationship from the primary cluster/database to a secondary cluster/database. (Note that the secondary database db _ name should already exist for this command to work.) The system replicates master _ db _ name to db _ name by replicating only the differences between the two. Note : For simplicity, in the rest of this blog post, we use a database name in place of the complex user, password, and host string used in 1. and 2. above. FLUSH TABLES WITH READ ONLY; This is also a new feature in SingleStore DB 7.1. This command marks databases read-only. Similar to its cousin, FLUSH TABLES WITH READ LOCK, the system allows read-only transactions to go through and flushes out already-started write transactions. Instead of queueing new WRITE transactions, the READ-ONLY command fails new WRITE transactions. This syntax is used to stop any new changes on the primary cluster so the secondary cluster has a chance to catch up before a DR failback. UNLOCK TABLES; This is an existing syntax for opening up the current database for reads and writes. STOP REPLICATING db_name; This is an existing syntax for stopping a DR replication relationship. Only after this command is run on a secondary cluster/db can it become a primary cluster/db to accept writes. Scenario 1: The Original Primary Cluster Goes Down, then Comes Back Later This is the classic DR failback scenario. The primary cluster goes down, and processing switches to the secondary. Then the primary cluster comes back up, and you want to switch processing from the secondary back to the primary, as quickly as possible. This is much quicker with the new WITH FORCE DIFFERENTIAL option for the REPLICATE command. 1 . DB1 is serving the traffic and DB2 is replicating from DB1. 2 . DB1 goes down (show in red background below). The user stops replication to DB2 and routes update and query traffic to DB2. New changes continue to be applied to DB2 and are stored as ChangeSet1 (CS1). 3 . DB1 comes back online. It’s time to get the replication going from DB2 to DB1 so we are protected from the next disaster. With the new feature, DB1 only needs to catch up with DB2 for the changes made when DB1 was offline. This is much faster than replicating the whole DB2 to DB1. Now DB1 is replicating from DB1, and the roles of primary and secondary are reversed. Unfortunately, the world does not stop during the catching up period. More changes (ChangeSet2) are applied to DB2. 4 . If you don’t need to make DB1 the primary database again, you are set. Life goes on. DB1 will catch up with DB2 again as time goes by. However, if DB1 is better equipped, with more memory, better CPU, more disk space, etc.; is closer to where most updates and/or queries originate; or is otherwise preferable, follow the steps below to make DB1 the primary database again. 5 . Start by “locking” DB2 as READ-ONLY. You can achieve this with the syntax “FLUSH TABLES WITH READ ONLY”. (Or, you can pause all writes from the application(s), if that’s possible in your environment, and make sure existing write transactions are given the opportunity to complete.) Soon DB1 will catch up with DB2. The time to catch up with ChangeSet2 should be much shorter than the time to catch up with ChangeSet1. 6 . Now we can have DB1 serving the traffic, since it has the same content as DB2. Stop replicating from DB2 to DB1 and direct apps to write to DB1. New changes (ChangeSet3) are applied to the one-time, and now current, primary, DB1. 7 . Let’s quickly unlock DB2 with the syntax “UNLOCK TABLES”. Then we can set up replication with the new syntax, “REPLICATE DB2 WITH FORCE DIFFERENTIAL FROM DB1”. DB2 is configured to replicate from DB1 now, starting from the beginning position where it was different from DB1. In other words, DB2 starts to catch up with CS3 and whatever future changes are made to DB1. Scenario 2: I Want to Upgrade my Network, One Cluster at a Time In this scenario, you need to upgrade your servers or networking connections, without taking the system down. These may be software or hardware upgrades/fixes. For example, you need to upgrade the OS on your servers to a new version, or you want to add more main memory and new SSDs to your machines. You want to rotate through clusters in my network to perform the upgrade. This section will describe how to perform this kind of rolling upgrade for SingleStore clusters. Since you need to rotate through your entire network to perform the upgrade, you just need to look at a pair of DR clusters at a time. The goal is to upgrade one node, then the other, with minimum downtime and no data loss. 1 . Before the upgrade starts, DB1 is serving the traffic, and DB2 is replicating from DB1. 2 . While DB1 is serving the traffic, bring down DB2 for the necessary upgrade (show in red background below). New changes (ChangeSet1) are made to DB1. 3 . Power up DB2 after its upgrade is done.Since DB2 still remembers that it is replicating from DB1, users do not need to do anything. The replication is starting from where DB2 went offline. ChangeSet1 is replicated to DB2. 4 . A small delta of changes, ChangeSet2, may exist between DB1 and DB2 at this time. 5 . Lock DB1 for READ ONLY, then wait until DB2 catches up with DB1. This wait should be rather short. 6 . Stop the replication from DB1 to DB2. Route the traffic from business applications to DB2. 7 . Unlock tables on DB1, bring it down (show in red background below) and upgrade it. More changes (CS3) are made to DB2 during this time. 8 . Power up DB1 and start to replicate from DB2 using the new syntax “REPLICATE <DB1 > WITH FORCE DIFFERENTIAL FROM <DB2 > ”. This statement will only replicate the missing parts (i.e., ChangeSet3)  in DB1 from DB2. 9 . DB1 catches up with DB2 when the statement “REPLICATE … WITH FORCE DIFFERENTIAL …” is completed and returned for the missing updates (ChangeSet3) CS3. However, more changes may come in during this time. 10 . DB2 is the primary serving the traffic now. If we prefer to restore DB1 to its former status as the primary, then we need to perform steps similar to steps 5-7 of Scenario 1 to swap the primary and the secondary. Conclusion This blog has demonstrated the new replication options in SingleStore DB 7.1 that replicate only the missing log records from the primary database. This feature saves a lot of time when you need to catch up a secondary cluster/database from a primary cluster/database. It can allow you to reverse a replication relationship in minutes, instead of hours or even days. You may now be able to accommodate server and networking failures, and needed upgrades, with minimal or no noticeable downtime. For more information, contact SingleStore , or you can try SingleStore for free . References to SingleStore documentation: Replicate Database https://docs.singlestore.com/v7.1/reference/sql-reference/operational-commands/replicate-database/ Stop Replicating Database https://docs.singlestore.com/v7.1/reference/sql-reference/operational-commands/stop-replicating/ Flush Tables https://docs.singlestore.com/v7.1/reference/sql-reference/data-definition-language-ddl/flush-tables/ Unlock Tables https://docs.singlestore.com/v7.1/reference/sql-reference/data-definition-language-ddl/unlock-tables/", "date": "2020-05-28"},
{"website": "Single-Store", "title": "announcing-memsql-7-1", "author": ["Domenic Ravita"], "link": "https://www.singlestore.com/blog/announcing-memsql-7-1/", "abstract": "With the SingleStore DB 7.1 release , we’re continuing our journey to a simpler data platform, offering speed, scalability, and SQL – accessible to everyone, and easy to use. The release is available today as downloadable software and at the core of SingleStore Managed Service , our elastic managed service in the cloud. Simplicity There is a trend, especially in the cloud, to add a new, specialized data management technology for each separate type of workload. Part of our solution to this is SingleStore Universal Storage , which unifies transactions and analytics in a single data table. With the 7.1 release, we add broader support for indexes and unique keys in columnstore tables. This allows fast, rowstore-type updates to be performed on highly compressed, disk-based columnstore data, with much lower total cost of ownership (TCO). Resiliency A modern database must recover quickly from hardware, software, and networking failures. In SingleStore DB 7.1, we add fast DR failback – that is, the ability to return to your original data configuration much more quickly, after an error. We accomplish this by tracking and restoring recent updates incrementally. The result is greater flexibility and less downtime. Extensibility In SingleStore DB 7.1, we have extended our programming language, SingleStore Procedural SQL (MPSQL), to directly support functions that exist in other systems, making it easier to bring existing programs into SingleStore. And we have added new functions that greatly simplify coding when fetching data. Manageability In SingleStore DB 7.1, we add the ability to back up data incrementally to more environments: Amazon S3, Azure Blob Store, and Google Cloud Platform. This makes data management easier in the cloud. You can now also upgrade SingleStore online, without downtime. And you will see improvements in SingleStore Tools and new, first-ever self-paced training modules . And More Our new release includes a range of other features in the areas of fast and flexible data ingest, query optimization for faster query response, backup flexibility for databases that grow in size, the ability to work with data tables, security, and more. For more information, please see below: SingleStore DB 7.1 Now Generally Available Fast Disaster Recovery Failback with SingleStore DB 7.1 What’s New in SingleStore Tools with SingleStore DB 7.1 Introducing SingleStore Self-Paced Training", "date": "2020-05-28"},
{"website": "Single-Store", "title": "whats-new-in-memsql-tools-7-1", "author": ["Micah Bhakti"], "link": "https://www.singlestore.com/blog/whats-new-in-memsql-tools-7-1/", "abstract": "SingleStore Tools is our newer approach to helping you configure and manage downloadable SingleStore software . (If you choose SingleStore Managed Service , we do the administration for you.) Most users are already on SingleStore Tools. In recent months, we’ve introduced a slew of new features that will encourage most remaining SingleStore users to move to Tools from our former solution, SingleStore Ops. In this blog post, I’ll describe the new features, the philosophy behind SingleStore Tools, and why we encourage you to use SingleStore Tools exclusively going forward. SingleStore introduced SingleStore tools and SingleStore Studio the year before last, to replace SingleStore Ops. Both the tools and Studio are updated every two weeks. This blog post focuses on improvements to SingleStore Tools which have occurred over the last few months, and we encourage you to use Tools to help with your transition to SingleStore DB 7.1 . We will describe changes to SingleStore Studio in a separate blog post in the near future. What’s New in SingleStore Tools We can summarize the new features in a (long) sentence: we have pre-installation configuration checking; online upgrades (meaning, no noticeable downtime for your database); improvements to backup management; new cluster reporting; and the put file capability, useful for index files, for example. These capabilities are only available in SingleStore Tools, so make sure you are on the latest version, to take full advantage of everything we have added. Read on for more details. Improvements to the Reporting Tool To begin with, if you are installing SingleStore for the first time on new hardware (in the cloud, or on-premises), you will want to use a new feature of memsql-report : the --validate-env flag. This is so important, when you do need it, that we gave it a whole separate blog post . In the near future, you will see a new flag for memsql-report . The — include-performance-checks flag is one of the reporting features that’s designed to be used on a SingleStore cluster that is already in operation. It will allow you to run a number of performance-oriented checks on the cluster, ensuring that everything is running optimally. Online Upgrade We have added integrated and automated support for rolling online upgrades into SingleStore Tools version 1.4.4. By running memsql-deploy upgrade –online, Tools will upgrade and restart the SingleStore nodes sequentially, by availability group, maintaining cluster availability through the upgrade. This is especially important for mission-critical applications where it’s difficult or impossible to take the cluster down for version upgrades. Backup Management SingleStore Tools version 1.5.0 added backup creation and management directly into the toolset (rather than requiring backups to be run as SQL queries to the cluster). This allows for enhanced backup management, and it means you can validate and restore backups – all with the native toolset. Some of the features now available natively for backup management in SingleStore tools: memsql-admin create-backup – back up one or more databases to a repository memsql-admin delete-backup – delete a backup memsql-admin list-backups – list all the latest backups memsql-admin restore-backup – restore a database from a backup memsql-admin summarize-backups – print summary information for backups memsql-admin validate-backup – verify that a backup is correct We have also added backup-specific enhancements to the tool, such as speeding up backup deletion by parallelizing the operation, and including backup tags (using - t) to create custom attributes. This allows you to search for those attributes when selecting from existing backups. Cluster Reports SingleStore Cluster Reports was originally a feature that was used by SingleStore Support to collect cluster information and diagnose problems with the cluster, including incorrect configurations. The team has now included this capability directly in our toolset as memsql-report . These reports started with two main operations. collect creates a report from the cluster. check allows you to evaluate the report to see if there are issues with the cluster or configuration. We also support collect-local , which runs the report on a single machine. This option is useful if there is an issue with connectivity, or if you just need the output from part of the cluster Put File To easily copy a file – such as an index file – to all nodes in a cluster, use this new command option: memsql-admin put-file . You can specify the source path ( -s ) and target path ( -t ) to easily copy a file to all nodes in the cluster. This makes it easy to distribute files around the cluster, without having to use a third party command or tool. How We Got Here If you are still using an older version of SingleStore, you may well still be using SingleStore Ops, our old deployment and management tool. SingleStore Tools was introduced a year and a half ago to incorporate many improvements. SingleStore Tools includes the SingleStore Toolbox and SingleStore Client, described here. SingleStore Tools also includes SingleStore Studio; we will provide an update on Studio in the near future. To begin with, each of the components in SingleStore Tools is separate from the SingleStore core software. This simplifies product development, increases flexibility, reduces the potential for configuration issues, and improves stability for all of SingleStore’s suite of software solutions. The new toolset also simplifies integration and improves security. SingleStore Tools: Provides a stateless runtime that communicates directly with the SingleStore engine Is secure by default, with all communication over SSH and our internal protocol, compatible with MySQL wire protocol Features built-in cluster reporting and diagnostics Offers fully automated online upgrades to new SingleStore releases Delivers simple and scalable cluster deployment, using cluster configuration templates Integrates directly with existing DevOps tools (Ansible, Chef, Puppet, and others) Includes machine-parsable outputs for every command New versions of Tools are released every two weeks. Here’s how the pieces fit together. SingleStore Toolbox SingleStore Toolbox contains all the command line tools needed to deploy, administer, and manage your cluster. This includes registering host machines with memsql-toolbox-config , installing memsql to host machines in the cluster with memsql-deploy , managing your cluster with memsql-admin , and generating and checking reports using memsql-report . Each of the tools in the toolbox communicates with the engine through memsqlctl , a low-level command interface directly integrated into the SingleStore engine. This interface means SingleStore tools has no client modules to deploy, and can never be out of sync with your engine version. Having a well-defined communication path also means thay commands can be combined or layered with each other to provide a simplified user experience. For instance, deployment of SingleStore can be done manually by registering host machines, installing SingleStore, and then adding aggregators and leaves. Or it can be done in an automated way using setup-cluster , which aggregates and runs these underlying operations all at once. SingleStore Studio SingleStore Studio is a browser-based graphic user interface that allows cluster administrators to quickly and easily monitor and debug any SingleStore deployment. SingleStore Studio can be deployed anywhere, and communicates with SingleStore clusters using standard SQL queries to the master aggregator, making it simple to deploy and configure. Since first building Studio we have enhanced it with a number of key additions including a complete physical and logical monitoring system, expanded authentication support, secure user connections, and expanded usage metrics. All these features combine to make Studio the best interface to date for monitoring SingleStore deployments. We’ll give an update on Studio in a blog post in the near future. SingleStore Client SingleStore client is a lightweight client application that allows you to run SQL queries against SingleStore directly from any terminal window. You use it in the same way as you use the mySQL client. This makes it incredibly simple to connect to your cluster as soon as you have deployed it. Installing SingleStore with SingleStore Tools SingleStore Tools make installation of a cluster easy by giving you several ways to deploy SingleStore: Basic installations can be done with setup-cluster , which takes host and leaf inputs, and configures and deploys everything needed for a production cluster. For larger and more complex configurations, setup-cluster can take a yaml configuration file for simple and repeatable deployment. Developers looking to deploy a simple SingleStore cluster with only the bare minimum can use the “cluster-in-a-box” deployment to launch just a master aggregator and a leaf node onto a single machine. Simple Install with Setup-Cluster A simple setup-cluster deployment is done by specifying all of the cluster attributes directly in the deploy command: memsql-deploy setup-cluster -i /path/to/yourSSHkey \n\n    --license [YOUR LICENSE KEY] \n\n    --master-host <main_IP_address> \n\n    --aggregator-hosts <child_agg_IP_address> \n\n    --leaf-hosts <leaf1_IP_address>,<leaf2_IP_address> \n\n    --password <secure_password> \n\n    --version 6.8 Large Cluster Install with Setup-Cluster and a Cluster Configuration File Larger cluster configurations can be configured using a yaml configuration, and deployed by running setup-cluster, with the –cluster-file flag pointing to the configuration file: memsql-deploy setup-cluster --cluster-file </path/to/cluster-file>\n\nlicense: <license-from-portal-singlestore.com>\n\n    memsql_server_version: 7.0.16\n\n    package_type: rpm\n\n    root_password: <secure-password>\n\n    hosts:\n\n    - hostname: 172.16.212.165\n\n      localhost: true\n\n      ssh:\n\n        host: 172.16.212.165\n\n        private_key: /home/<user>/.ssh/id_rsa\n\n      nodes:\n\n      - register: false\n\n        role: Master\n\n        config:\n\n          auditlogsdir: /data/memsql/Master/auditlogs/\n\n          datadir: /data/memsql/Master/data\n\n          plancachedir: /data/memsql/Master/plancache\n\n          tracelogsdir: /data/memsql/Master/tracelogs\n\n          port: 3306\n\n      - register: false\n\n        role: Leaf\n\n        config:\n\n          auditlogsdir: /data/memsql/Leaf1/auditlogs\n\n          datadir: /data/memsql/Leaf1/data\n\n          plancachedir: /data/memsql/Leaf1/plancache\n\n          tracelogsdir: /data/memsql/Leaf1/tracelogs\n\n          port: 3307\n\n    - hostname: 172.16.212.166\n\n      localhost: false\n\n      ssh:\n\n        host: 172.16.212.166\n\n        private_key: /home/<user>/.ssh/id_rsa\n\n      nodes:\n\n      - register: false\n\n        role: Leaf\n\n        config:\n\n          auditlogsdir: /data/memsql/Leaf2/auditlogs\n\n          datadir: /data/memsql/Leaf2/data\n\n          plancachedir: /data/memsql/Leaf2/plancache\n\n          tracelogsdir: /data/memsql/Leaf2/tracelogs\n\n          port: 3307 As you can see in the above example, we specify a few cluster settings – the license, SingleStore version, package type, and root password – and then hosts are defined by hostname or IP, and SingleStore nodes are nested underneath. This way it’s possible to deploy one or more SingleStore nodes to each host in the cluster. Users can optionally also specify other custom attributes, such as non-standard directories for things like auditlogs, data, plancache, and tracelogs. Lastly, each node deployed will have a role, such as a master (aggregator) or a leaf node, and a port, unique per host. (If you deploy multiple nodes to one host, make sure each node has its own port; we often use 3306, 3307, etc.) Manual Cluster Installation with SingleStore tools For a full manual deployment, each toolbox step can be run manually to register hosts, install memsql, and create each node in the cluster: memsql-toolbox-config register-host --localhost --host <IP_address>\n\nmemsql-toolbox-config list-hosts\n\nmemsql-deploy install --all --version 6.8\n\nmemsql-admin create-node --host <IP_address> --password <secure_password>\n\nmemsql-admin bootstrap-aggregator --memsql-id <SingleStore_ID> --license [YOUR LICENSE KEY]\n\nmemsql-admin create-node --host <IP_address> --password <secure_password>\n\nmemsql-admin add-aggregator --memsql-id <SingleStore_ID> --password <secure_password>\n\nmemsql-admin create-node --host <IP_address> --password <secure_password>\n\nmemsql-admin add-leaf --memsql-id <SingleStore_ID> --password <secure_password>\n\nmemsql-admin optimize This is a great way to deploy if you have non-standard configurations that prevent you from installing with setup-cluster, such as running SingleStore as a different user than the SSH user that is installing the software. (This would require an update to the memsqlctl.hcl file on each of the hosts, after registering them and installing SingleStore). Developer Installation with Cluster in a Box The memsql-deploy cluster-in-a-box installation provides a basic installation of one master aggregator and one leaf node on a single machine. It’s a quick and easy way to deploy SingleStore in its simplest form for testing and basic development activities: memsql-deploy cluster-in-a-box --license AAAAAAa/Aaa/AA== Migrating from SingleStore Ops to SingleStore Tools While many of our customers are already using SingleStore Tools to manage their clusters, for customers with existing deployments using the legacy SingleStore Ops, we have developed an easy-to-use migration process, making it easy to begin managing your cluster with SingleStore Tools. This integrated process migrates from SingleStore Ops to SingleStore Tools by putting Ops in manual control mode; applying a new, updated license; outputting the cluster configuration into a yaml cluster file; and then using that cluster file to register the cluster with tools. memsql-ops cluster-manual-control --enable\n\nmemsql-ops license-add --license-key <license>\n\nsudo memsql-ops migration-setup\n\nmemsql-deploy setup-cluster --cluster-file <cluster_file>\n\nmemsql-toolbox-config list-hosts\n\nmemsql-admin list-nodes\n\nmemsql-admin restart-node --all\n\nmemsql-ops memsql-list\n\nmemsql-ops memsql-unmonitor 44CDE71\n\nmemsql-ops memsql-list\n\nsudo memsql-ops agent-uninstall --uninstall-ops-only --all Note : If you are transitioning to SingleStore Tools, you can see a mapping from the cluster operations in the old SingleStore Ops tool to the new SingleStore Tools commands here . Try the Latest SingleStore with SingleStore Tools There is a lot of other great functionality in both the SingleStore database software and in  SingleStore Tools, and the best way to experience all of it is to try it for yourself. Check out our latest deployment guide using SingleStore tools here , or check out our SingleStore channel on YouTube , to see the latest and greatest capabilities of the world’s fastest scale-out SQL database. You can also try SingleStore software or SingleStore Managed Service for free .", "date": "2020-05-28"},
{"website": "Single-Store", "title": "why-we-need-management-and-scalability-to-benefit-from-the-power-of-data", "author": ["Gina von Esmarch"], "link": "https://www.singlestore.com/blog/why-we-need-management-and-scalability-to-benefit-from-the-power-of-data/", "abstract": "In this Forbes article , Nikita Shamgunov, SingleStore co-CEO and co-founder, talks about the incredible potential of data, which he explains has the superpower to enable us to do things that were previously not possible. He adds that once datasets get really big, you need data storage that is vast and essentially bottomless. Nikita emphasizes the importance of having a data strategy that provides 360-degree visibility; has a champion in the form of a chief data officer (CDO); and addresses artificial intelligence, data management, and the user experience.", "date": "2020-06-08"},
{"website": "Single-Store", "title": "what-is-htap", "author": ["Domenic Ravita"], "link": "https://www.singlestore.com/blog/what-is-htap/", "abstract": "HTAP, or hybrid transaction/analytics processing, combines transactions, such as updating a database, with analytics, such as finding likely sales prospects. An HTAP database supports both workloads in one database, providing speed and simplicity. And today, “cloud-native HTAP” is a thing; users want an HTAP database that they can mix and match smoothly with Kafka, Spark, and other technologies in the cloud. Use cases include fraud prevention, recommendation engines for e-commerce, smart power grids, and AI. HTAP databases work with – and, to a certain degree, are designed for – integration with streaming data sources, such as Kafka, and messaging systems used for advanced analytics, AI and machine learning such as Spark. They serve multiple analytics clients, from business analysts typing in SQL queries, to BI tools, apps, and machine learning models, which generate queries in the scores or thousands per second. Before HTAP – Separate Transactions and Analytics HTAP combines different kinds of data processing into one coherent whole. The two types of processing differ considerably. Transaction processing – adding and updating records in a database – demands a very high degree of reliability for single-record operations, along with accuracy and speed. “Update Sandy Brown’s current address” is an example of a transactional update. Analytics processing, on the other hand, means looking very quickly through one or more database tables for a single record, or many records, or total counts of a type of record. “Find me all the subscribers who live in Colorado and own their own home” is an example of an analytics request. The first effective databases, first widely used in the 1970s and 1980s, were transaction-oriented. They came to be called online transaction processing (OLTP) systems. OLTP systems were optimized to work on underpowered computers with small hard disks – by today’s standards, of course. The only analytics was through printed reports, which might be sorted on various key fields, such as by state or ZIP code. When analytics was added on later, the transactional systems were already busy, so the data was copied onto a separate computer, running different software. These databases are called online analytics processing (OLAP) databases. Data warehouses and data marts are specialized OLAP databases that house non-operational data for analysis. Data on OLAP systems was queried using various languages, which coalesced around structured query language (SQL). At first, analytics queries were entered directly by individual analysts; eventually, business intelligence (BI) programs were used to make querying easier. More recently, software applications generate queries of their own, often at the rate of thousands per second. An entire process and discipline called extract, transform, and load (ETL) was created, simply to move data from OLTP to OLAP. As part of the ETL process, data owners may mix different databases of their own, externally purchased data, social signals, and other useful information. However, the use of three different silos means that data in the OLAP databases is always out of date – often from one day to one week old. The Move to HTAP The OLTP/ETL/OLAP structure is still widely used today. However, over time, both OLAP and, more slowly, OLTP databases were given the ability to work in a distributed fashion. That is, a single data table can now be distributed across multiple machines. Being distributed across several servers allows the data table to be much larger. A distributed data table can have its performance boosted at any time, simply by adding more servers to handle more transactions or reply to more queries. A database – one or more data tables, serving related functions on overlapping data – can now run on a flexibly sized array of machines, on-premises or in the cloud. As these capabilities have added, the exciting possibility of intermixing OLTP and OLAP capabilities in a single database has come to fruition. The database software that makes this possible was named hybrid transaction and analytics processing (HTAP) by Gartner in 2013. This capability is so new that it has many names, including hybrid operational analytics processing (HOAP) and translytical databases (which combine trans_actions and ana_lytical functions). HTAP, HOAP, and translytical databases are also described as performing operational analytics – “analytics with an SLA,” or analytics that have to deliver near-real-time responsiveness. Gartner has also come up with augmented transaction processing (ATP), which describes a subset of HTAP workloads that include operational AI and machine learning. The Benefits of HTAP HTAP has many benefits. HTAP creates a simpler architecture, because two separate types of databases, as well as the ETL process, are replaced by a single database. And data copies are eliminated. Instead of data being stored in OLTP, for transactions, then being copied to OLAP – perhaps multiple times – for analytics, a single source of truth resides in the HTAP database. These fundamental changes have add-on benefits. Operations is much simpler and easier, because only one system is running, not several. Making a single database secure is easier than for multiple data copies on different systems. And data can be fresh – as soon as data comes in for processing, it’s also available for analytics. No more need to wait hours or days – sometimes longer – for data to go through OLTP and ETL before it’s available for analytics. Very large cost benefits can be achieved by HTAP, along with related increases in revenues and decreases in costs. Simplicity in architecture and operations results in significant cost savings. Higher performance makes existing revenue-producing functions more productive, and makes new ones possible. The Internet of Things (IoT) benefits greatly from HTAP. If you’re running a smart grid, you need to be running fast, from the latest data. Analysts, dashboards, and apps all need access to the same, updated data at once. Machine learning and AI are actually impractical without HTAP. There isn’t much point to running a machine learning algorithm if you can’t be learning from current, as well as historical, data. No one wants to run a predictive maintenance program that tells you that your oil well was likely to need urgent maintenance a week ago, or that there were several interesting travel bargains available yesterday. How SingleStore Fits SingleStore was conceived as an HTAP database before the term was even coined by Gartner. The company was founded in 2011 to create a general-purpose database that supports SQL, while combining transactions and analytics in a single, fast, distributed database. The result is now a cloud-native , scalable, SQL database that combines transactions and analytics. Today, SingleStore software is available for download, so you can run it in the cloud or on-premises, or in the form of an elastic managed service in the cloud, SingleStore Managed Service . SingleStore allows customers to handle transactional operations in rowstore tables, which run entirely in memory. Most analytics functions are handled by columnstore tables, which reside largely on disk. Data is moved between tables by Pipelines, a fast and efficient alternative to ETL. Today, SingleStore is going further, introducing SingleStore Universal Storage in 2019. Universal Storage is a new expression of the core idea behind HTAP. In Universal Storage, rowstore and columnstore tables each gain some of the attributes of the other. For instance, rowstore tables now have data compression, which was formerly the sole province of columnstore. And columnstore tables can now quickly find, and even update, a single record, or a few records – capabilities that were formerly the hallmark of rowstore. Increasingly, with Universal Storage, a single table type can fill both transactional and analytical needs. Any needed performance boost is provided by scalability, simply adding servers. Future plans for Universal Storage include even higher levels of convergence .", "date": "2020-06-10"},
{"website": "Single-Store", "title": "spark-connector-30", "author": ["Roxanna Pourzand"], "link": "https://www.singlestore.com/blog/spark-connector-30/", "abstract": "SingleStore has now released a new version of the Apache Spark Connector – Version 3.0 – which includes many new enhancements. The new Connector supports robust SQL pushdown, is compatible with the latest versions of Spark and the SingleStore engine, contains tight integration with Spark’s widely used DataSource API, and provides flexible load data compression options to expedite data ingest. Read on to learn more about the enhancements of the connector,  and how you can accelerate your workload using Apache Spark and SingleStore together. Apache Spark is widely known as a leading technology for big data and AI . If you use Spark in your workflow today, you already know that it’s a powerful data processing engine and that it’s best-in-class for data enrichment, computation, and analytics. Not only is it flexible, due to all the native libraries it contains (e.g., Structured Streaming, SQL, and machine learning), but it also supports a variety of programming languages. What you might not know is how you can use Spark and SingleStore together to accelerate and transform your data processing and analytics at scale. While Spark shines at analyzing large datasets, many workloads require a solution for data persistence. SingleStore can be your persisted operational storage layer, and at the same time, it can be the analytical backbone for Spark. SingleStore provides the solution – a fast database that is scalable, ingests data very rapidly, supports SQL, and supports queries with high performance and high concurrency. You can use SingleStore as self-managed software that you download and run in any cloud, or on-premises; or you can use SingleStore Managed Service , our fully managed, elastic cloud offering, on major public clouds. How do Spark and SingleStore work together most effectively? The answer, as hinted above, is the SingleStore Spark Connector; SingleStore has developed a new version of our Spark Connector, Version 3.0, now available to customers (documentation here ). Using SingleStore’s Spark Connector 3.0, you can leverage the computational power of Spark in tandem with the speedy data ingest and durable storage benefits of SingleStore. More on the Spark Connector The Spark Connector 3.0 is already used by many SingleStore customers. Like previous versions, this version of the Connector can load and extract data from database tables and Spark Dataframes, leveraging the SingleStore LOAD DATA command to accelerate ingest from Spark via smart compression. The Connector also serves as a true Spark Data Source, so it can be used with a variety of different languages, including Scala, Python, Java, SQL, and R. And the Connector supports robust SQL pushdown; that is, applicable operations in Spark get translated to true SQL and executed, with high performance and excellent concurrency, on SingleStore. The new Spark Connector 3.0 is compatible with the recent versions of Spark (versions 2.3 and 2.4). It is now easier to use and maintain, because it is fully integrated with JDBC’s API. Note : For those that are new to Spark and would like to learn more about it, please visit the Apache Spark documentation . SingleStore and Spark Reference Architecture There are many ways in which SingleStore and Spark can be used together in a solution architecture. SingleStore can be the input to Spark for analytics work; SingleStore can store output from Spark after data enrichment; or SingleStore can do both at the same time. Below is a reference architecture using Spark and SingleStore together. In this architecture, there is an application telemetry source streaming information into Kafka. The Kafka stream is then loading a subset of the data directly into SingleStore, and loading another subset into Spark for enrichment and further analytics prior to loading it into SingleStore. Furthermore, the architecture highlights that the SingleStore Spark Connector ensures seamless movement of data between the two sources, in either direction. Let’s dive into the details on how to use the SingleStore Spark Connector to implement this architecture. Using SingleStore Spark Connector 3.0 How do you actually put the SingleStore DB 3.0 Spark Connector to use? Let’s say you need to capture and structure telemetry data from your high-traffic application on the fly, similar to the architecture above. While the event stream may include a great deal of unstructured information about user interactions within the application, it may make sense to structure and classify a subset of that data before passing it to a database like SingleStore in a persistent, queryable format. Additionally, you may want to push existing data from the database into Spark, transform it, and return the enriched data back into the database. Processing data from your high-throughput stream in Spark allows you to efficiently segment or enrich the application events before writing data into a SingleStore table and querying it in real-time. True Spark Data Source Here we’ll describe some of the enhancements that make the SingleStore Spark Connector 3.0 different. The SingleStore Spark Connector 3.0 is a true Spark data source. It has robust SQL pushdown – the ability to execute SQL commands in SingleStore, instead of Spark – for maximum query performance benefits. And the new version of the Connector supports more load-data compression options. Details on these new capabilities follow. Our connector is compatible with Spark’s DataSource API. This API is beneficial because results are returned as a Spark DataFrame, and the results can quickly be processed in Spark SQL or joined with other data sources. Additionally, as a JDBC API, the DataSource API is standard across the industry. This API allows you to write high-level, easy-to-use programs that operate seamlessly with and across many different databases, without requiring you to know most of the low-level database implementation details. For example, to read data from SingleStore, you can use the following options, which will read from the table myTable in the database db . val df = spark.read\n    .format(\"memsql\")\n    .option(\"ddlEndpoint\", \"memsql-master.cluster.internal\")\n    .option(\"user\", \"admin\")\n    .option(\"password\",\"S3cur3pA$$\")\n    .load(\"db.myTable\") The integration with the reader/writer API allows us to use the JDBC standard spark.read.format syntax as we did above, specifying SingleStore as the data source format. Additionally, the ddlEndpoint , user , and password are SingleStore configuration options you can specify per query, or globally via Spark Configuration. In the above example, the options are specified at the query level. You can also run queries to create tables and load data using SparkSQL, since the connector is a native SparkSQL plugin. For example, the following will register the table my_table that exists in SingleStore in your Spark table registry under data . You can then execute SparkSQL queries against data in Spark directly, which will select from your SingleStore table. spark.sql(\"CREATE TABLE data USING memsql OPTIONS ('dbtable'='my_table')\") spark.sql(\"select * from data limit 10\").show() Finally, you can write data from Spark to SingleStore using the standard API df.write syntax. For example, this query writes a dataframe, mydataframe , to a SingleStore table called memsqldata : mydataframe.write\n.format(\"memsql\")\n.option(\"ddlEndpoint\", \"memsql-master.cluster.internal\")\n.option(\"user\", \"admin\")\n.option(\"password\",\"S3cur3pA$$\")\n.option(\"overwriteBehavior\", \"dropAndCreate\")\n.mode(SaveMode.Overwrite)\n.save(\"test.memsqldata\") The overwriteBehavior option specified in the configuration will drop and create the target SingleStore table to write the values in. You can also optionally specify truncate or merge . Using truncate will truncate the table before writing new values (rather than dropping them), while merge will replace all new rows, and update any existing ones, by matching on the primary key. For more information on the SingleStore Spark Connector configuration options, please visit the README or the Spark Connector 3.0 documentation. SQL Pushdown and SQL Optimization The most cutting-edge aspect of the SingleStore Spark Connector 3.0 is its SQL pushdown support – the ability for SQL statements to be evaluated much faster, in the database, rather than in Spark, where they would run slower. Complete statements are evaluated entirely in the database, where possible. The connector also supports partial pushdown, in cases where part of a query (e.g., a custom Spark function) must be evaluated in Spark. Our connector supports optimization and rewrites for most query shapes and compatible expressions. Additionally, our query optimizations utilize deep integration with Spark’s query optimizer. SQL Pushdown in Action To demonstrate the behavior of partial SQL pushdown, let’s assume we have a large table in SingleStore called temperatures_all that contains all cities in the United States, and their average high and low temperatures in Celsius for a given time period. Here’s a sample of the table below: |city|state|avg_low|avg_high|\n|Pasadena|California|18|27|\n|San Francisco|California|20|24|\n|St. Paul|Minnesota|1|22|\n... We want to do some computation in Spark using a custom user-defined function (UDF) to convert the high and low temperatures from Celsius into Fahrenheit, then obtain the result for San Francisco. So we register our SingleStore table temperatures_all using SparkSQL, under the name temperatures : spark.sql(\"CREATE TABLE temperatures USING memsql OPTIONS ('dbtable'='temperatures_all')\") We create and register our UDF in Spark: spark.udf.register(\"TOFAHRENHEIT\", (degreesCelcius: Double) => ((degreesCelcius * 9.0 / 5.0) + 32.0)) Then, using SparkSQL, we run the following query to convert the averages from Celsius to Fahrenheit. spark.sql(\"select city, state, TOFAHRENHEIT(avg_high) as high_f, TOFAHRENHEIT(avg_low) as low_f from temperatures where city='San Francisco'\").show() +-------------+----------+------+-----+\n|         city|    state|high_f|low_f|\n+-------------+----------+------+-----+\n|San Francisco|California|75.2  |63.5|\n+-------------+----------+------+-----+ In this case, the SingleStore DB 3.0 connector will be able to push down the following to SingleStore: ‘SELECT city, state…’ “…where city = ‘San Francisco’” It will leave the evaluation of the ‘TOFAHRENHEIT’ UDF on the fields avg_high and avg_low to Spark, since that is where the UDF lives. Appending .explain() to the query shows you the execution plan. This shows you exactly what is and isn’t getting pushed down from Spark to SingleStore. As you can see below, the final plan indicates a single projection on top of a SingleStore scan. Gather partitions:all alias:remote_0\nProject [a34.city AS `city#6`, a34.state AS `state#7`, a34.avg_high AS `avg_high#8`, a34.avg_low AS `avg_low#9`]\nFilter [a34.city IS NOT NULL AND a34.city = 'San Francisco']\nTableScan test.temperatures_all AS a34 table_type:sharded_rowstore As expected, in SingleStore the query shows that the ‘where‘ clause was pushed down, along with the specific column selection for the projection. SingleStore Query\nVariables: (San Francisco)\nSQL:\nSELECT `city#6` , `state#7` , `avg_high#8` , `avg_low#9`\nFROM (\n  -- Spark LogicalPlan: Filter (isnotnull(city#6) && (city#6 = San Francisco))\n  SELECT *\n  FROM (\n    SELECT ( `city` ) AS `city#6` , ( `state` ) AS `state#7` , ( `avg_high` ) AS `avg_high#8` , ( `avg_low` ) AS `avg_low#9`\n    FROM (\n      SELECT * FROM `test`.`temperatures_all`\n    ) AS `a32`\n  ) AS `a33`\n  WHERE ( ( `city#6` ) IS NOT NULL AND ( `city#6` = ? ) )\n) AS `a34` To see all the stages of the optimization and pushdown process,  you can append .explain(true) to your statement, which will provide more detailed information on the execution process. The above example shows the power of pushdown on a small data set. When you have billions of rows of data you are analyzing, the performance benefits of evaluating operations in SingleStore versus Spark are immense. Load Data Compression SingleStore’s Spark connector supports flexible load data compression options; you can use GZip or LZ4. Compression ensures maximum data loading performance when you are loading data from Spark into SingleStore. As an example, the following will write the table myTable from Spark into SingleStore, and use LZ4 compression. df.write\n    .format(\"memsql\")\n    .option(\"loadDataCompression\", \"LZ4\")\n    .save(\"test.myTable\") Conclusion Using SingleStore and Spark together has great benefits for high-velocity applications which require fast computation and diverse analytic libraries, extremely fast loading, and the need to query data as it’s being loaded. The SingleStore Spark connector ensures a tight integration between the two systems, providing high performance, and maximizing the benefits of SingleStore as the data source and Spark as the analytics framework. You can try the Spark Connector 3.0 by adding it to your library from SingleStore’s Github repository . Follow the README for detailed instructions on usage. We also have a guide on using Kafka, Spark, and SingleStore together , and a demo of the Connector you can test with Docker. If you are not yet using SingleStore, you can try SingleStore for free or contact SingleStore .", "date": "2020-06-10"},
{"website": "Single-Store", "title": "memsql-singlestore-memsql-7-1-episode-2", "author": ["Eric Hanson"], "link": "https://www.singlestore.com/blog/memsql-singlestore-memsql-7-1-episode-2/", "abstract": "Universal Storage, first introduced in the SingleStore DB 7.0 release, advances quickly in SingleStore DB 7.1. Universal Storage allows SingleStore to support the world’s largest workloads – as well as many smaller workloads – with extremely high performance and excellent price-performance. In this release, new Universal Storage features make it even easier to support blended analytical and transactional workloads that achieve near-rowstore performance for selective queries, but at columnstore costs. SingleStore Universal Storage appeared in SingleStore DB 7.0 , with notable improvements in performance and total cost of ownership (TCO) for both rowstore and columnstore workloads. In SingleStore DB 7.1, we make further improvements to rowstore-like aspects of performance on the columnstore side.  Support for high performance hybrid transactional and analytical processing (HTAP) workloads is becoming increasingly important in the database industry. Industry analysts refer to this kind of workload as “translytical,” “operational analytics,” “augmented transactions,” or “analytic/augmented transaction processing (ATP).”  We’ll use the term HTAP here.  SingleStore leads the industry for HTAP performance, outdistancing legacy products by a factor of ten or more in price-performance. But we’re not satisfied. We want to support the world’s largest workloads cost-effectively — more cost-effectively than anyone, even SingleStore, has achieved to date. SingleStore shipped the first installment of our groundbreaking Universal Storage technology in SingleStore DB 7.0 last December. The purpose of Universal Storage is to dramatically reduce total cost of ownership (TCO) for HTAP workloads. Moreover, Universal Storage technology can allow much larger online transaction processing ( OLTP) workloads to be run on SingleStore, with much lower TCO, than ever before. In the SingleStore DB 7.1 release, we’ve improved Universal Storage to handle more use cases more efficiently, and make the developer’s job easier when creating both HTAP and OLTP applications. Universal Storage Introduced in SingleStore DB 7.0 Beginning with its introduction in SingleStore DB 7.0 [ SS19 ] , Universal Storage has included several capabilities which improve TCO and performance for HTAP and OLTP: Hash indexes on columnstores [ CS19 ] , to allow fast location of a row in a columnstore, given its key value. Sub-segment access , which allows quick retrieval of a row from a columnstore – very quickly, in single-digit milliseconds – once its position is known. Row-level locking for columnstores , which allows many concurrent updates of multiple rows in a columnstore to proceed, without requiring transactions to wait. SPARSE compression for rowstores [ RS19 ] , which can cut RAM usage by half or more for wide tables in rowstore that have lots of NULL values (ie, many of the rowstore tables we see at SingleStore). The first three changes make it possible to accomplish many tasks in columnstore that were formerly only practical in rowstore. This allows customers to take advantage of the high degree of compression in columnstore, and the costs advantages of using this disk-based table type, to achieve previously impossible TCO for these workloads. Universal Storage Advances in SingleStore DB 7.1 The advent of sparse compression in rowstores preserves the speed advantages of rowstore tables, based as they are in memory, while cutting costs by roughly 50%. Also, together, these changes reduce the need to use mixed rowstore/columnstore implementations, in an effort to find price/performance sweet spots. (At the expense of added complexity.) The desired sweet spot can now often be found either entirely in rowstore, or entirely in columnstore.  And now, as part of SingleStore DB 7.1, we are making further improvements to Universal Storage on the columnstore side. These changes speed up more columnstore operations, enhancing the degree to which you can enjoy the large price advantages of columnstore (5-10x compression, and the use of disk rather than memory for main storage), along with the existing performance advantages of columnstore (fast scans, for example), and new performance improvements for specific operations in columnstore that bring it ever closer to rowstore-like execution times. The Next Installment of Universal Storage We’ve worked with many of our customers to understand how they’re using our product for HTAP. Something they’ve consistently asked for is enhancements to the ability to enforce uniqueness constraints automatically. They can already do this with SingleStore rowstores, which have supported unique key validation for years.  In addition, in SingleStore DB 7.0, with hash indexes and subsegment access on columnstores, you could use multiple statements to check for an existing key, then insert a new record with that key if the key was not found. But clearly, it made sense to support standard SQL UNIQUE constraints or keys on a columnstore table. That would make the developer’s job easier. Unique Hash Keys So, in SingleStore DB 7.1, we will now support single-column unique keys on columnstores, via an extension of our existing hash indexes. Here’s a simple example of how it works. First, we create a table, t , with a unique key, column a . create table t(\n\n  a int, \n\n  b decimal(18,5), \n\n  shard(a), \n\n  unique key(a) using hash, \n\n  key(a) using clustered columnstore); The clause, unique key(a) using hash , causes SingleStore to validate inserted and updated rows, making sure no duplicates are added to column a .  You must shard the table on the unique key so the uniqueness test can be done locally on a single leaf node [ MSL19 ] .  Now, we insert two rows with different keys: memsql> insert t values(1, 10.0);\n\nQuery OK, 1 row affected (0.11 sec)\n\nmemsql> insert t values(2, 20.0);\n\nQuery OK, 1 row affected (0.01 sec) Finally, we try to insert a duplicate key, 2 : memsql> insert t values(2, 30.0);\n\nERROR 1062 (23000): Leaf Error (127.0.0.1:3308): Duplicate entry '2' for key 'a_2' This fails because of the duplicate key. Performance of Uniqueness Checking To analyze the performance level of the hash index uniqueness checking, I inserted 16 million rows in the table, t , used above. Then I ran this statement to insert 1000 new rows: insert into t\n\nselect a+(select max(a) from t), 1000000*rand()\n\nfrom t\n\nlimit 1000; Running this command using the Profile option in SingleStore Studio (Profile) shows this took 39 milliseconds, which is a fraction of a millisecond per row. Profile shows that the following command to insert one row takes less than one millisecond: insert into t\n\nselect a+(select max(a) from t), 1000000*rand()\n\nfrom t\n\nlimit 1; Both of these INSERT statements are showing OLTP-level performance for uniqueness checking. Highly-Selective Joins on Columnstores SingleStore DB 7.0 introduced support for columnstore hash indexes, broadening the support for OLTP-type queries on columnstores. However, a common join pattern in OLTP is to have a very selective filter on one table, which produces one or a few rows from the source table, and then join those rows with another table. Databases for OLTP normally use a nested-loop join for this. For each and every row from the outer table, an index seek will be done on the inner table.  SingleStore DB 7.1 supports this kind of highly selective join using an adaptive hash join algorithm, which first does a hash build for the table with the highly-selective filter. Then, if only a few rows are in the hash table, it switches to perform a nested-loop join that seeks into the larger table (on the probe side) via the index on the join column of the table on the probe side. If, on the other hand, the hash build side produces a lot of rows, then a normal hash join will be done. Here’s an example of a simple schema and query that can take advantage of this new strategy for selective joins. create table orders(\n\n  oid int, \n\n  d datetime, \n\n  key(d) using clustered columnstore, \n\n  shard(oid), \n\n  key(oid) using hash);\n\ncreate table lineitems(\n\n  id int, \n\n  oid int, \n\n  item int, \n\n  key(oid) using clustered columnstore, \n\n  shard(oid), \n\n  key(oid) using hash); Now, add some sample data to orders: insert into orders values(1, now());\n\n-- repeat the statement below until orders has 33.5 million rows\n\ninsert into orders \n\nselect oid+(select max(oid) from orders), now()\n\nfrom orders; Add 67.1 million rows of data to lineitems , such that each line item belongs to an order, and each order has exactly two line items. insert into lineitems select oid, oid, 1 from orders;\n\ninsert into lineitems select oid + 1000*1000*1000, oid, 2 from orders; Find a selective datetime value for d to search on: select d, count(*)\n\nfrom orders\n\ngroup by d; The result shows that a couple of datetime values only appear in one row in orders , in my test. One of these is 2020-03-30 16:47:05 .  The following query uses this date to produce a join result with exactly two rows: select *\n\nfrom orders o join lineitems l on o.oid = l.oid\n\nwhere o.d = \"2020-03-30 16:47:05\"; It filters on o.d to find a single row of orders, then joins to lineitems via the hash index on lineitems.oid , using the new selective-join algorithm. The profiler in SingleStore Studio shows that this query runs in only one millisecond . That’s OLTP-level speed, with all the TCO advantages of columnstore. The relevant part of the profile plan shape is shown in Figure 1. The query plan works by first seeking into the orders table to get one row, in the ColumnStoreScan operator on the right. The hash build above then builds a hash table with one row in it.  Recognizing that the build side is small, the HashJoin operator dynamically switches to a nested-loop join strategy. It seeks the columnstore hash index on lineitems , on the left, to find matching rows. Then it completes the join and outputs two rows. Figure 1. Sub-plan of PROFILE PLAN for selective join of orders and lineitems . To see if a plan may be able to do a join via a hash index, look into the JSON profile plan. If the strategy is available in your plan, you will see join index in descriptions of columnstore filter operators. For example: \"executor\":\"ColumnStoreFilter\",\n\n\"keyId\":4294968023,\n\n\"condition\":[\n\n        \"o.oid = l.oid bloom AND l.oid = o.oid **join index**\"\n\n], Or, click on the ColumnStoreFilter operator in Studio and you may see a condition that mentions join index , like this, in the properties pane on the right: **CONDITION**\n\no.oid = l.oid bloom AND l.oid = o.oid **join index** Benefits Compared to No Indexes I ran the same query again without the hash indexes on orders and lineitems , and the query took 7 milliseconds, instead of 1 millisecond. The most surprising thing about this for me was that the join performance was as good as it was – 7ms, that is – even without the hash indexes. That’s an indication of how good our other join performance features are, like segment elimination [ OTD19 ] , operations on encoded data [ OED19 ] , Bloom filters, and sub-segment access. Nevertheless, there is a dramatic improvement, 7x, when the hash indexes are added. This test was run on a SingleStore Managed Service test cluster with only a single half-height leaf node. Of course, your results for selective join performance will vary depending on the type of hardware, cluster size, schema, and data size. Conclusion Universal Storage is a major, multi-release effort at SingleStore to dramatically improve TCO for both HTAP and OLTP. Our columnstore table type now has all the features needed to support HTAP and OLTP for many use cases.  We’re continuing to advance Universal Storage. For example, multi-column unique keys on columnstores, and UPSERT support for columnstores, are in our future. Stay tuned for continued improvements in this area in our releases after 7.1. References [ CS19 ] SingleStore Columnstore , SingleStore Concepts, https://docs.singlestore.com/v7.0/concepts/columnstore/ , 2019. [ MSL19 ] Leaf , SingleStore Concepts, https://docs.singlestore.com/v7.0/concepts/leaf/ , 2019. [ OED19 ] Understanding Operations on Encoded Data, SingleStore Concepts, https://docs.singlestore.com/v7.0/concepts/understanding-ops-on-encoded-data/ , 2019. [ OLTP17 ] What is OLTP? https://database.guide/what-is-oltp/ , 2017. [ OTD19 ] Optimizing Table Data Structures, SingleStore Documentation, https://docs.singlestore.com/v7.0/guides/development/development/optimizing-table-data-structures/ , 2019. [ RS19 ] Rowstore , SingleStore Concepts, https://docs.singlestore.com/v7.0/concepts/rowstore/ , 2019. [ SS19 ] SingleStore Universal Storage – And Then There Was One, https://www.singlestore.com/blog/memsql-Universal Storage-then-there-was-one/ , September, 2019.", "date": "2020-06-03"},
{"website": "Single-Store", "title": "powering-gameloft-with-memsql", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/powering-gameloft-with-memsql/", "abstract": "Gaming has gone from a niche activity to an entire industry focused on delivering real-time digital experiences. Gameloft, an award-winning developer and publisher of mobile and console games, has used SingleStore to extract actionable insights in real time from more than 100TB of raw data. According to Juniper Research , the gaming industry will touch $132 billion in total revenue by 2021 – well ahead of Hollywood . It is a fiercely competitive business, where every game developer’s dream is to shoot to the top of the charts. Gameloft, as a French-based leading publisher and developer of mobile and console games, has built a devout fanbase, spread all across the globe. All games are developed in-house, and they have emerged as one of the top innovators in the industry. Their in-house franchises boast winning titles such as Asphalt 9: Legends ( download ) and Dragon Mania Legends ( download ), and their top-tier collaborators include entertainment brands known around the world. Gameloft has won many awards; Minion Rush and Asphalt 8: Airborne are featured in the “Top 10 iOS Games by All-Time Worldwide Downloads,” from App Annie. Asphalt 9: Legends won a design award from Apple and two Webby Awards. You will find them on lists of the top gaming companies in the world . Awards for Gameloft’s Asphalt 9: Legends. The rising popularity of mobile games has driven Gameloft to create unique experiences for all digital platforms – gaming consoles and PCs, of course, but also smartphones, mobile phones, tablets (including Apple iOS and Android devices), set-top boxes, and connected TVs. With its global distribution network spanning 150-plus countries, and many millions of daily active users across the globe, Gameloft aims to marry the virtual and real-world through online gaming, every single day. But, with all this success, Gameloft had a business need for outstanding performance, in order to deliver real-time gaming KPIs, and to conduct business with customers – including in-game transactions – in real time. To do so, they had bet on a robust data infrastructure platform that will meet their current and future gaming needs. As shown by their many successes, and their close collaboration with leading players, Gameloft’s is running a serious business, and the stakes are high. Without a strong, ready-to-go data infrastructure to power their games, all the hard work done to create a compelling game could quickly go wasted. Gameloft experienced issues with: Processing the high influx of real-time key performance indicator (KPI) data across their growing fanbase. Effectively capitalizing on critical micro-moments in the game to deliver the right experiences, at the right time, and to make the right offers to customers. Gameloft was stuck behind delays in their analytics pipeline. No Real-Time Insights Today, nearly every industry is data-driven and needs actionable data. Gameloft was no different, and without real-time data insights, Gameloft could not fully optimize its monetization of games. For Gameloft, real-time insights were needed to drive several in-game tactics, including feature promotion, selective campaigning, and player retention schemes. With analytics delayed, they didn’t know how much revenue the game was making at a given point in time, and how they could keep the game more enjoyable for the players, keeping the game “sticky.” Without this, they were unable to capitalize on many important factors, like promoting the right features at the right time, or running a campaign based on player demographics and in-game behavior. There was a need to offer, for instance, free game coins at just the right moment, so a player can purchase more lives and continue playing. Gameloft Runs into Scaling Challenges Gaming is a “hits” business, and when a game goes viral, it’s crucial to be able to scale the backend infrastructure instantly. Several Gameloft games involve characters from major entertainment brands. These games are designed using the same spirit and atmosphere as in hit Hollywood movies. It was clear that the growth potential for these games was unlimited. When the need comes up for one of these games to scale, it needs to scale immediately. Some games that experience a successful first few days quickly lose momentum due to performance and availability issues. There have been games that hit a peak, as new users flood in – but then experience responsiveness issues, and get pulled from the market a few days later. Gameloft didn’t want to meet the same fate, and the legacy MySQL backend at Gameloft was not meeting their expectations: the need to calculate in-game KPIs, in real time, from live interactions with millions of users. Throw a few more million concurrent users into the pool of live accounts, and the MySQL database would run into speed and concurrency limitations that slowed down the experience for everyone. Gameloft clearly had to offload some of the work from the MySQL backend to scale. SingleStore’s elastic and robust technology turned the odds in their favor. Let’s see how… Old Architecture using MySQL Before they moved to SingleStore, Gameloft began by storing their KPIs in a MySQL cluster running on commodity hardware. This system was used for both vital KPIs that had stringent SLAs (such as the number of active users, player retention data, and user pings), and less-critical KPIs with relaxed SLAs. The MySQL cluster comprised 16 servers, with data manually partitioned across the different servers. In addition, the application servers had to be aware of which data resided where. Before the data hit the MySQL cluster, Gameloft used an off-the-shelf technology called MooseFS to stage incoming KPI data locally on their servers. MooseFS is a fault-tolerant, highly available, scale-out distributed file system. ETL jobs then processed the data from MooseFS, reading and transforming it before it was stored in the MySQL cluster for use in analytics. In terms of data size, the MySQL database had grown to roughly 100TB of raw data and counting, and Gameloft was not able to extract actionable insights in real-time. Gameloft was using stored procedures that took nearly 12 hours to generate the insights needed to improve the gaming experience. At that point, the value of the insights diminished – because the user’s in-game experience was already further along, and it would be out of context to influence the game with stale insight data. (Imagine driving your car, and getting a low fuel alert when the car is already out of gas.) Previous MySQL architecture: Data distribution was done on the application side, adding complexity. The application would then map-reduce data and aggregate it. Re-engineering for High-Speed Analytics and Scale with SingleStore As a solution to its real-time insights issues, Gameloft decided to adopt SingleStore, which offers high performance on ingest and queries, and a widely used SQL interface for queries and analytics applications. To reduce the risk initially, they opted for a Lambda-like architecture, where fast KPIs were processed using SingleStore, and the less critical KPIs were still processed using MySQL. SingleStore is a fully scalable relational database that supports SQL – but in a distributed fashion, such that SingleStore can offer scalability that’s unusual in relational databases. With inherent SQL support, SingleStore can be used as a nearly drop-in replacement for most existing relational databases, including MySQL. The built-in MySQL wire protocol support in SingleStore means that it works “out of the box” without any application rewrites. This is in stark contrast to NoSQL databases, which are heavily used in the gaming industry and are scalable, but lack SQL support . This makes them both difficult to write queries for and slow to respond to queries. For many teams at Gameloft, using SingleStore meant zero impact and no code changes, but tremendous benefits, from both the architectural and application points of view. So, how was SingleStore able to deliver the blazing fast KPIs needed by Gameloft? Well, the answer lies in the fact that SingleStore is a unique technology that marries in-memory with disk-based database technology and also provides a column store engine. With SingleStore at the heart of Gameloft’s new architecture, the fast KPIs were calculated at high speed providing near real-time data. From immediately analyzing KPI data, to using real-time intelligence to capitalize on certain game features, campaigns and timed events,  Gameloft would never miss an important in-game moment. After moving from MySQL to SingleStore, Gameloft is seeing “Analytics complete within 15 minutes, which used to previously take 12 hours,” says Marian Nicolae, Business Intelligence Manager at Gameloft. In the case of Gameloft, SingleStore was used for a specific use-case of speeding up KPI calculations, in combination with MySQL. What makes SingleStore stand out, compared to those other tools, is performance. With over 1.5TB of data in SingleStore, Gameloft’s deployment of SingleStore still had plenty of headroom for growth. Additionally, SingleStore at Gameloft was deployed on premise, but in the future Gameloft could easily migrate their SingleStore database to the cloud (with Managed Service), or use a hybrid solution to further reduce costs. Getting Started with SingleStore If you have workloads that require fast ingest, fast processing of large data tables, and fast queries, in operational analytics or other use cases , try SingleStore for free today , or contact us to learn how we can help you.", "date": "2020-06-16"},
{"website": "Single-Store", "title": "juneteenth-learn-reflect-and-take-action", "author": ["Raj Verma"], "link": "https://www.singlestore.com/blog/juneteenth-learn-reflect-and-take-action/", "abstract": "Juneteenth commemorates June 19, 1865 , a day on which enreplicad African Americans in Galveston, Texas , were informed that the Civil War had ended and that they were now free. It has been 155 years since that historic day, and racism continues to be at the root of so much pain and ugliness in our society. From the protests in the streets of Minneapolis to the disparities inflicted by COVID-19, African Americans throughout the United States are still struggling due to the embedded prejudices and racism within our society. As long as this is true, America’s twin ideals of freedom and equality remain out of reach. The Black Lives Matter movement is deeply personal because it addresses the needs of those who have been disenfranchised within our society and serves as a way for their voices to be heard. This is not a political statement. This is not about marketing or sales initiatives. This is about being human. Black lives don’t just matter — they are truly cherished and valued, and our society needs to make major strides to stop treating African Americans as if this isn’t the case. When it comes down to it, change starts with us. Last weekend, I was at my daughter’s graduation, where an insightful Hungarian monk gave the commencement address. He explained that our society is plagued by two viruses: COVID-19 and the treatment of Black people within America, both inhibiting the ability for members of our society to breathe. There is no greater cause than the one that is currently facing us. That’s why SingleStore on June 19 will observe a minute of silence followed by a presentation featuring comments from Milan Balinton, Executive Director of the African American Community Service Agency (AACSA) , who will share his perspective on the current discussion around the need for Black Americans to be treated better within our society. We also are giving employees the day off in hopes they will use the time to serve their communities, educate themselves on a topic they may not know enough about, and commit to inclusion and advocacy. Within our company, we are offering a full day of talks, workshops, and study groups, to educate ourselves on African American history and related issues. I believe with all of my heart that now is the right time to stand for our most authentic beliefs, and our most authentic desires for the generations that will follow us and the world that we want to leave for them. This is an important opportunity to reflect and act. The moment has come for us to help bring about a much-needed and very overlooked change; we should not let it pass. Today I speak to you not only as a CEO but as a father, an immigrant, and – most importantly – as a person who wants to help, and to do more for those who are struggling for their true freedom. At the end of the day, regardless of race, sexuality, or gender, we are all humans. Like all of you, I grow and change every day. I learn more about the world we live in. And I am chilled to my core by the way that so many people are being treated. There are many injustices facing our world, including the gravity of the struggle that African Americans face every day, the plight and inhumane treatment of migrant workers within India, and the struggles of the LGBTQ community around the world. No matter what, SingleStore stands with all disenfranchised communities. We want to do our part to bring awareness to the struggles of our fellow human beings and stand up to be the manifestation of the change that we hope to see in the world. Much time has passed since the liberation of African Americans from replicary; since 15-year-old Claudette Colvin was arrested for not giving up her seat on a bus, even before Rosa Parks; and since Ruby Bridges was one of the first African American children to attend a predominantly white school, where she was met with slurs and death threats. Such mistreatment did not only exist in the past; we still live in a society in which Black people are silenced and mistreated. While the 4th of July celebrates the liberation of America from Britain, Juneteenth celebrates the first semblance of liberation for African Americans. Juneteenth is a day to celebrate our African American colleagues. We also want this day to serve as a catalyst for other members of society. Let’s find ways to do more, to do better, and to help in our own ways as individuals. Let’s act in unison and be bold. The struggles that diverse populations across the globe experience cannot be solved in a day. But it is critical that we are taking steps to acknowledge and address historical injustices, and doing what we can to move forward.", "date": "2020-06-17"},
{"website": "Single-Store", "title": "telgoo5-mvno-att-sprint-t-mobile-verizon", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/telgoo5-mvno-att-sprint-t-mobile-verizon/", "abstract": "“In our business, you innovate, or you die. We had to find a company that innovates. We found that with SingleStore.” – Sandip “Sandy” Mehra, CEO, Telgoo5 and (v)WeCare Technology Imagine a business that provides customer service-as-a-service to a wide range of clients worldwide. They develop a robust, in-house digital infrastructure to help them serve a range of customer needs. Eventually, the digital infrastructure becomes so capable that they decide to offer it as a product in its own right – and the new product offering is a huge success. And they run it all on SingleStore. And no, we’re not talking about AWS – a SingleStore partner , and one of the public cloud platforms for SingleStore Managed Service . Instead, we’re talking about Telgoo5 , a long-time, and particularly innovative, SingleStore customer. It all started at Vcare, which started out providing call center support as a service to a wide range of customers. Now Vcare has spawned a new business: a real-time charging platform for mobile telecom operators called Telgoo5. Their system supports connectivity to all the major carriers – AT&T, Sprint, T-Mobile, Verizon, and others. Telgoo5’s competitors include online charging systems from CSG, NetCracker, and Oracle. The users of the Telgoo5 platform are Mobile Virtual Network Operators, or MVNOs, and they sell mobile access to their customers by the minute. This business is currently projected to grow to nearly $90B in size in the next few years. The MVNO must verify that the customer has sufficient balance available to make a phone call, send a text message, or use apps. They must also verify background data requests from a multitude of apps – messaging, mapping, navigation, and more. The verification must happen very quickly, or the phone fails to work properly for the customer. “In the current pandemic crisis, the two sides of our business are behaving differently,” says Sandip “Sandy” Mehra, Managing Partner of Telgoo5. “On the VCare side, where we provide customer support services, we are waiting and watching, to see how customer demand changes during the crisis. But on the prepaid mobile side, we anticipate steady demand, or even growth. Communication is more and more important, and prepaid mobile is a flexible alternative that customers like – especially with the performance that SingleStore is helping us to provide.” Incredibly fast analytics are required to meet the core customer need: the data connection must happen in a quarter of a second – yes, just 250ms – or less, or the data connection fails. Formerly, Telgoo5’s customers relied on a MySQL-based solution, running in five data centers around the world. Operators sharded the database manually, with partitioning problems and the need to reorganize data frequently causing downtime. The data centers often had problems or went down. And analytics latency meant that the 250ms requirement was often either barely met, or missed entirely. While this problem was shared by competitors, Telgoo5 was missing a big opportunity to stand out from the crowd. Mobile phone users lost their data connections, which reflected poorly on Telgoo5, and in turn reflected poorly on the carrier with mobile phone users. Unhappy customers and lost business for all concerned were the result. The SingleStore-Based Solution Four years ago, SingleStore pitched the company with a SingleStore-based solution, running on AWS, with everything in the cloud. They bought in, and today, Telgoo5 runs SingleStore on AWS. They have dropped all five of their physical data centers, each of which was a potential source of downtime and other problems. Both missed connections and downtime have gone from “all too common” to “almost unheard of.” Telgoo5 uses its new infrastructure to run machine learning models and AI programs, monitoring customer success and suggesting interventions, such as special offers and discounts on additional service levels, that help keep customer loyalty high. (Yes, as in our joint blog post with Fiddler.ai, Telgoo5 is using AI to reduce churn .) Also, as 5G begins to be deployed, Telgoo5 smoothly uplevels to the much higher data volumes and much shorter response times needed by 5G, with nary a hiccup. Data connections are often made in 35ms, more than 80% faster than required. Telgoo5’s telecom customers have more than tripled their business volume, with no impact on uptime or performance. Telgoo5 simply scales their infrastructure smoothly as business volume grows – and confidently reaches out to new customers. Telgoo5 is able to grow the solutions horizontally and vertically, giving the industry charging capability without limits. Telgoo5 is facing different challenges in its two lines of business. In the call center and related support services, Telgoo5’s original business, disruption caused by the novel coronavirus has upended business planning worldwide. The need of some companies for call center support is likely to drop until the economy recovers. But the online charging systems business is seeing an ever greater need for mobile communications. Prepaid mobile service, which is what Telgoo5 supports, provides a flexible and cost-effective alternative to subscription plans. Telgoo5 anticipates that demand may be likely to stay steady, if not increase. With their backbone running on SingleStore and deployed on AWS, Telgoo5 is well equipped to maintain and grow their share of this competitive market. The Online Charging Systems Challenge Online charging systems are used to provide data services for prepaid mobile users – the most common form of mobile phone billing in the developing world, and increasingly popular in economically developed countries. When a phone call is made, or a data connection is requested, metadata about the request – not actual voice or data content – goes out to the online charging system. The online charging system has a brief window, typically 250ms, to either make the voice or data connection, or fail. A mobile phone may instantly make 60 simultaneous requests for data. Requests come from mapping apps such as Google Maps, messaging programs such as WhatsApp and Skype, email, built-in text messaging, and many others. The data requests are made in 5MB chunks. For each request, if the connection is not made in time, or if a voice call or data transfer fails in progress, the phone service has failed the customer. It doesn’t take many failures for customers to demand refunds, cancel service, switch suppliers, etc. At worst, a customer can develop the belief that prepaid services in general simply don’t work well enough, taking them entirely out of the market that Telgoo5 serves. A typical connection handshaking process. Each transaction is supported by a customer transaction report (CTR), a record of the interaction. Hundreds of millions of CTRs are created and logged in a typical day. These CTRs support business on the scale of tens of millions of dollars a month. As phone call and data requests are fulfilled, the customer’s prepaid account is decremented for the voice call or data transmission. As certain data limits are reached, data speeds may be throttled until more headroom is purchased for the account. “We expect a lot from a database,” says Mehra. “Not only are we making requests and responding; we are creating plans, adding subscribers, removing subscribers, moving subscribers from one plan to another. We are executing real-time transactions, but beneath the surface, the database is doing so much more. You can think of it as creating virtual plans, as the customer goes about their day.” SingleStore is a uniquely capable, fully distributed relational database, with full SQL support and high concurrency. (That is, the ability to scale to support many thousands of queries per second, from any number of requestors.) Before using SingleStore, Telgoo5 was stuck with the same bad choices as other providers: Manual sharding . Many customers of MySQL and other single-process relational databases shard their database manually, then try to maintain and balance the shards through their operations staff. Paying a provider for manual sharding . Other customers pay a large database company for complex, integrated, hardware-plus-software solutions. They are basically paying an external company to shard and manage their single-node database for them. No cloud . Many providers offer no cloud solution, or limited solutions. This leaves customers building or leasing data center capabilities, with both internal and external providers finding it difficult to maintain acceptable levels of uptime and responsiveness. Missed SLAs . All of the existing database solutions strained to meet the required 250ms SLA with any regularity or reliability, causing problems for telcos and their end customers, who constantly suffered from bad connections. As an example of the kinds of problems that other operators suffer frequently, customers in the Philippines suffered a nearly eight-hour outage last year. Roughly 60 million subscribers suffered complete loss of service. Before using SingleStore, Telgoo5 maintained five data centers just to run their manually-sharded MySQL database in these on-premises locations. Before SingleStore/ Issues With SingleStore/ Results Technical solution -   Manually-sharded MySQL -   Constant architectural and operation difficulties -   Fully scalable SingleStore -   No architectural issues and ease of operations Infrastructure -   Five leased data centers -   Failure to meet SLAs; operational and management hassles -   Distributed on AWS -   Easily meeting SLAs; much easier operations and management Performance -   Frequently failed to meet 250ms connection window -   Dropped calls, dropped data connections, slow responses -   Regularly achieves 35ms connections -   Completed calls, completed data connections, fast responses Development & testing -   Development and test environments very different from deployment -   Constant hassles in services rollouts and trouble-shooting -   Development, test, and deployment environments very similar / identical -   Much easier services rollouts and trouble-shooting Business scale -   Difficult to maintain stable presence in challenging business -   3x growth in business scale and many likely opportunities for growth Flexibility -   Very difficult to make changes -   Easy to make changes for arbitrarily challenging environments Table 1. Technical, operational, and business prospects all improved with SingleStore. How Telgoo5 Stands Out SingleStore, by contrast, has had truly cloud-native software for many years – since before the term “cloud-native” even caught on. When Telgoo5 approached SingleStore, we offered them a very different solution: Telgoo5 moved to SingleStore, a fully distributed relational database, with no need for manual sharding. Operational costs dropped sharply. Telgoo5 runs SingleStore on Linux, on commodity hardware, with low server costs and easy maintenance, and easy scalability. This gives Telgoo5 nearly endless choices for development, testing, deployment, and backup. Because SingleStore supports ANSI SQL and is MySQL wire protocol-compliant, Telgoo5’s applications could more easily move from MySQL to SingleStore. These widely-used standards allowed Telgoo5 to choose from a wide range of business intelligence tools. Telgoo5 now runs entirely in the cloud, running on SingleStore in selected AWS regions. This solution delivers far greater reliability, flexibility, and uptime, with much less operations oversight. “We use all rowstore tables – in this business, with the demanding response times, everything is in-memory,” said Mehra. “Our original customer is so happy – knock on wood,” says Mehra. “They’ve had almost no downtime in the past three years, while their business has grown exponentially. Downtime is gone, and response times are great. We are getting new leads from potential customers worldwide – all to be hosted on SingleStore and running in AWS.” All of this is improving Telgoo5’s market share and prospects in the telecom billing and revenue management market – a market projected to nearly double in size in the next five years, reaching nearly $20B in size. “With SingleStore, we’re able to deliver, and outperform our customer’s expectations. We have no single point of failure. We complete transactions well within the SLA and give amazing service. We consistently deliver outstanding services to our clients.” How the Architecture Works The Telgoo5 platform brings in a range of inputs into a series of data stores, which then power a wide range of business intelligence tools, reporting tools, and apps. There are three inputs, two of which go to SingleStore. The Diameter Routing Agent and HTTP requests both go directly into SingleStore, with HTTP requests processed through a high availability (HA) architecture, and also sent to MySQL. Performance Co-Pilot (PCP), an open source framework for monitoring system performance, sends input directly to MySQL. Data then flows across four data stores: SingleStore handles transactional data processing and data analytics, sending results to three BI tools – Qlikview, RapidMiner, and Tableau – and on to MySQL. MySQL handles reporting and processing for the PCP dump, while data from both SingleStore and MySQL is used for reporting, data analytics, and predictive modeling using SQL, Python, and R. Processed data is then sent to AWS Aurora for distribution to reporting and email output, using Python and R. AWS Amazon S3 is used for long-term storage. Four data stores process analytics data for Telgoo5, with SingleStore in the lead position. This amalgamated data processing system supports a wide range of critical reports, updates, and predictive analytics models. SQL, Python, R, and Ruby are all used for data processing, analytics, and updates. QlikView, Tableau, and RapidMiner provide BI capabilities. And an internally created dashboard called R Shiny is integrated to the SingleStore database, as are email outputs and XML data processing, coded in Python and R. Business Results from the New Architecture “As a result of this new architecture, the size of our original business relationships have changed. Our telecom company customers are able to grow their customer bases, and end customer satisfaction has increased, because their services hardly ever go down. They not only increase revenues – they become more profitable as well.” “We are able to track multiple indexes for each of our customers. If any measurement falls short, we reach out and let them know. They are very happy with our business intelligence (BI) capabilities and our proactive reporting,” says Mehra. When Telgoo5 first engaged with SingleStore, they felt they were buying into the future. And SingleStore has continued to help deliver futuristic capabilities to Telgoo5 ever since. Today, Telgoo5 uses machine learning and AI, running against the SingleStore database, to monitor quality of service – QoS, in telecom parlance – and to detect potential customer churn. Specific, targeted, cost-effective interventions are made available, helping to keep QoS high and churn low. “The big thing in telecom is, how do you predict which customer is going to churn? For our competitors to process, say, 100 million CTRs – less than one day’s worth – might take them several hours. We can do this in minutes. Both we and our customers can do deep dives on the data.” Before / MySQL After / SingleStore Database MySQL SingleStore How to accommodate volume Hand-sharding and manual partition management, with downtime Easy scale-out without downtime Data infrastructure Five contract data centers, with downtimes, operational issues, etc. AWS availability zones worldwide Cloud-native? Not available when decision was made Yes (now has Kubernetes Operator) Meeting 250ms SLA? Often missed, resulting in failed data connections Nearly always achieved; many responses in ~35ms Missed connections and downtime Common; competitor in Philippines recently had nearly 8 hour downtime Vanishingly rare New customer/capacity setup Months of advance work Smooth scaling with demand as needed Cost basis Big capex investments before capacity can come online Smooth increase in opex as business volume increases Time required for routine reports Many hours Several minutes 5G readiness (“exponential increase” in data volumes & velocity) Extremely challenging Easy Machine learning/AI responsiveness Not real-time; offline/batch only Near-real-time / meets SLAs Competitiveness Average Exceptional Table 2. Before (MySQL) and after (SingleStore) comparison for Telgoo5. How Telgoo5 is Growing Their Business With all this information, the telecom provider cost-effectively grows their business; and Telgoo5, as the online charging system provider, using SingleStore as their core data platform, grows right along with them. Telgoo5 is also using their initial success and positive client feedback to make aggressive proposals for new business worldwide. As Sandip Mehra describes it, “Previously, with our data center-based solution, we had a very high Capex (capital expenditure) for any new client – and we had to make the investment up-front, before we could even get started. It took three to six months to grow a client’s infrastructure; now, it’s instant.” “With SingleStore,” continued Mehra, “running in the cloud, we have a much more manageable Opex (operating expenditure), which starts small and scales smoothly with demand. The money we save there goes into client satisfaction; we offer excellent support, and easily scale the backend horizontally and vertically, as needed. With our original client, we’ve smoothly supported them as they’ve tripled their business, with great customer satisfaction. And we do all this while maintaining excellent response times.” “This puts us in a powerful position, compared to our competition. They are using traditional databases and have high fixed costs.” 5G will demand far more from telecoms providers than the previous standard, 4G.(Image courtesy Cabling Installation and Maintenance.) 5G represents a big effort across the telecoms world. It generates far more data – an order of magnitude greater, or more – for a given operation, and demands much faster response times. “5G exponentially increases the amount, and the speed, of data that are flowing in,” says Mehra. “The need for real-time charging and customer rating increases, and we have to do it a lot faster.” “In our business, you innovate, or you die. We had to find a company that innovates. We found that with SingleStore.” Conclusion Like everyone else, Telgoo5 is facing rapid change in these challenging times. However, their move to SingleStore, to power their prepaid mobile phone charging solution, put them in much stronger shape for any circumstances. Telgoo5 is now prepared to maintain and grow this new line of business. If you are interested in knowing more about SingleStore for your own business, you can try SingleStore for free or contact us .", "date": "2020-06-19"},
{"website": "Single-Store", "title": "memsql-true-digital-flatten-the-curve-of-covid-19", "author": ["Gina von Esmarch"], "link": "https://www.singlestore.com/blog/memsql-true-digital-flatten-the-curve-of-covid-19/", "abstract": "SingleStore is powering in-the-moment analytics for True Digital Group’s COVID-19 tracking in Thailand, as described in this press release, published today. Also see our case study . SingleStore Powers True Digital Group’s Effort to Flatten the Curve of COVID-19 Partnership Aims to Save Lives Through Proactive Insights on Streaming and Geospatial Data SAN FRANCISCO – June 10, 2020 – SingleStore, the Database of Now™ for operational analytics and cloud-native applications, announced today that it is enabling in-the-moment analytics for a contact tracing app from True Digital Group Co., Ltd. (True Digital Group), the digital arm of True Corporation Public Company Limited, Thailand’s leading telecommunication company. The web portal aims to prevent COVID-19 spread in Thailand by using anonymized cell phone location data on 500,000 location events every second for over 30 million mobile phones to track population movement in two-minute intervals. This vast amount of real-time, geospatial data provides a view of population densities enabling the Thai government authorities to see when large gatherings are forming and quickly helps them to adapt their policies. “With the capabilities that we have at hand, we could not stand idly by in this situation,” said Bernd Sven Vindevogel, chief analytics officer at True Digital Group. “We took it as our responsibility as a member of the society to offer our help, and with our digital capabilities as a digital transformation enabler, we revisited our analytics solutions and chose to repurpose them to fit the situation while making sure that the data used in the process will always remain safe and anonymized. Due to the urgency of the pandemic and the amount of data needed, we chose SingleStore as our partner to help and make sure that our solution is always going to give an accurate and real-time result that will save lives and prevent any further development of the pandemic.” The Challenge Prior to this pandemic, True Digital Group’s analytics team was already leading in the analytics space for telco, with the successful deployment of a platform able to crunch trillions of relevant customer data points like location, browsing etc. True Digital Group’s Analytics team successfully launched location services based on this, for example, to optimize retail branch locations. The computation strategy was to land data in Hadoop, then run map-reduce operations on the data on a scheduled batch basis every 24 hours to create the aggregates and derived data. However, the tool was designed to operate in batch mode and did not require a real-time result. Then the pandemic hit, which changed everything. Now, the need changed from a “delayed-data-is-good-enough” mindset to an “in-the-moment, analytics with an SLA” mindset, as every second means increased potential for new cases. In order for the project to produce a meaningful result, the data process needs to be precise, accurate, and lightning fast. The Solution Given how quickly the coronavirus was spreading, True Digital Group’s analytics team evaluated its options and went directly to SingleStore. It is known that SingleStore has a reputation for speed, scale and structured query language (SQL). For fast-changing data, the response time of the backend database is a crucial component in ensuring a fast user experience. Based on its expertise in this space, True Digital Group identified SingleStore as being uniquely suited to these types of workloads, which require scaling to address the addition of new data and entail highly-concurrent lookup queries as well as complex analytic queries at speed. SingleStore supports more than a trillion rows per second on raw data and comfortably handles the ingestion of 500,000 events per second. The Result The first functional version of the True Digital Group COVID-19 spread prevention web-portal was built in under two weeks. The web portal uses React and Web Workers in the frontend and SingleStore Pipelines and time series functions processing on the backend. The SingleStore technology allows for fast ingestion of events and geospatial functions used to plot data on the map in real time. “We are living in unprecedented times. The solutions that are needed going forward will have to be radically different as the need for speed, scale and rapid deployment on commodity hardware will be essential. We are thankful to all the data architects and developers at True Digital Group who worked on this project and we are humbled to have contributed to their efforts through our technology for this worthy cause. We look forward to meeting them in person sometime soon,” said SingleStore Co-CEO Raj Verma. True Digital Group and its partners continue to iterate on the app to demonstrate new possibilities for its use by Thai government authorities for analysis. Read the full case study here . About SingleStore SingleStore is The Database of Now, powering modern applications and analytical systems with a cloud-native, massively scalable architecture. SingleStore delivers maximum ingest, accelerated transaction processing and blisteringly fast query performance, including AI integration and machine learning models, all at the highest concurrency. Global enterprises use the SingleStore distributed database to easily ingest, process, analyze and act on data, to thrive in today’s insight-driven economy. SingleStore is optimized to run on any public cloud or on premises with commodity hardware. Visit www.singlestore.com or follow us @SingleStoreDB. About True Digital Group True Digital Group is one of the core businesses of True Group, Thailand’s fully-integrated telecommunications, and digital service providers. Aiming to become the ultimate digital enabler in Southeast Asia, True Digital Group continuously expands its ecosystem to deliver a portfolio of high-quality digital services to customers. True Digital Group’s key businesses include Digital Media, O2O and Privilege, Data Analytics, IoT and Digital Solutions, as well as True Digital Academy. True Digital Group has built deep competences in cutting-edge technologies such as artificial intelligence, big data, blockchain, cloud, Internet of Things (IoT), and robotics. With this, True Digital Group is able to build a unique ecosystem of digital platforms and solutions, addressing the digital needs of consumers, merchants, and enterprises. True Digital Group also launched its regional operations across Southeast Asia, with Indonesia and the Philippines as its first two markets. For more information, please visit www.truedigital.com . Contact: analytics.partnerships@truedigital.com Contact Gina von Esmarch 415-203-4660 gvon@singlestore.com", "date": "2020-06-10"},
{"website": "Single-Store", "title": "the-importance-of-data-management-and-ai-during-covid-19-with-nikita-shamgunov", "author": ["Gina von Esmarch"], "link": "https://www.singlestore.com/blog/the-importance-of-data-management-and-ai-during-covid-19-with-nikita-shamgunov/", "abstract": "Join us for an in-depth discussion with Nikita Shamgunov, SingleStore co-CEO, and David Yakobovitch , the founder of the HumAIn Podcast . This 45-minute podcast episode covers a range of topics – from how other countries are using data and AI to help fight the cause and save lives during COVID-19 to how tech companies are using their platforms and products for good during the pandemic. Nikita also shares his predictions on how COVID-19 will change data management and digital transformation, how technology can help during the COVID-19 era, and how we can look to history for inspiration. “A lot can be accomplished through technology. But to deliver value, you need technology and people — and people who know how to use that technology,” said Nikita. “In fact, in World War II, Alan Turing was doing important work with this group of talented individuals to crack the Nazi code,” Nikita added. “He was driving that impact through technology and the intellect those individuals possessed. Similarly, when we face the crisis of scale of COVID, we cheer for the frontline workers every day as they save lives and put themselves at risk. But there’s plenty of work for information workers and data scientists, too. A smart politician would call for help from frontline medical workers, but also call for assistance from information workers — many of whom are in Silicon Valley.” Nikita continued: “The big tech companies like Apple, Google and Facebook have a tremendous amount of power and tremendous ability to help — both with the vast reach of technology, and monetarily. Individuals working for small tech can help too, by volunteering and making technology free for the right causes. That’s what we at SingleStore are doing with our technology, and as individuals.” Powered by RedCircle", "date": "2020-06-24"},
{"website": "Single-Store", "title": "keeping-the-discussion-and-movement-going-diversity-and-inclusion-at-work", "author": ["Gina von Esmarch"], "link": "https://www.singlestore.com/blog/keeping-the-discussion-and-movement-going-diversity-and-inclusion-at-work/", "abstract": "Last week, on Friday, June 19th, the U.S. celebrated Juneteenth. This holiday, recognized by most US states, commemorates the day back in 1865 when enreplicad African Americans in Galveston, Texas, got the word that the Civil War had been over for almost two and a half years – and that they were just then receiving notice of their freedom. SingleStore marked the occasion by observing a minute of silence, scheduling a day of programming related to systemic racism and the Black experience in America, and establishing Juneteeth as an annual company holiday. In the true spirit of SingleStore’s core values, employees were given the opportunity to learn and grow outside of their comfort zone with a focus on “learning, extending perspectives, and expanding virtues” as a community. Employees were invited to join in a Juneteenth  presentation by Milan Balington, executive director of the African American Community Service Agency , who spoke about the need for Black Americans to be treated better. The Juneteenth program that SingleStore organized also included the presentation and follow-up discussion of the documentary “ 13th ”; a discussion of the film “ Get Out ”; a pre-reading and discussion of Michelle Alexander’s book “ The New Jim Crow ”; and viewing and discussion of the podcast “ 1619 .” SingleStore also held a planning session for a SingleStore-sponsored virtual STEM/STEAM Day event for underserved children, which is  a collaborative event with co-sponsors Cisco and the National Coalition of 100 Black Women-Silicon Valley Chapter. The event is tentatively planned for March 2021. While the Juneteenth holiday provides all of us with a great opportunity to consider and learn about the Black struggle in the U.S., our efforts to get educated about and act to improve this situation should not – and cannot – end there. As employers, SingleStore and other companies across the country and around the world must work to better understand the barriers people of color face in attaining and retaining job opportunities. Society and businesses benefit when we do a better job at addressing diversity and inclusion, and eliminate bias. SingleStore Vice President-Legal, Aileen Casanave, is the executive sponsor for SingleStore’s Diversity & Inclusion efforts. She served as the moderator of Friday’s post-presentation Q&A with Balington, explaining why these efforts are so important, and how the COVID-19 pandemic has only increased the challenges that people of color face in the job market. Casanave notes that research shows that traditional recruiting steps tend to be part of a weeding-out process, rather than a true hiring process, for people of color. “Studies show that resumes with names like Jamal or LaToya, while containing the same skills and education as resumes with names like Michael and Lisa, are more likely to be rejected,” says Casanave, president of the National Coalition of 100 Black Women-Silicon Valley Chapter. On average, she says, people of color are less likely to obtain interviews, so they gain less experience in interviewing. As a result, they may then come across as less experienced or comfortable, when they do get an interview, compared to other candidates. She adds that shelter-in-place orders instituted in light of the coronavirus have amplified the challenges that people of color face during the recruiting and hiring process. She explains that, because interviews are often now conducted remotely rather than in person, people of color must contend with challenges – such as the need for appropriate face-front lighting issues, so that melanin-rich skin is clearly illuminated for less-than-optimal laptop camera, tablet, or mobile phone interactions. And, because they may have less experience in interviews, they may be even more awkward when talking via microphone and looking at the interviewer via a laptop or cell phone screen. “The extra challenge in the new-normal workplace is making a solid first impression,” Casanave notes. “Given the added challenges that people of color must overcome, the existing, widespread feeling of having to work twice as hard to make the same impact as other candidates are exacerbated.” Lauren Clark, who handles recruiting for SingleStore, adds, “As a startup, we are all working together as owners with a vested interest in building this company, and this effort doesn’t exist in a vacuum. Teams must work together across the organization and with our customers to enable and ensure our individual and collective ability to succeed. This is only possible when we hire individuals with the ability to work on diverse teams, and with a diverse range of people.” The SingleStore hiring process, she explains, includes asking the following questions: What skill sets are not represented within this team? How can we get people who are not just a culture fit, but are culture adds? How can we bring new perspectives, greater diversity, and fresh approaches to the table? “The best way to do this is by having a diversity of thought, education, experience, outside interests, and expertise,” she says. “We work hard to clearly define roles and interview processes, as well as to train employees to be aware of their unconscious bias and level the playing field. A consistent process ensures we’re comparing candidates on an apples-to-apples basis, helping us to do evaluations fairly, and to avoid unconscious bias. We recognize that conscious efforts toward diversity in the workplace drive innovation, increase creativity, make recruitment more successful, decrease turnover, and capture more of the market.” As Raj Verma shared in his recent blog post about Juneteenth , the SingleStore team is learning and growing all the time. These efforts accelerate that process.", "date": "2020-06-26"},
{"website": "Single-Store", "title": "memsql-google-bigquery-leader-tigani-chief-product-officer", "author": ["Gina von Esmarch"], "link": "https://www.singlestore.com/blog/memsql-google-bigquery-leader-tigani-chief-product-officer/", "abstract": "The following press release has been issued by SingleStore today – Tuesday, June 30th, 2020 – to announce the hiring of Jordan Tigani, SingleStore’s new chief product officer. SingleStore Names Former Google BigQuery Leader Tigani as Chief Product Officer New VP of Client Technical Services Asmar Brings Additional Power to the Database of Now SAN FRANCISCO – June 30, 2020 – SingleStore, The Database of Now™ for operational analytics and cloud-native applications, has named Jordan Tigani as chief product officer. Tigani was formerly the director of product management for Google BigQuery. Tigani assumes oversight of SingleStore’s engineering and product teams, with the goal of scaling these operations and expanding on the success of SingleStore’s cloud products. Tigani is acclaimed for having been one of the founders of Google BigQuery and leading its growth for the last few years, in both engineering and product leadership roles. At SingleStore, he will help define and execute on the product roadmap, solve customer problems, drive product innovation, and accelerate revenue growth. “The product and engineering effort at SingleStore represents half of the company,” said Nikita Shamgunov, SingleStore co-CEO and co-founder. “Given Jordan’s deep technology experience, unparalleled cloud and database domain expertise, and proven success, he is the ideal match for this position, and SingleStore could not be more excited to welcome him aboard.” “SingleStore offers me a great opportunity to use both sides of my brain,” said Tigani. “Having been a product manager, an engineering manager, and an engineer, this role fits my background extremely well. SingleStore has enormous potential. It reminds me of BigQuery a few years ago – stellar technology, just needing a bit of polish and focus before taking off into the stratosphere. I’m excited to be a part of that journey.” Prior to joining Google, a decade ago, Tigani had various engineering roles at early stage startups, and spent several years in the Windows kernel and Microsoft Research teams. During his time at Google, he authored two books on Google BigQuery, and correctly predicted 14 out of 15 matches in the 2014 World Cup, as part of an effort to demonstrate the power of integrating enterprise data warehouse and machine learning technologies. Shamgunov added, “Jordan is an important strategic hire who will help SingleStore realize the next level of our product evolution – to provide a single pane of glass for all data analytic workloads, in a world which is decisively based on cloud and multi-cloud environments and built for scale.” To complement the work that Tigani is overseeing, SingleStore is placing a strong focus on customer obsession from product design to sale. The company’s recent hire of Paul Asmar as vice president of client technical services underscores SingleStore’s commitment to its customers. Asmar is responsible for pre- and post-sale customer success, with a focus on enabling and expanding adoption, consistency, profitability, repeatability, speed, and time to value. “Customer success is my passion,” said Asmar. “Customer challenges are opportunities to make those we serve successful and turn them into repeat, referenceable customers. I’ve had a lot of success creating customer advocates over the past two decades working for leading enterprise software companies.” About SingleStore SingleStore is The Database of Now, powering modern applications and analytical systems with a cloud-native, massively scalable architecture. SingleStore delivers maximum ingest, accelerated transaction processing, and blisteringly fast query performance, including AI integration and machine learning models, all at the highest concurrency. Global enterprises use the SingleStore distributed database to easily ingest, process, analyze and act on data, to thrive in today’s insight-driven economy. SingleStore is optimized to run on any public cloud, or on-premises with commodity hardware. Visit www.singlestore.com or follow us @SingleStoreDB. Contact Gina von Esmarch 415-203-4660 gvon@singlestore.com", "date": "2020-06-30"},
{"website": "Single-Store", "title": "celebrating-pride-month-a-great-innovator-and-understanding-where-ai-stands-today", "author": ["Gina von Esmarch"], "link": "https://www.singlestore.com/blog/celebrating-pride-month-a-great-innovator-and-understanding-where-ai-stands-today/", "abstract": "June is typically associated with summer vacations, weddings, and Wimbledon – and the worldwide celebration of Pride Month . But 2020, as we all know, is not a typical year. It’s a year of change and, most importantly, reflection. Throughout history, periods of upheaval are often the genesis of strides toward achieving greater awareness, progress, and change. With parades, and other public celebrations, canceled, Pride Month is still an opportunity to drive momentum related to diversity and inclusion. There are records of movements for LGBTQ rights dating back to the 1920s. But the defining date that catapulted LGBTQ rights to center stage was on June 28, 1969, when the Stonewall Inn bar was raided by the police. Unlike most prior raids, this event was unique, in that the patrons fought back. Marsha P. Johnson famously declared, “I got my civil rights!,” as a shot glass was thrown into a mirror. This is now known as “ the shot (glass) that was heard around the world ”. The protests and riots, underscoring the much-needed awareness of this marginalized community, energized people to take action and create a change. Pride Month celebrates this. In respect of this movement, I want to celebrate a great innovator, lost too soon, who helped lay the intellectual groundwork that all of us in computing depend on every day: Alan Turing. On my first day at SingleStore, I noticed a conference room in SingleStore’s offices, named in Turing’s honor, with a large graphic image of Turing adorning the wall. Our co-CEO, Nikita Shamgunov, described him to me as one of Britain’s greatest minds — brilliant as a war hero, a giant of computer science, and a gay man who wanted the right to live freely. Born this month in 1912, Turing designed a computer that helped crack Nazi Germany’s code in World War II. Turing also is credited with laying the groundwork for artificial intelligence (AI) and modern computing, and creating the Turing Test, a method for determining whether machines can think . The test, which Turing introduced in the 1950 paper “Computing Machinery and Intelligence,” involves a human asking questions of an AI system and a human . The human asking the questions must then decide which of the respondents is the AI, and which is human. If the AI system wins, it has successfully met the Turing Test, and won the so-called “imitation game.” (The 2014 movie The Imitation Game , based on Turing’s life and tragic death, was a worldwide success.) Today, we may be close to meeting the Turing Test. You can have meaningful conversations with an AI system. The system can often trick people into thinking that they’re speaking to a human being, when they’re not. The ACM Prize in Computing recently went to the creators of AlphaGo , an AI system that plays Go. The system’s creators also repurposed it to play chess against the most advanced computerized systems in the world. Computers have been beating humans at chess for a while now, but this next-generation, AI-based approach became the best program to play chess so far. In discussing the Turing Test with me recently, Nikita commented: “In my opinion, the Turing Test doesn’t set the bar in the right place. It sets a bar, but that bar can be gamed. “I think what we want – and where we will get, eventually – is to more general intelligence. When that happens, we will reach an interesting inflection point in the history of computing and in the history of humanity, often referred to as the Singularity. “On the journey to general intelligence, it’s still very early days. We are seeing ever-increasing use of AI. Yet there are a lot of things AI enables us to do that we couldn’t do before, such as understanding what’s captured in a picture, or allowing cars to drive themselves. This is all evidence of AI moving forward. “But general intelligence is beyond that – it’s what humans can do. Humans can write books and prove mathematical theorems, think in abstract ways, and create things. And while a lot of work is going into artificial general intelligence, we are nowhere near advancing to the same level or capabilities that the human brain can achieve. “Today AI is still a supporting tool. It’s a better knife in the arsenal of knives for a tremendous chef. However, we have not replaced the chef.” That said, the Turing Test is a bar that was set in the 1950s. And that is absolutely a milestone. It put us on an important path, just as the Stonewall rebellion did in 1969. I don’t know how many years it will take to fully exceed the bar set by the Turing Test for next-generation AI. But I do know that innovation is a journey, not a destination. With that in mind, I’m encouraged by the strides made by the LGBT community, and happy to recognize Turing’s many achievements.", "date": "2020-06-30"},
{"website": "Single-Store", "title": "memsql-new-ui-memsql-clusters", "author": ["Roxanna Pourzand"], "link": "https://www.singlestore.com/blog/memsql-new-ui-memsql-clusters/", "abstract": "SingleStore’s new Install UI gives you a new, easier method to install clusters, complementing the current command-line interface. Just follow six short steps and your cluster will be up and running. We are excited to announce the release of a new tool that allows you to install a self-managed SingleStore cluster almost entirely through a browser interface. The user interface sits on top of our database management toolset, SingleStore Tools. The new user interface (UI) complements the existing command-line interface (CLI), so you have multiple options for installing SingleStore clusters. Previously, SingleStore Tools solely offered command-line methods to install the database. You can still use the command-line methods should you prefer them. We offer two CLI methods: one to specify your desired configuration directly in the command line , and one to drive the installation by configuring and passing in a YAML file . But now, if you want something more accessible than the command line, look no further! This new user interface improves the experience so the process of installing the SingleStore database is as quick and seamless as possible, so you are quickly generating and accessing real-time insights from your data. Using this interface, you can worry less about which options to specify in the CLI and how to specify them when installing your SingleStore database. Instead, the interface will walk you through all of that. In six easy steps, you can have a cluster up running and start querying away. Here are the steps: 1. Install SingleStore Packages . The first step is to install the SingleStore Tools (and other deployment packages) on the machine that you are deploying from. The exact command will depend on your Linux distribution; you can see more details here .  For a RedHat distribution, you would run the following commands. The first will add the SingleStore repository, and the second will install the packages sudo yum-config-manager --add-repo https://release.singlestore.com/production/rpm/x86_64/repodata/memsql.repo && \\\nsudo yum install -y memsql-client memsql-toolbox memsql-studio Link to the Deploy UI . Then, you need to run the SingleStore Tools command that will serve you the link to the Deploy user interface: memsql-deploy ui You may have to update the link with your host information before you input it into your browser. For security reasons, the link will expire within 24 hours, but you can always generate a new one. 3. Review system requirements and Input License Key Once you are in the user interface, you will be asked to review the SingleStore system requirements. Then, you will be prompted to input your License Key (which you can obtain from the Customer Portal ). You will also indicate whether the machines you are installing on have internet access; if you are installing offline, you must have the memsql-server package on the machine. 4. Provision hosts . Next, you provision the hosts that you are installing SingleStore on by entering hostnames and SSH keys. 5. Configure the cluster . After you provision your hosts, configure the cluster by inputting a superuser password, confirming high availability settings, and selecting which SingleStore nodes you will be running on the provisioned hosts (i.e., aggregators or leaves). 6. Confirm your configuration . Finally, review and confirm your configuration. Then press the button to continue with the installation. 7. Connect to SingleStore Studio . The last, optional step is to connect your cluster to SingleStore Studio, SingleStore’s visual management tool. You can alternatively connect to the database using a MySQL client. The user interface will walk you through all of these options. Tune into the below video to see how this feature works! You can also read more about the steps to access this interface here and the requirements to use it. That’s it! We look forward to your feedback on this new feature. Note that this new user interface is for self-managed SingleStore installations, driven by the downloadable SingleStore software. With self-managed SingleStore, you can install the software on on-premises hardware, or on cloud instances that you manage yourself. If you want to use a fully managed SingleStore database-as-a-service, use Managed Service https://www.singlestore.com/managed-service/ ), where you can create a cluster and have it running, managed by SingleStore, at the click of a button.", "date": "2020-07-01"},
{"website": "Single-Store", "title": "enabling-coronavirus-research-memsql-safegraph", "author": ["Carl Sverre"], "link": "https://www.singlestore.com/blog/enabling-coronavirus-research-memsql-safegraph/", "abstract": "As part of the #singlestore4good initiative, hosted at SingleStore.org , SingleStore is excited to contribute to the SafeGraph COVID-19 Data Consortium . The Consortium provides free access to relevant datasets to researchers, non-profits, and governments. SingleStore has created a repository to help existing customers, and those who wish to get started with SingleStore for free, to use our software in processing coronavirus-related data. SafeGraph is a company that offers point of interest (POI), business listing, and foot traffic data. They have started the COVID-19 Data Consortium to enable access to free data for responses to the worldwide coronavirus crisis. SingleStore joins more than 1,000 organizations contributing to data consortium, including the US Centers for Disease Control, the California Governor’s Office, and Johns Hopkins Hospital. It’s easy to get started using SafeGraph’s COVID-19 datasets today and to gain the benefits of leveraging speed, scalability, and SQL with SingleStore, for free . As CDC director Robert Redfield told the US Senate health committee , “There are a number of counties that are still doing this pen and pencil.” At SingleStore, we  encourage data-led approaches to the coronavirus crisis. Along with joining the consortium, we have created a repository, with scripts and SQL files, to help you quickly get started using existing SafeGraph and American Census data, running on a SingleStore Cluster. Check out the repo here: https://github.com/memsql/memsql-covid-datasets . While this blog post is focused on the COVID-19 (coronavirus) research effort, this is just one of the worthy causes we are supporting through our singlestore.org initiative. Qualified organizations interested in an enterprise license, or free Managed Service usage, can reach out to us at SingleStore . How to Get Started Here is an example of how quickly you can get started doing your own COVID-19 research, using the American Census dataset included in the repository. 1 . Create a Managed Service Trial The following example requires a running SingleStore cluster. If you are already a SingleStore customer, with either a free license or an Enterprise license, you can use your existing SingleStore instance. If you do not yet have a SingleStore instance, we suggest spinning up a free trial on SingleStore Managed Service . 2 . Clone the Datasets Repository from Github to Your Machine Carry out these commands on your computer to load data into your SingleStore cluster. # Start by cloning the datasets repo to your machine\ngit clone https://github.com/memsql/memsql-covid-datasets.git\n\n# Next, you will need to export some environment variables\n# to specify connection details for your SingleStore cluster:\nexport MEMSQL_HOST=\"*******.db.singlestore.com\"\nexport MEMSQL_PORT=\"3306\"\nexport MEMSQL_USER=\"admin\"\nexport MEMSQL_PASS=\"************\"\nexport MEMSQL_DB=\"census\"\n\n# For convenience we will define a temporary wrapper around the MySQL CLI\nalias memsql=\"mysql -h ${MEMSQL_HOST} -P ${MEMSQL_PORT} -u ${MEMSQL_USER} -p${MEMSQL_PASS}\" 3 . Create the Database and Load the Data Again, carry out these commands on your machine. # Create the database\nmemsql -e \"CREATE DATABASE ${MEMSQL_DB}\"\n\n# Now you can load the data!\ncat memsql-covid-datasets/american_census/schema.sql | memsql ${MEMSQL_DB}\ncat memsql-covid-datasets/american_census/pipelines.sql | memsql ${MEMSQL_DB}\n\n# At this point, all of the tables are loaded and the data is streaming\n# in via SingleStore Pipelines.  Open the SingleStore console and take a look:\n\nmemsql ${MEMSQL_DB}\n\n...\n\nMySQL [census]> show tables;\n+--------------------+\n| Tables_in_census   |\n+--------------------+\n| attribute          |\n| field              |\n| fips_codes         |\n| geographic_summary |\n| geometry           |\n+--------------------+\n5 rows in set (0.19 sec)\n\nMySQL [census]> show pipelines;\n+---------------------+---------+-----------+\n| Pipelines_in_census | State   | Scheduled |\n+---------------------+---------+-----------+\n| attribute           | Running | True      |\n| field               | Running | False     |\n| fips_codes          | Running | False     |\n| geographic_summary  | Running | False     |\n| geometry            | Running | True      |\n+---------------------+---------+-----------+\n5 rows in set (0.10 sec)\n\nMySQL [census]> select pipeline_name, batch_state, start_time, rows_per_sec from information_schema.pipelines_batches_summary;\n+--------------------+-------------+----------------------------+--------------------+\n| pipeline_name      | batch_state | start_time                 | rows_per_sec       |\n+--------------------+-------------+----------------------------+--------------------+\n| attribute          | Queued      | 2020-07-02 00:18:01.825899 |               NULL |\n| geometry           | In Progress | 2020-07-02 00:17:32.345491 |  582.9841048751618 |\n| fips_codes         | Succeeded   | 2020-07-02 00:17:31.536224 | 105.77593498406135 |\n| attribute          | Succeeded   | 2020-07-02 00:17:30.883479 | 4124020.8754720194 |\n| field              | Succeeded   | 2020-07-02 00:17:31.219881 | 248.05038971989336 |\n| geographic_summary | Succeeded   | 2020-07-02 00:17:31.988135 |  6971.634916464289 |\n+--------------------+-------------+----------------------------+--------------------+\n6 rows in set (0.28 sec) Note : Loading all of the data can take some time. You can check the progress of all the pipelines using the following query: MySQL [census]> select pipeline_name, sum(if (file_state = 'loaded', 1, 0)) / count(*) as progress from information_schema.pipelines_files group by pipeline_name;\n+--------------------+----------+\n| pipeline_name      | progress |\n+--------------------+----------+\n| attribute          |   1.0000 |\n| field              |   1.0000 |\n| geographic_summary |   1.0000 |\n| geometry           |   1.0000 |\n| fips_codes         |   1.0000 |\n+--------------------+----------+\n5 rows in set (0.14 sec) 4 . Run Initial Queries As with the previous two steps, carry out these commands on the server instance that will run your SingleStore cluster. Once the data is loaded, let’s run an example query! How many healthcare workers are registered in the Census? MySQL [census]> select sum(value) from field, attribute where field_level_4 like \"%Healthcare%\" and field_level_6 not like \"%margin of error%\" and id = field_id;\n+------------+\n| sum(value) |\n+------------+\n|   14845235 |\n+------------+\n1 row in set (0.445 sec) Which counties have the largest number of registered healthcare workers? MySQL [census]> select state, county, sum(value) from field, attribute, geometry where field_level_4 like \"%Healthcare%\" and field_level_6 not like \"%margin of error%\" and id = field_id and attribute.census_block_group = geometry.census_block_group group by 1, 2 order by 3 desc limit 10;\n+-------+--------------------+------------+\n| state | county             | sum(value) |\n+-------+--------------------+------------+\n| CA    | Los Angeles County |     379064 |\n| IL    | Cook County        |     232916 |\n| AZ    | Maricopa County    |     183230 |\n| TX    | Harris County      |     172601 |\n| CA    | San Diego County   |     136268 |\n| CA    | Orange County      |     128229 |\n| FL    | Miami-Dade County  |     111558 |\n| NY    | Queens County      |     102049 |\n| NY    | Kings County       |     100497 |\n| WA    | King County        |      95217 |\n+-------+--------------------+------------+\n10 rows in set (0.768 sec) Taking the Next Step Hopefully that gives you a taste of what you can do with our new SingleStore COVID-19 Datasets repo! Now that you’ve gone through the process of loading the US Census data, consider joining the SafeGraph COVID-19 Data Consortium , to get access to much more data, for free. We look forward to hearing about your progress; email us at covidresearch@singlestore.com .", "date": "2020-07-02"},
{"website": "Single-Store", "title": "forbes-nikita-management-scalability-power-of-data", "author": ["Nikita Shamgunov"], "link": "https://www.singlestore.com/blog/forbes-nikita-management-scalability-power-of-data/", "abstract": "This piece, which discusses the limitless potential for data and the need for chief data officers (CDOs) in progressive companies, appeared first in Forbes . The author, Nikita Shamgunov, is co-founder and co-CEO of SingleStore, and a member of the Forbes Technology Council. Data has incredible potential. It has the superpower to enable us to do things that were not previously possible. Armed with the right data, organizations can deliver better performance, financial results and user experiences. Businesses use their data to understand supply and demand, and when and where additional capacity is needed. Governments and other organizations can use data to help save lives in battles like we’re fighting today, as well as to gain data dominance and insights to win wars. One immediate example that comes to mind is COVID-19 contact tracing. Contact tracing (or cellphone tracing) is proving useful in helping to marginalize the spread of the virus. Given the global crisis and risks people are facing in some areas, countries such as China, Israel, Singapore and South Korea were among the first to pass laws enabling these practices during such situations. Once someone is identified as infected, these governments can access that person’s mobile phone records and instantly run a geo search. This real-time technology poses obvious privacy concerns , as it allows governments to identify people who were in the vicinity of an infected person in the past two weeks, locate them and test them. If they test positive, those people can be quarantined. This is particularly important considering some people who come in contact with infected individuals get the virus but don’t display symptoms, so they are unaware they are infected. The U.S. is reportedly doing work on this front, too. Reports say that the U.S. is gathering advertising data to understand where people are congregating. And part of the coronavirus economic relief bill earmarks $500 million for the Centers for Disease Control and Prevention (CDC) to help build a surveillance and data collection system that models some of the strongest use case guidelines to anonymize data while preventing spontaneous large gatherings from occurring. Big Datasets Require Bottomless Storage And Infinite Amount Of Compute In the U.S. alone if we were to track these data points, that would include hundreds of millions of subscribers — and that’s just people. There are also other devices with cellphone connections. If you track the location of each device on a per-second, or even a per-minute, basis, you would be able to collect a tremendous amount of data. Once the datasets get really big, you need data storage that is vast and essentially bottomless. Your storage should be able to store all data for you, regardless of how much data you have, and it should be accessible in the cloud. Cloud technology provides an infinite number of Lego bricks from which to assemble your solution. A cloud-native solution gives you infinite storage and infinite storage compute. When such databases are paired with artificial intelligence (AI), they allow organizations to also spot the unknowns. The possibilities expand. You can find clusters of infections and then predict where the spread is going to happen next. You can also do modeling, which enables you to answer a tremendous number of critical questions. Based on those findings, you can then identify questions you didn’t even think about asking with insights surfaced by combing through the dataset. This is when you see data revealing its true power. Enabling Organizations To Understand Situations And Optimize Experiences This global pandemic illustrates why it is essential for businesses and institutions to keep a pulse on their data. It is critical to understand not just how many cases there are, but also how people are moving around. By understanding what is being advised to prevent the spread of the virus, how it is being enforced and how it is having an impact, you need speed to insight. The answers to these questions must be quickly accessible; otherwise, you are operating blind. To help track the spread of COVID-19, there is no need to reinvent the wheel — the business tools to gather insights from data exist today. The data problems of COVID-19 are not fundamentally different from what Uber does to track supply and demand, Hulu does to get viewer insights and Amazon does to make product recommendations. Many businesses figured out a long time ago that harnessing data correctly is insanely powerful. Cellular service and video streaming providers rely on data-driven insights to understand the quality of service that customers in different corners of the country are experiencing. These service providers use this technology to identify where and when they need more capacity on their networks. In this instance, these multimillion-dollar decisions, such as whether to build a cell tower based on usage patterns, come down to the data providing a 360-degree view to predict business outcomes. But That Level Of Visibility Requires A Data Strategy And A Champion Companies thriving in this new normal are typically those that are focused on getting 360 visibility into what their clients do. In many ways, COVID-19 has presented us all with the opportunity to rethink our data strategies because businesses had to change how they did things within days of receiving the shelter-in-place orders. Data strategy used to be one of many priorities of the CIO. It is now critical that progressive companies hire chief data officers (CDOs). The larger the company, the more prominent that CDO position should be. This is because companies commonly have data stored in disparate systems. That makes it hard to understand the 360 journeys of customers. It takes effort and energy to bring all the data together to get 360 visibility and create strategy around artificial intelligence, data management and user experience. It is essential to have an executive voice, such as a CDO, to drive data strategy. The federal government has a rule requiring every U.S. agency to have a CDO. Interestingly enough, the Centers for Disease Control and Prevention just posted its CDO job on March 31 . We have never before needed CDOs, data strategies and massively scalable solutions as much as we do now. If you’re interested in learning more about SingleStore, you can use SingleStore for free or contact us .", "date": "2020-07-13"},
{"website": "Single-Store", "title": "memsql-expands-collaboration-with-amazon-web-services-joins-isv-workload-migration-program", "author": ["Gina von Esmarch"], "link": "https://www.singlestore.com/blog/memsql-expands-collaboration-with-amazon-web-services-joins-isv-workload-migration-program/", "abstract": "The following press release has been issued by SingleStore today – Wednesday, July 15th, 2020 – to announce expanded collaboration between SingleStore and AWS. SingleStore Expands Collaboration With Amazon Web Services, Joins ISV Workload Migration Program New Effort Will Enable Customers to Accelerate the Move to Cloud-Based Operational Analytics SAN FRANCISCO – July 15, 2020 – SingleStore, The Database of NowTM for operational analytics and cloud-native applications, has expanded its collaboration with Amazon Web Services (AWS) by joining the AWS ISV Workload Migration Program (WMP). By participating in this program, which helps AWS Partner Network (APN) Technology and Consulting Partners migrate independent software vendor (ISV) workloads to AWS via a repeatable migration process, SingleStore will accelerate the customer journey to the cloud for operational analytics. “Our customers include global enterprises that are leaders in their industries. Having SingleStore DB 7.1 available on the AWS platform, which is suited for all types of workloads, gives customers easy access to the fastest, most scalable SQL database in the world,” said SingleStore co-CEO Raj Verma. “SingleStore is The Database of Now, providing solutions for all enterprise workloads offering speed, scale and SQL. That makes this a uniquely powerful combination poised to fuel expansion for both companies.” The expanded collaboration brings significant value to SingleStore customers and the companies themselves. Together, AWS and SingleStore provide a blueprint for adoption, robust technology and other resources to enable customers to harness their operational data at scale. SingleStore’s modern data architecture can leverage the power of AWS services like Apache Kafka and Amazon SageMaker for streaming data, artificial intelligence (AI) and machine learning (ML). SingleStore also leverages solutions like Amazon Elastic Cloud Computing (Amazon EC2) and Amazon Simple Storage Service (Amazon S3) to provide full extensibility for real-time operational analytics. Broadening this collaboration comes at a particularly opportune time, as Gartner expects that three-fourths of databases will be deployed or migrated to a cloud platform by 2022 . This trend will be largely due to databases used for analytics via the software-as-a-service (SaaS) model. AWS recently selected SingleStore as one of five technologies it is highlighting in the AWS Global Financial Services Campaign. The campaign illustrates SingleStore’s ability to support financial services companies in managing time series data. AWS and SingleStore first collaborated in 2018. In addition to its membership in the AWS ISV WMP, SingleStore is an Advanced Technology Partner in the APN, a global program for technology and consulting businesses that leverage AWS to build solutions and services for customers. SingleStore’s bring-your-own license and metered offerings are available in AWS Marketplace, and the company has successfully transacted private offers through that platform. Managed Service by SingleStore – a fully managed, on-demand, elastic cloud database – also runs on AWS. “Businesses in the financial services, manufacturing and telco spaces are looking for proven and innovative partners for reducing the time-to-insight with the scalability and convenience of the cloud. Decision velocity — the ability to make faster decisions — is paramount for every organization,” said R “Ray” Wang, principal analyst, founder and chairman of Constellation Research. “This relationship brings all those components together.” About SingleStore SingleStore is The Database of Now, powering modern applications and analytical systems with a cloud-native, massively scalable architecture. SingleStore delivers maximum ingest, accelerated transaction processing and blisteringly fast query performance, including AI integration and machine learning models, all at the highest concurrency. Global enterprises use the SingleStore distributed database to easily ingest, process, analyze and act on data, to thrive in today’s insight-driven economy. SingleStore is optimized to run on any public cloud or on-premises with commodity hardware. Visit www.singlestore.com or follow us @SingleStoreDB. Contact Gina von Esmarch 415-203-4660 gvon@singlestore.com", "date": "2020-07-15"},
{"website": "Single-Store", "title": "load-balanced-failover-in-memsql-7-1", "author": ["Hristo Stoyanov"], "link": "https://www.singlestore.com/blog/load-balanced-failover-in-memsql-7-1/", "abstract": "This blog post describes a new approach to failover – the ability for SingleStore to continuously maintain live copies of the data on each SingleStore leaf node, and to switch over to those seamlessly when a fault occurs on the primary copy. In the new approach, copies of data are distributed more widely, and performance after a failure is better. In SingleStore DB 7.1, we introduce a new mechanism to place redundant copies of data within a SingleStore cluster. Building on top of our fast synchronous replication , SingleStore provides high availability. The new partition placement mechanism gracefully handles failures by making sure the workload that had formerly been on the failed node is distributed among multiple healthy nodes. Load-balancing the placement of partition replicas allows us to continue serving the workload with minimal performance degradation. A New Approach, with Benefits In previous versions of SingleStore, through SingleStore DB 7.0, nodes are paired for replication purposes. So if one node fails, all its traffic is sent to its paired node, creating a “hot spot” that will have roughly double the traffic of other nodes. From the customer’s point of view, queries that hit the “hot spot” are prone to suddenly slowing down, while other queries will continue to operate at their former speed. In SingleStore DB 7.1, we have implemented an improvement that significantly reduces this problem. We now load balance replicas of the original, master partitions across multiple leaf nodes. This avoids creating a bottleneck when a leaf fails, while maintaining excellent overall availability. Note . If you use SingleStore Managed Service , the concerns described in this blog post are managed for you by SingleStore. This blog post is only relevant for self-managed SingleStore, whether hosted by on-premises servers or in the cloud. The design of the new feature focuses on several goals: 1 . Evenly Distribute the Workload after a Node Failure In previous releases, SingleStore implemented a straightforward paired-leaf clustering scheme. Leaves form pairs. Within a pair, each leaf replicates the data of its paired leaf. It was simple to set up. This scheme has an issue: when one leaf fails, its paired leaf now has the live copies of its original data, plus the just-promoted copies of the data that had formerly been served from the failed leaf. The healthy paired leaf now has two nodes worth of data, and will have to serve two nodes worth of queries. That makes this leaf overloaded and slow, compared to the remaining healthy leaves, which each continue to have only one node’s share of live data. Customers experience a significant reduction in cluster performance after hardware failure and inconsistencies in responsiveness. In these previous versions, SingleStore paired leaves, and set up partition replication with each pair. The following picture illustrates a cluster with four leaves, split into two availability groups. The first pair is Leaf 1 and Leaf 2, while the second pair is Leaf 3 and Leaf 4. Partitions db_0 and db_1 have master copies on Leaf 1 and replicas on Leaf 2, while partitions db_2 and db_3 have their master copies on Leaf 2 and replicas on Leaf 1. In the old approach, master copies of data and replicas are distributed in a mirrored fashion across paired nodes. In this cluster setup, if Leaf 1 fails, the replicas of db_0 and db_1 on Leaf 2 must be promoted to masters to continue operation. Then Leaf 2 would see a 100% increase in workload, as it would have to serve queries for four master partitions, while Leaf 3 and Leaf 4 continue to each serve two master partitions each. Leaf 2 becomes a bottleneck. In the old approach, a failure left one node with twice the data of the others. In load-balanced mode , SingleStore will place partitions in a different way. The partition placement algorithm will distribute primaries evenly, then distribute the replicas needed by any one leaf node across multiple other leaf nodes. In this new mode, db_0 will have a replica on Leaf 1, while db_1 will have a replica on Leaf 4. More generally, the master copies of a partition on any given leaf will get their replicas spread out across multiple nodes. In the new approach, replicas derived from the mastercopies on a given leaf are distributed across several nodes. If Leaf 1 fails in this setup, SingleStore promotes the replica of db_0 on Leaf 2 to master, and the replica of db_1 on Leaf 4 to master. Each of these two leaves would see a 50% increase in workload, thus achieving a better distribution of the workload. In the new approach, a failure sees theburden distributed across more nodes. Larger clusters would see an even smoother distribution of the workload. More nodes means that the workload can be spread out to a larger number of nodes, each getting a smaller fraction of the workload from the failed node. 2 . Create a Balanced Partition Placement Previous versions of SingleStore maintain a good distribution of partitions on a given cluster. The load-balanced mode will place partitions in a way that all leaves have the same number of master partitions; the same number of replicas; failure resulting in the workload being equally spread among multiple other leaves. We introduce the term fanout as the number of other leaves that will receive additional workload after node failure. The fanout is determined automatically, based on the size of the cluster. 3 . Minimize Data Movement Necessary to Achieve a Balanced Partition Placement A major goal with our design was to minimize the number of partitions we have to move. Moving a partition from one leaf to another involves copying substantial amounts of data. Promoting a replica of a partition to master involves pausing the workload while the change happens. While this locking might be very short, it increases the latency of running queries. Additionally, this feature eliminates the need to provide buffer space on every single node to be able to handle twice the workload. This allows a higher utilization of the resources available on every host. The new mode also significantly reduces the likelihood of encountering out-of-memory (OOM) errors after a leaf failure. With the new mode, you can make full use of all healthy leaves. In the older mode, if one leaf goes down, its (healthy) paired leaf will also be taken out of operation when you run REBALANCE. With the new mode, it is possible to call RESTORE REDUNDANCY to ensure that every partition still maintains a replica. This gives you more time and ease around fixing a failed leaf, or provisioning and adding a new one. Memory, disk, and bandwidth requirements in normal operations remain the same for the new, load-balanced mode as for the older, paired mode. Functional Description The feature is enabled through a new variable called leaf _ failover _ fanout . This global variable is considered when running clustering operations, as well as CREATE DATABASE and REBALANCE operations. The master aggregator node is the only node that will read this variable. The variable is a sync var , to enable easier handling of master aggregator failovers. The syntax for the global variable is: SET GLOBAL leaf_failover_fanout = [ ‘load_balanced’ | **_'paired'_** ]; The default value of the global variable will be ‘paired’ . In the initial version of this feature, these two options ( load _ balanced and paired ) are the only allowed values. In particular, leaf _ failover _ fanout having a value of ‘paired’ means that the cluster will have the same behavior of pairing leaves as it did in versions up to SingleStore DB 7.0. That is, each leaf node replicates to one, and only one, other leaf node. Setting leaf _ failover _ fanout to ‘load _ balanced’ values enables the new mode, where the master copies of partitions are placed evenly on leaves, but duplication of each master copy as a replica onto a different leaf is planned separately. For more specifics, see our documentation; in particular, the section on enabling high availability in load-balanced mode . Customer Use Case Examples Here we show how to accomplish two tasks using the new capability: how to create a database with automatic fanout; then, how to grow and shrink a cluster that is using load-balanced failover. Creating a New Database with Automatic Fanout A customer can set up a new cluster from scratch and start using the fanout behavior by setting the global variable to ‘load _ balanced’ . Following the initial setup of the cluster with all the ADD AGGREGATOR/LEAF commands: set global leaf_failover_fanout=’load_balanced’;\n\nCREATE DATABASE db; This uses the default partitions and replicates them in a balanced way. The fanout is automatically determined based on the number of leaves available, as described below. Growing and Shrinking a Cluster In the new mode, customers continue to use the same sequence of steps to add or remove leaves. Setting the global variable to ‘load _ balanced’ makes sure that subsequent rebalances cause the fanout to change to the appropriate value. ADD LEAF root@’x.x.x.x’:y; // for every new leaf Further, enable the feature by setting the global variable SET GLOBAL leaf_failover_fanout=’load_balanced’; Since setting the global variable will not trigger any expensive operations, the administrator will have to run: REBALANCE PARTITIONS ON db; // execute for every database. Any subsequent rebalance operations, whether triggered manually or by add/remove leaf commands, will follow the new scheme for partition placement. Upgrading to the New Behavior For existing SingleStore users, the primary way to transition to the fanout behavior after upgrade from SingleStore DB 7.0, or previous versions, to SingleStore DB 7.1 or later, will be to set the global variable to ’load _ balanced’ and run REBALANCE on all the databases. SingleStore does not do this in upgrade on this initial release of the feature, since the rebalance is a long operation – it can take an hour or more. In order to enable the feature post-upgrade, execute the following commands: SET GLOBAL leaf_failover_fanout=’load_balanced’;\n\nREBALANCE PARTITIONS ON db; -- execute for every database.q Choosing Whether to Load Balance Clusters Using the new load balanced option is, well, optional. The defalt is to continue using the paired option from previous releases. How to decide which option to use? For most new databases, the new option has no upgrade costs, and no noticeable runtime costs. But, if and when you do have leaf nodes crash, the new option gives you better performance, post-crash. It also requires less buffer to be set aside for each node, saving you either space on your own servers, or money for operations in the cloud. So we recommend using the new option for new databases. The only question, then, is whether it’s worthwhile to transition to the new feature for existing databases. As described above, you simply turn load balance on, then rebalance your database. What will this cost you? Bandwidth . The rebalance operation necessary to transition to the new behavior on an existing SingleStore cluster will require moving a lot of partitions. If your cloud provider charges you for internal cluster traffic, this can turn into a monetary cost. Time . You have to run a full rebalance of your database. This may take an hour, or hours. SingleStore can advise you on how long a rebalance is likely to take. Disk space. The rebalance operation will create additional copies of your data for the duration it takes to move it. Having at least 30% free disk is recommended before making the switch. Here’s how to estimate the costs. For memory and disk, you will need about one-third free memory and disk space, over and above the size of the database. For bandwidth, the number of partitions that need to be moved during the transition is: N * (fanout-1) / fanout where N is the number of master partitions (and is also the number of replica partitions). Fanout is approximated by: # leaves / 2 This formula yields the number of partitions that need to be moved for specific numbers of leaves. For small clusters, half or more of the replica partitions are moved; for larger clusters, the number of replica partitions that need to be moved approaches 100%. Conclusion We recommend that you use load-balanced failover for all your new databases. It provides better performance after a node failure, giving you more flexibility as to when you have to rebalance. It also reduces your need for buffer space, saving you money. For existing databases, we recommend that you move to load-balanced failover the next time you rebalance your cluster. If that need is not otherwise imminent, you can decide whether it’s worth rebalancing simply to move to the new features. We believe that, in most cases, it will be worthwhile to move in the near future. If you are interested in learning more, and you are not yet a SingleStore user, you can try SingleStore for free , or contact SingleStore to find out more.", "date": "2020-07-16"},
{"website": "Single-Store", "title": "memsql-announces-latbc-strategic-partnership", "author": ["Gina von Esmarch"], "link": "https://www.singlestore.com/blog/memsql-announces-latbc-strategic-partnership/", "abstract": "The following press release has been issued by SingleStore today – Wednesday, July 22nd, 2020 – to announce the strategic partnership between SingleStore and top Latin American technology consultancy LATBC. SingleStore Establishes Commitment to Latin American Market with LATBC Strategic Partnership Established Company to Provide Service, Support in Mexico, Central America, and the Caribbean SAN FRANCISCO – July 22, 2020 – SingleStore, The Database of Now™ for operational analytics and cloud-native applications, has forged a new partnership with Latin America Business Consulting (LATBC) as the exclusive reseller for SingleStore in Mexico, Central America, and the Caribbean. LATBC, which has been operating in the region since 2003, will provide pre-sales and post-sales services, delivery, and support. This relationship extends the reach of the SingleStore team in this important region. Working with LATBC better positions the company to deliver cutting- edge technologies, enabling businesses in this part of the world to be globally competitive in this new era – one in which data has become an organizations’ most important asset. The SingleStore-LATBC partnership provides customers with an opportunity to do business with a local entity that offers promotions, service, and support suited to their needs. SingleStore’s selection process for its partner ecosystem requires that the partner company is aligned strategically with SingleStore, and adheres to the perfect blend of technological skill sets, geographic coverage, and company culture to deliver a seamless customer experience. “We are excited to welcome LATBC as a SingleStore strategic partner,” said SingleStore co-CEO Raj Verma. “LATBC embodies our ideal partner characteristics and is now part of our extended team in the Latin America region, representing SingleStore at the highest levels of the business sphere.” SingleStore’s desire to create this partnership was motivated in large part by an interest in working with Xavier Espinosa de los Monteros, founder of LATBC, and currently CEO and executive chairman. Espinosa de los Monteros is well-known and respected in the market, in which LATBC provides data and analytics solutions to important industry verticals such as financial services, media, and telecommunications. “Our commitment at LATBC is to help clients create bridges that move them into the future and onto new and valuable paths,” said Espinosa de los Monteros, LATBC CEO and executive chairman of the board. “Innovation is part of our DNA. LATBC’s ongoing push to embrace new models, our significant experience in the region, and the power of SingleStore’s groundbreaking technology will forge a path for our companies – and our customers – to build and benefit from the new data economy.” About LATBC LATBC is a top technology consulting firm committed to data. Over the past 17 years they have delivered more than 250 successful projects in the US and Central & South America, with thousands of incredible collaborators. LATBC is a true believer in the value of data. Their goal is to make data the rainmaker, the power, the engine and the fuel of every enterprise. Big is not enough in data; what matters is how you take action with it. Visit latbc.com or follow us @latbc. About SingleStore SingleStore is The Database of Now™, powering modern applications and analytical systems with a cloud-native, massively scalable architecture. SingleStore delivers maximum ingest, accelerated transaction processing, and blisteringly fast query performance, including AI integration and machine learning models, all at the highest concurrency. Global enterprises use the SingleStore distributed database to easily ingest, process, analyze, and act on data, to thrive in today’s insight-driven economy. SingleStore is optimized to run on any public cloud, or on-premises with commodity hardware. Visit www.singlestore.com or follow us @SingleStoreDB. Contact Gina von Esmarch 415-203-4660 gvon@singlestore.com", "date": "2020-07-22"},
{"website": "Single-Store", "title": "digital-transformation-podcast-10-key-insights-into-business-in-the-covid-19-era", "author": ["Raj Verma"], "link": "https://www.singlestore.com/blog/digital-transformation-podcast-10-key-insights-into-business-in-the-covid-19-era/", "abstract": "Listen in to this in-depth discussion with Raj Verma, SingleStore co-CEO, and Kevin Craine, host of the Digital Transformation Podcast . This 25-minute podcast interview covers how digital transformation has shifted from being a “nice-to-have” to a tactical “must-do.” Raj discusses how software and data have helped businesses plan strategies to navigate their COVID-19 response and recovery. He shares advice, and a call-to-action for companies to reframe their business strategies. Listen here for the full interview: Data Strategies for a Post-COVID Business World . In the COVID-19 era, businesses are undergoing digital transformations much faster than anyone could have predicted.  When we look back at this period of time, I think we will find that COVID-19 is going to make us stronger, as companies, as individuals, and economically. To hear more about the 10 key areas that Kevin and Raj explore during this conversation, click below.", "date": "2020-07-22"},
{"website": "Single-Store", "title": "tapjoy-moving-to-memsql", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/tapjoy-moving-to-memsql/", "abstract": "Tapjoy is an early and enthusiastic customer of SingleStore. They described their move to SingleStore on the Tapjoy engineering blog . This blog post gives some of the high points of Tapjoy’s experience, spelling out why Tapjoy moved – and what they now get from SingleStore. (This blog post is updated from the original version, which appeared several years ago. It includes more details, from several sources, including recent updates to SingleStore.) Tapjoy is a leader in advertising and app monetization. They use video ads, offers, and rewards to help app publishers grow their user base and monetize mobile apps. They have been very successful, with their SDK embedded in more than 30,000 mobile apps. Tapjoy reaches more than 800 million active users a month and has more than a dozen offices worldwide. Tapjoy’s engineering team is also global. They have built up a robust data processing backend, which processes billions of data points a day. The backend includes Kafka for data streaming, Amazon’s RDS, MySQL, PostgreSQL, and Google BigQuery. They’ve established solid procedures for problem-solving, vendor assessment, and change management. The company faced performance issues with a workload running in MySQL. They carried out a careful assessment of alternatives, tested them, then made their move – the move to SingleStore. What Tapjoy Needed Tapjoy’s growth often leads them to improve their stack with new components that scale better. Several years ago, the company had a key workload, including financial data, in MySQL. This workload was an exception to a key TapJoy rule: no applications of large size and scale on MySQL . Widely known problems with MySQL include: Not scalable for performance . (See the link, and the MySQL vs. SingleStore comparisons below.) The database is owned by Oracle, which is not transparent about the MySQL roadmap . MySQL is suffering defections from many users. Any key workload that stops performing well causes Tapjoy business risks similar to those of any digitally-driven company: inability to grow the business efficiently, greater need for technical personnel, difficulty meeting service level agreements (SLAs), and lower quality of service. The team at Tapjoy assessed alternatives. They wanted to move fast, so they used a strict set of criteria: ACID-compliant . The new product needed to be a relational database – either a traditional RDBMS or a NewSQL contender. NoSQL was not an option. Scalable . The system needs to be designed for scalability. While Tapjoy has a strong DevOps team, they weren’t going to be asked to support a system that wasn’t ready to scale. 10x-capable . In particular, any new system needed to scale to meet demand at least an order of magnitude greater than was already occurring at the time. Drop-in replacement . The new solution needed to “swap in” smoothly for MySQL; the change needed to be quick, easy, and as code-compatible with MySQL as possible. The need for scalability was where MySQL fell down, and that’s not only a MySQL issue; RDBMS alternatives tend to lack the ability to scale to the extent Tapjoy needs. What Tapjoy Considered With all these constraints, Tapjoy only found a few alternatives worthy of serious consideration: Sharded MySQL . Many organizations meet scalability demands by sharding MySQL. Neither engineering nor Ops wanted to take on this challenging task, nor to stay with a system that was already failing to do the job. AuroraDB . The AuroraDB option in AWS Relational Data Service (RDS) is wire-compatible with MySQL, a big advantage. However, when tested, it didn’t clear the bar for performance. AuroraDB is scalable by design, but it did not meet Tapjoy’s needs for scalability, in practice. SingleStore . SingleStore was already in use by the Tapjoy data science team. This team described SingleStore as ACID-compliant and fully scalable, offering stellar performance. And, like AuroraDB, it’s MySQL wire-compatible. SingleStore is designed from scratch to meet demanding requirements like Tapjoy’s. SingleStore Offers Huge Performance Gains As part of their consideration of alternatives, the team compared SingleStore to alternatives, and here’s where SingleStore really stood out: the legacy system was far slower than SingleStore. Sean Kelly, who wrote the Tapjoy engineering blog post about the move to SingleStore, sets the stage: “SingleStore had everything we wanted, and after an initial pilot, we confirmed it would meet our stated requirements. But the proof is in the pudding. We set out to build a cluster, shadow-write data to it, and put it through a battery of tests to make sure it would hold up to our use cases.” The tests included putting SingleStore under 20x stress test loads and running it with twice the amount of historical data available as the previous database solution, while that database was run under existing cluster load. Summing a financial data point for a single partner – an operation that needs to be repeated over and over, given Tapjoy’s huge reach in gaming – took more than a minute (68 seconds) on the old solution. It took 1.23 seconds – 55 times faster – on SingleStore. Monthly reporting can be crucial, especially when customers are pulling their financial reports together – and paying their bills. Under the old system, summing a single financial data point for the entire platform over a month took nearly half an hour (28 minutes); with SingleStore, it took 1.1 seconds, an increase of more than 1500x. Along with Finance, Marketing is a big constituent for any in-house data system, and there’s a regular need to pull lists of recently active customers, for a variety of purposes. On the old system, this took 24 seconds; with SingleStore, running under 20 times the load, it took three seconds, an eight-fold improvement. As indicated by load test results, SingleStore met all of Tapjoy requirements and turned out to be a clear winner in the head-to-head load tests. SingleStore also met Tapjoy’s other requirements: ACID compliance, SQL compatibility, scalability, and the ability to drop in as a replacement for MySQL. Conclusion After initially adopting SingleStore for this single use case, Tapjoy went on to use SingleStore for several use cases. SingleStore and Tapjoy announced the results of the two companies’ work together, and Tapjoy presented on the topic at the In-Memory Computing Summit . You can see a more recent presentation of Tapjoy’s architecture in this presentation, Building a Real-Time Data Science Service for Mobile Advertising . And, if you’re interested in trying SingleStore for your own challenging data issues, you can try SingleStore for free or contact SingleStore .", "date": "2020-07-27"},
{"website": "Single-Store", "title": "memsql-ramps-up-the-cadence-of-financial-dashboards-for-an-industrial-machinery-leader", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/memsql-ramps-up-the-cadence-of-financial-dashboards-for-an-industrial-machinery-leader/", "abstract": "According to analysts, “the growth rates of insights-driven businesses makes them an economic tidal wave” – one that will earn $1.8 trillion dollars by 2021. How does a long-established, Global 500 company, listed in the Top 100 global brands , become an insights-driven business? A leading producer of industrial machinery is taking bold steps in this direction, powered by SingleStore. Industrial machinery is a broad and varied business. It includes components that go into planes, trains, and automobiles – and the machines that make and power those components. The largest industrial machinery companies are global and have hundreds of thousands of employees. These might seem to be classic “old economy” businesses – primarily involved in making things, not software or services, and requiring a great deal of capital and people to get things done. Yet leading companies in this segment are well-represented on the cutting edge of technology, and are heavy users of SingleStore. As with other cutting-edge businesses, industrial machinery companies need speed, across their global operations. For the SingleStore customer profiled here, the immediate need for speed was in analytics. Like many large organizations, the company has an internal financial reporting application, which we’ll refer to here as Cadence. The Need for Speed in Cadence Cadence is a high-visibility product within this global company. In fact, given that the company has hundreds of thousands of employees, along with a large network of suppliers, customers, and other stakeholders, Cadence is a widely-used and well-known application by any standard. Cadence supports direct SQL queries from a wide range of users, and from widely used business intelligence (BI) tools such as Dundas BI, Looker, Microsoft Power BI, and Tableau. Cadence users include personnel at all levels of the company, including the very top. Users were eager to get the data Cadence provided, and dashboards and custom applications made the data easily accessible and actionable – at first. A representative dashboard from Dundas BI. (Image courtesy Software Connect.) However, as the demand to use Cadence grew, performance became a problem. Like many large companies, this one is a Microsoft shop for many of its software needs. With site licensing in use for many products, the cost per seat for Microsoft SQL Server was negligible. Yet the cost of poor performance was too great to ignore. (We have a lot of SQL Server experience among the SingleStore product team, including the author of our very popular blog post on the problems with NoSQL .) To try to achieve acceptable performance in SQL Server, the company had tried using the cubes feature of SQL Server Analysis Services. The cubes feature was required because it brings all needed data into memory, which was the only way to get performance somewhere close to what the company needed to power dashboards. The cubes feature, though, requires the creation of a data warehouse and other steps. One of the project leaders described the process of creating data cubes as “really data-intensive” and “prone to error.” Loading the cubes into memory took hours. A depiction of data cube operations. (Image courtesy Medium.) So the company was stuck with a slow, hard to use, and complex data infrastructure that was not meeting its needs in powering Cadence and other analytics needs. They went looking for an alternative. Finding a New Solution Personnel at the company Googled several terms related to what they needed, including “fastest in-memory database” – and found SingleStore. The ironic thing here is that SingleStore has both in-memory rowstore tables and disk-based columnstore tables, both with excellent performance. The two outstanding aspects of SingleStore, in relation to this company’s needs, are incorporated into the company’s name: “Mem” for performance. The “Mem” in SingleStore represents SingleStore’s in-memory rowstore and memory-led, disk-based columnstore tables. The company needs near-real-time responsiveness, including for complex operations, and they get it with SingleStore. “SQL” for compatibility. In the SingleStore name, SQL represents that SingleStore is a fully relational database (not a NoSQL database ), with full ANSI SQL support. The company uses a wide range of business tools, and has internal SQL expertise, so compatibility is vital. SingleStore is a fully scalable relational database that supports SQL. Due to SQL support, it can be used as a nearly drop-in replacement for most existing relational databases. The built-in MySQL wire protocol support in SingleStore means that it works “out of the box” with a very wide range of BI tools, including the ones this company uses. (This is in stark contrast to NoSQL databases, which are scalable, but lack SQL support . This makes them both difficult to write queries for and slow to respond to queries.) SingleStore works very well as a drop-in replacement for slow-performing analytics databases. Our recent webinar on data migration shows how easily customers can move to SingleStore for this purpose. What makes SingleStore stand out, compared to those other tools, is performance, across both rowstore and columnstore tables. The company ran tests that took 40 seconds in Amazon Aurora, two seconds in SQL Server, and 30ms in SingleStore. A test of 100 different SQL statements took 750ms in SQL Server and only 9ms in SingleStore. After a thorough review, they decided to make the move to SingleStore. Running Orders of Magnitude Faster with SingleStore Initially, getting started with SingleStore took some work. As a fully distributed relational database, using SingleStore “required some study to understand how the database works, and set up good sort and shard keys,” according to the project lead. Now, compared to the former, SQL Server-based solution, running SingleStore is much simpler. According to the project lead, “The installation of SingleStore was straightforward, and we are now seeing 10x to 100x performance improvements compared to MSSQL.” The company is able to get the performance they need, at low cost, using SingleStore’s disk-based columnstore tables in SingleStore. Only a few tables are in rowstore, which runs faster for many operations, but is memory-based – and therefore more expensive to run. With SQL Server Analytics Services, by contrast, the cubes that supported both BI tools and apps had to run entirely in memory. Now, with SingleStore, dashboards update much faster. The company processes tables with hundreds of millions of rows in SingleStore, totalling more than half a terabyte in size, and still has plenty of headroom for growth. They can respond, in real time, to critical issues and opportunities alike. The company now has the performance it needs, with compatibility and operational simplicity. The project lead describes his team as “very happy” with SingleStore. SingleStore has performed so well that the company now has big plans for replacing MSSQL and other database implementations with SingleStore instances. “We will move our three biggest projects to SingleStore in the next few months,” said the project lead. “After that, all our new projects will use SingleStore. Existing projects will be migrated as we have time.” Getting Started with SingleStore If you have workloads that require fast ingest, fast processing of large data tables, and fast queries, in operational analytics or other use cases , try SingleStore for free today , or contact us to learn how we can help you.", "date": "2020-07-29"},
{"website": "Single-Store", "title": "ventana-research-cites-data-infrastructure-as-key-to-ai-success", "author": ["Gina von Esmarch"], "link": "https://www.singlestore.com/blog/ventana-research-cites-data-infrastructure-as-key-to-ai-success/", "abstract": "Advisory firm Ventana Research has released a new Analyst Viewpoint piece, describing the need for “a bridge between analytics and operational systems” in AI and machine learning. Although the report does not name any specific product, such a bridge must have “scalability, elasticity and real-time processing capabilities” – a nearly perfect description of SingleStore . In this Analyst Viewpoint, David Menninger SVP and Research Director at Ventana, describes AI and ML as useful – but as also being demanding on an organization’s IT infrastructure. Both large volumes of historical data, and live, up-to-the-minute new data, need to be brought together in real time. The process of scoring records – something that SingleStore is widely used for, as in this case study from child safety nonprofit Thorn – is key to the success deployment of many ML models. The report also cites key benefits of machine learning. Intriguingly, four of the six key benefits are forward-looking, areas where a company is moving into the future: competitive advantage, customer experience, increased revenues, and the ability to respond to opportunities faster. These reasons, combined, are cited as primary by 87% of the respondents in a previous Ventana report. Only 13% of respondents cited pain reduction, making things better in the short term: reducing errors and lowered costs.", "date": "2020-07-29"},
{"website": "Single-Store", "title": "bct-limit-covid-19-using-memsql", "author": ["Gina von Esmarch"], "link": "https://www.singlestore.com/blog/bct-limit-covid-19-using-memsql/", "abstract": "The following press release has been issued by SingleStore today – Wednesday, July 28th, 2020 – to announce the use of SingleStore technology for COVID-19 tracking by SingleStore partner Bahwan Cybertek (BCT). Middle Eastern Technology Company Partners With BCT on Effort to Limit COVID-19 Spread using SingleStore Joint Endeavor Highlights the Power of Digital Transformation, Massively Scalable Architecture SAN FRANCISCO – July 29, 2020 – In an effort to help a government in the Middle East to minimize the spread of COVID-19, Bahwan Cybertek (BCT) will deploy technology from SingleStore, The Database of Now™for operational analytics and cloud-native applications. The effort is led by a large technology company in the region. The client company offers digital services and information and communications technology solutions in several categories, including cybersecurity, digital media, financial technology, IT and telecommunications. For this project, it is collecting data from other telcos in the region to present the relevant government ministry with a unified view of the coronavirus situation and enable it to maximize the impact of the solution. This project requires the ingestion of data from 1.2 billion location points to be joined to 5 million location tiles, each 100 meters square, to specify the location of mobile phones. The location tiles are aggregated into cities, districts, and regions to provide an understanding of point-in-time population density. At first, the company attempted to use technology from Teradata for this joint process, but a test query never returned. When Hadoop was tried, it took more than a day to return results. SingleStore replies to the same complex query in 20 minutes. Using SingleStore, the complete workflow process was built in just three weeks. The client company obtained the SingleStore technology through its work with BCT, the leading systems integrator in the Middle East, which also has significant operations in India. BCT specializes in digital experience, digital supply chain management, and predictive analytics. Leveraging its international network of industry experts across a range of industries, BCT has delivered digital transformation solutions to more than 1,000 customers in banking, government, oil & gas, power, retail, and supply chain management (SCM)/logistics across Africa, Asia, the Far East, the Middle East, and North America. “SingleStore is excited to be working with BCT as a systems integration and reseller partner to serve this client, which evaluated several market-leading products and chose SingleStore for its ability to manage workloads at a massive scale, pull in data from a very diverse set of systems with high concurrency, and to answer SQL queries at amazing speeds,” said Raj Verma, SingleStore co-CEO. “This customer achieved more than three million transactions per second within the first few weeks of using SingleStore.” BCT has deep relationships with its customers and helps its clients – some of whom are also SingleStore clients – get the maximum value from their technology investments. BCT is building deep capabilities related to SingleStore technology, which it is combining with its other domain and technology experience to help customers succeed throughout their digital transformation journeys. Meanwhile, BCT, SingleStore, and the client company are now launching multiple additional projects, reducing the client company’s dependence on Oracle and Teradata, and augmenting its existing Hadoop environment. “Companies that invest more in digital transformation actually outperform their peers over time,” said S. Durgaprasad (DP), co-founder, director, and group CEO of BCT. “These companies are more prepared for disruption, and better able to monetize new digital channels and build bigger user bases. What’s more, this phenomenon exists regardless of industry vertical.” About SingleStore SingleStore is The Database of Now, powering modern applications and analytical systems with a cloud-native, massively scalable architecture. SingleStore delivers maximum ingest, accelerated transaction processing and blisteringly fast query performance, including AI integration and machine learning models, all at the highest concurrency. Global enterprises use the SingleStore distributed database to easily ingest, process, analyze and act on data, to thrive in today’s insight-driven economy. SingleStore is optimized to run on any public cloud or on premises with commodity hardware. Visit www.singlestore.com or follow us @SingleStoreDB. Contact Gina von Esmarch 415-203-4660 gvon@singlestore.com", "date": "2020-07-29"},
{"website": "Single-Store", "title": "hadoop-the-chronicle-of-an-expected-decline", "author": ["Yassine Faihe"], "link": "https://www.singlestore.com/blog/hadoop-the-chronicle-of-an-expected-decline/", "abstract": "This blog post describes how Hadoop has moved through the famous Gartner Hype Cycle, and how it is now often combined with SQL-native solutions to deliver results. The author, Yassine Faihe, is Head of International Solution Consulting at SingleStore. This piece originally appeared, in slightly different form, on LinkedIn last December. Hadoop, which was once poised to rule the large-scale data analytics space and toll the bell for traditional data warehouses, is currently experiencing a hard fall. This apparent decline began nearly ten years ago, when articles and blog posts discussing the failure of Hadoop and questioning its future started to proliferate: ‘The Hadoop Hangover’, ‘Why Hadoop is Dying’, ‘Hadoop is Failing Us,’ and so on. The decline of Hadoop was further illustrated by the merger of Cloudera and Hortonworks, and the sudden acquisition by HPE of the remains of MapR. Hadoop’s adoption is currently stagnating, and even declining, despite persistent enthusiasm from practitioners. Today, many wonder what is the future of Hadoop and, more importantly, how do Hadoop adopters still make the most of their investments? In this post, I will argue that Hadoop is simply going through the evolution cycle of any emerging technology, and I will attempt to analyze this evolution from the perspective of the Gartner Hype Cycle . This image appears courtesy of Wikipedia . Technology Trigger Nearly fifteen years ago, the Hadoop breakthrough occurred and was heavily reinforced by early adopters and media coverage. The technology trigger was the moment when Hadoop became an Apache project with HDFS and MapReduce, and Yahoo deployed their first clusters in production, followed by other large websites, and then emerging data companies. Hadoop was cool and open source, and it disrupted the storage industry – while bringing a distributed compute capability needed for modern data sizes, thus triggering an unprecedented rush. Peak of Inflated Expectations The peak is an interesting and funny milestone for technologies in general, in the sense that, at this point, technologies are often considered to be a sort of panacea for any problem in a given domain or industry. For Hadoop, this peak happened when the terms “Hadoop” and “Big Data” became synonymous. A virtuous circle was created by reported success stories from notable data companies, and later reinforced by enterprises starting to operationalize their big data strategy by implementing Hadoop-based data lakes. However, those claimed successes were not always fully supported by facts, and failures were overlooked. Almost every enterprise embarking on a digital transformation had to demonstrate some kind of tech-savviness, and was either investigating or experimenting with Hadoop as part of becoming more data-driven. The strong coverage by mainstream tech media and the perception that Hadoop was “free” further accelerated and amplified the phenomenon. At the same time, huge amounts of funding from venture capitalists were flowing into the startups behind major Hadoop distributions. Hadoop ,as an Apache effort and distribution, grew to encompass more than 30 projects – from storage to compute frameworks to security and much more. Companies wanting to get a competitive edge by analyzing a large amount of diverse data jumped on Hadoop, looking at it as a silver bullet. Many adopted it without doing their due diligence. They were instead driven by the curiosity of engineers, who were eager to experiment with this emerging set of technologies. Hadoop was so hyped that it became the de facto answer to almost any data-related issue. That was reinforced by Hadoop’s ability to store big structured and unstructured data at a fraction of the cost of a storage area network (SAN). It was easy to get data in, so companies went all-in! Only later would they learn that their data lakes would become, in effect, “write-only.” Trough of Disillusionment It is at this stage of the hype cycle that failures become noticeable and the technology is called into question. This typically happens when more and more projects are promoted, from experimentation and pilots to production at scale, and more mainstream companies adopt the technology, thus increasing the likelihood of failure. For Hadoop, this started in about 2015, when some customers began to report on production issues related to operability, the lack of enterprise-grade security, and performance. Very often, failures such as these are not caused by the technology per se, but by the way it is used. The hype prevented those adopters from differentiating between what they needed, what was promised, and what the technology could actually deliver. They eventually failed to get the expected value out of their investment, thus experiencing disillusionment. Hangover is the result of over-inflated expectations. The main reason that explains this failure is Hadoop’s inability to analyze data to produce insights at the required scale, with the needed degree of concurrency, and at speed. Storing data on Hadoop was easy, but getting back insights at speed and scale has been a common problem expressed by many practitioners. Thus, Hadoop has been given the nickname “data jail”. While there is a plethora of querying engines that are often faster than Hive, the default, and which claim to be faster than one another, performance is still far from people’s expectations. In addition, Hadoop projects often had no particular analytics strategy aligned with business needs and driven by tangible outcomes. The approach was to land all data in a data lake and hope that people would then come and derive insights the way they wanted. This approach was not only applied to greenfield projects, but also to replace underperforming data warehouses. For the latter, data was typically extracted from multiple relational databases, denormalized, massaged in Hadoop, and eventually put back into data marts, to be served to analysts and data-consuming applications. Another equally important reason for problems to have arisen is related to the complexity inherent to operating a Hadoop cluster, as well as developing applications. Skilled engineers are pretty scarce and expensive, so companies lacking an engineering/hacking culture had a high failure rate in comparison to technology startups. Before moving to the next section, I would like to put a disclaimer here. There are undeniably companies that have succeeded with Hadoop. They have appropriately set expectations from the very beginning or quickly adjusted as they went through first failures, and they were able to align appropriate engineers and create a kind of Hadoop culture. Slope of Enlightenment I believe that this is the current status of Hadoop. As the trough of disillusionment is being exited, Hadoop enters a maturity phase, where it will be used for what it does best. However, the adoption rate will decline by an order of magnitude, compared to what was initially anticipated. With failure comes a better understanding of the technology and the emergence of best practices. Early adopters have already gone through this process and have successfully tamed their Hadoop deployment. They have acknowledged that Hadoop is not the panacea for big data analytics and that a re-architecture effort has to be undertaken, in light of adjusted expectations, to either supplement Hadoop with other technologies – whether commercial or open source – or- in some extreme cases- replace Hadoop and write off their investment. It is now generally accepted that Hadoop’s sweet spot is as a low-cost, high-volume storage platform with reliable batch processing. It is definitely not suitable for interactive analytics; but, with the advent of optimized data formats (ORC, Parquet) and query engines (Impala, Presto, Dremel…) performance has drastically improved, making it possible to analyze historical data with acceptable performance. Plateau of Productivity Here, companies are reaching a critical milestone in their journey, and they have to assess the realization of their investment in Hadoop in order to decide whether they should pull the plug or capitalize on their learnings to make it work and harness benefits. Since this stage hasn’t yet been entered by mainstream users, I’ll just share emerging best practices that I learned from early adopters, and the trends I’m currently observing amongst the community of practitioners. As mentioned above, Hadoop evolved from distributed data storage and processing, namely HDFS and MapReduce, to a framework comprising multiple capabilities to address specific requirements dictated by the nature of the data and analytics needs. At the same time, data architects started to apply two key design principles: separation of concerns and data tiering (i.e. a mixture between data freshness and its associated value) to ensure distinct alignment between capabilities, requirements, and economics. Here are the outcomes: The first one came in the form of an “aha!” moment as users noticed that SQL and the relational model are actually a pretty powerful way to manipulate data, despite the buzz surrounding NoSQL technologies. It is indeed better to design a thoughtful data model and expose it to users via SQL, rather than a schema-less approach where analytics users will have to try to figure out the structure later. So relational databases – in which users have heavily invested, in terms of effort, infrastructure, and skills – are not going anywhere. The new generation of relational databases, called NewSQL, addresses the speed and scale issues that the previous generation suffered from. Unlike NoSQL, all of the key functionality – such as ACID compliance, relational structures, and comprehensive SQL support – are preserved, making for much better performance. Cloud is gaining traction as it offers a viable alternative to those being challenged by operational issues. All Hadoop capabilities exist on major clouds in the form of fully managed pay-as-you-go services. Being freed from the operational burden, users will now focus on architecting their data platform and experimenting with the vast amount of analytics capabilities to choose the most appropriate ones, as per the aforementioned design principles. Also available as a service on the cloud are full-stack analytics platforms with reintegrated storage and analytics components that only require customization. That being said, heavily-regulated industries won’t make a full jump to the cloud, so it seems that the future is about managing data in a hybrid fashion, both on-premises and on multiple clouds. HDFS is seeing its lunch being eaten by object storage, not only in the cloud, but also on-premises, with technologies such as MinIO, Pure Storage, and Scality. More specifically, AWS S3 is now considered as the de-facto standard for object storage, and vendors are seeking compatibility with that protocol. An S3 emulation layer on top of HDFS could allow Hadoop users to continue to take advantage of their investment. An initial architecture for historical data analytics is becoming a common pattern amongst the early adopters. It consists in dumping all data exhaust on HDFS and batch processing it using Hive or SPARK to structure it, perform a first level of aggregation, and eventually produce ORC or Parquet files. Based on the performance requirements, data volume, and economics, those files are either analyzed in place with query engines, or moved to a first-tier NewSQL database (this is often referred to as ‘Hadoop augmentation’). Although this approach provides interactive analytics capabilities, it is not suitable for use cases with real-time requirements, because data is batch-processed in Hadoop. For real-time analytics, the architecture that was just described should be supplemented with a stream processing flow to transform data as it arrives – from a Kafka queue, for instance. The two processing systems are working in parallel and asynchronously to populate two tables that are eventually merged at query time. This architecture is known as Lambda Architecture. Recently, it has been simplified into the form of a single processing flow that is fast enough to handle both batch and real-time, known as Kappa Architecture. With the rise of HOAP platforms (Hybrid Operational Analytic Processing), this architecture will be further simplified by keeping the operational store in-synch (via CDC) with operational systems in transactional mode, as well as in an append-only mode with a Kafka bus, while performing transformations on the fly and serving complex analytics queries – SQL, on a relational model – to multiple users. As data is aging, it is automatically cooled down in a historical analytics store, and eventually moved to a data lake. In addition to having a policy-based data management capability to move data from one tier to another, these systems also allow for ‘data continuity’; i.e., data in a table can be spread across several tiers, but users only query that single table. Again, those are my own observations and recommendations. Hadoop has evolved from a technology platform to a ‘way of doing things,’ leveraging building blocks in the data storage and manipulation space. Its adoption will continue, but at a slower rate, and with more realistic expectations as to capabilities, cost, and business outcome. If you have another view on how Hadoop will reach the plateau of productivity, or you would like to add other design patterns for data architectures to the discussion, please feel free to share in the comments.", "date": "2020-08-06"},
{"website": "Single-Store", "title": "reverse-engineer-chicago-mercantile-exchange", "author": ["Dendi Suhubdy"], "link": "https://www.singlestore.com/blog/reverse-engineer-chicago-mercantile-exchange/", "abstract": "The exchange is at the heart of our economic system. From the Rialto in Venice (fourteenth century), the Grand Bazaar in Turkey (seventeenth century), the Amsterdam Bourse in Holland (seventeenth century) and the New York Stock Exchange (twentieth century), markets have purposely been built to be a place for buyers and sellers of goods and services to meet and transact. In this blog post, we explain how we built an exchange, with SingleStore at the core. After the invention of computers, and the widespread adoption of the Internet, the marketplace went online. The buying and selling that used to take the spice trader in the Dutch East Indies – now modern-day Indonesia – about a month, with price matching provided by the Amsterdam Bourse, now only takes a fraction of a second using automated computerized trading systems; a better price discovery method for the buyers and sellers had evolved. Building such a centralized, computerized trading platform is not an easy task. We thought that since the matching engine is at the core of many economic activities, there must be a step-by-step tutorial to build a trading system from scratch available somewhere on the Internet. Our assumption turned out to be completely wrong. Building an exchange requires knowledge spanning from multiple distinct fields, such as game theory (mathematics), algorithmic and data structure (computer science), and market microstructure (finance). Inspired by Cinnobar (now part of Nasdaq), and LSE, which operates the Oslo Bourse exchange, our team decided to rebuild the foundations from the ground up: we set our sights on building a financial exchange from scratch. The tutorials that you do find on the web are geared towards high-frequency market making, or proprietary buyers and sellers on a financial exchange. In financial lingo, the tutorials are for the buy side/sell side, and not on the exchange side – how you listen to price movements, make a decision, and send an automated order (either to buy or sell a financial instrument) to the exchange. We never found any information regarding how you would receive orders from clients, match them, and conduct proper clearing mechanisms after a trade has happened – that is, the clearing process. Also, after a trade happens, how do you make sure each market participant listens correctly to the price changes that have happened during the trade – that is, the price data feed. We call this the centralized exchange problem and, truth be told, this has been solved efficiently by large financial groups such as the National Association of Securities Dealers Automated Quotations Exchange (NASDAQ), the New York Stock Exchange (NYSE), and the Chicago Mercantile Exchange (CME) group. One particular implementation of a financial exchange intrigued us: the CME. The reason the CME is different is because the platform not only has standardized spot trading, but it also has standardized options, futures, and spread markets with implied orders functionality. Many technology groups have solved the spot market pretty easily. You might have heard about several cryptocurrency exchanges appearing around the globe, such as BitMEX, Binance, Bitfinex and Coinbase. You can easily trade, using leverage in the range of 1-100x, several financial products such as perpetual inverse swaps (which are basically spot on leverage) and/or directly buying/selling cryptocurrencies, such as Bitcoin and Ethereum. Building A Spot Exchange Before  explaining our solution, let’s cover how a typical spot exchange operates. You have multiple clients that send a sequence of orders, each with a quantity and price. The order then goes to a journey as follows: Receive order in gateway, timestamp the order Send the order to the risk management system; calculate if client has enough balance If they do, send the order to the matching engine The matching engine then spits out events such as trade event, order event, depth event The events then gets passed to several other parts of the exchange The market data feed engine takes the trades and depth and blasts it to the client The spot clearing system takes both sides of trades and debits and credits the account of each trader. (This sequence will also determine the round trip time (RTT) of an exchange; more on that later.) Now here, bear in mind that the clearing system at the end takes the counter-party position of the client’s trade and delivers the financial instrument. Say a prop trader buys 3000 shares of Goldman Sachs ($GS) @ $207.36 (in US dollars), and another hedge fund in New York sells 3000 shares of $GS @ $207.36. The clearing system will then debit $622,080 from the traders account and credits the trader with 3000 $GS shares – and also vice versa, credit $622,080 from the hedge fund and debits 3000 $GS shares. The spot exchange clearing engine solves the centralized clearing problem; e.g the clearing engine becomes the seller for every buyer and the buyer for every seller. Thus making sure market integrity exists and ensuring trades happen. If you add leverage to the equation, this becomes a little bit more complicated. Here, for most equity centralized exchanges, they do not lend money to individual/institutional traders. To ensure its clearing objective, the clearing house would need to ensure the brokerage house that lends the trader could always pay the order value sent by the buyer. This is also true for a leveraged seller (short seller). Now imagine if you don’t have intermediaries like brokerage houses. For a leveraged trade to happen, now the clearing house is exposed to credit default risk of the leveraged buyer and or short seller. During each tick, the clearing house needs to listen for price changes and compute the leverage buyer’s/seller’s position and maintain that. In the event of a drastic price change, the centralized clearing house should take over the position and liquidate it on the open market. The level in which a leveraged buyer/seller gets liquidated is called the liquidation price. Building a Derivatives Exchange Now, imagine this matching system but for derivatives. A derivative is a financial contract that is tied to an underlying price. For example an option on Goldman Sachs ($GS) equity is a right to purchase (or sell) Goldman Sachs ($GS) at a specific price predetermined now (called the strike price) at a later date (for example in a month). The main difference between a spot financial exchange and a derivative financial exchange is its ability to conduct mark-to-market computation of all positions of all traders in the exchange. For a computer scientist, this is a simple matrix computation problem, but imagine, for every single tick of market data. Revisiting the spot market order matching process, process 1-7 looks similar but with few extra steps. Receive order in gateway, timestamp the order Send the order to the risk management system, calculate if client has enough balance If they do, send the order to the matching engine The matching engine, then spits events such as trade event, order event, depth event The events then gets passed to several other parts of the exchange The market data feed engine takes the trades, and depth and blasts it to the client The derivative clearing system does a couple of things Take the latest trade, compute volatility projections, price bands (moving average) and several other metrics and recompute every single value of the portfolio of each trader on the exchange The traders that have positions that are too risky or are not contained within the respected margin balance, will get liquidated. On liquidation, the position will be taken care of the independent clearing house, and be sold immediately. On delivery date, the derivative clearing house will make sure the seller of the contract delivers the financial contract which depends on a cash/physical settlement. Now algorithmically this becomes one step harder. To have step number 7 be efficient, one could utilize parallel computation methods such as GPGPU computing. Building Derivatives Pipeline for Implied Markets If you have traded on the Chicago Mercantile Exchange’s Globex system, you might as well admire the beauty of how the whole architecture works. Especially around a feature called “implied markets” (for more on implied markets see https://www.cmegroup.com/education/videos/implied-markets-video.html ). According to the CME’s confluence website ( https://www.cmegroup.com/confluence/display/EPICSANDBOX/Implied+Orders ) there are several rules of a quantity of an implied order sent to the market. These are: Implication requires at minimum two orders in related markets in the proper combination. Implied bids do not trade against implied offers. Implied bids can exist at the same or inverted price levels with implied offers. When this occurs, CME Globex ceases dissemination of the implied bid; however, the bid is still calculated and can be traded against. Implied OUT quantity will not disseminate when the leg value of the included spread is in a ratio greater than one. The price and quantity are still calculated and can be traded against. Implied quantity in futures markets does not have time priority. Implied quantity is not available: When the market is not in a matching state (e.g. Pre-Open) When implied calculation has been suspended On an implied market there are two different calendar futures, say since right now we are in July there would be the N20 (July 2020) and Q20 (August 2020) futures delivery dates. You will then have another implied market which is the delta differential between the two order books N20 and Q20 which is called the N-Q20 calendar spreads. To solve the implied market problem, one would need three instances of the matching engine that are asynchronous and event driven that listens to not only incoming orders from the risk checking system but also makes sure that the calendar spread order book is updated too. Source: https://www.cmegroup.com/education/videos/implied-markets-video.html Bitwyre’s Solution to the Implied Markets Problem It took us a while to realize that a financial exchange system is an asynchronous event-driven system and to build one, one should keep in mind doing things asynchronously is different from doing things synchronously. We decided on building an algorithmic-controlled order passing system to the matching engines. This algorithmic microservice is called the Credit Risk Account Management (CRAM) service. Internally our engineers call it “The Oracle”, for the sake of the joke :). Receive order in gateway, timestamp the order Send the order to the risk management system, calculate if client has enough balance If they do, the Oracle does a check If it’s a normal non-implied market order, send it to it’s respected matching engine If it’s tied to an implied market say a calendar future and a calendar spread, send the order asynchronously one to the calendar future matching engine and one to the calendar spread matching engine. The matching engine, then spits events such as trade event, order event, depth event The events then gets passed to several other parts of the exchange The market data feed engine takes the trades, and depth and blasts it to the client The derivative clearing system does a couple of things Take the latest trade, compute volatility projections, price bands (moving average) and several other metrics and recompute every single value of the portfolio of each trader on the exchange The traders that have positions that are too risky or are not contained within the respected margin balance, will get liquidated. On liquidation, the position will be taken care of the independent clearing house, and be sold immediately. On delivery date, the derivative clearing house will make sure the seller of the contract delivers the financial contract which depends on a cash/physical settlement. Somehow if you try to look at CME’s solution to this problem, we accidentally came with the same thing without seeing their implementation. Below is the Globex architecture you will see the separations of each functionalities such as Order Entry, Drop Copy, Credit Control and Risk Management, Matching Pricing and Market Integrity, Market Data and Clearing. Each which represents the algorithmic process of 1-8 that I have mentioned above. Be advised that since CME does not directly handle end customers’ trades(although they do via brokerage houses), the clearing system is not as computational heavy as if they did. Source: Trading Services. Confluence page the CME group ( https://www.cmegroup.com/confluence/display/EPICSANDBOX/Trading+Services ) Our core value proposition of building Bitwyre are the following: currently there exist no market in the world where an institution or a retail speculator/hedger would be able to liquidate or purchase large holdings of cryptocurrencies rapidly. To solve that problem we built several solutions 1. We built markets where volumes of trades are hidden from all market participants. When a trader liquidates his/her holdings in the darkpools, there would be minimal slippage. 2. As an institutional trader like Jim Simon’s Renaissance Technologies/George Soros’s Quantum fund/Paul Tudor Jones fund, might be able to have exposure to cryptocurrency price movements  without having the risk of actually holding the coin which does bear the custodian risk e.g being hacked. Our cash settlement system allows us to use the Bitcoin Real Time Index (BRTI) from the CME and or a spot bitcoin index that we compute internally to swap the cash differences at the delivery date. 3. For professional market makers we also offer colocation services so that they can have predictable and fast execution service for their proprietary market making algorithms. During the author’s discussion with several spot crypto currency exchange founders, they mentioned that aggregate throughput is a significant problem in current exchange pipelines. Building a simple exchange with Python or PHP might take you to 100-1000 orders/second, but it will not allow you to ingest 100,000-1,000,000 orders/second as is common with professional exchanges like the CME/NASDAQ/NYSE. During the last 40+ years these centralized exchanges have invested heavily in FPGA technology that allow them to have a very efficient low-latency high throughput pipeline. At Bitwyre we try to take a different approach. With the lack of crypto exchanges that take on being an execution venue for institutional investors, we decided to specialize in that route. That also means we will be able to have non-lit/dark markets, and colocation services. Technology-wise, we started our codebase with C++, conducting operations all in-memory with SingleStore as our database and a Kafka-compatible streaming platform Redpanda, while slowly utilizing exciting new unikernel technology such as IncludeOS and HermitCore, we can try to lower the latency for pipeline processing and hopefully later we can optimize it to a FPGA pipeline. One more important point of our architecture is that every computation is done in memory, which speeds the computation compared to saving every operation on disk. For that purpose the core database system that we use is SingleStore. There have been several in-memory databases out there geared for financial services but we decided that SingleStore is the best for our application as from my experience Redis has problems with multithreaded asynchronous access needed in low-latency processing applications like us. We also have tried KDB+ but we didn’t like the semantics of programming language to conduct the time series operations. We also understand less of the inner workings of Kdb+ since it’s closed source and has a high price tag for a small startup like us. As with Redpanda as a drop in replacement for Kafka, we prefer SingleStore to kdb+ since it’s a drop in replacement to MariaDB/MySQL. By doing the SET/GET, HSET/HGET, LPUSH/RPUSH operations with JSON schemas to Redis, the conversion to SQL was not as painful as we thought. The CTO, our lead quant and I, ported our Redis operations to SQL operations in 2 weeks (we are talking about a codebase of 200 thousand lines of code, spanning in 100 microservices, where each have 5-10 Redis operations per microservice). Once we succeeded in writing the SQL, we tested it out with MariaDB and we then dropped in the SingleStore database on our compute clusters as a replacement for MariaDB once we made sure extensive testing was done. Another very interesting feature of the CME Globex exchange such as Velocity Logic, Circuit Breakers are not implemented on Bitwyre because of the nature of cryptocurrency derivatives contracts that trade on our platform. Crypto derivatives trade 24/7 compared to trading on bitcoin futures and bitcoin options on the CME. If we implement Velocity Logic and Circuit Breakers, the inherent volatile nature of Bitcoin would probably make us hit the circuit breakers once every 2 months. Other features such as Market and Stop Order Protection and Self-Match Prevention is an integral aspect to ensure market integrity on our platform. We implement both on the CRAM/Oracle. The reason is that despite being in Asia where markets are not as regulated as in the United States, we still want to ensure fair market participation and make sure we filter out wash trades from our system. Also unlike the CME, we are a derivatives exchange, a clearing house, and brokerage house all together, centralizing most risks for the cryptocurrency trader. Our security protocols for custodian operations exceeds current implemented custodian operations done by centralized equity market exchanges. For example the need for multisignature operations, warm, and hot wallet transfers. In current equity exchanges, the custodian operations are conducted by an independent facility. The delivery of say 1 US barrel of WTI Crude Oil from the seller of the futures contract to the buyer of the futures contract happens electronically between two brokerage houses that conduct the buying/selling on behalf of the seller/buyer of the futures contracts. This settlement process is conducted by a derivatives clearing house. Seldom this process fails since the process only consists of two parts swapping balance and mark-to-market positions. In very rare circumstances can they fail. As a bearer instrument, the owner of the private key of a cryptocurrency bears the ability to move them around. Infamous cryptocurrency exchange hacks happen seldom because the hacker manipulated trading records, but most of the time happens on a hot wallet breach e.g when a hacker gets their hands on the private keys of the exchanges hot wallets and transfers them to a crypto address that they control. To mitigate these risks, several crypto currency exchanges have offloaded the risk by introducing institutional custodianship of cryptocurrencies. Others, like us, for example, conduct separation of wallet functionalities into withdrawal wallet (only allowed for transferring out), deposit wallet (only for transferring in), warm wallet (transition of funds in between) and cold wallet storage. If you’re interested to learn more about building derivatives exchanges for fun, don’t hesitate to ping us! Don’t forget to subscribe to our alpha testnet launch at https://bitwyre.com. —— Special thanks to Domenic Ravita for allowing us to write this blog. Thanks to Joshua Levine (who invented INSTINET/Island of NASDAQ, long live the SOES Bandits!!!!!!!!!!!!) for his inspiration for the Bitwyre team. To Aditya Kresna, our Chief Research and Development (CR&D) who discussed with the author on the implementation of this design system and to Yefta Sutanto, our Chief Technology Officer, whom with patience and dedication has deployed our engineering solution. Acknowledgements also goes to Aditya Suseno our CFO and John Schwall who have been inspiring us during the journey building this open-source financial exchange infrastructure. Shout out to Muhammad Augi, Sudhanshu Jashoria, Gaurav Jawla, Gandharv Sharma, Veronika Vřešťálová, Genevieve Brechel, Gary Basin, Erik Rigtorp, Matt Godbolt, Zachary David, Ezra Rapoport, David Senouf, Eric Charvillat, David Amara, Stefan Cheplick, Byrne Hobart, Remco Lenterman, Themis Sal, Per Buer, Alexander Gallego, Alexy Khrabrov for inspiring us during the design of our electronic exchange network. Also MILA friends, Sai Krishna G.V, Alex Fedorov, and Florian Golemo for their discussion on the order routing problem using deep reinforcement learning. Last but not least to my former supervisor who always supports me to build awesome cool things Prof Yoshua Bengio. Reference CME Circuit breakers https://www.cmegroup.com/education/videos/cme-globex-circuit-breakers.html https://www.cmegroup.com/education/videos/dynamic-circuit-breakers-market-integrity-control.html CME Protection for Market and Stop Orders https://www.cmegroup.com/education/protection-functionality-for-market-and-stop-orders.html CME Globex Timestamp https://www.cmegroup.com/education/videos/cme-globex-timestamps.html Bitcoin Paper https://bitcoin.org/bitcoin.pdf Derivatives Clearing house default problem https://www.nytimes.com/2019/05/03/business/central-counterparties-financial-meltdown.html ISDA centralized clearing http://online.wsj.com/public/resources/documents/ISDApaper05232011.pdf", "date": "2020-08-10"},
{"website": "Single-Store", "title": "memsql-shortlisted-as-a-finalist-in-the-2020-saas-awards", "author": ["Gina von Esmarch"], "link": "https://www.singlestore.com/blog/memsql-shortlisted-as-a-finalist-in-the-2020-saas-awards/", "abstract": "The following announcement is being released by SingleStore today on the news that SingleStore is being shortlisted as a finalist in two categories for the 2020 SaaS Awards: Best SaaS for Business Intelligence or Analytics, and Best SaaS Innovation in the Internet of Things. SingleStore Shortlisted as a Finalist in the 2020 SaaS Awards International Software Awards Program Recognizes SingleStore Managed Service for Best SaaS for Business Intelligence or Analytics and Best SaaS Innovation in the Internet of Things SAN FRANCISCO – AUGUST 10, 2020 – SingleStore , The Database of Now TM for operational analytics and cloud-native applications, is a finalist in the 2020 Software as a Service Awards program in two categories: Best SaaS for Business Intelligence or Analytics, and Best SaaS Innovation in the Internet of Things. Hundreds of organizations applied for these awards, with entries coming from North America, Europe, the U.K., the Middle East, and Australia. SaaS Awards winners will be announced on August 25, 2020. SingleStore was named as a finalist for its unique combination of the world’s fastest in-memory rowstore, world’s fastest disk-based columnstore, predictive low-latency analytic query capability, and MySQL compatibility, offered in a single cloud database, SingleStore Managed Service. SingleStore calls its blend of multiple datatypes SingleStore Universal Storage TM . Universal Storage can eliminate the need for separate databases and separate tables for transactions, analytics, and caching. SingleStore is able to ingest and process millions of real-time events per second, while simultaneously serving thousands of highly-concurrent requests from API calls, and queries from real-time BI dashboards and machine learning models, all within milliseconds. “We are honored to be named a finalist in the 2020 SaaS Awards,” said Raj Verma, co-CEO of SingleStore. “SingleStore Managed Service delivers instant, effortless access to the world’s fastest, most scalable data platform for operational analytics, machine learning, and AI. Growth in the database market is moving towards the cloud. To survive in the era of COVID-19, successful businesses are pivoting towards immediate digital transformation. Those businesses that had already begun are accelerating their efforts. SingleStore Managed Service brings real-time decision-making to a new level, and unlike our competitors, we are built for this very use case, and offer the lowest latency in the marketplace.” James Williams, head of operations for the SaaS Awards, said: “Software-as-a-service’s force for positive disruption never seems to abate, with seemingly unending solutions for modern business. This year is a special case. SaaS is not only at the forefront of remodeling existing business processes; it’s also helping organizations to use pioneering solutions to respond to unavoidable global disruptions. Indeed, SaaS technologies are now celebrated as providing new and inventive ways for organizations to perform complex tasks in a changing international landscape. From fulfilling orders, to arranging meetings, to delivering new business services, SaaS technologies are more important than ever.” Williams added that the program donated entry fees from shortlisted entrants in this year’s Communication, Collaboration and Conferencing category to the World Health Organization’s COVID-19 Response Fund. “Together, we have raised $3,555 for the fund, an achievement we hope underscores the vital need for sustained organizational innovation across all sectors,” he said. To view the full shortlist, please visit: https://www.cloud-awards.com/2020-software-awards-shortlist/ About SingleStore SingleStore is The Database of Now TM , powering modern applications and analytical systems with a cloud-native, massively scalable architecture. SingleStore delivers maximum ingest, accelerated transaction processing, and blisteringly fast query performance, including AI integration and machine learning models, all at the highest concurrency. Global enterprises use the SingleStore distributed database to easily ingest, process, analyze, and act on data, to thrive in today’s insight-driven economy. SingleStore is optimized to run on any public cloud, or on-premises with commodity hardware. Visit www.singlestore.com or follow us @SingleStoreDB. About the SaaS Awards The SaaS Awards is a sister program to the Cloud Awards, which was founded in 2011. The SaaS Awards focuses on recognizing excellence and innovation in software solutions. Categories range from Best Enterprise-Level SaaS to Best UX or UI Design in a SaaS Product. About the Cloud Awards The Cloud Awards is an international program which has been recognizing and honoring industry leaders, innovators, and organizational transformation in cloud computing since 2011. The awards are open to large, small, established, and start-up organizations from across the entire globe, with an aim to find and celebrate the pioneers who will shape the future of the Cloud as we move into 2021 and beyond. Categories include the Software as a Service award, Most Promising Start-Up, and “Best in Mobile” Cloud Solution. Finalists are selected by a judging panel of international industry experts. For more information about the Cloud Awards and SaaS Awards, please visit https://www.cloud-awards.com/ Media Contact Gina von Esmarch 415-203-4660 gvon@singlestore.com", "date": "2020-08-10"},
{"website": "Single-Store", "title": "iot-analytics-infiswift", "author": ["Floyd Smith"], "link": "https://www.singlestore.com/blog/iot-analytics-infiswift/", "abstract": "Infiswift uses AI to help meet real-world challenges, largely through Internet of Things (IoT) deployments. They optimize the operation of physical devices using connectivity and data. The company’s innovative platform makes it easy to connect and manage any number of endpoints at scale, with security, and to build solutions that open new services or improve existing ones. Infiswift empowers its customers to be data-driven in industries such as renewable energy, agriculture, and manufacturing. Infiswift has chosen SingleStore as the real-time insights engine of its platform to deliver fast, reliable analytics, and to power constantly updated machine learning (ML) models. We’ve updated this blog post with fresh insights from our recent interview with Infiswift CTO Jay Srinivasan . The Impact of Device Insights The Infiswift platform reliably and securely ingests data from any endpoint, combining scalability and simplicity, a combination which most IoT environments struggle to deliver. The platform delivers real-time insights across streams of data as they enter the system. Operators use the data to improve equipment reliability and optimize costs. The platform uses the MQTT protocol at the core of a sophisticated publish-subscribe mechanism, including multi-level, hierarchical security protection, down to the level of an individual data point. A key ingredient to the analytics system is the need for an adaptable database platform that can manage fast ingestion of intermittent data. Infiswift has found the platform they need: SingleStore. Real-Time and Historical Data for IoT Systems IoT systems access millions of devices that generate large amounts of streaming data. For some equipment, a single event may prove critical to understanding and responding to the health of the machine in real time, increasing the importance of accurate, reliable data. While real-time data remains important, storing and analyzing the historical data also unveils opportunities for new optimizations and operating techniques. The combination of both real-time and historical data provides the most complete view for the IoT platform. Database Requirements for IoT Business demands put extra pressure on the Infiswift platform to process data quickly, including device authentication and state. Equipment messages come at intervals of seconds or subseconds, driving the requirement for a high-throughput, memory-optimized database. The engineering team needed a database to rapidly ingest changing data, while also persisting to disk for long-term analysis. Because of the large data volumes, the database needs to optimize disk resources by leveraging columnstore data compression. “We need the data to be processed extremely rapidly, basically in real time. We chose SingleStore because we needed a high-throughput database that can handle our data volumes and velocity with the reliability our customers require.” – Jay Srinivasan SingleStore Advantages The Infiswift team needed a database platform that could handle the scale of ingesting and analyzing millions of events in real time. After several technology evaluations, the team chose SingleStore. “It’s a really well-thought-out product; that’s why I love it.” Rowstore and Columnstore for Optimal Performance The unique combination of an in-memory rowstore engine with a memory and disk-based columnstore made for efficient processing and a simplified architecture. “We wanted to store lots of sensor data, so obviously that will be disk-based storage. But at the same time, we wanted the platform to be fast for real-time data distribution, which means in-memory.” JSON Support SingleStore was an early leader in making JSON data a first-class data type within a relational database, making it fast and easy to run SQL commands against semi-structured JSON data. JSON data is fundamental to IoT, so this capability is critical to Infiswift. Scalable Transactions The application requires transactional consistency to deliver highly accurate updates for constantly changing sensor events. Infiswift could not get what it needed from NoSQL-based solutions ; the speed and compatibility of SQL were vital to them. “We need a platform that can support these high-energy, high-frequency transactions… we take only 24ms. to complete the round trip.” Relational SQL The Infiswift engineering team has an investment in ANSI SQL, along with MySQL, for app development. SingleStore ensures existing technology commitments could remain intact. In particular, SingleStore is compatible with MySQL wire protocol, making it easy to use SingleStore as a drop-in replacement. “At the end of my time at Google, we were moving to Google Spanner, which is actually a SQL-based interface.” Compression The large amount of data collected required sophisticated compression to maximize server resources. SingleStore provides compression of roughly 7x for the application. AI/ML Integration AI is a key part of Infiswift’s value proposition, and SingleStore provides fast, simultaneous access to both new, streaming data and historical data, as needed for AI. In particular, Infiswift needs to update machine learning models, running within a Spark framework, in real time – and SingleStore readily supports Spark integration . “For our machine learning models… in the training phase, our workload runs directly against the same SingleStore cluster that also provides us analytics support and 7x compression for columnstore data.” Licensing and Deployment Flexibility SingleStore offers a free edition that allows innovative, fast-moving startups like Infiswift to build and scale. Infiswift is able to flexibly move from SingleStore on-premises, SingleStore in the cloud, and MySQL deployments for small accounts. “SingleStore has the ability to be completely hostable on-premises for us. It’s not just a matter of moving across clouds; it’s one of our most important requirements, that we also be able to host completely on-prem. SingleStore is hostable anywhere we want.” Conclusion Infiswift was looking to meet four key requirements, not expecting to find a single product that could offer all of them: 1. an in-memory rowstore, 2. on-disk columnstore with compression, 3. SQL support, and 4. availability both on-premises and in the cloud. Not only has SingleStore met all these requirements, with excellent performance; it has also provided two additional capabilities that proved to be important: support for JSON, which is used my nearly all modern IoT devices; and MySQL wire protocol compatibility, which makes SingleStore a drop-in replacement for MySQL, without changing application code. Building IoT applications involves a balance of technology to enable the scale, security, and performance for operational requirements. The Infiswift platform eliminates complexity and delivers on the promise of a high-performance IoT system that can efficiently enable data-driven operations. Now, with SingleStore, each application built on the platform has the reliability and performance that customers require to drive transformational analytics, machine learning, and AI for their operations. To learn more about SingleStore for IoT applications, try SingleStore for free or contact SingleStore . IoT Solution Overview SingleStore for Energy Applications Sample demo application for monitoring wind turbines", "date": "2020-08-19"},
{"website": "Single-Store", "title": "digital-twins-important-step-data-economy", "author": ["Gina von Esmarch"], "link": "https://www.singlestore.com/blog/digital-twins-important-step-data-economy/", "abstract": "In this Forbes article , Nikita Shamgunov, SingleStore co-founder & co-CEO, discusses the importance of digital twins. While digital twins are frequently associated with industrial and heavy machinery use cases, a digital twin can be built for any business or product. Digital twins can also be found in consumer electronics and financial services. Nikita explains that for a digital twin to work, a business must create a live mirror, or clone, of all the data that is flowing through the veins of the enterprise. The organization must consolidate that data and implement a single pane of glass across all data services, providing a complete view of the business. This, he adds, requires the business to embrace modern data management, which creates the foundation for these next-generation capabilities. Successfully creating and maintaining a digital twin, however, requires real-time data access and scalable data management, which SingleStore can help to provide. You can try SingleStore for free today or contact us .", "date": "2020-08-12"},
{"website": "Single-Store", "title": "memsql-powers-the-rapid-rise-of-thentia-regulatory-technology", "author": ["Gina von Esmarch"], "link": "https://www.singlestore.com/blog/memsql-powers-the-rapid-rise-of-thentia-regulatory-technology/", "abstract": "The following announcement is being released by SingleStore today to announce the adoption of SingleStore as the core operational database for cloud-based regulatory licensing, assurance, and enforcement technology from Thentia, a global leader in regulatory software. SingleStore Powers the Rapid Rise of Thentia Regulatory Technology As More Regulators Seek Digital Tools Amid COVID-19, There’s a Greater Need for Speed, Scale SAN FRANCISCO – AUGUST 25, 2020 – SingleStore, The Database of Now TM for operational analytics and cloud-native applications, announced today that Thentia, a global leader in regulatory software, is launching SingleStore as the core operational database for its cloud-based regulatory licensing, assurance, and enforcement technology. SingleStore’s speed and scale will enable Thentia to achieve fast response times and automate a historically paper-based regulatory compliance process. At a time when scamming is on the rise , the solution helps to bring greater confidence and responsiveness to health and safety systems. Thentia offers cloud-based applications that independent and government regulators in sectors such as healthcare use to manage tasks digitally and securely. Demand for Thentia’s technology has exploded amid the pandemic, as a growing number of regulators look for modern, digital solutions that enable them to work efficiently, remotely, and safely to protect public health. “We’re seeing incredible growth at Thentia, and our relationship with SingleStore enables us to support and build upon that growth,” said Julian Cardarelli, CEO of Thentia. “With SingleStore, we can process billions of events and do real-time analytics, providing our customers a level of analytical power they have never before experienced.” Harry Cayton CBE, a world-renowned expert on regulatory practices and a recent addition to the Thentia board of directors , explained, “This is important now, because it enables regulators to do more with less at a moment in which COVID-19 makes work especially challenging and many governments are streamlining operations, looking to work smarter.” “Data is more important than ever, especially for individuals and organizations that are charged with protecting public safety,” said SingleStore co-CEO Raj Verma. “We are grateful to have the opportunity to work with Thentia to enable regulators, and all the people they serve, to benefit from the power of real-time data delivered over a scalable and secure architecture.” SingleStore makes it possible for Thentia to build a wealth of reports for its customers, who can pull these reports to get detailed, in-the-moment information about such metrics as complaint increases or decreases, complaint handling times, and number of licensees. With SingleStore, Thentia also will be able to leverage machine learning and predictive algorithms to generate insights. This is far more effective than the alternative, attempting to gain insights from Excel documents, which are less engaging for users, and which become outdated almost immediately. About Thentia Based in Toronto, Canada, Thentia is an industry leader in using proprietary technology to help regulatory bodies efficiently fulfill their regulatory obligations. Thentia services a wide variety of clients throughout Canada and the United States using cutting-edge software and industry-leading expertise in regulatory standards. About SingleStore SingleStore is The Database of Now TM , powering modern applications and analytical systems with a cloud-native, massively scalable architecture. SingleStore delivers maximum ingest, accelerated transaction processing, and blisteringly fast query performance, including AI integration and machine learning models, all at the highest concurrency. Global enterprises use the SingleStore distributed database to easily ingest, process, analyze, and act on data, to thrive in today’s insight-driven economy. SingleStore is optimized to run on any public cloud or on-premises with commodity hardware. Visit www.singlestore.com or follow us @SingleStoreDB. Contact Gina von Esmarch 415-203-4660 gvon@singlestore.com", "date": "2020-08-25"},
{"website": "Single-Store", "title": "memsql-named-2020-saas-awards-winner", "author": ["Gina von Esmarch"], "link": "https://www.singlestore.com/blog/memsql-named-2020-saas-awards-winner/", "abstract": "The following announcement is being released by SingleStore today on the news that SingleStore is a winner in the 2020 SaaS Awards, for Best SaaS Innovation in the Internet of Things. SingleStore Named 2020 SaaS Awards Winner Global Software Awards Program Announces Final Winners San Francisco, CA – 25 August 2020 – SingleStore, The Database of Now™ for operational analytics and cloud-native applications, is a winner in the 2020 SaaS Awards Program in the category of The Best SaaS innovation for The Internet of Things . The SaaS Awards celebrate excellence in software and accept entries from across the world, including the US, Canada, Australasia, the UK, and EMEA. Categories for the 2020 awards program also include Best Enterprise-Level SaaS Product, Best UX or UI Design in a SaaS Product, and Best Security Innovation in a SaaS Product. New categories include Bespoke SaaS Solution of the Year and Best SaaS for Digital Marketing. “We are extremely proud to be recognized as a winner in the 2020 SaaS Awards, which demonstrates our commitment to innovation, customer satisfaction, and tangible results,” said Nikita Shamgunov, co-CEO and founder of SingleStore. “IoT analytics requires streaming data ingestion in real-time with low latency queries in the cloud. SingleStore Managed Service uniquely provides this, as the world’s first cloud operational database for modern business intelligence and operational analytics. We are steadfast in our support of fast ingest and concurrent analytics for always-on devices and sensor systems, delivering instant, actionable insights.” James Williams, Head of Operations for the SaaS Awards, said: “This year’s swathe of remarkable submissions has been delivered in an atmosphere of seismic disruption across all industries. The final winners all represent something especially creative, intelligent, or simply successful, which often offer completely transformative solutions to modern business needs.” Hundreds of organizations entered the 2020 awards, with international entries coming from North America, Canada, Australia, the UK, Europe, and the Middle East. To view the shortlist and list of winners, please visit: https://www.cloud-awards.com/2020-software-awards-shortlist/ A sister program to the SaaS Awards, The Cloud Awards ( https://www.cloud-awards.com/ ), will soon be accepting submissions for a new 2020-21 program, continuing its recognition of excellence in cloud computing, with a deadline of 23 October. Contact details Gina von Esmarch VP Marketing & Communications 415-203-4660 gvon@singlestore.com For the SaaS Awards James Williams Head of Operations https://www.cloud-awards.com/software-as-a-service-awards/ james@cloud-awards.com About the SaaS Awards The SaaS Awards is a sister program to the Cloud Awards, which was founded in 2011. The SaaS Awards focuses on recognizing excellence and innovation in software solutions. About the Cloud Awards The Cloud Awards is an international program which has been recognizing and honoring industry leaders, innovators, and organizational transformation in cloud computing since 2011. The awards are open to large, small, established, and start-up organizations from across the entire globe, with an aim to find and celebrate the pioneers who will shape the future of the Cloud. Categories include the Software as a Service award, Most Promising Start-Up, and “Best in Mobile” Cloud Solution. Finalists are selected by a judging panel of international industry experts. For more information about the Cloud Awards and the SaaS Awards, please visit https://www.cloud-awards.com/ . About SingleStore SingleStore is The Database of Now TM , powering modern applications and analytical systems with a cloud-native, massively scalable architecture. SingleStore delivers maximum ingest, accelerated transaction processing, and blisteringly fast query performance, including AI integration and machine learning models, all at the highest concurrency. Global enterprises use the SingleStore distributed database to easily ingest, process, analyze, and act on data, to thrive in today’s insight-driven economy. SingleStore is optimized to run on any public cloud, or on-premises with commodity hardware. Visit www.singlestore.com or follow us @SingleStoreDB.", "date": "2020-08-25"},
{"website": "Single-Store", "title": "spin-up-a-memsql-cluster-on-windows-in-20-minutes", "author": ["Rob Richardson"], "link": "https://www.singlestore.com/blog/spin-up-a-memsql-cluster-on-windows-in-20-minutes/", "abstract": "You can run SingleStore software effectively on Windows, using the Windows Subsystem for Linux Version 2 (WSL 2). This is a development cluster running on a single host, and not suitable for production deployments – but fully capable for many development and demo tasks. You can also run SingleStore Managed Service for free or spin up a SingleStore cluster on Linux , on Docker Desktop , on Kubernetes , or on Vagrant , all in roughly 10 minutes. Read on to see how you can run SingleStore on WSL on Windows. SingleStore is a Linux program exclusively, so it doesn’t run directly on other operating systems. However, Windows features WSL 2, which hosts a Linux kernel in a lightweight Virtual Machine (VM), allowing you to run any Linux program on Windows. WSL is well-regarded for its convenience and efficiency. We’ll use WSL Version 2 to run SingleStore on Windows. The other blog posts in this series describe the process as being “in 10 minutes.” And, in this case, you can spin up the SingleStore cluster on WSL 2 in about 10 minutes. The trouble is, if you don’t already have WSL 2 in place, it also takes roughly another 10 minutes to get WSL 2 installed and updated. So we’ll say “in 20 minutes” for this one. If you’re not all that familiar with Linux, you’ll find installing a Linux program to be much different than installing a Windows program. In Windows, we often launch an MSI, click Next a dozen times, and the program is installed. Linux administration is primarily done through the command line, with input coming from a terminal program – so we’ll use a lot of terminal commands. This might be scary on first look, but we’ll walk through this carefully to ensure you can get it running smoothly. To run SingleStore, you’ll need a machine with at least four cores and 4GB of RAM. On Windows, you can verify your system’s specs by right-clicking on the task bar at the bottom, choosing Task Manager, and switching to the Performance tab. Look there to make sure that your machine meets the minimum requirements. Part I: Get a Free SingleStore License SingleStore provides free licenses that allow you to run a four-node cluster, with community support, for as long as you’d like. We at SingleStore have many users who are getting a great deal of utility from their free licenses. You can, at any time, upgrade to an Enterprise license, which has a licensing cost attached. The Enterprise license gives you committed support resources, with a service level agreement (SLA) and the ability to run more nodes. You can contact SingleStore for more information. To get a free license for SingleStore: Register at singlestore.com/free-software/ Click the link in the confirmation email. Go to the customer portal and login. Click “Licenses” and you’ll see your license for running SingleStore for free. This license never expires, and is good for clusters up to four nodes and up to 128GB of combined RAM. This is not the license you’ll want for a production cluster, but it’s great for these “kick the tires” scenarios. Note this license key. We’ll need to paste it into place later in these instructions. Part II: Install WSL 2 and Start Linux Windows Subsystem for Linux Version 2 (WSL 2) debuted with Windows 10 update 2004 (May 2020 Update), and can install on any version of Windows 10, including Windows 10 Home. Though you could adapt the steps below to WSL version 1, your mileage may vary, and we have only tested these steps for WSL 2. We’ll roughly follow Microsoft’s instructions to install WSL 2 on Windows 10: https://docs.microsoft.com/en-us/windows/wsl/install-win10 Install all Windows Updates:\\\nRun Windows Update by going to Start -> Settings -> Update and Security and click “Check for Updates”. Install all available updates, including the May 2020 update. Install Windows Subsystem for Linux: Click Start, type “Turn Windows Features On or Off”, and open the “Turn Windows Features On or Off” app. Check the boxes for both Virtual Machine Platform and Windows Subsystem for Linux . Click OK to begin the install. Windows may invite you to reboot after this is installed. Set WSL to default to version 2:\\\nOpen a command prompt or PowerShell terminal by going to Start -> type “cmd”, choose Command Prompt, and type this: wsl --set-default-version 2 If this command results in an error, it means you don’t yet have Windows 10 version 2004. Return to Windows Update and check again. Update the Linux kernel: In future versions of Windows, this will be done with Windows Update. For now, we need to run an installer. Download the kernel update from https://wslstorestorage.blob.core.windows.net/wslblob/wsl_update_x64.msi and install. Install Ubuntu into WSL: Open the Windows Store by going to Start -> type “Store” -> pick the Microsoft Store app. From the search box on the top-right, type “Ubuntu” and download it. You’ll need to login to a Microsoft account to begin the download. The Ubuntu app in the Microsoft Store will install the latest Ubuntu distribution, currently Ubuntu 20.04. Set Ubuntu as the default WSL kernel: Open a command prompt or PowerShell terminal by going to Start -> type “cmd”, choose Command Prompt, and type this: wsl --list -v You should see a star next to Ubuntu like so: >wsl --list -v\n\n  NAME          STATE           VERSION\n\n* Ubuntu        Stopped         2 If Ubuntu is not the default, set it, as follows: wsl --setdefault Ubuntu\nwsl --list -v Ensure it’s WSL 2:\\\nOpen a command prompt, if you don’t have one open already, and type: wsl --list -v This lists all your distributions. If you install Docker Desktop and enable WSL2 mode, you’ll see their kernels here too. If any of the distributions don’t show as Version 2, update them to Version 2: wsl --set-version Ubuntu 2 Launch Ubuntu: Open a command prompt and type: wsl This will launch your default Linux kernel. You’re now in a real, live Linux shell on your Windows box. This is exciting! You’ll notice the terminal prompt on the left changed. It now shows forward slashes instead of backslashes. It ends with a $ instead of a > . Welcome to Linux! The first time you launch this, it’ll invite you to create a username and password. You’ll use this password to become “root” (Administrator) in Linux. Once the password is set, let’s exit: exit We’re now back at the regular Windows terminal. Windows Subsystem for Linux version 2 (WSL2) is now installed. Next, let’s tweak it slightly. Part III: Configure WSL In this section, we’ll ensure WSL has enough (but not too much) resources to run. Then we’ll enable the user to run privileged Linux commands without a password, required for cluster setup. (Windows calls it administrator, but in Linux we call it super user or su. The Linux sudo command sudo is short for “superuser do.”) Finally, we’ll get all available updates. 1. .wslconfig: Open Windows Explorer in your user home directory (typically c:/Users/yourname). Click in the address bar – just below the ribbon tabs, and above all the files – and type: cmd This will create a command prompt in this directory. Next type this: echo [wsl2] > .wslconfig This creates the .wslconfig file and ensures it doesn’t have a .txt extension. Open this .wslconfig file in your favorite editor, such as Notepad. Edit the file to look like this: [wsl2]\nmemory=4GB\nprocessors=4 Adjust these values to be between 4 and 32 gigs ram, and 4 or more processors, and to be less than or equal to your machine’s resources. SingleStore’s free tier product works only up to 32 gigs RAM per leaf node, and we’ll only run one leaf. Run as root without requiring a password: Open a terminal in any directory and launch the visudo tool by typing: wsl sudo visudo visudo is the program used to edit the sudoers file. This file lists programs and users that are allowed to switch to root without a password. Use the arrow keys to get to the bottom of the file, and then add this line, swapping in your username: rob ALL=(ALL) NOPASSWD:ALL Usernames are case-sensitive. In Windows, my login is Rob, but as I setup WSL, I chose rob, so I entered all lower-case here. (The Microsoft world tends to prefer mixed case, whereas the Unix world prefers – and sometimes requires – all lower-case.) Type cntrl + x to exit, and choose Y to save. Test that it works correctly by typing: sudo Verify it doesn’t prompt you for your password. Exit out of the shell: exit Update Ubuntu: Open a command prompt or PowerShell window. Get into WSL as root: wsl sudo su You can verify you’re at a root prompt because your terminal starts with root@. Update Ubuntu by typing: apt-get update\napt-get upgrade -y\napt-get dist-upgrade -y\napt-get autoremove -y Exit out of the Linux shell: exit You may wish to run these Linux upgrade commands periodically to keep Ubuntu updated. In future versions of Windows, the system may run these commands for you, as part of Windows Update.\\\nWe’ve now opened a terminal, launched into Linux, became root, and run commands twice now. This is easy! Part IV: Install SingleStore The memsql-deploy command in SingleStore Tools has a “cluster in a box” flag that installs one master node and one leaf node on a single machine. (Confusingly, there’s also a SingleStore “cluster in a box” Docker container.) In this tutorial we’ll use the SingleStore Tools command to quickly provision a test cluster. This is a configuration that can run on a single machine/server, with minimal resources. With WSL 2 installed and configured, we’re now ready to install SingleStore into Ubuntu. We’ll roughly follow the install docs from https://docs.singlestore.com/v7.1/guides/deploy-memsql/self-managed/linux/step-1/ to install SingleStore’s “cluster in a box” configuration onto Ubuntu running on Windows. 1. Install SingleStore management tools Note : Like all Linux software installations, SingleStore must be installed as root (Administrator). Open a Windows command prompt by going to Start -> type “cmd” -> choose Command Prompt, and in the terminal type: wsl sudo su You’re now in Linux as root. You can verify this because your terminal will have changed to show root@ at the beginning. From the Linux terminal as root, type (copy/paste) these lines: wget -O - 'https://release.singlestore.com/release-aug2018.gpg'  2>/dev/null | apt-key add -\n\napt-key list\n\napt install -y apt-transport-https\n\necho \"deb [arch=amd64] https://release.singlestore.com/production/debian memsql main\" | tee /etc/apt/sources.list.d/memsql.list\n\napt update\n\napt install -y memsql-toolbox memsql-client memsql-studio Be careful of long lines here. Some of these commands may wrap to a second in this post, and if so, make sure you enter them as a single command. After these lines finish, you’ll have these pieces installed (but not running) in WSL: memsql-toolbox : A suite of command-line tools for installing and configuring SingleStore clusters. memsql-client : This allows you to type memsql at a Linux command prompt and run queries against the database. This tool can be handy for scripting backups or tweaking data. memsql-studio : This is a browser-based administration and query tool designed for use with SingleStore. All of these tools are installed, but none are running yet. We also haven’t yet provisioned our cluster. Exit the shell: exit 2. Create a SingleStore Cluster Note : Unlike installing software, creating the cluster needs to be done as a regular user, not as root. Open a Windows command prompt by going to Start -> type “cmd” -> choose Command Prompt and type: wsl This will start WSL 2, rather than WSL 1, because we specified that above. Grab your license key from singlestore.com/free-software/ and create a variable for the license key: export LICENSE_KEY=paste_your_license_key_here Create a “cluster in a box” SingleStore cluster: memsql-deploy cluster-in-a-box --license $LICENSE_KEY --password YOUR_PASSWORD_HERE Alternatively, you can run the setup wizard, and configure the cluster in a browser. Also alternatively, you can create a YAML file describing your cluster and run memsql-deploy setup-cluster –cluster-file memsql.yml 3. Verify the Cluster is Running Open a Windows command prompt by going to Start -> type “cmd” -> choose Command Prompt, and type: wsl memsql-admin list-nodes Note in the output both nodes are running: >wsl memsql-admin list-nodes\n\n+------------+--------+-----------+------+---------------+--------------+---------+----------------+--------------------+--------------+\n\n| SingleStore ID  |  Role  |   Host    | Port | Process State | Connectable? | Version | Recovery State | Availability Group | Bind Address |\n\n+------------+--------+-----------+------+---------------+--------------+---------+----------------+--------------------+--------------+\n\n| 850151AEC3 | Master | localhost | 3306 | Running       | True         | 7.1.4   | Online         |                    | 0.0.0.0      |\n\n| D77EEB532D | Leaf   | localhost | 3307 | Running       | True         | 7.1.4   | Online         | 1                  | 0.0.0.0      |\n\n+------------+--------+-----------+------+---------------+--------------+---------+----------------+--------------------+--------------+ 4. Start SingleStore Studio SingleStore Studio is the browser-based database administration and SQL Query tool. In most configurations we’d run this as a Linux service, but systemd is not available in WSL2, so we’ll run it as a background process. Open a Windows command prompt by going to Start -> type “cmd” -> choose Command Prompt, and type: wsl sudo su We’re straight into a root prompt. Let’s start SingleStore Studio as a background process: memsql-studio &>/tmp/memsql-studio.log & You can close this terminal, and SingleStore Studio will stay running until you reboot. Register the SingleStore cluster with SingleStore Studio: Open a browser to http://localhost:8080/ WSL automatically proxies traffic between the local machine and the Linux kernel running in WSL2. This makes it “magically” work on localhost . Perfect! Click “Add or Create Cluster”. Click “Add an Existing Cluster”. Set Hostname to localhost and port to 3306 . Enter credentials: username is root , and use the password you set running the memsql-deploy command when installing the cluster. Set the Environment to “Development”, and set the cluster name and description to “localhost”. With all these set, the “Submit” button on the bottom right lights up. Click “Submit”. You’re now logged into the cluster. Part V: Use SingleStore With the SingleStore cluster running, and SingleStore Studio started, we’re ready to login and start using the database. 1. Launch SingleStore Studio Open a browser to http://localhost:8080/ If you’re not logged in already, choose localhost cluster, enter username of root and your password. 2. Run Queries Click on the SQL Editor tab on the left. In the SQL window, run each of these queries by pasting it into place and pushing the run button on the top right: Loading Gist. Please Wait... 3. Check the Results After the SELECT command, you should see the row in the database on the bottom pane. It worked! Part VI: Connect your App SingleStore is wire-compatible with MySQL, so you can use most MariaDB or MySQL drivers and admin tools to connect to SingleStore. For example, you could write a C# program to connect to the database created above: APPENDIX Restarting the Cluster If you reboot your machine or shut down the WSL2 kernel, you’ll need to restart the SingleStore cluster and the SingleStore Studio browser-based admin tool. 1. Restart the cluster Open a terminal and type: wsl memsql-admin start-node --all -y Verify it’s running: wsl memsql-admin list-nodes 2. Restart SingleStore Studio Open a terminal and type: wsl sudo su\nmemsql-studio 2>&1 /tmp/memsql-studio.log & Verify it’s running:\\ http://locahost:8080/ You can close this terminal and SingleStore Studio will continue running until you reboot. Stopping the Cluster By default, when Windows turns off the WSL Linux Kernel, it just stops all the processes. If you’d like, you can shut down SingleStore more gently before this happens. 1. Shut down the cluster Open a terminal and type: wsl memsql-admin stop-node --all -y 2. Verify it’s stopped wsl memsql-admin list-nodes Specifically Stopping WSL2 Kernel In most cases, you don’t need to specifically stop the WSL2 kernel, as Windows handles this automatically. But in the rare case you need this, it can be handy: 1. Shutdown WSL Open a terminal and type: wsl --shutdown 2. Verify the kernel is stopped wsl --list -v", "date": "2020-08-26"},
{"website": "Single-Store", "title": "10-data-predictions-for-your-data-strategy-in-2021", "author": ["David Yakobovitch"], "link": "https://www.singlestore.com/blog/10-data-predictions-for-your-data-strategy-in-2021/", "abstract": "With the world still recovering and reorganizing from COVID-19 and How the Global Pandemic has reshaped supply chains, infrastructure, and the way that business is processed, 2021 is anticipated to be a year to make big gains for companies that embrace and implement strategic Digital First Strategies. Whether Mobile First, Tablet First, or Laptop First strategies are at the forefront of your organization, data will continue to reign supreme as 5G continues its global roll-out. What strategies can you take to ensure that your enterprise is ready to capture your market share and successfully be a player in the #DataRevolution of 2021?  Here are 10 initiatives to get you started in the New Year. 1 – Build a Global Versioned Data Lake The success of large-scale Enterprise data systems requires versioning and control of data systems.  Whether planning on user, role and group control; or specific updates that don’t overwrite and you can ensure the integrity of your system, it is important to design your data architecture where you know your reads and writes are successful, as you scale from User 1 to User 1 Billion.  At SingleStore, we provide one platform for all your data workloads. When it comes to operationalizing real-time workloads from your cloud data lake, our technology enables data lake acceleration and modernization . 2 – Place Security Top of Mind From Solar Windows and Microsoft Cloud, to other top Security hacks of 2020, it’s already a known secret that if there is a compromise, it will be exploited. And with Personally identifiable information (PII), the rise of global data protection standards such as GDPR and CCPA, and the need for compliance with SOC2 and ISO27001, data security is an essential part for scaling Enterprise Data Strategy in 2021.  When you choose a system that can be the one to help you scale at speed, you want to ask questions to your preferred technology partners and vendors such as:  What security compliance standards do you meet today?  What certifications are you testing against?  What in-house security team do you have?  How do you measure security standards at your organization?  At SingleStore, from SingleStore Managed Service through SingleStore DB, our data products are built from the ground-up with Enterprise-grade security standards to ensure your data is safe and secure both at rest and in-transit.  If you’re building a cybersecurity application, you’ll find that the speed, scale, and SQL advantages that SingleStore brings provides better threat prevention, as Nucleus Security has found. 3 – Develop for Scale in Mind As our society continues to start as Digital First, the amount of data being processed continues to grow exponentially, and this can slow down systems with uneven and legacy architecture.  Developers could consider building architecture that plans for Scale First with container orchestration systems such as Kubernetes.  By reducing the overhead for administration and management of scaled data systems, your team can quickly respond to incidents, spike in usage, and build robust data systems that are designed with Scale in Mind.  At SingleStore, our systems include native support for Kubernetes operators , and are designed with functionality for unlimited horizontal scaling through both User Interfaces and the Command Line. 4 – Adapt for Multi-Cloud Our customers are distributed and so are employees in 2021, leading to the need for a Multi-cloud strategy.  Systems that once thought only about being on-premise for security, can now be adapted to the Multi-Cloud.  From systems such as Amazon Web Services, Google Cloud Platform and Microsoft Azure, to Open Shift, Digital Ocean, Alibaba Cloud, and Huawei Cloud, SingleStore runs in any cloud environment so that your applications never experience downtime. 5 – Design for Performance When you think of choosing a database for a new application, often the first ones that come to mind are freely-available open source databases like PostgreSQL and MySQL. But once your application takes off and your tables approach as little as 10,000 rows, you start to see performance bottlenecks in your app. Putting band-aids on these databases or finding hosted services for them don’t adequately solve the issue because they weren’t built as cloud-native, distributed SQL databases, which modern applications need.  The good news is that it’s straight-forward to modernize by moving to SingleStore with its MySQL wire protocol compliance. Whether for mobile applications, native web-based applications, or Cloud Native and serverless applications, performance can mean the difference between a downtime during a critical event, or being always up and always ready for your customers who depend on you during essential events. 6- Upgrade Technology Frequently As developers, one of the biggest challenges is determining how and when to process updates, whether they should be nightly builds, weekly updates, rolling updates, or long term stable solutions.  Our team at SingleStore has made this easy for your developers by offering patches (weekly builds), maintenance updates (monthly with incremental updates) and updated releases (quarterly to bi-annually with significant product updates).  With SingleStore, you are in control for which features you would like released to your developers to supercharge your developer ecosystem. 7- Tune and Optimize for Incredible Performance Long running queries, pipelines, and batch processes that have not been changed for months or years can use a face lift.  The performance increase can often be hours on a single query, being a game changer for your entire workflow.  SingleStore regularly offers new tuning that includes optimizations to both the core engine technology  and to advanced SQL techniques to optimize your queries further . 8- Renew your Training and Education Efforts Learning for all is more important than learning for one.  At SingleStore, education is a fundamental part of developer velocity and developer experience.  From Enterprise training for DBAs and Developers, to public training for developers looking to integrate with the SingleStore Managed Service for their applications, or students ready to pick up state-of-the-art technology for their resume and next career moves, training and certification can give your team the confidence they need to implement technology, and to hire new resources who are ready from Day 1 on the market. 9- Open Source Software for Community Initiatives Data strategy often starts in the Open Source world, and SingleStore is an advocate for change at the earliest of levels.  Research institutions, non-profits, and Universities can partner with SingleStore for an OpenSource license to build for good , and to give back with novel technology ideas. 10- Open Access for All Developers SingleStore is built with SQL first, and supports common programming languages such as JSON, and its extensions into Scala, Python, R and other programming frameworks for software engineers, data scientists, machine learning engineers, and data analysts. SingleStore is the next generation platform for database administrators, software engineers, data scientists, data analysts, and machine learning engineers to build Enterprise-grade applications at scale.  Deploy for free at SingleStore.com/try-free or learn more and Book a Demo today.", "date": "2021-01-08"},
{"website": "Single-Store", "title": "carbon-cloud-and-the-modern-data-estate", "author": ["Domenic Ravita"], "link": "https://www.singlestore.com/blog/carbon-cloud-and-the-modern-data-estate/", "abstract": "On this National Cut your Energy Costs Day, it’s a good time to think about our carbon footprints at home and at work as data professionals. Since the first home was electrified in Manhattan in the 1880s, our home electricity usage has grown dramatically. According to Energy.gov, residential homes now account for 22% of all electricity consumption in the U.S. Roughly 63% of this electricity is still generated by nonrenewable fossil fuel sources in the U.S. according to the Energy Information Administration ., but this varies a lot based on where you live in the country. In Georgia where I live, nonrenewable fossil fuel sources account for about 71% of electric generation.  No matter where you live, saving energy brings immediate benefits to you and helps reduce our carbon footprint. As today is National Cut your Energy Costs Day, it’s a good time to think about how changing some habits will save money on your monthly electricity bill, but the larger collective impact of cutting your energy use helps the environment by reducing the carbon footprint. Three energy-saving tips that can make a difference.  First,  install a programmable thermostat. These can learn household behaviors and set temperatures at the optimal levels for comfort and may save as much as 15 percent of electricity consumption.  Second, finish replacing those energy-hungry incandescents with LED bulbs. Finally, unplug the multiple devices, laptops, televisions, and even the coffee pot.  The bricks and wall warts for those electronics and appliances are energy vampires which draw power even when the device is off and can account for as much as 20% of your monthly bill. But as important as our personal energy habits are, perhaps we should be more environmentally conscious about the impact of our choices as IT and data professionals. Our home and work lives have blurred in the restricted lifestyle this pandemic has caused and our new home-bound behaviors are driving the largest, fastest adoption of digital services the world has ever seen. By day, we’ve turned to video conferencing from the kitchen counter for work. By night, we’re watching The Queen’s Gambit on Netflix and the Mandalorian on Disney+. (I highly recommend both.) But you may be wondering how the use of these digital services are impacting electricity usage. Electricity and the Cloud In the early months of the pandemic after air travel dropped precipitously, the carbon footprint of video streaming services received a lot of attention. With the energy consumption of information and communication technologies (ICT) increasing 9% every year, The Shift Project’s Lean ICT Report found that the carbon footprint from ICT sectors increased from 2.5% to 3.7% of global emissions, compared to air travel’s 2.4% share. Of the 3.7%, 19% come from data centers and 16% from network operations. Of course, video streaming services are just one type of digital service among many more SaaS applications delivered by both public cloud data centers and enterprise-owned data centers. and they require massive amounts of electricity to operate.  The National Resources Defense Council (NRDC) estimated that data center electricity consumption would increase to roughly 140 billion kilowatt-hours annually in 2020, the equivalent annual output of 50 power plants, costing American businesses $13 billion in electricity bills and emitting nearly 100 million metric tons of carbon pollution per year. This is roughly 2-3% of all electricity usage in the U.S. per year. Although it’s invisible to us, our collective use of digital services is making a big impact on electricity usage and the environment. There is some good news on data centers. Efficiency improvements have reduced the growth rate in their electricity consumption over the last 20 years. A study commissioned by the Department of Energy and conducted by the Lawrence Berkeley National Laboratory used decades worth of data the observe the trend in electricity usage of data centers and found that from 2000 to 2020 the rate of increase in electricity usage was estimated to stabilize close to 4% from 2010 to 2020 compared to a 90% increase from 2000 to 2005 and a 24% from 2005 to 2010. Part of the efficiency gain is attributed to the reduced growth in the number of servers operating in public cloud data centers. Servers in the public cloud are operated at a higher server utilization rate than enterprise-managed data centers. Amazon Web Services commissioned a study from 451 Research showing that their infrastructure-as-a-service was more than 3.6 times as energy efficient as the median of surveyed U.S. enterprise-owned data centers. They attribute that efficiency advantage to a much higher server utilization and more efficient hardware. Google and Microsoft Azure data centers are achieving similar efficiency gains over corporate-owned data centers. Managing the Data Estate But just as our personal energy habits in our homes have a large effect on energy use, so do our IT decisions. How we manage the data powering these SaaS applications in the context of carbon may be the next big challenge because where data is stored, where it’s copied, how it’s processed, and where it’s transmitted all add up and have an impact. You or your Cloud Operations team sees the effect of that for your company’s SaaS product in your cloud utility bill every month. Some of the line items may pop out, like a large cluster of m5.12xlarge instances in a test environment that’s been left running for 30 days with no activity. In this case, the cloud-saving habit is no different than your home energy-saving habit: Turn off the lights when you leave the room! The carbon impact of other cloud decisions we make may be less obvious. Modern customer and business experiences delivered by SaaS applications depend on a diverse data backend. Microsoft refers to this as the “modern data estate” with data stored in different data locations across different types of data stores from operational databases to data warehouses to data lakes. Into this data estate flows a deluge of data from an increasing number of different sources. Within the data estate we ingest, manage, and analyze this data using various types of storage appropriate to the processing and need for freshness. In the data estate, you need to retain a long history of data to be able to access past and present data and predict the future. I think the analogy of the estate is a useful one for thinking about the carbon impact of our data management decisions. Within the estate we have assets and liabilities, in terms of data assets and workloads. The data liabilities include the cost of copying and moving data. It has been conventional wisdom of late to pick a datastore-per-workload. There are complex decision trees available on how to pick from among almost 20 different specialty datastores such as time-series, key-value, document, wide-column, unstructured text, graph or relational data. There’s also the choice about the type of processing needed in terms of transactions or analytics. Consider the real-time data assets and workloads needed for your SaaS application or business. Think about how many different types of datastores are involved in creating, storing and processing those data assets and workloads. Also consider the machine learning models which operate on that data. The tally may be 3, 4 or more. Because it’s as easy and convenient to spin up new datastores as it is to flip on a light, your data estate may be large and sprawling which requires an estate staff with specialized skill sets to manage each of those assets.  At SingleStore, we’ve encountered  scenarios where as many as 14 different types of datastores were involved in producing real-time assets and serving real-time analytics. Serving these diverse workloads on diverse data structures for real-time use cases is inherently inefficient. Big data becomes even bigger when it’s copied and moved rather than operated on in place and as it arrives from streaming sources. In terms of the “data estate”, we can reduce the liability and cost of creating, processing, and maintaining real-time assets by consolidating these workloads. There’s no need to give up on the convenience of instant availability in the cloud or the data access styles and structures you’ve grown accustomed to when designing your application. Many have already moved off single-node and special purpose databases to achieve greater efficiency by combining real-time operational and analytical workloads on diversely-structured data, from binary and raw text to key-value, document and relational. Such as sharing hardware at the cloud infrastructure level is resulting in higher server utilization and greater energy efficiency for data centers, building applications with a unified database that supports diverse workloads on diversely-structured data reduces your data estate’s liabilities. I argue that it also has the effect of increasing the value of the real-time data assets as well since designing SaaS applications with SingleStore reduces latency and stores data more efficiently through a combination of innovations than other datastores. Takeaway So, unplug those energy vampires in your home and across your data estate. Take a modern approach to cut your energy consumption. Consider the advantages you gain by combining real-time workloads into fewer datastores to not only simplify and accelerate your data, but also to conserve electricity and reduce the carbon footprint. By renewing and modernizing your data estate through reducing special purpose datastores, you’re directly following the environmental ethos of reduce, reuse, and recycle. I’ve said before that you must simplify to accelerate . Consider that by doing so, you may also “simplify to save”.", "date": "2021-01-10"},
{"website": "Single-Store", "title": "celebrating-martin-luther-king-jr-s-leadership-and-values", "author": ["Raj Verma"], "link": "https://www.singlestore.com/blog/celebrating-martin-luther-king-jr-s-leadership-and-values/", "abstract": "Today, Martin Luther King Jr. (MLK) is celebrated, honored and remembered as one of America’s greatest leaders for his transformative ability to move the needle forward in regard to the civil rights movement in the United States and throughout the world. King sought equality and human rights for African Americans, the economically disadvantaged, and all victims of injustice through peaceful protest. Commitment. Determination. Clarity. Vision. Inspiration. Transformational Mindset. These words all describe the attributes of a great leader. I’ve had a lifelong passion for studying the qualities of great leaders throughout history, and I have always admired and respected his ability to lead from the front. He set a higher standard for winning hearts and minds, through eloquence, persuasion, passion, reason and practicality. When I reflect back on his leadership, I am struck with the belief that all of us can learn from his lessons. I’d like to highlight his brilliance as a leader. Commitment: He had an undeterred commitment to doing more, belief in his conviction, and he discovered new ways to innovate upon his focus. He knew that there was no room to waver when it came to being a successful leader. He exemplified courage for his convictions, stood behind them, and made sure that others understood his belief. Determination & Clarity: He remains one of the most eloquent speakers this country has ever seen. He was able to motivate millions through his speeches. In fact, he was so well spoken, and inspiring, that his words live on today and continue to inspire people more than 50 years later. He left no room for doubt as to what he wanted to accomplish. Vision & Inspiration: Like all good leaders, he could read a room, or a situation, which often helped him anticipate the needs of the people that were around him. Similar to a strategic chess player, King thought several moves ahead. He knew that people “derived inspiration from involvement.”  He understood that to get results he had to inspire others with his well communicated vision. Through his words and actions others were encouraged to dream big and then deliver on that dream and more. Transformation Mindset: His respect for others, and sometimes tolerance, enabled him to bring about transformational change with long lasting impacts for his followers. Like many powerful leaders throughout history, King did not have a title that granted him legislative authority, his true leadership power was conveyed through his actions, clarity of vision and his belief in all that he sought to accomplish. King remains one of the country’s most impressive leaders to this day. His command of these qualities, like the above, affords us powerful lessons for those every-day leaders to make and be the change – at home, at work, with peers and within family. Martin Luther King Jr. will always be considered one of the most exceptional people in American history. As you spend time with your families over the long holiday weekend, in this persistent new normal, I would encourage you to reflect on some of the values King shared. I am humbled to honor one of the most transformational people of our generation. I celebrate his ability to be a student of life, to possess the ability to see the big picture and to communicate that vision, and to be one of the most inspirational and timeless leaders of our modern era.", "date": "2021-01-18"},
{"website": "Single-Store", "title": "how-to-accelerate-analytics-for-saas-developers", "author": ["Domenic Ravita"], "link": "https://www.singlestore.com/blog/how-to-accelerate-analytics-for-saas-developers/", "abstract": "Everyone remembers their first experience in a Tesla. Mine was in Las Vegas with a good friend and CIO. I was in town for a tech conference and that provided a good opportunity to reconnect and discuss a new project. He offered to pick me up at my hotel. That was the moment I was first introduced to Tesla’s unique Ludicrous mode. It was exhilarating. The Strip became one long, breathless blur. If you haven’t experienced zero to 60 mph in 2.2 seconds, take a look at the “Tesla reactions” genre of videos online. Breakthroughs in Scaling Data Analytics Wouldn’t it be great to get that kind of reaction to your product experience? Whether your customers are launching queries for generating fastboards , leaderboards for gaming analytics , filtering audience views, or generating BI reports, a constant challenge is scaling your data infrastructure without slowing your services or showing your users the dreaded spinning wait animation. It may be time to hit the accelerator on the analytics in your SaaS product to give your users the thrill of your version of Ludicrous mode. SingleStore is the data engine that powers Ludicrous mode for real-time, interactive data analytics in your SaaS applications. As an application developer, you generally choose the database that you’re familiar with, is general purpose, and has broad support. If you’re building in a cloud environment, this often leads you to a hosted relational database, like Azure SQL, AWS RDS for MySQL or Google Cloud SQL. These work fine early on, but start to show cracks as your SaaS product gains rapid adoption. This is the moment you start to encounter data bottlenecks which show up in your customer experience. Solving the data bottlenecks is the essential thing to get right and is the frequent obstacle. How can you ensure backend scalability while simultaneously focusing on delivering a simple, easy-to-use service? When application developer Gerry Morgan started encountering data bottlenecks in DailyVest’s portfolio analytics API for their 401(k) customers, he and fellow engineer Kevin Lindroos identified the culprit as their AzureSQL database. While it had served them well initially, as data grew their costs grew but performance was never more than simply adequate. As they extrapolated their customer growth plans, they determined that they needed a better way to control costs as they grew their customers. So, they began the search for a new database platform that could support their growth and the ad hoc analytical queries over large data volumes their portfolio analytics product required. This led them to consider columnstore databases. For application developers unfamiliar with columnstores, they are generally best for analytical workloads whereas rowstores are generally best at transactional workloads. ( Should you use a rowstore or columnstore? ) After investigating columnstore databases such as AWS RedShift, Vertica, MariaDB, and kdb+, they discovered SingleStore met - or rather exceeded - all of their requirements. The benefits were clear. It had a better total cost of ownership, provided a managed service in Azure, executed stored procedures multiple times faster than AzureSQL, and accelerated database backups from 1 hour to just 10 minutes. To learn more about how these application developers scaled and accelerated the analytics in their SaaS applications, watch How DailyVest Drove a 90% Performance Improvement . For IEX Cloud, the data bottleneck they encountered when scaling their cloud service was a little different. IEX Cloud is a division of IEX Group, the company made famous by Michael Lewis’ 2014 book “Flash Boys: A Wall Street Revolt”. The key service IEX Cloud delivers is a real-time financial market data API. It requires the collection and aggregation of many historical and real-time data sources which are then processed and served to their customers. Balancing the flow of data from disparate sources and to over 130,000 consumers with serving over 1.5 billion API responses per day and 800,000 data operations per second demands quite a lot of simultaneous read and write volume on their database backend. Tracking the real-time changes in stock prices is a write-intensive operation while serving billions of API requests against that fast-changing data is read-intensive. Serving real-time streaming analytics with metrics like P/E ratios and market capitalization on real-time streaming data and historical data adds compute-intensive workloads to the mix. Furthermore, as a data aggregator, IEX Cloud must refresh reference data from many 100s of providers throughout the day through ETL. They expect the number of ETL processes will soon be in the 1000s. Compounding the situation, daily market volatility correlates to volatility in the volume of API traffic from their customers. IEX Cloud needed improved performance in multiple areas that their initial Google Cloud SQL for MySQL service wasn’t delivering. These requirements include high performance bulk-loaded data through ETL, streaming data ingestion, store all the data, perform real-time analytics, low latency responses to many parallel API requests, and the ability to easily scale horizontally. After trying a variety of database types, including CockroachDB, YugaByte, Clickhouse, and Google BigQuery, IEX Cloud found that only SingleStore could satisfy all of their requirements, and do it in just one system that was cost-effective, had an established community, and good support. Learn more about this SaaS analytics scaling challenge in The FinTech Disruption of IEX Cloud webinar. Common First Steps When performance and scaling issues arise, application developers are among the first to know in today’s DevOps world. Application performance monitoring alerts the team and triage get into motion. At this point, if a DBA is available the investigation begins, queries are profiled and indexes are modified or added. If handling read volume is the issue, a common technique is to provide a read replica by replicating from the primary database. This offloads work, but at the cost of adding latency and duplicating data. If the data is fast-changing, the approach is less effective as the data in the replica is out-of-date all too often. Caching is the next option for scaling read-heavy workloads. You’ve seen it work great for static assets in Gatsby, Next.js, or React Static, but managing your dynamic business data this way is another animal. Managing cache eviction is complicated and expensive for fast-changing data. Another challenge is that the size of your cached data must fit into the memory of a single machine. This works well for small datasets, but you’ll soon be looking for a way to scale the cache if your data is large. Scaling out a cache by adding more nodes, for Redis for instance, provides the availability of data but at the cost of data consistency. It also adds infrastructure and more complexity. Another option for scaling is to use database partitioning. This technique cuts the data into sizes that will fit into a single server, no matter how much data you have. There are various types of partitioning/sharding to ensure no downtime or data loss in the event of a node failing. There are various approaches for partitioning and indexing the data based on your queries. You can try to do this yourself, but it may get you more than you bargained as an application developer. There is an easier way which provides the scalability with the speed and simplicity you need. Solving for Scale, Simply SingleStore solves 3 key challenges for SaaS applications which embed live or real-time analytics without the need of changing your application design, adding a cache, manual partitioning, or other external middleware: Scaling Data Ingest Scaling Low Latency Queries Scaling User Concurrency SingleStore is a distributed, highly-scalable SQL database. This is an important characteristic as it’s the foundation for how it addresses each of the SaaS analytics scaling issues. For scaling data ingestion, SingleStore breaks the scaling bottleneck by providing parallel ingestion from distributed streaming sources in addition to bulk data loads. As mentioned earlier, both are important for IEX Cloud’s analytics API. Next, query responses can return trillions of rows per second on tables with over 50 billion records. This is the kind of low latency query speed that makes fans out of your SaaS application users. Finally, SingleStore scales to meet the need of your growing customers through support for high concurrency. For Nucleus Security, every millisecond matters when it comes to thwarting cyberattacks. Their original database choice was MariaDB but it failed to keep up with their needs to perform more frequent vulnerability scans and serve real-time analytics to a quickly growing number of government users. SingleStore delivered the high concurrency needed for their rapidly growing SaaS applications while dramatically improving performance by 50x, at ⅓ the costs of the alternatives. Scott Kuffer, co-founder of Nucleus Security, describes the details in the Every Millisecond Counts in Cybersecurity webinar. Every successful SaaS application includes analytics either as a core offering or as an adjunct feature. Analytics places increased demand on the database. Customers won't wait for your spinning animation while their data loads. So, it is imperative to deliver the app and the analytics fast. Otherwise, you see more incidents raised, negative reviews are posted on social feeds and review sites, and you may find that your customer churn increases. In short, your business goes down and you have to fight to get your growth back. This can happen in the blink of an eye when switching costs are relatively low for SaaS services. That’s no way to claim your slice of the $157 billion global SaaS application market. SingleStore accelerates and scales the analytics in your SaaS application by delivering scalable data ingestion with single millisecond low latency queries queries, and high concurrency. But beyond the ludicrous speeds and thrills it delivers, our customers rave about our customer support and optimization services, our established robust community, and how cost-effective the solution is. To learn more about SingleStore Managed Service to scale the analytics in your SaaS application, join us for the upcoming webinar on April 8.", "date": "2021-03-23"},
{"website": "Single-Store", "title": "a-programmers-perspective", "author": ["Sasha Krassovsky"], "link": "https://www.singlestore.com/blog/a-programmers-perspective/", "abstract": "Linus Torvalds had some interesting things to say about AVX512: “I hope AVX512 dies a painful death… I absolutely detest FP benchmarks, and I realize other people care deeply. I just think AVX512 is exactly the wrong thing to do…”. Having had the unique opportunity of migrating a portion of SingleStore’s library of SIMD kernels from AVX2 to AVX512/VBMI over the last few months, I disagree. For one thing, AVX512 is not designed solely for floating-point workloads. SingleStore’s code fundamentally deals with whizzing bytes around, and AVX512 is more than up to the task. In case you haven't heard of SingleStore, it's an extremely high-performance distributed SQL database management system and cloud database service that can handle all kinds of workloads, from transactional to analytical. It really shines for real-time analytics (summary aggregate queries on large volumes of rapidly changing data). We get our speed through compilation and, on our columnstore access method, vectorization. Squeezing the last, best bit of performance out of our vectorized execution is where SIMD comes in. AVX2 has given us several-times speedups. I investigated whether we can double that yet again with AVX512/VBMI. As an Intel partner with early access, I tested the performance of Ice Lake which was launched today. The results were good: on individual kernels, I could often approach or achieve a 2x speedup over the AVX2 implementation, simply by virtue of the doubled register size. While previous generations of CPUs supporting AVX512 had downclocking issues, the Icelake chips seemed to have negligible drops in clock speed even when running an AVX512 workload on all cores. Below is a chart showing the performance of the three versions of ByteUnpacking, a kernel which takes an array of values of byte width X and extends each value to byte width Y. This is denoted as ByteUnpack_X_Y in the chart. SingleStore uses ByteUnpacking extensively as data is read from disk and decoded. However, even ignoring the performance aspects, I’d still argue that AVX512 is a big step over AVX2 for one important reason: ease of use. Developers have vectorized their code for years and will continue to do so. By making their SIMD instructions easier to use, Intel is saving developers thousands of hours. AVX512 is easier to use for two reasons: New, more powerful instructions Predicated instructions More Powerful Instructions AVX512 and especially VBMI/VBMI2 provide a whole suite of exciting new instructions: Multishift (bit-level selection of bytes) Expand/Compress Arbitrary byte-level permutes The byte-level permutes are huge. When designing a SIMD algorithm, it’s often necessary to move data around within a register. In AVX terminology, intra-lane movement is called a ` shuffle ` while inter-lane movement is called a ` permute ` . This operation comes up in the byte unpacking kernel shown above. As a toy example, suppose we want to put all of the even-indexed bytes in an AVX register into the lower half and the odd-indexed bytes into the top half. AVX2 provides 32- and 64-bit permutes and byte-granular shuffles. The code would look something like: __m256i vecShuffle = _mm256_set_epi8(0xf, 0xd, 0xb, 0x9, 0x7, 0x5, 0x3, 0x1,\n                                     0xe, 0xc, 0xa, 0x8, 0x6, 0x4, 0x2, 0x0,\n                                     0xf, 0xd, 0xb, 0x9, 0x7, 0x5, 0x3, 0x1,\n                                     0xe, 0xc, 0xa, 0x8, 0x6, 0x4, 0x2, 0x0);\nvec = _mm256_shuffle_epi8(vec, vecShuffle);\nvec = _mm256_permute4x64_epi64(vec, 0xd8); This pattern of a shuffle and a permute for arbitrary byte rearrangement is both frequent and cumbersome, as now there are two steps to the process. In contrast, AVX512 can do this in a single operation: __m512i vecShuffle = _mm512_set_epi8(63, 61, 59, 57, 55, 53, 51, 49,\n                                     47, 45, 43, 41, 39, 37, 35, 33,\n                                     31, 29, 27, 25, 23, 21, 19, 17,\n                                     15, 13, 11,  9,  7,  5,  3,  1,\n                                     62, 60, 58, 56, 54, 52, 50, 48,\n                                     46, 44, 42, 40, 38, 36, 34, 32,\n                                     30, 28, 26, 24, 22, 20, 18, 16,\n                                     14, 12, 10,  8,  6,  4,  2,  0);\nvec = _mm512_permutexvar_epi8(vecShuffle, vec); Not only is this fewer instructions, it’s actually easier to think about: In the AVX512 version, we simply pick which byte ends up where by index. It’s more readable, less error-prone, and easier to write. Predicated Instructions AVX512 provides variants of each instruction prefixed with ` mask ` or ` maskz ` , which perform the operation only on elements with a 1 for the corresponding element in the input mask. The obvious use-case for this is conditionally performing operations. For example, SingleStore makes extensive use of both bit vectors (where true is denoted by a ` 1 ` bit) and byte vectors (where true is denoted by a ` 0xff ` byte). Using these predicated instructions, converting between the two is trivial: __mmask64 bits = *bitVector;\n__m512i bytes = _mm512_maskz_set1_epi8(bits, 0xff);\n_mm512_storeu_si512(byteVector, bytes); On top of that, AVX512’s comparisons provide bit vectors rather than byte vectors! This means that predicated instructions flow seamlessly one into another. For example, suppose we wanted to compact every element in an array greater than 10 and store it to some pointer ` output ` . We can use an instruction introduced in VBMI2 called ` compress ` , which squashes every element with a ` 1 ` in the mask to be adjacent to each other. The ` store ` variant of ` compress ` writes these to memory rather than outputting a register. AVX512/VBMI2 makes this task trivial: __m512i vec10 = _mm512_set1_epi32(10);\n__mmask16 mask = _mm512_cmpgt_epi32_mask(vecInput, vec10);\n_mm512_compressstoreu_epi32(output, mask, vecInput); The mask scheme gives the added advantage of easily knowing the number of elements passing the comparison using a popcount. The sheer utility provided by predicated instructions makes AVX512 much easier and simpler to program. Conclusion For the first time, I feel that I’m dealing with a friendly SIMD instruction set. I’ve only explored the tip of the iceberg with regards to the improved useability and developer-time savings provided by AVX512/VBMI. If your team is looking to improve performance but finds AVX2 or earlier daunting, give AVX512/VBMI a try. It really is easier to develop faster algorithms faster than on any other SIMD platform I’m aware of. Even if manually writing intrinsics is not your cup of tea, I’m sure compilers will have a much easier time of vectorizing code for AVX512/VBMI, and the performance benefits alone could be well worth it.", "date": "2021-04-06"},
{"website": "Single-Store", "title": "the-modern-database-experience", "author": ["Ero Carrera"], "link": "https://www.singlestore.com/blog/the-modern-database-experience/", "abstract": "For Developers, By Developers Sometimes, if you’re really lucky, you get unsolicited raves from developers who just love what your technology does for them. It’s amazing to be able to showcase opinions that developers write to share with other developers. We recently had this experience with Jack Ellis from Fathom, and this week, we received this contribution from Ero Carrera , Software Developer and Ex-Googler. We’re grateful to Ero for this guest blog post, where he shares his experience working with Modern Databases, how he discovered SingleStore experience with SingleStore, and how it’s been such a good fit for his engineering work. In Ero’s words: First, some background: I have been working with SQL on-and-off for over 20 years. Over that time, I spent uncountable hours with Oracle, MySQL, PostgreSQL, and SQLite, optimizing queries, backends, and designing schemas. While I learned a lot fiddling with different databases, a lot of effort went into setting them up, keeping them up to date, backing them up, and optimizing their settings to get the most out of each solution. These last ten years I worked at Google, where I was lucky to experience one of the most incredible technology stacks in the industry. As an engineer, the internal database technologies allow you to nearly forget about the backend and just use it, at ridiculously large scales, without having to care about storage or even optimizing queries much. Being able to just focus on building systems is something really easy to get addicted to. Some of the projects I worked on were on intelligence analysis for cybersecurity. On those it was necessary to find and analyze relationships between different datasets of indicators of malicious activity. Each of the datasets was accessible via an interface using the “Google flavored” SQL, what is known outside Google as BigQuery. To read the datasets one could simply write queries to produce and massage the data into the desired format and subsequently process that, no need to worry about the SQL engine, networking, or storage (for the most part). Each of those datasets were in the tens to hundreds of Terabytes (many billions of rows). Working at that scale and being able to forget about the stack of technologies (of course being conscious of performance and following best-practices) was simply incredible. Database Discovery Upon leaving Google last year, I was afraid about the state of affairs in the “real world”, whether I might have gotten too used to those fancy toys. I started taking a look at what’s available. I was hoping to find something of industrial strength that made my life as easy as possible, but still had all the bells and whistles of modern databases. My needs were leaning towards the relational side, while I love key-value databases, I needed to be able to run analytics on structured data, so I wanted something “SQL native”. My datasets were nowhere close to those I had at Google but I still wanted speed and the fast query times that enable “interactive research”. For my current projects I have time series data, where I need to join multiple tables, with complex nested queries, to extract aggregate statistics. The tables are in the tens of millions of rows, which are joined and grouped-by in several rounds. In the pipeline I’ve built so far I still haven't found a case where SingleStore doesn’t return results in near-real time. A good friend who was starting his second startup recommended ClickHouse for time-series data. I tried to play with it, but bumped into some friction setting up a test environment and then having to interface it with their own libraries. I needed something easy to maintain, with a very comfortable interface, and eventually found a great analysis, Selecting a Database for an Algorithmic Trading System , that convinced me to try SingleStore (still going by MemSQL in the article). I was drawn to it by how easy SingleStore is to interface with, just use the standard MySQL libraries and interfaces! A docker image was readily available, it could easily bulk-ingest CSV data, plus all the bells-and-whistles of distributed databases. The in-memory rowstore and on-disk columnstore reminded me of the similar optimizations of Google’s internal tools, which made them so incredibly fast. Some of the technologies used at Google are discussed in the white paper An Inside Look at Google BigQuery and some posts in Stack Overflow, where a brief explanation of ColumnIO, Google’s columnar format, is given. Additionally, the possibility of spinning up a managed instance in any of the main cloud providers seemed interesting, but was not on my radar originally (more on that in a bit). My SingleStoreModernNew Horizons in the Database Experience: My initial experience with SingleStore’s Docker image was very smooth. I was able to get it up and running within minutes. Documentation was up-to-date and I had no trouble setting it all up. My Python code could just use MySQL Connector/Python to connect to the instance, it all worked. The workflow calls for scheduled ingestions of a few thousand to a few million records from CSV dumps. LOAD DATA worked like a charm to read them, only taking a few seconds for the largest dataset. My previous pipelines exported data into HDF5 that was later analyzed in Mathematica. I found that set up terribly slow and cumbersome compared with SingleStore plus a BI tool like Metabase . More recently I’ve been tempted to try SingleStore’s managed service to not even have to bother with the occasional updates (which were pretty easy anyway) or starting/stopping the Docker image. It could not have been easier, again I found the documentation clear and I was able to launch my own cluster with ease. I only needed to update the config files to point my pipelines to the new instance (I chose to host it in AWS) and it just worked. Additionally, given that I had to make zero code changes, I can easily go back and forth between the Docker instance and the managed cluster, just by updating a configuration file. There was one wish I had related to the managed service, it was that of being able to suspend it when I’m not using it. I am currently working on a hobby project, I do not need other users accessing the database, hence keeping the instance spinning is a bit wasteful. I reached out to the very responsive SingleStore team and they were happy to let me know that there’s an upcoming on/off feature. So I’ll be able to rely on the managed instance exactly as much as I need to. How great is that? I have also been playing with UDFs to compute some metrics for my analytics and again, it works. I could simply write function definitions, for scalar functions, in SQL and call them from my queries, leading to much simpler SQL. Conclusion I have yet to find any place where SingleStore does meet my needs. Granted, I have not yet pushed it very hard, but it has made me much more productive, allowing me to focus on the logic of my pipelines ( Prefect , ingesting & preparing data, and training & deploying some ML models) and building analytics dashboards, for which I am using Metabase (which also works like a charm with SingleStore). While there’s extensive documentation on how to optimize for high performance settings for more demanding conditions. I have not yet managed to write any queries that take more than a few seconds at most. Multiple WITH clauses and nested queries are handled gracefully. I am very happy to have come across SingleStore. It has made me much more productive and provided me with a “database experience” as the one I had at Google. I only look forward to my projects to grow and see how far it can be pushed! Ero Carrera Software Engineer, Ex-Googler Experience SingleStore for yourself! Install the SingleStore Db for FREE or Deploy our Managed Service with $500 in FREE credits.", "date": "2021-05-27"},
{"website": "Single-Store", "title": "why-you-should-use-a-scale-out-sql-dbms-even-when-you-dont-need-one", "author": ["Eric Hanson"], "link": "https://www.singlestore.com/blog/why-you-should-use-a-scale-out-sql-dbms-even-when-you-dont-need-one/", "abstract": "Everybody has heard of the KISS principle -- Keep It Simple, Stupid. When applied to data management technology, it implies that you should use a simple solution when it works. Developers often believe that means using a single-node DBMS like MySQL, PostgreSQL, or SQL Server. So, what is a “single-node” database? Essentially, it’s a database designed for a single machine. The capacity of a single machine dictates the resource constraints of processing power, connections, and storage. If you need more processing power or storage, you can vertically scale, meaning you upgrade to a more powerful machine. This can work, up to the scale limits of the largest available machine. By contrast, scale-out databases are built as distributed databases from the start, that is, designed to be run across machines. Sometimes people see scale-out relational database technology, like SingleStore, as strictly for high-end applications, beyond what one single-node DBMSs can handle. The thinking goes that they are inherently more complex to use than a single-node DBMS. I'm here to tell you that the KISS principle is right. But people who think it means you should only use scale-out for \"extreme\" apps are dead wrong. Let's look at the arguments for why scale out is not \"simple,\" and address them one by one. We'll see that in many cases, scale out actually makes things simpler. It's hard to get and set up and manage the hardware and software. First, database platform-as-a-service (PaaS) offerings like SingleStore Managed Service handle all that for you. You can choose a size and dial up and down whenever you need to. For self-hosting, using the public cloud or a well-organized enterprise data center with a range of hardware SKUs means you can pick out and provision machines fairly easily. My single-node database is fast enough. For some problems with small data, a single-node DBMS may be fast enough. But there are thousands of applications out there with medium-to-large data on a single node system. The people who built them may think they are fast enough, but what if they could run queries instantaneously? Research shows that response time under ¼ second feels instantaneous , and that generates incredible user satisfaction. This drives users to explore more freely, learning more about the data, which helps them make better decisions. Your single-node DBMS might be able to provide near-instant responses for a few users, but what if there are many users? Enterprises are pursuing digital transformation initiatives to get more out of the data they have, not just scale to handle bigger data sets. The levels of speed and concurrency you can get from scale-out enable new applications that unlock data's value, enabling digital transformation. If my problem gets big, I can just add more nodes of my regular RDBMS.  A common pattern for scaling applications is to create multiple databases on multiple nodes using a single-node DBMS. This could be done in a number of ways, such as\\\n(a) putting each customer's data in a different database with the same schema, and spreading those databases across multiple nodes or\\\n(b) creating replicas of the same database to scale out the workload.\\\nEither way, this is anything but simple because you have to decide at the application level how to split up your data. Moreover, there might be a situation where you need more than one node to handle the workload for a single database. At that point, your single-node database runs out of steam. My problem is not big enough to benefit from scale out.  You'd be surprised about how small an application can be and still benefit from scale out. Here are a couple of examples. First, SingleStore has a customer with a few billion rows of data in a data mart that would easily fit on a single-node database. But they are extending quarter-second response time for dashboard refreshes to several hundred concurrent users, with intra-day updates, enabling a complete digital transformation in how the data is consumed. It's sort of a real-time data mart. As a second example, we have customers that have less than a million rows of data but are using brute-force searches of vector data for AI image-matching applications using DOT_PRODUCT and EUCLIDEAN_DISTANCE functions in SQL. This brute force approach gives them better match fidelity than multi-dimensional vector indexing that's not available in SQL DBMSs to date, and still let's them integrate their match queries with other SQL query constructs. And see the later discussion about using brute-force scale out to simplify away the need for complex solutions like pre-calculated aggregate tables. Plenty of people with only a few hundred thousand rows of data can benefit from that. It's hard to design my scale-out database to get it to perform. Yes, there are a couple of new concepts to learn with a scale-out database, mainly sharding (for partitioning data across nodes) and reference tables (for duplicating small dimension tables onto each node). But the horsepower you get from a good, high-performance scale-out database actually simplifies things. E.g. you may need materialized views or pre-calculated summary aggregate tables with a single-node database but not with a scale-out database. Pre-calculated aggregates and materialized views are tricky to use and introduce design problems that are conceptually harder than deciding how to shard your data. If you know when to use an index, you can learn how to shard your data and use reference tables in a few minutes. And you don't have to get it right the first time; it's easy to reconfigure (create a new table with the configuration you want, INSERT...SELECT... data into it, drop the old one, rename the new one, and you're done). I can't find people with the skills to use a scale-out DBMS.  Modern SQL-based scale-out databases are based on standard SQL, and are often compatible with MySQL or Postgres. SingleStore is largely compatible with MySQL, for example. So hundreds of languages and tools can connect to these SQL-based scale-out databases. And almost all of the SQL skills people have from working with SQL databases like MySQL and Postgres are transferable. I want to use a general-purpose database and there are no general-purpose databases with scale out that work for me. A general purpose tool simplifies life, that's true, by making skill sets applicable to multiple problems, and reducing the effort to search for the right tool. Fortunately, there is a general-purpose SQL database that scales out and runs anywhere. I think you know what database that is. I could go on, but you get the idea -- the spartan simplicity of relational databases and SQL carries its benefits over to scale-out SQL systems. And scale out simplifies lots of things, and enables digital transformation opportunities that can be valuable to your business and your career. There's a corollary to the KISS principle that applies here also, often attributed to Albert Einstein: \"Everything should be made as simple as possible, but no simpler.\"  In this context, that means you shouldn't give up on really valuable application innovations made possible by the performance you can get from scale out, due to perceived complexity. Finally, scale out is to improve speed and scalability. SingleStore scales out, but it also has other important technologies to make things faster, including in-memory row store tables, columnstores, compilation of queries to machine code, vectorization, and a high-performance plan cache. All of these things squeeze the most out of each processor core in your system. So, next time you are about to reach for your trusty single-node DBMS, ask yourself, can I reach higher, and do more, and keep it simple at the same time?", "date": "2021-04-19"},
{"website": "Single-Store", "title": "how-to-power-enterprises-with-intelligent-applications-with-jordan-tigani-of-singlestore", "author": ["Gina von Esmarch"], "link": "https://www.singlestore.com/blog/how-to-power-enterprises-with-intelligent-applications-with-jordan-tigani-of-singlestore/", "abstract": "Jordan Tigani is the Chief Product Officer at SingleStore. He was the co-founding engineer on Google BigQuery. He also led engineering teams then product teams at BQ. Overview of HumAIn Podcast Season 5 |Posted on May 5, 2021 |Posted in: # data science , # fast analytics , # intelligent applications , # jordan tigani , # singlestore SingleStore powers Comcast with their streaming analytics to drive proactive care and real-time recommendations for their 300K events per second. Since switching to SingleStore, Nucleus Security converted its first beta account to a paying customer, increased the number of scans Nucleus can process in one hour by 60X, and saw speed improvement of 20X for the slowest queries. To be more competitive in our new normal, organizations must make real-time data-driven decisions. And to create a better customer experience and better business outcomes, data needs to tell customers and users what is happening right now. With the pandemic accelerating digitization, and new database companies going public (Snowflake) and filing IPOs (Couchbase), the database industry will continue to grow exponentially, with new advanced computing technologies emerging over the next decade. Companies will begin looking for infrastructure that can give real-time analytics — they can no longer afford to use technology that cannot handle the onslaught of data brought by the pandemic. True Digital in Thailand utilizes SingleStore’s in-the-moment analytics to develop heat maps around geographies with large COVID-19 infection rates to see where people are congregating, pointing out areas to be avoided, and ultimately, flattening the curve of COVID-19. In two weeks’ time, SingleStore built a solution that could perform event stream processing on 500K anonymized location events every second for 30M+ mobile phones. Businesses need to prioritize in-app analytics: This will allow you to influence customer’s behaviors within your application or outside of it based on data. Additionally, businesses must utilize a unified database that supports transactions and analytics to deliver greater value to customers and business. Enterprises must access technology that can handle different types of workloads, datasets and modernize infrastructure, and use real-time analytics. Please click here for the full podcast. Shownotes Links: – https://www.linkedin.com/in/jordantigani – https://twitter.com/jrdntgn – www.SingleStore.com – https://www.linkedin.com/company/singlestore/ – https://www.singlestore.com/media-hub/releases/research-highlights-spike-in-data-demands-amid-pandemic/ – https://www.singlestore.com/media-hub/releases/businesses-reconsidering-existing-data-platforms/ About HumAIn Podcast The HumAIn Podcast is a leading artificial intelligence podcast that explores the topics of AI, data science, future of work, and developer education for technologists. Whether you are an Executive, data scientist, software engineer, product manager, or student-in-training, HumAIn connects you with industry thought leaders on the technology trends that are relevant and practical. HumAIn is a leading data science podcast where frequently discussed topics include ai trends, ai for all, computer vision, natural language processing, machine learning, data science, and reskilling and upskilling for developers. Episodes focus on new technology, startups, and Human Centered AI in the Fourth Industrial Revolution. HumAIn is the channel to release new AI products, discuss technology trends, and augment human performance.", "date": "2021-05-06"},
{"website": "Single-Store", "title": "three-common-mysql-errors", "author": ["Sarung Tripathi"], "link": "https://www.singlestore.com/blog/three-common-mysql-errors/", "abstract": "MySQL has tens of thousands of users leveraging it for complex technical challenges, but single-box database systems still reach their limits when pushed. In this blog post, we summarize three of the most common errors users see on MySQL and some tips and tricks on how to approach them, as well as what to look for when seeking a more scalable alternative. Feeling like you’ve maxed out MySQL? Let’s dive in and learn more. Error #1: Too Many Connections When building applications, you may find that as you adopt consumers you are starting to hit limits with the amount of connections allowed. The default in MySQL is 151 connections, but you should be careful to increase this too high as MySQL uses one thread per connection and having too many active threads may hurt your performance. Each thread needs memory, and memory is expensive. Many users have tried to address this challenge by adopting flavors such as AWS RDS MySQL still face issues with increase loads, particularly with table/row level locking increasing the number of client connections Here at SingleStore, many users leverage our cloud DBaaS to handle highly concurrent workloads. Its MySQL wire protocol compatibility makes it easy to migrate your workloads. Connections are also extremely efficient for SingleStore, as the engine is smart when mapping connections to threads. If connections are idle, we immediately allocate threads to other work instead of hoarding them. This allows SingleStore to scale up to handling more connections than threads in most cases. SingleStore’s default value for max_connections is 100,000 By comparison, AWS RDS hard limit is 16,000. Google Cloud SQL for MySQL limit is 4,000 Azure Database for MySQL limit is 20,000 Error #2: Out of Memory Running out of memory in MySQL could mean a few different things. The easiest place to start is to make sure tables have not exceeded the allocated memory (especially for temporary tables). This can also often be caused by highly concurrent workloads over large queries that require lots of memory (GROUP BY, for instance). The InnoDB engine requires careful tuning when MySQL becomes resource constrained A common approach after exhausting all options of a single box VM’s memory has been something like MariaDB’s columnstore architecture. However, columnstore architectures are typically built largely for analytical purposes and are not so great at handling real-time ingest or transactional queries. SingleStore’s Universal Storage allows users to get the TCO benefits of a columnstore database, with the performance and usability of memory-constrained rowstores like fast seeks and updates. Universal Storage allows applications to run operational transactions  on data that cannot be stored in RAM at an affordable cost. Hash indexes and UPSERT ability on columnstore tables give users the ability to leverage disk for OLTP, HTAP and analytics all in one place Error #3: The Table is Full MySQL users often come across this issue when they run out of memory or disk space Memory-related concerns typically arise because users have not allocated enough memory to handle both query processing as well as data storage. Disk-related issues are a bit easier to resolve given the lower TCO, however adding more disk may ultimately lead to a disproportionate amount of storage as it relates to the rest of the resources on the single MySQL VM. The InnoDB engine helps by compressing data by around 50%. SingleStore’s Universal Storage (i.e., columnstore) offers 75-90% compression, helping dramatically reduce the amount of disk required to hold your data. However, simply compressing data is not enough. For columnstore to provide best in class performance, the engine must leverage seekable encodings and vectorized execution. SingleStore does both. Conclusion SingleStore offers a distributed, scalable alternative to MySQL. Universal Storage helps thousands of users support highly concurrent workloads with high compression rates. SingleStore offers many more unique differentiators such as Pipelines to rapidly ingest data from Kafka, S3, GCS, etc. -- with just a single SQL query! Try SingleStore Managed Service today with the help of one of our cloud engineers. Click here to access your $500 in free credits!", "date": "2021-05-20"}
]