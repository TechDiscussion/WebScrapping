[
{"website": "Instagram-Engineering", "title": "types for python http apis an instagram story", "author": ["Anirudh Padmarao"], "link": "https://instagram-engineering.com/types-for-python-http-apis-an-instagram-story-d3c3a207fdb7", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts And we’re back! As we mentioned in the first part of our blog post series , Instagram Server is a Python monolith with several million lines of code and a few thousand Django endpoints. This post is about how we use types to document and enforce a contract for our Python HTTP APIs. In the next few weeks, we’ll share details on more tools and techniques we’ve developed to manage our codebase’s quality. When you open up the Instagram app on your mobile client, it makes requests to our Python (Django) server over a JSON, HTTP API. To give you some idea of the complexity of the API we expose to the mobile client, we have: over 2000 endpoints on the server over 200 top-level fields in the client data object that represents that image, video, or story in the app 100s of engineers writing code for the server (and even more on the client!) 100s of commits to the server each day that may modify the API to support new features We use types to document and enforce a contract for our complex, evolving HTTP API. Let’s start at the beginning. PEP 484 introduced a syntax for adding type annotations to Python code. But why add type annotations at all? Consider a function that retrieves a star wars character: To understand the get_character function, you have to read its body. it takes an integer character id it takes a calendar system enum (e.g. BBY or “Before Battle of Yavin”) it returns a character with fields id, name, and birth year The function has an implicit contract that you have to re-establish every single time you read the code. But code is written once and read many times, so this doesn’t work well. Further, it’s hard to verify that the callers of the function and the function body itself adhere to the implicit contract. In a large codebase, this can lead to bugs. Consider instead the function with type annotations: With type annotations there is an explicit contract. You only have to read the function signature to understand its input and output. A typechecker can statically verify that code conforms to the contract, eliminating an entire class of bugs! Let’s develop an HTTP API to retrieve a star wars character, and use type annotations to define an explicit contract for it. The HTTP API should take the character id as a url parameter and the calendar system as a query parameter. It should return a JSON response for the character. To implement this API in Django, you first register the url path and the view function responsible for taking a HTTP request to that url path and returning a response. The view function takes the request and the url parameters (in this case, id) as input. The function parses and casts the calendar query parameter, fetches the character from a store, and returns a dictionary that is serialized as JSON and wrapped in a HTTP response. Although the view function has type-annotations, it does not define a strong, explicit contract for the HTTP API. From the signature, we don’t know the names or types of the query parameters, or the fields in the response or their types. Instead, what if we could make the view function signature exactly the same as that of the earlier type-annotated function? The function parameters can represent request parameters (url, query, or body parameters). The function return type can represent the content of the response. Then, we would have an explicit, easy-to-understand contract for the HTTP API that the typechecker can enforce. So, how can we implement this idea? Let’s use a decorator to transform the strongly-typed view function to the Django view function. This design requires no changes to the Django framework. We can use the same routing, middleware, and other components that we are familiar with. Let’s dive into the implementation of the api_view decorator: That’s a dense bit of code. Let’s go over it piece by piece. We take as input the strongly-typed view, and wrap it into a regular Django view function that we return: Now let’s look at the implementation of the Django view. First we have to construct the arguments to the strongly-typed view function. We use introspection with the inspect module to get the signature of the strongly-typed view function and iterate over its parameters: For each of the parameters, we call an extract function, which extracts the parameter value from the request. Then, we cast the parameter value to the expected type from the signature (e.g. cast the calendar system from a string to an enum value). We call the strongly-typed view function with the parameter arguments that we’ve constructed: It returns a strongly-typed class (e.g. Character ). We take that class, transform it into a dictionary, and wrap it into a JSON, HTTP response: Great! So now we have a Django view that can wrap the strongly-typed view. Finally, let’s take a look at that extract function: Each parameter may be a url parameter or a query parameter. The url path of the request (the url path we registered as the first step) is accessible on the Django URL resolver’s route object. We check if the parameter name is present in the path. If it is, then it’s a url parameter, and we can extract it from the request in one way. Otherwise, it’s a query parameter, and we can extract it in another way. And that’s it! This is a simplified implementation but it illustrates the main ideas. The type used to represent the HTTP response content (e.g. Character ) can use either a dataclass or a typed dictionary. A dataclass is a concise way to define a class that represents data. Dataclasses are the preferred way to model HTTP response objects at Instagram. They: automatically generate boilerplate constructors, equals, and other methods are understood by typecheckers and can be typechecked can enforce immutability with frozen=True are available in the Python 3.7 standard library, or as a backport on the Python Package Index Unfortunately at Instagram, we have a legacy codebase which uses large, untyped dictionaries passed between functions and modules. It would be difficult to migrate all this code from dictionaries to dataclasses. So, while we use dataclasses for new code, we use typed dictionaries for legacy code. Typed dictionaries allow us to add type annotations for dictionary client objects and benefit from typechecking, without changing runtime behavior. The view function expects us to return a character. What do we do when we want to return an error to the client? We can raise an exception, which the framework will catch and translate to an HTTP error response. This example also shows the HTTP method in the decorator, that specifies the allowed HTTP methods for this API. The HTTP API is strongly typed with a HTTP method, request types, and response types. We can introspect the API and determine that it should take a GET request with a string id in the URL path and a calendar enum in the query string, and it will return a JSON response with a Character . What can we do with all of this information? OpenAPI is an API description format with a rich set of tools built on top of it. If we write a bit of code to introspect our endpoints and generate an OpenAPI specification from them, we can take advantage of that ecosystem of tools. We can generate HTTP API documentation for the get_character API that includes the names, types, and documentation for the request and response. This is the right level of abstraction for client developers who want to make a request to the endpoint; they shouldn't have to read Python code. There are additional tools we could build such as a \"try it out\" tool to make requests in the browser, so developers can hit their HTTP APIs without having to write code. We could even code generate type-safe clients for end-to-end type-safety. With this we can have a strongly-typed API on the server and call it with a strongly-typed API on the client. We could also build a backwards-compatibility checker. What happens if we release a version of the server that has id , name , and birth_year required fields, and later realize that we don't know the birth year of every character? We want to make birth year optional, but old clients that expect birth year can crash. Although we have an explicit type for the API, that explicit type can change (birth year goes from required to optional). We can keep track of the changes to the API, and warn developers as a part of their workflow if they make changes that can break clients. There are a spectrum of application protocols that machines can use to communicate with each other. On one end of the spectrum, we have RPC frameworks like Thrift or gRPC. They generally define strong types for the request and the response, and generate code on the client and server to make requests. They may not communicate over HTTP or even serialize their data into JSON. On the other end of the spectrum, we have unstructured Python web frameworks, with no explicit contract for requests or responses. The approach we have taken captures many of the benefits of more structured frameworks, while continuing to communicate via HTTP + JSON with minimal application code changes. It’s important to note that this is not a new idea. In strongly-typed languages, there are many frameworks that provide an API like the one we’ve described. In Python, there is also prior art with the APIStar framework. We’ve successfully rolled out types for HTTP APIs at Instagram. We’ve been able to adopt this across our codebase since the framework is easy and safe to adopt for existing views. The value is clear to product engineers: the generated documentation becomes the means by which server and client engineers can communicate. Many thanks to my team members: Keith Blaha (who co-built the framework), Carl Meyer, Jeremy Fu, Chris Leung, Mark Vismonte, Jimmy Lai, Jennifer Taylor, Benjamin Woodruff, and Lee Bierman. If you want to learn more about this work or are interested joining one of our engineering teams, please visit our careers page , follow us on Facebook or on Twitter . Stories from the people who build @Instagram 1.6K 7 API Python Django Rest Api 1.6K claps 1.6K 7 Written by Infrastructure @ Instagram Stories from the people who build @Instagram Written by Infrastructure @ Instagram Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-09-13"},
{"website": "Instagram-Engineering", "title": "interview with tamar shapiro instagrams head of analytics", "author": ["Instagram Engineering"], "link": "https://instagram-engineering.com/interview-with-tamar-shapiro-instagrams-head-of-analytics-c81946d02b90", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts Tamar is the Head of Analytics at Instagram. She supports a team of data scientists and data engineers responsible for analytics, data science , and experimentation. Tamar also leads the Diversity and Inclusion task force at Instagram. She is a passionate advocate for women in tech and has served as a mentor for Minds Matter and Women Unlimited , as a speaker for Girls Who Code , and as co-chair of the Analytics Collective. This interview was conducted by Serena, an Instagram engineering manager. How did you become an engineer? When I was in college, the field of Data Science did not yet exist. I studied Statistics & Biometry in undergrad. Later on, I went to graduate school for Engineering. What was your first coding language? I used Java when I took my first computer science class, but most of my math and statistics classes required SAS, SPSS, and MATLAB. What do you listen to while you work? Believe it or not, I don’t listen to music at all while working. I think it’s distracting. Plus, these days I’m not at my desk long enough to listen to music anyway. What do you do when you get stuck on a problem? I like to talk it out with people. Discussing the problem out loud helps me think about the problem differently, and it helps to hear from other people who either have encountered a similar problem before or may look at things from a different perspective. What do you do when you feel a lot of pressure? I usually like to take a break. If I have time, I’ll take a walk around the NY building’s floors or our office’s amazing rooftop garden. If I don’t have time, I just take a beat — I take some deep breaths to clear my head, and that can be very helpful. Please tell us about your favorite project at IG. This is like being asked to pick your favorite child! My recent favorite is the private like counts test, a product exploration aimed at shifting the focus from the quantity of interactions to the quality of interactions on your feed posts. What I really love about it is that we are challenging the status quo. We are thinking differently about a part of the product that has been a vital part of the Instagram experience from the beginning. What makes working at Instagram unique? After two years here, I can confidently say that it’s our culture. One of our values is “people first” — meaning we’re always laser focused on how to create the best experience for our community. We approach every project through the lens of the people who use Instagram. How would you describe the engineering culture at Instagram? Very collaborative. We put the Instagram community first, and we work together to create the best products. Everyone here is so passionate about building amazing products for our community, and it’s so energizing to be a part of it. What makes you excited about coming into work every day? Working on a product that impacts the lives of billions of people throughout the world. It’s incredibly exciting. It’s also amazing to think about the potential impact we can have with every new idea and project we work on. It’s a huge responsibility, and one that we and I take very seriously. Your favorite place to eat in the city? I love sushi. Sushi of Gari is my favorite neighborhood sushi place in New York. What is your favorite thing to eat at the office? A smoothie from Frozen Palm, our smoothie bar in the New York office, which provides a lot of great (and healthy!) options. What’s your favorite Instagram account? @ girlswhocode , which is an organization that I volunteer for. They work on (and post about) important research that’s relevant to women in tech, and their ultimate goal is to help young women find career paths in STEM. I like to follow along to see what programs they’re working on and to hear stories about some of the alumni. Tell us about your happiest day at Instagram It was my second Faceversary (second-year mark at Facebook) and my team pulled out all the stops with flowers, cupcakes, etc. My manager, Adam Mosseri, was out of the office, but Nam, our head of engineering, surprised me at a meeting by printing Adam’s face on a piece of paper. We took a selfie with the cutout of Adam’s face, pretending he was there and celebrating with us. It was hilarious, but also very special for me because it was a great way to reflect back on all that’s happened over the past couple of years here. What is one of the best things you learned while working at Instagram The importance of good communication and context sharing. We work in such a fast-paced environment that often we assume people have the context they need to do their jobs well. Sometimes, it’s important to slow down and take the time to communicate the bigger picture. Coolest celebrity sighting at the office? The cast of Crazy Rich Asians visited the office, and many people were excited to see them in real life. We welcomed Henry Golding, Michelle Yeoh, Awkwafina, Ken Jeong, and book author Kevin Kwan for a FB Live from our New York office where they answered fan questions, and @evachen went Live with the cast from her Instagram account for a Q&A ahead of the film’s release. If you haven’t seen the movie, you’re missing out :) What does your desk setup look like? It’s super clean. I am a neat freak and a minimalist too. I think a clean space helps me keep my focus. But it’s hard to keep a clean desk here because there’s always some new piece of swag that manages to make its way onto my desk. What career advice would you like to give to female engineers who are early in their careers? So many — but one I’d like to touch on here is the importance of networking. It’s easy to think that we need to just sit at our desks and do really good work, but that’s only one part of the equation. It’s important to build relationships with people who are not in your immediate team, or company. Networks can open up opportunities for you in the future, and it’s also important to communicate your work with a broader audience. As women, I don’t think we advocate enough for ourselves, and we need to do a better job communicating our accomplishments. Another is confidence. Women, especially those early in their careers, are often prone to self-doubt. It ends up being our biggest enemy as we second-guess ourselves too much. It prevents us from pursuing opportunities to be challenged and grow. How can we overcome it? For me, what really worked was the “fake it until you make it” mindset. Observe the behavior of your confident colleagues and try to mirror it. On this topic, I really recommend The Confidence Gap . How do you achieve work-life balance? Do you believe in work-life balance at all? :) I do! It’s all about setting boundaries and not apologizing for these boundaries. We need to be very diligent about where we draw that line. For me, this might mean that I leave the office by a certain time so I can have quality time with my kids in the evening, and for others it might be something else. The key is sticking to these boundaries and carving out the time that you need to achieve that balance. Do you think work-life balance is harder to achieve as you become more and more senior in the company? I don’t think so. The demand definitely becomes greater, but you become better at ruthless prioritization. Most of the time, I don’t feel hindered by being a female engineer as I don’t have a problem speaking up or leaning in. But there are still times — for example, when I am pregnant — I worry that people will think less of me, and I worry whether I will still be able to meet my colleagues’ expectations. How can we overcome these thoughts? It’s a really great question and one that goes back to the confidence piece I mentioned earlier. Creating a human being is something to be very, very proud of — it’s amazing! We need to think about success in our lives in a broader way than just our work. You are building a family, you are a talented engineer, and all of these things are what make you you. I find that concerns around performance during a pregnancy are often a manifestation of our own self-doubt. While you might not feel as good or sharp as you’re used to, others are probably unaware, and you are your own worst critic. Everyone has good days and bad days, when they’re feeling distracted or not performing their best. We need to be kind to ourselves and be confident in our abilities. If you want to learn more about this work or are interested joining one of our engineering teams, please visit our careers page , follow us on Facebook or on Twitter . Stories from the people who build @Instagram 336 31 Professional Development Instagram Data Science Analytics Team 336 claps 336 31 Written by Stories from the people who build @Instagram Written by Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-09-24"},
{"website": "Instagram-Engineering", "title": "python at scale strict modules", "author": ["Carl Meyer"], "link": "https://instagram-engineering.com/python-at-scale-strict-modules-c0bb9245c834", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts Welcome to the third post in our series on Python at scale at Instagram! As we mentioned in the first post in the series , Instagram Server is a several-million-line Python monolith, and it moves quickly: hundreds of commits each day, deployed to production every few minutes. We’ve run into a few pain points working with Python at that scale and speed. This article takes a look at a few that we imagine might impact others as well. Consider this innocuous-looking sample module: When someone imports this module, what code will run? We’ll run a bunch of regex code to compile that string to a pattern object. We’ll run the @route decorator. Based on what we see here, we can assume that it's probably registering this view in some url mapping. This means that just by importing this module, we're mutating global state somewhere else. We’re going to run all the code inside the body of the Person class, which can include arbitrary code. And the Model base class might have a meta-class or an __init_subclass__ method, which is still more arbitrary code we might be running at import. The only line of code in this module that (probably) doesn’t run on import is return \"Hello World!\" , but we can't even say that for sure! So by just importing this simple eight line module (not even doing anything with it yet!), we are probably running hundreds, if not thousands of lines of Python code, not to mention modifying a global URL mapping somewhere else in our program. So what? This is part of what it means for Python to be a dynamic, interpreted language. This lets us do all kinds of useful meta-programming . What's wrong with that? Nothing is wrong with it, when you're working with relatively small codebases and teams, and you can guarantee some level of discipline in how you use these features. But some aspects of this dynamism can become a concern when you have millions of lines of code worked on by hundreds of developers, many of whom are new to Python. For example, one of the great things about Python is how fast you can iterate with it: make a change and see the result, no compile needed! But with a few million lines of code (and a messy dependency graph), that advantage starts to turn sour. Our server startup takes over 20s, and sometimes regresses to more like a minute if we aren't paying attention to keeping it optimized. That means 20-60 seconds between a developer making a change and being able to see the results of that change in their browser, or even in a unit test. This, unfortunately, is the perfect amount of time to get distracted by something shiny and forget what you were doing. Most of that time is spent literally just importing modules, creating function and class objects. In some ways, that's no different from waiting for another language to compile. But typically compilation can be incremental : you can just recompile the stuff you changed and things that directly depend on it, so many smaller changes can compile quickly. But in Python, because imports can have arbitrary side effects, there is no safe way to incrementally reload our server. No matter how small the change, we have to start from scratch every time, importing all those modules, re-creating all those classes and functions, re-compiling all of those regular expressions, etc. Usually 99% of the code hasn't changed since last time we reloaded the server, but we have to re-do all that slow work anyway. In addition to slowing down developers, this is a significant amount of wasted compute in production, too, since we continuously deploy and are thus reloading the site on production servers constantly all day long. So that's our first pain point: slow server startup and reload due to lots of wasted repeat work at import time. Here’s another thing we often find developers doing at import time: fetching configuration from a network configuration source. In addition to slowing down server startup even further, this is dangerous, too. If the network service is not available, we won’t just get a runtime error failing certain requests, our server will fail to start up. Let’s make this a bit worse, and imagine that someone has added some import-time code in another module that does some critical initialization of the network service. They don’t know where to put this code, so they stick it in some module that happens to get imported pretty early on. Everything works, so they move on. But then someone else comes along, adds an innocuous import in some other part of the codebase, and through an import chain twelve modules deep, it causes the config-fetching module to now be imported before the one that does the initialization. Now we’re trying to use the service before it’s initialized, so it blows up. In the best case, where the interaction is fully deterministic, this could still result in a developer tearing their hair out for an hour or two trying to understand why their innocent change is causing something unrelated to break. In a more complex case where it’s not fully deterministic, this could bring down production. And there’s no obvious way to generically lint against or prevent this category of issue. The root of the problem here is two factors that interact badly: 1) Python allows modules to have arbitrary and unsafe import side effects, and 2) the order of imports is not explicitly determined or controlled, it’s an emergent property of the imports present in all modules in the entire system (and can also vary based on the entry point to the system). Let’s look at one more category of common errors. Here we’re in a view function, and we’re attaching an attribute to some class based on data from the request. Likely you’ve already spotted the problem: classes are global singletons, so we’re putting per-request state onto a long-lived object, and in a long-lived web server process, that has the potential to pollute every future request in that process. The same thing can easily happen in tests, if people try to monkeypatch without a contextmanager like mock.patch . The effect here is pollution of all future tests run in that process, rather than pollution of all future requests. This is a huge cause of flakiness in our test suite. It's so bad, and so hard to thoroughly prevent, that we have basically given up and are moving to one-test-per-process isolation instead. So that's a third pain point for us. Mutable global state is not merely available in Python, it's underfoot everywhere you look: every module, every class, every list or dictionary or set attached to a module or class, every singleton object created at module level. It requires discipline and some Python expertise to avoid accidentally polluting global state at runtime of your program. One reasonable take might be that we’re stretching Python beyond what it was intended for. It works great for smaller teams on smaller codebases that can maintain good discipline around how to use it, and we should switch to a less dynamic language. But we’re past the point of codebase size where a rewrite is even feasible. And more importantly, despite these pain points, there’s a lot more that we like about Python, and overall our developers enjoy working in Python. So it’s up to us to figure out how we can make Python work at this scale, and continue to work as we grow. We have an idea: strict modules . Strict modules are a new Python module type marked with __strict__ = True at the top of the module, and implemented by leveraging many of the low-level extensibility mechanisms already provided by Python. A custom module loader parses the code using the ast module, performs abstract interpretation on the loaded code to analyze it, applies various transformations to the AST, and then compiles the modified AST back into Python byte code using the built-in compile function. Strict modules place some limitations on what can happen at module top-level. All module-level code, including decorators and functions/initializers called at module level, must be pure (side-effect free, no I/O). This is verified statically at compile time via the abstract interpreter. This means that strict modules are side-effect-free on import : bad interactions of import-time side effects are no longer possible! Because we verify this with abstract interpretation that is able to understand a large subset of Python, we avoid over-restricting Python’s expressiveness: many types of dynamic code without side effects are still fine at module level, including many kinds of decorators, defining module-level constants via list or dictionary comprehensions, etc. Let’s make that a bit more concrete with an example. This is a valid strict module: We can still use Python normally, including dynamic code such as a dictionary comprehension and a decorator used at module level. It’s no problem that we talk to the network within the _wrapped function or within hello_world , because they are not called at module level. But if we moved the log_to_network call out into the outer log_calls function, or we tried to use a side-effecting decorator like the earlier @route example, or added a hello_world() call at module level, this would no longer compile as a strict module. How do we know that the log_to_network or route functions are not safe to call at module level? We assume that anything imported from a non-strict module is unsafe, except for certain standard library functions that are known safe. If the utils module is strict, then we’d rely on the analysis of that module to tell us in turn whether log_to_network is safe. In addition to improving reliability, side-effect-free imports also remove a major barrier to safe incremental reload, as well as unlocking other avenues to explore speeding up imports. If module-level code is side-effect-free, we can safely execute individual statements in a module lazily on-demand when module attributes are accessed, instead of eagerly all at once. And given that the shape of all classes in a strict module are fully understood at compile time, in the future we could even try persisting module metadata (classes, functions, constants) resulting from module execution in order to provide a fast-path import for unchanged modules that doesn’t require re-executing the module-level byte-code from scratch. Strict modules and classes defined in them are immutable after creation. The modules are made immutable by internally transforming the module body into a function with all of the global variables accessed as closure variables. These changes greatly reduce the surface area for accidental mutation of global state, though mutable global state is still available if you opt-in via module-level mutable containers. Classes defined in strict modules must also have all members defined in __init__ and are automatically given __slots__ by the module loader’s AST transformation, so it’s not possible to tack on additional ad-hoc instance attributes later. So for example, in this class: The strict-modules AST transformation will observe the assignments to attributes name and age in __init__ and add an implicit __slots__ = ('name', 'age') to the class, preventing assignment of any other attributes to instances of the class. (If you are using type annotations, we will also pick up class-level attribute type declarations such as name: str and add them to the slots list as well.) These restrictions don’t just make the code more reliable, they help it run faster as well. Automatically transforming classes to add __slots__ makes them more memory efficient and eliminates per-instance dictionary lookups, speeding up attribute access. Transforming the module body to make it immutable also eliminates dictionary lookups for accessing top-level variables. And we can further optimize these patterns within the Python runtime for further benefits. Strict modules are still experimental. We have a working prototype and are in the early stages of rolling it out in production. We hope to follow up on this blog post in the future, with a report on our experience and a more detailed review of the implementation. If you’ve run into similar problems and have thoughts on this approach, we’d love to hear them! Many thanks to Dino Viehland and Shiyu Wang, who implemented strict modules and contributed to this post. If you want to learn more about this work or are interested joining one of our engineering teams, please visit our careers page , follow us on Facebook or on Twitter . Stories from the people who build @Instagram 1.8K 10 Python Modules Developement Velocity Development Runtime 1.8K claps 1.8K 10 Written by Stories from the people who build @Instagram Written by Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-10-17"},
{"website": "Instagram-Engineering", "title": "static analysis at scale an instagram story", "author": ["Benjamin Woodruff"], "link": "https://instagram-engineering.com/static-analysis-at-scale-an-instagram-story-8f498ab71a0c", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts Instagram Server is entirely Python powered. Well, mostly. There’s also some Cython, and our dependencies include a fair amount of C++ code exposed to Python as C extensions. Our server app is a monolith, one big codebase of several million lines and a few thousand Django endpoints [1], all loaded up and served together. A few services have been split out of the monolith, but we don’t have any plans to aggressively break it up. And it’s a fast-moving monolith; we have hundreds of engineers shipping hundreds of commits every day. We deploy those commits continuously, every seven minutes, typically deploying to production around a hundred times per day. We aim to keep less than an hour of latency between a commit landing in master and going live in production. [2] It’s really, really difficult to keep this massive monolithic codebase, hurtling along at hundreds of commits per day, from devolving into complete chaos. We want to make Instagram a place where all our engineers can be productive and ship useful features quickly! This post is about how we’ve used linting and automated refactoring to help manage the scale of our Python codebase. In the next few weeks, we’ll share more details of other tools and techniques we’ve developed to manage other aspects of our codebase’s quality. If you’re interested in trying out some of the ideas mentioned in this post, we recently open-sourced LibCST which serves as the heart of many of our internal linting and automated refactoring tools. [1] For more about how we use Django, watch the talk “Django at Instagram” we gave at Django Under the Hood conference in 2016. [2] For more on our continuous deployment, watch the talk “Releasing the world’s largest Python site every seven minutes” we gave at PyCon 2019. Linting helps developers find and diagnose issues or anti-patterns they may not have known about as they encounter them. This is important for us, because with hundreds of engineers it becomes increasingly difficult to disseminate these ideas. Linting is just one of many forms of static analysis that we use within Instagram. The naive way of implementing a lint rule is with a regular expression. Regular expressions are easy to write, but Python is not a regular language , and so it’s difficult (or sometimes impossible) to robustly match a pattern in the code with a regular expression. On the other end of the spectrum, we have tools like mypy and Pyre , two Python static type checkers, which can do whole-program analysis. Instagram uses Pyre. These tools are powerful, but hard to extend and customize. When we talk about linting at Instagram, we’re usually referring to simple abstract syntax tree based lint rules. This is how we typically write custom lint rules for Instagram’s server codebase. When Python executes a module, it starts by running a parser over the source code. That implicitly creates a parse tree, a kind of concrete syntax tree (CST) . This parse tree is a lossless representation of the input source code. Every detail, like comments, parenthesis, and commas are preserved in this tree. We could regenerate the original source code using this tree. Unfortunately, this creates a complex tree, making it hard to extract the semantics we care about. Python compiles the parse tree into an abstract syntax tree, or AST. This is a lossy conversion, and details about “syntactic trivia”, like comments, parenthesis, and commas are thrown away. However, the semantics of the code are preserved. At Instagram, we’ve developed LibCST , which provides the best of both worlds. It provides a lossless representation like a concrete syntax tree (CST), but still makes it easy to extract semantics like an abstract syntax tree (AST). Our lint rules use LibCST’s syntax tree to match patterns in code. As a high level representation, the syntax tree is easy to inspect and removes the problem of dealing with a non-regular language. Let’s say that you’ve got a circular dependency in your module due to a type-only import. Python lets you solve this by putting type-only imports behind an if TYPE_CHECKING: guard to avoid actually importing anything at runtime. Later, someone adds another type-only dependency to the code and guards it. However, they might fail to realize that there was already a type guard in the module. We can prevent this redundancy with a lint rule! We’ll start by initializing a counter of type checking blocks we’ve encountered. Then, when we encounter a type-checking conditional, we’ll increment the counter, and verify that there is no more than one type checking block. If this happens, we’ll generate a warning at that location by calling our report helper. These lint rules work by traversing LibCST’s tree, and collecting information. In our linter, this is done using the visitor pattern . As you may have noticed, rules override visit and leave methods associated with a node’s types. Those visitors get called in a specific order. Our philosophy at Instagram is to “do the simple thing first”. Our first custom lint rules were implemented in a single file, with a single visitor, using shared state. The single visitor class had to be aware of the state and logic of all of our unrelated lint rules, and it wasn’t always clear what state corresponded with what rule. This works fine when you’ve got a few custom lint rules, but we have nearly a hundred custom lint rules, which made the single-visitor pattern unmaintainable. Of course, one possible solution to this problem is to define multiple visitors and to have each visitor re-traverse the entire tree each time. However, this would incur a large performance penalty, and the linter must remain fast. Instead, we took inspiration from linters in other language ecosystems, like JavaScript’s ESLint , and developed a centralized visitor registry. When a lint rule is initialized, all of the rule’s method overrides are stored in the registry. When we traverse the tree, we look up all of the registered visitors and call them. If a method is not implemented, we don’t need to call it. This reduces the computation cost of every new lint rule. Though we usually run the linter over the small number of recently changed files, we can run all of our new lint rules in parallel over Instagram’s whole server codebase in just 26 seconds. Once we had a performant framework in place, we built a testing framework that works to enforce best practices by requiring tests for both false-positives, and false-negatives. With nearly a hundred custom rules, pedantic lints can quickly turn into a waste of time for developers. Spending time fixing style nits and deprecated coding patterns gets in the way of more important progress. We’ve found that with too many nags engineers start to ignore all lints, even the important ones. At a certain point, it doesn’t matter what good advice we present, it just gets ignored. Let’s say we needed to deprecate a function named ‘fn’ for a better named function called called ‘add’. Unless developers are made aware of the fact that ‘fn’ is deprecated, they won’t know not to use it. Even worse, they won’t know what to use instead. So, we can create a lint. But any sufficiently large codebase is bound to have plenty of other lints already. Chances are that this important lint will get lost in the noise. So, what can we do about it? We can automatically fix many issues found by lint. Much like lint itself is documentation on demand, auto-fixers can provide fixes on-demand. Given the sheer number of developers at Instagram, it’s not feasible to teach each developer all of our best practices. Adding auto-fixers allows us to educate and on-board developers to new best practices on-demand. Auto-fixers also allow us to preserve developer focus by removing monotonous changes. Essentially, auto-fixers are more actionable and educational than simple lint warnings. So, how do you build an auto-fixer? Syntax tree based linting gives us the offending node. There’s no need to duplicate discovery logic since the lint rule itself already exists! Since we know the node we want to replace, and we know its location in source, we can replace it with the updated function name safely! This works great for fixing individual lint violations as they are introduced, but when we introduce a new lint rule we may have hundreds of existing violations of it. Can we proactively resolve all the existing cases at once? A codemod is just a scriptable way to find issues and make changes to source code. Think of a codemod as a refactor on steroids: It can be as simple as renaming a variable in a function or as complex as rewriting a function to take a new argument. It uses the exact same concept as a lint, but instead of alerting the developer, it can take action automatically. So, why would you write a codemod instead of a lint? In this example, we want to deprecate the use of get_global . We could use a lint, but the fix time is unbounded and spread across multiple developers. Even with an auto fixer in place, it can be some time before all code is upgraded. To fix this, in addition to linting against get_global , we can also write a codemod! At Instagram, we believe that deprecated patterns and APIs left to slowly wither away take focus away from developers and reduce code readability. We’d rather proactively remove deprecated code than let it disappear over time. Given the sheer size of the code and number of active developers, this often means automating deprecations. If we can remove deprecated patterns from our code quickly, we keep all of Instagram productive. Okay, so how do you actually codemod? How do you replace just the text you care about, while preserving comments, spacing, and everything else? You can use a concrete syntax tree like LibCST to surgically modify code while preserving comments and spacing. So, if I wanted to rename “fn” to “add” in the below tree, I would update the Name node to have the value “add” instead of “fn” and then write the tree back to disk! Now that we know a bit about codemods, let’s look at a practical example. Instagram has been working hard on getting to a fully typed codebase, and we’ve made a lot of progress using codemods. Given a body of untyped functions, we can try to generate return types by simple inference! For example, if a function returns only one primitive type, we assign it that return type. If a function returns the result of a boolean operation such as a comparison or “isinstance”, we assign it a bool return type. We found that in practice this is a pretty safe operation to perform across Instagram’s codebase. Well, what if a function doesn’t explicitly return or it implicitly returns None? if a function doesn’t explicitly return a type, assign it a None type. Unlike the last example, this can be more dangerous due to common developer patterns. For example, I might throw a NotImplemented exception in a base class method and return a string in all subclass overrides. Its important to note that all of these techniques are a heuristic, but they turn out to be right often enough to be useful! Let’s take it a step further. Instagram uses Pyre, a full-program static type checker similar to mypy, to verify types in our codebase. What if we used Pyre’s output in order to make this codemod more powerful? As you can see in the below Pyre output snippet, Pyre gives us basically everything we need to fix up annotations automatically! When pyre runs it constructs a detailed analysis of control flow in each function. So, it can sometimes have a pretty good guess as to what an unannotated function should be returning. This means that if pyre thinks that a function returns a simple type, then we assign it that return type. However, now we potentially have to handle imports. Which means we have to know if something is already imported or is defined locally. I’ll touch a little bit on how we figure that out later. What benefit do we get from automatically adding easy to infer types? Well, types are documentation! If a function is fully typed, developers don’t have to read the code to understand its pre- and post-conditions. Lots of us have run into Python code like this, and Instagram’s code is no exception. If get_description was not typed, I might have to look at multiple source modules to figure out what exactly this function returns. Even for simpler functions that are easy to infer, function definitions with types are much easier for humans to read. Also, Pyre doesn’t evaluate correctness inside a function body unless the function itself is fully annotated. In the below example, it would be nice to know that our call to some_function is going to crash before it gets to production. In this case we will end up learning the hard way because some_other_function is missing a return annotation! If we had auto-inferred a “None” return type using our heuristic, we would find out about this issue before it causes a problem. This example is obviously contrived, but the issue remains an important one for Instagram. If you have millions of lines of code, you end up missing seemingly obvious problems like this during reviews. At Instagram, these simple inferences got us an almost 10% boost to functions typed. That adds up to thousands upon thousands of functions that a human no longer has to fix up. The benefits of better typed code are obvious, but leads to yet another benefit: Having a fully-typed codebase unlocks even more advanced codemods. If we trust our type annotations then we can use Pyre to unlock additional possibilities. Lets look back at our function renaming example. What if the function we’re renaming is a class method, not a global function? If we pair type information we get from pyre with a rename codemod, we can suddenly fix up the call sites as well as the definition! In this example, since we know the left hand side of a.fn , we know that it is safe to rename to a.add . We can unlock more powerful codemods with scope analysis. Remember the problem from earlier, where adding type hints meant that we had to potentially add imports? If you have scope analysis then you know what types used in the file come from imports, which ones are defined locally, and which types are missing. Similarly, if you know that a global variable is shadowed by a function argument, you can avoid accidentally changing it if you are renaming that global variable. One thing has become clear in our quest to fix all of the problems at Instagram: Finding the code we want to modify is often more important than the modification itself. Often, people want to do simple things, such as renaming a function, adding an argument to a method or splitting a module up. None of these is hard to do, but with the size of our codebase it becomes impossible for a human to find every line that needs modifying. That’s why pairing the ability to codemod with robust static analysis is so important. It allows us to narrow down what pieces of the code we should consider in order to make codemods safer and more powerful. Many thanks to my team members: Jennifer Taylor (who co-wrote this article), Mark Vismonte, Jimmy Lai, Ray Zeng, Carl Meyer, Anirudh Padmarao, Keith Blaha, and Lee Bierman. If you want to learn more about this work or are interested joining one of our engineering teams, please visit our careers page , follow us on Facebook or on Twitter . Stories from the people who build @Instagram 1.5K 2 Python Static Analysis Linting Codemod Programming 1.5K claps 1.5K 2 Written by Stories from the people who build @Instagram Written by Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-08-16"},
{"website": "Instagram-Engineering", "title": "making direct messages reliable and fast", "author": ["Tommy Crush"], "link": "https://instagram-engineering.com/making-direct-messages-reliable-and-fast-a152bdfd697f", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts Over the last few years, Instagram Direct has grown to be a core part of sharing content and experiences with your close friends on Instagram. This isn’t a privilege we take lightly; it’s incredibly important that we deliver each message as reliably and as quickly as possible. The Internet transfers data at the Speed of Light, but in many cases, due to large distances and network inefficiencies, the human eye can still pickup on the delay between a request’s start and finish. Additionally, network requests can fail for a wide variety of reasons, such as connection-loss or transient server issues. In this blog post, we’ll discuss Instagram’s infrastructure on iOS and Android to not only retry these actions, but also to make the whole experience feel much faster and reliable to the user. Often, when someone is building a mobile app that wants to “mutate” state on the server (such as tell the server to send a message), they have their view layer initiate a network request. It doesn’t take long before the developer realizes that this request could easily take a second or two even in good network conditions. A common pattern used to make it seem like the application is fast and responsive (i.e., reduce the user-perceived latency) is to “optimistically” guess the expected output of the successful request and immediately apply it to the view — all before the request is even made. We call this concept “optimistic state”. In this iOS example, I have an app that stores a color. The existing color, Red, is stored in _savedColor , but when MyViewController has set off a network request to change it, the app immediately overwrites the view's color to Blue, in the _updatingToColor value. This makes the app feel much faster than waiting for the request to complete. This pattern, however, becomes unmanageable as the application grows. If I leave MyViewController, the other views in the app that depend on the same color value don't reflect this ongoing request. This confuses the user, and makes the app look inconsistent, buggy, and slow! To handle this, many developers simply apply the Color change to the app's global data-cache. In fact, Direct also used to apply optimistic changes to our global caches. But this poses many problems, too. What happens if some other event (such as fetching the Color from network) overwrites my ongoing-Blue color back to Red? This concept is referred to as “ Clobbering ”. It creates weird experiences for the user, and it's difficult for developers to debug/reproduce. Additionally, tying a network request to a short-lived ViewController causes its own set of issues. If my request fails for a retriable reason, such as a loss of network, we should be able to perform this request again later, even if MyViewController is deallocated. As you can quickly see, optimistic state and network-request retry-logic are easy to build, but difficult to get right. Given the number of different network conditions we must operate within, and the number of product surfaces we must support, building a consistent retry and optimistic state policy is a difficult task. To solve these problems, we built a piece of infrastructure that we call the Mutation Manager. The Mutation Manager is designed to answer the questions above. Specifically, we wanted to make it effortless for mobile engineers to get: Intelligent and customizable auto-retry strategies for their network requests, with backoff behavior and retries across cold-starts. Optimistic State applied to all surfaces of the application, and free lifecycle management (adding, removing, handling clobbering, etc). Direct’s Mutation Manager (or short: DMM) achieves these goals (and more) by creating a centralized service that owns the network requests, serializes them to disk for retries, and safely manages the flow of their resulting optimistic state. In Instagram Direct, all surfaces implement this pattern. Let’s follow an example: imagine you navigate into a Direct thread with a message your friend just sent you. In that scenario, these steps occur: Submit a “Mark Thread as Read” mutation to the DMM. The DMM saves an entry into the OptimisticState cache. This entry is an instruction object, which describes the desired data change before the data is given to any UI. Mutation is saved to disk, in case we need to retry after an app termination such as a crash. The UI will then use ViewModels, which represent the merged state of the published-data and the entry that was saved to the OptimisticState cache. The network request is sent out. Each mutation has a MutationToken in its payload, which is a unique id created on the client. Once we have received the new confirmed state (with the matching MutationToken) from the server and updated the published-data, we remove and thus stop applying the entry from the optimistic state cache. After all our mutations and surfaces were migrated to this pattern, optimistic state became an afterthought of product development, and yet all UX surfaces remain consistent and the app feels fast to the user. Since optimistic state and server data are stored separately, and only merged on-the-fly at the View layer, Clobbering is impossible. Of course, nothing is free. The amount of client-side processing happening here has definitely increased. But, in practice, we’ve been able to mitigate any performance issues by keeping the application of the Optimistic State entry to the View Model as cheap as possible. The DMM also preserves the order in which requests were sent, so mutations that should be visually ordered now get this support for free; for example, the DMM will only send the messages in the exact order in which the API was invoked, sending messages in the order that users expect a messaging service to work. As seen above, there are many benefits to centralizing mutations and the flow of optimistic state. The Mutation Manager not only enforces good patterns for the app, but its also makes adding new mutations to this system extremely simple and quick. When adding a new mutation type, the compiler will guide you to answer all necessary questions about this request (network payloads, optimistic entries, etc). This ensures that as our team grows, our UX remains performant and reliable. Let’s take the “Mark Thread as Read” mutation as an example. Previously, this mutation applied optimistic state directly to the Server-Data cache. As a result, that data could accidentally be clobbered back to an Unread state. To prevent this, we introduced merging logic directly in the Server-Data cache, which, while functional, was unfortunately quite complicated. However, once the mutation was moved onto the DMM, not only did it drastically simplify the merging logic, but it also resulted in a more consistent experience for the user. Additionally, requests within the Mutation Manager are easier to debug. In our employee-only debug builds, the Mutation Manager logs events to a file that can be uploaded by the bug reporter. The engineer is then able to easily parse these logs and diagnose the request. In this example, we can see the request failed on the first attempt with a 500 error code, retried a second later, and succeeded. As you can see, this infrastructure allows our product teams to move quickly without compromising performance and reliability. Check out our open engineering roles to join a fast-moving team at Instagram today. Stories from the people who build @Instagram 670 3 Instagram iOS Android Software Engineering Messaging 670 claps 670 3 Written by Software Engineering @ Instagram (@tommycrush) Stories from the people who build @Instagram Written by Software Engineering @ Instagram (@tommycrush) Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-06-05"},
{"website": "Instagram-Engineering", "title": "making instagram com faster part 2", "author": ["Glenn Conner"], "link": "https://instagram-engineering.com/making-instagram-com-faster-part-2-f350c8fba0d4", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts In recent years instagram.com has seen a lot of changes — we’ve launched stories, filters, creation tools, notifications, direct messaging and a myriad of other features and enhancements. However, as the product grew, one unfortunate side effect is that our web performance began to suffer. Over the last year we made a conscious effort to focus on improving this. This ongoing effort has thus far resulted in almost 50% cumulative improvement to our feed page load time. This series of blog posts will outline some of the work we’ve done that led to these improvements. In part 1 , we showed how using link preloads allows us to start dynamic queries earlier in the page load i.e. before the script that will initiate the request has even loaded. With that said, issuing these requests as a preload still means that the query will not begin until the HTML page has begun rendering on the client, which means the query cannot start until 2 network roundtrips have completed (plus however long it takes to generate the html response on the server). As we can see below for a preloaded GraphQL query, even though it’s one of the first things we preload in the HTML head, it can still be a significant amount of time before the query actually begins. The theoretical ideal is that we would want a preloaded query to begin execution as soon as the request for the page hits the server. But how can you get the browser to request something before it has even received any HTML back from the server? The answer is to push the resource from the server to the browser, and while it might look like HTTP/2 push is the solution here, there is actually a very old (and often overlooked) technique for doing this that has universal browser support and doesn’t have any of the infrastructural complexities of implementing HTTP/2 push. Facebook has been using this successfully since 2010 (see BigPipe ), as have other sites in various forms such as Ebay — but this technique seems to be largely ignored or unused by developers of JavaScript SPAs. It goes by a few names — early flush, head flushing, progressive HTML — and it works by combining two things: HTTP chunked transfer encoding Progressive HTML rendering in the browser Chunked transfer encoding was added as part of HTTP/1.1, and essentially it allows an HTTP network response to be broken up into multiple ‘chunks’ which can be streamed to the browser. The browser then stitches these chunks together as they arrive into a final completed response. While this does involve a fairly significant change to how pages are rendered on the server side, most languages and frameworks have support for rendering chunked responses (in the case of Instagram we use Django on our web frontends, so we use the StreamingHttpResponse object). The reason this is useful is that it allows us to stream the contents of an HTML page to the browser as each part of the page completes — rather than having to wait for the whole response. This means we can flush the HTML head to the browser almost immediately (hence the term ‘early flush’) as it doesn’t involve much server-side processing. This allows the browser to start downloading scripts and stylesheets while the server is busy generating the dynamic data in the rest of the page. You can see the effect of this below. Additionally, we can use chunked encoding to send data to the client as it completes. In the case of server-side rendered applications this could be in the form of HTML, but we can push JSON data to the browser in the case of single page apps like instagram.com. To see how this works, let’s look at the naive case of a single page app starting up. First the initial HTML containing the JavaScript required to render the page is flushed to the browser. Once that script parses and executes, it will then execute an XHR query which fetches the initial data needed to bootstrap the page. This process involves multiple roundtrips between the server and client and introduces periods where both the server and client are sitting idle. Rather than have the server wait for the client to request the API response, a more efficient approach would be for the server to start working on generating the API response immediately after the HTML has been generated and to push it to the client. This would mean that by the time the client has started up the data would likely be ready without having to wait for another round trip. The first step in making this change was to create a JSON cache to store the server responses. We implemented this by using a small inline script block in the page HTML that acts as a cache & lists the queries that will be added to this cache by the server (this is shown in a simplified form below). After flushing the HTML to the browser the server can execute the API query itself and when it completes, flush the JSON data to the page as a script tag containing the data. When this HTML response chunk is received and parsed by the browser, it will result in the data being inserted into the JSON cache. A key thing to note with this approach is that the browser will render progressively as it receives response chunks (i.e. they will execute complete script blocks as they are streamed in). So you could potentially generate lots of data in parallel on the server and flush each response in its own script block as it becomes ready for immediate execution on the client. This is the basic idea behind Facebooks BigPipe system where multiple independent Pagelets are loaded in parallel on the server and pushed to the client in the order they complete. When the client script is ready to request its data, instead of issuing an XHR request, it first checks the JSON cache. If a response is present (or pending) it either responds immediately, or waits for the pending response. This has the effect of changing the page load behavior to this: Compared to the naive loading approach, the server and client can now do more work in parallel — reducing idle periods where the server and client are waiting on each other. The impact of this was significant: desktop users experienced a 14% improvement in page display completion time, while mobile users (with higher network latencies) experienced a more pronounced 23% improvement. In part 3 we’ll cover how we further improved performance by taking a cache first approach to rendering data. If you want to learn more about this work or are interested joining one of our engineering teams, please visit our careers page , follow us on Facebook or on Twitter . Stories from the people who build @Instagram 1K 3 JavaScript Instagram Web Performance Web Development 1K claps 1K 3 Written by Interested in games, art, technology, and distributing the future. Stories from the people who build @Instagram Written by Interested in games, art, technology, and distributing the future. Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-09-06"},
{"website": "Instagram-Engineering", "title": "adaptive process and memory management for python web servers", "author": ["Qi Zhou"], "link": "https://instagram-engineering.com/adaptive-process-and-memory-management-for-python-web-servers-15b0c410a043", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts At Instagram, process and memory management is a frequent point of consideration in building large-scale, distributed, fault-tolerant systems. This need is augmented by many of our long-running applications, responsible for servicing and handling the requests of hundreds of millions of users. Since a large part of our stack is built on Python, one common approach to this problem is the pre-fork worker process model, where worker processes are forked before a request comes in. However, while this approach simplifies the development and deployment experience, it is not without challenges. For example: How should we best balance number of worker processes and memory utilization for optimal server throughput? How should we reduce redundant work among worker processes? Since in most cases server processes occupy the majority of host memory, how can we ensure that it is within a healthy range? We’ve done work on optimizing the process and memory management of our uWSGI pre-fork servers, and we’re excited to share some of our learnings. uWSGI has a relatively simple but effective process model: Step 1: At start time, the uWSGI master reads in a static config Step 2: The uWSGI master forks a certain number of worker processes Step 3: Each worker process handles one request at a time Note that there are warmup costs associated with this model as well as early computations which every worker process needs to perform. Subsequently, having every worker process do this individually would be redundant and increase overall memory usage. To mitigate this, we leveraged shared memory between master and worker processes. Nonetheless, over time — due to memory leaks, cache, memory fragmentation, etc… — the memory consumption of the worker processes kept increasing. This posed a problem as an increase in memory consumption meant decreased system reliability and efficiency (e.g. more prone to out-of-memory errors or more swapping events). In response, we periodically respawned worker processes to reclaim memory and to ensure optimal and healthy memory utilization. Previously we used two per-worker thresholds to control respawn: reload-on-rss and evil-reload-on-rss . Initially, the master process allocates a piece of shared memory which contains an entry for each worker. A worker process starts a background thread to collect its RSS usage and updates its entry in shared memory. At the end of each request, a worker process checks whether its RSS is higher than reload-on-rss . If yes, the worker process performs some cleanup tasks, sets a deadline for itself and calls exit() . The master process checks the status of the workers against the following conditions: If a worker initiated the exit process but didn’t finish before the deadline, the master process will sigkill it to avoid bigger issues. If the RSS of a worker exceeds evil-reload-on-rss , master process will sigkill it too. As part of the operation, the master process also detects dead workers and will try to respawn new ones. However, since worker respawn is expensive (i.e. there are warm-up costs, LRU cache repopulation, etc..), an additional goal is to reduce respawn rate as much as possible. Previously, this was primarily achieved by increasing available memory and adjusting worker respawn thresholds (and thus their frequency). Legacy uWSGI respawn is based on worker RSS, which includes both shared and private memory. As mentioned earlier, the worker processes share a lot of pages with the master process. High RSS doesn’t mean reclaimable memory is also high. If the host has plenty of free memory, some worker respawns could be unnecessary. uWSGI processes consume most of the host physical memory. For large applications like Instagram, resource usage varies widely across different hosts (due to varying cluster workload, traffic fluctuations, etc..) Therefore, the N-per-worker-thresholds-based respawn is an unsuitable control for host level memory usage. uWSGI uses a static config to decide the number of worker processes. However, most of the time the number of required workers is considerably lower than the maximum number of allowed workers. As a result, ~20% to 40% of the workers remain idle for ~90% of the time. Not only is this wasted memory, but it also causes higher process contention , and TLB and cache pollution . As part of addressing the problems outlined above, our guiding principles were (a) reduce uWSGI respawn rate, (b) prevent resource waste, and (c) have a tight control on host memory usage to ensure the health of the system. As a result, we made the following changes to uWSGI: We implemented host level memory utilization-based uWSGI respawn. There are still two thresholds, but they are based on host level memory utilization. For illustrative purposes, let’s call them L (for Lower bound) and U (for Upper bound). Each worker process still runs a background thread to update its memory usage in shared memory. Master loop checks current host level memory utilization. If it’s greater than L , then the master process picks a certain number of workers with the highest memory usage and sets a flag in their corresponding entries in shared memory. At the end of each request, a worker checks this flag instead and acts accordingly. If host memory utilization is greater than U then master would sigkill the workers immediately. The number of workers to kill is decided by a simple quadratic algorithm. The closer it is to the upper limit, the more workers we’d like to kill to avoid OOM. The uWSGI master monitors the size of the tcp connection backlog. The expectation is that workers are able to quickly process requests, and that the queue is short. When process parallelism is more than needed, however, we may empty the listen queue while there could be some workers still sitting idle and consuming physical memory. To make this logic smarter, we implemented listen queue-based adaptive uWSGI respawn. Specifically, if there are at least certain number of idle workers, and if the listen queue is nearly empty, the master process will delay worker respawn. Otherwise, it’ll spawn additional worker processes (up to some limit). Both changes have proven to be successful. They have improved efficiency through better memory utilization, avoiding unnecessary worker respawns, and reduced costs related to unnecessary worker processes. As a result, we were able to achieve more than 11% capacity improvement. The changes also removed some spikiness in host memory utilization. It has been in much better shape now and has helped improve site reliability. Thanks to Jianlong Zhong , Lisa Guo , Matt Page , Carl Meyer , Raymond Ng for their insights and contributions to these changes. Qi Zhou is a software engineer at Instagram. Stories from the people who build @Instagram 382 1 Programming Instagram Engineering Python Scaling 382 claps 382 1 Written by Stories from the people who build @Instagram Written by Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-07-15"},
{"website": "Instagram-Engineering", "title": "making instagram com faster part 1", "author": ["Glenn Conner"], "link": "https://instagram-engineering.com/making-instagram-com-faster-part-1-62cc0c327538", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts In recent years, instagram.com has seen a lot of changes — we’ve launched stories, filters, creation tools, notifications, and direct messaging as well as myriad other features and enhancements. However, as the product grew, one unfortunate side effect was that our web performance began to suffer. Over the last year, we made a conscious effort to improve this. Our ongoing efforts have thus far resulted in almost 50% cumulative improvement to our feed page load time. This series of blog posts will outline some of the work we’ve done that led to these improvements. Correctly prioritizing resource download and execution and reducing browser downtime during the page load is one of the main levers for improving web application performance. In our case, many of these types of optimizations proved to be more immediately impactful than code size reductions, which tended to be individually small and only began to add up after many incremental improvements (though we will talk about these in future installments). They were also less disruptive to product development, requiring less code change and refactoring. So initially, we focused our efforts in this area, beginning with resource prefetching. As a general principle, we want to inform the browser as early as possible about what resources are required to load the page. As developers, we often know what resources we are going to need ahead of time, but the browser may not become aware of those until late in the page loading process. These resources mainly include those that are dynamically fetched by JavaScript (other scripts, images, XHR requests etc.) since the browser is unable to discover these dependent resources until it has parsed and executed some other JavaScript first. Instead of waiting for the browser to discover these resources itself, we can provide a hint to the browser that it should start working on fetching those resources immediately. The way we do this is by using HTML preload tags. They look something like this: At Instagram, we use these preload hints for two types of resources on the critical page loading path: dynamically loaded JavaScript and preloading XHR GraphQL requests for data. Dynamically loaded scripts are those that are loaded via import('...') for a particular client-side route. We maintain a list of mappings between server-side entrypoints and client-side route scripts — so when we receive a page request on the server-side, we know which client-side route scripts will be required for a particular server-side entrypoint and we can add a preload for these route specific scripts as we render the initial page HTML. For example, for the FeedPage entrypoint, we know that our client-side router will eventually make a request for FeedPageContainer.js, so we can add the following: Similarly, if we know that a particular GraphQL request is going to be made for a particular page entrypoint, then we should preload that XHR request. This is particularly important as these GraphQL queries can sometimes take a long time and the page can't render until this data is available. Because of this, we want to get the server working on generating the response as early as possible in the page lifecycle. The changes to the page load behavior are more obvious on slower connections. With a simulated fast 3G connection (the first waterfall below -without any preloading), we see that FeedPageContainer.js and its associated GraphQL query only begin once Consumer.js has finished loading. However, in the case of preloading, both FeedPageContainer.js and its GraphQL query can begin loading as soon as the page HTML is available. This also reduces the time to load any non-critical lazy loaded scripts, which can be seen in the second waterfall. Here FeedSidebarContainer.js and ActivityFeedBox.js (which depend upon FeedPageContainer.js) begin loading almost immediately after Consumer.js has completed. In addition to starting the download of resources sooner, link preloads also have the additional benefit of increasing the network priority of async script downloads. This becomes important for async scripts on the critical loading path because the default priority for these is Low. This means that XHR requests and images in the viewport will have higher network priority, and images outside the viewport will have the same network priority. This can cause situations where critical scripts required for rendering the page are blocked or have to share bandwidth with other resources (if you’re interested, see here for an in-depth discussion of resource priorities in Chrome). Careful use (more on that in a minute) of preloads gives an important level of control over how we want the browser to prioritize content during initial loads in cases where we know which resources should be prioritized. The problem with preloads is that with the extra control it provides, comes the extra responsibility of correctly setting the resource priorities. For example, when testing in regions with very slow overall mobile and wifi networks and significant packet loss, we noticed that <link rel=\"preload\" as=\"script\"> network requests for scripts were being prioritized over the <script /> tags of the JavaScript bundles on the critical page rendering path, resulting in an increase in overall page load time. This stemmed from how we were laying out the preload tags on our pages. We were only putting preload hints for bundles that were going to be downloaded asynchronously as part of the current page by the client-side router. In the example for the logged out page, we were prematurely downloading (preloading) SomeConsumerRoute.js before Common.js & Consumer.js and since preloaded resources are downloaded with the highest priority but are not parsed/compiled, they blocked Common & Consumer from being able to start parsing/compiling. The Chrome Data saver team also found similar issues with preloads and wrote about their solution here . In their case, they opted to always put preloads for async resources after the script tag of the resource that requests them. In our case we opted for a slightly different approach. We decided to have a preload tag for all script resources and to place them in the order that they would be needed. This ensured that we were able to start preloading all script resources as early as possible in the page (including synchronous script tags that couldn’t be rendered into the HTML until after certain server side data was added to the page), and ensured that we could control the ordering of script resource loading. One of the main surfaces on instagram.com is the Feed, consisting of an infinite scrolling feed of images and videos. We implement this by loading an initial batch of posts and then loading additional batches as the user scrolls down the feed. However, we don’t want the user to wait every time they get to the bottom of the feed (while we load a new batch of posts), so it’s very important for the user experience that we load in new batches before the user hits the end of their current feed. This is quite tricky to do in practice for a few reasons: We don’t want off-screen batch loading to take CPU and bandwidth priority away from parts of the feed the user is currently viewing. We don’t want to waste user bandwidth by being over-eager with preloading posts the user might not ever bother scrolling down to see, but on the other hand if we don’t preload enough, the user will frequently hit the end of feed. Instagram.com is designed to work on a variety of screen sizes and devices, so we display feed images using the img srcset attribute (which lets the browser decide which image resolution to use based on the users screen size). This means it's not easy to determine which image resolution we should preload & risks preloading images the browser will never use. The approach we used to solve the problem was to build a prioritized task abstraction that handles queueing of asynchronous work (in this case, a prefetch for the next batch of feed posts). This prefetch task is initially queued at an idle priority (using requestIdleCallback), so it won’t begin unless the browser is not doing any other important work. However if the user scrolls close enough to the end of the current feed, we increase the priority of this prefetch task to ‘high’ by cancelling the pending idle callback and thus firing off the prefetch immediately. Once the JSON data for the next batch of posts arrives, we queue a sequential background prefetch of all the images in that preloaded batch of posts. We prefetch these sequentially in the order the posts are displayed in the feed rather than in parallel, so that we can prioritize the download and display of images in posts closest to the user’s viewport. In order to ensure we actually prefetch the correct size of the image, we use a hidden media prefetch component whose dimensions match the current feed. Inside this component is an <img> that uses a srcset attribute, the same as for a real feed post. This means that we can leave the selection of the image to prefetch up to the browser, ensuring it will use the same logic for selecting the correct image to display on the media prefetch component as it does when rendering the real feed post. It also means that we can prefetch images on other surfaces on the site — such as profile pages — as long as we use a media prefetch component set to the same dimensions as the target display component. The combined effect of this was a 25% reduction in time taken to load photos (i.e. the time between a feed post being added to the DOM and the image in that post actually loading and becoming visible) as well as a 56% reduction in the amount of time users spent waiting at the end of their feed for the next page. Stay tuned for part 2: Early flushing and progressive HTML (and why you don’t necessarily need HTTP/2 push for pushing resources to the browser). If you want to learn more about this work or are interested in joining one of our engineering teams, please visit our careers page , follow us on Facebook or on Twitter . Stories from the people who build @Instagram 2.9K 4 Web Development Web Performance Instagram 2.9K claps 2.9K 4 Written by Interested in games, art, technology, and distributing the future. Stories from the people who build @Instagram Written by Interested in games, art, technology, and distributing the future. Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-08-01"},
{"website": "Instagram-Engineering", "title": "improving instagrams music audio quality", "author": ["Chris Hsu"], "link": "https://instagram-engineering.com/improving-instagrams-music-audio-quality-284e555102e9", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts Instagram plays a critical part in forming meaningful communities where people can connect with each other and share what matters most to them. To help best facilitate these connections, we craft our app with high quality sharing experiences that we can take pride in. One way we work hard to improve the Instagram experience is by improving audio quality. Instagram’s Music Sticker song suggestions for the pop music genre Audio quality is a measure of how closely the audio we deliver to Instagram apps matches the original uncompressed audio file. Instagram delivers compressed audio to enable smooth video playback with fewer stalls caused by rebuffers. However, in exchange for smoother playback, this introduces the risk of compression artifacts. Some examples of compression artifacts are reduced clarity in high frequency sounds, weaker bass, and noise. These differences collectively lower the audio quality perceived by listeners. Instagram’s video system has access to multiple levers that affect audio quality. The audio codec selection, sample rate, and bitrate all contribute to the quality of the audio encoding. Different audio codecs have different levels of lossy compression , and they perform differently on different types of content. With the scale and range of Instagram’s content, it’s important to rigorously evaluate which codecs best fit the content and install metrics to track audio quality. Instead of potentially focusing plenty of engineering time to build an audio quality metric, we pursued the simple solution first and aimed to demonstrate that Instagram listeners cared about audio quality via existing engagement metrics. Changing the audio codec was not the simplest solution, so we decided to keep AAC as our audio codec selection for our audio quality improvement experiment. Sample rate affects the upper bound of frequencies that our audio encodings can represent correctly. The Nyquist-Shannon Sampling Theorem says that: “A band limited continuous-time signal can be sampled and perfectly reconstructed from its samples if the waveform is sampled over twice as fast as its highest frequency component.” Instagram uses an industry standard 44.1kHz sample rate, more than enough to convey the 20kHz max that most people can hear, so we ruled out sample rate as a variable worth changing. Bitrate , measured as kilobits per second (kbps), varies linearly with the number of bits in the audio file. In other words, a higher bitrate means more data and less compression in the audio encoding. This allows the compressed audio encoding to retain more features of the original audio file with fewer compression artifacts. When the bitrate is too low, the encoder removes audio details that it considers less important. Since we kept the audio codec and sample rate constant, and bitrate was simple to change, we chose to vary the bitrate in our audio quality improvement experiment. Prior to our audio quality improvement efforts, Instagram’s default bitrate for audio in videos was 64kbps. The microphone on a phone doesn’t produce a rich audio signal, so despite the low bitrate, Instagram’s audio compression performed well for most content. However, as Instagram creators started posting studio-produced audio content (e.g. music recordings), it became clear that 64kbps was not sufficient for delivering high quality audio. We received reports that Instagram’s audio sounded “blown out” or too low quality for artists to want to share certain songs on Instagram. When we tested the Instagram app, we observed common compression artifacts. For example, in Instagram’s Music Sticker Stories , we noticed that the compressed audio for snare drums, cymbals, voice, and reverb sounded drier and thinner than they did in the original recordings. We unfortunately can’t simply increase bitrate for all content. We need to split bandwidth between audio and video because of limited overall bandwidth, so this is a zero-sum game. High quality video has a bitrate so high that the difference between 64kbps and 128kbps audio has a negligible impact on playback rebuffers. However, in low bandwidth situations we serve video at much lower bitrates. In these situations, a difference of 64kbps can be substantial in the playback experience. While we can increase the audio bitrate, we must weigh the tradeoffs between audio quality and video quality. Increasing this bitrate for all content is particularly risky, since we know that most content has simple audio and will not benefit from the audio side of the tradeoff. In our experiment, we aimed to make the right quality tradeoff for the right content. To find the strongest signal on Instagram listeners’ preferences for audio quality, we considered ways to focus our audio quality improvements. From our previous experiments on visual quality, we knew that quality of experience is subjective and unique to content type and community type. Audio quality sensitivity depends on each listener’s attention to audio details and the quality of the playback speaker (e.g. the device’s default external speaker or headphones). We worried that some Instagram listeners with low-end mobile phone speakers may not focus on general audio quality. Musicians, on the other hand, know Instagram as a platform where they can create music communities, so we suspected that many Instagram listeners would be sensitive to music audio quality. We expected to see the strongest correlations between audio quality and engagement in Instagram’s music content where the audio frequency range is wide and full. To obtain this signal, we ran a targeted audio quality improvement test on the product where we expected audio quality to make the biggest impact: Music Sticker Stories. a music sticker that plays a song by Relient K To avoid diluted results from non-music content, we leveraged Instagram’s video and audio encoding tag system to zoom in on Stories audio encodings in the A/B test. All audio encodings in the control group used our default 64kbps bitrate. We ran two test groups: one group where the audio encodings used a 96kbps bitrate and another group where the audio encodings used a 128kbps bitrate. In the experiment results, we saw clear engagement wins from improved audio quality in Music Sticker Stories. The 128kbps test group delivered the best results. We measure video engagement by watch time (i.e., time spent watching videos) and view percent (i.e., the percentage of a video a viewer finishes watching). Both watch time and view percent improved despite regressions in visual quality and rebuffers. We expected the regressions in visual quality and rebuffers because we shifted our bandwidth usage from video to audio. However, the engagement metric wins exceeded our expectations. These metrics demonstrated that Instagram viewers are more willing to watch complete Music Sticker Stories videos even with playback performance regressions because the audio quality is better. Increasing the audio bitrate for Music Sticker Stories is only the beginning of delivering a personalized video quality of experience to the Instagram community. To help us make the right tradeoffs between audio quality, visual quality, and smooth playback, we are considering future plans to build bandwidth aware audio ABR (i.e., adaptive bitrate) and content identification (i.e., identifying which video content has music). Many thanks to my great team members: Donald Chen, Haixia Shi, Chris Ellsworth, Bill Phillips, Mackenzie Pearson, who helped to make this happen. Donald Chen (Android) and Chris Hsu (Server) are software engineers on Instagram Media Infrastructure team. If you want to learn more about this work or are interested joining one of our engineering teams, please visit our careers page , follow us on Facebook or on Twitter . Stories from the people who build @Instagram 443 4 Music Audio Quality Video Technology Instagram Engineering 443 claps 443 4 Written by Building something @ Instagram Media Infra Stories from the people who build @Instagram Written by Building something @ Instagram Media Infra Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-06-28"},
{"website": "Instagram-Engineering", "title": "video upload latency improvements at instagram", "author": ["Ryan Peterman"], "link": "https://instagram-engineering.com/video-upload-latency-improvements-at-instagram-bcf4b4c5520a", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts In June 2013 Instagram introduced video uploads. At the time the system was simple. To ingest video and make it available for playback, we had the Instagram app upload the entire video file to the server once the client had finished recording it. Then we would transcode the video to a controlled set of video versions of different qualities to make sure the video file was playable on as many devices as possible. Once all the video versions were available, we “published” the video and made it available for viewing. At Instagram, our community is sensitive to upload times. Users want to see their video stories and direct video messages made available for others as soon as possible. For that reason, upload latency is an important metric at Instagram. Over the years we have developed strategies for reducing that latency. Let’s start by defining upload latency for this article as the time it takes once the server has received all video bytes from the client, until the video is made “publishable” or available for viewing. A simple improvement to reduce video upload latency is to only do the minimal work necessary before a video is considered publishable. The idea is, instead of blocking until all video versions are available, we can publish the video once the highest-quality video version is available. The rest of the video representations are not mandatory for playback, but provide a playback experience with less stalling for our users with bandwidth constraints. This reduces latency in cases where the lower-quality versions took longer than the highest-quality version to process, and increases the success rate of video uploads, since we only depend on one version instead of all versions. We represent our video data model with a graph-based storage system. All video versions are attached to a “video asset” parent node, which allows us to reason about the video at the uploaded media level instead of the video version level. This abstraction enables simple and unified publishing logic. To implement the above mentioned improvement, we mark the video asset as “publishable” by flipping a boolean in the video asset when the callback is received from our video processing service. By making the publishing signal only depend on this one version, we open the opportunity to make our video processing model more resilient and flexible. If the pipeline fails to produce the optional encodings for some time, we can still allow the videos to be published since we have the highest quality version available. Later, we can fill in the rest of the versions on demand. One tradeoff of making the publishing signal only depend on the highest-quality version is that users with bandwidth constraints may experience a suboptimal playback experience until the rest of the versions are complete. For instance, the video will be published as soon as the highest-quality version is ready, but there may be a few lower-quality versions still being processed that will not be available until later. When users with bandwidth constraints view the video initially, they may experience a higher stall rate if only the highest-quality video is available. In practice, in a significant majority of cases, the remainder of the encodings are made available soon after the mandatory version is complete. Another approach to make video uploads faster is to ask the client to cut up the video after it has been recorded. Once the video is chunked up into segments, the client uploads them to the server and labels each segment with an index so they can be recombined in order later. When the server receives the segments, it does the transcoding process in parallel thereby saving time. Once the segments are all transcoded, we combine them together so they are available for playback. On the server side, we structure each video processing pipeline as a directed acyclic graph. Each node is a unit of execution that runs on a worker machine and each edge represents a dependency between two nodes. Each node runs once all of its dependencies have finished. As an example here is a simplified video processing pipeline execution for a basic nonsegmented pipeline: In this example pipeline, the majority of the work that is happening is in the transcoding node. If we can parallelize that portion, then we can reduce upload latency significantly. Our segmented pipeline aims to parallelize that portion by adding a transcoding task per segment. Then we add a stitch task, which concatenates the frames of each segment’s video and places the resulting video in a new video container. This stitch task depends on each per segment task in the pipeline as follows: Segmented uploads reduce upload latency in many cases but come with a few tradeoffs. For instance, segmented uploads increase the complexity of the pipeline. There are some quality metrics that are only available per segment at transcode time, such as SSIM. These metrics are not helpful to us on a per segment basis. Therefore, we need to do a duration weighted average of the SSIM of all segments to come up with the SSIM of the whole video. Similarly, handling exceptions is more complex since there are more cases to handle. Also by segmenting up the video, we have introduced another step in the pipeline that stitches all the transcoded segments together. This requires additional CPU that wasn’t necessary in the nonsegmented case as well as is another step, which could fail, causing our success rate to decrease. More importantly, the stitching step increases IO requirements significantly in the resulting system. Each segment is transcoded on an individual worker machine. When stitching the segments together, we want to have all those segments available locally on another worker machine that is performing the stitch. Therefore, that other worker machine must download all the segments from the network, which greatly increases IO utilization. On segment length, the smaller we make the segments, the more work we can do in parallel. However since there is some fixed overhead in setting up a worker machine to transcode a segment, we want to keep the segment length above a certain threshold. If we make the segments too small then we are wasting resources just allocating machines to do tiny amounts of work. In practice setting the segment length to something on the order of a few seconds works well for us. In addition, this isn’t always a net win in terms of upload latency. The benefits of segmented uploads diminish as the initial video gets shorter. For instance, below depicts a comparison between nonsegmented video processing and segmented video processing plotted against time for a short video and a long video. For both I assume that video processing time is directly proportional to the length of the video. Δt is the upload latency win between segmented and nonsegmented pipeline execution. The win is much smaller for the shorter video compared to the long video: Overall, we make the decision to segment the video at the beginning of the upload process depending on the product and the length of the video. Some video products, such as stories, have enforced length maximums that are short enough that segmenting isn’t worth the complexity. On the other hand, for video products like IGTV, a minimum length is enforced that is long enough to make segmented uploads always worthwhile. Another performance optimization we use to improve the upload latency and save CPU utilization is something we call a “passthrough” upload. In some cases, the media that is uploaded is already ready for playback on most devices. If so, we can skip the video processing altogether and store the video directly into our data model. This reduces upload latency since we do not need to transcode the video in these cases. An important part of this approach is setting comprehensive checks in place so that the video that enters Instagram conforms to our standards for playback. In addition to the pipelines that transcode video, we add another pipeline that checks some of the properties of the passed-in video, such as its codec and bitrate, to confirm the video is eligible for passthrough. If the video has a less supported codec, then fewer Instagram users will be able to play the video. Similarly, if the bitrate is too high then loading the video for playback over the network will take too long. Once the codec and bitrate pass our eligibility criteria, we then check the video file with an internal tool that reports on the topology, consistency, and streams storage consistency. If the video file is inconsistent, we attempt to repair the original video file. This internal tool also reliably identifies buffer overflow attack scenarios so that we do not serve any malicious files to our users. At the end we transmux the repaired video with the original audio and store that in our data model: The resulting passthrough pipeline completes much more quickly than the transcoding pipelines. Since our checks guarantee that this video version is playable, we allow this callback to mark the video asset as publishable. This greatly improves our video processing latency and additionally improves the quality of the content since transcoding is a lossy process. The main tradeoff here is captured in the bitrate ceiling we have set for a few reasons. If the bitrate of the original video is too high and we perform a passthrough upload, then we will store a much larger file than we would have otherwise if we transcoded the video. Also, there are diminishing returns in visual quality as bitrate increases, which is even more apparent when these videos are played on mobile devices with limited screen size. In the case of high-bitrate original videos, there is less visual quality win when comparing the passthrough version with the highest-quality transcoded version. In practice, our bitrate ceiling allows us to control these tradeoffs. All in all, passthrough uploads can be especially useful for our most latency-dependent video products such as direct video messaging. Over the years, video processing at Instagram has improved significantly. This infrastructure has provided much more value to our users in the form of efficiency, reliability, and quality. We are currently working on making the high-level procedures mentioned above even more efficient. One promising area is generating and purging encodings on demand as the video ages and interacts with the outside world. For instance, we may want to alter what representations a certain video has based on data such as popularity or how old the video is. Since older content isn’t viewed as much, we may not need to store all video versions. Instead we can just have a subset of the video versions for the small bit of traffic that looks far into the past. If an old video becomes suddenly popular then we may want to regenerate those versions on demand. Designing the system that manages what video representations we have over each video’s life cycle involves many interesting challenges. What signals should we choose to consume? How do we efficiently manage and iterate over the existing videos in a performant manner? There is high potential for impact considering the scale of Instagram, where many of our 1B+ users create or consume video every day. If this sounds interesting to you, join us! Many thanks to my team members, Runshen Zhu, Ang Li, Rex Jin, Haixia Shi, and Juehui Zhang, who helped build the above improvements to our infrastructure. Ryan Peterman is a software engineer on Instagram’s Media Infrastructure team in San Francisco. Stories from the people who build @Instagram 744 3 Tech Instagram Computer Science Engineering Video Technology 744 claps 744 3 Written by Software Engineer @ Instagram, www.ryanlpeterman.dev Stories from the people who build @Instagram Written by Software Engineer @ Instagram, www.ryanlpeterman.dev Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-06-13"},
{"website": "Instagram-Engineering", "title": "from fbu to swe my journey of proving myself to myself and how instagram helped me do it", "author": ["Instagram Engineering"], "link": "https://instagram-engineering.com/from-fbu-to-swe-my-journey-of-proving-myself-to-myself-and-how-instagram-helped-me-do-it-ea4a531a4a57", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts Written by Amy Huyen, 2-time intern at Instagram. On my first day of intern orientation last summer, I walked into Facebook wide-eyed and in awe of all the talent that surrounded me. I saw my peers networking with one another, discussing past internship experiences, and using technical jargon that I couldn’t understand. I felt like an impostor. I was an inexperienced freshman who had only been coding for 10 months. I had no industry experience, was only versed in Python, and couldn’t even begin to comprehend what tech at a big company was like. Unlike most software engineering interns, I had been admitted into Facebook’s freshmen-sophomore Facebook University program (“FBU”). Instead of working directly on company products, FBU interns spend three weeks in an intensive mobile development bootcamp, then five weeks developing their own mobile apps in teams of three. I should’ve been ecstatic to be surrounded by such brilliant individuals, but instead, I felt guilty. Like I had stolen someone’s spot — someone more qualified and more experienced than me. I spent the entirety of my 8-week internship battling these feelings of incompetence and fear. I worried that other interns would look at me and think, “How did she scheme her way into this company?” or “How did they let someone like her into the company?” Nevertheless, I shut these thoughts away and focused on creating the best Android app I could with my team. At the end of my internship, I walked away with an app that matched skilled volunteers to non-profit organizations, a return software engineering internship offer, and memories that would last me a lifetime. However, the impostor syndrome lingered. Now, more than ever before, I felt like I had to prove to my peers at school that my SWE internship offer was legitimate — that I had earned it because of my abilities, not because of my gender. I had to prove to the world that I was truly qualified for the role I had been offered. Moreover, I had to prove to myself that I was enough. I accepted my return offer with the determination to prove others wrong. In May of this year, I started my second internship at Facebook. I would work on the Instagram Growth team as an iOS intern, which was daunting because, for the second summer in a row, I was working on something completely foreign to me. During the weeks leading up to my internship, I worried that my decision to pursue an iOS role in the company was a mistake. I’d never coded in Objective-C before and was barely more experienced than I was last year. The imposter syndrome crept back in, and it was paralyzing . Fast forward 12 weeks, and now I’m at the end of my internship here. I’m a different, more confident version of myself. This summer, I spearheaded Instagram’s invites project for empty state surfaces on iOS, developed lasting relationships with the members of my team, and created real impact in the company. I learned about industry coding standards and workflows, immersed myself in Facebook’s culture. First, a quick summary of my work, and then the lessons I learned. My work this summer involved adding an invite unit to Direct and Search’s empty state using Instagram’s IGListKit framework . In Direct, I introduced two different user experiences: 1) adding an invite card to the end of the existing collection of activators, and 2) converting all the cards into rows to increase visibility of each of the calls to action: I also surfaced an invite row in Search’s empty state. When a user searches for an account that doesn’t exist, they are prompted to invite that friend to Instagram: I used to apologize profusely for mistakes in my code or for “wasting” others’ time with my stupid questions. However, apologies fall flat when you keep repeating the same mistakes. Stop apologizing and start acting on feedback instead . As I was building the invite unit to show to Instagram users in various empty state surfaces across the app, I encountered numerous challenges. My code quality was far from perfect, and my code reviewers often pointed out different areas where I could improve. During my first couple of weeks, I would apologize constantly for these nits, but I soon realized that words are just empty promises if they aren’t backed by actions. The best way to thank your peers for their time and patience is by showing them your progress. Stop apologizing and, instead, start proving to your peers and managers that their time wasn’t wasted on you. During my first few weeks, the thought of asking another engineer for help terrified me. Fear of judgement crippled me from the start, and the thought of being considered “annoying” held me back even more. I was so obsessed over the idea that everyone would be too busy to entertain me that I found myself writing, deleting, and rewriting messages to my manager when I needed help. I wasted so many hours of my life drafting these messages only to never send them out. Ultimately, I was so fixated on my own perception of myself that I never even considered how other engineers truly saw me. It wasn’t until I finally mustered up the courage to ask an iOS engineer on another team for advice that I realized just how eager everyone at Instagram is to help. Never did they give off the impression that I was a nuisance to them or interfering with their everyday work. In fact, they welcomed these opportunities to understand my project and get to know me as a coworker. From that day forward, I stopped drafting and redrafting my messages to my peers and manager. Instead, I just hit send fearlessly because I realized my time was just as valuable as theirs. However, another crucial thing I learned from asking questions is that asking questions isn’t enough. Listening is the most important part . As humans, we often prepare ourselves to respond while others are still speaking. We don’t take the time to acknowledge others’ words because we’re too adamant that our solutions and thoughts are the only ones that can be right. That’s an awful way of thinking. Not every conversation is a battle, and not every one of your solutions is the most optimal. The moment I decided to drop that mindset and truly listen to the words of my peers, I learned more than I ever could’ve imagined. So yes, ask your questions, but listen to those answers too. From the time I was young, I’ve been told to step out of my comfort zone, but it’s never spurred me into action. No one willingly throws themselves into uncomfortable situations, and I’m no different. For the first 8 weeks of my internship, I focused primarily on iOS client-side development. Although I had no previous experience in this area, I slowly but surely grew familiar with the codebase, and gained confidence in my abilities. However, when my manager Vivian encouraged me to try some server-side work towards the end of my internship, I was intimidated and hesitant to accept. A part of me was fearful of the new learning curve that I’d have to overcome again, this time during my ninth week, when I theoretically should be the most productive. I grew uncertain of my abilities again and was certain that I’d fail and disappoint my team. As it turns out, working on server code wasn’t all that different from learning the client code. As I did before in my first weeks, I analyzed the existing codebase, contacted other engineers at Instagram for assistance, and threw myself headfirst into the new project. Soon, without realizing it, the server-side code became a part of my comfort zone. What I’ve discovered is that adopting the mindset of conquering new obstacles instead of abandoning your existing accomplishments has done wonders in helping me tackle new challenges. If we all slowly chip away at the unknown one piece at a time, we can and will prevail. (Even writing this blog post required a lot of courage from me, as I am by no means a professional.) Perhaps my biggest takeaway from my internship at Instagram is that I am worthy of my position here. From working on the Growth team, I’ve gained a sense for how Instagram products are conceived, built, and delivered. I’ve developed a deep appreciation for all the steps that go into the delivery of a new feature onto our users’ screens. I’ve gained full stack development experience, had measurable impact on the product, and collaborated with a variety of professionals: designers, data scientists, product managers, and more. I’ve taken ownership of a feature and watched as this idea evolved from a design on my computer screen to a fully functioning feature on users’ devices. None of this would’ve been possible if I didn’t deserve my position here. None of this would’ve been possible if I had only gotten into Facebook because I was a girl. None of this would’ve been possible if I didn’t show my team that I had what it took to succeed as an intern here. So to all the hopeful interns out there who are confronting the same struggles as I did: stop trying to prove yourself to others when you haven’t even proven yourself to you . Step boldly into your internships and know your own worth because once you silence that nagging voice of doubt inside your head, everything else will fall into place. And if you can’t quite do that just yet, take a summer to unapologetically expand your comfort zone and surround yourself with people who celebrate your successes with you as I did this summer at Instagram. You’ll be amazed how much you’ll accomplish when you stop doubting your own worth and start believing the people who believe in you. Ultimately, I’ve grown so much over the course of these past twelve weeks, both technically and personally. Thanks to everyone on the Instagram Growth team for making my summer here an unforgettable one, and special thanks to Vivian Yang and Serena Jiang for their unwavering support. You’ve all helped me believe in myself, and for that I’m eternally grateful. Amy Huyen was a summer 2019 intern at Instagram. If you want to learn more about this work or are interested in joining one of our engineering teams, please visit our careers page , or follow us on Facebook or on Twitter . Stories from the people who build @Instagram 238 9 Internships Instagram Growth Mindset Impostor Syndrome Team 238 claps 238 9 Written by Stories from the people who build @Instagram Written by Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-08-23"},
{"website": "Instagram-Engineering", "title": "core modeling at instagram", "author": ["Thomas Bredillet"], "link": "https://instagram-engineering.com/core-modeling-at-instagram-a51e0158aa48", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts At Instagram we have many Machine Learning teams. While they all work on different parts of the product, they all directly contribute to Instagram’s mission of strengthening relationships through shared experiences. This post is about one such team: Core Modeling. Core Modeling’s mission is to increase the predictive power of models. This requires solid understanding of the fundamentals and models used, a mix of research and applied machine learning. Here we summarize a few battle-tested methods we use to improve our models. Those are part of our core modeling arsenal and are commonly used here at Instagram. Features are representations of data in a machine learning model. Some of them are more important to the model, some less, and it’s often hard to know in advance which is which. N-grams are a contiguous sequence of n items from a given sample of text or speech. Embeddings of n-grams using quantized dense features and categorical features that we jointly learn with the rest of our models have been very useful for us. There are two steps involved: creating the embeddings, and learning them with the model. To create these embeddings, we first estimate the target cardinality for the n-grams, mostly dictated by model size capacity. We select the features by feature importance and smoothness of distribution, because rough distributions are harder to quantize. The embeddings are then randomly initiated for each bucket of the n-grams. Finally, to jointly learn these embeddings, we have to make a model-dependent choice on how to combine these with the main prediction pipeline. For an MLP we could add the dot product of said embeddings with a hidden layer, then apply gradient descent . That would be a sparse neural network application. All in all this is useful to learn higher order interactions between those important features and the rest. It forces the model to acknowledge those high order relationships and have the extra benefit of finding interesting representations that could be used in KNN/retrieval settings for those n-grams. Learning low dimension representations of entities is not only useful for recommendations and retrieval, but also with helping models generalize better. Because most prediction models are trained on a specific surface belonging to a larger product, training entity embeddings representing the holistic product experience brings a valuable source of meta information to myopic models. (In Instagram for example, the home feed or Explore would be one surface, where Instagram is the larger product.) At Instagram we built different pipelines generating user embeddings that are used in both retrieval phases and point models . We mainly leverage two techniques to learn these embeddings: Word2Vec : We define the interactions that characterizes best our users, say a “like” is the interaction we are looking after. We then generate rows with list of users that have liked other users, for example if user1 liked user2 and user3 then we’d have a row with (user1, [user2, user3]). We define a notion of distance between users and train various versions of embeddings based on that idea. Deep Learning : Neural networks naturally learn representations of features in hidden layers we leverage a multi tower like architecture as shown below. This allows us to scale our embeddings to very high coverage since it’s not only fast to compute but we can also load those embeddings offline allowing for lightning fast (using FAISS ) similarity search at inference time. One difficulty of integrating those embeddings in a model is that we usually have a mixture of embeddings that we need to merge — or “pool” — together into one. For example: if we are building a neural network to predict the probability of a viewer commenting on a media as part of its sparse features the network will be fed a list of the last N user IDs on whose content this viewer commented. The neural network will then have to learn a representation for each user. This could be done using the method illustrated below where the network performs a decomposition of a sparse matrix to get embeddings for users with whom this user has interacted, then a pooling layer would blend these together to represent the centroid of the users on whose content this user has commented. The assumption is that the closer (as in distance) a new user is to that centroid, the more likely that new user would be to comment upon their content. There are a few important hyper-parameters to this method. Hashing : First the model needs to hash the user IDs into a predefined, countable set. This can be tricky as some user IDs occurs more often than others, e.g. if those users post more or have more followers. Therefore, to have even coverage, it’s important that they don’t end up in the same hashed bucket. This led us to work on a better hashing strategy which took frequency into account. Dimensionality : We also have to factor in the embeddings dimension. Given that there are many users and that the model needs to have a lookup table of the embeddings in memory this gives strong restrictions on the embeddings size as we have |hash_users_size| x |embeddings_dimension| floats to store. To keep models lean and efficient we have tools that monitor the cardinality of each of our sparse features. Those tools can automatically perform dimensionality reductions on the learned embeddings , and alert if we are off in terms of dimensionality or hash size. Pooling: There are many well-known pooling methods (attention, sum, average, max, etc.) to aggregate those hidden states. They are all semantically different. This is still somewhat a “ black box ” for us, an area for understanding and development. For now, we usually tune this with our general bayesian optimization of hyper-parameters. We see some similar patterns over and over again across our machine learning teams. The below fundamentals come up often enough that want to share our approach in the hopes it helps others. But first, what is the “ cold start ” problem? Example: Let’s imagine you use one of the methods mentioned earlier and you have generated embeddings. Now you want to predict whether a user will be interested in buying a product. To do that you compute the dot product between the two embeddings and use that as your prediction This can work well in practice, but what happens when a new user or product appears and you don’t have embeddings generated for them yet? Or simply don’t have enough data to even generate the embeddings? This is the cold start problem. If the models used for a task are using historical features, or features who do not have coverage for new use-cases, their predictive power will be subpar. There are a few ways to deal with such cases. At Instagram we monitor feature coverage fed into a model and if it is lower than a threshold we have fallback options that are less accurate but only use high fidelity features. We are also in the process of building mixture embeddings on user clusters, those have full coverage and are a good substitute for interaction history. Coming up we will be baking this into our training pipelines where each feature will have a “reliability” score and we will automatically produce fallback models for every model trained. In many cases the products and the ecosystem are highly non-stationary, seasonal, and sensitive to trends. Therefore training a model offline leads to a “stale” prediction whose accuracy likely deteriorates over time. At Instagram we usually have two types of model updates: Recurring : Periodically (typically daily) a model will load the past N days of training data ( N is usually 1 or 7), and keep training by loading the previous “snapshot” of the previous period. The model would keep propagating the gradients as if it was simply training on more data. The snapshot is then published and our inference infrastructure immediately picks it up for production purposes. Online: Here the model will actually be fed realtime data and will update itself often. We usually use online training for domains that are highly sensitive to trends. We get guaranteed freshness at a high engineering and maintenance cost. The pipeline to compute and update real time data may be resource-intensive. Recurring training provides a few advantages. It’s fairly painless to have an up-to-date model since it leverages the existing training methods. It’s also less resource-intensive since all the data is computed offline, doesn’t have to be there in real time. There may also be a computational cost to setting up proper vetting criteria to publish a snapshot. We usually evaluate against a fixed golden set, and a changing test set, as a good practice. Both of these methods have caveats, too. Depending on which gradient descent method and learning rate you use, your model may not be really updating after a significant number of days. For example by using Adagrad the square of the gradients will keep increasing until updates become non-meaningful. On the other hand you also usually want some resilience to new data. In particular a new bad day of data shouldn’t throw off the models too much. At Instagram we built a tool to monitor the correlations between weights and predictions between each snapshots on a fixed training data set and a held out in time one, this allows us to control for the learning rate and optimization methods mentioned above as well as detecting bad snapshots before they get published. It also helps us notice if our underlying training data has suddenly changed and model performance is affected. We can then use our other tools to inspect the data and pin point what specifically changed. It is common at the scale of Instagram to see sub-populations of users behaving very differently for a given prediction tasks. For instance, users in country A may have a higher like rate than in country B. Machine learning usually handle these things pretty well by learning those disparities, however sometimes it is highly useful to handle those cases separately. One idea to tackle this is by aggregating distributions and statistics for features and labels, then computing a Kullback–Leibler divergence between sub-populations to monitor how divergent they are. If we decide that a sub-population is behaving differently enough that it warrants special attention, we usually resort to a sparsely gated mixture of expert type of models where a gate can learn which experts to activate at inference time as a function of the input features. As soon as a machine learning pipeline becomes mature the iteration speed matters even more. It takes more and more effort for increasingly marginal gains. Therefore, assessing quickly how a model or value model is performing is key to the teams’ success. More than that it is critical to understand the ecosystem consequences of an experiment in depth to properly reason about causality, thereby avoiding engineers blindly performing a random walk along the path of hyper-parameters. We have built a cohesive tool that replays past traffic using control and test treatments, and computes a panel of ecosystem and model metrics to help engineers with their project. This allows an engineer to quickly check that the expected results are moving in the intended fashion. We are now working on predicting how an online experiment is doing by leveraging those offline metrics to reduce iteration speed even more. More on that in a future post. Ranking use cases arises naturally on our platform and therefore we wanted to share some ranking-specific practices that have proven useful for us. Multi-stage ranking: At our scale model size and inference speed is important and resource-intensive. The inventory we rank is large, so it is computationally impossible to perform complex inference on a large set of items. For these reasons we usually set up multi-stage ranking as soon as the inventory or request volume go past a certain threshold. The idea is to use a simpler model as first pass on many items, then pick the top N items ( N much smaller than the original amount of items) on which we perform the more resource-intensive inference. Our preferred 0-pass or 1-pass models are either dot products between embeddings (simple and fast) or small LambdaRank models Loss function and inverse propensity weighting: When the ranked list of items doesn’t have a human-generatable ideal relevant ranking (unlike most information theory cases), most pipelines default to point-wise models instead of Learning-To-Rank framework. For instance, one might rank the Instagram stories by computing P[tapping on the story] for each available medias and sorting by the probability. This works pretty well, albeit the loss function becomes an issue, because in most ranking use-cases the top items are much more impactful than the rest. At training time the loss function will optimize for examples, regardless of their position. Then one might find a situation where of the two models, one has a better validation performance than the other, yet performs worse online. That happens because the model is doing slightly worse for the high scores and much better for the lower scores. We have a couple methods to deal with this. The simplest approach is to borrow ideas from inverse propensity weighting and weight training examples by their positions. This helps skew the loss function towards meaningful errors. Bias features: Data bias is usually high for ranking use-cases. The action you are trying to model is predominant for items at the top of the list. For example, the like rate for the first post is higher than for post #20. To deal with this we often add position and batch size in the last layer of a neural network, or to co-train your model with a logistic regression using these features, to help the model account for this bias. The latter formulation allows us to do position ranking on a large set of items. One can perform model inference once for all items then pick the best one at position 0. Then simply perform inference again on the small logistic regression to find the best pick for position 1, and so on. We hope that you found this informative and helpful. These projects are a sample of the work done by our modeling teams, who are consistently inspired by research, and take advantage of the wealth of data we have at Instagram. There are a few on-going projects that we will detail in future blog posts, so please share comments, feedback, and requests. Stories from the people who build @Instagram 407 1 Machine Learning Instagram Engineering AI Modeling 407 claps 407 1 Written by Stories from the people who build @Instagram Written by Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-05-17"},
{"website": "Instagram-Engineering", "title": "thundering herds promises", "author": ["Nick Cooper"], "link": "https://instagram-engineering.com/thundering-herds-promises-82191c8af57d", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts Suppose you’re writing a simple service: it handles inbound requests from your users, but doesn’t hold any data itself. This means that in order to handle any inbound requests it needs to refer to a backend to actually fetch whatever is required. This is a great feature as it means your service is stateless — it’s easier to test, simpler to scale, and typically easier to understand. In our story, you’re also lucky as you only handle a single request at a time with a single worker doing the work. Your service diagram might look like this: All is well but latency is an issue. The backend you use is slow. To address this, you notice that the vast majority — perhaps 90% — of the requests are the same. One common solution is to introduce a cache : before hitting the backend, you check your cache and use that value if present. With the above design, 90% of incoming requests do not hit the backend. You’ve just reduced your resource requirements by 10x! Of course, there are some additional details you now need to consider. Namely, what happens if the cache is erased, or if a brand new request appears? In either scenario, your assumption of 90% similarity is broken. There are many ways you may choose to work around this, for example: If the cache is erased because you’ve had to restart the service, you could consider making the cache external: a) Leveraging memcache, or some other external caching service. b) Using local disk as something “external” to your service you can load on startup. c) Using something clever with shared memory segments to restart without discarding the in-memory state. If the request is a brand new feature, you could handle it incrementally: a) Deploy the new client that makes the request slowly, or adjust your distribution plans so it’s not all at once. b) Have your service perform some sort of warmup, perhaps with fake requests emulating what you expect. Furthermore, your service might not actually live alone. You might have lots of other instances of it, and an existing instance could be communicated with to accelerate your cache warmup routine. All of these add complexity, some of them add intricate failure modes or make your service just more troublesome to deal with (what do you mean I can’t dial everything to 100% immediately?). Chances are your cache warms pretty quickly, and the less boxes on your service diagram the less things go wrong. Ultimately, you’re happy with doing nothing. The above diagram isn’t really what your service looks like. It’s a runaway success and each service is doing quite a lot of work. Your service actually looks like this: However, the cache-empty case is now much more problematic. To illustrate: if your cache is hit with 100 concurrent requests, then, since the cache is empty, all of them will get a cache-miss at the one moment, resulting in 100 individual requests to the backend. If the backend is unable to handle this surge of concurrent requests (ex: capacity constraints), additional problems arise. This is what’s sometimes called a thundering herd . The aforementioned alleviation scenarios may help improve the resilience of the cache — if you have a service-start resilient cache you can be safe to continually push things without worrying that this would cause awfulness. But this still doesn’t help you for the genuinely new request. When you see a new request, by definition it is not in the cache, and in addition it may not even be predictable! You have many clients, they don’t always tell you about this in advance. Your only option is to slow the release of new features in order to pre-warm the cache, and hope everyone does this… or is it? A Promise is an object that represents a unit of work producing a result that will be fulfilled at some point in the future. You can “wait” on this promise to be complete, and when it is you can fetch the resulting value. In many languages there is also the idea of “chaining” future work once the promise is fulfilled. At Instagram, when turning up a new cluster we would run into a thundering herd problem as the cluster’s cache was empty. We then used promises to help solve this: instead of caching the actual value, we cached a Promise that will eventually provide the value . When we use our cache atomically and get a miss, instead of going immediately to the backend we create a Promise and insert it into the cache. This new Promise then starts the work against the backend. The benefit this provides is other concurrent requests will not miss as they’ll find the existing Promise — and all these simultaneous workers will wait on the single backend request. The net-effect is you’re able to maintain your assumptions around caching of requests. Assuming your request distribution has the property that 90% are cache-able; then you’d maintain that ratio to your backend even when something new happens or your service is restarted. At Instagram, most of our C++ services are implemented with folly::Future . It provides a SharedPromise abstraction to simplify the implementation of the above behavior. By using this Promise instead of just a raw value when implementing a cache, you can improve the cold cache behavior within your service. We manage so many different services at Instagram that reducing the amount of traffic to our servers via Promise-based caches has real stability benefits. Stories from the people who build @Instagram 1K 9 Thanks to Kiro Risk . Infrastructure 1K claps 1K 9 Written by Stories from the people who build @Instagram Written by Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-04-17"},
{"website": "Instagram-Engineering", "title": "lessons learned at instagram stories and feed machine learning", "author": ["Thomas Bredillet"], "link": "https://instagram-engineering.com/lessons-learned-at-instagram-stories-and-feed-machine-learning-54f3aaa09e56", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts Instagram machine learning has grown a lot since we announced Feed ranking back in 2016. Our recommender system serves over 1 billion users on a regular basis. We also now use machine learning for more than just ranking Feed and Stories: we source and recommend posts from Hashtags you follow, blend in different types of content together, and power intelligent app prefetching. All of the different ways Instagram uses machine learning deserves its own post, but we want to discuss a few lessons we’ve learned along the way of building our ML pipeline. We made a few decisions for how we do modeling that have been beneficial to us either by improving our models’ predictive power and providing top line improvements or by maintaining the accuracy and lowering our memory consumption. First off we went with caffe2 as our general modeling framework, meaning we write and design our models through that platform. Caffe2 is significantly more optimized for our workflows than other options, and it gives us the most headroom with model weight per CPU cycle at inference time. “Stack Footprint” is an important metric for the ML team because we use several CPU intensive statistical techniques in our networks (pooling, etc). We also use models with ranking losses and point-wise models (log loss for example). This gives us more control in our final value function so we can fine-tune trade offs between our key engagement metrics. In terms of core machine learning, we saw some very good accuracy wins by taking into account position bias in our model. We added a sparse position feature to our last fully connected layer to avoid confusing the model too much. In general, co-learning sparse embeddings is another impactful area for us, that comes in many flavors to properly capture our users’ interests. We tune our final value function regularly by fitting a Gaussian Process to learn the effect of the value functions’ parameters on our top line metrics measured through series of A/B tests. Our users’ habits change over time. Similarly, the ecosystem is subject to trend effects (during seasonal events such as the Superbowl). Because of this, data freshness is important. Stale models cannot capture changes in user behavior or understand new trends. Quantifying the effects of data freshness was helpful to us. We monitor the drift in KL-divergences between key behavioral distributions to inform us of the “staleness” of our pipeline. One way to keep our models fresh is to have online learning models or at least some periodic training. In this setting, one of our biggest challenges was coming up with a reasonable adaptive learning rate strategy since we want new examples to still matter in gradient updates (even for models that have been training on months of data). Novelty effect are another hard problem that we face. We frequently run A/B tests where the treatment arm shows positive engagement for the early days and slowly trends down to neutral. On one hand it’s actually quite telling that subtle changes can temporarily drive engagement. We believe it stems from the fact that long running models tend to “exploit” too much and those tests bring some new areas of exploration. The time scale of these effects are also interesting. We’ve seen changes that would take over a month to plateau (with engagement trending up or down). On the other hand, we learned the hard way that novelty effects can be subtle and should be carefully controlled while launching new experiences that may be effected. We recently had a rather serious post-mortem where two experiments prone to novelty effects interacted together very poorly in the few hours after launch. While it isn’t perfect, we now have some modeling that can predict the magnitude and the length of novelty-prone experiments. With this we can iterate faster by mitigating risk and terminating tests earlier. There are many different complexities with large scale ML and A/B testing. Besides the novelty effects mentioned above, we also face statistical problems. Imagine having 10 ranking engineers each launching a new test everyday : it’s quite likely that several of those tests improve engagement metrics with statistical significance! On top of that, some of those experiments may only target specific cohorts of users, and the measured effects aren’t necessarily as important on the overall population. This makes the test results even harder to assess. Our current set of best practices try to make sensible trade offs between engineer iteration speed and our confidence interval in the changes we launch. These best practices require strict replication on increasingly larger population of users before we approve an A/B test for production. Machine learning is a stochastic process by definition. When we do performance reviews, our engineers and researchers are calibrated against traditional software engineers working on less volatile projects. It is entirely possible to do all the right things but to come up short in terms of bottom line metrics. At Instagram, we are keen to stick to the scientific method for our experimentations. Even if an A/B test doesn’t directly result in a launch, we can often leverage it to provide interesting product insights in the future. This also prevents bad scientific patterns of random-walking through the hyper parameters of our pipelines to find some local optima. We now call this pattern “human gradient descent”. With this, we require principled hypothesis to verify before launching a test. As machine learning engineers, we aren’t just looking to ship features, but we also want to learn. Each experiment has a specific set of outcomes. We’re not random walking. Blending different types of content is another challenge we faced. For example, a video and a photo have very different distributions for the possible actions. For example you could imagine that “liking” a post and “commenting” on a post or “completing” a video are three actions with very different distributions (likes happen more often than comments, etc.) Naively it makes more sense to rank photos with P[Like] (probability of a viewer liking a post) and videos with P[completion] (probability of a viewer viewing more than X% of a video). That puts machine learning engineers in a difficult position when we want to merge the lists to come up with a final ordering for the viewer. We tackled this problem by fitting a mapping from one value function distribution (such as P[Like]) to a reasonably well-behaved distribution such as a Gaussian. In that output space, the lists are now comparable and we can unambiguously say a piece of content is superior to another one. We were way too late with adding a proper backtesting framework. For very large scale and impactful machine learning systems, engineers and researchers really need to open up the models and understand as precisely as possible the effect their experiment has. It’s very hard to do this without solid tooling. We worked on a replaying tool that takes in whatever new model/ranking configuration you want to test and outputs a panel of useful metrics to understand the overall ecosystem impact that your change may have. Our goal with this is to reduce as much online experimentation as possible to reduce the risk of exposing users to bad experiences and to speed up our iteration speed. All large-scale systems require serious infrastructure, and thankfully at Instagram we have a stellar ML Infrastructure team (they originally built Feed ranking and spun out from it). All model inference, features extraction, training data generation and monitoring is taken care of by their infrastructure. Not having to worry about scaling concerns and focussing fully on statistical modeling is one of the most significant efficiency boosts for our engineers. On top of that, the ML infra team create tools that let us dive deep to understand our models better which help us improve our users’ experience. Another beneficial feature is the ability to fine-tune our final value function. This takes our models as input, adds our business logic, and returns a final score per media. Personalizing that value function is both impactful and complex. We chose to to have high level heuristics around cohorts of users who benefit less from our recommendation system and tune the value function specifically for them. Another personalization strategy that shows promising early results is factoring in some user affinity models. Trying to quantify how much affinity a user may have with other users/types of content helps us tailor and fit our function specifically to our viewers. Finally we have our value model: the formulaic description that combines different signals into a score and incorporates our business logic. This is complex code where product heuristics meets statistical predictions. We’ve seen significant improvements over the years by tuning this value model. We typically use Gaussian processes and Bayesian optimizations to span the space of hyper parameters of the model and find a region that works well for us. There is another article detailing this procedure here . We hope that this summary of our machine learning pipeline and problems we face is helpful. In future posts, we will go in even more depth on some of the issues mentioned above. Whether we are predicting user actions, building content understanding convolutional neural networks, or creating latent user modes, these lessons help us make fewer mistakes and iterate faster so we can constantly improve ML for everyone on Instagram! If you are excited to apply machine learning to provide value to our global community at scale, we’re always looking for more great talent to join us . Stories from the people who build @Instagram 753 2 Machine Learning 753 claps 753 2 Written by Stories from the people who build @Instagram Written by Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-12-18"},
{"website": "Instagram-Engineering", "title": "instant feedback in ios engineering workflows", "author": ["Instagram Engineering"], "link": "https://instagram-engineering.com/instant-feedback-in-ios-engineering-workflows-c3f6508c76c8", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts At Instagram and Facebook, we’ve seen how adopting React Native in product development has allowed our engineers to move and iterate faster on products. Viewing the effects of our changes is now just a single tap of ⌘R away! But working with native code doesn’t provide the same experience. Instead, engineers need to recompile, link, install, and launch the application they’re developing every time they want to see the results of their changes. Even on a small application, that process drops people out of their workflow. And at our scale, the result is that engineers spend a significant amount of time waiting for their computers to complete work, and often end up context-switching away to other applications, losing even more time. Late last year we set out to bring the advantages of React Native’s developer experience to engineers working on our native iOS codebase, written in Objective-C, with the goal of allowing engineers to see their changes in seconds. To achieve this, we skip most of the steps necessary to restart the application by loading the new code while it’s still running. To reload native code, the first step is to turn the modified code into something that we can load into a running application: a dynamic library. This operates on a file-level granularity: we compile only the files that have changed, then link them into a dynamic library. This is very fast — engineers using the tool are primarily iterating on a single file. An iOS device won’t permit an application to load dynamic libraries from outside of its bundle. Additionally, it won’t allow an application to write to its own bundle. Therefore, all loadable binaries must have been in the application’s bundle when it was installed, and loading our new library is impossible. Fortunately, neither of these limitations apply to the iOS simulator. Each simulator’s filesystem is simply a directory in host Mac’s filesystem. Additionally, the dlopen function in the iOS simulator doesn't filter by file location, which makes the ability to write directly to the application's bundle irrelevant anyway. Within the application code, we’ve added a startup function that stars a small, debug-only background task that watches a specific directory. After creating the dynamic library, the tool locates it and moves it into that directory. This triggers a callback in the application, which takes these steps: Load the new dynamic library. Determine which Objective-C classes have been added. Find the original versions of these classes. Objective-C classes are themselves objects, and they’re easy to look up by name. Use the Objective-C runtime APIs to replace the implementations of methods on the original versions of the classes with the new implementations. This applies to all methods of the class, as we don’t have any way to determine which have changed and which haven’t. Write a response back to the same directory for the tool to read. Our new code is now running in the application, and will be called whenever one of these methods is next invoked. Of course, this is only available while in development with the iOS simulator, not on the device or in the production app. Reinstalling and restarting an application to see new changes isn’t fast, but it’s simple and reliable. Click a button or press a keyboard shortcut in Xcode, and eventually, the results of your code will appear on-screen. We match this in our tooling: we place a button in Xcode’s toolbar, and add a keyboard shortcut to trigger the loading of new code. We have an another advantage over full restarts: we can keep all of the current application state. A restart will reset the application back to its entry point — for our apps, that’s generally a feed. Engineers working on other products will have to click their way back to their products’ views before they can see if their changes worked correctly. However, after new code has been loaded, nothing happens: all of the previous state that has been built up is unaffected by the engineer’s changes. Fortunately, ComponentKit , a declarative iOS UI framework used at Facebook, has a useful method for performing an instant relayout: +[CKComponentDebugController reflowComponents] . Views built with ComponentKit can invoke it after new code is loaded, causing their new layout code to be evaluated and the on-screen views to update immediately. The result is that when working on UI code written with ComponentKit, engineers can make changes to their view layouts, and see them within seconds. Previously, engineers would need to batch their updates and think about when they wanted a full restart. They can now work interactively and iteratively. Our tool has become popular with our teams here, especially among engineers working with ComponentKit. In larger apps, it has allowed engineers to see the results of their changes up to twenty times faster. The combination of fast performance and tight integration with existing developer tools and frameworks has made native code reloading an important part of many iOS engineers’ workflows here at Instagram and Facebook. Nate Stedman is an iOS engineer at Instagram New York. Stories from the people who build @Instagram 1.6K 10 React iOS Software Development 1.6K claps 1.6K 10 Written by Stories from the people who build @Instagram Written by Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-04-23"},
{"website": "Instagram-Engineering", "title": "san francisco", "author": ["Instagram Engineering"], "link": "https://instagram-engineering.com/san-francisco-856cbf69397", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts Today we’re excited to announce that Instagram is opening our third full-stack office in San Francisco. Kevin Systrom and Mike Krieger founded Instagram in San Francisco in 2010, and now — over seven years and 800 million Instagrammers later — we’re thrilled to be back. The Bay Area has incredible talent, which has helped us create a great and growing team in Menlo Park. Now that Instagram has returned to San Francisco, we’re excited to tap into more talent from across the region. To start, the San Francisco office will focus on product development from the Creation and Communication team, whose goal is to inspire people to share what matters to them. This is the team behind some of Instagram’s most impactful products, including Stories, Live, Direct, and Profile. Some of the new work we announced at F8 , including Camera Effects, Sharing to Stories, and video chat, will be based in San Francisco. To build these teams, we recruit diverse talent across engineering, product management, design, research, data engineering, and analytics. Our engineering teams are full-stack, and span iOS, Android, and server. Our development teams include graphics experts, and mobile and server infrastructure specialists who build robust systems to power our products at scale, like Live and Direct. The teams in San Francisco are responsible for some of Instagram’s biggest bets, including low-latency video, camera and AR effects, and other rich new user experiences. Our San Francisco office will continue to expand over the coming months. It’s designed to embody Instagram’s core values, putting community first through art from local and global Instagrammers, inspiring creativity, and simplicity — but with a distinctly San Francisco spin. Instagram has always operated with a lean team, and we’ve taken a careful approach to growth. Opening a new engineering hub in San Francisco is a big step and something we took very seriously. We know that successful new offices need top, full-stack talent tackling big projects. For example, in our New York City hub, which was established in 2015, we colocated the product and infrastructure teams behind experiences like Home, Discovery, and Shopping, and have since shipped some of Instagram’s most impactful features. Now that we’re in San Francisco, continued collaboration across offices will be more important — and exciting — than ever. I’m personally incredibly excited to be growing our team in San Francisco. I grew up in the city and have seen it change so much over the years. It’s part of Instagram’s history too, and it’s great to see us take part in the vibrant local community, tap into its creative talent, and be a part of the ever-changing city landscape. If you want to build the next generation of amazing products at Instagram San Francisco, please join us ! Eddie Ruvinsky is the Engineering Director with the Creation and Communication teams at Instagram San Francisco. Stories from the people who build @Instagram 953 10 San Francisco 953 claps 953 10 Written by Stories from the people who build @Instagram Written by Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-05-17"},
{"website": "Instagram-Engineering", "title": "five things i learned about working on content quality at instagram", "author": ["Brunno Attorre"], "link": "https://instagram-engineering.com/five-things-i-learned-about-working-on-content-quality-at-instagram-5031b1342bea", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts Recommended content, which is surfaced in places like Explore or hashtags, is a central part of people’s experience on Instagram. As people browse this “unconnected content” from accounts that they aren’t already linked to on Instagram, it’s extremely important to identify and deal with content that violates our Community Guidelines or might be considered offensive or inappropriate by the viewer. Last year, we formed a team dedicated to finding and taking action on both violating and potentially offensive content on these unconnected surfaces of Instagram, as part of our ongoing effort to help keep our community safe. This work differs from conventional platform work. Platform teams at Facebook traditionally focus on solving a problem across a number of surfaces, such as News Feed and Stories. However, Explore and Hashtags are particularly complicated ecosystems. We chose to create a bespoke solution that builds on the work of our platform teams, and apply it to these complex surfaces. Now, a year later, we are sharing the lessons we learned from this effort. These changes are essential in our ongoing commitment to keep people safe on Instagram, and we hope they can also help shape the strategies of other teams thinking about how to improve the quality of content across their products. One of the toughest challenges this year has been identifying how to accurately measure the quality of content. There’s no industry benchmark when it comes to measuring quality in a deterministic way. In addition, when measuring the quality of experiments and A/B tests from multiple engineering teams, trying to hand-label each test group subset from our experiments proved to be time intensive and unlikely to produce statistically significant results. Overall, this was not a scalable solution. We transitioned across many different types of metrics: from using deterministic user signals to rating both test and control groups for all experiments. This transition of metrics over experimentation took significant effort and led us to spend many iteration cycles understanding the results of our experiments. In the end, we decided to combine manual labels for calibration and software-generated scores together, to get the best of both worlds. By relying on both human labels for calibration and a classifier, we were able to scale the calibrated classifier score (in other words, the probability of a content violation at a given score) to the entire experiment. This allowed us to achieve more statistically significant approximation of impact when compared to either human labels and classifiers alone. Conclusion: Don’t try to solve quality without operationalizing your metrics, and make sure your engineers have a reliable online metric they can reference in their experiments. Also, when thinking about quality, think about how you can rely on classifier scores and manually-labelled data to approximate the directionality and magnitude of your launches. Historically, we have always used classifiers that would predict whether a piece of content is good or bad at upload time, which we call “write-path classifiers.” Having a write-path classifier has the advantage of being efficient, but it has a major drawback: it can only look at the content itself (i.e. pixels and captions). It cannot incorporate real-time features, those which can provide a lot of insight into whether a piece of media is good or bad, such as comments or other engagement signals. Last year, we started working on a “read-path model”. This “read path model” is an impression-level real-time classifier for detecting unwanted content (photos, videos), combining both the upload time signals and the real-time engagement signals at media and author level. This particular model, therefore, would run every time a user makes a request to see a page on Explore, scoring each candidate in real time at the request level. This model turned out to be extremely successful. By using real time engagement signals in combination with the content features, it was capable of capturing and understanding bad behaviors associated with violating content. Conclusion: if you are considering applying quality signals into your ranking model, using a read-path model trained with both content-level and engagement-level features can be a more reliable and precise means of achieving better results. While we know read-path models are important in filtering violating and potentially inappropriate content from unconnected surfaces at ranking level , we found that having a basic level of protection at the sourcing level is still necessary. That’s where write-path level classifiers come into play. But what does ranking and sourcing level mean? At Instagram, we have two steps to serve content to our community in Explore and hashtag pages: The sourcing step constitutes the queries necessary to find eligible content to show someone, with context on that person’s interests. The ranking step takes eligible content and ranks it according to a given algorithm/model. We learned the following when it came to finding eligible content at the sourcing level: You need filters at sourcing level for low prevalence issues. Low prevalence violations are a very small volume of your training data, meaning content may be overlooked by your read-path models. Therefore, using an upload path classifier makes a lot of sense in these cases, and provides protection for these low prevalence issues. You need high precision filters to provide basic protection across all surfaces. If you only source “bad” content and leave the filtering to happen only at the ranking step, you will end up with not a lot of content to rank, reducing the effectiveness of your ranking algorithms. Therefore, it’s important for you to guarantee a good standard at sourcing to ensure most of the content you are sourcing is benign. Conclusion: the combination of basic protection at sourcing, fine tuned filtering at ranking, and a read-path model allowed us to uphold a high quality standard of content on Explore. However, it’s important to always keep in mind that your protection at sourcing should always be high precision and low volume to avoid mistakes. This is something that goes beyond engineering, but it’s been a key to our work. When working on quality, it’s important for you to measure the performance of the models that you use in production. There are two reasons why: Having a precision and recall measurement calculated daily can help quickly identify when your model is decaying or when you have a problem in performance of one of the underlying features. It can also help alert you to a sudden change in the ecosystem. Understanding how your models perform can help you understand how to improve. A low precision model means your users may have a poor experience. Having those metrics and a way to visualize the content labeled as “bad” has been a crucial improvement for our team. These dashboards allow our engineers to quickly identify any movement in metrics, and visualize the types of content violations required to improve the model, accelerating feature development and model iteration. Conclusion: monitor your precision and recall curve daily, and make sure you understand the type of content being filtered out. That will help you identify issues, and quickly improve on your existing models. We learned a lot by using raw thresholds as filters and adapted accordingly. Facebook is a complex ecosystem, and models have many underlying dependencies that could break and affect the upstream features of your model. This in turn can impact score distribution. Overall, the issue with using raw thresholds is that they are too volatile. Any small change can cause unexpected fluctuations on surfaces, especially when suddenly you have a big metric movement from one day to the next. As a solution, we recommend a calibration dataset to perform a daily calibration of your models, or a percentile filtering mechanism. We recently moved both our content filter and ranking frameworks to use percentiles, allowing us to have a more stable infrastructure, and we aim to establish a a calibration framework in the coming months. Conclusion: use a percentile framework instead of raw thresholds, or consider calibrating your scores against a daily updated dataset. Maintaining the safety of Instagram is imperative to our mission as a company, but it is a difficult area across our industry. For us, it’s critical to take novel approaches when tackling quality problems on our service, and not to rely on approaches learned in more traditional ML ranking projects. To wrap up, here are some of our key takeaways: Operationalizing a quality metric is important and you should always think if there are ways of relying more on machine learning to scale your human labels. Always think holistically about about how to apply quality enforcement on your ranking flow and try to think about integrating models on multiple layers of your system to achieve the best results. Always remember that the experience of those using your service is your most important priority, and make sure you have tools that visualize, monitor and calibrate the models you are using in production, to guarantee the best experience possible. If you want to learn more about this work or are interested joining one of our engineering teams, please visit our careers page , follow us on Facebook or on Twitter . Stories from the people who build @Instagram 239 15 Machine Learning Data Instagram 239 claps 239 15 Written by ML Engineer at Instagram NYC Stories from the people who build @Instagram Written by ML Engineer at Instagram NYC Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-01-25"},
{"website": "Instagram-Engineering", "title": "powered by ai instagrams explore recommender system", "author": ["Ivan Medvedev"], "link": "https://instagram-engineering.com/powered-by-ai-instagrams-explore-recommender-system-7ca901d2a882", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts This post was originally published on the Facebook AI blog . Over half of the Instagram community visits Instagram Explore every month to discover new photos, videos, and Stories relevant to their interests. Recommending the most relevant content out of billions of options in real time at scale introduces multiple machine learning (ML) challenges that require novel engineering solutions. We tackled these challenges by creating a series of custom query languages, lightweight modeling techniques, and tools enabling high-velocity experimentation. These systems support the scale of Explore while boosting developer efficiency. Collectively, these solutions represent an AI system based on a highly efficient 3-part ranking funnel that extracts 65 billion features and makes 90 million model predictions every second . In this blog post, we’re sharing the first detailed overview of the key elements that make Explore work, and how we provide personalized content for people on Instagram. Before we could execute on building a recommendation engine that tackles the sheer volume of photos and videos uploaded daily on Instagram, we developed foundational tools to address three important needs. We needed the ability to conduct rapid experimentation at scale, we needed to obtain a stronger signal on the breadth of people’s interests, and we needed a computationally efficient way to ensure that our recommendations were both high quality and fresh. These custom techniques were key to achieving our goals: Building the optimal recommendation algorithms and techniques is an ongoing area of research in the ML community, and the process of choosing the right system can vary widely depending on the task. For instance, while one algorithm may effectively identify long-term interests, another may perform better at identifying recommendations based on recent content. Our engineering team iterates on different algorithms, and we needed a way for us to both try out new ideas efficiently and apply the promising ideas to large-scale systems easily without worrying too much about computational resource implications like CPU and memory usage. We needed a custom domain specific meta-language that provides the right level of abstraction and assembles all algorithms into one place. To solve this, we created and shipped IGQL, a domain-specific language optimized for retrieving candidates in recommender systems. Its execution is optimized in C++, which helps minimize both latency and compute resources. It’s also extensible and easy to use when testing new research ideas. IGQL is both statically validated and high-level. Engineers can write recommendation algorithms in a Python-like way and execute fast and efficiently in C++. In the code sample above, you can see how IGQL provides high readability even for engineers who haven’t worked extensively in the language. It helps assemble multiple recommendation stages and algorithms in a principled way. For example, we can optimize the ensemble of candidate generators by using a combiner rule in query to output a weighted blend of several subquery outputs. By tweaking their weights, we can find the combination that results in the best user experience. IGQL makes it simple to perform tasks that are common in complex recommendation systems, such as building nested trees of combiner rules. IGQL lets engineers focus on ML and business logic behind recommendations as opposed to logistics, like fetching the right quantity of candidates for each query. It also provides a high degree of code reusability. For instance, applying a ranker is as simple as adding a one-line rule to our IGQL query. It’s trivial to add it in multiple places, like ranking accounts and ranking media posted by those accounts. People publicly share billions of high quality pieces of media on Instagram that are eligible inventory for Explore. It’s challenging to maintain a clear and ever-evolving catalog-style taxonomy for the large variety of interest communities on Explore — with topics varying from Arabic calligraphy to model trains to slime. As a result, content-based models have difficulty grasping such a variety of interest-based communities. Because Instagram has a large number of interest-focused accounts based on specific themes — such as Devon rex cats or vintage tractors — we created a retrieval pipeline that focuses on account-level information rather than media-level. By building account embeddings, we’re able to more efficiently identify which accounts are topically similar to each other. We infer account embeddings using ig2vec, a word2vec -like embedding framework. Typically, the word2vec embedding framework learns a representation of a word based on its context across sentences in the training corpus. Ig2vec treats account IDs that a user interacts with — e.g., a person likes media from an account — as a sequence of words in a sentence. By applying the same techniques from word2vec, we can predict accounts with which a person is likely to interact in a given session within the Instagram app. If an individual interacts with a sequence of accounts in the same session, it’s more likely to be topically coherent compared with a random sequence of accounts from the diverse range of Instagram accounts. This helps us identify topically similar accounts. We define a distance metric between two accounts — the same one used in embedding training — which is usually cosine distance or dot product. Based on this, we do a KNN lookup to find topically similar accounts for any account in the embedding. Our embedding version covers millions of accounts, and we use Facebook’s state-of-the-art nearest neighbor retrieval engine, FAISS , as the supporting retrieval infrastructure. For each version of the embedding, we train a classifier to predict a set of accounts’ topic solely based on the embedding. By comparing the predicted topics with human-labeled topics for accounts in a hold-out set, we can assess how well the embeddings capture topical similarity. Retrieving accounts that are similar to those that a particular person previously expressed interest in helps us narrow down to a smaller, personalized ranking inventory for each person in a simple yet effective way. As a result, we are able to utilize state-of-the-art and computationally intensive ML models to serve every Instagram community member. After we use ig2vec to identify the most relevant accounts based on individual interests, we need a way to rank these accounts in a way that’s fresh and interesting for everyone. This requires predicting the most relevant media for each person every time they scroll the Explore page. For instance, evaluating even just 500 media pieces through a deep neural network for every scrolling action requires a large amount of resources. And yet the more posts we evaluate for each user, the higher the possibility we have of finding the best, most personalized media from their inventory. In order to be able to maximize the number of media for each ranking request, we introduced a ranking distillation model that helps us preselect candidates before using more complex ranking models. Our approach is to train a super-lightweight model that learns from and tries to approximate our main ranking models as much as possible. We record the input candidates with features, as well as outputs, from our more complicated ranking models. The distillation model is then trained on this recorded data with a limited set of features and a simpler neural network model structure to replicate the results. Its objective function is to optimize for NDCG ranking (a measure of ranking quality) loss over main ranking model’s output. We use the top-ranked posts from the distillation model as the ranking candidates for the later-stage high-performance ranking models. Setting up the distillation model’s mimicry behavior minimizes the need to tune multiple parameters and maintain multiple models in different ranking stages. Leveraging this technique, we can efficiently evaluate a bigger set of media to find the most relevant media on every ranking request while keeping the computational resources under control. After creating the key building blocks necessary to experiment easily, identify people’s interests effectively, and produce efficient and relevant predictions, we had to combine these systems together in production. Utilizing IGQL, account embeddings, and our distillation technique, we split the Explore recommendation systems into two main stages: the candidate generation stage (also known as sourcing stage) and the ranking stage. First, we leverage accounts that people have interacted with before (e.g., liked or saved media from an account) on Instagram to identify which other accounts people might be interested in. We call them the seed accounts. The seed accounts are usually only a fraction of the accounts on Instagram that are about similar or the same interests. Then, we use account embeddings techniques to identify accounts similar to the seed accounts. Finally, based on these accounts, we’re able to find the media that these accounts posted or engaged with. There are many different ways people can engage with accounts and media on Instagram (e.g., follow, like, comment, save, and share). There are also different media types (e.g., photo, video, Stories, and Live), which means there are a variety of sources we can construct using a similar scheme. Leveraging IGQL, the process becomes very easy — different candidate sources are just represented as different IGQL subqueries. With different types of sources, we are able to find tens of thousands of eligible candidates for the average person. We want to make sure the content we recommend is both safe and appropriate for a global community of many ages on Explore. Using a variety of signals, we filter out content we can identify as not being eligible to be recommended before we build out eligible inventory for each person. In addition to blocking likely policy-violating content and misinformation, we leverage ML systems that help detect and filter content like spam. Then, for every ranking request, we identify thousands of eligible media for an average person, sample 500 candidates from the eligible inventory, and then send the candidates downstream to the ranking stage. With 500 candidates available for ranking, we use a three-stage ranking infrastructure to help balance the trade-offs between ranking relevance and computation efficiency. The three ranking stages we have are as follows: First pass: the distillation model mimics the combination of the other two stages, with minimal features; picks the 150 highest-quality and most relevant candidates out of 500. Second pass: a lightweight neural network model with full set of dense features; picks the 50 highest-quality and most relevant candidates. Final pass: a deep neural network model with full set of dense and sparse features. Picks the 25 highest-quality and most relevant candidates (for the first page of Explore grid). If the first-pass distillation model mimics the other two stages in ranking order, how do we decide the most relevant content in the next two stages? We predict individual actions that people take on each piece of media, whether they’re positive actions such as like and save, or negative actions such as “See Fewer Posts Like This” (SFPLT). We use a multi-task multi-label (MTML) neural network to predict these events. The shared multilayer perceptron (MLP) allows us to capture the common signals from different actions. We combine predictions of different events using an arithmetic formula, called value model, to capture the prominence of different signals in terms of deciding whether the content is relevant. We use a weighted sum of predictions such as [w_like * P(Like) + w_save * P(Save) — w_negative_action * P(Negative Action)]. If, for instance, we think the importance of a person saving a post on Explore is higher than their liking a post, then the weight for the save action should be higher. We also want Explore to be a place where people can discover a rich balance of both new interests alongside existing interests. We add a simple heuristic rule into value model to boost the diversity of content. We down-rank posts from the same author or same seed account by adding a penalty factor, so you don’t see multiple posts from the same person or the same seed account in Explore. This penalty increases as you go down the ranked batch and encounter more posts from the same author. We rank the most relevant content based on the final value model score of each ranking candidate in a descendant way. Our offline replay tool — along with Bayesian optimization tools — helps us tune the value model efficiently and frequently as our systems evolve. One of the most exciting parts of building Explore is the ongoing challenge of finding new and interesting ways to help our community discover the most interesting and relevant content on Instagram. We’re continuously evolving Instagram Explore, whether by adding media formats like Stories and entry points to new types of content, such as shopping posts and IGTV videos. The scale of both the Instagram community and inventory requires enabling a culture of high-velocity experimentation and developer efficiency to reliably recommend the best of Instagram for each person’s individual interests. Our custom tools and systems have given us a strong foundation for the continuous learning and iteration that are essential to building and scaling Instagram Explore. If you want to learn more about this work or are interested joining one of our engineering teams, please visit our careers page , follow us on Facebook or on Twitter . Written by Ivan Medvedev, Haotian Wu, and Taylor Gordon. Stories from the people who build @Instagram 732 12 Machine Learning Recommendations Recommender Systems 732 claps 732 12 Written by Stories from the people who build @Instagram Written by Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-11-26"},
{"website": "Instagram-Engineering", "title": "instagram darkmode", "author": ["Tim Johnsen"], "link": "https://instagram-engineering.com/instagram-darkmode-58802b43c0f2", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts One of the most exciting announcements at WWDC this year was the introduction of platform-wide dark mode in iOS 13. During WWDC a group of enthusiastic iOS engineers and designers from Instagram’s design systems team banded together to begin plotting out what it would take to adopt dark mode in our app. This week’s update to Instagram includes full support for iOS dark mode. This took months of work and collaboration between numerous design and engineering teams in the company. As such, we wanted to take some time to share how we approached adopting dark mode and some of the obstacles we encountered along the way. Apple did an excellent job shaping how dark mode works in iOS 13. Most of the heavy lifting is done on your behalf by UIKit. Because of this, one of the key principles we had when building out dark mode support in our app was that we should “stand on the shoulders of giants” and try to stick with Apple’s APIs as much as possible. This is beneficial for several reasons. Ease of use — UIKit does most of the work in selecting appropriate colors and transitioning between light mode and dark mode. If we wrote our own APIs we’d have to handle this ourselves. Maintainability — Apple maintains the APIs so we don’t have to. Any wrappers we have can ultimately be switched over to just use UIKit APIs as soon as our minimum supported OS version is iOS 13. Familiarity — Newcomers to Instagram’s iOS codebase who are familiar with how UIKit does dark mode will feel right at home. That being said, we didn’t use UIKit’s APIs alone since most developers in the company and our build systems are all still using Xcode 10, and introducing iOS 13 APIs would cause build breakages. We went with the approach of writing thin wrappers around UIKit APIs that are compatible with Xcode 10 and iOS 12. Another principle we followed was to introduce as few APIs as possible, and only when needed. The key reason for this was to reduce complexity for product teams adopting dark mode: it’s harder to misunderstand or misuse APIs if there are fewer of them. We started off with just wrappers around dynamic colors and a semantic color palette that our design systems team created, then introduced additional APIs over time as the need grew within the company. To increase awareness and ensure steady adoption, whenever we introduced a new API we announced it in an internal dark mode working group and documented it in an internal wiki page for the project. Apple defines some handy dark mode primitives and concepts, and since we decided to build on top of their APIs we embraced these as well. Covering them at a high level, we have. Dynamic colors — Colors that change in response to light mode/dark mode changes. Also can change in response to “elevation” and accessibility settings. Dynamic images — Similar to dynamic colors, these are images that change in response to light mode/dark mode changes. Semantic colors — Named dynamic colors that serve a specific purpose. For example “destructive button color” or “link text color”. Elevation level — Things presented modally in dark mode change colors very slightly to demonstrate that they’re a layer on top of the underlying UI. This concept largely hasn’t existed in light mode because dark dimming layers are sufficient to differentiate modal layers presented on top of others. One of the key APIs iOS 13 introduces for dark mode support is UIColor ’s +colorWithDynamicProvider: method, which generates colors that automatically adapt to dark mode. This was the very first API we sought to wrap for use within Instagram and is still one of our most used dark mode APIs. We’ll walk through implementing it as a case study in building a backwards-compatible wrapper. The first step in building such an API is defining a macro that allows us to conditionally compile out code for people that are still using stable versions of Xcode. This is what ours looks like: Next we declare a wrapper function. Our wrapper for dynamic colors looks like this: Within this function we use our macro to ensure that developers using older versions of Xcode can still compile. We also introduce a runtime check so that the app continues to function normally on older versions of iOS. If both checks pass we simply call into the iOS 13 +colorWithDynamicProvider: API, otherwise we fall back to the light mode variant. You may notice that we’re passing an IGTraitCollection into IGColorWithDynamicProvider 's block instead of a UITraitCollection . We introduced IGTraitCollection as a struct that contain's UITraitCollection 's userInterfaceStyle and userInterfaceLevel values as isLight and isElevated respectively since those properties are only available when linked with newer iOS versions. More on that later. Now that we have IGColorWithDynamicProvider we can use it everywhere in the app where we need to use dynamic colors. Developers can use this freely without worrying about build failures or run time crashes regardless of what version of Xcode they or their peers are using. Instagram has historically had a semantic color palette that was introduced in our 2016 redesign , and we collaborated with our design systems team to update all the colors in it to support dark mode using IGColorWithDynamicProvider . Here’s an example of one of these colors. Once we had this pattern defined for wrapping UIKit’s API we continued to add more as they were needed. The set we ended up with is: IGColorWithDynamicProvider as shown here IGImageWithDynamicProvider for creating “dynamic images“ that automatically adapt to dark mode. IGActivityIndicator functions for creating activity indicators with styles that work in light mode, dark mode, and older versions of iOS. IGSetOverrideUserInterfaceStyle for forcing views or view controllers into particular interface styles. IGSetOverrideElevationLevel for forcing view controllers into particular elevation levels. Small side note: We discovered towards the end of our dark mode adoption that our implementation of dynamic colors had equality implications because a new instance of UIColor was returned each time and the only thing that was comparable about each was the block passed in. In order to resolve this we modified our API slightly to create single instances of each of semantic colors so that they were comparable. Doing something like dispatch_once -ing your semantic colors or using asset catalog-based colors and +colorNamed: will produce comparable colors if your app is sensitive to color equality. One tricky thing when adopting technologies in iOS betas is getting adequate test coverage. Convincing people using the internal build of Instagram to install iOS 13 on their devices isn’t a great idea because it’s unstable and challenging to help set up, and even if we were to get people testing on iOS 13 the builds we distribute internally were still largely being linked against the iOS 12 SDK so the changes wouldn’t show up anyway. I briefly touched on our IGTraitCollection wrapper for UITraitCollection that came in handy in the course of building out dark mode. One clever testing trick this IGTraitCollection wrapper afforded us is something we’ve come to call “fake dark mode” — which is an internal setting that overrides IGTraitCollection to become dark even in iOS 12! Nate Stedman, one of our iOS engineers in New York, came up with this setting when we were first working on dark mode. Our API for generating IGTraitCollections from UITraitCollections came to look like this. Where _IGIsDarkModeDebugEnabled is backed by an NSUserDefaults flag for fake dark mode. There are of course some limitations with faking out dark mode in iOS 12, most notably userInterfaceLevel isn’t available in iOS 12, so “elevated“ dynamic colors never appear in fake dark mode. Forcing particular styles via our -setOverrideInterfaceStyle: wrapper has no effect in fake dark mode. UIKit components that use their default colors don’t adapt to fake dark mode in iOS 12 since they have no knowledge of dark mode. With this addition to our dark mode wrappers we were able to get much broader test coverage than we otherwise would have. Dark mode has been a highly requested featured of ours for quite a while. We had been a little reluctant in introducing dark mode in the past because it would’ve been a tremendous undertaking, but the excellent tools that Apple provides and their emphasis on dark mode in iOS 13 finally made it possible for us! Of course the actual implementation still wasn’t easy, we’ve been working on this since WWDC and it demanded ample design and engineering deep dives into every part of the app (and admittedly, we have probably missed some). This journey has been worth it, on top of the benefits dark mode provides such as eye strain reduction and battery savings, it makes our app look right at home on iOS 13! A huge thank you to Jeremy Lawrence, Nate Stedman, Cameron Roth, Ryan Olson, Garrett Olinger, Paula Guzman, Héctor Ramos, Aaron Pang, and numerous others who contributed to our efforts to adopt dark mode. Dark mode is also available in Instagram for Android. If you want to learn more about this work or are interested joining one of our engineering teams, please visit our careers page , follow us on Facebook or on Twitter . Stories from the people who build @Instagram 890 1 iOS Dark Mode Ios 13 Instagram Engineering 890 claps 890 1 Written by iOS engineer at Instagram, previously at Pinterest, Flipboard. Stories from the people who build @Instagram Written by iOS engineer at Instagram, previously at Pinterest, Flipboard. Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-01-22"},
{"website": "Instagram-Engineering", "title": "instagram data saver mode", "author": ["Cristina Acha"], "link": "https://instagram-engineering.com/instagram-data-saver-mode-ffb01fd5a6bd", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts We recently shipped Data Saver Mode, a new feature on Instagram for Android that helps the app consume less mobile data. In this post, we’ll go over why we decided to work on this feature, our approach in developing it, the engineering implementation, and how people were impacted by it. The motivation behind building Data Saver Mode was threefold: First, research suggested many people felt constrained while using Instagram because it consumed a big percentage of their data. We ran a survey in a few different countries (United States, Great Britain, India, Indonesia, Brazil, France, Germany, Japan, Argentina) in early 2018, which asked the question ‘How much data does Instagram use?’. More than 50% of respondents selected ‘a lot more than I expect’ or ‘a little more than I expect’. Furthermore, 30% of respondents said they run out of data every month, and 28% of people said they would use Instagram more if there was a Data Saver Mode feature. The second motivation was that we noticed Instagram would consume more data than we had initially desired. Intuitively, this made sense, as the more someone uses Instagram, the more content is consumed. However, we did note that overall data efficiency (i.e, time spent using the actual app for every megabyte of data consumed) could be improved. To illustrate, below you can see how IG data efficiency ranked below than that of Facebook, Facebook Lite, and WhatsApp. The third motivation was Android’s Native Data Saver feature, which was starting to gain traction among users. From Android 7.0 onwards, people can enable Data Saver for the entire device. When enabled, the system will block background use of cellular data, as well as signaling the app to consume less data while in the foreground. We have used IG while having native Data Saver mode on, and because our app is media-heavy, this native implementation causes a disruptive user experience, as photos and videos would either load very slowly, or simply not load at all. Google provides APIs to check whether someone has turned on Data Saver at the system level, but has not provided a way for developers to change native Data Saver implementation at the application level [1]. People can also restrict apps in native Data Saver, so if someone enables Data Saver, they could restrict IG and use our custom Data Saver Mode instead, for a less disruptive user experience. Below are the levers that we used for our Data Saver implementation: 1. Disabling Video Prefetch We currently prefetch upcoming videos in a person’s feed and stories viewer so that the videos are ready to play when a user arrives to a video on screen. We hypothesized that this behavior uses more data, especially if the user does not end up scrolling to the upcoming videos later in the feed. Thus, we can disable video prefetch so that we are only fetching video content when the user has paused their scrolling at a video, indicating they are watching the video. This would reduce data usage since a person may not want to watch all videos they are scrolling through in their feed. 2. Disabling Video Auto Play We currently automatically play all videos when they become visible on screen without user interaction. We hypothesized that this behavior uses more data, especially if the user does not intend to watch every single video that they scroll through. Thus, we can disable autoplay, and display a play button to allow users to manually play videos. This is a more drastic version of disabling video prefetch, since it requires extra user interaction to engage with a video media. 3. Reducing media quality/resolution Currently, we decide image and video resolution based on constraints such as a user’s connectivity and bandwidth. Rendering high resolution media is going to consume more data than low resolution media, given the larger file size. This can matter quite a bit for users in unique connectivity situations, which can be difficult to detect at the application level. For example, people using pocket mobile will appear as if they are on wifi, when in reality, they are using cellular data. Thus, we can provide a setting for users to decide at what connectivity setting they want to view higher resolution media. This allows users to still browse the content they care about, without using too much data. The tradeoff with enabling the above three levers is that we want to ensure users still have a consistent browsing experience with reasonable media loading time, given that Instagram is a media-heavy app. At Instagram, we understand that a meaningful portion of our users are in markets where connectivity can only be accessed through mobile cellular data, as opposed to at-home connectivity (i.e, WiFi). Demand for affordable connectivity has grown, so the cost of data becomes a key factor in a user’s decision to engage with online content. While we can primarily look at emerging markets, there are also industrial countries where the high cost of data means that a considerable part of the population is also data-conscious. Taking all this into account, we tested in Indonesia, India, Argentina, Germany and France. The team tested several different variants of Data Saver with the above three parameters, and with variants that displayed user-visible options for disabling autoplay and controlling media quality. During this first country test, we found that disabling video prefetch provided a good balance of reducing data usage while still providing a reasonable browsing experience. Predictably, by not auto-playing video content, people consumed less video. However, we also saw that people valued explicit control over media quality and auto-play. We found that there were two variants that tied as best performing test variants: Disabling video prefetching. Option of choosing when to receive High Resolution Media (“Never”, “only on Wi-Fi”, or “both on Wi-Fi and cellular”), with selection defaulted to “only on Wi-Fi” Disabling video prefetching. Option of choosing when to receive High Resolution Media (“Never”, “only on Wi-Fi”, or “both on Wi-Fi and cellular”), with selection defaulted to “both on Wi-Fi and cellular” The variants are the same, the only difference is that the default selection is different (“only on Wi-Fi” and “both on Wi-Fi and cellular”) for the High Resolution Media option. Each performed better in certain countries, so we decided we would do another country test in Canada and Great Britain (as previous research had shown these countries are also data conscious). This informed our decision to finally test globally and launch to everyone. For our global launch, we decided to keep the High Resolution Media user option and defaulted it to “only on Wi-Fi”. For our test in CA & GB, the best performing version was the one that disabled video prefetching and defaulted to high resolution media on both Wi-Fi and Cellular. Note that as this is an opt-in feature and only ~10% of people in the test group opted in, so the results we saw in our A/B test were quite diluted. Nonetheless, we saw a sizable decrease in data usage while on cellular. We also saw increases in number of interactions, number of media created, and other engagement metrics. These were significant wins, especially when you take into account that only 10% of users in the test group were driving them. Finally, we only saw regressions in video loading metrics, which we expected from our disabling of video prefetching, but they were not too big. In our CA & GB test, 1.6% of users in the default high resolution media only on Wi-Fi switched to high resolution media on both Wi-Fi and Cellular, and 10% of users in the default high resolution media on both Wi-Fi and Cellular switched to high resolution media only on Wi-Fi. Admittedly, we are not entirely certain as to why we saw this large difference, but one of our hypotheses is that people who are very conscious about their data consumption are more likely to actively employ options to conserve data. For our global test, the best performing version was the one that disabled video prefetching and defaulted to high resolution media only on Wi-Fi. This test was done through a less targeted lens, but we still saw improvements in engagement and data consumption on cellular. Given all of these positive results, we concluded this feature would be very beneficial for data-constrained users and we shipped it globally in June 2019. [1] https://developer.android.com/training/basics/network-ops/data-saver The Data Saver Mode feature wouldn’t have been possible without the collaboration of research, data, engineering, and product. Thanks to Elisa Lou, who co-authored this post with me and is the engineer who worked on Data Saver Mode — couldn’t have done it without you. Thanks to Kat Li, Jeff LaFlam, Michael Midling, Colin Shepherd and many more. If you want to learn more about this work or are interested in joining one of our engineering teams, please visit our careers page , follow us on Facebook or on Twitter . Stories from the people who build @Instagram 547 10 Tech Instagram Android Data 547 claps 547 10 Written by Stories from the people who build @Instagram Written by Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-12-13"},
{"website": "Instagram-Engineering", "title": "making instagram com faster part 3 cache first", "author": ["Glenn Conner"], "link": "https://instagram-engineering.com/making-instagram-com-faster-part-3-cache-first-6f3f130b9669", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts In recent years instagram.com has seen a lot of changes — we’ve launched stories, filters, creation tools, notifications, and direct messaging as well as a myriad of other features and enhancements. However, as the product grew, a side effect was that our web performance began to slow. Over the last year we made a conscious effort to improve this. This ongoing effort has thus far resulted in almost 50% cumulative improvement to our feed page load time. This series of blog posts will outline some of the work we’ve done that led to these improvements. In part 1 we talked about prefetching data and in part 2 we talked about improving performance by pushing data directly to the client rather than waiting for the client to request the data. Since we’re already pushing data to the client at the earliest possible time in the page load — the only faster way to get data to the client would be to not have to fetch or push any data at all. We can do this using a cache-first rendering approach, though this does mean that we have to display stale feed data to users for a short period of time. With this approach, when the page is loaded, we immediately present users with a cached copy of their previous feed and stories tray, and then replace it with fresh data once it’s available. We use Redux to manage state on instagram.com, so at a high level the way we implemented this was to store a subset of our Redux store on the client in an indexedDB table, and then rehydrate the store when the page first loads. However, because of the asynchronous nature of indexedDB access, server data fetching, and user interactions, we can run into problems where the user interacts with the cached state, but then we want to ensure that those interactions are still applied to the new state when it arrives from the server. For example, if we were to handle caching in a naive way we could run into the following problem: We begin loading from cache and from the network concurrently and since the cached feed is ready first, we display it to the user. The user then proceeds to like a post, but once the network response for the latest feed comes back it overwrites that post with a copy that doesn’t include the like action that the user applied to the cached copy (see the diagram below). To solve this issue, we needed a way to apply interactions to the cached state, but also store those interactions so they can be replayed later over the new state from the server. If you’ve ever used Git or similar source control systems before, this problem might seem familiar. If we think of the cached feed state as a branch, and the server feed response as master, what we effectively want to do is to do a rebase operation, applying the commits (likes, comments etc.) from our local branch onto the head of master. This brings us to the following design: On page load, we send a request for the new data (or wait for it to be pushed) Create a staged subset of the Redux state While the request/push is pending, we store any dispatched actions Once the request resolves, we apply the action with the new data and any actions that have been pending to the staged state When the staged state is committed, we simply replace the current state with the staged one. By having a staging state, all the existing reducer behavior can be reused. It also keeps the staged state (which has the most recent data) separate from the current state. Also, since staging is implemented using Redux, we just need to dispatch actions to use it! The staging API consists of two main functions: stagingAction & stagingCommit (as well as a couple of others for handling reverts and edge cases that we won't cover here). stagingAction accepts a promise that resolves an action to be dispatched to the staged state. It initializes the staging state and keeps track of any actions that have been dispatched since it was initialized. In the source control analogy we can think of this as creating a local branch as any actions that take place will now be queued and applied over the staged state when the new data arrives. stagingCommit commits the staging state to the current state. If any async actions on the staging state are pending, it will wait before committing. This is similar to a rebase in source control terms, in that we apply all our local changes (from the cache branch) on top of master (the new data from the server), leaving our local branch up to date. To enable staging, we wrap the root reducer with a reducer enhancer that handles the stagingCommit action and applies the staged actions to the new state. To use all this, we just need to dispatch the relevant actions and everything is handled for us. For example, if we want to fetch a new feed and apply it to a staged state, we can do something similar to the following: Using cache-first rendering for both feed posts and the stories tray led to a 2.5% and 11% improvement in respective display done times and bought the user experience more in-line with what is available on the native iOS and android Instagram apps. In part 4 we’ll cover how we reduced the size of our codebase and improved its performance through code size and execution optimizations. If you want to learn more about this work or are interested joining one of our engineering teams, please visit our careers page , follow us on Facebook or on Twitter . Stories from the people who build @Instagram 2K 5 React Redux JavaScript Web Performance Instagram 2K claps 2K 5 Written by Interested in games, art, technology, and distributing the future. Stories from the people who build @Instagram Written by Interested in games, art, technology, and distributing the future. Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-10-11"},
{"website": "Instagram-Engineering", "title": "building type mode for stories on ios and android", "author": ["Instagram Engineering"], "link": "https://instagram-engineering.com/building-type-mode-for-stories-on-ios-and-android-8804e927feba", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts Instagram recently launched Type Mode , a new way to post creative, dynamic text styles and backgrounds to Stories. Type Mode was an interesting challenge for us because it is the first time we were going to create a way for people to post Stories without photo or video components — and we wanted to make sure Type Mode was still a fun, customizable and visually expressive experience. Making Type Mode function seamlessly on both iOS and Android had its own set of challenges, including dynamically resizing text and custom background fills. In this post we’ll take a look at how we approached this work on both iOS and Android platforms. With Type Mode, we wanted to create a text input experience that let people emphasize certain words or phrases. One way to do that was to build fully justified text styles that dynamically resize each line to fill the available width (used in Modern, Neon, and Strong). iOS The main challenge on iOS was to render dynamically resizing text in a native UITextView, which would let people enter text in a quick and familiar way. RESIZING PRE-TEXT STORAGE COMMIT While you input text on a line, the size of text on that line should scale down until it hits some minimum font size. To accomplish this, we use a combination of UITextView.typingAttributes , NSAttributedString , and NSLayoutManager . First, we need to calculate what font size our text will be rendered as. We can grab the range of the line we are currently typing on using [NSLayoutManager enumerateLineFragmentsForGlyphRange:usingBlock:] . From that range we can create a sizing string to calculate a minimum font size. In order to actually draw the text at the correct size, we need to use our new font size in our UITextView 's typingAttributes . UITextView.typingAttributes are attributes that apply to new text being typed by the user. A good place for this is [id<UITextViewDelegate> textView:shouldChangeTextInRange:replacementText:] . This means that as the user types, the font size will shrink until we hit some specified minimum. When this happens, the UITextView will wrap our text as it normally would. FINALIZE TEXT POST STORAGE COMMIT After our text has been committed to text storage, we may need to clean up some sizing attributes. Our text could have wrapped, or the user could have “emphasized” text by manually adding a line break to write larger text on a separate line. A good place to put this logic is in [id<UITextViewDelegate> textViewDidChange:] . This happens after the text has been committed to text storage and initially laid out by the text engine. To get a list of character ranges for each line, we can use NSLayoutManager : We then need to manipulate the NSTextStorage by setting attributes on ranges that have the correct font size for each row. There are three stages to editing NSTextStorage , which is itself a subclass of NSMutableAttributedString . Call [textStorage beginEditing] to indicate we are making one or more changes to the text storage. Send some editing messages to NSTextStorage . In our case, the NSFontAttributeName attribute should be set with the correct font size for that row. We can use a similar method for calculating the font size as we did earlier. 3. Call [textStorage endEditing] to indicate we are done editing text storage. This invokes the [NSTextStorage processEditing] method which will fix attributes on the ranges we changed. This also invokes the correct NSTextStorageDelegate methods. TextKit is a powerful and modern API that is tightly integrated with UIKit. Many text experiences can be designed with it, and new text APIs are being released in most new iOS versions. With TextKit, you can do anything from creating custom text containers to modifying the actual generation of glyphs. And since it is built on top of CoreText and integrated with APIs like UITextView, text input and editing continues to feel like a native iOS experience. Android doesn’t support full width justification out of the box, but the framework APIs give us all the tools we need to implement it ourselves. The first step is to lay the text out at the minimum text size. We’ll scale things up later, but this will tell us how many lines we have and where the line breaks are: Next we need to go through the layout and resize each line individually. There’s no direct way to find the perfect text size for a particular line, but we can easily approximate it with a binary search for the largest text size that doesn’t force a line break: Once we’ve found the right size for each line, we apply it as a span. Spans let us use different text sizes for each line instead of a single text size for the entire string: Now each line of text fills the available width! We can repeat this process each time the text changes to get the dynamic resizing behavior we’re looking for. We also wanted Type Mode to let people emphasize words and phrases with text backgrounds (used in Typewriter and Strong). Another way we can leverage NSLayoutManager is for drawing custom background fills. NSAttributedString does have a NSBackgroundColorAttributeName , but it is not customizable nor extensible. For example, if we used NSBackgroundColorAttributeName , the background of the entire text view would be filled in. We could not exclude whitespace, have spaces between the lines, or draw the fill with a corner radius. Thankfully, NSLayoutManager lets us override drawing the background fill. We need to create a NSLayoutManager subclass and override drawBackgroundForGlyphRange:atPoint: With our drawBackgroundForGlyphRange:atPoint method, we can once again leverage [NSLayoutManager enumerateLineFragmentsForGlyphRange:usingBlock] to grab the glyph ranges of each line fragment. We can then use [NSLayoutManager boundingRectForGlyphRange:inTextContainer] to get the bounding rectangle of each line. This allows us to draw a background fill around any arbitrary text with our own specified shapes and padding. NSLayoutManager can also be utilized to draw other text attributes like strikethroughs and underlines. Android At first glance, it feels like this should be simple to implement on Android. We can add a span that modifies the text background color and be done: That’s a good first attempt (and was the first thing we built), but it comes with a few limitations: The background tightly wraps the text and there’s no way to adjust the padding. The background is rectangular and there’s no way to adjust the corner radius. To address those issues, we tried using LineBackgroundSpan . We were already using it to render the rounded bubble background on Classic text, so it seemed like a natural fit for the new text styles as well. Unfortunately our new use case uncovered a subtle bug in the framework Layout class. If your text has multiple LineBackgroundSpan instances on different lines, then Layout will not iterate through them properly and some of them may never be rendered. Thankfully we can sidestep the framework bug by applying a single LineBackgroundSpan to the entire string and then delegating to individual background spans ourselves: Instagram has a very strong prototyping culture, and the design team’s Type Mode prototypes let us get a real feel for the user experience with each iteration along the way. For example, with the Neon style, we needed a way to take a single color from our palette and then generate an interior color and a glow color for the text. A designer on the project played around with some approaches in his prototype, and when he found one he liked we essentially just copied his logic on Android and iOS. This level of collaboration with the design team was a special part of this launch and made the development process really efficient. If you’re interested in working with us on Stories, check out our careers page for roles in Menlo Park, New York, and San Francisco. Christopher Wendel and Patrick Theisen are iOS and Android engineers at Instagram, respectively. Stories from the people who build @Instagram 1.6K 8 Android iOS UI UI Design 1.6K claps 1.6K 8 Written by Stories from the people who build @Instagram Written by Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-04-14"},
{"website": "Instagram-Engineering", "title": "open sourcing a 10x reduction in apache cassandra tail latency", "author": ["Instagram Engineering"], "link": "https://instagram-engineering.com/open-sourcing-a-10x-reduction-in-apache-cassandra-tail-latency-d64f86b43589", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts At Instagram, we have one of the world’s largest deployments of the Apache Cassandra database. We began using Cassandra in 2012 to replace Redis and support product use cases like fraud detection, Feed, and the Direct inbox. At first we ran Cassandra clusters in an AWS environment, but migrated them over to Facebook’s infrastructure when the rest of Instagram moved. We’ve had a really good experience with the reliability and availability of Cassandra, but saw room for improvement in read latency. Last year Instagram’s Cassandra team started working on a project to reduce Cassandra’s read latency significantly, which we call Rocksandra. In this post, I will describe the motivation for this project, the challenges we overcame, and performance metrics in both internal and public cloud environments. At Instagram, we use Apache Cassandra heavily as a general key value storage service. The majority of Instagram’s Cassandra requests are online, so in order to provide a reliable and responsive user experience for hundreds of millions of Instagram users, we have very tight SLA on the metrics. Instagram maintains a 5–9s reliability SLA, which means at any given time, the request failure rate should be less than 0.001%. For performance, we actively monitor the throughput and latency of different Cassandra clusters, especially the P99 read latency. Here’s a graph that shows the client-side latency of one production Cassandra cluster. The blue line is the average read latency (5ms) and the orange line is the P99 read latency (in the range of 25ms to 60ms and changing a lot based on client traffic). After investigation, we found the JVM garbage collector (GC) contributed a lot to the latency spikes. We defined a metric called GC stall percentage to measure the percentage of time a Cassandra server was doing stop-the-world GC (Young Gen GC) and could not serve client requests. Here’s another graph that shows the GC stall percentage on our production Cassandra servers. It was 1.25% during the lowest traffic time windows, and could be as high as 2.5% during peak hours. The graph shows that a Cassandra server instance could spend 2.5% of runtime on garbage collections instead of serving client requests. The GC overhead obviously had a big impact on our P99 latency, so if we could lower the GC stall percentage, we would be able to reduce our P99 latency significantly. Apache Cassandra is a distributed database with it’s own LSM tree-based storage engine written in Java. We found that the components in the storage engine, like memtable, compaction, read/write path, etc., created a lot of objects in the Java heap and generated a lot of overhead to JVM. To reduce the GC impact from the storage engine, we considered different approaches and ultimately decided to develop a C++ storage engine to replace existing ones. We did not want to build a new storage engine from scratch, so we decided to build the new storage engine on top of RocksDB. RocksDB is an open source, high-performance embedded database for key-value data. It’s written in C++, and provides official API language bindings for C++, C, and Java. RocksDB is optimized for performance, especially on fast storage like SSD. It’s widely used in the industry as the storage engine for MySQL, mongoDB, and other popular databases. We overcame three main challenges when implementing the new storage engine on RocksDB. The first challenge was that Cassandra does not have a pluggable storage engine architecture yet, which means the existing storage engine is coupled together with other components in the database. To find a balance between massive refactoring and quick iterations, we defined a new storage engine API, including the most common read/write and streaming interfaces. This way we could implement the new storage engine behind the API and inject it into the related code paths inside Cassandra. Secondly, Cassandra supports rich data types and table schema, while RocksDB provides purely key-value interfaces. We carefully defined the encoding/decoding algorithms to support Cassandra’s data model within RocksDB’s data structure and supported same-query semantics as original Cassandra. The third challenge was about streaming. Streaming is an important component for a distributed database like Cassandra. Whenever we join or remove a node from a Cassandra cluster, Cassandra needs to stream data among different nodes to balance the load across the cluster. The existing streaming implementation was based on the details in the current storage engine. Accordingly, we had to decouple them from each other, make an abstraction layer, and re-implement the streaming using RocksDB APIs. For high streaming throughput, we now stream data into temp sst files first, and then use the RocksDB ingest file API to bulk load them into the RocksDB instance at once. After about a year of development and testing, we have finished a first version of the implementation and successfully rolled it into several production Cassandra clusters in Instagram. In one of our production clusters, the P99 read latency dropped from 60ms to 20ms. We also observed that the GC stalls on that cluster dropped from 2.5% to 0.3%, which was a 10X reduction! We also wanted to verify whether Rocksandra would perform well in a public cloud environment. We setup a Cassandra cluster in an AWS environment using three i3.8 xlarge EC2 instances, each with 32 cores CPU, 244GB memory, and raid0 with 4 nvme flash disks. We used NDBench for the benchmark, and the default table schema in the framework: TABLE emp ( emp_uname text PRIMARY KEY, emp_dept text, emp_first text, emp_last text ) We pre-loaded 250M 6KB rows into the database (each server stores about 500GB data on disk). We configured 128 readers and 128 writers in NDBench. We tested different workloads and measured the avg/P99/P999 read/write latencies. As you can see, Rocksandra provided much lower and consistent tail read/write latency. We also tested a read-only workload and observed that, at similar P99 read latency (2ms), Rocksandra could provide 10X higher read throughput (300K/s for Rocksandra vs. 30K/s for C* 3.0). We have open sourced our Rocksandra code base and benchmark framework , which you can download from Github to try out in your own environment! Please let us know how it performs. As our next step, we are actively working on the development of more C* features support, like secondary indexes, repair, etc. We are also working on a C* pluggable storage engine architecture to contribute our work back to the Apache Cassandra community. If you are in the Bay Area and are interested in learning more about our Cassandra developments, join us at our next meetup event here . Dikang Gu is an infrastructure engineer at Instagram. Stories from the people who build @Instagram 4.5K 19 Cassandra Open Source Rocksdb Instagram 4.5K claps 4.5K 19 Written by Stories from the people who build @Instagram Written by Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-03-05"},
{"website": "Instagram-Engineering", "title": "making instagram com faster code size and execution optimizations part 4", "author": ["Glenn Conner"], "link": "https://instagram-engineering.com/making-instagram-com-faster-code-size-and-execution-optimizations-part-4-57668be796a8", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts In recent years instagram.com has seen a lot of changes — we’ve launched stories, filters, creation tools, notifications, and direct messaging as well as a myriad of other features and enhancements. However, as the product grew, a side effect was that our web performance began to slow. Over the last year we made a conscious effort to improve this. This ongoing effort has thus far resulted in almost 50% cumulative improvement to our feed page load time. This series of blog posts will outline some of the work we’ve done that led to these improvements. In part 1 we talked about prefetching data, in part 2 we talked about improving performance by pushing data directly to the client rather than waiting for the client to request the data, and in part 3 we talked about cache-first rendering. In parts 1–3 we covered various ways that we optimized the loading patterns of the critical path static resources and data queries. However there is another key area we haven’t covered yet that’s crucial to improving web application performance, particularly on low-end devices — ship less code to the user — in particular, ship less JavaScript . This might seem obvious, but there are a few points to consider here. There’s a common assumption in the industry that the size of the JavaScript that gets downloaded over the network is what’s important (i.e. the size post-compression ), however we found that what’s really important is the size pre-compression as this is what has to be parsed and executed on the user’s device, even if it’s cached locally. This becomes particularly true if you have a site with many repeat users (and subsequent high browser cache hit rates) or users accessing your site on mobile devices. In these cases the parsing and execution performance of JavaScript on the CPU becomes the limiting factor rather than the network download time. For example, when we implemented Brotli compression for our JavaScript assets, we saw a nearly 20% reduction of post-compression size across the wire, but NO statistically significant reduction in the overall page load times as seen by end users. On the other hand, we’ve found reductions in pre-compression JavaScript size have consistently led to performance improvements. It’s also worth making a distinction between JavaScript that is executed on the critical path and JavaScript that is dynamically imported after the main page has completed. While ideally it would be nice to reduce the total amount of JavaScript in an application, a key thing to optimize in the short term is the amount of eagerly executed JavaScript on the critical path (we track this with a metric we call Critical Bytes Per Route). Dynamically imported JavaScript that lazy loads is generally not going to have as significant an effect on page load performance, so it’s a valid strategy to move non-visible or interaction dependent UI components out of the initial page bundles and into dynamically imported bundles. Refactoring our UI to reduce the amount of script on the critical path is going to be essential to improving performance in the long term — but this is a significant undertaking which will take time. In the short-term we worked on a number of projects to improve the size and execution efficiency of our existing code in ways that are largely transparent to product developers and require little refactoring of existing product code. We bundle our frontend web assets using Metro (the same bundler used by React Native) so we get access to inline-requires out of the box. Inline-requires moves the cost of requiring/importing modules to the first time when they are actually used. This means that you can avoid paying execution cost for unused features (though you’ll still pay the cost of downloading and parsing them) and you can better amortize the execution cost over the application startup, rather than having a large amount of upfront computation. To see how this works in practice, lets take the following example code: Using inline requires this would get transformed into something like the following (you’ll find these inline requires by searching for r(d[ in the Instagram JS source in your browser developer tools) As we can see, it essentially works by replacing the local references to a required module with a function call to require that module. This means that unless the code from that module is actually used, the module is never required (and therefore never executed). In most cases this works extremely well, but there are a couple of edge cases to be aware of that can cause problems — namely modules with side effects. For example: Without inline requires, Module C would output {'foo':'bar'} , but when we enable inline-requires, it would output undefined , because B has an implicit dependency on A. This is a contrived example, but there are other real world cases where this can have effects i.e. what if a module does some logging as a part of its initialization - enabling inline-requires could cause this logging to stop happening. This is mostly preventable through linters that check for code that executes immediately at the module scope level, but there were some files we had to blacklist from this optimization such as runtime polyfills that need to execute immediately. After experimenting enabling inline requires across the codebase we saw an improvement in our Feed TTI (time to interactive) by 12% and Display Done by 9.8%, and decided that dealing with some of these minor edge cases was worth it for the performance improvements. One of the primary drivers that drove the adoption of compiler/transpiler tools like Babel was allowing developers to use modern JavaScript coding idioms but still have their applications work in browsers that lacked native support for these latest language features. Since then a number of other important use-cases for these tools arose including compile-to-js languages like Typescript and ReasonML, language extensions such as JSX and Flow type annotations, and build time AST manipulations for things like internationalization. Because of this, it’s unlikely that this extra compilation step is going to go disappear from frontend development workflows any time soon. However, with that said it’s worth revisiting if the original purpose for doing this (cross browser compatibility) is still necessary in 2019. ES2015 and more recent features like async/await are now well supported across recent versions of most major browsers, so directly serving JavaScript containing these newer features is definitely possible — but there are two key questions that we had to answer first: Would enough users be able to take advantage of this to make the extra build complexity worthwhile (as you’d still need to maintain the legacy transpiling step for older browsers), And what (if any) are the performance advantages of shipping ES2015+ features To answer the first question we first had to determine which features we were going to ship without transpiling/polyfilling and how many build variants we wanted to support for the different browsers. We settled on having two builds, one that would require support for ES2017 syntax, and a legacy build that would transpile back to ES5 (in addition we also added an optional polyfill bundle that would only be added for legacy browsers that lacked runtime support for more recent DOM API’s). Detecting support for these groups is done via some basic user-agent sniffing on the server side which ensures there is no runtime cost or extra roundtrip time from doing client-side detection of which bundles to load. With this in mind, we ran the numbers and determined that 56% of users to instagram.com are able to be served the ES2017 build without any transpiling or runtime polyfills, and considering that this percentage is only going to go up over time — it seems like its worth supporting two builds considering the number of users able to utilize it. As for the second question — what are the performance advantages of shipping ES2017 directly — lets start by looking at what Babel actually does to transpile some common constructs back to ES5. In the left hand column is the ES2017 code, and on the right is the transpiled ES5 compatible version. From this we can see that there is a considerable size overhead in transpiling these constructs (even if you amortize the cost of some of the runtime helper functions over a large codebase). In the case of Instagram, we saw a 5.7% reduction in the size of our core consumer JavaScript bundles when we removed all ES2017 transpiling plugins from our build. In testing we found that the end-to-end load times for the feed page improved by 3% for users who were served the ES2017 bundle compared with those who were not. While the progress that has been made so far is impressive, the work we’ve done so far represents just the beginning. Theres still a huge amount of room left for improvement in areas such as Redux store/reducer modularization, better code splitting, moving more JavaScript execution off the critical path, optimizing scroll performance, adjusting to different bandwidth conditions, and more. If you want to learn more about this work or are interested joining one of our engineering teams, please visit our careers page , follow us on Facebook or on Twitter . Stories from the people who build @Instagram 1.8K 7 JavaScript Es2017 React Native Web Performance Instagram 1.8K claps 1.8K 7 Written by Interested in games, art, technology, and distributing the future. Stories from the people who build @Instagram Written by Interested in games, art, technology, and distributing the future. Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-11-01"},
{"website": "Instagram-Engineering", "title": "let your code type hint itself introducing open source monkeytype", "author": ["Instagram Engineering"], "link": "https://instagram-engineering.com/let-your-code-type-hint-itself-introducing-open-source-monkeytype-a855c7284881", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts Today we are excited to announce we’re open-sourcing MonkeyType , our tool for automatically adding type annotations to your Python 3 code via runtime tracing of types seen. At Instagram we have hundreds of engineers working on well over a million lines of Python 3. Every day we have new engineers joining the team from other projects and other languages who need to ramp up quickly and get productive in our codebase. And we’re constantly shipping new code into production, every few minutes, all day long, every day. So we’re keen to make our code easier for new developers to read and understand, as well as more amenable to static analysis that shrinks the domain of possible bugs. Type annotations and static type checking fit that bill. Writing new code with type annotations is easy enough; most of our engineers are eager to do that. But the returns on static type checking are low until we reach a critical mass of type-annotated code, especially our core frameworks and libraries. In other words: we have a lot of existing code that needs type annotations added. Our first forays into manually adding type annotations were discouraging. It can take hours to annotate a single module, sometimes painstakingly tracing through multiple layers of function calls and objects to understand the possible types at some call site. (This is, of course, the same pain that anyone trying to maintain that function might experience; that’s why we want to add type annotations!) So we built MonkeyType. Instead of guessing or spelunking for the right types, let your test suite or (better!) your production system tell you what the real types are. Sounds great! I’ve run pip install monkeytype . What's next? Before MonkeyType can tell us anything useful, we need to let it trace some function calls. The simplest way to do this is with monkeytype run , which runs any Python script under MonkeyType tracing. For instance, you can easily run your test suite under MonkeyType: (or monkeytype run `which pytest` , or whatever your preferred flavor.) While your tests are running, MonkeyType inspects the argument types and return/yield type of every function call, recording them in a database. (By default it keeps them in a local SQLite database, but like just about everything MonkeyType does, this is configurable .) Of course, your test suite may not provide the best type information — sometimes tests use fakes instead of the real types, and we’ve found plenty of cases where type checking revealed that our tests were accidentally passing in different types from production. So if you don’t want to annotate based on your test suite, you can record call traces from production runtime. For this use case, MonkeyType provides a context manager API : If you need even more flexibility, you can create your own CallTracer , install it with sys.setprofile() , and remove it when you're ready. Once you've got some call traces recorded, you can generate a stub file for any module: If the stub looks reasonable and you want to apply the annotations directly to your code, MonkeyType will do that too: Review the now-annotated code in some/module.py , correct the annotations if needed, and commit it! Because of the backing-store design , you don't have to record traces and annotate all at one go. You can collect traces into the database over a long period of time and gradually annotate more and more modules from the collected data as you're ready to do so. With configurable type rewriters , you can easily tweak MonkeyType’s generated type annotations for your preferred type annotation style or specific edge cases in your codebase. For lots more details on customization options, refer to the documentation . Check out the code , give it a spin, and let us know what you think! We’re looking forward to your bug reports and suggested improvements. If you’d like to contribute, we have a list of suggested starter tasks in the issue tracker. MonkeyType does require Python 3.6+, and generates only Python 3 style type annotations (no type comments). If you aren’t quite there yet, you may want to start with our PyCon 2017 keynote on how we migrated those million lines of code to Python 3 last year. We choose a small random sample of production web requests and turn on MonkeyType tracing via a Django middleware. Traces are stored and retrieved from SCUBA , Facebook’s data analysis platform, using a custom CallTraceStore . We run tracing constantly, so we are always adding new type traces from production. Since production code changes frequently, this keeps our traces up to date. Whenever an engineer wants to add type annotations to a module, they just run monkeytype stub and then monkeytype apply , fix any type errors revealed by the new annotations, and submit a diff! With MonkeyType’s help, we’ve already annotated over a third of the functions in our codebase, and we’re already seeing type-checking catch many bugs that would have otherwise likely shipped to production. Race you to 100%! Carl Meyer is an engineer on Instagram’s infrastructure team. Stories from the people who build @Instagram 3.4K 8 Python Python3 Open Source 3.4K claps 3.4K 8 Written by Stories from the people who build @Instagram Written by Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-12-16"},
{"website": "Instagram-Engineering", "title": "copy on write friendly python garbage collection", "author": ["Instagram Engineering"], "link": "https://instagram-engineering.com/copy-on-write-friendly-python-garbage-collection-ad6ed5233ddf", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts At Instagram, we have the world’s largest deployment of the Django web framework, which is written entirely in Python. We began using Python early on because of its simplicity, but we’ve had to do many hacks over the years to keep it simple as we’ve scaled. Last year we tried dismissing the Python garbage collection (GC) mechanism (which reclaims memory by collecting and freeing unused data), and gained 10% more capacity. However, as our engineering team and number of features have continued to grow, so has memory usage. Eventually, we started losing the gains we had achieved by disabling GC. Here’s a graph that shows how our memory grew with the number of requests. After 3,000 requests, the process used ~600MB more memory. More importantly, the trend was linear. From our load test, we could see that memory usage was becoming our bottleneck. Enabling GC could alleviate this problem and slow down the memory growth, but undesired Copy-on-write (COW) would still increase the overall memory footprint. So we decided to see if we could make Python GC work without COW, and hence, the memory overhead. If you read our last GC post carefully, you’ll notice the culprit of the COW was ahead of each python object: The theory was that every time we did a collection, it would update the gc_refs with ob_refcnt for all tracked objects — but unfortunately this write operation caused memory pages to be COW-ed. A next obvious solution was to move all the head to another chunk of memory and store densely. We implemented a version where the pointer in the gc_head struct didn’t change during collection: Did it work? We used the following script to allocate the memory and fork a child process to test it: With the old gc_head struct, the child process’s RSS memory usage increased by ~60MB. Under the new data structure with the additional pointer, it only increased by ~0.9 MB. So it worked! However, you may have noticed the additional pointer in the proposed data structure introduced memory overhead (16 bytes — two pointers). It seems like a small number, but if you consider it applied to every collectable Python object (and we usually have millions of objects in one process, with ~70 processes per host), it could be a fairly big memory overhead on each server. 16 bytes* 1,000,000 * 70 = ~1 GB Even though the new gc_head data structure showed promising gains on memory size, its overhead was not ideal. We wanted to find a solution that could enable the GC without noticeable performance impacts. Since our problem is really only on the shared objects that are created in the master process before the child processes are forked, we tried letting Python GC treat those shared objects differently. In other words, if we could hide the shared objects from the GC mechanism so they wouldn’t be examined in the GC collection cycle, our problem would be solved. For that purpose, we added a simple API as gc.freeze() into the Python GC module to remove the objects from the Python GC generation list that‘s maintained by Python internal for tracking objects for collection. We have upstreamed this change to Python and the new API will be available in the Python3.7 release ( https://github.com/python/cpython/pull/3705 ). We deployed this change into our production and this time it worked as expected: COW no longer happened and shared memory stayed the same, while average memory growth per request dropped ~50%. The plot below shows how enabling GC helped the memory growth by stopping the linear growth and making each process live longer. Thanks to Jiahao Li, Matt Page, David Callahan, Carl S. Shapiro, and Chenyang Wu for their discussions and contributions to the COW-friendly Python garbage collection. Zekun Li is an infrastructure engineer at Instagram. Stories from the people who build @Instagram 2.1K 7 Python Python3 Python Programming 2.1K claps 2.1K 7 Written by Stories from the people who build @Instagram Written by Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-12-20"},
{"website": "Instagram-Engineering", "title": "improving performance with background data prefetching", "author": ["Instagram Engineering"], "link": "https://instagram-engineering.com/improving-performance-with-background-data-prefetching-b191acb39898", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts The Instagram community is bigger and more diverse than ever before. 800m people now visit every month, 80% of whom are outside of the United States. As the community grows, it becomes more and more important that our app can withstand diverse network conditions, a growing variety of devices, and non-traditional usage patterns. The client performance team at Instagram New York is focused on making Instagram fast and performant, no matter where someone is using it. Specifically, our team prioritizes instantaneous content delivery, zero wasted bytes over the network, and zero wasted bytes on disk. We recently decided to focus on effective background prefetching as a way to break the dependency of Instagram usability from network availability and a user’s data plan. Most of the world does not have enough network connectivity. Michael Midling, our data scientist, put together this map to represent the average network bandwidth while using Instagram in different countries around the world. Darker green regions, like Canada, have somewhere around 4+Mbps, vs. lighter green areas like India, which have an average network bandwidth of 1Mbps. We cannot assume that media will be available to watch when a user opens Instagram and starts watching stories or scrolling through feed. If you want to build a fast media app in India, where network availability is not rich and a round-trip time could be above two seconds, you need to come up with a different strategy than fetching resources in realtime. If we want everyone to be able to access Instagram and see videos and photos from their closest friends and interests, we have to be able to react to different network bandwidth speeds. Building apps that adapt to these network conditions has its challenges. One of our solutions was to add the user’s connection type into our logging events. This allowed us to observe the different usage patterns divided by connection type, which helped us adapt. We strive to be respectful of people’s data plans and try to maximize the data sourced over unmetered connections. The graph shows how Instagrammers around the world access our app. For example, we need to adapt to the network patterns in Indonesia where people are switching from one SIM card to another as soon as they run out of data and mainly access the app on cellular connection. On the other hand, people in Brazil mostly use our app over wifi. What if the network fails all together? Historically we would display a screen with grey boxes and hope for the user to come back and retry when they had a better connection. But this isn’t a good experience. Sporadic network connection and cellular network congestion are also concerns. When we’re in one of the light green areas of the map above, where network bandwidth is low, we need to find a way to shorten or eliminate people’s wait time. Our goal is for people to perceive no network connection as being online, but there is no one-size-fits-all solution for this. Below are some of the techniques we apply to adapt the offline experience to different types of usage in different conditions. We came up with a series of strategies. First, we focused on building the Offline Mode experience. With this experience we unlocked the possibility to deliver the content from disk as if it is coming from network. Second, leveraging this cache infrastructure, we built a centralized background prefetching framework, to populate this cache with unseen content. Through data analysis and user research, we’ve come up with a few principles that represent the major pain points and areas for improvement: Offline is a status, not an error The offline experience is seamless Build trust through clear communication You can see how this was implemented in the video: Using the response store, image and video cache, we can deliver content to the UI screen when it’s not retrieved from the network, which simulates a successful network call. There are three main components: the device screen, the device network layer that composes the HttpRequests, and the device network engine that takes care of delivering our network requests to the server. After building the ability to deliver content from disk, we noticed improvements to how people used Instagram in high-growth markets. We decided to cache the content after downloading from the network with the belief that seeing older content would be better than seeing gray boxes and white screens. But the ideal solution was still showing new content. That’s where background prefetching came in. At Instagram one of our engineering values is “do the simple thing first,” so our first approach was not to build the perfect background prefetching framework. The first prototype just prefetched data when the app got backgrounded, if and only if the user was under wifi connection. This BackgroundPrefetcher iterated through a list of runnables, executing one by one. This first prototype allowed us to: Iterate on kinds of content to prefetch Analyze the actual effects of delivering unseen cached content on the user experience Benchmark final framework (stability) The reality is that apps are complex, and so are people! When it comes to deciding what type of media to prefetch, you have to carefully analyze usage patterns. For example, some people may use some features more than others. Our home screen has a great diversity of items, from the Stories tray to individual stories media. We could also prefetch photos and videos for feed, the messages that you have pending, items for you to browse in Explore, or your most recent notifications. In our case we started simple, only prefetching some media for Explore, Stories and main feed. Building a centralized framework that is flexible enough to adapt to different use cases helps us maintain efficiency and scale properly. Apart from the ability to schedule the jobs in our framework that prefetched data with no control on the background, we added extra logic on top. Centralizing the logic for background prefetching to a single point made it possible to apply rules and verify that certain conditions are met, like: Control connection type -> unmetered JobCancellation -> if conditions change or app gets foregrounded, we want to be able to cancel whatever work we were doing Batching requests together, and prefetching only once, at the most optimal time, in between sessions Collect Metrics → how long does it take for all the work to be finished? How successful are we at scheduling and running the background prefetching jobs? Let’s take a look at the workflow for our background prefetching strategy on Android: When the main activity starts (meaning that the app gets foregrounded), we instantiate our BackgroundWifiPrefetcherScheduler. We also enable what type of jobs will be run. This instance registers itself as a BackgroundDetectorListener. For context, we have implemented a structure that will tell us every time that the app gets backgrounded in case we want to do something before the app gets killed (like sending analytics to the server). When the BackgroundWifiPrefetcherScheduler gets notified, it calls our home-made AndroidJobScheduler to schedule the background prefetching work. It will pass in the JobInfo. This JobInfo contains information about what service to launch, and what conditions need to be met in order for this work to get kicked off. Our main conditions are latency and unmetered connections. Depending on the Android OS, we may take other things into consideration, like if power saving mode is enabled or not. We have experimented with different values of latency, and we are still working to provide a personalized experience. Currently we prefetch on background only once in between sessions. To decide at what time we will do this after you background the app, we compute your average time in between sessions (how often do you access Instagram?) and remove outliers using the standard deviation (to not to take into account those times where you might not access the app because you went to sleep if you are a frequent user). We aim to prefetch right before your average time. After that time has passed by, we check if the connection conditions met our requirement (unmetered/wifi). If this is the case, we will launch the BackgroundPrefetcherJobService. If not, this will be pending until the connection condition is met. (Or device is not in battery saving mode if applicable). BackgroundService will create a serialExecutor to run every background job in a serial fashion. However, after obtaining the http response, we prefetch media in an asynchronous manner. After all the work is done, we notify the OS so our process can be killed and we optimize for memory/battery life. Killing this running service on Android is important to release memory resources that will not be used anymore. All of this is user scoped. We need to be able to address when someone logs out or the user switches. If the user logs out, we will cancel the scheduled work to avoid waking up the service unnecessarily. For Android specifically, we: Looked for an effective way to schedule jobs on background so we could persist data across sessions and specify network requirements. Analyzed how many users were under Lollipop (Android OS released on 2014) as the APIs for Android JobScheduler interface were only available starting on this OS. Turned out this was a case we could not skip…. we needed a compatible version for people using Lollipop. Researched to find an open source/existing solution to schedule jobs on Android for older OS versions. Despite of finding great libraries, none of them fit our use case, as they pulled a dependency on Google Play Service. For context, on Instagram we believe on maintaining our first in class position in terms of APK size. Finally, we ended up building a custom-performant compatible solution for Android JobScheduler APIs. At Instagram we are very data driven and rigorously measure the impact of the systems that we build. That is why when we were designing our background prefetching framework we were also thinking about what metrics should be in place to get the proper feedback. The fact that we count with a centralized framework also helps us collect metrics at a higher level. We thought that it was very important to accurately evaluate the trade-offs and be able to answer questions about how many prefetched bytes were unused or what global CPU regressions were caused. One thing that helped us a lot is that we mark/associate a network request policy to every network request to indicate its behavior and type. This was already built into the app but we leveraged it to slice our prefetching metrics. We attach a request policy to the http request fired and specify if the request is a prefetch request. Another thing that we specify in the policy is the requestType. A request can be specific for Image, Video, API, analytics, etc. This will help us with: Request prioritization Better analyze trade offs by dimensiom like global CPU regression, data usage, and cache efficiency Here we can see a snapshot of that requestPolicy object as defined in our Android codebase. We define a request to be “on screen” when the request belongs to a content that the user is waiting for. offScreen requests have a probability > 1% of the user not interacting with this data requested. We wanted to know how many of our prefetched bytes were actually used, so we looked into how items placed in the cache were being used. We built our entire cache logger in a way that met the following specs: Be scalable. It should be able to support new added caches instances through its API. Be fault tolerant and robust. It should tolerate cache failures (no logging action) or inconsistencies across timestamps. Be reliable. It should persist data across sessions. Use minimal disk and latency on logging. Cache reads/writes happen very often, therefore we want to add minimal overhead. The logging during cache reads/writes can lead to more crashes and higher latency. We also wanted to know how much data we used when we added a new background prefetch request. We have a layered base network engine on the device, and as we mentioned, we attach a requestPolicy to every network request. This makes it super easy for us to track data usage in our app, and observe how much data we consume downloading images, videos, JSON responses, etc. We also wanted to analyze how the data usage gets distributed between data usage over wifi vs. data usage over cellular. This unlocked the possibility of experimenting with different prefetching patterns. What other benefits can background prefetching give you beyond breaking the dependency on network availability and reducing cellular data usage? If we reduce the sheer number of requests made, we reduce overall network traffic. And by being able to bundle future requests together, we can save overhead and battery life. Something that we took into account before implementing background prefetching is the potential risk of causing global CPU regression. And how can you cause regression? Let me give you an example. Imagine our endpoint that serves the API to get your Instagram main feed. Person A’s device will make a call to the main feed API every time person A opens Instagram to get the latest feed first page. This API has several heavy computational operations like ranking and categorizing content based on seen state. If we then do background prefetching every time in between sessions, we would be increasing the load considerably, right? In an attempt to minimize the server side regression, for our first version of the background prefetcher system, an engineer on the feed team, Fei Huang, opened up a different endpoint for background prefetching. This one just fetches new feed posts that are not in view state and return the newest N=20 items. This was our workflow process when building our system. Our team did not open the API to other engineers until we could ensure the quality of the framework and the benefit to the user. As more people join Instagram, this work only becomes more important. We look forward to keep making Instagram more efficient and performant for everyone around the world. Lola Priego is a software engineer on the client performance team at Instagram New York. Stories from the people who build @Instagram 3.5K 9 Android Instagram 3.5K claps 3.5K 9 Written by Stories from the people who build @Instagram Written by Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-11-02"},
{"website": "Instagram-Engineering", "title": "organizing for outcomes how instagram derived form from function and set up engineering for long", "author": ["James Everingham"], "link": "https://instagram-engineering.com/organizing-for-outcomes-how-instagram-derived-form-from-function-and-set-up-engineering-for-long-21a52bc052eb", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts An excerpt of this article originally appeared in Harvard Business Review . Many people working in tech live in fear of new executive hires, and I understand why. Too often, companies onboard leaders to ‘shake things up’ and solve for stagnancy by haphazardly upending everything. This is not to say that organizational change is bad; in fact, my own experiences prove just the opposite. Teams must evolve and continue to evolve to maintain relevance and provide growth opportunities, among other things. But organizational change for its own sake — made without consideration for how structure supports strategy — will undoubtedly run amok. A manager’s capacity to implement new organizational architecture is our superpower. And like any superpower, it must be used carefully and always with the greater good in mind. Unfortunately, effective reorganizations are not the norm, which isn’t necessarily the fault of the leadership teams who implement them. Mischaracterizations of reorgs and their purpose undermine employee trust before proposed changes have even been introduced. What’s worse: anti-reorg folklore doesn’t need truth or malice behind it to spread across an organization like wildfire. Most people are inherently skeptical of change and look for ways to justify their aversion. This means that managers and executives can unknowingly plant the seeds of doubt that sabotage a reorg right out of the gate. I know — I’ve seen it happen. I’ll never forget one example. After three months running a large team at a previous company, I had developed a strong sense of our existing organizational architecture and its limitations. A few days before I had planned to pull the trigger on a much-needed reorg, the CEO called us into an all-hands. When someone asked a question about the pending changes, one of the executives told him that reorgs were just something people did to prove their value-add. Her tone was tongue-in-cheek, but her underlying message was more insidious. It placed genuine attempts to better team infrastructure into the category of “Executive Theater,” and it lumped in considered strategy with hasty attempts to save face. But her comment was just one manifestation of a problem that stretches beyond the bounds of any single organization: too many executives underestimate the power of reorganization, leaving good companies to collapse under the weight of bad design. Setting up teams for success requires first understanding why teams end up arranged poorly and the consequences of those arrangements. Though the problems created by poor architecture vary across organizations, we can typically ascribe the root of flawed design to one or a combination of the following six factors: 1. The organizational architecture doesn’t align with company strategy and values. Changing company goals without changing the organization that executes them is like sending a football team out onto the baseball field and telling the quarterback that he needs to make it to first base. It doesn’t matter how talented or athletic the quarterback is, or how well the players work together; the team has been organized, trained, and incentivized to score a touchdown — not a home-run. Technical teams are no different. 2. Executives focus on individual high-performers instead of the organizational architecture’s ability to facilitate high performance across the board. The strengths of one particular engineer or manager can never compensate for the debilitating effects of a strategy-structure mismatch. Not even a star employee can innovate her way past the roadblocks created by poor architecture. 3. The organization is structured to solve short-term problems, not to reach long-term goals. Executives often respond reactively when brought into an organization grappling with an ongoing and seemingly intractable problem. It’s understandable — no one wants to finally make captain only to find out they’re in charge of a sinking ship. But instead of setting up a temporary team to patch up the leak, many executives structure the entire organization around efforts to address the most glaring issue. Once it’s resolved, the new structure is rendered obsolete. 4. The organizational architecture creates conflict and unnecessary complexity. An organizational model is a machine: the fewer moving parts, the fewer failures. Models that don’t clearly define ownership, priorities, and success metrics will stymie creativity, execution, and collaboration. Well-designed organizations facilitate decision-making by simplifying the chain of command; poorly designed organizations prevent decision-making because they don’t had a clear chain of command in the first place. 5. The structure does not support new opportunities or career paths for those within it. A well-designed organization should scale nearly effortlessly. It should facilitate vertical and horizontal growth, and it shouldn’t cage people in too tightly — there should be flexibility in the plan that allows people to shift positions and take on more responsibility when warranted. Without the possibility of new heights, people aren’t incentivized to reach. 6. Everyone freaks out. When executives announce a reorganization without involving their reports or asking for feedback, people are more likely to panic, disagree, or resist. It’s understandable; many of us associate stability with security. Leadership needs to keep that in mind and bring affected teams into the conversation early. They need to do more than just alert people when the organization has arrived at its new destination — they need to bring them along for the ride. It breeds toxic company politics. Poor organizational architecture makes it difficult for people to work transparently. Interdependent teams that do not share a common set of goals and priorities have to work around one another to meet deadlines. When lobbying or escalating to managers is the only way to ensure that needs get serviced, external pressure becomes the motivation for execution rather than a collective desire to win. It produces more decision-makers than decisions. Lack of clear role definition disrupts decision-making processes at the highest level. Capable senior managers waste time explaining concepts and providing context to less experienced ones, many of whom should not have been brought on to make the decision in the first place. It makes organizations chronically inefficient. Architecture that does not foster autonomy or facilitate decision-making slows execution. Teams that have separate priorities but mutual needs will roadblock one another constantly, and individuals unsure of where to find answers and approval will be disempowered to move quickly. It compromises product quality. When organizations waste energy fighting the side effects of bad design, quality suffers. Executing inevitably requires concessions, and because interdependent teams rely on one another for service, no one can control for the quality of their own output. In theory, everyone is responsible for product quality, meaning that in practice, no one is. By the time I joined Instagram in 2015, engineering was a balloon about to pop. The existing structure could not support the 115-person organization, let alone the 300-person organization it would soon become. Ownership had become less clear and dependencies more complicated. Siloed off into four departments — mobile clients, backend, data, and monetization — teams worked separately on the same problems, each in their own codebase. As a result, the teams would constantly and unavoidably roadblock one another. If the iOS team in clients had made improvements to the search function in their app, they couldn’t ship the changes until the backend team had made correlating changes to their search technology. But if backend had already prioritized video, then its engineers would wait to attend to search, preventing clients from executing indefinitely. And this wasn’t specific to the search function or clients team — no one seemed able to move forward without first mobilizing everyone else to move with them. With no architecture capable of supporting such a rapid scale, Instagram engineering had slowly devolved into a 115-person three-legged race. As head of engineering, my job was to cut everyone loose. The proverbial scissors? Reorganization. We took the following approach: Determine desired outcomes. Gather your leadership team together and agree on the results the reorganization should achieve. Don’t enter the discussion assuming the best way forward is the most obvious one. Ask: What do we want to happen? What’s not working? How can we address these problems holistically? At Instagram, this conversation elicited 20 proposed outcomes from clear accountability to an inspired workforce. Then, we prioritized them, from №1 to №20, based on how accurately they reflected the goals of the company. For instance, maintaining the momentum of our product meant constantly shipping value to our customers; that translated into a prioritization of speed over cost efficiency. Commit to a set of organizational principles. Finesse top outcomes into organizational principles. Make a succinct argument for their primacy and then evangelize them. Remember, the new organizational structure reveals itself naturally once it has a guiding light. Ours was this: Move as fast as possible. Build clear accountability with the fewest decision-makers. Write simple and clear KPIs. Scale to a much larger organization. Maintain a very high level of quality and craftsmanship. The principles we chose aligned with company strategy, addressed otherwise intractable problems, and accounted for the short and long-term needs of the organization. Our principles also set us up to achieve the outcomes that did not make the list. Having an inspired workforce, for example, would follow from having an empowered workforce — a likely byproduct of an organization that innovates constantly (№1), executes quickly (№2), and clearly defines its success metrics (№3). Draw the organizational framework. When it comes time to put marker to whiteboard, think big picture. I mapped out a matrix of every project and problem I knew of at Instagram and the decision-makers for each. Then, I crowdsourced it. My direct reports had more information so I left it to them to fill in the blanks. The exercise revealed major problem areas in our existing framework; for instance, we noticed that one feature was owned by one engineering manager and six product managers. In line with Principle №2, we landed on a one-to-one mapping of engineering and product management to cut down on unnecessary decision-makers and to encourage decision-making. Depending on the company, a similar matrix may point to a different set of problems. As a general rule, however, those crafting a new organizational framework should aim to: Eliminate dependencies. Avoid the three-legged race effect. Autonomous teams execute faster and willingly own the quality of their output. To that end, we created six full stack teams (Consumption, Creation, Communication, Growth, Community Engineering, and Business & Monetization), each led by a product manager and an engineering manager; and two platform teams (Core Client and Core Infrastructure). Vertical teams brought everyone working on the same problems together. Each team has a specific set of responsibilities, each responsibility has a task force, and each task force has an owner. Clearly define roles. Paint over gray areas; it will increase employee satisfaction and retention. Our new structure explicitly defines engineers’ responsibilities, metrics, and team leads. Since implementing it, we’ve found that when people understand expectations, they’re more likely to exceed them. Avoid assigning titles. When drawing out organizational charts, resist the urge to put names in boxes. Assigning positions early on creates bias; the minute an individual becomes the center focus of a reorg decision, organizational principles get thrown out the window. Organizational architecture is a machine — not a seating chart for a wedding. Leadership should act accordingly, designing the organizational architecture around the positions they’ll need to produce the outcomes they want. Get concentric buy-in from executives. Start with a clear perspective and then circulate it to each management layer. I sought feedback on my organizational ideas from direct reports first and then incorporated their suggested changes. From there, I moved to the next layer of management, and then to other departments — product, design, and research — iterating with each layer. This prevented design by committee but built consensus while maintaining transparency. Account for existing teams. Once leadership has settled on an organizational structure, begin to fill in the boxes. Even then, continue thinking about outcomes, not people. Inevitably, accommodating key employees will require some concessions. There will be people who are the best fit for a position they don’t want, and someone who’s a lesser fit will have to fill it. When that happens, figure out a way to complement their weaknesses without compromising the integrity of the design. Always think in terms of risk mitigation, and, whenever possible, find structural solutions to the problems that emerge along the way. Communicate with intention, clarity, and care. Reorganizations are unsettling and disruptive, so be thoughtful about delivery and message. Highlight opportunity growth, and reassure people of their continued value. As much as possible, inform affected teams in advance. Remember, bad data is worse than no data, so don’t tell reports about changes until they have been solidified. That being said, ward against surprise. At our all-hands, I asked people to raise their hand if they were surprised by the reorg announcement. 20% of the engineers present raised their hand. Then I asked those same people to raise their hand again if their job had changed. Not a single person raised their hand the second time around. Strive for that. When it comes time to announce the changes to the entire company, do it gently and do it: Electronically. Have a written explanation of changes prepared and polished. Make sure managers have a supporting message to send out to their direct reports. Leadership should coordinate communication across the company. After executives send out their announcement, managers should immediately send a follow-up to their reports, explaining how the reorganization will affect their teams specifically. In person. Don’t just send out an email and call it a day. Schedule an all-hands. Walk people through the entire process. Begin where leadership started and take the room through it step by step. Executives have likely been working on the reorg for weeks or months by the time of the announcement; it’s unfair to expect that everyone else will be able to digest the changes all at once. Continuously. Be available. Build an FAQ forum so that people can ask and answer questions about the reorg and its implications. Foster an environment that makes people feel safe sharing doubts in the form of questions or concerns. Unexpressed discontent will turn into tension moving forward, so encourage honesty. Prepare to evolve. Always think ahead. Know the answers to questions like: What happens when we double in size? What future challenges might the structure pose and how can we solve for them now? Evaluate every proposed change — however small — through the rubric of the organizational principles. Uphold them relentlessly. When a manager suggests an organizational change, make sure that they can explain how their suggested change supports the organizational principles. How will the change make us execute faster? More clearly define roles? Add engineers without breaking? No matter how successful, never assume the organizational architecture is immune from corruption. Don’t get stuck. Fix continuously. It takes energy to evolve — that’s where the opportunity exists. At Instagram, we meet every six months to audit and tweak because we want to make sure our structure always aligns with our strategy. That’s why we reorganized in the first place. Word to the Wise About Reorgs In my 15 years as an engineer and my 20 years as a manager, I’ve come to realize that most of all dysfunction in an organization stems from ambiguity. This goes for everything from accountability chains to job descriptions. For a reorganization to add value, it must solve for opacity at every level. That means that the process of implementation should be transparent too. Communication about a reorg should never operate like a game of telephone. Rumors beget the question: “Am I safe?” Remember, reorganizations can set people up for success or they can set them up for failure. Take that seriously. Make time to brainstorm the best possible solution. Honor the organizational principles. Make sure everyone else honors them, too. At Instagram, our rapidly growing team understands the importance of maintaining an organizational structure that supports smooth scaling, better communication, faster execution, and fewer politics. And it makes sense; our organizational architecture has proven enormously generative. In our last review, we found that Instagram engineering is actually accelerating our ability to ship. More than ever, engineers feel empowered to innovate. The launch of one of our newer features, Instagram Stories, captures their momentum. In just over three months, a team of less than a dozen conceived, created, and then shipped Stories to half a billion users. Enabled by the new structure, they worked independently without blocking or being blocked by other teams. And that’s just one example. Across the board, output per engineer has increased — even as we’ve grown from 115 to nearly 400. The lesson? Structural problems demand structural solutions. Start innovating one. Stories from the people who build @Instagram 383 2 Leadership Management Organization Reorganization Principles 383 claps 383 2 Written by Head of Engineering, Instagram Stories from the people who build @Instagram Written by Head of Engineering, Instagram Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-11-27"},
{"website": "Instagram-Engineering", "title": "app modularization and module lazy loading at instagram and beyond", "author": ["Instagram Engineering"], "link": "https://instagram-engineering.com/app-modularization-and-module-lazy-loading-at-instagram-and-beyond-46b9daa3fea4", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts Instagram prides itself on having a lean app. But as the number of engineers and features grows, so do the challenges to size. We start to face issues like: increase in application size downloaded from an app store increase in cold start time, and more generally, increased time to interaction with different surfaces inside the app increase in disk space usage decrease in developer velocity from increased complexity of the app or build time To tackle these issues, we recently enacted an Instagram-wide code modularization effort to create strict isolation between features. We do this on both Android and iOS, but this post will primarily focus on Android. Once a feature is modularized, we can take the feature out of the primary executable file and compile it to the dedicated file and lazily load it during app runtime on-demand. Lazy loading can bring a combination of benefits like improved cold start time (especially on Dalvik) and overall runtime, decreased disk footprint, and better developer velocity if module hotswapping is implemented. This post will walk through how we approach modularization and lazy loading at Instagram, and how you can implement it in your own app with our new open-sourced framework. Modularization is the process of separating and creating clear boundaries between logical components of code. For Instagram, we’ve been focused on modularizing feature code since it represents a large part of the codebase, and each feature has clear boundaries. Reducing cross-feature dependencies with modularization helps us solve the challenges of a growing app in a couple of ways. One benefit is that it helps us protect our app’s start time from growing as the amount of code in the app increases. Before modularization, a chain of code references from one feature to another might have risked loading in all of that code during app start time. But by breaking apart these references with modularization, we can wait to load code when it’s actually needed using methods like Lazy Loading (see below). In addition, modularization has other great benefits for overall codebase sanity, build times, developer velocity, Instant Apps, etc. The goal of modularization is to have a clearly defined interface that separates the implementation of the feature from outside code. Start by auditing all the incoming dependencies to the feature. For each dependency, figure out if it should be removed or kept. Since each case is different, there is no simple way of making this evaluation — but here are some considerations to make: Is the dependency a result of misplaced code? Sometimes a dependency exists because there’s a shared library method or misplaced method in the feature that should actually live outside of the feature. If so, move the referenced code out of the feature and into the appropriate place. Is there a way to rewrite the dependency to be more generic rather than have it reference feature-specific code? Does the dependency exist because we’re accessing some feature-specific logic earlier than necessary? If so, try to defer this logic until later. Is the surrounding logic overloaded? If so, simplify the logic into the feature-specific part to make it simpler to reason about the necessity of the dependency. If the dependency is still necessary after making these considerations, then it belongs as part of the feature’s interface. If we’ve done the modularization well, the feature’s interface should contain just a few critical methods. Some examples are: Methods that allow navigating to the feature Lifecycle methods that listen for app-wide lifecycle events Methods that expose core functionality in the feature. While modularization will decouple a feature from the outside world and create a clean interface to the feature, lazy loading will take this further and compile the feature out of the main executable (dex) file to a separate dex file. Each lazily loaded feature will live in a separate file, which will bring benefits like: allowing the feature to load in memory only when really needed instead of on every cold start. It offloads code from the main executable file, which remains smaller and guarantees better cold start time. On Dalvik it offloads methods off the main dex file, decreasing performance penalty of multi dex. clustering feature code together in memory, as it lives in one file and provides most optimal execution in terms of memory access using less disk space if some modules remain unused because feature code will never be uncompressed possibility of having an adaptable app with different sets of features shipped to different users (e.g. non-business users don’t need business features), thus decreasing initial app size And for developer velocity, we added support for hotswapping of lazy loaded modules, which means developers will be able to keep coding and seeing changes without restarting the app. Generally speaking we load a module when we expect it to be used in the near future. Once the module is loaded it will remain in memory until the next cold start. Module loading may incur a small latency (depending on the size of the module), so different tactics may be applied: Loading a module in the background when the user is one click away from the module. This may mean that the module remains unused if the user doesn’t decide to click into that feature or navigates back. But if there’s high probability of clicking into that module, then it’s a good solution. Loading a module once the user navigates into the module. If loading latency is small (below 50ms) for the majority of cases (e.g. p99), then we could simply block on loading, and once it’s done navigate into the feature. Otherwise a simple spinner or a progress bar can be displayed so that the app does not appear as frozen. Some modules are by nature asynchronous, which eases lazy loading since it will be the part of the asynchronous loading. An example is a video player that runs in a secondary process. Instagram initially shows a screenshot of the video while it loads in the background (often times fetched from the network). Lazy loading would happen in that secondary process and be completely transparent to users. We open sourced our framework for lazy loading at ig-lazy-module-loader so that other developers can use it in their apps, or simply learn from our implementation. It can be beneficial if you’re hitting a performance penalty of multi-dex on Dalvik or have many features which may remain unused by some users and you want to save disk space. We recently moved our mobile infrastructure engineering teams (iOS and Android) to New York City. If this blog post got you excited about what we’re doing, we’re hiring — check out our careers page . Mona Huang and Julian Krzeminski work on Android at Instagram. Stories from the people who build @Instagram 1.4K 3 Android Instagram 1.4K claps 1.4K 3 Written by Stories from the people who build @Instagram Written by Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-11-03"},
{"website": "Instagram-Engineering", "title": "10 questions with shupin mao well being tech lead", "author": ["shelly"], "link": "https://instagram-engineering.com/10-questions-with-shupin-mao-well-being-tech-lead-3b19f19b168d", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts Shupin Mao is a senior software engineer at Facebook. During her last four years at the company, Shupin helped several teams and gained experience across Instagram and Facebook, including the Instagram Well-being team. Here she shares what got her into engineering, favorite moments, lessons learned, and more. How did you become an engineer? When working on course projects during my undergrad and grad study, I felt the passion for solving coding problems, which was the main motivation to apply for an engineering position after graduation. After joining Facebook, I was still motivated to solve practical problems every day and learn new skills/knowledge, which affirmed my career choice. What was your first coding language? C was my first coding language back to school days. Objective-C was my first coding language at my full-time work at Facebook. What do you listen to while you work? All kinds of piano songs which can help me keep focused. For example, Ghibli’s relaxing piano pieces are good. What do you do when you get stuck on a problem? I will usually take a short walk to the nearest snack kitchen and look for some snacks. I feel walking and eating can help me think better. Tell us about your favorite project at Instagram? My favorite project at Instagram was the work we did to combat drug and firearms sales, together with the Facebook Community Integrity team. It was one of our first projects collaborating with this team. The project itself was exciting: we adopted a new machine learning model technology, and we shipped several models to production. The part I valued most was the amazing collaboration experience across different teams, including cross-functional partners such as privacy, policy, and legal. What makes working at Instagram unique? Instagram has fewer engineers compared to Facebook (the app), while being responsible for a product as important as other FB products. So it’s quite common for Instagram engineers to be responsible for large scope of work. Instagram has a flatter management structure. There are more opportunities to communicate or present directly to org leads. Bigger scope and more direct/transparent communication make me feel stronger ownership and fulfillment of my projects and work. How would you describe the engineering culture at Instagram? We valued user experience and user privacy highly. We treated all users’ experience very seriously. A lot of projects were driven by user reports or feedback. We also worked very closely with legal, policy, and privacy. Every project or product change needs to be extremely carefully discussed and reviewed by these experts. On our team, we leave 20% of our time to deal with ad-hoc or unexpected issues. A very respectful and supportive work environment. We value the culture of “ Be the Ally ” very much in our org. I personally benefit a lot and also contribute to this valuable culture in my daily work. A Data-driven approach. We valued data analysis highly in Instagram. Most of the projects have very analytical goals. We tracked metrics closely in our daily work. Fun and optimistic and positive working atmosphere. People here are very good at bringing fun into work. You can hear a lot of fun stories or jokes during Q&A or meetings. Even during the intense times, you can still see people making fun with each other and encouraging each other (work was still completed with high efficiency and quality). What makes you excited about coming into work every day? Exciting projects and brilliant colleagues. Your favorite place to eat in the city? A lot of Chinese restaurants :) What is your favorite thing to eat at the office? Pocky in strawberry flavor What’s your favorite Instagram account? @ kuviabear , I like following the sweet daily life of cute Kuvia! Tell us about your happiest day at Instagram. To be honest, it’s hard to choose which was the happiest day at Instagram. I still remember the first day I walked into our building and sat with my team. I was so impressed by all the decorations inside the building, also the sweet corner views from my desk. I also remember all the relaxing casual conversations that happened in micro-kitchen during or after happy hours, so many fun jokes and laughs. And there are also intense but exciting days near deadlines, when everyone was fighting for the same goal within such a short amount of time. I remember how thrilled and excited the whole team was when we hit our goal or launched the projects. What is one of the best things you learned while working at Instagram? I gained a lot of valuable experience in coordination and collaborations across teams/roles in Instagram. I would say Instagram may provide the best example on how engineers and cross-function team members, such as product, legal, privacy expert, work together among the whole company. And my team has many cross team partners, most of them are located remotely. The projects I worked on provided me a lot of great opportunities to learn how to work with different teams and people closely and smoothly. What does your desk setup look like? One monitor and one Apple Mac Pro (I got it for iOS development and I probably should return it now 😂). What was your favorite offsite? Well-being Team Offsite at Clay By The Bay . We had a great day on learning and practicing working with clay. And my favorite part was that we received our “work” as the outcome :P. If you want to learn more about this work or are interested joining one of our engineering teams, please visit our careers page , follow us on Facebook or on Twitter . Stories from the people who build @Instagram 225 5 Startup Team Instagram Careers Career Advice 225 claps 225 5 Written by Stories from the people who build @Instagram Written by Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-11-08"},
{"website": "Instagram-Engineering", "title": "profiling cpython at instagram", "author": ["Instagram Engineering"], "link": "https://instagram-engineering.com/profiling-cpython-at-instagram-89d4cbeeb898", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts Instagram employs Python in one of the world’s largest settings, using it to implement the “business logic” needed to serve 800 million monthly active users. We use the reference implementation of Python, known as CPython, as the runtime used to execute our code. As we’ve grown, the number of machines required toserve our users has become a significant contributor to our growing infrastructure needs. These machines are CPU bound, and as a result, we keep a close eye on the efficiency of the code we write and deploy, and have focused on building tools to detect and diagnose performance regressions. This continues to serve us well, but the projected growth of our web tier led us to investigate sources of inefficiency in the runtime itself. To determine what we needed to optimize and which techniques we should use, we had to figure out: What the Instagram workload looked like from the interpreter’s perspective, and Where the interpreter spent its time when it was running our code. We started by collecting various bits of data from CPython as it executed Instagram server code. Unsurprisingly, from the interpreter’s perspective, the Instagram workload looks like an object-oriented, web server workload: 90% of instructions by the interpreter dealt with operand stack manipulation, control flow, and attribute access. Our workload wasn’t loopy — 94% of loops terminated after four or fewer iterations. Function call overhead was quite high and accounted for roughly 30% of the time spent by the interpreter. Attribute access was the next most resource-intensive category of opcodes, and accounted for another 28% of time spent in the interpreter. Attribute loads were not particularly polymorphic; 85% of loads occurred at monomorphic sites. Based on the data above, we decided to work to reduce the computational cost of function calls and attribute access. We collected all of our data by instrumenting an interpreter running in our lab environment, InstaLab. InstaLab replays production traffic to web servers that are running in an isolated environment using a request mix taken from the top five views (by CPU instructions). It provides us with a way to collect representative performance data without fear of negatively impacting user experience. For each different data set, we instrumented CPython, built a new interpreter, and collected the relevant data from InstaLab. From the perspective of the interpreter, Instagram is just a sequence of bytecode instructions to be executed. As a first step in understanding what the interpreter was doing, we instrumented CPython to collect bytecode execution frequencies. The top ten instructions by frequency are shown in the graph below: Immediately, we could see that LOAD_FAST dominated. In fact, LOAD_FAST, STORE_FAST and LOAD_CONST accounted for roughly 40% of all opcodes executed. This was expected — CPython is a stack machine and each of these instructions moves values to/from the operand stack. Although these instructions are cheap, they are executed quite frequently, and generate memory traffic. This suggested techniques to eliminate loads and stores (e.g. switching to a register based bytecode) would be an effective optimization. When we excluded the opcodes above, we could get a better sense of the “real work” done by the interpreter when it executed Instagram code. The following graph shows the distribution of execution frequency for the top twenty opcodes (of 120 total). These instructions accounted for 90% of the remaining distribution. We grouped them broadly into categories: Attribute access — LOAD_ATTR, STORE_ATTR, LOAD_GLOBAL Control flow — CALL_FUNCTION, RETURN_VALUE, POP_JUMP_IF_FALSE, COMPARE_OP, FOR_ITER, JUMP_ABSOLUTE, POP_JUMP_IF_TRUE, POP_BLOCK, JUMP_FORWARD, YIELD_VALUE, CALL_FUNCTION_KW, SETUP_EXCEPT Container Manipulation — BINARY_SUBSCR, BUILD_TUPLE, UNPACK_SEQUENCE Operand Stack Manipulation — POP_TOP, LOAD_DEREF This conformed to what we expected to see from an object-oriented, web server workload (as opposed to a numeric workload). Looking at execution frequencies only told part of the story. In addition to knowing what the interpreter was doing, we also wanted to know where the interpreter spent most of its time. To that end, we measured the number of CPU instructions that were retired while executing each opcode using the perf_event APIs. We bracketed each opcode body in the interpreter dispatch loop with a read of a hardware instruction counter, then subtracted the two values to compute the “time” spent executing the opcode. For cases where calls were made back into the interpreter, or out to C functions (e.g. the CALL_FUNCTION family of opcodes), we deducted the amount of “time” spent in the callee from the cost attributed to the opcode. The graph below shows the top 10 opcodes by percentage of cumulative CPU instructions retired. This data painted a slightly different picture, as function calls and attribute loading jumped to the head of the distribution. In terms of “time spent,” the most resource-intensive categories of opcodes were: Control flow — CALL_FUNCTION, CALL_FUNCTION_KW, COMPARE_OP Attribute loading — LOAD_ATTR, STORE_ATTR, LOAD_GLOBAL Operand stack manipulation — LOAD_FAST, STORE_FAST, LOAD_CONST Based on the data above, as a first step we decided to spend time optimizing attribute access and reducing function call overhead. One time-honored technique to accelerate attribute access and method dispatch in dynamic language runtimes is the polymorphic inline cache . Inline caches, in combination with dense attribute storage (using something like hidden classes ), can dramatically speed up attribute lookup. However, their efficacy depends on the degree of polymorphism seen at the call sites. We instrumented CPython to record the types seen for each execution of LOAD_ATTR in order to gauge the potential of inline caching. The graph below shows the distribution of LOAD_ATTRs by degree of polymorphism. Based on the data above, inline caching looked like it would be an effective technique to accelerate attribute access. Roughly 85% of LOAD_ATTRs occured at monomorphic sites; an inline cache of size four should have been able to handle roughly 96% of attribute loads. The final question we wanted to answer was: “how loopy is our code?” Answering this question helped us determine how effective optimizations targeted at reducing loop overhead would be at accelerating our workload. We instrumented CPython to record the number of iterations performed by each instance of a loop. The graph below shows the distribution of loop instances grouped by the number of iterations performed. As we can see, our workload was not loopy. In fact, 94% of loops terminated after four or fewer iterations. As a result of this exercise we identified two major sources of inefficiency for our workload in CPython: function call overhead and attribute access requirements. The data we’ve collected suggests that well-known techniques should be effective at mitigating the inefficiency. As a next step, we’re using this information to guide our efforts to optimize CPython. Matt Page is a software engineer on Instagram’s efficiency and reliability team. Stories from the people who build @Instagram 1.8K 10 Python Instagram 1.8K claps 1.8K 10 Written by Stories from the people who build @Instagram Written by Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-11-17"},
{"website": "Instagram-Engineering", "title": "a year of stories launching is the easy part", "author": ["Instagram Engineering"], "link": "https://instagram-engineering.com/a-year-of-stories-launching-is-the-easy-part-d4251acef662", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts One year ago today we launched Instagram Stories to the world. We knew we were making a major change to a product that hundreds of millions of people around the world love and use daily. The weight of this responsibility — plus the opportunity to create something new and delightful for the Instagram community — drove the team to do our best work. After months spent discussing, designing, and building, the final weeks of the project were both stressful and rewarding as the whole company came together to make the launch a success. …And then it was out there. The urgency of shipping quickly gave way to anxious anticipation. With eyes glued to metrics dashboards, we waited to see how the community would respond. Within a few weeks, we could see a clear pattern of growth that indicated we had found market fit. This is where the real journey began. Shipping a new product is characterized by focused intensity, but maintaining a growing product is all about balance. Balance between adding new features and keeping the app fast and stable; between structured prioritization of work and providing opportunities for creativity and fresh ideas; between bringing new features to market quickly and patient craftsmanship; between maintaining a small, tight-knit team and attracting contributors with new skills and perspectives. It’s impossible to strike these balances perfectly. The best you can hope for is to make the right correction when you sense things are going too far in one direction. Here are a few examples of how we found balance over the past year. Leading up to the launch, we went through an exercise to prioritize all the outstanding feature work to determine what would be included in the first version. Cutting unnecessary features is an important discipline to practice when launching and maintaining a product. Our product manager, Nathan Sharp, worked with the team and our leadership to create a burndown list of feature work so engineers and designers always knew the next high-priority item whenever a project was completed. The features on this list were inspired by feedback from the community, user research, and our own intuition. We particularly sought out opportunities to parlay Instagram’s strengths as a platform into delightful new experiences. Thinking along these lines led to one of our first significant feature updates, @mentions. We noticed a trend of story posts that called out other accounts by mentioning them with an @ symbol. We realized we could enhance this experience to support social context, discovery, and distribution. @mentions align well with our mission of strengthening relationships using Instagram’s graph of friends and interests to share who you’re with or who you’re thinking about. We also took advantage of Instagram Direct messaging as a way to send notifications about mentions privately so the mentioned person could enjoy the moment as well. The Stories viewer receives extra metadata so the mention text is interactive and navigates the user to the mentioned account’s profile. Creating new experiences like @mentions was a constant source of motivation for the team, but we also realized the importance of balancing new feature work with improving the quality of existing features. We designed our engineering schedule to include breaks in new feature development for dedicated performance and quality sprints. Refactoring code to be more flexible, improving logging, fixing crashes, and performance tuning were recognized as equally valuable uses of time. In fact, some of these engineering-driven projects led to some of our largest increases in user engagement. For example, iOS engineer Ryan Olson worked with Rui Wang, a backend engineer in our New York office , to develop a flexible system to control media fetching based on user behavior. This enables optimization for story viewer responsiveness without impacting other parts of the user experience. These improvements drove large gains in story consumption participation and story impressions, and also increased participation in story production. The wins we saw outside of the directly affected area demonstrate that making the product perform better in one area drives overall engagement, as users receive a higher quality and more enjoyable experience. Thanks to our burndown list, the engineering team always knew the next priority — whether it was a new feature or a prioritized bit of performance and stability work. This structure helped the team plan and distribute work among engineers, but still allowed flexibility to pursue innovative new ideas. A core value of the Stories team culture is appreciation for creativity and initiative. We know that good ideas can come from anyone on the team, regardless of their role, and we encourage engineers to prototype ideas they are excited about. Often these ideas make their way into the app, and the opportunity to influence the roadmap motivates us and continually infuses the product with fresh ideas. A great example of this was the development of our “Selfie Sticker” feature. Eric Smith, a product engineer on iOS, realized that sometimes you want to show your reaction to a moment in your story. He developed a prototype that added a selfie camera overlay to a post, which would take a picture and include it as a sticker in the final render. Everyone on the team loved the feature so we shipped it. The community has embraced selfie stickers and we see new creative uses of it every day. Another example of an engineering-driven feature was Rewind. In order to build our sticker pinning feature, we needed to be able to backtrack through a video to find where a pixel region was at the beginning. In the process of building this, we developed an approach for reversing videos that we thought might be fun as a new capture format. After sharing a few ridiculous videos of team members reverse jumping off tables and chairs we were convinced. Because the engineering work was essentially a by-product of another feature we were already working on, the relative cost of shipping Rewind was low. We shipped it and the community has enjoyed having a new creative tool to tell their stories. At Instagram our engineering motto is “Do the simple thing first”. The idea is not that you should cut corners, but that you should try to distill the problem you are solving into its simplest form and solve it with focus and quality. Asking, “Am I doing the simple thing first?” is a great test to apply when you’re building the first version of a product, but how does that translate to V2, V3, and V(n)? It’s actually just the same rule on a new baseline, or you could say “Do the next simplest thing next.” By adding complexity in layers, you maintain the velocity to deliver a steady stream of improvements to the product. The discipline of doing the simple thing first is balanced with an appreciation for details and craft. Sometimes a little extra finishing touch can make a huge difference in how delightful the product is to use. Good product engineers are able to get to the core of what a feature is trying to accomplish, implement that goal as simply as possibly, and polish it to a mirror finish. One example of the type of craftsmanship we value was the work of Kevin Jung on the Android sticker UI. The mockups called for a blurry translucent background behind the sticker drawer so the photo or video beneath could show through. Android does not provide a built-in component for achieving this, and falling back to a darkened layer didn’t feel right in our UI. Kevin decided to develop a reusable component for blurring the contents below a view. The component paints a Drawable with a sampled down capture of the view hierarchy beneath it and processes the blur through a fast native library. This BlurDrawable can even handle playing video by capturing frames from a TextureView and merging them with the rest of the hierarchy. There are challenging memory and performance constraints to overcome in order to make this component work smoothly on a wide range of Android devices, but the time investment was worth it. That blurry background really ties the room together. The sprint to launch Stories was both stressful and inspiring. The team cared deeply about bringing this experience to our community and we spent long hours working toward our goal. Memories of sleeping bags in conference rooms and weekends spent polishing all the rough edges come to mind, but this bonded the team with a shared vision and values of collaboration, ownership, and creativity. This enabled us to work well together and achieve the balances needed to grow the product over the past year. As Stories grew to be a more and more prominent part of the Instagram experience, we looked for people to join the team who shared our values, but also brought new perspectives and skills to the project. New engineers filled out the team, like Nick Freeman, our first dedicated server engineer; Timothy Kukulski, who implemented a cross platform system for our creative drawing tools; Bogdan Chiritoiu, who helped launch camera effects on Android; and Shiyi Zhao, who led the stickers project on iOS. We grew at a sustainable pace to ensure our values were maintained and enhanced as new people joined. We also collaborated with other teams to share the responsibility of growing Stories. Sharing our vision and values with these teams helped us function as one. An example of this was our work to enable face effects in Stories, which was one of the top requests from our community. The investment required to quickly build a camera effects platform from scratch was prohibitive, so we looked beyond our team for assistance. Frank Du, our camera tech lead, coordinated with a core technology team to integrate a shared camera platform that enables these kinds of effects. During the integration process, Frank guided the team toward simplicity and craftsmanship, resulting in improvements to the library that allowed us to integrate it with our codebase and make it better for all consumers. Today Instagram Stories has over 250 million active users and is continuing to grow. It’s hard to imagine Instagram without Stories at this point; however, from the moment the project began, success was never guaranteed. In the past year, the team found the intensity and focus to launch and the balance to maintain and grow the product. I’ve mentioned just a few of the people who worked so hard to get us where we are today, but there are many more talented engineers contributing in big and small ways every day. We’re all looking forward to the next year of challenges and bringing new ways for our community to strengthen relationships and share experiences. We’ll need more help in the coming year so if you are excited about this mission, please consider joining us ! Will Bailey is an engineer on the Stories team. Stories from the people who build @Instagram 1.2K 8 Startup Instagram Engineering 1.2K claps 1.2K 8 Written by Stories from the people who build @Instagram Written by Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-12-20"},
{"website": "Instagram-Engineering", "title": "launching iglistkit 3 0", "author": ["Instagram Engineering"], "link": "https://instagram-engineering.com/launching-iglistkit-3-0-4135270a831c", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts The Instagram engineering team in New York City are excited to announce the release of IGListKit 3.0 ! We’ve been baking this release on GitHub master branch for several months with the generous help of our open source contributors. If you’ve used Instagram on iOS, you’ve used IGListKit. Instagram continues to use the latest IGListKit branch as we ship new features to hundreds of millions of users every week. Since open sourcing it, Instagram has introduced many new features backed by IGListKit to each tab of the app. Among the 38 enhancements, changes, and fixes in the 3.0 release, there are a couple major additions that we’re excited to share. IGListBindingSectionController is a new tool that performs the same diffing magic that we use on sections, but now on the cells themselves . It unlocks the ability to dynamically animate changes (delete, insert, move) to individual cells instead of a cross-fade reload on all of the cells. More importantly, it encourages modeling your lists with data, transformations, and view models which results in a one-way data flow architecture. Let’s use a design like Instagram’s home tab as an example. Say you have a list of Post models that power an assortment of cells: PhotoCell , ActionCell , and CommentCell . With vanilla UICollectionView , you connect a data source that controls the sections, item count, cells, and sizes. Often this is done in a UIViewController alongside other business logic, leading to bloat commonly referred to as massive view controller . Instead, with IGListKit, you can create an IGListBindingSectionController that receives a Post object and transforms it into view models , which then configure your cells. Whenever something changes, call -updateAnimated:completion: and cells will animate into place. Check out our calendar example to see how it works! IGListCollectionViewLayout is a new vertically-scrolling layout that positions sections next to each other when they don’t fill the full width. It mimics how UICollectionViewFlowLayout positions cells within the same section. We spent a lot of time testing and tuning this layout to make it as fast and stable as possible! Using IGListCollectionViewLayout is as simple as assigning it to a UICollectionView - and you’re done! We are incredibly appreciative to all 47 contributors ! A special thanks to everyone that made 3.0 possible: Adam Roberts, Adlai Holler, Bas Broek, Benny Wong, Bofei Zhu, Charles, Diogo Tridapalli, Hesham Salman, James Sherlock, Jeff Bailey, Jesse Squires, Nikolai Johan Heum, Phil Cai, Rune Madsen, Ryo Fukuda, Viktor Gardart, and Vincent Peng . We’re sending out limited t-shirts to help show our appreciation! If you’re interested in helping shape the future of IGListKit, we have starter tasks to help get your foot in the door. You can read more about the direction of the project in our new Vision document . Finally, we’re proud of how IGListKit has grown both internally and externally, so we wanted to share a few interesting stats about the 3.0 release: Dozens of IGListKit-backed features in Instagram, with over 42,000 diffs executed per second Test coverage is at a whopping 98% 4 radars filed 5,700 stars on GitHub Over 3,500 app installs via CocoaPods Stories from the people who build @Instagram 621 6 iOS Open Source Instagram 621 claps 621 6 Written by Stories from the people who build @Instagram Written by Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-06-14"},
{"website": "Instagram-Engineering", "title": "setting up instagrams new york eng team", "author": ["Mike Krieger"], "link": "https://instagram-engineering.com/setting-up-instagrams-new-york-eng-team-979371e19f65", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts 2016 was a huge year for Instagram — between launching Stories, our new look & icon, and big changes to Explore and Direct — we shipped more products, more quickly, and with more engineers than we ever have before. Another big change — though not quite as visible to the community — has been the start and growth of our New York engineering team. In just one year, the engineering team grew from 11 to 55, and by the end of 2017 will be larger than all of Instagram engineering when we first started the office. Some of our biggest product and engineering teams are now based in NYC, including feed, search, explore, stories, commerce, and mobile performance & quality. Since this was one of the biggest team changes we’ve ever gone through, I wanted to share what we’ve learned so far about making our second engineering office successful. Opening several offices was the last thing on me and my co-founder Kevin’s mind in the first couple years while we were getting Instagram off the ground — we knew that at that critical first stage, having everyone in the building (really, the same room) was key, since strategies & designs evolved quickly. As we grew past our first 100 engineers, we’d considered expanding beyond our California presence, specifically with an eye towards NYC. The benefits were promising — a new set of great folks to work with in a new city (particularly in areas like Machine Learning), connections with the vibrant fashion and art scenes in NYC, and being able to offer more flexibility to those who wanted to move elsewhere but still work for us. And Facebook’s existing NYC presence meant we would have facilities and recruiting support early on. At the same time, we were worried that the increased communication overhead would cause us to move slower or drop things along the way. Unlike GitHub and Automattic, two companies who built in remote collaboration from the beginning, we’d be making a leap into multiple offices after already growing significantly in one place. Ultimately, we realized that there would never be a “perfect time” to open another engineering office, but doing so at around our scale — 120 engineers at the time — would let us learn and iterate before the organization grew too large. We couldn’t have done this move any earlier — we just weren’t set up for it. As we wrapped up 2015, Instagram was still organized by function — an iOS team, an Android team, an Infra team, etc — which meant that there weren’t pieces we could cleanly separate. Having a portion of the iOS team based in another office but still closely coordinating with their counterparts in California would not lead to a happy team. So before making the investment in NYC, we went through the most significant re-structuring to date on our engineering team, instead organizing ourselves by product vertical — Explore, Feed, Search, Content Creation. This meant we could consider much more atomic units of teams when thinking about what initiatives to move to NYC. The second office wasn’t the only reason we went through the changes — it also solved major communication problems between our PM and eng teams, and created more team unity. We’ll share more about how we’re structured and the pros/cons in a future eng blog post. With our team now organized by product vertical, we found that our Explore and discovery efforts, which didn’t have many engineers yet, were an independent unit that would be a good starter team for our new NYC office. While we knew there’d be engineers excited to join our brand-new NYC presence, we wanted to kick things off with a base of people who had been at Instagram in California for a while and could be the culture-carrying pioneers. Specifically, we wanted to move tech leads and senior engineers, rather than managers, first. That way the office could hit the ground running, and we could recruit the right eng managers over time while also building the team’s momentum by shipping improvements to Explore. Luckily, two of our senior engineers on our machine learning and ranking teams, Thomas and Linji, were up for uprooting their lives and being our first NYC engineers. We found a somewhat make-shift office in Chelsea for the team that would grow as they hired great talent from NYC. The first office had a broken-down air conditioning unit and no doorknobs, but it gave the team a sense of scrappiness and reminded me of our early Instagram offices: As soon as we had a small group of engineers, I flew out and did a hackathon with the team, where everyone collaborated on out-there ideas for a day or two and presented the demos they had built. It helped build relationships with the new engineers that might otherwise have taken months or several visits to build up. As part of that hackathon, we built the first prototype of a ranked main feed for Instagram…which would prove to be a bit of foreshadowing of what was to come. As we were planning the first half of 2016, we knew the largest project we were going to take on was ranking Instagram’s main feed. The natural instinct was to put it “close to home”, since it was going to affect our most significant surface. We decided to do the risky thing and base it in NYC. It wasn’t a totally crazy idea; most of the work would involve machine learning and ranking, which the NYC Explore team had expertise in (they had been the first to prototype the idea at that first hackathon). And finally, this was a primarily engineering-led project, which meant there wouldn’t be the need to have overly-frequent meetings back with the California office. It worked out great — the team was able to build a feed ranking system from scratch and ship it in less than half a year, with huge impact on the time spent in feed and the number of photos and videos liked by our community. The challenge and scale of the project meant the team gelled more quickly than they might have otherwise; and the importance of the project kept the office at the top of my and the rest of Instagram leadership’s minds. An unexpected benefit: because meetings across the coasts require more coordination, we found that teams became more judicious when considering whether a topic deserved a meeting or just an email. We also formed our Core Client team in NYC, responsible for our iOS and Android apps’ quality, performance, and release process. By setting this team up in NYC (and seeding it with some of our best mobile engineers from California), we had a strong mobile presence to complement our ranking & Machine Learning initiatives. Our philosophy has been to grow our NYC presence by investing in the current teams there or starting brand-new teams, but occasionally we also decide to move whole teams there. This is a rare occurrence, since it’s disruptive to the team and in most cases, the majority of the team doesn’t want to make the move. A lesson we’ve learned is that moving existing teams can create uncertainty for the rest of the California teams — the feeling of “wait, is my team next?”. We found that over-communicating about our principles for when to move teams (and our plan to make that a rare occurrence) helped with some of these concerns. Given how successful our initial efforts have been in NYC, many of our teams have gotten excited to move there too. We’ve developed a checklist for teams considering branching out to NYC or spinning up new efforts there. If you’re thinking about building another presence for your own company, these questions are a good starting point to help you assess how successful your new team or office will be. Is there a growth path for your team to be 30+ people within a year or so, and have senior engineering leadership? What adjacencies exist between your team and the existing teams in NYC? For example, can they collaborate with the mobile perf & quality teams, or work with our ranking experts? How will the team be seeded? Is there an ambitious goal or project to be shipped in the first six months of the team’s move? Next up for NYC is our Instagram Commerce team, which is working on the Shopping features we recently announced . If you’re interested in meeting with or joining our NYC engineering team, please check out our careers page . Stories from the people who build @Instagram 372 6 Startup Engineering New York Instagram 372 claps 372 6 Written by Stories from the people who build @Instagram Written by Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-02-15"},
{"website": "Instagram-Engineering", "title": "dismissing python garbage collection at instagram", "author": ["Instagram Engineering"], "link": "https://instagram-engineering.com/dismissing-python-garbage-collection-at-instagram-4dca40b29172", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts By dismissing the Python garbage collection (GC) mechanism, which reclaims memory by collecting and freeing unused data, Instagram can run 10% more efficiently. Yes, you heard it right! By disabling GC, we can reduce the memory footprint and improve the CPU LLC cache hit ratio. If you’re interested in knowing why, buckle up! Instagram’s web server runs on Django in a multi-process mode with a master process that forks itself to create dozens of worker processes that take incoming user requests. For the application server, we use uWSGI with pre-fork mode to leverage memory sharing between master and worker processes. In order to prevent the Django server from running into OOM, the uWSGI master process provides a mechanism to restart the worker processes when its RSS memory exceeds the predefined limits. We started by looking into why worker RSS memory grows so fast right after it is spawned by the master process. One observation is that even though the RSS memory starts with 250MB, its shared memory drops very quickly — from 250MB to about 140MB within a few seconds (shared memory size can be read from /proc/PID/smaps ). The numbers here are uninteresting because they change all the time, but the scale of shared memory dropping is very interesting — about 1/3 of the total memory. Next we wanted to understand why this shared memory becomes private memory per process at the beginning of the worker spawning. Linux kernel has a mechanism called Copy-on-Write (CoW) that serves as an optimization for forked processes. A child process starts by sharing every memory page with its parent. A page copied to the child’s memory space only when the page is written to (for more details refer to the wiki https://en.wikipedia.org/wiki/Copy-on-write ). But in Python land, because of reference counting, things get interesting. Every time we read a Python object, the interpreter will increase its refcount, which is essentially a write to its underlying data structure. This causes CoW. So with Python, we’re doing Copy-on-Read (CoR)! So the question is: are we copy-on-writing immutable objects such as the code objects? Given PyCodeObject is indeed a “sub-class” of PyObject , apparently yes. Our first thought was to disable the reference counting on PyCodeObject . At Instagram, we do the simple thing first. Given that this was an experiment, we made some small but hacky changes to CPython interpreter, verified the reference count number didn’t change on code object, and then shipped that CPython to one of our production servers. The result was disappointing because there was no change on shared memory. When we tried to figure out why, we realized we couldn’t find any reliable metrics to prove our hack worked, nor could we prove the connection between the shared memory and the copy of code objects. Apparently, something was missing here. Lesson learned: prove your theory before going for it. After some googling on Copy-on-Write, we learned Copy-on-Write is associated with page faults in the system. Each CoW triggers a page fault in the process. Perf tools that come with Linux allow recording hardware/software system events, including page faults, and can even provide stack trace when possible! So we went to a prod server, restarted the server, waited for it to fork, got a worker process PID, and then ran the following command. Then, we got an idea about when page faults happen in the process with stack trace. The results were different than our expectations. Rather than copying the code object, the top suspect is collect , which belongs to gcmodule.c , and is called when a garbage collection is triggered. After reading how GC works in CPython, we have the following theory: CPython's GC is triggered deterministically based on the threshold. The default threshold is very low, so it kicks in at a very early stage. It maintains linked lists of generations of objects, and during GC, the linked lists are shuffled. Because the linked list structure lives with the object itself (just like ob_refcount ), shuffling these objects in the linked lists will cause the pages to be CoWed, which is an unfortunate side effect. Well, since GC is backstabbing us, let’s disable it! We added a gc.disable() call to our bootstrapping script. We restarted the server, but again, no luck! If we look at perf again, we’ll see gc.collect is still called, and the memory is still copied. With some debugging with GDB, we found that apparently one of the third-party libraries we used (msgpack) calls gc.enable() to bring it back, so gc.disable() at bootstrapping was washed. Patching msgpack is the last thing we would do because it leaves the door for other libraries to do the same thing in the future without us noticing. First, we need to prove disabling GC actually helps. The answer again lives in gcmodule.c. As an alternative to gc.disable , we did gc.set_threshold(0) , and this time, no libraries brought it back. With that, we successfully raised the shared memory of each worker process from 140MB to 225MB, and the total memory usage on the host dropped by 8GB per machine. This saved 25% RAM for the whole Django fleet. With such big head room, we're capable of running a lot more processes or running with a much higher RSS memory threshold. In effect, this improves the throughput of Django tier by more than 10%. After we experimented with a bunch of settings, we decided to try it on a larger scale: a cluster. The feedback was pretty quick, and our continuous deployment broke because restarting our web server became much slower with GC disabled. Usually restarting takes less than 10 seconds, but with GC disabled, it sometimes took more than 60 seconds. It was very painful to re-produce this bug because it’s not deterministic. After a lot of experiments, a real re-pro shows in atop. When this happened, the free memory on that host dropped to nearly zero and jumped back, forcing out all of the cached memory. Then came the moment where all the code/data needed to be read from disk (DSK 100%), and everything was slow. This rung a bell that Python would do a final GC before the interpreter shut down, which would cause a huge jump in memory usage in a very short period of time. Again, I wanted to prove it first, then figure out how to deal with it properly. So, I commented out the call to Py_Finalize in uWSGI’s python plugin, and the problem disappeared. But apparently we couldn't just disable Py_Finalize as it was. We had a bunch of important cleanups using atexit hooks that relied on it. What we ended up doing is adding a runtime flag to CPython that would disable GC completely. Finally, we got to roll it out to a larger scale. We tried our entire fleet after this, but the continuous deployment broke again. However, this time it only broke on machines with old CPU models (Sandybridge), and was even harder to re-pro. Lesson learned: always test the old clients/models because they’re often the easiest ones to break. Because our continuous deployment is a fairly fast procedure, to really catch what happened, I added a separate atop to our rollout command. We're able to catch a moment where cache memory goes really low, and all of uWSGI processes trigger a lot of MINFLT (minor page faults). Again, by perf profiling, we saw Py_Finalize again. Upon shutdown, other than the final GC, Python did a bunch of cleanup operations, like destroying type objects and unloading modules. Again, this hurt shared memory. Why do we need to clean up at all? The process is going to die, and we’re going to get another replacement for it. What we really care about is our atexit hooks that do cleanup for our apps. As to Python’s cleanup, we don’t have to do it. This is what we ended up with in our bootstrapping script: This is based on that fact atexit functions run in the reverse order of registry. The atexit function finishes the other cleanups, then calls os._exit(0 ) to exit the current process in the last step. With this two-line change, we finally finished rolling it out to our entire fleet. After carefully adjusting the memory threshold, we got a 10% global capacity win! In reviewing this performance win, we had two questions: First, without garbage collection, wasn’t the Python memory going to blow up, as all memory allocation wouldn’t be freed ever? (Remember, there is no real stack in Python memory because all objects are allocated on heap.) Fortunately, this was not true. The primary mechanism in Python to free objects is still reference count. When an object is de-referenced (calling Py_DECREF ), Python runtime always checks if its reference count drops to zero. In such cases, the deallocator of the objects will be called. The main purpose of garbage collection is to break the reference cycles where reference count does not work. Second question: where did the gain come from? The gain of disabling GC was two fold: We freed up about 8GB RAM for each server we used to create more worker processes for memory-bound server generation, or lower the worker respawn rate for CPU-bound server generation; CPU throughput also improved as CPU instructions per cycle (IPC) increased by about 10%. With GC disabled, there was a 2–3% of cache-miss rate drop, which was the main reason behind the 10% IPC improvement. CPU cache miss is expensive because it stalls CPU pipeline. Small improvements on the CPU cache hit rate can usually improve IPC significantly. With less CoW, more CPU cache lines with different virtual addresses (in different worker processes) point to the same physical memory address, leading to better cache hit rate. As we can see, not every component worked as expected, and sometimes, the results can be very surprising. So keep digging and sniffing around, and you’ll be amazed how things really work! Chenyang Wu is a software engineer and Min Ni is an engineering manager at Instagram . Stories from the people who build @Instagram 1.7K 13 Programming Python Instagram 1.7K claps 1.7K 13 Written by Stories from the people who build @Instagram Written by Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-01-17"},
{"website": "Instagram-Engineering", "title": "iglistkit 2 1 0 released", "author": ["Instagram Engineering"], "link": "https://instagram-engineering.com/iglistkit-2-1-0-released-c91742a4a18", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts We are excited to release IGListKit 2.1, following our original 1.0 announcement just a couple months ago. We are especially proud that version 2.1 was driven entirely by the open source community! In addition to all of the feature and fixes included in version 2.1, our 29 contributors added: Full support for Xcode XIBs and Storyboards Compatibility and tests for tvOS A CocoaPods subspec to just use the diffing algorithm Compatible with macOS! A single-item grid layout Along with the release, we started publishing guides to help you get started using the framework and answer common questions. There is a Getting Started guide to help you get up and running, an Equality theory guide to customize how IGListKit diffs your models, and a Migration guide to help update between major versions. On top of the new guides, we published a great new tutorial on RayWenderlich.com showing how to refactor an old UICollectionView with IGListKit and demonstrating features of the framework. We are already hard at work collecting ideas for IGListKit 3.0 and continuing to iterate and improve for both the open source community and our hundreds of millions of people that interact with products powered by IGListKit! If you’re interested in helping contribute, check out our starter-task issue tag to find a task to begin with! Ryan Nystrom is the author of IGListKit and an iOS engineer at Instagram’s New York office working on app infrastructure. Stories from the people who build @Instagram 191 1 iOS Swift 191 claps 191 1 Written by Stories from the people who build @Instagram Written by Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-01-04"},
{"website": "Instagram-Engineering", "title": "bringing wide color to instagram", "author": ["Mike Krieger"], "link": "https://instagram-engineering.com/bringing-wide-color-to-instagram-5a5481802d7d", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts Last September, Apple announced the iPhone 7 and 7 Plus, which include cameras that capture a greater range of colors than previous models, and screens that can display that wider color range. We’ve just finished updating Instagram to support wide color, and since we’re one of the first major apps to do so, I wanted to share the process of converting the app to help any others doing the conversion. In my role as CTO I’ll often do deep-dives on a particular technical area, and wide color was my main area for November and December 2016. For years, most photos captured and shared have been in the sRGB color space. sRGB has great compatibility with most displays, so it became the standard for images shared on the Web — and more recently, on mobile. For years, sRGB did a good job of representing the colors displayed on most monitors. But as display and camera technology improves, we’re starting to be limited by the colors represented in sRGB. Take, for example, this “photo room” we have at Instagram HQ: When captured by an iPhone 7 Plus, most of the oranges and colors in the room are outside the sRGB color gamut, so detail is lost unless we use a wider color space. The color space that Apple chose for its devices going forward is Display P3. Here, highlighted in blue, are all the portions of the image that are outside of sRGB but present in Display P3 ; in other words, parts of the image where information is getting lost: Next, we’ll walk through what we needed to change at each step of the Instagram image pipeline to bring wide color support to Feed, Stories, and Direct. When we started this project, none of us at IG were deep experts in color. For a good starting point, I recommend Craig Hockenberry’s new book ; an early draft was helpful as we started converting Instagram. The most useful tool when working on wide color compatibility is a “canary image” that will only show itself if you’re in wide color. Here’s our sample one . If that just looks like a red square to you, you’re likely on a monitor that can only display sRGB colors. If you open it on a wide-color display device, you should see the Instagram logo “magically” appear — otherwise, the information is lost. You can use this canary to identify exactly where in the process your app is losing wide color information — the step where it turns back into just a red square. This is the easy part. As of iOS10, Apple’s APIs will output wide-color images when available from compatible cameras. One tweak we made while we were looking at this was converting to the new AVCaptureDeviceDiscoverySession , which let us take full advantage of the new dual lens system on the 7 Plus. After we capture images (or import them from the Camera Roll), we often apply simple operations like crops and resizes. Most of these are done in Core Graphics , so there were a few changes we had to make for wide-color compatibility. If you’ve ever done image manipulation in Core Graphics , the following pattern will be familiar to you: As a legacy API, it’s not wide-color aware. Instead, we’ll use the new UIGraphicsImageRenderer: What we did to simplify the transition at IG was to create a wrapper class around UIGraphicsImageRenderer that takes a block of image drawing actions that accepts a CGContext. It’s implemented as a category on UIImage , so engineers can use [UIImage renderedImageWithSize:(CGSize) actions:(ImageActionsBlock)actions] , whereas ImageActionsBlock ’s single argument is a CGContextRef . On iOS9 it will use the old UIGraphicsBeginImage approach, calling the block once the context is ready; on iOS10 it uses the new renderer, calling the block inside imageWithActions . In other places — like when initializing a CGContext for other drawing operations — it’s common to use CGColorSpaceCreateDeviceRGB when creating a CGColorSpaceRef . This will create an sRGB colorspace on most devices, and we’ll lose our wide color information. Most of the initial work for wide color on Instagram was tracking down everywhere that this color space was hard-coded. Instead, we can see if our screen supports wide colors (using UIScreen.mainScreen.traitCollection.displayGamut ), and if so, use CGColorSpaceCreateWithName(kCGColorSpaceDisplayP3) . Again, we found that creating a wrapper that returns the appropriate colorspace for that device was helpful. When we’re downloading images and aren’t sure what color space to use, we instead use CGImageGetColorSpace , so once we serve Display P3 images to our iOS app, we only create wide-color graphics contexts when needed. Instagram uses OpenGL for most of its image editing and filtering. OpenGL isn’t color managed; it operates on a range (say, 0.0 to 1.0), and it’s up to the output surface to determine what colors that actually maps to. The good news is that this meant we had to make very few changes to make our GL pipeline wide-color compatible. The biggest change was to ensure that when we extracted pixel buffers from our GL surface, we were using the appropriate colorspace before converting from a CVPixelBufferRef to a CGImageRef . We did have trouble getting EAGLView , the built-in way of displaying GL content in a UIView , to be color space-aware. Our solution was to render to an offscreen buffer, grab a wide color image from the buffer, and place it back on the screen using a UIImageView , which is wide-color compatible by default. This wouldn’t work for high-frame-rate applications like games, but was sufficient for our needs. If you’re developing a high-frame-rate application in wide color and have solved this, please reach out and I’ll add the information to this post. At this point, we’ve captured a wide color image, resized it in CoreGraphics, and put it through OpenGL, all while preserving wide color. The last step is taking our UIImage and turning it into a JPEG. This is one of the simplest transitions: replace the legacy UIImageJPEGRepresentation with UIGraphicsImageRenderer and its jpegData method. It’s at this point that you can load up your exported image (Xcode’s debugger integration for opening UIImages in Preview is handy here) in Photoshop and check the resulting image’s color profile and other color information. Once the images are received by our backend, we do some final resizing in Python using Pillow . We then serve images globally through Facebook’s CDN. Our challenge was that most of our app’s users are currently using devices that aren’t wide-color compatible — and many don’t have good color management built in. Converting images between multiple color profiles on the fly would have added complexity to either our CDN or mobile apps. To keep things simple, we opted to store both a wide-color and non-wide version in our backend, and use the Python ImageCms library for conversion between the two at storage time ( here’s a handy tutorial ). This library works in tandem with Pillow and accepts an Image object when converting: At read time, our apps specify whether their display has a wide color gamut in their User-Agent, and the backend dynamically serves the image with the right profile. In the future, when most images captured are wide color and most displays are color managed, we’ll likely revisit the double-writing approach. It’s still early days for wide color, and documentation is still sparse, which is why I wanted to share the nitty gritty of how we converted Instagram. If in the process of converting your own app you hit any questions, please drop a note in the comments. And if you’re interested in joining Instagram’s iOS team, take a look at our openings . Stories from the people who build @Instagram 930 22 Thanks to josh d . Design iOS Instagram 930 claps 930 22 Written by Stories from the people who build @Instagram Written by Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-01-09"},
{"website": "Instagram-Engineering", "title": "react native at instagram", "author": ["Instagram Engineering"], "link": "https://instagram-engineering.com/react-native-at-instagram-dd828a9a90c7", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts React Native has come a long way since it was open-sourced in 2015. Fewer than two years later, it’s being used not only in Facebook and Facebook Ads Manager, but also in many other companies, from Fortune 500 companies to hot new startups . Developer velocity is a defining value of Instagram’s mobile engineering . In early 2016, we started exploring using React Native to allow product teams to ship features faster through code sharing and higher iteration speeds, using tools like Live Reload and Hot Reloading that eliminate compile-install cycles. Integrating React Native into an existing native app can create challenges and additional work that you don’t encounter when you start an app from scratch. With this in mind, we decided to start exploring these challenges by porting the simplest view we could think of: the Push Notifications view. This view was originally implemented as a WebView, so we thought that it wouldn’t be too hard to beat its start up times. On top of that, this view didn’t require us to build much navigation infrastructure — the UI was quite simple and translations were determined by the server. The first problem that popped up was adding React Native as a dependency without pulling in the entire library . Doing so would not only increase the binary size, but would also have a large impact on methods count, making Instagram for Android go multi-dex with all the performance consequences this entails (yes, Instagram is still single-dex! ). We ended up selectively pulling in only the view managers we needed at that time and writing our own implementations for the ones that depended on libraries we didn’t want to pull in. Ultimately, React Native ended up adding ~3500 methods. Features written in React Native barely require defining Java methods, so we believe this investment will be worthwhile in the long run. As part of the Push Notification Settings experiment, we audited React Native’s impact on several metrics, including crashes and out of memories. We found these metrics to be neutral both on the initial experiment and when we looked into retaining the bridge instance when the user left a React Native feature (so the next time they enter one we didn’t have to re-create it). React Native has a start up overhead mostly caused by having to inject the JavaScript bundle into JavaScriptCore (the VM used by React Native both on iOS and Android) and instantiate native modules and view managers. Although the React Native team has come a long way in improving performance , for Instagram integration we wanted to measure this gap to figure if the tradeoffs would make sense for us. To do so, we ported the existing native Edit Profile view to React Native. Along the way, we built product infrastructure that started being used by product teams in parallel (e.g. navigation, translations, core components). We ended up leveraging ideas and infra already built by the React Native team, namely Random Access Module Bundling , Inline Requires , Native Parallel Fetching and plenty more already integrated into the framework. As mentioned on the previous section, the Core Client team ported the Push Notification Settings and the Edit Profile views to React Native. We also ported the Photos Of view to start looking into performance when powering lists with React Native: In addition to these examples, several product teams have shipped features in React Native. Instagram has a lightweight interface for promoting posts called Post Promote . This product was originally implemented as a WebView because that technology allowed the team to iterate faster than with native code. The problem with WebViews is that the UX doesn’t feel native and start up is pretty slow. The promote team ported this feature to React Native and got fantastic improvements on startup times and user experience. It is worth mentioning that despite this being a very complex creation flow, it only added 6 methods to the Android DEX. Over 600M people come to Instagram every month and discover a wealth of new interest-based inspiration while connecting with their communities. However, they’re not always ready to act on this inspiration at the moment of discovery, and often want to revisit this content later when they’re ready. In response to this need, the Save team implemented support for saving posts and revisiting them when they want to via a new, private tab on their profile that is only visible to them. The Save team implemented the iOS version of the list of saved posts in React Native. Checkpoints are flows triggered from the server in response to suspicious actions (e.g: when we need to verify your phone number, when we think your account might have been compromised, etc). Historically, checkpoints have been implemented using WebViews. As mentioned before, WebViews are good for code sharing and fast iteration speeds, but the UX doesn’t feel native and startup times can be slow. The Protect and Care team started working on revamping some of these flows. They decided to use React Native to leverage code sharing while keeping a great user experience and low startup times. We want Instagram to be a safe place where everybody can capture and share their most important moments. As the Instagram community grows and people from every corner of the world share more content we want to work diligently to maintain what has kept Instagram positive and safe, especially regarding the comments on your photos and videos. With this goal in mind, the Feed team launched a feature that allows users to moderate the comments they receive on their posts. Lead Gen Ads is a call to action surface that allows users to share information with advertisers. Advertisers can customize the forms on this surface. React Native allowed product teams to ship features faster to both our iOS and Android apps. The list below shows the percentage of code shared between the apps for some of the products, which could be used as a proxy to measure how we managed to improve developer velocity: Post Promote: 99% SMS Captcha Checkpoint: 97% Comment Moderation: 85% Lead Gen Ads: 87% Push Notification Settings: 92% We recently moved our mobile infrastructure engineering teams (iOS and Android) to New York City. If this blog post got you excited about what we’re doing, we’re hiring — check out our careers page . Martin Bigio , Don Yu , Brian Rosenfeld and Grace Ku are Software Engineers on the Core Client team at Instagram New York. Stories from the people who build @Instagram 8.7K 41 React React Native Instagram 8.7K claps 8.7K 41 Written by Stories from the people who build @Instagram Written by Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-02-06"},
{"website": "Instagram-Engineering", "title": "engineering the instagram stories team", "author": ["Instagram Engineering"], "link": "https://instagram-engineering.com/engineering-the-instagram-stories-team-e16ec62e364d", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts Earlier this year, we pivoted around the idea of making Instagram a place for all your moments — the highlights and everything in between. In August, we introduced Instagram Stories as a new way to share moments as they happen, without the pressure of saving them to your profile. And it’s been amazing to see the response from the community — in less than three months, more than 100 million people already use it every day. Launching Stories was both invigorating and nerve-wracking. We made countless changes to our mobile apps and infrastructure to make it globally available to over 500 million people within a day (no pressure). And given all the moving pieces, we expected issues. While nothing ever goes off without a hitch, in retrospect, our launch was one of the smoothest I’ve experienced — certainly at this scale. Having managed teams for large scale products and services at startups and more established companies, I wanted to reflect on what made this project go well, and what we could have done to make it go more smoothly. If only there was a simple formula for success! Building great software is tricky, and so is building great teams to build great software. Let’s take a look at a few themes from the Stories project. I am fortunate to work with incredibly smart individuals, but great talent is not enough to make a high-performing team. To set a team up for success, a manager needs to understand the individuals’ needs, but more importantly, the needs of the team as a whole. Most often, simply ask and listen before figuring out how to help. During early development, the Stories team encountered constant iteration and shifts in direction that could have easily resulted in endless thrash if we didn’t keep an eye on the overall health of the team. We reviewed the product as a group about every two weeks, but still weren’t aligned. Based on ongoing observations and feedback from the team, we periodically made team and process adjustments. Fortunately, we’re not big on process at Instagram. To ideate more efficiently, we challenged our product direction to align on a shared vision, trimmed the set of key stakeholders, rebalanced the team based on shifting workload, and held frequent (even daily) reviews with our founders Kevin and Mike. Much like there can be a lifecycle in software development, there can also be one for a team. Bruce Tuckman’s model of group psychology is interesting — he classified research into a set of states a group can be in: forming, storming, norming, and performing. While it’s not necessary to typecast a team into one of these states, this model can provide a framework to determine what actions to take to help a team thrive. As important as it is to be open to changes that support a team, it’s just as vital to know when to get out of the way. The Stories team had incredibly high output and focus, and making changes for the sake of it could have destabilize productivity and morale. It was an amazing moment when we locked in our final direction. After many months of prototyping other ideas, the team had faith in the creative process and ultimately landed on a product approach that felt right. As one team member expressed, it’s hard to stop a group of smart people working together on something they all believe in. All stakeholders need to believe, as this reinforces morale, helps maintain focus, and ultimately fuels productivity. With the team united on the product, things really started to fall into place and we could shift into execution. Much like composition in photography can significantly alter the effect of the result, so can the composition of a team. At an earlier point in the project, we noticed an imbalance between the workload, size, and makeup of the team. We were overstaffed, so we offered a few engineers other impactful projects to work on. Whether through proactive or reactive efforts, minimal staffing was most helpful during prototyping stages. As a mobile-first product, we also learned it was healthy to loosely couple development across iOS and Android, even while prototyping. We did not strive for feature parity, but high level functionality kept good pace across platforms. Rather than having one platform always lead and the other follow, both platforms led. This created some interesting dynamics. Some features were further along on iOS, while others were further along on Android. We were able to leverage more engineers by spanning platforms. This also allowed for more cross-pollination of ideas and solutions, especially as we built the same functionality across platforms within a short time span. This encouraged a healthy pace as one team that introduced a feature influenced another to get to parity quickly, and vice-versa. For example, we experimented with different animation transitions when swiping between stories. The final cube transition we ultimately chose landed on Android first and got the team excited to promptly add it to iOS. More importantly, by working on platforms in parallel, both teams were able to innovate at the same time, instead of having team A work through engineering challenges followed by team B to focus on porting. It was also critical to have a healthy mix of talent on the team. We had incredibly strong technical leads per mobile platform (one of whom doubled as the technical lead for the project as a whole) along with a lead for our backend services. They all worked well within their own respective tracks, with each other, and cross-functionally across engineering, product, design, and analytics. Each lead was accountable for technical progress and had a voice in shaping the product. In addition to staffing leads, we had engineers who were simply great at prototyping quickly, which helped get the product into actual hands and let us frequently vet the user experience. Once we locked in our final product direction, the team grew with volunteers across the company, each with key skills to efficiently expedite development and help get the project over the finish line. We formed tracks to focus on camera infrastructure, creative tools, and core product. After we shipped, we once again rebalanced to support a more stable state workload while preserving the platform leads structure. Craft is one of Instagram’s values and applies throughout our work, including product design, code, and quality. Maintaining a commitment to quality and attention to detail with an ambitious deadline can be a challenge, and sometimes it can be a unifying force. Once we were happy with our product direction, we discussed a shipping timeline that we knew was aggressive. Nevertheless, fueled by determination and excitement to share Stories with the world, the team signed up for it. This forced us to ruthlessly prioritize and focus on the minimum viable product. This kept with one of Instagram’s core principles of doing the simple thing first. Although we were tempted to include more features in the initial release, we carefully decided when to do so. The team steadily and tirelessly worked on our most important tasks, but as the date drew closer we realized we needed help. Soon, members from numerous other teams stepped up to assist. Even Mike pitched in with camera improvements and neon drawing. We continued to make progress, and soon found our whole company was eager to help. Apart from getting valuable feedback from employees from actively dogfooding Stories, we held a company-wide bug bash. Participation was truly company-wide, including Kevin and Mike. With our target date a few weeks away, I mentioned to Mike that despite everyone’s amazing effort, it still felt like we were coming in hot. Mike responded, “We are, but if we give ourselves more time we’ll be tempted to add scope.” We stuck with our launch date, the team kept at it. While having an ambitious target pushed the team well past its comfort zone of working hard, that was not sustainable as the norm. But our deadline bonded the team closely together for a common, albeit really challenging goal, and ultimately brought the whole company closer together. This is still paying dividends today. Not too long before launch, our infrastructure team scrambled to ensure our capacity would exceed increased expected demand. That’s not always easy at the scale Instagram has, but the team was able to respond swiftly. We are fortunate to build product with amazing infrastructure, supported by teams who are maniacally focused on performance, quality, and developer velocity. Our server stack includes open source components like Django and Cassandra along with internal systems like Facebook Tao. Some of the mobile tools and libraries we benefitted from are open source like IG JSON Parser, IG Disk Cache, IGListKit UICollectionView data binding framework for iOS, Rebound animation library for Android, and FLEX developer tool for iOS. We also leveraged exceptional tools and services from our friends at Facebook that helped us easily collect and analyze data that we can trust. Instagram’s ongoing investments in client and system infrastructure have enabled our product teams to build and deploy quickly and at scale. While these are some factors that contributed to a successful Stories launch, there were countless others like having trust in one another, strong communication, grit, openness, empathy, and diversity. Perhaps one of the most important meta takeaways is to anticipate and embrace change. Projects and teams are dynamic, and these takeaways certainly aren’t a checklist to follow. Be prepared to draw your own conclusions to what makes your projects successful. The path to get Stories into the hands of our community has been an amazing and humbling experience. As an engineer, nothing is more rewarding than giving people a new platform to express themselves at massive scale. We have so much more to do, and I’m grateful and excited to work with our fantastic teams on the next chapter. Eddie Ruvinsky is an Engineering Director who leads several product engineering groups at Instagram. Stories from the people who build @Instagram 231 3 Startup Software Development Instagram 231 claps 231 3 Written by Stories from the people who build @Instagram Written by Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-12-02"},
{"website": "Instagram-Engineering", "title": "video running instagram at scale 2016", "author": ["Instagram Engineering"], "link": "https://instagram-engineering.com/video-running-instagram-at-scale-2016-22840f3d22ab", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts Instagram is one of the most popular social apps and has more than 500M monthly active users. We will talk about how Instagram was able to scale its Django/Python web framework by putting the right metric and tooling in place to gain visibility into the stack, fix inefficient code paths, and improve server capacity with increased parallelism in network IO access. Click the link below to watch the full talk! atscaleconference.com Stories from the people who build @Instagram 44 1 Python Web Development Instagram 44 claps 44 1 Written by Stories from the people who build @Instagram Written by Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-10-10"},
{"website": "Instagram-Engineering", "title": "open sourcing iglistkit", "author": ["Instagram Engineering"], "link": "https://instagram-engineering.com/open-sourcing-iglistkit-3d66f1e4e9aa", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts Today, we are excited to announce that we’re open sourcing one of Instagram’s core frameworks: IGListKit . This framework powers how we take data from the server and turn it into fast and flexible lists. To do this, we combined a familiar data-driven UICollectionView architecture with a state-of-the-art diffing algorithm. With this setup, we created a tool that lets engineers with varying levels of experience work quickly and safely on Instagram. Outside of Instagram, IGListKit can help anyone who is building lists to skip tedious and error-prone setup, and utilize one of the fastest diffing algorithms available for iOS. IGListKit was created to improve Instagram’s stability and to increase how quickly our engineering team could work. Before IGListKit, Instagram was plagued with an architecture common to larger-scale apps: the massive view controller. On top of that, we suffered from too much inheritance. Making a single change in one object sometimes required making changes in everything it was inherited from. We spent the last year breaking apart a lot of our coupled components into smaller, more reusable objects. After this, we replaced our traditional UICollectionView setup with the more robust IGListKit. Each section in IGListKit is powered by an IGListSectionController . These objects implement a required protocol, IGListSectionType , with methods that power the cells and behavior within the section. For example, a section with a single cell driven by String data: To drive the backing UICollectionView , IGListKit uses an object called IGListAdapter to convert an array of data into section controllers. IGListAdapter returns the cells that later end up in the collection view. Implementations then connect to the adapter’s data source to return data and section controllers when asked: IGListKit reduces the chance of having “massive view controllers” by dividing responsibilities into multiple layers: the view controller, list adapter, section controller, and the cell. This design has a positive side effect of building your lists with independent components, meaning you end up with reusable section controllers and cells. The result is a one-way data flow, where each component has no knowledge of its parent. In Instagram, our data can change a lot. For example, new data arrives from the server each time you like a photo or you receive a direct message in real-time. UICollectionView can handle all types of updates (deletes, reloads, moves, and inserts), but performing those updates without crashing can be a little tricky. We built a lightning-fast diffing algorithm based on a paper published back in 1978 by Paul Heckel. This algorithm finds all of the possible updates that UICollectionView needs in linear time (that’s O(n)). The version of IGListKit that is deployed to Github is the same version we use internally at Facebook, so that the open source project is always being tested and maintained at industry standards. If you’d like to contribute, check out these tasks as a starting point. We look forward to working on this together! Ryan Nystrom is a software engineer on the iOS infrastructure team at Instagram New York. Stories from the people who build @Instagram 576 6 iOS Instagram Open Source 576 claps 576 6 Written by Stories from the people who build @Instagram Written by Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-10-11"},
{"website": "Instagram-Engineering", "title": "beautiful performant android ui", "author": ["Instagram Engineering"], "link": "https://instagram-engineering.com/beautiful-performant-android-ui-62ce61ca748c", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts At Instagram, our mission is to help people capture and share the world’s moments. We care deeply about the moments that people share on our platform, so enhancing how people view these moments is really important. Instagram recently launched a new design for Explore that includes “video channels”, which play in a new, full-screen, immersive experience. We optimized the user interface to make videos easier to watch, and it serves as a great example of how we approach building user interfaces on Android. We’ll share our approach and techniques, and hope they help you improve the look and feel of your own apps! As we were building this video viewer, we had several goals in mind. We wanted to provide an immersive user experience where you come to “sit back and watch” funny, creative, and engaging videos from holidays and special events like Halloween, New Year’s Eve, and the Oscars. The viewer automatically scrolls for you at the end of each video, but you still have control over the viewer by being able to scroll at the pace you want (as in feed). We also gave the immersive viewer a new look to make it a distinct experience from feed. Important differences to note are: The viewer paginates and centers the playing video in the middle of screen for you so that you don’t have to scroll. In feed, you have to manually drag and make sure the video is on the screen. In the viewer, switching to the previous or the next video is as simple as a fling or tap. We remove nonessential UI (such as video icons and feedback tools) to create fewer distractions. We use full-screen mode, hiding the status bar, to make the viewer feel immersive. The viewer has a dark theme and only the center video is highlighted to make it easier to focus while it’s playing. The immersive viewer is built as a subclass of ListView, to which we added custom touch event handling by overriding dispatchTouchEvent(). This allows us to reroute the touch events through a GestureDetector to our gesture listener, which would then determine whether it should consume these events and perform a custom action, or do nothing and just let the ListView handle them. Let’s take a fling action as an example. When you fling a video in the viewer, our gesture listener consumes it and initiates a custom pagination animation. The normal ListView fling action will not be triggered: Other scroll actions like dragging are not consumed by our gesture listener, and are instead dispatched to the ListView so that you can still drag the viewer as you would with a normal ListView. Animations are fun. For this reason, they are often misused. It’s tempting to add flashy transitions that look really cool the first few times they are viewed, but quickly get obnoxious. The best animations bridge the gap between action and reaction, clarifying changes in UI elements in response to an external or internal trigger (i.e. user input, video finished playing, etc). They are quick, smooth, and subtle. In fact, they are so subtle that they may not be consciously registered by the user as animations, rather contributing to a general feeling of delight. For the immersive viewer, we used a combination of alpha and scale animations to accentuate certain UI elements during navigation. We applied spring-based interpolations, rather than polynomial, to make these animations feel natural. Alpha Animations Let’s say you need a view to appear or disappear in your UI. To achieve this effect, you’d normally just change its visibility via setVisibility(View.VISIBLE) or setVisibility(View.GONE). But when you test, it can feel quite jarring because there is no transition whatsoever but your UI is suddenly changed. Alpha animations can make this experience much nicer. We use many alpha animations in immersive viewer, including: Fade in/out the video header containing the username Fade in/out the dark color overlay for an idle video Fade in/out the blurred cover overlay for an idle video Fade in/out the top and bottom shadows for the center video Fade in/out the heart that appears when double tapping to like Taking the video header as an example, here is how it looks like with and without the alpha animation when navigating to another video: Left (without animation), Right (with animation): The alpha animations make these experiences more natural and smooth as the current header fades out and the next header fades in. Without the animation, the headers pop in and out and the UI feels choppy. Spring Physics Rebound is a library built by Will Bailey (a fellow Instagram engineer) that makes it easy to apply spring physics to animations. We use Rebound for many animations in our app because we believe the spring dynamics make them look more natural than the polynomial-based interpolations provided by Android’s native Animation and Animator classes. Also, Rebound gives us the ability to incorporate real world properties, such as the velocity at which the user flings their finger on screen, into a spring’s motion. The pagination animation respects the velocity of the fling such that if you fling slowly, the viewer paginates with the respective low velocity. If you fling quickly, the viewer paginates with the respective high velocity. This natural experience is the reason why we chose springs to drive animations in the viewer. When building the viewer, we started with a single spring to sync together all the animations that run in parallel as you navigate: the pagination animation and the alpha animations. We made this decision because if, for example, the pagination animation finishes before the alpha animations, the views would continue to change in opacity even after they stop moving. Code structure and fling example: Our scroller object, PagingListViewScroller, encapsulates spring operations and tracks information like the current list position of the center video. We also have the custom ListView, scrubber, and other components (e.g. fragment) that listen to notifications from the scroller. When you fling a video in the viewer, it is handled as follows: The ListView consumes the action and triggers a vertical scroll event through the scroller, which sets the underlying spring in motion with the appropriate velocity and target offset. Then whenever the spring value updates, the scroller gets notified which in turn invokes the appropriate callback on each listener to take action. Scale Animations Scale animations can be used to create visual effects that complement the UI. When entering the immersive viewer, we have an opening animation where a black background starts with zero height in the middle of the screen and then scales up to fill the entire screen, after which we fade in the immersive viewer. The motivation behind this was to mimic the behavior of turning on an old television and give the transition a fun, nostalgic touch. Left (Without animation), Right (With animation): The transition is much smoother with the animation, and we avoid the sudden change in UI that we discussed previously with alpha animations. Building UI with cool animations and beautiful designs is important, but none of it is worthwhile if it isn’t performant. By making the correct optimizations, we can improve the design and feel of the UI without degrading performance. Using the Hierarchy Viewer to analyze view render times and the Traceview to profile method time spent, we optimized the immersive viewer to provide a smooth experience. Reduce Number of Views One optimization we made to the viewer was to use the minimum number of views. This is something to consider in any application; the more views you have on screen, the more work your device has to do to render them. Furthermore, they take up memory and time to be instantiated, which could significantly impact memory on lower end devices and slow down loading of the UI. Our first implementation had four views per video: Blurred cover image overlay Dark color overlay Top shadow Bottom shadow We reduced these views into one custom ImageView where the source is set to the blurred cover image. Then we overrode its onDraw() method to draw color for the dark color overlay and two drawables, one for each shadow. On animation steps, we set a custom alpha on each of these elements and invalidate the view, so that its onDraw() method will be called to be redrawn and reflect the current state of the animation. Optimize Alpha Animations There are numerous alpha animations in the immersive viewer that take place as you navigate, so we had to optimize the way we change the alpha property of elements. There are different ways to change the alpha property — one is all setAlpha(float) on a View. This method is a two step process, where the first step is to allocate an off-screen buffer in GPU memory called a hardware layer and draw the view onto it. Then in the second step, the GPU copies pixels from the hardware layer to the screen, applying any alpha value that we set. The advantage of this two-step process is that the alpha blending will be correct on overlapping content on the screen. However, the significant downside is that the first step adds a lot of overhead. There are two approaches to optimize this method, but each has tradeoffs: view.setLayerType(View.LAYER_TYPE_HARDWARE, null); One approach is to set hardware layer type on the view you’re animating. This caches the hardware layer and reuses it so that we only do the first step once. While this approach keeps the alpha blending correct and makes the alpha animation performant, it still takes up GPU memory, so you have to remember to release the layer when you’re done using it. Also, this approach loses much of its performance gains if you invalidate the view too many times because the first step is run again on each invalidation. Hence it is not applicable to the immersive viewer because the viewer is built on ListView, which frequently invalidates its children views due to recycling and view rebinding. @Override public boolean hasOverlappingRendering() { return false; } Another approach is to create a custom view and override this method to return false. This bypasses the hardware layer and the view is drawn directly to screen when you change its alpha property. A downside is that the alpha blending will be incorrect on overlapping content, but this may not be a problem depending on your application. You may also have to create numerous custom views if you want alpha animations on multiple views, as in the case of immersive viewer. Most importantly, this method is only supported on API level 16 and above. Another way to change the alpha property is to call setAlpha(int) on a Drawable. We chose this method for the alpha animations in immersive viewer as neither of the optimization approaches above fit our use case. This changes the alpha property of the drawable instance, as opposed to a view. It also doesn’t use any hardware layer, so the drawable is drawn directly onto screen. Compared to the hasOverlappingRendering() approach, this is better on two accounts. Not only is the method supported on all API levels, but we can also easily change the alpha property of the drawable underlying our view instead of defining a custom view. The downside is that the alpha blending will be incorrect on overlapping content. However, this is acceptable for the viewer as there are very few instances of overlapping content and we do not need 100% correct alpha blending even when elements overlap. Here’s an example in the immersive viewer to show the difference in performance between the two setAlpha methods: Left (setAlpha on views), Right (setAlpha on drawables): By setting alpha on views, the viewer begins to stutter as we scroll and its performance becomes worse with all the allocation of hardware layers. By setting alpha on drawables, the viewer remains smooth throughout the navigation. Redraw Specific Views It’s important to redraw only the views that changed, especially during animations. For a ListView, this means invalidating only the child views whose properties have changed instead of calling notifyDataSetChanged() on the adapter, which redraws the entire ListView. In immersive viewer, we do not call notifyDataSetChanged() during our animations because we only want to reflect the UI state changes of the current video and the next video to play instead of the entire list. For every one of our features, we strive to provide the best user experience possible without regressing performance. Immersive viewer is no exception. We used various animations to make video viewing on Instagram more delightful, but we also put a ton of effort into making performance optimizations to ensure this delight is shared amongst all users, not just those with high-end devices. We hope that you can incorporate some of our approaches to building beautiful and performant experiences into your own Android development! Kevin Jung is a software engineer at Instagram. Stories from the people who build @Instagram 829 7 Android App Development Android Instagram 829 claps 829 7 Written by Stories from the people who build @Instagram Written by Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-06-21"},
{"website": "Instagram-Engineering", "title": "instagram android four years later", "author": ["Instagram Engineering"], "link": "https://instagram-engineering.com/instagram-android-four-years-later-927c166b0201", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts The first version of Instagram for Android was built in four months by a team of two engineers. It’s been four years since that launch, and in that time we’ve added features such as video, direct messaging, photo maps, advertiser support, and new ways to discover and explore the amazing content shared by users around the world. We’ve regularly released new filters, editing tools, and apps to unlock creative potential. Almost 30 engineers now work in our Android codebase every day. All this — yet Instagram for Android is still one of the fastest-starting apps on the platform, and is only a 16MB APK download for most users. How did we scale the team and create so many awesome new features while maintaining our best-in-class app size and performance? We’ve focused on providing the best possible experience for a small, well-scoped feature set, we have an extremely efficient UI layer, we take ownership of all the important code in our app, and we’ve invested heavily in maintaining our values as the team grows. A core value of Instagram Engineering is to “Do the Simple Thing First” . We build for the use case that exists now, rather than the one that may exist later. We still care deeply about performance — but “doing the simple thing first” reminds us not to prematurely optimize our code or chase every small performance win. We think holistically and pragmatically, always considering the downside of increased complexity that often accompanies micro-optimization. At the core of this principle is the idea that the Instagram app is simply a renderer of server-provided data, much like a web browser. Almost all complex business logic happens server-side, where it is easier to fix bugs and add new features. We rely on the server to be perfect, enforced through continuous integration testing, and dispense with null-checking or data-consistency checking on the client. Because of this, the app crashes when there is malformed data, rather than remain in a weird state. Our automated crash reporting triggers alarms and an investigation, and we can fix the bug. It is much easier to fix a crash, with an attached stack trace, than to debug a weird state issue based on a user’s report. Living in a fast-growing codebase for four years has made us value straightforward, readable, debuggable code, and so we do not heavily rely on opaque code-gen, runtime annotation processing, or other clever “magic”. The only annotation processing that we use happens at compile-time, generating Java source files that look and behave as if they were handwritten. We prefer code to be right there on the screen in front of us, not hiding behind a complex meta-processor. It is simple for new developers to ramp up in this environment; they can easily trace what’s happening in the app and track down bugs. The original Instagram app was much simpler than what exists today. As a small team, building quickly to keep up with market pressure, we used a lot of inheritance to share code. This approach didn’t scale with the team’s growth: it led to a confusing, tightly-coupled, brittle architecture, where execution bounced between different levels of a class hierarchy. Our desire to have small, simple, single-purpose classes has led us to embrace the principle of ”composition over inheritance” . We’ve found that it often takes a little more thought to build things without falling back on inheritance, but as a team grows, more emphasis needs to be placed on architecture to give a solid foundation for future development. Equally important to us is to “Optimize What Matters”. We have a high bar for the performance of our most-used features, and as the product has grown and evolved we have needed to continually reevaluate our previous assumptions. Sometimes code must be rewritten to deal with new feature requirements or operating system capabilities. This is best illustrated with a series of examples: JSON Parsing Architecture. We originally used object-mapped parsing to deserialize JSON responses from the server. It was the easiest way to get up and running. Before we even shipped Instagram 1.0, we found that performance was lacking — it could take over 30s to render the “explore” tab on lower-end devices. We replaced the object-mapped parsing code with handwritten stream-parsing code and saw that time go down to a few seconds. This worked for a year, but it was tedious and error-prone to hand-write parsing code. If done wrong, it could put the parsing thread in an infinite loop. At that point, we wrote an annotation-based parser generator . After it had stabilized, we deployed it for the entire app and entirely removed our dependence on object-mapped parsing. Comment Rendering. The addition of Emoji to the Android operating system tanked the performance of our comment rendering, causing the app to drop frames while scrolling the feed. We investigated the root cause and designed a sophisticated text layout caching mechanism to keep our feed fast. Activity Screen. We originally used a webview to display our “activity” screen, where you can see your likes and comments. Our theory was that we’d want to add new types of stories often, but this didn’t end up being the case in practice. And for a complex set of reasons, using a webview slowed down our cold start time by over 30%! We rewrote the screen in native code and saw our startup time drop to historic lows, while making the screen feel better than ever. Our cold-start time is now the second-fastest among the top 100 apps in the Android store. Feed. We are always adding features to make it easier to find awesome content on Instagram. For example, we recently added a full-screen, immersive video viewer to explore, and the ability to view top posts on hashtag and location feeds. As the different “feed” screens in the app have diverged, the architecture of our core feed code, which relied heavily on inheritance, made even the simplest features difficult to build cleanly. We removed all inheritance from feed Fragments and ListAdapters in a large, multi-month refactor, building a library of reusable components that could be easily used together to build new products. The resulting code was more robust and flexible, yet also dramatically simpler. Networking. We rewrote our inheritance-based HTTP request-generation and processing code in a declarative style in order to A/B test different low-level HTTP frameworks. We were able to deploy a client-side HTTP2 implementation with full confidence: our experimental data showed faster end-to-end request time on every API endpoint with no regressions. We have a sophisticated set of tools, mostly built by our counterparts working on Facebook for Android, which report on and analyze all sorts of data about our apps in the wild. We track scrolling performance, start time, data usage, stability, and bug reports to make sure that we’re never regressing on our commitment to provide the best experience to our users. The first version of Instagram was a luscious, skeuomorphic masterpiece with textures, shadows, and gradients everywhere. In early 2014, we embarked upon one of our largest optimizations yet: a project to overhaul Instagram’s look and feel on Android. Our goals were to make the app both faster and more beautiful. We designed and built an interface that makes use of flat colors, lines, and simple icons, combined with a subtle sense of space and layout to create a refined, efficient UI layer. We expected some performance gain, but the magnitude of our results surprised us. I’ll summarize them here: Startup time. Images take time to decode and occupy precious memory. We overhauled our app to remove textures, instead painting flat colors to the screen. We were efficient with our usage of icons, colorizing them in code to avoid loading multiple versions. Reducing our startup asset count from 29 to 8 reduced cold start by 120ms across devices. This is not only felt at app startup, but every time we show a new screen. App size. Converting image usage to trivial painting code, and using ColorFilter to colorize assets programmatically allowed us to cut our total asset count in half, reducing the app’s size by multiple megabytes. Developer efficiency. A simpler UI is one that is faster to build. We have a library of components in the app that are easy to reuse, and new features don’t require difficult layout or positioning code to position background images and shadows correctly. We have a standard set of dimensions and colors — defined semantically — to reduce the amount of cognitive load and communication necessary between engineer and designer. Two years later, our engineers still love developing in this environment, and the app maintains a remarkable visual consistency between features. Over time, as we have refined our codebase via relentless optimization, we have reduced or eliminated dependencies on many third-party libraries that commonly appear in other Android apps, preferring to fully own our infrastructure code. One of my colleagues likes to describe our app as a “race car” — every single component is specialized for the job it needs to do. Our image cache, for example, is homegrown, and comprises less than 1500 lines of Java code. It is designed to download, decode and display large images while the user is scrolling feed, without dropping frames. It is not a general purpose image library, so it eschews features that are not directly needed by the product, but it works extremely well for our use case. As mentioned above, we developed a JSON parser/serializer generator which works with jackson-core (a low-level streaming JSON parser) to generate fast, memory-efficient parsing code. We do not use dependency injection, as we believe the code size, complexity, and performance hit do not justify the benefits. We use only a small subset of Guava, carefully evaluated for performance on mobile. We do not include the Play Services library, writing our own code to interface with GCM. At a time when many popular Android apps are multidex, we still ship a single dex file. Secondary dexes incur a performance penalty on every method call, and loading too much code is generally bad because it eats up a lot of memory. We carefully track method reference count. It’s important that we do not unintentionally or carelessly add new dependencies. We created tests that run for every diff that check against an approved set of libraries that can be included in the app. If the diff adds a new library without fixing the test (which triggers a review from engineers on our team), it cannot be committed. We have also specified method count ‘budgets’ for internal libraries, and created tests to enforce them. We recognize that writing so much custom code may not be a feasible approach for smaller teams which do not have the resources to write everything from scratch. For us, too, this was an iterative process: as our team grew and new features made more demands on size and performance, we started to be much more selective about the code we shipped. We removed third-party libraries that we could replace with our own code, tailored to our use case and therefore smaller. Now that we have a great set of libraries, we’ve made sure that they are reusable amongst our various apps. Having an “app starter kit” shortened the development time of both Layout and Boomerang by months. Tests enforce that the “app starter kit” doesn’t depend on app-specific code. As the team size doubled, doubled, and then doubled again, it became important to teach new engineers about the codebase and our mobile engineering philosophy. We didn’t want people to invent new solutions for problems we had already solved because of lack of awareness. Our main teaching tool has always been code review. Every diff at Instagram (and Facebook) is reviewed by another engineer. We are especially thorough, asking people to conform to patterns already in the app and to make their code fairly robust, as well as checking for obvious errors. We pair each new engineer with an experienced mentor, who serves as their main reviewer and can answer most questions. The best, longest-tenured engineers are expected to make themselves available regularly to debate and inform technical architecture decisions, and provide specialized expertise to help solve problems — not just sequester themselves away writing code. The average engineer spends 20–40% of her day on various code review and mentorship activities. We try to make our code as self-documenting as possible. We use annotations such as @Nullable, or for an even stronger guarantee, Guava’s Optional class, to document the null-contract of methods. We are ruthless about proper naming, believing that it prevents bugs and promotes readability. We strongly-type everything , which in addition to documenting allows the compiler to do as much work as possible to prevent bugs. We use enums regularly because they are safer than Strings or ints and their performance downsides are minimal. As the team has grown, we’ve constantly reevaluated how we work — doubling down where appropriate and shifting our tactics when things aren’t working. The thing that hasn’t changed is the values that underlie all these decisions. A team that shares a set of values can work well even when decentralized. We spend a couple hours every month presenting our mobile engineering values and culture to all new engineers who join Instagram. Every team and codebase develops its own philosophy and strategy as it grows and matures. This is the one that has worked really well for us, built on many hours writing code and reading other people’s insights shared via blog posts like this one. We hope you can glean some useful strategies and ideas to incorporate into your development process. Over the coming weeks, we’ll be sharing more specific details on a number of projects we’ve worked on recently that line up with the learnings above…stay tuned! We recently moved our mobile infrastructure engineering teams (iOS and Android) to New York City. If this blog post got you excited about what we’re doing, we’re hiring — visit our careers page . Tyler Kieft works on Android and iOS at Instagram Stories from the people who build @Instagram 807 10 Android Android App Development Instagram 807 claps 807 10 Written by Stories from the people who build @Instagram Written by Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-06-21"},
{"website": "Instagram-Engineering", "title": "web service efficiency at instagram with python", "author": ["Instagram Engineering"], "link": "https://instagram-engineering.com/web-service-efficiency-at-instagram-with-python-4976d078e366", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts Instagram currently features the world’s largest deployment of the Django web framework, which is written entirely in Python. We initially chose to use Python because of its reputation for simplicity and practicality, which aligns well with our philosophy of “do the simple thing first.” But simplicity can come with a tradeoff: efficiency. Instagram has doubled in size over the last two years and recently crossed 500 million users, so there is a strong need to maximize web service efficiency so that our platform can continue to scale smoothly. In the past year we’ve made our efficiency program a priority, and over the last six months we’ve been able to maintain our user growth without adding new capacity to our Django tiers. In this post, we’ll share some of the tools we built and how we use them to optimize our daily deployment flow. Instagram, like all software, is limited by physical constraints like servers and datacenter power. With these constraints in mind, there are two main goals we want to achieve with our efficiency program: Instagram should be able to serve traffic normally with continuous code rollouts in the case of lost capacity in one data center region, due to natural disaster, regional network issues, etc. Instagram should be able to freely roll out new products and features without being blocked by capacity. To meet these goals, we realized we needed to persistently monitor our system and battle regression. Web services are usually bottlenecked by available CPU time on each server. Efficiency in this context means using the same amount of CPU resources to do more work, a.k.a, processing more user requests per second (RPS). As we look for ways to optimize, our first challenge is trying to quantify our current efficiency. Up to this point, we were approximating efficiency using ‘Average CPU time per requests,’ but there were two inherent limitations to using this metric: Diversity of devices. Using CPU time for measuring CPU resources is not ideal because it is affected by both CPU models and CPU loads. Request impacts data. Measuring CPU resource per request is not ideal because adding and removing light or heavy requests would also impact the efficiency metric using the per-requests measurement. Compared to CPU time, CPU instruction is a better metric, as it reports the same numbers regardless of CPU models and CPU loads for the same request. Instead of linking all our data to each user request, we chose to use a ‘per active user’ metric. We eventually landed on measuring efficiency by using ‘CPU instruction per active user during peak minute.’ With our new metric established, our next step was to learn more about our regressions by profiling Django. There are two major questions we want to answer by profiling our Django web service: Does a CPU regression happen? What causes the CPU regression and how do we fix it? To answer the first question, we need to track the CPU-instruction-per-active-user metric. If this metric increases, we know a CPU regression has occurred. The tool we built for this purpose is called Dynostats. Dynostats utilizes Django middleware to sample user requests by a certain rate, recording key efficiency and performance metrics such as the total CPU instructions, end to end requests latency, time spent on accessing memcache and database services, etc. On the other hand, each request has multiple metadata that we can use for aggregation, such as the endpoint name, the HTTP return code of the request, the server name that serves this request, and the latest commit hash on the request. Having two aspects for a single request record is especially powerful because we can slice and dice on various dimensions that help us narrow down the cause of any CPU regression. For example, we can aggregate all requests by their endpoint names as shown in the time series chart below, where it is very obvious to spot if any regression happens on a specific endpoint. CPU instructions matter for measuring efficiency — and they’re also the hardest to get. Python does not have common libraries that support direct access to the CPU hardware counters (CPU hardware counters are the CPU registers that can be programmed to measure performance metrics, such as CPU instructions). Linux kernel, on the other hand, provides the perf_event_open system call. Bridging through Python ctypes enables us to call the syscall function in standard C library, which also provides C compatible data types for programming the hardware counters and reading data from them. With Dynostats, we can already find CPU regressions and dig into the cause of the CPU regression, such as which endpoint gets impacted most, who committed the changes that actually cause the CPU regression, etc. However, when a developer is notified that their changes have caused a CPU regression, they usually have a hard time finding the problem. If it was obvious, the regression probably wouldn’t have been committed in the first place! That’s why we needed a Python profiler that the developer can use to find the root cause of the regression (once Dynostats identifies it). Instead of starting from scratch, we decided to make slight alterations to cProfile, a readily available Python profiler. The cProfile module normally provides a set of statistics describing how long and how often various parts of a program were executed. Instead of measuring in time, we took cProfile and replaced the timer with a CPU instruction counter that reads from hardware counters. The data is created at the end of the sampled requests and sent to some data pipelines. We also send metadata similar to what we have in Dynostats, such as server name, cluster, region, endpoint name, etc. On the other side of the data pipeline, we created a tailer to consume the data. The main functionality of the tailer is to parse the cProfile stats data and create entities that represent Python function-level CPU instructions. By doing so, we can aggregate CPU instructions by Python functions, making it easier to tell which functions contribute to CPU regression. At Instagram, we deploy our backend 30–50 times a day . Any one of these deployments can contain troublesome CPU regressions. Since each rollout usually includes at least one diff, it is easy to identify the cause of any regression. Our efficiency monitoring mechanism includes scanning the CPU instruction in Dynostats before and after each rollout, and sending out alerts when the change exceeds a certain threshold. For the CPU regressions happening over longer periods of time, we also have a detector to scan daily and weekly changes for the most heavily loaded endpoints. Deploying new changes is not the only thing that can trigger a CPU regression. In many cases, the new features or new code paths are controlled by global environment variables (GEV). There are very common practices for rolling out new features to a subset of users on a planned schedule. We added this information as extra metadata fields for each request in Dynostats and cProfile stats data. Grouping requests by those fields reveal possible CPU regressions caused by turning the GEVs. This enables us to catch CPU regressions before they can impact performance. Dynostats and our customized cProfile, along with the monitoring and alerting mechanism we’ve built to support them, can effectively identify the culprit for most CPU regressions. These developments have helped us recover more than 50% of unnecessary CPU regressions, which would have otherwise gone unnoticed. There are still areas where we can improve and make it easier to embed into Instagram’s daily deployment flow: The CPU instruction metric is supposed to be more stable than other metrics like CPU time, but we still observe variances that make our alerting noisy. Keeping signal:noise ratio reasonably low is important so that developers can focus on the real regressions. This could be improved by introducing the concept of confidence intervals and only alarm when it is high. For different endpoints, the threshold of variation could also be set differently. One limitation for detecting CPU regressions by GEV change is that we have to manually enable the logging of those comparisons in Dynostats. As the number of GEVs increases and more features are developed, this wont scale well. Instead, we could leverage an automatic framework that schedules the logging of these comparisons and iterates through all GEVs, and send alerts when regressions are detected. cProfile needs some enhancement to handle wrapper functions and their children functions better. With the work we’ve put into building the efficiency framework for Instagram’s web service, we are confident that we will keep scaling our service infrastructure using Python. We’ve also started to invest more into the Python language itself, and are beginning to explore moving our Python from version 2 to 3. We will continue to explore this and more experiments to keep improving both infrastructure and developer efficiency, and look forward to sharing more soon. Min Ni is a software engineer at Instagram. Stories from the people who build @Instagram 1.3K 11 Python Instagram Tech 1.3K claps 1.3K 11 Written by Stories from the people who build @Instagram Written by Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-06-26"},
{"website": "Instagram-Engineering", "title": "improving video playback on android", "author": ["Instagram Engineering"], "link": "https://instagram-engineering.com/improving-video-playback-on-android-2f6c6a0058d", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts As Instagram has grown, and mobile network bandwidth has improved, video has become a larger part of the app experience. The time people spent watching video in the last half increased by more than 40%, and yesterday, we announced longer video to give creators more flexibility and open up new ways to connect with the community. In this post, we’ll share some of the things we’ve done to improve video playback on Android and maintain user experience while increasing video length 4x. Instagram introduced 15 second video in 2013. One of our core engineering values is “Do the Simple Thing First,” so we implemented a basic video playback mechanism on Android. When a video appears in feed, the app first downloads the whole video file to the phone storage. After the download is complete, the file is passed to the Android built-in MediaPlayer to play the video. This design worked well when the maximum video length was only 15 seconds. However, we recently decided to increase the maximum video length to 60 seconds. We recognized this would stress our current design for a few reasons: Video cannot play until the whole file is downloaded. As the video becomes larger, playback would wait even longer for the video to download. People on slower network connections might need to wait for a very long time before they can play the first second of the video. Video won’t play if there’s a disk space issue. If there is not enough space in the disk to store the whole video file in the disk cache or if there’s some failure in the I/O, the video will never play. With those issues in mind, we decided to build a new video cache that could stream the video to the MediaPlayer as the video file is being downloaded. This would remove the bottleneck of waiting for the download to finish. We started to look into how to build a streaming video cache that worked with our current video player. We found that the Android MediaPlayer only supports playing a video from either a file or a URL . We wouldn’t have the complete video file to give to the MediaPlayer when we are still downloading, so this leaves us with playing the video from a URL. The naive approach of passing our CDN URL to the MediaPlayer would mean that we wouldn’t have any control over the bytes downloaded, so we would not be able to cache the video in case a person wanted to watch it again. Knowing this fact, we decided to have the MediaPlayer interact with a local proxy server instead, and have the proxy server serve the byte stream to the media player while also storing it into the disk cache. It turns out that this new design solves many of the problems in the previous design with some extra benefits: Playback is no longer blocked by the video download, i.e. it plays as soon as there’s enough content. Playback is no longer dependent on disk space. Even if the disk cache malfunctions, it would just be treated as a cache miss and streamed directly from the server. Allows more prefetching adjustments. This new mechanism allows us to explore more options in regard to prefetching video files depending on the network condition, as the prefetched content can now be directly streamed while we download the rest of the file on background. We tested our new implementation extensively across many devices and Android versions. Our testing surfaced an issue: Some Android Lollipop devices made multiple (up to 3) download requests before playing the video — they would request the beginning of the file, then make a HTTP range request for the end of the file, then request the beginning again. To figure this out, we need to understand a little about the mp4 file format. An mp4 is a container for one or more video and audio tracks, and it contains metadata that specifies where each of these tracks start. We had already reconfigured our video encoding to place this metadata at the start of the file, instead of the end (the default), to avoid the inefficient behavior of making multiple requests to play the video. We hypothesized that the MediaPlayer was not playing nice with our video encoding. Upon deeper investigation, we managed to scope the problem down to devices that use NuPlayer, which is the default implementation of MediaPlayer starting in Lollipop (replacing the older AwesomePlayer). Looking deeper into the MPEG4Extractor and NuCachedSource2 code, we deduced that NuPlayer was not handling one of the MP4 container atoms (specifically, the FREE atom) correctly and was getting confused. Removing FREE atoms solved the problem, and we modified our video transcoding tier to include this cleanup logic before we launched the new video playback. We were pretty satisfied with the result of the new implementation. Below are some of the (statistically significant) results we gathered after performing a limited A/B test of streaming vs. non-streaming playback with 15-second videos in the wild: The table above shows three metrics: percentage of videos played within 1 second of scrolling on screen, the average start delay to the first frame of the video, and the 95th percentile of the start delay distribution . We initially started by gathering data on the number of videos played within 1 second and the average start delay for all videos. We noticed that the new streaming cache performed worse for the number of videos that played within 1 second (0.8% fewer videos), but better for average time to play: 18% lower! Since these results contradicted each other, we dug in to analyze the data further. We sliced the data to exclude videos that started playing within 200ms. The majority of the videos that play within 200ms are probably cached fully on disk, which shouldn’t be affected by our new implementation. We hypothesized that removing the data “noise” from these cached videos would help us get a more accurate measurement of the impact of the streaming player, and help us to understand the discrepancy in performance between less than 1 second video play time vs. the average. Removing the lower end of the distribution gave us a more accurate view of the impact of the streaming player: the average and p95 time to play are faster by 22–28%. However, this still didn’t explain why the number of videos that played within 1 second was actually lower with streaming enabled. We dug in deeper still and plotted the time to play distribution of the videos: This plot shows the percentage of videos (y-axis) that are played within a certain number of seconds (x-axis). Looking at the distribution, we see that the streaming cache (black bars) improved the start time in general, but it seemed to regress the start time for videos that had formerly fallen into the 0–1 second bucket. To try to explain this result, we circled back to check the implementation of MediaPlayer. We discovered that using the setUrl API incurs an artificial 1 second start delay on some older versions of Android MediaPlayer (to give the streaming buffer extra time to fill). This matches the data plot above and explains the result we gathered. In general, the streaming cache has shifted the distribution to the left (you can see the long tail of only blue bars), but pushes some of the leftmost distribution slightly to the right. The impact of this artificial delay is minimal, because we wrote code to shortcut the proxy server and play the file directly if it is fully cached (the setFile API does not have a delay). We did the simple thing first when we first launched video on Instagram, and shipped a stable and robust video-viewing experience to our users in a timely manner. But doing the simple thing first does not mean that we sacrifice quality or performance. On the contrary, we focus on what matters most at the time and continuously reevaluate what we have to make sure that the whole app experience is top-notch. Video playback is one recent example where changing product requirements led us to rethink our initial assumptions. It also showcases the importance of collecting and analyzing data to verify that performance improvements are actually working as expected! We hope that by providing a better video playback experience we will enable more people to enjoy the moments that they have captured and shared on Instagram. Stories from the people who build @Instagram 403 3 Mobile Infra Performance 403 claps 403 3 Written by Stories from the people who build @Instagram Written by Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-05-02"},
{"website": "Instagram-Engineering", "title": "performance usage at instagram", "author": ["Instagram Engineering"], "link": "https://instagram-engineering.com/performance-usage-at-instagram-d2ba0347e442", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts At Instagram, we treat performance as a feature, and as we build products, we constantly think about how we can make things faster and more efficient. We’ve found that improving performance can actually drive usage, and by making small changes we can also improve user experience with the service. Here, we explore one case study where identifying and fixing low-hanging fruit reduced network usage, improved client-side reliability, reduced server-side latency, and in turn we saw an improvement in app-wide user experience. Before a mobile client downloads actual media (i.e. photos or videos) from our CDNs, it must first fetch JSON metadata (“media bundles”) from our Django webserver endpoints. Depending on the endpoint, the compressed response is typically 10–60 kB. Each bundle contains information like media id, metadata about the author, the number of likes, the caption, and the most recent comments (called “preview” or “summary” comments). When you open Instagram to the main feed, you will notice that you only see up to three preview comments below each photo (in addition to the caption). In grid view (e.g. in the Search Explore tab or user profiles), no preview comments are visible at all. 3 preview comments per item are visible in feed, and no preview comments are visible in grid view. However, we had been sending up to 20 comments to the client with each bundle. Originally, this was intended as an optimization to make the “View all comments” screen load faster. But when viewed holistically, this now seems like a poor trade-off for these reasons: *Media are viewed more commonly than their comments, and we should optimize for the common case. *Comment bundles are particularly heavy: They contain the comment text itself as well as its id, timestamp, and author metadata (including profile picture URL). The authors and content of the comments are usually unique from comment to comment, so compression performs poorly. *Generating profile picture URLs is a CPU-inefficient operation because we must dynamically compute the correct CDN URL. The more comments we load, the more profile picture URLs we need to generate. *When a user clicks on “View all # comments” we ask the server for new comments anyway! For these reasons, reducing the maximum number of summary comments in each media bundle seemed like an obvious thing to do. But it was still unclear how much of a user-facing impact it would have. After all, this only makes a difference in the order of tens of kilobytes per payload, a difference that is dominated by the size of photo or video files. We hypothesized that the impact on network latency should be negligent — if a user were using a connection slow enough where downloading a few more kilobytes matters, just about any Internet service would probably be too difficult to use anyway. But, considering the possible bandwidth and CPU savings, we decided to do an experiment to see if there were in fact any user-facing effects. We ran an A/B experiment that reduced the maximum number of summary comments in each bundle from 20 to 5. This dropped the median response size of the main newsfeed endpoint from 15 KB to 10KB, while the median response size size of the “Explore Posts” endpoint dropped from 46 KB to 23 KB. This drop is even more pronounced when considering response sizes at higher percentiles: at the 95th percentile, median response size of the main feed endpoint dropped from 32 KB to 16 KB. As expected, reducing the size of the payload by a few kilobytes had no perceptible impact on network latency. But it had a surprising impact on memory usage: reducing the average memory usage for each feed screen ended up significantly improving the stability of the entire app. Android out-of-memory (OOM) errors dropped 30%! We hypothesize that the difference between platforms results from the Android market: some Android phones come with very low amounts of RAM, and correspondingly high memory pressure. Median CPU usage on our most popular endpoints, like the main feed endpoint, dropped 20%! This translated into a median savings of 30ms in server-side wall time (and thus reduced end-to-end latency), and at the 95th percentile, we saved 70ms in server-side wall time. That makes a difference! When we launched this across all our users, CPU across our entire Django fleet dropped about 8% and egress dropped about 25%. Egress is a measure of site health, and such a drop would normally be alarming. But in this case, it’s a good sign that we’re reducing the load on our infrastructure! During the A/B test, we saw app-wide impressions across all mobile platforms increase by 0.7%, and likes increase by 0.4%. This was driven by increases of impressions on all surfaces — for instance, “Explore Photos” impressions increased over 3%, and user profile scrolls increased 2.7%. These trends continued over time, confirming that good performance brings users back. Percent increase in user profile scrolls over 3-month period Question baked-in assumptions. In this case, we asked, “Why do we send 20 comments per media bundle?” Sometimes, questioning baked-in assumptions can lead to identifying low-hanging fruit. Measure. Before optimizing, take time to understand the potential impact. We saw how heavy comments were when we inspected the payload, and profiling led us to the realization that we could save CPU. Optimize for the most common case. A cardinal rule of optimization. Here, we consciously chose to optimize media loads rather than comment loads. Do the simple thing. “Do the simple thing first” is dogma at Instagram. After identifying the potential problem, we chose the simplest and most obvious course of action. And despite its simplicity, it yielded big results. Empathize. This is oft-repeated at Facebook. We use powerful phones on powerful networks, so this change was personally imperceptible. Yet it still impacted many people. Again, it’s worth noting that the observed improvements on Android outpaced those on iOS. This makes sense — Android phones tend to be cheaper and less powerful. Follow your nose. Here at Instagram NY, we’re chiefly responsible for ranking media (for instance, we personalize and rank media for “Explore Photos”). So a performance optimization like this wasn’t directly related to our work. But our intuition told us that this would be worth pursuing, and one of the best things about working at Instagram is that joining a specific team doesn’t constrain which parts of the code base we can touch. Within reason, we have the latitude to pursue anything we think is worthwhile. Thanks to Lisa Guo, Hao Chen, Tyler Kieft, Jimmy Zhang, Kang Zhang, and William Liu. Stories from the people who build @Instagram 219 Mobile Infra Performance 219 claps 219 Written by Stories from the people who build @Instagram Written by Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-05-02"},
{"website": "Instagram-Engineering", "title": "continuous deployment at instagram", "author": ["Instagram Engineering"], "link": "https://instagram-engineering.com/continuous-deployment-at-instagram-1e18548f01d1", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts At Instagram, we deploy our backend code 30–50 times a day… whenever engineers commit changes to master… with no human involvement in most cases. This may sound crazy, especially at our scale, but it works really well. This post talks about how we implemented this system and got it working smoothly. Continuous deployment has a number of advantages for us: It lets our engineers move really fast. They aren’t limited to a few deployments per day at fixed times; instead, they can get code deployed whenever they want. This means that they waste less time and can iterate on changes very quickly. It makes it much easier to identify bad commits. Instead of having to dig through tens or hundreds of commits to find the cause of a new error, the pool is narrowed down to one, or at most two or three. This is also really useful when you identify a problem later and go back to debug. The metrics or data that indicate the problem can be used to identify an accurate start time, and from there we can find which commits were deployed at that time. Bad commits get detected very quickly and dealt with, which means that we don’t end up with an undeployable mess in master and cause significant delays for other unrelated changes. We’re always in a state where we can get important fixes out quickly. The success of this implementation can largely be attributed to its construction’s iterative approach. Instead of building this system on the side and suddenly switching over, we evolved the current mechanisms until they became continuous deployment. Before continuous deployment, engineers deployed changes on an ad-hoc basis. They’d land changes, and if they wanted them deployed soon, they’d run a rollout. Otherwise they’d wait for another engineer to come along and do so. Engineers were expected to know how to do a small scale test beforehand: they would do a rollout targeting one machine, log into that machine and check the logs, and then run a second rollout targeting the entire fleet. This was all implemented as a Fabric script, and we had a very basic database and UI called “Sauron” which stored a log of rollouts. The first step was adding canarying, which was initially simply scripting what engineers were already expected to do. Instead of running a separate rollout targeting one machine, the script deployed to the canary machine, tailed the logs for user, and asked whether it should continue to the full deploy. Next came some basic analysis of the canary machine: a script collected the HTTP status codes for each request, categorized them, and applied hard-coded percentage thresholds (e.g. less than 0.5% 5xx, at least 90% 2xx). However, this would only warn the user if the thresholds failed. We already had a test suite, but it was only run by engineers on their development machines. Code reviewers had to take the author’s word that the tests passed, and we didn’t know the test status of the resulting commit in master. So we setup Jenkins to run tests on new commits in master and report the result to Sauron. Sauron would keep track of the latest commit which had passed tests, and when doing a rollout this commit would be suggested instead of the latest commit. Facebook uses Phabricator (http://phabricator.org/) for code reviews, and has a Continuous Integration system called Sandcastle which integrates well with Phabricator. We got Sandcastle to run tests whenever a diff was created or updated, and report the result to the diff. To get to automation, we first had to lay some groundwork. We added states to rollouts (running, done, error), and made the script warn if the previous rollout was not in “done” state. We added an abort button in the UI which would change the state to “abort,” and got the script to check the state occasionally and react. We also added full commit tracking; instead of Sauron only knowing the latest commit which had passed tests, it now had a record for every commit in master, and knew the test status of each specific one. Then we automated the remaining decisions which humans needed to make. The first decision was which commit to roll out. The initial algorithm was to always select a commit which had passed tests and select as few commits as possible — never more than three. If every commit had passed tests, it would select one new commit each time, and there could be at most two consecutive commits with non-passing test runs. The second decision was whether the rollout was successful. If more than 1% of hosts failed to deploy, it would be considered failed. At this point, doing a rollout when things were normal simply consisted of answering “yes” a couple times (accepting the suggested commit, starting the canary, and continuing to the full deploy). So we allowed these questions to be answered automatically, and got Jenkins to run the rollout script. At first engineers implementing this only enabled Jenkins when they were at their desks supervising, until they didn’t need to supervise it anymore. While we were doing continuous deployment at this stage, it wasn’t completely smooth yet. There were a couple kinks to work out. Engineers would often land diffs that broke tests, which would cause all subsequent master commits to also fail tests, and thereby prevent anything from being deployed. The oncall would need to notice this, revert the offending commit, wait for tests to pass on the revert, and then manually roll out the entire backlog before the automation could continue. This defeated one of the main advantages of continuous deployment, which was deploying very few commits per rollout. The problem here was that tests were slow and unreliable. We made various optimizations to get tests running in five minutes instead of 12–15 minutes, and fixed the test infrastructure problems that were causing them to be unreliable. Despite these improvements, we still regularly have a backlog of changes that need to be deployed. The most common cause is canary failures (both false and true positives), but there are occasionally other breakages. When the cause was resolved, the automation would pick up and deploy one commit at a time, so it would take a while to clear the backlog and cause significant delays for newly landed diffs. The oncall would usually step in and deploy the entire backlog at once, which defeats one of the main advantages of continuous deployments. To improve this, we implemented backlog handling in the commit selection logic, which made the automation deploy multiple commits when there was a backlog. The algorithm is based on setting a time goal in which to deploy every commit (30min). For each commit in the queue, it calculates the time remaining to meet the goal, the number of rollouts that can be done in that time (using a hard-coded value), and the number of commits that would have to be deployed per rollout. It takes the maximum commits/rollout value, but caps it at three. This allows us to do as many rollouts as possible, while getting every commit out in a reasonable time. One specific cause of backlogs was that rollouts got slower as the size of our infrastructure increased. We got to a point where ssh-agent pegged an entire core authenticating SSH connections, and the fab master process also pegged a core managing all the tasks. The solution here was to switch to Facebook’s distributed SSH system. So what do you need in order to implement something similar to what we’ve done? There are a few key principles which make our system work well, which you can apply to your own. Tests: The test suite needs to be fast. It needs to have decent coverage, but doesn’t necessarily have to be perfect. The tests need to be run often: during code review, before landing (and ideally blocking lands on failure), and after landing. Canary: You need an automated canary to prevent the really bad commits from being deployed to the entire fleet. It doesn’t need to be perfect, however — even a very simple set of stats and thresholds can be good enough. Automate the normal case: You don’t have to automate every situation; just automate the known, normal situations. If anything is abnormal, make the automation stop and let humans step in. Make people comfortable: I think that a big barrier to this kind of automation is when people feel disconnected and out of control. To address this, the system needs to provide good visibility into what it has done, is doing, and (preferably) is about to do. It also needs good stop mechanisms. Expect bad deploys: Bad changes will get out, but that’s okay. You just need to detect this quickly, and be able to roll back quickly. This is something that many other companies can implement. Continuous deployment systems don’t need to be complex. Start with something simple that focuses on the principles above, and refine it from there. This system is working well for us at the moment, but there are further challenges which we will face and improvements we’d like to make. Keeping it fast: Instagram is growing quickly, and as such the commit rate is going to continue to increase. We need to keep the rollout fast in order to maintain very few commits per rollout. One possibility here is to split the rollout into multiple stages and implement pipelining. Adding canarying: As the commit rate increases, canary failures and backlogs are going to impact more and more developers. We want to stop more bad commits from getting into master and blocking deployment, and so we’re implementing canarying as part of Landcastle. After tests pass, Landcastle will test the change with production traffic and fail the land if it doesn’t pass the canary thresholds. More data: We want to improve the canary’s detection capabilities, so we’re planning to collect and check more stats like per-view function response codes. We’re also experimenting with collecting stats from a set of control machines and comparing those to the canary stats, instead of the current static thresholds. Improving detection: It would be good to reduce the impact of bad commits not being caught by the canary. Instead of testing on one machine and then deploying to the entire fleet, we could add more stages in between (a cluster or a region), checking metrics at that level before continuing. Stories from the people who build @Instagram 632 3 Infra Performance 632 claps 632 3 Written by Stories from the people who build @Instagram Written by Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-05-02"},
{"website": "Instagram-Engineering", "title": "building an open source carefree android disk cache", "author": ["Instagram Engineering"], "link": "https://instagram-engineering.com/building-an-open-source-carefree-android-disk-cache-af57aa9b7c7", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts Caching various files on disk has always been an integral part of many mobile apps. At Instagram, we use caching to store and recover images, videos, and text files. As a media-heavy application, the Instagram Android app requires a lightweight but stable disk cache system. When we first built the app, we started with the open source DiskLruCache library. It served us well until we found one major issue with the cache’s design: the cache code’s exception handling logic is cumbersome and prone to developer error. For example, the following code snippet shows how to properly handle a simple write-to-disk operation using DiskLruCache: As you can see, because the DiskLruCache doesn’t support stub instances, when the file storage is not available (either the cache directory not accessible, or there is not enough storage space left), we have no choice but to let the mDiskLruCache fallback to NULL. This seemingly harmless fallback requires all engineers to explicitly check that the cache is not equal to NULL before they ever want to use it. After confirming that the cache is available, the disk caching code also needs to go two extra steps to get to the OutputStream: retrieving the Editor object from the cache entry using the cache key, and then getting the OutputStream from the Editor. Both of these steps might throw IOExceptions, and the retrieving Editor from disk cache could also return NULL. If any of these failed cases ever happens, the engineers need to figure out on their own how to gracefully handle the crash, properly close all the streams/editors/snapshots, and make sure the partial files won’t mess up the cache. If you think this is already complicated, just imagine how complicated it could get when handling two editors in the same code block, or implementing a read-process-write case inside a single method. Missing any one of those NULL checking or mishandling any of the IOExceptions will result in many crashes daily on client devices. Over time, as our app becomes more complex and more engineers joined and worked on the same code base, the disk caching code became extremely flaky and hard to maintain. For over a year, cache-related NPEs (Null Pointer Exception) and IOExceptions topped our crash list. After doing several small patches, we soon figured out these small fixes won’t solve the problem. The fix made the code look even worse, and new crashes kept coming. To fix the issue completely, we knew we had to rethink what a disk cache is, and to redesign the disk cache to make the whole thing easier to use and maintain. A cache, by definition, can always tell the developer “I don’t have this item.” We use this principle to simplify the case that the cache can’t even be opened, or that there are disk errors. We simply report that we don’t have the item, and let writes fail silently. And for cases like IOExceptions, we ideally shouldn’t let the developers guess what’s happening inside the cache, and handle all the possible scenarios. The cache should be smart enough to handle most of the failed cases itself, and guarantee that no incomplete file will be cached and that all cache entries get closed properly. When we decided to build IGDiskCache , we decided to focus on four main changes: 1. Simplify cache initialization and null-checking: Support stub cache instance when the disk cache is not available or accessible, so that we don’t need to check the mDiskLruCache != null every time we want to use it. Handle the IOExceptions smartly, as most of the exception handling logic (e.g. close cache entry, close input/output stream, discard the incomplete file) is reusable and there is no need to make the programmers handle all these edge cases themselves. Flatten the cache, and remove the unnecessary level of Editors/Snapshots. This makes the cache entry’s commit/abort/close logic much cleaner and easier to read. Prevent engineers from mis-using the cache. This includes requiring NULL checking for the cache entries after retrieving them from the cache; and ensuring all the time-consuming tasks (like cache initialization and close) to be executed only on non-UI threads. From initial design to implementation, it took us about a month to build the initial version of the IgDiskCache, and a few more weeks to update all the call sites and test the module thoroughly. After we launched it in production, we were able to dramatically reduce the number of crashes in the app. Also, because of the built-in enhanced checking conditions, IgDiskCache was able to help us identify quite a few race conditions in our apps which were extremely hard to detect otherwise. The UI thread checking also prevents engineers from executing inefficient disk IO operations on the main thread. The code also looks much simpler, and easier to reason about. Our story with IgDiskCache is a good example of how we tackle app reliability issues, and make our code cleaner and easier to maintain. We hope you’ll find it useful too! We recently moved our mobile infrastructure engineering teams (iOS and Android) to New York City. If this blog post got you excited about what we’re doing, we’re hiring — check out our careers page . Jimmy (He) Zhang is a software engineer at Instagram. Stories from the people who build @Instagram 195 2 Android Open Source Tech 195 claps 195 2 Written by Stories from the people who build @Instagram Written by Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-06-24"},
{"website": "Instagram-Engineering", "title": "instagrams neighborhood flavors", "author": ["Instagram Engineering"], "link": "https://instagram-engineering.com/instagrams-neighborhood-flavors-308b41abfdef", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts Any city dweller can point out their city’s hipster neighborhood, tourist neighborhood, and club neighborhood, yet these categories can be hard to quantify without already knowing what you’re looking for. Neighborhoods have distinct institutions, types of stores, and demographics that jointly contribute to feel. But what about less tangible traits? The funky neighborhood? The youthful neighborhood? In this post, we use Instagram hashtags to discover nationwide neighborhood “flavors.” Using a bit of statistical jujitsu, we show that groups of hashtags occur together in specific categories of neighborhoods across the country: the artisanal hipsters (#saison, #upcycle, #chickens), the downtown shoppers (#verawang, #architectureporn, #mensshoes), the foodies (#tapas, #rawbar, #crudo), the families (#thatsmybaby, #runningerrands, #homeworkout), and the bohemian urbanists (#brunchlife, #rooftopview, #livejazz). Then we calculate the flavor-distance between any two neighborhoods, and find similar neighborhoods in different cities. What is the Williamsburg of San Francisco? Why, the Mission. (Maybe you already knew that. But did you know the Williamsburg of Dallas is Casa Linda?) For this study, we consider all U.S. geo-annotated Instagram posts that used hashtags between October 2014 and July 2015. All data were anonymized and analyzed in aggregate; researchers did not analyze any individual’s location or account. We map hashtags to neighborhood boundaries for U.S. cities. These neighborhoods contain 24% of hashtagged, geolocated posts. To avoid spammers, we count the number of unique Instagrammers who used each hashtag in each neighborhood, and only consider hashtags with at least 1,000 unique users in the United States during our time period. To categorize neighborhoods, we want types (our “flavors”) that span many cities but differentiate neighborhoods within them. Among #tbt (“throwback thursday”), #sfpride2015, #libertybell, and #foodtrucks, only the last intuitively seems to have the right mix: it occurs in multiple cities but only in certain neighborhoods within each city. #tbt occurs everywhere, and fails to distinguish one neighborhood from another. #sfpride2015 (common all over San Francisco) and #libertybell (common in certain Philadelphia neighborhoods) are too city-specific. But #foodtrucks, a priori, seems like the type of tag that will occur in many cities, but target only the foodie neighborhoods. Figure 1 maps this intuition. For Philadelphia (top row) and San Francisco (bottom), we map the pointwise mutual information (PMI) of the given hashtags in each neighborhood.[1] This measures the relative prevalence of the hashtag in each neighborhood, using the national rate as a baseline. #tbt carries no city or neighborhood information; it is as prevalent in Philadelphia and San Francisco as in the nation at large, and is evenly distributed in neighborhoods within them. #sfpride2015 occurs disproportionately in San Francisco, but within San Francisco is used in many neighborhoods; it carries a lot of city information, but little neighborhood information. #libertybell carries both; it is limited to only Philadelphia, and within Philadelphia is limited to only specific neighborhoods. #foodtrucks, finally, hits our sweet spot; it occurs in both cities, but disproportionately in specific neighborhoods within them. Information Theory and Flavors We can plot each hashtag on these two dimensions: how much a hashtag occurs in specific states versus how much it occurs in specific neighborhoods.[2] We use information gain, which is a summary measure of how much the distribution of a given hashtag matches the distribution of all hashtags. Hashtags that are much spikier than Instagram usage in general will contain more information. Figure 2 maps the information gain in each dimension for all hashtags with over 1000 unique users. From bottom to top, hashtags increasingly occur in specific states. From left to right, hashtags increasingly occur in specific neighborhoods within a city. The top left tend to be broad hashtags about a city: #sdlife and #iloveny. Similarly, #snowstorm occurs in specific states, but spares no neighborhood within. The top right are specific places in specific cities: #libertybell and #alamo. The bottom left are tags that are used evenly in cities and neighborhoods: #tbt, #barberlife, #brothers. The bottom right is ostensibly what we are looking for: neighborhood-specific hashtags in many cities. Except that maybe it isn’t. The extreme bottom right actually pinpoints specific buildings that occur in many cities: #floorseats (sports arenas), #edsheeran (concert halls), #delay (airports), #cheesecakefactory. #jellyfish posts are usually at the aquarium, not the beach. All of these identify a single neighborhood (building, really). These are probably too specific to be a neighborhood flavor. Instead, our sweet spot is the bottom middle. With most of these hashtags, it’s easy to imagine how they divide each and every city: #singleladies identifies night-life, #growingtoofast residential families with little kids, #bibimbap Korean restaurants. To limit ourselves to “flavorful” hashtags, for the rest of this analysis we use hashtags only in the bottom middle/right of Figure 2: below #growingtoofast on the y-axis and to the right of #pride and #blackbusiness on the x-axis. This includes the building-specific tags in the bottom right; these neighborhoods will simply form their own topics, such as “airports” and “arenas” which we’ll ignore. It’s important to realize that there are many possible sources of neighborhood patterns in hashtags. Some of the hashtags are clearly functions of physical traits of a neighborhood: #foodtrucks and #sorority are created because food trucks and sororities are actually present. Others are functions of demographics and culture: #pride in LGBTQ-friendly neighborhoods and #mommylovesyou in family neighborhoods. Interestingly, #sleeve (these posts are full-arm tattoos, not clothing) carries neighborhood information while #tattoo does not; this suggests sleeves are still part of a geographically confined subculture while tattoos have gone mainstream. Other hashtag patterns also operate through culture, but primarily through word-choice. While these researchers believe that people in all neighborhoods have best friends, only people in very specific neighborhoods have #bestfrands (those neighborhoods include Federal Hill, Baltimore; Brooklyn, Jacksonville; and Roxborough, Philadelphia). Now that we have a set of flavorful hashtags, we want to identify flavors as groups of hashtags that occur together. We fit a topic model [4], treating neighborhoods as documents containing a bag-of-hashtags, with 20 topics. Topic modeling is based on a simple generating process: topics are distributions of hashtags that co-occur. Each neighborhood has its own distribution over the topics; when a hashtag is generated, first the neighborhood picks one of its topics, then it picks a hashtag from within that topic. A neighborhood can (and will) contain multiple topics. The model provides two sets of results: (1) each topic has a vector of proportions of hashtags and (2) each neighborhood has a vector of proportions of topics. We have hand-named the topics given the high-scoring tags. Below, we present wordclouds of selected topics with words sized by the topic PMI, or the log relative usage of the hashtag in the topic versus the hashtag overall. The order of topics is random, and not related to prevalence of the topic in the country. It’s important to note that Instagram users are not a representative set of all members of a neighborhood. When we label neighborhoods by Instagram hashtags, we should keep in mind that this only represents how Instagram users view and use the neighborhood, and not all people in the space. For each topic, I’ve also selected the three neighborhoods with at least 10,000 posts that have the highest score for that topic, limiting to one neighborhood per city. It is undeniable that people like to Instagram about food and alcohol. Especially in these neighborhoods, with #craftcocktails and #trufflefries. We’ve found #foodieheaven. The highest scoring hashtags in this topic are plazas (#treelighting, #iceskating) and offices (#officeview, #powerlunch), but these neighborhoods also draw #architecturelovers and people on #shoppingdays. Top Neighborhoods This topic moves to the residential parts of cities. These tags are generally created at home, and either come from parents of young children or soon-to-be (#gettingbig, #pregoproblems, #happymother), call out family (#sisinlaw, #mimadre), or are selfies at home (#pajamaday, #homeworkout, #bareface). Top Neighborhoods (Only New York’s residential neighborhoods have enough hashtags to make the 10,000 cut). Asian restaurants, and perhaps Asian residents, tend to cluster in similar neighborhoods. With more topics, we would expect the model to differentiate among cultures; here, it identified that broadly, many Asian cultures cluster together. If you want to find the neighborhood with #dimsum, #bibimbap, and #banhmi, this is your flavor. Top Neighborhoods Finally! The artisanal hipsters. The neighborhoods with #distilleries, where people live the #vanlife and practice #glassblowing. Top Neighborhoods Distinct from the artisans above, the bohemian urbanists post pictures of #cityart, live the #brunchlife, and celebrate LGBTQ-friendly #prideweek. While there is certainly overlap between neighborhoods with these tags and neighborhoods with the artisanal ones, the two sets move in different directions often enough to earn their own flavors. Top Neighborhoods This topic combines groups of hashtags under the umbrella of Black Instagram culture. The leading hashtags are Instagram-related (#doubletapp, #summerfollowparty, #cashgaintrain), but other tags celebrate Black hair (#naturalhairstyles, #twists), name historically Black fraternities and sororities (#alphaphialpha, #deltasigmatheta), and advocate to #freebree, referring to Bree Newsome, who scaled the South Carolina State House flag pole to remove the Confederate flag. Top Neighborhoods This topic finds the outdoorsy field trip neighborhoods, mostly dominated by #zoos, but with #picnics and #naturewalks sprinkled in. Top Neighborhoods People in these neighborhoods Instagram about America: presidents #georgewashington and #frankunderwood, memorials for #ww2 and #martinlutherkingjr, and the principles #freedomisntfree and #lovecantwait. #brady and #gronk may be happy to learn that the #newenglandpatriots semantically snuck their way into this group. Top Neighborhoods We can also look at all of the topics present in a given neighborhood. Below, I present the top four topics for a few neighborhoods, and examples of public posts that use one of the topic’s hashtags. Williamsburg, Brooklyn Lake Park, Milwaukee Bouldin Creek, Austin Hyde Park, Chicago Central Harlem, Manhattan Silverlake, Los Angeles Chinatown, Philadelphia We can think of the topic scores as embedding neighborhoods in a 20-dimensional space. With this, we can measure flavor-distance between neighborhoods. We calculate cosine similarity among all neighborhoods, and for selected neighborhoods in Brooklyn, calculate the most similar neighborhoods in Chicago, Dallas, Memphis, Philadelphia, San Francisco, and Seattle. The numbers in parentheses are the cosine similarities, which have a minimum of 0 and a maximum of 1. [1] Treating each hashtag use as an iid observation of the pair (H = h,N = n), with H a categorical variable for the hashtag and N a categorical variable for the neighborhood, we calculate pmi(h,n) = ln(p(H = h|N = n)/p(H = h)). Probabilities were estimated using a Simple Good-Turing algorithm to avoid over-estimating the importance of small-count occurences. [2] We use Information Gain in the state, rather than city, because some geographically-specific hashtags still occurred in multiple cities, for example the Bay Area in San Francisco and the five boroughs of New York. Certainly, hashtags that occur in a single city will also occur in a single state. However, hashtags that occur only many neighboring cities will also occur in only one or two states. [3] We use the definitions InfoGain(S;h) = H(S) — H(S|H=h), and InfoGain(N|C;h) = H(N|C) — H(N|C;H=h), with H(.) standing for the entropy, C a categorical variable over cities, and S a categorical variable over states. [4] We used the Latent Dirichlet Allocation by Blei, Ng, and Jordan (2003) Stories from the people who build @Instagram 18 Data 18 claps 18 Written by Stories from the people who build @Instagram Written by Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-05-02"},
{"website": "Instagram-Engineering", "title": "lessons learned with 3d touch", "author": ["Instagram Engineering"], "link": "https://instagram-engineering.com/lessons-learned-with-3d-touch-73b1cdc03c8b", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts Last week, Apple released the latest iPhone line, starting with the 6s, which adds a new hardware feature called 3D Touch. As part of the screen, there are new sensors that can tell when you are pressing firmly on the screen and measures precise changes in pressure. Along with the new hardware, Apple added three APIs that let developers add another dimension of interaction to their apps. Quick Actions let you select up to four context menu items from your app’s icon. Another feature lets you “Peek” into content and then “Pop” deeper into it. The last API gives developers details about the depth of a UITouch with force and maximumPossibleForce. We had the opportunity to integrate this new technology into Instagram early on and were excited by how natural the shortcuts and peeking on photos and videos felt. The API for adding 3D interactions was seamless to use, and along the way we collected pointers for how to add them to your apps. Quick actions give your users a shortcut to jump to a part of your app by removing any steps in between. For Instagram, we added shortcuts for some of the most commonly used actions in the app: Search, Direct, Activity, and Creation. Adding shortcuts to the home screen is really simple. All you need to do is create an array of UIApplicationShortcutItems and add them to your app’s delegate. Each item has a type string and an optional UIApplicationShortcutIcon. In Instagram, we created UIApplicationShortcutItem categories to generate our shortcuts: Depending on if the user is logged in or logged out, we dynamically configure the shortcut items on the app delegate: Since our architecture already handled actions like app links and push notifications, we ended up using the same routing to handle shortcut actions in -application:performActionForShortcutItem:completionHandler:. Just as you would use a magnifying glass to get a closer look, peeking on content like photos and videos is a way to get a little more info without committing to loading the whole thing. The simple, first choice for Instagram was to add peeking on smaller images and videos. The Peek and Pop API has a delegate called UIViewControllerPreviewingDelegate which is registered to a view. In Instagram, we just register the view of a controller that can receive touches. When a 3D Touch occurs, the delegate decides if there is a peek for whatever item in the view was touched. If the delegate determines that a peek can happen, it is also responsible for two things: set the source rect of the view that is being peeked and return a controller to present. When a 3D touch happens, you are given a context with information about the source view, and the point where the touch occured. Your custom delegates are responsible for mapping the CGPoint to a view, and then to corresponding data that gets peeked. Most of Instagram is built on UICollectionViews and UITableViews. The classes have excellent APIs for turning data into UI and vice-versa. Combining the simple Peek and Pop API with our existing views and infrastructure gave Instagram photo and video peeks with minimal effort! We also added a profile peek for headers or whenever someone is tagged in a comment. The profile-peek delegate works similarly to post-peeks: Find the cell that received a 3D Touch using the location and looking up the NSIndexPath Convert the location to a CGPoint relative to the text view in the cell Get the attributes for the NSAttributedString at the given CGPoint If a username attribute exists, return an IGUserPreviewController The activity feed in Instagram contains both username tags and thumbnails that we wanted to peek. Each of the UIViewControllerPreviewingDelegate objects we built for profiles and media were built specifically for those views. We had to somehow combine the work being done in both delegates. Instead of copy-pasting into a new delegate, we used the composition pattern with our profile and post delegates to create a new object that simply forwards touch events: The controllers returned from a preview delegate are simple UIViewController subclasses with a few unique qualities. For starters, the controllers are completely non-interactive. You can’t tap buttons or add any custom gestures. Instead, you can provide an array of UIPreviewActionItem-conforming objects. In Instagram we just used UIPreviewActions. These are presented and work similar to UIAlertController action items. Throughout Instagram, we only load the data that we need at the time and cache it for later. This saves time waiting for the network and doesn’t consume our user’s bandwidth for data that isn’t used. In some cases, when displaying a profile peek, we don’t have all the data we need in order to show latests photos or follower counts in the peek. Peek view controllers still receiveviewDidLoad, viewWillAppear:, and other UIViewController events that we use to fetch network resources and update the view. Lastly, we found out that changing preferredContentSize on the controller will resize the peek view, even if it has already been presented. Unfortunately, there isn’t a way to animate this property yet. For Instagram, 3D Touch is much more than a 2015 version of the “right click”. The interaction adds another level of depth and carries a different intent. You aren’t yet committed to navigating to the content, but it’s clear that you are interested. With this in mind, Peek and Pop gives you a glimpse of what lies ahead and lets you quickly back out to continue browsing. Quick Actions allow you to jump straight to creating a new post or look at what’s been happening in your Instagram without having to wait for everything to load up first. With our focus on keeping things simple, quick actions make it even easier to use Instagram. Ryan Nystrom is a software engineer at Instagram. Thanks also to Joshua Dickens, designer at Instagram. Stories from the people who build @Instagram 121 iOS Mobile Performance 121 claps 121 Written by Stories from the people who build @Instagram Written by Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-05-02"},
{"website": "Instagram-Engineering", "title": "flexible feature control at instagram", "author": ["Instagram Engineering"], "link": "https://instagram-engineering.com/flexible-feature-control-at-instagram-a7d3417658df", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts At Instagram, we tune and release features every day. Depending on the feature itself, we often need gradual roll out and conditional feature control. Say we want to target a certain age cohort in a particular group of countries, and we want to slowly rollout to 30% of that group over the course of a week to test things out. Engineers can hardcode these “conditions” and wait for deployment, but it has several drawbacks: Not realtime. Deployment needs time. Although at Instagram we have systems to execute smooth and fast rollouts, it still takes some time, not to mention that you have to line up to wait for other developers’ code changes. Not safe. Writing up check conditions might seem easy, but you can also make mistakes easily, and if you do, it could have consequences (like leaking your secret features or failing user requests). And to the first point, there’s no instant fix — you have to wait for another rollout to revert it. Previously, we used a thin abstraction over these requirements that we called “IG Gatekeeper”(not to be confused with Facebook’s Gatekeeper). It basically allowed us to define a whitelist and a release percentage for the feature. It’s super simple and handy, but as we’ve grown, we’ve identified more capabilities we need, like custom geo-targeting. Unfortunately, Gatekeeper wasn’t designed for this and wasn’t easily extensible. We’ve since embarked on several efforts to make an extensible gating system, and in this post we’re going to talk about Gate Logic — our latest gating system that’s currently managing hundreds of features at Instagram. When thinking about a gating system, there are several things we need to consider: Reactivity. We don’t want to wait minutes or hours for the change to take effect. After we decide to save our change, it should be in production within seconds. This also infers we should have the ability to revert changes quickly. Safety. It’s going to a core part of the whole tech stack, so it has to be very safe. It can neither leak features nor break things. Efficiency. It’s going to be used everywhere across the codebase, so we need to make it very fast so that it runs unnoticeably. Flexibility. It should be able to handle future requirements without having to change fundamental parts of the system. Before Gate Logic, engineers would write custom if conditions in their code. To move the testing logic out to an external system, which ensured easy tuning, efficiency and safety. Imagine instead of writing things like: With Gate Logic, the Python side simplifies down to: This piece of the checking logic is a gate . You might be thinking, “How are we supposed to ensure safety if we allow people to write anything in the definition of the logic and send it to production in seconds?” Well, we’re not doing that. To gain the most control of the system, we’re forcing people to write their logic in a DSL (Domain Specific Language). The language looks very similar to the host language (Python), but we added extra sauce. Flexibility: Custom language can be as expressive as we want to. Safety: Python is not type safe, so we’ll do type checking for the logic before you save it. Efficiency: To make it at least as fast as writing the logic in Python, we compile the logic into Python bytecode. To sum it up, we’re embedding a compiled and typed language in Python. We want to have a simple typed language to enable engineers to express their rolling-out condition. One of the simple and natural things we can use is the boolean logical expression, things like (A AND B) OR (C AND D) . It’s easy to implement (code interview question level), but how do we add type support and enough flexibility? If the expression is a tree, and we’re adding type information to every node, the root and result of logical operators are Boolean. Considering the example above, if we obtain the country of the user and compare it with a fixed set of strings, we’re doing X in Y. It’s exactly the same pattern, except the operands and operator are different, and rather than boolean variables and logical operators, we have real values and some arithmetic operators. This gives us a hint on our road to an expression-based language. We append type information on each node and operand, and we force typing rules on each of the operators. For example, the operator in is a binary operator, which takes operands with type T and Set[T] . We have a rule-based type checking system, which is much simpler than the type system in a real strongly typed programming language, but it’s suitable for our case. Here’s a real example to show what the language can give us: It’s worth noting here that while we do allow writing constants at the language level, it’s forbidden on the UX level. All constants must be moved to a section called *Parameters *so that the code is cleaner and more readable. Non-engineers like PMs and sales folks, who will sometimes tune the parameters or add someone to a whitelist, also use the system, so our UI must also be friendly to them. We have a special gate which checks whether the current user is eligible for our internal dogfooding. This logic will be used during almost all new features rollouts, so to save the trouble of writing it in the logic every time (it’s pretty complicated), we added a *reference* functionality to the language. By saying @internal_dogfooding, you’re checking the gate called *internal_dogfooding*, and put the result of that in where you insert the reference. Sound familiar? Yes, function calls! If each gate is a function, you’re calling the other gate/function and embed the result in your future result. This is where “composability” kicks in. When designing a system, we should always try to find the minimal set of elements that can be re-used and composed together to form a much bigger system. Distributed systems are hard. We have thousands of web servers running in production, and our goal is to make them in sync within seconds of when the data is updated. We can’t afford to save in the database and make all the servers query the database every time, and the database doesn’t really have things like listeners to watch for data changes. Fortunately we are distributing a small amount of data so we can use Zookeeper. But remember, we also need to keep the histories for fast rollbacks. So the data is actually stored in two places — all the histories go into databases and are retained there forever, while only the latest versions of gates are distributing to production servers using Zookeeper. And we have a system to project the database table into a Zookeeper object automatically, so that we don’t need to update Zookeeper by hand. So when one of the engineers updates one of the gates, a new revision will be created in the database and projected to Zookeeper at the same time, and Zookeeper will distribute the data into all the production servers in seconds. We have the language, and now it’s time to compile it. We implemented a four-stage textbook-style compiler — Tokenizer, Parser, Type Checker, Code Generation — where every part is a little bit different (simpler) than a usual programming language. In the parser, instead of using some fancy parsing algorithm like Look-Ahead Left-to-right Rightmost Deviation parser (whose name is also fancy), since our grammar is unambiguous, we’re implementing a simple recursive function which looks for parentheses groups and recursively parses sub-expressions. It outputs an syntax tree with only five kinds of syntax nodes. In the code gen phase, we translates the typed syntax tree checked by the type checker to a Python AST, and then call the magic function compile to convert it into a native Python bytecode. Everything looks like it’s going well now. We just convert a custom expression into a Python expression, evaluate it, and put the result in the if statement at runtime. What could go wrong? The answer is a lot! Even though we have compiled bytecode, an easy mistake is to call eval at runtime to run it. Here’s an example, with the logic of (user in $whitelist) AND (user.percentage < 10)with a whitelist of three users. You might be generating the native equivalent of the following Python code: We’ve all been taught eval is evil, and in this case, it’s evil because it’s very slow. An easy view of this problem is, calling eval vs calling a real function at runtime. What’s the difference? Well, function objects are code objects in memory, and the Python interpreter just runs the code object directly, instruction by instruction. While evaling a piece of code, we first need to translate the code string/bytecode into a code object. Since our checking logic is usually simple, this translation time is usually much longer than the running time of the actual logic. I’ll give you a second to think about how to get over this :-) In fact, we can remove this overhead by noticing the difference between run-once and run-always. Ideally, the translation from bytecode string to Python code object is only needed once, but we’re running it every time. So instead of generating an bytecode to eval, we generate a function definition. Now the output code looks like: At runtime, we evaluate/execute this function definition once and call the generated function every time. eval in Python is only for expression, so we have to use exec here. This optimization allows us to run the code 3x faster on average. Inlining is one of the most useful optimization techniques in compilers. In this case, we have a lot of operators that have corresponding native operators. We used to call functions in the operator module for simplicity, but function calls in Python are also pretty slow. The ideal solution is to generate native Python operator calls when possible, which gives us the real native performance and benefits like short-circuit when evaluating boolean expressions. We’ve noticed that when we have whitelists with more than a couple hundred people, and we generate the code like the example above, we’re essentially building the big set every time. The big set is the same, so we can build it once and use that object ongoing. Another place where the redundant work is less obvious, but really a performance killer, is gatelogic.lang.Percentage.from_user. This involves multiple LOAD_ATTR instructions in the bytecode, and it’s really unnecessary. We can factor those out by pulling them up into a higher order function. Then all this building-set and loading-attributes work becomes a single LOAD_DEREF instruction. This gives us a 2x boost and more than 10x boost for the big whitelist. We are trading a bit of memory to save running time. In this post, we covered the basic design of Gate Logic, our feature gating system, and how we implement and optimize the system. With Gate Logic, we’re able to control feature rollout in a safe and flexible way, with no performance loss compared to hard-coded Python, and it has taken the responsibility of rolling-out many Instagram new products, Search and Explore, Web Redesign and more. Moving forward, we’ll be working on extending the system for all kinds of requests from our engineers, integrating with various Facebook experimentation tools, and integrating and building API for the clients. I hope you found this interesting, and see you next time! Chenyang is a software engineer at Instagram. Stories from the people who build @Instagram 50 Infra Performance 50 claps 50 Written by Stories from the people who build @Instagram Written by Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-05-02"},
{"website": "Instagram-Engineering", "title": "scaling the datagram team", "author": ["Instagram Engineering"], "link": "https://instagram-engineering.com/scaling-the-datagram-team-fc67bcf9b721", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts If you’ve been following our recent product launches and posts, you may be curious about how our data infrastructure team functions and how it has grown to support the new products and experiences on Instagram. We operate a very lean team — only 20 engineers supporting Search, Explore, Trending, Account Suggestions, and Data Infrastructure — and have created a unique model that gives engineers end-to-end impact while working cohesively with the product and infrastructure teams. This post is about how we evolved into our team structure and lessons we hope you can also apply to your team as you scale. When we started thinking about how to build out a data team for Instagram in 2013, there were only about 35 engineers working on our mobile apps and backend. I wasn’t one of them, and there was no engineering team dedicated to data infrastructure or products that relied on data processing like ranking and machine learning. I had experience with data infrastructure and product development at Facebook, and saw the chance to close the usual gap between those two worlds in a way that would fit Instagram’s engineering structure. Instead of focusing on infrastructure problems or offering some sort of ranking service to product teams, we decided to innovate and make it more of an end-to-end engineering team, covering both infrastructure and product aspects. Looking back, this was the best decision we could have made because it allowed us to be extremely efficient and take on some big impactful projects. Thus, Datagram, the Instagram Data Team, was born. Our first challenge was to define a good scope for the team that wouldn’t interfere directly with the existing platform-oriented teams at Instagram (iOS, Android, and infrastructure). Luckily the need for data-oriented solutions was a strong point in our favor. With the idea of being end-to-end, we organized the team around the lifecycle of data, demonstrated by the picture below. The first step is the collection of the data from the existing product and systems. That’s mostly infrastructure work like logging frameworks, streaming technologies, database scrapes, and data warehousing. After that comes the processing of the collected data, which includes things like real-time stream processing, data pipelines, ranking and machine learning algorithms. Processed data can then be used to power products like recommendation systems, discovery surfaces, and search. Building the application logic for such products is the last step of our approach and it closes the cycle as it generates more data to be collected. This gave us a very well-defined “horizontal” scope that didn’t limit us to a specific subset of technologies or platforms, and avoided ownership ambiguity with the existing teams. The remaining question was whether to define a “vertical” scope for the team (the specific products we would cover) but we just followed the recipe of the other teams at Instagram and left it open to all existing parts of the product. Being end-to-end and not restricted to a product allows us to be efficient in two ways. First, we only build the systems and the frameworks we actually need for our products. For example, when we were building our account suggestions feature, we only collected the data we needed and only did the ranking and machine learning necessary to build the product. Also, because this was all done by the same team of engineers, we were able to move really fast, with very easy coordination. Second, we can be very efficient in the prioritization and execution of our projects at the company level. If there is an important sprint around a certain product area, it’s simple (and somewhat obvious) to de-prioritize other areas and get more hands on deck for the urgent deadline. Everyone on the team has a broad knowledge of the different products we are involved with, the infrastructure we have built, and their relative priorities. In general, something we learned along the way is that avoiding scopes centered on specific products and technologies makes everyone more open to different ideas and less defensive about existing solutions. When sprint scenarios arise, everyone is actually eager to jump in and help with it, and often bring great ideas with them. The best way to showcase how we work is through examples. Here are a few of the projects we’ve worked on in the last year. As you can read extensively in our previous blog post , we’ve made some significant improvements to our Search product and its infrastructure over the last year, executed by just a handful of engineers. This was only possible because, when we were faced with the challenge of improving search, we were able to address it with a holistic perspective starting from the changes we needed to make on how we collected the data, our indexing infrastructure, and integrating all that efficiently with our ranking algorithms and UI. A year and a half or so ago, the Explore tab would show only popular photos from our community, regardless of your preferences or connections. This experience wasn’t the best and we saw a lot of potential to increase the value and the engagement in that surface. Once more, we looked at the problem from a high level and broke it down into a series of long-term improvements to the product, being careful to make sure each step had some immediate gains as well so we didn’t have an all-or-nothing type of big deliverable. We personalized the photos people see based on their connections, created a surface to show account recommendations, and recently introduced trending places and hashtags . Finally, now that we have all our content indexed in our search infrastructure, we can use it as the source for photos explore content, allowing for better ranking and personalization. As important as suggestions are to activate new accounts and help them connect to their friends and interests, very little was being done before the team started. We extended the basic infrastructure to collect the data we needed to calculate the best recommendations, implemented some basic algorithms (mostly informed heuristics) and slowly introduced advanced ranking and machine learning techniques. As we evolved our suggestion systems, new opportunities surfaced and we evolved it into the idea of account pivots, automatically surfacing related recommendations when you follow a user’s profile. Some of this work pushed us to fundamentally change the way we were fetching our data , otherwise we wouldn’t be able to provide a good user experience in terms of reliability and latency. I doubt this would be in our radar if we weren’t doing everything end-to-end. Besides all the user-facing products we help build, our systems provide all of the data being used by our analytics team to assess the healthy of our growth and engagement. We introduced new logging frameworks and a built new systems to collect online and offline data. Obviously, we don’t want to reinvent the wheel so those things were only created because the differences in our technology stacks prevented us from using existing Facebook solutions. Having said that, we made sure to connect our data collection mechanisms to Facebook’s data warehousing systems (e.g., Hive, Presto), saving us a tremendous amount of work. As the examples above demonstrate, the key to our success so far has been our end-to-end ownership of data problems and their solution. But our secret sauce also includes ruthless prioritization, only hiring people with the right experience to solve the problems we have, and favoring diversity of background so people can teach what they know and learn about the things they don’t know. Growing the team is inevitable no matter how efficient we are and we believe our principles should be able to scale. The main ideas of owning projects end-to-end and not being tied to platforms or temporary initiatives can be used as guidelines in any team or organization. We have started to introduce sub-teams on Datagram and things are going quite well so far with coverage areas divided into broader end-to-end themes like Discovery, Content, and Activation. Sub-teams share technologies and collaborate in multiple projects, which has kept us extremely efficient. Later this summer we are even starting a small Datagram presence in New York with a focus on content ranking across multiple parts of the app. The future is hard to predict, but it looks quite promising! Rodrigo Schmidt manages Instagram’s data infrastructure engineering team. Stories from the people who build @Instagram 28 Data Infra 28 claps 28 Written by Stories from the people who build @Instagram Written by Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-05-02"},
{"website": "Instagram-Engineering", "title": "search architecture", "author": ["Instagram Engineering"], "link": "https://instagram-engineering.com/search-architecture-eeb34a936d3a", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts Instagram is in the fortunate position to be a small company within the infrastructure of a much larger one. When it makes sense, we leverage resources to leapfrog into experiences that have taken Facebook ten years to build. Facebook’s search infrastructure, Unicorn, is a social-graph-aware search engine that has scaled to indexes containing trillions of documents. In early 2015, Instagram migrated all search infrastructure from Elasticsearch into Unicorn. In the same period, we saw a 65% increase in search traffic as a result of both user growth and a 12% jump in the number of people who are using search every time they use Instagram. These gains have come in part from leveraging Unicorn’s ability to rank queries using social features and second-order connections. By indexing every part of the Instagram graph, we powered the ability to search for anything you want — people, places, hashtags, media — faster and more easily as part of the new Search and Explore experience in our 7.0 update. Instagram’s search infrastructure consists of a denormalized store of all entities of interest: hashtags, locations, users and media. In typical search literature these are called documents. Documents are grouped together into sets which can be queried using extremely efficient set operations such as AND, OR and NOT. The results of these operations are efficiently ranked and trimmed to only the most relevant documents for a given query. When an Instagram user enters a search query, our backend encodes it into set operations and then computes a ranked set of the best results. Instagram serves millions of requests per second. Many of these, such as signups, likes, and uploads, modify existing records and append new rows to our master PostgreSQL databases. To maintain the correct set of searchable documents, our search infrastructure needs to be notified of these changes. Furthermore, search typically needs more information than a single row in PostgreSQL — for example, the author’s account vintage is used as a search feature after a photo is uploaded. To solve the problem of denormalization, we introduced a system called Slipstream where events on Instagram are encoded into a large Thrift structure containing more information than typical consumers would use. These events are binary-serialized and sent over an asynchronous pub/sub channel we call the Firehose. Consumers, such as search, subscribe to the Firehose, filter out irrelevant events and react to remaining events. The Firehose is implemented on top of Facebook’s Scribe which makes the messaging process asynchronous. The figure below shows the architecture: Since Thrift is schematized, we re-use objects across requests and have consumers consume messages without the need for custom deserializers. A subset of our Slipstream schema, corresponding to a photo like is shown below: Firehose messages are treated as best-effort and a small percentage of data loss is expected in messaging. We establish eventual consistency in search by a process of reconciliation or a base build. Each night, we scrape a snapshot of all Instagram PostgreSQL databases to Hive for data archiving. Periodically, we query these Hive tables and construct all appropriate documents for each search vertical. The base build is merged against data derived from Slipstream to allow our systems to be eventually consistent even in the event of data loss. Assuming that we have ingested our data correctly, our search infrastructure enables an efficient path to extracting relevant documents given a constraint. We call this constraint a query, which is typically a derived form of user-supplied text (e.g. “Justin” with the intent of searching for Justin Bieber). Behind the scenes, queries to Unicorn are rewritten into S-Expressions that express clear intent, for example: which translates to “people named maxime followed by people I follow”. Our search infrastructure proceeds in two (intermixed) steps: Candidate generation: finding a set of documents that match a given query. Our backend dives into a structure called a reverse index, which finds sets of document ids indexed by a term. For example, we may find the set of users with the name “justin” in the “name:justin” term. Ranking: choosing the best documents from all the candidates. After getting candidate documents, we look up features which encode metadata about a document. For example, one feature for the user justinbieber would be his number of followers (32.3MM). These features are used to compute a “goodness” score, which is used to order the candidates. The “goodness” score can be either machine learned or hand-tuned — in the machine learning case, we may engineer features that discriminate for clicks or follows to a given candidate. The result of the two steps is an ordered list of the best documents for a given query. As part of our search improvements, Instagram now takes into account who you follow and who they follow in order to provide a more personalized set of results. This means that it is easier for you to find someone based on the people you follow. Using Unicorn allowed us to index all the accounts, media, hashtags and places on Instagram and the various relationships between these entities. For example, by indexing a user’s followers, Unicorn can provide answers to questions such as: “Which accounts does User X follow and are also followed by user Y” Equally, by indexing the locations tagged in media Unicorn can provide responses for questions such as: “Media taken in New York City from accounts I follow” While utilizing the Instagram graph alone may provide signals that improve the search experience, it may not be sufficient to find the account you are looking for. The search ranking infrastructure of Unicorn had to be adapted to work well on Instagram. One way we did this was to model existing connections within Instagram. On Facebook, the basic relationship between accounts is non-directional (friending is always reciprocal). On Instagram, people can follow each other without having to follow back. Our team had to adapt the search ranking algorithms used to store and retrieve account to Instagram’s follow graph. For Instagram, accounts are retrieved from unicorn by going through different mixes of: “people followed by people you follow” and “People followed by people who follow you” In addition, on Instagram, people can follow each other for various reasons. It doesn’t necessarily mean that a user has the same amount of interest in all the accounts they follow. Our team built a model to rank the accounts followed by each user. This allows us to prioritize showing people followed by people that are more important to the searcher. Sometimes, the best answer for a search query can be a hashtag or a place. In the previous search experience, Instagram users had to explicitly choose between searching for accounts or hashtags. We made it easier to search for hashtags and places by removing the necessity to select between the different types of results. Instead, we built a ranking framework that allows us to predict which type of results we think the user is looking for. We found in tests that blending hashtags with accounts was such a better experience that clicks on hashtags went up by more than 20%! This increase fortunately didn’t come at the cost of significantly impacting account search. Our classifiers are both personalized and machine-learned on the logs of searches that users are doing on Instagram. The query logs are aggregated per country to determine if a given search term such as “#tbt” would most likely result in a hashtag search or an account search. Those signals are combined with other signals, such as past searches by a given user and the quality of the results available to show, in order to produce a final blended list of results. Instagram’s search infrastructure is used to power discovery features far away from user-input search. Our largest search vertical, media, contains the billions of posts on Instagram indexed by the trillions of likes. Unlike our other tiers, media search is purely infrastructure — users never enter any explicit media search queries in the app. Instead, we use it to power features that display media: explore, hashtags, locations and our newly launched editorial clusters. Lacking an explicit query, we get creative with our media reverse index terms to enable slicing along different axes. The table below shows a list of some term types currently supported in our media index: Within each posting list, our media is ordered (“statically ranked”) reverse-chronologically to encourage a strong recency bias for results. For example, we can serve the Instagram’s profile page for @thomas with a single query: (term owner:181861901). Extending to hashtags, we can serve recent media from #hyperlapse through (term hashtag:#hyperlapse). Composing Unicorn’s operators enable us to find @thomas’ Hyperlapses, by issuing (and hashtag:#hyperlapse owner:181861901). Many of terms exist to encourage diversity in our search results. For example, we may be interested in making sure that some #hyperlapse candidates are posted by verified accounts. Through the use of Unicorn’s WEAK AND operator we can guarantee that at least 30% of candidates come from verified accounts: We exploit diversity to serve better content in the “top” sections of hashtags and locations. Although postings lists are ordered chronologically we often want to surface the top media for a given query (hashtag, location, etc.). After candidate generation, we go through a process of ranking which chooses the best media by assigning a score to each document. The scoring function consumes a list of features and outputs a score representing the “goodness” of a given document for our query. Visual: features that look at the visual content of the image itself. Concretely, we run each of Instagram’s photo through a deep neural net (DNN) image classifier in an attempt to categorize the content of the photo. Afterwards, we perform face detection in order to determine the number and size each of the faces in the photo. Post metadata: features that look at non-visual content of a given post. Many Instagram posts contain captions, location tags, hashtags and/or mentions which aid in determining search relevancy. For example, the FEATURE_IG_MEDIA_IS_LOCATION_TAGGED is an indicator feature determining whether a post contains a location tag. Author: features that look at the person who made a given post. Some of the richest information about a post is determined by the person that made it. For example, FEATURE_IG_MEDIA_AUTHOR_VERIFIED is an indicator feature determining whether the author of a post is verified. Features in our index can be divided broadly into three categories: Depending on the use case, we tune features weights differently. On the “top” section of location pages we may wish to differentiate between photos of a location and photos in a location and down-rank photos containing large faces. Instagram uses a per-query-type ranking model that allows for modeling choices appropriate to a particular app view. Our media search infrastructure also extends itself into discovery, where we serve interesting content that users aren’t explicitly looking for. Instagram’s Explore Posts feature showcases interesting content from people near to you in the Instagram graph. Concretely, one source of explore candidates “photos liked by people whose photos you have liked”. We can can encode this into a single unicorn query with: This proceeds inwards-outwards by: liker:<userid> : posts that you’ve liked (extract owner:…) : the owner of those posts (apply liker:..) : media liked by those owners After this query generates candidates, we are able to leverage our existing ranking infrastructure to determine the top posts for you. Unlike top posts on hashtag and location pages, the scoring function for explore is machine-learned instead of hand tuned. This project wouldn’t be possible without the contributions of Tom Jackson, Peter DeVries, Weiyi Liu, Lucas Ou-Yang, Felipe Sodre da Silva and Manoli Liodakis Stories from the people who build @Instagram 520 2 Web Infra Performance 520 claps 520 2 Written by Stories from the people who build @Instagram Written by Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-05-02"},
{"website": "Instagram-Engineering", "title": "trending on instagram", "author": ["Instagram Engineering"], "link": "https://instagram-engineering.com/trending-on-instagram-b749450e6d93", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts With last week’s Search and Explore launch, we introduced the ability to easily find interesting moments on Instagram as they happen in the world. The trending hashtags and places you see in Explore surface some of the best, most popular content from across the community, and pull from places and accounts you might not have seen otherwise. Building a system that can parse over 70m new photos each day from over 200m people was a challenge. Here’s a look at how we approached identifying, ranking, and presenting the best trending content on Instagram. Intuitively, a trending hashtag should be one that is being used more than usual, as a result of something specific that is happening in that moment. For example, people don’t usually post about the Aurora Borealis, but on the day we launched, a significant group of people was sharing amazing photos using the hashtag #northernlights. You can see how usage of that hashtag increases over time in the graph below. And as we write this blog post, #equality is the top trending hashtag on Instagram. Similarly, a place is trending whenever there is an unusual number of people who share photos or videos taken at that place in a given moment. The U.S. Supreme Court is trending, as there are hundreds of people physically there sharing their support on the recent decision in favor of same-sex marriage. Given examples such as the above, we identified three main elements to a good trend: Popularity — the trend should be of interest to many people in our community. Novelty — the trend should be about something new. People were not posting about it before, or at least not with the same intensity. Timeliness — the trend should surface on Instagram while the real event is taking place. Overall, we want major events like #nbafinals or #nyfashionweek to trend, together with the places they happen, as soon as they start and as long as the community is interested in them. In this post we discuss the algorithms we use and the system we built to identify trends, rank them and show them in the app. Identifying a trend requires us to quantify how different the currently observed activity (number of shared photos and videos) is compared to an estimate of what the expected activity is. Generally speaking, if the observed activity is considerably higher than the expected activity, then we can determine that this is something that is trending, and we can rank trends by their difference from the expected values. Let’s go back to our #northernlights example in the beginning of this post. Regularly, we observe only a few photos and videos using that hashtag on an hourly basis. For #northernlights, starting at 07:00am (Pacific time), thousands of people shared content using that hashtag. That means that activity in #northernlights is well above what we expect. Conversely, more than 100k photos and videos are tagged with #love every day, as it is a very popular hashtag. Even if we observe 10k extra posts today, it won’t be enough to exceed our expectations given its historical counts. For each hashtag and place, we store counters of how many pieces of media were shared using the hashtag or place in a 5-minute window over the past 7 days. For simplicity, let us focus on hashtags for now, and let’s assume that C(h, t) is the counter for hashtag h at time t (i.e., it is the number of posts that were tagged with this hashtag since time t-5min till time t ). Since this count varies a lot between different hashtags and over time, we normalize it and compute the probability P(h, t) of observing the hashtag h at time t . Given the historical counters of a hashtag (aka the time series), we can build a model that predicts the expected number of observations: C’(h, t) , and similarly, compute the expected probability P’(h, t) . Given these two values for each hashtag, a common measure for the difference between probabilities is the KL divergence , which in our case is computed as: S(h, t) = P(h, t) * ln(P(h, t)/P’(h, t)) What this does is essentially consider both the currently observed popularity , which is captured by P(h, t) , and the novelty , computed as the ratio between our current observations and the expected baseline, P(h, t)/P’(h, t) . The natural log (ln) function is used to smooth the “strength” of novelty and make it comparable to the popularity. The timeliness role is played by the t parameter, and by looking at the counters in the most recent time windows, trends will be picked up in real-time. How do you compute the expected baseline probability given past observations? There are several facets that can influence the accuracy of the estimate and the time-and-space complexity of the computation. Usually those things don’t get along too well — the more accurate you want to be, the more time-and-space complexity the algorithm requires. So we had to be careful when considering the number of samples (how far back we look), the granularity of sample (do we really need counts every five minutes?) and how fancy the algorithm should be. We experimented with a few different alternatives, like simply taking the count of the same hour last week, and also regression models up to crazy all-knowing neural networks. Turns out that while fancy things tend to have better accuracy, simple things work out well, so we ended up with selecting the maximal probability over the past week’s worth of measurements. Why is this good? Very easy to compute and relatively low memory demand. Quite aggressive about suppressing non-trends with high variance. Quickly identifies emerging trends. There are two things we over-simplified in this explanation, so let us refine our model a bit more. First, while some hashtags are extremely popular and have lots of media, most of the hashtags are not popular, and the 5-minute counters are extremely low or zero. Thus, we keep an hourly granularity for older counts, as we don’t need a 5-minute resolution when computing the baseline probability. We also look at a few hours worth of data so that we can minimize the “noise” caused by random usage spikes. We noted there is a trade-off between the need to get sufficient data and how quickly we can detect trends — the longer the time frame is, the more data we have, but the slower it will be to identify a trend. Second, if the predicted baseline P(h, t) is still zero, even after accumulating a few hours for each hashtag, we will not be able to compute the KL divergence measure (division by zero). Hence we apply smoothing, or put more simply: If we didn’t see any media for a given hashtag in the past, we mark it as if we saw three posts in that timeframe. Why *three* exactly? That allows us to save a large amount of memory (>90%) while storing the counters, as the majority of hashtags do not get more than three posts per hour, so we can simply drop the counters for all of those and assume every hashtag starts with at least three posts per hour. Even with all that memory savings, there is a lot of data to store. Instagram sees millions of hashtags per day, and several tens of thousands of places. We want the trending backend to respond quickly and scale well with our growth, so we designed a sharded stream processing architecture that we explain later. The next step is to rank the hashtags based on their “trendiness,” which we do by aggregating all the candidate hashtags for a given country/language (the product is enabled in the USA for now) and sorting them according to their KL divergence score, *S(h, t)*. We then impose a lower-bound so that we drop all candidates below a certain score. This way we get rid of non-interesting trends and create a smaller list of interesting trends. We noticed that some trends tend to disappear faster than the interest around them. For instance, the amount of posts using a hashtag that is trending at the moment will naturally decrease as soon as the event is finished. Therefore, its KL score will quickly decrease, then the hashtag won’t be trending anymore, even though people usually like to see photos and videos from the underlying event of a trend for a few hours after it is over. In order to overcome those issues, we use an exponential decay function to define the time-to-live for previous trends, or how long we want to keep them for. We keep track of the maximal KL score for each trend, say SM(h) , and the time tmax where S(h, tmax) = SM(h) . Then, we also compute the exponential decayed value for SM(h) for each candidate hashtag at the moment so that we can blend it with their most recent KL scores. Sd(h, t) = SM(h) * (1/2)^((t — tmax)/half-life) We set the decay parameter *half-life* to be two hours, meaning that SM(h) is halved every two hours. This way, if a hashtag or a place was a big trend a few hours ago, it may still show up as trending alongside the most recent trends. People tend to use a range of different hashtags to describe the same event, and when the event is popular, multiple hashtags that describe the same event might all be trending. Since showing multiple hashtags that describe the same event can end up being an annoying user experience, we group together hashtags that are “conceptually” the same. For example, the figure illustrates all the hashtags that were used in media together with #equality. It shows that #equality was trending together with related hashtags, such as #lovewins, #love, #pride, #lgbt and many others. By grouping these tags together, we can show #equality as the trend, and save the need to sift through all the different tags until reaching other interesting trends. To do this, there are two important tasks that need to be achieved — first, understand which hashtags are talking about the same thing, and second, find the hashtag that is the best representative of the group. In slightly more “scientific” terms, we need to capture the affinities between hashtags, cluster them based on these affinities, and find an exemplar point that describes each cluster. There are two key challenges here — first we need to capture some notion of similarity between hashtags, and then we need to cluster them in an “unsupervised” way, meaning that we have no idea how many clusters there should be at any given time. We use the following notion of similarity between hashtags: Cooccurrences — hashtags that tend to be used together (cooccur on the same piece of media) — for example #fashionweek, #dress, #model. Cooccurrences are computed by looking at recent media and counting the number of times each hashtag appears together with other hashtags. Edit distance — different spellings (or typos) of the same hashtag — for example #valentineday, #valentinesday — these tend not to cooccur because people rarely use both together. Spelling variations are taken care of using Levenshtein distance . Topic distribution — hashtags that describe the same event — for example #gocavs, #gowarriors — these have different spelling and they are not likely to cooccur. We look at the captions used with these hashtags and run an internal tool that classifies them into a predefined set of topics. For each hashtag we look at the topic distribution (collected from all the media captions in which it appeared) and normalize it using TF-IDF . Our hashtag grouping process computes the different similarity metrics for each pair of trending hashtags, and then decides which two are similar enough to be considered the same. During the merging process, clusters of tags emerge, and often these clusters will also be merged if they are sufficiently similar. Now that we have talked about the entire process for identifying trends at Instagram, let us take a look at how the backend was implemented in order to incorporate each component described above. Our trending backend is designed as a stream processing application with four nodes, which are connected in a linear structure like an assembly line, see the top part of this diagram: Nodes consumes and produces a stream of “log” lines. The entry point receives a stream of media creation events, and the last node outputs a ranked list of trending items (hashtags or places). Each node has a specific role as follows: pre-processor — the original media creation events holds metadata about the content and its creator, and in the pre-processing phase we fetch and attach to it all the data that is needed in order to apply quality filters in the next step. parser — extracts the hashtags used in a photo or video so that each one of them becomes a separate line in the output stream. This is where most of the quality filters are applied, so if a post violates any of them, it is simply ignored. scorer — stores time-aggregated counters for each hashtag. The counters are all kept in memory, and they are also persisted into a database so that they can be recovered in case the process dies. This is also where our scoring function S(h, t) is computed. Every five minutes, a line is emitted that contains each hashtag and its current value for S(h, t) . ranker — aggregates all the candidate hashtags and their trending scores. Each hashtag has actually two possible scores: their current values and their exponential decayed values based on their historical maximum, Sd(h, t) as described above. This stream-lined architecture lets us partition the universe of hashtags so that there can be many instances of each node running in parallel , each one of them taking care of a different partition. That is desirable as the scorer is memory-bound and the amount of data may be too big to fit into a single machine. Besides, failures are isolated to specific partitions, so if one instance is not available at the moment, trending would not be entirely compromised. Besides, We need to make sure the scorer’s input is partitioned properly so that all streamed log lines for a given hashtag are processed by the same exact instance, otherwise the counters for one hashtag would be split among several instances. We wanted to serve requests for trending hashtags and places without imposing a considerably high load on our trending backend. Therefore, trends are served out of a read-through caching layer powered by memcached , and a Postgres database in case there is a cache miss, see the bottom part of the diagram above. Trends are periodically pulled from the ranker , then we run our algorithm to group similar trends together and the results are stored in Postgres, that’s how new trending hashtags and places are flushed into the app. Stories from the people who build @Instagram 535 5 Data Infra 535 claps 535 5 Written by Stories from the people who build @Instagram Written by Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-05-02"},
{"website": "Instagram-Engineering", "title": "under the hood instagram in 2015", "author": ["Instagram Engineering"], "link": "https://instagram-engineering.com/under-the-hood-instagram-in-2015-8e8aff5ab7c2", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts 2015 was a big year for Instagram — between reimagining the discovery experience with Search and Explore and trending content, launching new stand-alone creative apps, rebuilding Direct, and scaling our infrastructure across data centers, a lot was happening under the hood. As the year winds down, we wanted to recap some of our biggest product, design, and engineering moments from the year, what we learned, and what we imagine for the next run around the sun. We’ve made a lot of progress on creative tools, discovery, and communication this year, and as we look to 2016, we’re excited to apply what we’ve learned. When Instagram was built in 2010, phone cameras weren’t great, and filters were built to make sure you could share beautiful photos you were proud of. Since then, our cameras have improved, but the desire for creative control and personalization has grown and expanded past anything we initially conceived. Here’s a look at some of our biggest creative tools moments this year. In March, we launched Layout, our second standalone app. Like with all of our features, the inspiration for Layout came from how the community was using Instagram. We were increasingly seeing that people wanted flexibility — they were overlaying, mirroring, resizing, and flipping duplicates of the same photo or multiple photos to create new looks. In fact, one in five of our monthly active users were sharing images that combined multiple photos. We decided to zero in on this behavior and see if we could make it a simple, easy, and native experience within Instagram. Working with our UEX team, we built a set of features that responded directly to user research and feedback. For example, the majority of collage apps require you to select your format first, and your photos second. This kind of felt like having to choose your filter before taking your photo, so we decided to show you photos first, and then offer different Layout previews. We also created a “Faces” tab because we saw that the vast majority of collage-type photos on Instagram were of people, and wanted to make the photos with people easily accessible. The Instagram square is iconic — it’s simple, aesthetically pleasing, and part of our DNA. But over the past few years, the creativity from our community has inspired us and opened our eyes to what’s possible when you experiment with image formats. As we watched our feeds, it looked like portrait and landscape formats were becoming increasingly popular — and the data backed it up. We found that one in five impressions in feed weren’t squares, and we knew we had to take the leap to enable the flexibility and creative freedom to share any moment, in any format. Boomerang Boomerang was our second creative app of the year, and started as a hackathon project this summer with a designer and small group of Instagram’s Android engineers. It takes a burst of photos and stitches them together into a high-quality mini video that plays forward and back, helping you turn everyday moments into something fun and unexpected. We’ve been inspired by the creativity of the video community on Instagram over the past two years and with looping videos and Hyperlapse, people are experimenting with motion in new and exciting ways. Boomerang unlocks additional ways to experiment with motion. Boomerang was also the first app we’ve been able to launch on iOS and Android at the same time. Boomerang on iPhone uses the same technology that made Hyperlapse possible. When recording on the phone, we store camera rotation data alongside the video. At playback time, we align the rotations to the video frames in real-time and create a synthetic camera that has a cinematic feel. With over 400 million people using Instagram every month, and uploading an average of 80M photos a day, the amount of content on Instagram is constantly growing and evolving. Before this year, there wasn’t always a consistent way to find and parse this content, and we wanted to create a way to surface the best and most relevant content in a personalized way. Early this year we launched search, hashtag and location pages on web, and later launched a reimagined Search and Explore with trending. Now, we’re experimenting with live curations around major events like Halloween and New Years. We’re just getting started, and as we move into next year, discovery will continue to be a core focus. But first, let’s take a deeper look at what we launched this year. Instagram’s Datagram team builds the systems that power Search, Explore, Trending, Ranking, and Account Suggestions. The team works end-to-end, organized around the lifecycle of data. This year, the datagram team built Instagram’s search infrastructure and trending system that can parse over 70m new photos each day from over 200m people. In early 2015 we migrated our search infrastructure to Facebook’s unicorn, creating a 65% increase in search traffic as a result of both user growth and a 12% jump in the number of people who are using search every time they use Instagram. This move, which allowed us to rank queries using social features and second-order connections, made it possible for us to index every part of the Instagram graph and power the ability to search for anything you want — people, places, hashtags, media — faster and more easily in Search and Explore. And in order to show all this content in a timely, relevant way, we also built a system to define, identify, and rank trends.For example, people don’t usually post about the Aurora Borealis, but on the day we launched, a significant group of people was sharing amazing photos using the hashtag #northernlights. We also built a way to group trends so that you’re not seeing duplicates. For example, the figure below illustrates all the hashtags that were used in media together with #equality. It shows that #equality was trending together with related hashtags, such as #lovewins, #love, #pride, #lgbt and many others. By grouping these tags together, we can show #equality as the trend, and save the need to sift through all the different tags until reaching other interesting trends. Read more about our search infrastructure here and ranking here . This year Apple released the iPhone 6s, which added a new hardware feature called 3D Touch. We were able to integrate this new technology into Instagram early on and were excited by how natural the shortcuts and peeking on photos and videos felt. For example, you might use 3D Touch when you are interested in a piece of content but aren’t committed to navigating there yet. With this in mind, Peek and Pop gives you a glimpse of what lies ahead and lets you quickly back out to continue browsing. And Quick Actions allow you to jump straight to creating a new post or look at what’s been happening in your Instagram without having to wait for everything to load up first. These APIs were seamless to use, and helped us create new ways for people to find and interact with content. Read more about what we learned from building 3D Touch here . When we’re building features for Instagram, we prioritize simplicity, utility, and performance. Until recently, we kept all photos at a 640x640 resolution to ensure consistent quality, but now that there is such a variety of device resolutions, it didn’t make sense to limit resolution anymore. This summer we improved the resolution of photos on Instagram, keeping all images between 320 and 1080 at their original resolution, upscaling small images to 320x320, or downscaling large images to 1080x1080 as necessary. We revisited our production and distribution pipeline to optimize for quality and payload so that the vast majority of images will now look as good or better, while being as small as possible (and most often smaller), at all resolutions! We were able to do this thanks to progressive jpeg, which was designed to allow people see “something” before an image is fully received. Now every time we find a new way to surface content, we can make sure it looks its best. Look out for more information on photo resolution on the engineering blog in the coming months. Part of Instagram’s mission is to give people a visual voice — as phone cameras become better and more ubiquitous, images are emerging as a new form of language. We want to make sure the community has the freedom to express this voice however they want — whether it’s publicly or 1:1. We also want to make sure that people are able to communicate whenever or wherever they are. This year was big for communication on Instagram — we launched a new version of Direct, built an app on the Apple Watch, and made major improvements to our infrastructure to ensure people can connect reliably. We launched Instagram Direct in 2013 as a way to share moments with one person or a smaller group. Since then, tens of millions of people were using it to connect to their community, but wanted to be able to keep conversations going, and more easily share what they were finding on Instagram. This year, we completely rebuilt Direct’s infrastructure to make it easier to start and have visual conversations, and support threaded messages, more fully-featured groups, and sharing posts from feed. When we launched, instead of releasing to a percentage of users, one country, or everyone at once, we decided to do a unique rollout that would utilize the network effect of messaging and allow people to have the feature as soon as they heard about it. We started with a few countries, and anyone those people messaged had the new version of Direct turned on instantly. When Apple launched the Watch, we were excited to try out a new platform and reimagine Instagram for a quick, on-the-go experience. Instagram’s existing iPhone app wasn’t exactly transferable — the Watch didn’t have a camera, and wasn’t suited for an infinite scrolling feed. So we decided to focus in on immediacy and personalization — how could we alert you to the most important moments posted by the people you care most about? We built notifications that allowed you to see when your friends posted photos, and quick like and emoji responses so you can participate in their moment quickly and easily. In 2013, we had 200 million monthly active users and it was clear that we needed to make some infrastructure changes to continue to support our growing community. At that point we began “Instagration” — moving our infrastructure from AWS to Facebook’s servers. This year, with 400 million monthly active users, we decided to scale our infrastructure geographically to three data centers to make sure the community continues to have a reliable experience no matter where they are. Just a few months later, we put it to the test. Facebook regularly tests its data centers by shutting them down during peak hours, and a couple months ago, right as we had finished migrated our data to a new data center, Facebook ran a test and took it down. And we passed the test without users noticing! To learn more about how our data center migration has helped give us more flexible capacity planning and acquisition, higher reliability, and better preparedness for disasters here . Over the last year we’ve made a step change in the quality of how and when you discover what’s happening, improved how the community connects and communicates, and opened up new ways for people to show what’s happening in their world and share their visual voices. In 2016 we’ll continue improving these experiences and features, as well as create some brand new experiences that we hope will inspire and connect our community (and be fun to build). Happy New Year! Stories from the people who build @Instagram 57 Infra Mobile Design Tools 57 claps 57 Written by Stories from the people who build @Instagram Written by Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-05-02"},
{"website": "Instagram-Engineering", "title": "instagration pt 2 scaling our infrastructure to multiple data centers", "author": ["Instagram Engineering"], "link": "https://instagram-engineering.com/instagration-pt-2-scaling-our-infrastructure-to-multiple-data-centers-5745cbad7834", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts In 2013, about a year after we joined Facebook, 200m people were using Instagram every month and we were storing 20b photos. With no slow-down in sight, we began Instagration — our move from AWS servers to Facebook ’s infrastructure. Two years later, Instagram has grown to be a community of 400 million monthly active users with 40b photos and videos, serving over a million requests per second. To keep supporting this growth, and to make sure the community has a reliable experience on Instagram, we decided to scale our infrastructure geographically. In this post, we’ll talk about why we expanded our infrastructure from one to three data centers and some of the technical challenges we met along the way. Mike Krieger, Instagram’s co-founder and CTO, recently wrote a post that included a story about the time in 2012 when a huge storm in Virginia brought down nearly half of our instances. The small team spent the next 36 hours rebuilding almost all of our infrastructure, which was an experience they never wanted to repeat. Natural disasters like this have the potential to temporarily or permanently damage a data center — and we need to make sure to sustain the loss with minimal impact on user experience. Other motivations for scaling out geographically include: Resilience to regional issues: More common than natural disasters are network disconnects, power issues, etc. For example, soon after we expanded our services to Oregon, one of the racks containing memcache and async tier servers was powered off, which caused large exceptions to user requests. With our new infrastructure in place, we were able to shift traffic away from the region to mitigate the issue while we recovered from the power failure. Flexible capacity expansion: Facebook has a few data centers. It is much easier to expand Instagram’s capacity where it is available when our infrastructure is ready to expand beyond one region and even when there is considerable delay in network latency. It helps to make quick decisions on getting new features ready for users without having to scramble for infrastructure resources to support them. So how did we start spreading things out? First let’s take a look at Instagram’s overall infrastructure stack. The key to expanding to multiple data centers is to distinguish global data and local data. Global data needs to be replicated across data centers, while local data can be different for each region (for example, the async jobs created by web server would only be viewed in that region). The next consideration is hardware resources. These can be roughly divided into three types: storage, computing and caching. Instagram mainly uses two backend database systems: PostgreSQL and Cassandra. Both PostgreSQL and Cassandra have mature replication frameworks that work well as a globally consistent data store. Global data neatly maps to data stored in these servers. The goal is to have eventual consistency of these data across data centers, but with potential delay. Because there are vastly more read than write operations, having read replica each region avoids cross data center reads from web servers. Writing to PostgreSQL, however, still goes across data centers because they always write to the primary. Web servers, async servers are both easily distributed computing resources that are stateless, and only need to access data locally. Web servers can create async jobs that are queued by async message brokers, and then consumed by async servers, all in the same region. The cache layer is the web servers’ most frequently accessed tier, and they need to be collocated within a data center to avoid user request latency. This means that updates to cache in one data center are not reflected in another data center, therefore creating a challenge for moving to multiple data centers. Imagine a user commented on your newly posted photo. In the one data center case, the web server that served the request can just update the cache with the new comment. A follower will see the new comment from the same cache. In the multi data center scenario, however, if the commenter and the follower are served in different regions, the follower’s regional cache will not be updated and the user will not see the comment. Our solution is to use PgQ and enhance it to insert cache invalidation events to the databases that are being modified. On the primary side: Web server inserts a comment to PostgreSQL DB; Web server inserts a cache invalidation entry to the same DB. On the replica side: Replicate primary DB, including both the newly inserted comment as well as the cache invalidation entry Cache invalidation process reads the cache invalidation entry and invalidates regional cache Djangos will read from DB with the newly inserted comment and refill the cache This solves the cache consistency issue. On the other hand, compared to the one-region case where django servers directly update cache without re-reading from DB, this would create increased read load on databases. In order to mitigate this problem, we took two approaches: 1) reduce computational resources needed for each read by denormalizing counters; 2) reduce number of reads by using cache leases. The most commonly cached keys are counters. For example, we would use a counter to determine the number of people who liked a specific post from Justin Bieber. When there was just one region, we would update the memcache counters by incrementing from web servers, therefore avoiding a “select count(*)” call to the database, which would take hundreds of milliseconds. But with two regions and PgQ invalidation, each new like creates a cache invalidation event to the counter. This will create a lot of “select count(*)”, especially on hot objects. To reduce the resources needed for each of these operations, we denormalized the counter for likes on the post. Whenever a new like comes in, the count is increased in the database. Therefore, each read of the count will just be a simple “select” which is a lot more efficient. There is also an added benefit of denormalizing counters in the same database where the liker to the post is stored. Both updates can be included in one transaction, making the updates atomic and consistent all the time. Whereas before the change, the counter in cache could be inconsistent with what was stored in the database due to timeout, retries etc. In the above example of a new post from Justin Bieber, during the first few minutes of the post, both the viewing of the new post and likes for the post spikes. With each new like, the counter is deleted from cache. It is very common that multiple web servers would try to retrieve the same counter from cache, but it will have a “cache miss”. If they all go to the database server for retrieval, it would create a thundering herd problem. We used memcache lease mechanism to solve this problem. It works likes this: Web server issues a “lease get”, not the normal “get” to memcache server. Memcache server returns the value if it’s a hit. In this case, it is no different than a normal “get.” If the memcache server does not find the key, it returns a “first miss” to just one web server within *n* seconds; any other “lease get” requests during that time will get a “hot miss.” In the case of “hot miss”where the key had been deleted from cache *recently,* it would return stale value. If the cache key is not filled within *n* seconds, it again issues a “first miss” to a “lease get” request. When a web server receives “first miss,” it goes to the database to retrieve data and fill the cache. When a web server receives “hot miss” with a stale value, it can typically use that value. If it receives “hot miss” without any value, it can choose to wait for the cache to be filled by the “first miss” web server. In summary, with both of the above implementations, we can mitigate the increased database load by reducing the number of accesses to the database, as well as the resources required for each access. It also improved the reliability of our backend in the cases when some hot counters fall out of cache, which wasn’t an infrequent occurrence in early days of Instagram. Each of these occurrence would cause some hurried work from an engineer to manually fix the cache. With these changes, those incidents have become memories for old-timer engineers. So far, we have focused mostly on cache consistency when caches become regional. Network latency between data centers across the continent was another challenge that impacted multiple designs. Between data centers, a 60ms network latency can cause problems in database replication as well as web servers’ updates to the database. We needed to solve the following problems in order to support a seamless expansion: As a Postgres primary takes in writes, it generates delta logs. The faster the writes come in, the more frequent these logs are generated. The primaries themselves store the most recent log files for occasional needs from the replicas, but they archive all the logs to storage to make sure that they are saved and accessible by any replicas that need older data than what the primary has retained. This way, the primary does not run out of disk space. When we build a new readreplica, the readreplica starts to read a snapshot of the database from the primary. Once it’s done, it needs to apply the logs that have happened since the snapshot to the database. When all the logs are applied, it will be up-to-date and can stream from the primary and serve reads from web servers. However, when a large database’s write rate is quite high, and there is a lot of network latency between the replica and storage device, it is possible that the rate at which the logs are read is slower than the log creation rate. The replica will fall further and further behind and never catch up! Our fix was to start a second streamer on the new readreplica as soon as it starts to transfer the base snapshot from the primary. It streams logs and stores it on local disk. When snapshot finishes transfer, the readreplica can read the logs locally, making it a much faster recovery process. This not only solved our database replication issues across the US, but also cut down the time it took to build a new replica by half. Now, even if the primary and replica are in the same region, operational efficiency is drastically increased. Instagram is now running in multiple data centers across the US, giving us more flexible capacity planning and acquisition, higher reliability, and better preparedness for the kind of natural disaster that happened in 2012. In fact, we recently survived a staged “disaster.” Facebook regularly tests its data centers by shutting them down during peak hours. About a month ago, right as we had finished migrated our data to a new data center, Facebook ran a test and took it down. It was a high-stakes simulation, but fortunately we survived the loss of capacity without users noticing. Instagration part 2 was a success! Stories from the people who build @Instagram 454 2 Infra Performance 454 claps 454 2 Written by Stories from the people who build @Instagram Written by Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-05-02"},
{"website": "Instagram-Engineering", "title": "c futures at instagram", "author": ["Instagram Engineering"], "link": "https://instagram-engineering.com/c-futures-at-instagram-9628ff634f49", "abstract": "Machine Learning Android iOS Infrastructure Data Open Source Team View All Posts Over the past few months, we’ve built two high-performing recommendation services that handle tens of thousands of queries per second and generate tens of millions connections per day. In this blog post, we want to share our experience of scaling these two services using Futures and, most importantly, how we fine-tuned the details. The first recommendation service is “Suggested Users.” The SU service fetches candidate accounts from various sources, such as your friends, accounts that you may be interested in, and popular accounts in your area. Then, a machine learning model blends them to produce a list of personalized account suggestions. It powers the people tab on explore, as well as one of the entry points after new users sign up. It is an important means of people discovery on Instagram and generates tens of millions follows per day. The second service is “Chaining.” Chaining generates a list of accounts that a viewer may be interested in during every profile load. The performance of this service is important — it must be ready to be hit for every profile visit, which translates to over 30,000 queries per second. These two services share similar infrastructure: they need to make outbound network calls to retrieve suggestions from various sources, load features and rank them before returning them to our Django backend for instrumentation and filtering: While most of our backend logic lives in Django, we write the services that generate and rank suggestions in C++ using fbthrift. To understand the evolution of our services’ threading model, we need to understand the life cycle of a thrift request. An fbthrift server has three kinds of threads: acceptor threads, I/O threads and worker threads. When a request comes in: An acceptor thread accepts the client connection and assigns it to an I/O thread; The I/O thread reads the input data sent by a client, and passes it to a worker thread and the I/O thread will again be responsible for sending outbound requests later; The worker thread deserializes the input data into parameters, calls the request handler of the service in its context and spawns additional threads for outbound calls or computation. The important part is that the thrift request handler runs in a worker thread and not in an I/O thread. This allows the server to be responsive to clients — even if all the worker threads are busy, the server will still have free I/O threads to send an overloaded response to clients and close sockets. The initial version of the service loaded candidates and features synchronously. To reduce latency, all the I/O calls were issued in parallel in separate threads. At the end of the handler was a join() primitive which blocked until all the threads were done. What this essentially means is that one worker thread could only service one client request at a time, and one single request would block as many threads as the number of outbound calls. This has several disadvantages: It leads to a large memory footprint — each thread by default has a stack size of several MBs. We need a separate worker thread to service each client request (and more threads created in the handler to make the I/O calls in parallel). If each request makes M outbound calls, we will have O(M * N) threads waiting for responses. Thread scheduling also becomes a bottleneck in the kernel at around 400 threads. With this model, we had to run several hundred instances of server across many machines to support our QPS, because we are not utilizing CPU resource or memory efficiently. Clearly, there was room for improvement. The fbthrift offers three ways to handle requests: synchronous, asynchronous and future-based. The latter two offer non-blocking I/O and this is how it works : every I/O thread has a list of file descriptors on whose status change it waits on in an event loop (it detects this status change through the select()/poll()/epoll() system call). When the status of the file descriptor changes to “completed,” the I/O thread calls the associated callback. In order to do non-blocking I/O under this mechanism, two things need to be specified: A callback which will be called when the I/O is complete The I/O thread whose list should hold the file descriptor corresponding to your I/O operation (This is done by specifying an event base). This is a typical event-driven programming system. It gives us many nice things: Waiting on select()/poll()/epoll() puts a thread to sleep, which means it does not busy wait. Thus, it is efficient. To be clear, the synchronous I/O does not necessarily busy wait either, but it requires allocating one thread per I/O call. One I/O thread can take care of the I/O of multiple outbound requests. This reduces the memory footprint and synchronization costs associated with a large number of threads, and leads to a more scalable system. One worker thread does not need to wait for all the I/O associated with a single client request to complete before moving on to the next client request. Thus, one worker thread can perform computation for multiple concurrent client requests. Once again, this gives us the benefits mentioned in 2. At this point, we were in a pretty good shape in terms of scalability. However, the callback based programming syntax has many deficiencies. For one, it leads to code growing sideways when callbacks are nested, something known as the “callback pyramid.” This has an impact on code readability and maintainability, and we needed a different async programming paradigm. Two other paradigms are very popular at Facebook — the async/await paradigm used in Hack, which is similar to generators, and the Futures paradigm (through the folly::Futures open-source framework). Futures improve upon the callback-based paradigm with their ability to be composed and chained together. For example, the above code can be written as follows in this paradigm: This is an example of futures chaining. This solves the ‘callback pyramid’ problem, and makes the code much more readable. It provides the ability to combine Futures together, and also provides very nice error handling mechanisms. (Checkout the github for more examples and features of this API.) After moving to Futures, we had a performant system with clear, readable code. At this point, we did some profiling to find opportunities for fine-tuning and improvement. One curious thing we noticed was that there was significant delay between the completion of the I/O calls and starting the execution of the ‘then’ handler. In fact, the callbacks in the Future chain above are executed in I/O thread contexts, and some of the work they do is non-trivial. This is the source of bottleneck — I/O threads are limited in number and are meant to service I/O status changes. Executing handlers in their context meant that they could not respond to I/O status changes fast enough, causing the delay in execution of handlers. Meanwhile, the worker threads were sitting idle, leading to low CPU utilization. The solution was simple — execute handlers in the context of worker threads. Fortunately, the Futures API provides a very nice interface to achieve this: This relieves I/O threads of actual computation such as ranking and reduced our I/O threads busy time by 50%. In addition, this helps prevent cascading failures where I/O threads are all busy and thus none of the callbacks are executed fast enough, which causes most of the requests time out. With folly Futures, our services can fully exploit system resources and are more reliable and efficient than the ones with synchronous I/O. We were able to increase the peak CPU utilization of the Suggested User service from 10–15% to 90% per instance. This enabled us to reduce the number of instances of the Suggested Users service from 720 to 38. Chaining achieved 40 ms average end-to-end latency and under 200ms p99, handling more than 30,000 queries per second. It runs on only 38 instances (each instance handles around 800 requests per second). Thanks to Sourav Chatterji, Thomas Dimson, Michael Gorven and Facebook Wangle Team — they all have made great contributions to this effort. Stories from the people who build @Instagram 55 2 Web Infra Performance 55 claps 55 2 Written by Stories from the people who build @Instagram Written by Stories from the people who build @Instagram Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-05-02"}
]