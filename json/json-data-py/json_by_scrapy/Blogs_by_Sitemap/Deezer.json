[
{"website": "Deezer", "title": "women hip hop", "author": ["Niels Nicolas"], "link": "https://deezer.io/women-hip-hop-e00fb19cc885", "abstract": "Previous stories About Jobs Playlist inspired by the article: www.deezer.com Hip-Hop was born in 1973 on 1520 Sedgwick Avenue in the Bronx, NYC. On January 4th 2018, it became the most consumed music genre in the United States, surpassing rock for the first time in history. In between those years, Hip-Hop and especially rap music evolved a lot and, little by little, the role of women in rap culture gained greater importance. The expansion of streaming services is responsible for much of Hip-Hop’s success story in the last couple of years. With the help of Deezer’s stream database, we were able to paint a picture of the ever-growing role that females have in Hip-Hop culture. This post details the outcomes of our study, through the lens of three data visualisation tools. Throughout this article we will talk a lot about the gender balance and gender score. These notions help us understand the number of female listeners within an audience. We will be using this criteria to compare artists and their songs with each other. To begin, we looked at the 200 most streamed Hip-Hop (1) artists of 2018. Of course, Deezer is a French platform, so you will see a lot of both American and French artists. We ended up with only 13 female acts, 2 Hip-Hop groups with both male and female rappers and 185 solo male artists. When it comes to the audience, Hip-Hop has an average female gender balance of 32%, which makes it the genre (2) with the second-biggest male audience on Deezer. Although it should be noted that the global gender balance on Deezer’s active base, is around 37%. As mentioned earlier in the introduction, this study revolves around three dashboards. The first one is a visualisation of our 200 artists on a map, that measures both their gender (3) balance and popularity (4) . With this dashboard we can start to see what the gender balance could depend on. The second dashboard gives a visualization of the evolution of the gender balance for each artist. The third one goes one step further, giving a map of the popularity and gender balance of the top 50 songs from the 200 artists. The first dashboard is a map that allows us to visualize the gender balance and the popularity of the 200 artists that we are focusing this study on. How should you read this dashboard? → The higher up an artist is on the map, the more streamed they are. → The further to the right , the higher the female gender score is. From this dashboard alone, we can observe that female listeners prefer to listen to female artists. In fact, all of the 13 females from our 200 artists are in the top quarter of the highest gender score artists. In 2018, Becky G was the female artist with the lowest gender balance (+6), and she is still the 44th most streamed out of the 200 artists. We can also observe that two factors seem to be especially important when trying to figure out what makes a rap artist successful with a female audience: melodies and popularity . Now let’s be very clear about what we define as melodies in this article. For a very long time, the great majority of rappers would stick to just rapping and singing on a track wouldn’t cross their mind. This new wave of making Hip-Hop songs started in the 2010’s and got very popular in this generation of new rappers. Before rappers used jerky flows, and fewer artists would move between singing and rapping. Nowadays, the great majority of rappers have incorporated melodies in their songs, especially with the rise of Autotune, a pitch correction software that is popular in rap music. Here are two Hip-Hop records as examples: one from Hamza with melodies, and one from Kalash Criminel, which has more of an old rap music kind of feel. https://www.deezer.com/track/417195132 https://www.deezer.com/track/362777091 At this point, we will talk about the gender score and also about the gender balance, which is the percentage of women within the audience. Mainstream rappers get access to a larger female audience. As we can see in the first dashboard, very successful artists such as Maitre Gims, J Balvin, Drake or even JUL, have a gender score above both the genre and the general Deezer average. Being on the charts, having millions of views on music videos, give these artists access to a much larger public. There is one pretty noticeable difference between artists from the two sides of the map when it comes to the music itself. For each era (Old School and New School Rap), we have noticed that specific groups end up falling into specific parts of the dashboard. Old school American rappers such as Nas, DMX, 2Pac or 50 Cent can be seen on the bottom left of the map. This shows that these rappers have a larger male following. But let’s not forget that, because the dashboard is only based on the streaming habits of our users in 2018, listeners seem to enjoy new content. The left side is also where you can find the emerging rappers. Let’s take a look at four specific artists: Zola, Josman, YL, Remy. The common thing between these artists is that they all broke out in 2018, and that their gender score is between (-17) and (-12). They represent a list of emerging artists who will most likely reach a better gender balance as their exposure keeps growing. We also realized that rap artists who go from rapping to singing are more likely to end up on the right hand side of the dashboard, where the gender score is higher. For that reason, artists like Still Fresh, Hamza or the french group Columbine have a much higher gender score than artists like Kalash Criminel or 21 Savage. Remember I said it wasn’t easy for emerging rap artists to reach a high gender score when they are still up-and-coming? Well, fortunately there are exceptions that suggest this is not always true. Ash Kidd is still an emerging artist, with a niche audience. However, he’s in the top 10 artists with the highest gender score in 2018. We can guess that the melodic themes and productions that define his music have given him access to a larger female audience, without charting singles or massive media coverage. The second dashboard allows us to scroll through the progression of the 200 artists from our study, from 2016 to 2018. We will now look at the trends and patterns we discovered. Overall, we can say that the gender balance is evolving for much of our artist population. Only a few artists stay stable over time, such as super-established artists Beyonce, Rihanna, Akon, Nelly, etc. Globally, the gender balance is improving for Hip-Hop artists. As it shifted to being the number one music genre, rap music became more diverse and accessible. Today, the range of artists within Hip-Hop culture is a lot wider than what it used to be. This transformation explains why the number of successful female rappers is growing and why the female audience is gravitating more and more towards rap music. We had the opportunity to present these conclusions to our Hip-Hop and Rap Music Editor, Mehdi Maizi , over here at Deezer, and we were happy to find he shared our point of view. We came across an interesting pattern during our gender balance evolution study: for every new track release, the gender balance collapses locally for a day or two before returning to its initial figure. In other words, we have a lot more men than women listening to an artist shortly after a new track is released. Popularity plays an important role in understanding an artist’s gender balance. Essentially, the more popular the artist, the closer to a 50% gender balance. Remember Hip-Hop music has an average gender balance of 34%, so when an artist gains popularity, the gender balance tends to improve. But the same phenomenon can be observed for artists that start their career with a high gender balance. Therefore, we can observe Aya Nakamura’s gender balance going down from 80% to 60% (of women), as a consequence of releasing “Comportement” and “Djadja”, the two biggest-selling records of her discography. With the third dashboard, we can go into further detail about the artists’ catalogs, which will help us understand how popularity affects the gender balance. Here, we have focused on the top 50 tracks of the 200 artists that we studied. On this new map, the x-axis measure the popularity and the y-axis measures the gender. Once again, different patterns can be identified. Things get interesting when you analyze artists with an eclectic mix of sounds in their discography, Booba for example. Booba’s top 50 songs vary between 17% and 40%. Looking at the graphics, again we realize that content and popularity have a significant influence on the gender balance, even for specific songs. Let’s compare two different songs from an artist. If we look at Booba’s ‘Nougat’ and ‘Petite Fille’, all of which are respectively certified platinum and diamond records, we can understand how rap music can shift the gender balance, sonically. ‘Petite Fille’ paints a very interesting picture of what a rap song could sound like in 2019: the record is almost entirely dedicated to Booba’s daughter, with a very emotional video clip starring the little girl. A melodic flow with lyrics that are a mix of love and raw rap: the song appeals to a wider range of listeners and it’s still deeply rooted in Hip-Hop culture. To highlight the effect of popularity on gender balance, we looked at two American rappers: Juice Wrld and Desiigner. These two artists have one thing in common: a cohesive discography and one clearly detached and identified hit record. Desiigner smash hit ‘Panda’ peaked at #1 on the Billboard charts and Juice Wrld ‘Lucid Dreams’ record peaked at #2. The dashboard speaks for itself on this situation: The exposure an artist receives from a hit record has a marked effect on the gender balance, as you can see with these two examples. Compared to the previous dashboard, Aya Nakamura brings the contrasting example of an artist with a high gender balance: Looking at Nakamura’s discography, we see that Djadja and its remixes are by far the most gender balanced records. Again, we can see that popularity tends to push the gender balance closer to 50%. During our study we searched for other potentially significant variables that could have an effect on the gender balance. We assumed that BPM could be one of them, with certain songs and certain tempos resonating more with a specific gender. However, we found this was not the case at all! We can’t see any trends emerging from the BPM on the map: the top five songs with the highest gender balance have five different BPMs. Generally speaking, we observe a dispersed scatter plot for almost every artist: the fact that the scatter plot is very dispersed proves that there is no correlation between the tempo and the gender balance. Stereotypes are never good! We can conclude from this study that women are becoming increasingly interested in Hip-Hop music. Even if we can still observe some disparities within the artists’ catalogs (just like in the Booba example), rap music is slowly getting away from being confined to a strictly masculine audience and instead is becoming a very popular and balanced genre. Thanks to the first dashboard, we see that women are still preferring rap music from female artists. Thanks to the second dashboard, we saw that the gender balance generally shows a positive progression. And with the third dashboard we found insights on what it takes to conquer women’s hearts today, while making rap music. For all three dashboards, new sounds and popularity are the two strong variables that are significant to understand the evolution of the gender balance. Sonically, rap music is evolving into something more accessible and popularity is putting Hip-Hop in the spotlight. Beyond the data, Cardi B is the perfect example of what the future looks like for female rappers. In fact, ever since her breakout hit “Bodak Yellow” in 2017, she’s been making great moves in the industry. Her debut album “Invasion of Privacy” was the 2019 Grammy winner for Best Rap Album , making her the first-ever woman to ever win this award. This is an open door for more and more women to get inspired to follow her path and become relevant in the Hip-Hop music landscape. (1) We considered as “Hip-Hop” any artist with at least one album labeled as Hip-Hop in their discography. (2) The first genre with the biggest male audience on Deezer is rock music. (3) Gender information was provided by our users when they first signed in. (4) Popularity was measured based on the streams made throughout the year on these artists. www.deezerjobs.com Stories of engineering, data, product and design teams… 195 2 Hip Hop Women In Music Data Stories Data Science Deezer 195 claps 195 2 Written by @nielsixxi everywhere on the internet Stories of engineering, data, product and design teams building the future of music streaming Written by @nielsixxi everywhere on the internet Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-07-31"},
{"website": "Deezer", "title": "deezer contest where product and qa meet", "author": ["Guillaume Grillat"], "link": "https://deezer.io/deezer-contest-where-product-and-qa-meet-fabc91a65a7e", "abstract": "Previous stories About Jobs As the event will take place in Paris, this post is available both in French and in English (scroll down for the English version). Nous organisons Deezer ConTEST , premier événement qui associera une quarantaine de participants dans le cadre d’une journée de challenges, de brainstorming et de chasse aux bugs autour de 4 thématiques spécifiques à une application de streaming : l’offre gratuite, le téléchargement et l’utilisation offline, les connexions instables et la recherche de contenus dans une bibliothèque de 53 millions de titres. Nous souhaitons que cet événement soit fun (challenges en équipe, prix spéciaux et nombreux cadeaux à gagner, etc.) mais aussi et surtout inspirant et formateur (apprentissage, échange de best practices, présence d’experts QA et Produit, etc.), afin que chacun reparte avec des conseils pratiques à l’issue de la journée. A cette occasion, nous avons le plaisir de nous associer à deux communautés Product et QA de référence : Product Stories : communauté Produit incontournable, portée par Laure Albouy , Thomas Didier et Arnaud Breton , dont la philosophie est bien résumée par leur introduction: “Come hear the behind the scene stories about the people who build the products you love to use everyday” Agile Testing Paris : une communauté de plus de 1500 membres impliqués dans des activités de tests (c.à.d. tout le monde !) et sensibles au déploiement en environnement Agile. Une approche pédagogique prônée par son administrateur Jean-Pierre Lambert et à retrouver sur sa chaîne Youtube Scrum Life . → Voici les speakers référents issus des communautés Produit et QA qui interviendront lors de cette journée : Au programme: une matinée et une après-midi de challenges autour de notre application Deezer, une table ronde avec les experts Produit et QA mentionnés ci-dessus à l’heure du déjeuner, enfin, la possibilité d’échanger avec eux tout au long de la journée et de partager retours d’expériences et autres anecdotes. Les participants seront accompagnés toute la journée par les équipes produit et qualité de Deezer, qui seront à disposition pour toute question relative à nos services de streaming web et mobile. Cette journée sera riche en discussions, et vous donnera l’occasion — si vous nous rejoignez — d’agrandir votre réseau professionnel, de découvrir de nouvelles approches en termes de produit et de gestion de la qualité… et de remporter des cadeaux (abonnements Deezer, enceintes connectées, etc.) ! Envie d’apprendre et de rencontrer vos pairs ? De grandir sur votre métier ? Prenez quelques secondes pour nous en dire plus sur vous en remplissant ce formulaire : https://dzr.fm/ConTEST ⚠️ A noter qu’il est obligatoire de s’inscrire avant le 1er mai pour pouvoir participer à cette journée ! La participation à cette journée est GRATUITE . Nous la voulons aussi inclusive et collaborative que possible. Nous accorderons une attention particulière à la motivation des candidats et à la diversité des parcours. A bientôt ! We are hosting Deezer ConTEST on May 18th, i.e. the first event of this kind where 40 participants will gather for a full day of challenges, training, brainstorming and bug hunting. We will focus on 4 main themes that are central to a streaming platform’s services: freemium, download and offline use, low bandwidth networks and content search among 53 million tracks. We want this event to be fun (with team challenges, special prizes and goodies awarded to the winners, etc.) but also inspiring and informative (through the sharing of concrete testing techniques, training, meeting Product and QA experts, etc.) so that each participant leaves with a range of tips and methods that they will be able to put into practice the next day. On this occasion, we are pleased to partner with two key Product and QA communities : Product Stories is a major Product community founded by Laure Albouy , Thomas Didier and Arnaud Breton , whose philosophy can be summarized as follows: “Come hear the behind the scene stories about the people who build the products you love to use everyday”. Agile Testing Paris is a community of over 1,500 people involved in testing activities (as everyone should be!) in an Agile environment. Its pedagogical approach is well supported by its founder, Jean-Pierre Lambert, who runs the Youtube channel Scrum Life . → Here are the speakers from these Product and QA communities who will take part in the event: Agenda for the day: one morning and one afternoon sessions to take part in challenges on our Deezer app, a round table discussion with the aforementioned Product and QA experts during lunch break, and the possibility to talk with them throughout the day and share feedback and experiences. The participants will be assisted by Deezer’s Product and Quality teams, which will be available all day to answer all questions about our web and mobile streaming services. This day will be full of information and discussion! Should you join us, you will have the opportunity to expand your professional network, discover new product and testing approaches…and win some goodies and prizes (Deezer subscriptions, smart speakers, etc.) Do you wish to learn and meet your peers? To get better at your job? Please take a few seconds to fill out this form and tell us more about yourself: https://dzr.fm/ConTEST ⚠️ Registration is mandatory if you wish to attend the event. The deadline to register is May 1st . Please note that registration and participation are FREE . We want this day to be as inclusive and cooperative as possible. We will pay special attention to the candidates’ motivation and variety of backgrounds. See you soon! www.deezerjobs.com Stories of engineering, data, product and design teams… 17 1 Product Product Management QA Testathon Deezer 17 claps 17 1 Written by Tech Community Ambassador @leboncoinEng. My goal ? Helping my teams to flourish at work, by meeting exciting & innovative people & gaining new knowledge. Stories of engineering, data, product and design teams building the future of music streaming Written by Tech Community Ambassador @leboncoinEng. My goal ? Helping my teams to flourish at work, by meeting exciting & innovative people & gaining new knowledge. Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-05-13"},
{"website": "Deezer", "title": "from paris to bordeaux an interview with deezers cto", "author": ["Pauline Munier"], "link": "https://deezer.io/from-paris-to-bordeaux-an-interview-with-deezers-cto-d49af6a72282", "abstract": "Previous stories About Jobs In May 2018 Deezer announced the opening of a second French office in Bordeaux. As the team is growing , Matthieu Gorvan, Chief Technology Officer at Deezer, explains the reasons for the establishment of a new office in France and talks about Deezer’s future in Bordeaux. On this (special) occasion, the interview is available both in French and in English (scroll down for the English transcript). Pourquoi avoir ouvert un nouveau bureau en France ailleurs qu’à Paris ? Paris n’est pas attractif pour tout le monde. D’une part, un nombre croissant de collaborateurs chez Deezer ne souhaitent plus poursuivre leur route à Paris et aspirent à de nouveaux horizons, et d’autre part des personnes que l’on souhaiterait recruter ne veulent pas venir à Paris. Le choix d’installer une partie de nos équipes en région nous permet d’offrir une réelle alternative pour retenir nos talents et en attirer de nouveaux. Par ailleurs, nous souhaitions conserver le côté pratique, c’est pourquoi nous sommes restés en France, dans le même fuseau horaire, afin d’éviter le décalage. La proximité avec Paris (2h de train) nous permet également d’envisager des déplacements d’un bureau à l’autre dans le cas de projets ou task forces exceptionnels, et d’optimiser la communication entre les bureaux. Cette installation est aussi un moyen de tester nos capacités à pouvoir travailler à distance. Qu’est-ce que Deezer gagne à s’implanter en région ? Pour attirer les talents dans le monde de la tech, au-delà d’être une marque attirante, il faut faire preuve de flexibilité. Rassembler tous les talents sur un seul lieu à Paris n’est plus suffisant. Ainsi, vouloir s’implanter en région valorise clairement notre marque employeur. En ce qui concerne Bordeaux, il y a un écosystème tech grandissant et on a envie de participer à son explosion, et d’en bénéficier par la même occasion. Les perspectives de partenariats, d’évolutions conjointes, d’innovation, etc. avec les acteurs locaux sont aussi intéressantes. Ensuite, le marché évolue vite, ce qui nous oblige à être hyper réactifs en termes de recrutement. Être physiquement présents à Paris et à Bordeaux nous ouvre ainsi la possibilité de recruter plus rapidement et avec plus de flexibilité. Cela facilite également la construction et le renforcement des équipes. Enfin, on souhaite donner la possibilité à nos employés d’évoluer dans un cadre de vie différent, plus en adéquation avec leurs projets de vie personnels. Deezer avait une équipe très jeune à ses débuts. Certains ont maintenant un, deux ou trois enfants et souhaitent quitter Paris malgré leur attachement à la marque Deezer. Avec un bureau à Bordeaux, on propose à ces personnes de les accompagner dans leur projet de vie tout en leur permettant de continuer à travailler chez nous. Cela comble ainsi leurs besoins à la fois personnels et professionnels. Nos talents comptent énormément sur nous pour les fidéliser. A l’inverse qu’est-ce que Deezer peut apporter au niveau local ? Deezer est une marque forte dans le monde de la tech et va volontiers participer au développement de l’écosystème local, qui est déjà bien enclenché. Nous serons présents sur des évènements en région et nous souhaitons aussi en organiser de manière très active, comme nous le faisons déjà sur Paris. On a d’ailleurs déjà organisé un premier meetup dans nos locaux bordelais fin novembre, en collaboration avec CDiscount et AT Internet, sur des thématiques d’infrastructure. Pourquoi le choix s’est-il porté sur Bordeaux ? On recherchait une ville attractive d’un point de vue technologique (i.e. bénéficiant d’un écosystème tech), avec un bon cadre de vie et dynamique en termes de recrutement (avec un pool de talents intéressants attachés à leur vie en région). On avait donc listé plusieurs villes correspondant à ces critères et le choix s’est fait selon la motivation des équipes qui étaient prêtes à porter le projet et l’ADN Deezer en région. Ces personnes, sur lesquelles on compte pour créer sur place un environnement en adéquation avec la culture de Deezer, ont fait pencher la balance en faveur de Bordeaux. Les villes de Bordeaux et Nantes, qui faisait partie des villes finalistes, nous ont soutenu, aidé et ce, dès la naissance du projet. Dans les deux cas, la région et la ville étaient très réactives et soucieuses d’accompagner au mieux notre implantation, notre recherche de locaux et de faciliter l’installation des collaborateurs concernés. Ce mouvement représente un gros changement pour nous, comme pour eux, donc c’est rassurant et motivant de se sentir accompagnés. Pour Bordeaux, Invest in Bordeaux nous a beaucoup aidé. Quelles activités sont installées dans les nouveaux locaux ? Plusieurs équipes sont réparties entre Paris et Bordeaux. Travailler avec des équipes à distance est un défi managérial qu’il faut être capable d’accepter et Cédric Delfosse, Head of Engineering Core System, a la volonté et la seniorité requises pour le relever. Certaines de ses équipes sur les sujets d’Infrastructure et de Core Backend sont partagées entre nos deux bureaux. Nous avons confiance en leur motivation et implication pour faire en sorte que cela fonctionne, sachant que ce sont des profils suffisamment seniors qui connaissent bien l’entreprise. Ces équipes étaient volontaires pour ce mouvement, c’est pourquoi elles ont été les premières à s’établir à Bordeaux. Une autre équipe, dédiée au tooling, s’est agrandie tout en déménageant en partie à Bordeaux. Romain Lods, lui aussi volontaire, dirige cette équipe dont l’objectif est de répondre à la demande croissante d’outils en interne et d’améliorer l’efficacité de divers corps de métiers (développeurs, product managers, brand, product marketing, etc.) Quelles sont les prochaines étapes ? Notre objectif premier était d’accompagner dix collaborateurs avant juillet 2018 et de s’assurer que la communication et le travail à distance s’organisent parfaitement entre Paris et Bordeaux. Depuis, quelques collaborateurs de Paris et de nouvelles recrues bordelaises ont rejoint l’équipe. Nous hébergeons également une start-up avec laquelle nous collaborons (Simbals), ce qui facilite les échanges. En 2019, nos priorités pour Bordeaux sont : - d’accueillir des stagiaires, afin de contribuer activement à la formation des futurs diplômés de la région; - de former une nouvelle équipe dédiée à l’exploration de nouvelles opportunités technologiques, business, produit ou design, et au développement d’applications prototypes. Rappelons que notre bureau bordelais peut accueillir jusqu’à 50 personnes, et lors de sondages internes, 40 à 80 personnes à Paris avaient exprimé l’envie de déménager en région. Des mouvements sont donc à prévoir cette année et on les accompagnera comme il se doit ! Par ailleurs, Bordeaux abrite beaucoup de grandes écoles et universités qui forment de nombreux talents. On souhaite s’en rapprocher et leur proposer de travailler conjointement sur des projets innovants, challengeants et créateurs de valeur qui peuvent être intéressants et formateurs pour leurs étudiants. Envisages-tu d’aller à Bordeaux ? Je l’ai envisagé. Au départ, lorsque j’étais porteur du projet, j’avais moi-même constaté cette envie de changement et de nouveau cadre de vie chez certains employés. J’avais donc envie d’aller en région, de les accompagner et de participer à la création d’un nouveau bureau. Il se trouve que j’ai été nommé CTO en septembre 2017 donc il est devenu plus compliqué pour moi de déménager car la plupart de mes équipes se trouvaient à Paris (150 personnes). Néanmoins, je ne voulais pas abandonner le projet et, par chance, Romain Lods était motivé pour le reprendre. Au final, je suis un peu “triste” de ne pas y aller mais très content et fier que le projet se soit réalisé. Il y a d’autres très beaux challenges pour moi à Paris ! A l’avenir, si je venais à avoir 50% de mes équipes à Bordeaux, je me poserais plus sérieusement la question de m’y installer. Pour l’instant, je n’y pense pas. Je veux juste que tout fonctionne correctement, que l’on continue à donner du sens à ce bureau et à maintenir le dynamisme des équipes sur place. On verra plus tard pour mon cas personnel ! Why did Deezer decide to open a new French office outside of Paris? Paris is not necessarily ideal for everybody. More and more Deezer co-workers wish for new spaces and no longer want to live in Paris, and some men and women we are interested in recruiting also don’t want to come live here. We decided to offer a solution to these people, to both keep our talents and attract new ones. We chose to expand elsewhere in France because we wanted to keep things simple and stay in the same time zone to avoid staggered working hours. In particular, proximity with Paris (two hours by train) allows our teams to move easily to one office or the other for one day or more if a specific project or mission requires it. We are also testing our capacity to work from a distance and ensure optimal communication in between offices. What is the benefit for Deezer of setting up in another region? In order to attract talents in the tech world, you need to be well known but also flexible. Having a Paris-centric approach is not enough. Opening an office in Bordeaux gives visibility and adds real value to our employer brand. Besides, there is a growing technological ecosystem in Bordeaux and we wish to be part of it. Opportunities for partnerships, joint developments, innovation, etc. are also promising. Deezer is part of a market that evolves quickly and therefore needs to be super reactive and able to assemble new teams rapidly. Being established both in Paris and Bordeaux, we should gain flexibility and have more opportunities to hire talents in order to create or reinforce teams in either of our offices. Finally, we wish to offer our employees a different living environment that is more in line with their personal life plans. Initially, most Deezer employees were quite young. Some have one, two or three children now and wish to leave Paris, despite their attachment to the Deezer brand. With the opening of a new office in Bordeaux, we are meeting both their personal and professional needs: we are helping them with their life project while allowing them to still work for Deezer. Our talent means a lot to us and we don’t want to let them go, especially if they care about Deezer too. In return, what can Deezer bring locally? Deezer has a strong name in the technological field so we will contribute to the existing local ecosystem and help it grow further. We intend to be very active locally, take part in external events and also organize events of our own (as we already do in Paris). In this regard, we already organized our first meetup in the Bordeaux offices last November, on infrastructure topics and in collaboration with CDiscount and AT Internet. Why Bordeaux? We were looking for an attractive city in terms of technology (i.e. with an existing tech ecosystem), life environment (pleasant to live in) and recruitment (i.e with a pool of talents who don’t want to leave the region). We listed several cities but the final choice was made according to the motivation of the teams that really wanted to move and were ready to carry the project. These team members, who were highly motivated and whom we trusted to bring the Deezer DNA and culture to a new city, chose Bordeaux. Bordeaux, like Nantes (which made the shortlist), are cities that showed an immediate interest in our project and made sure that we would be well informed, supported and accompanied all along. In both cases, we knew that the city and its region were very reactive and ready to help us find premises, establish ourselves and help our team members settle down. In particular, Invest in Bordeaux was a great help to us. It is a big step for us so it is reassuring and motivating to feel supported. What activities are located in Bordeaux? A few teams are now located both in Paris and Bordeaux. Working with teams from a distance is a managerial choice and we needed it to be accepted. Cédric Delfosse, Head of Engineering Core System, willingly accepted the challenge. Two of his teams on the Infrastructure and Core Backend parts are spread across the two offices. The team members have a good knowledge of the company and are senior enough, and Cédric himself is senior enough to supervise the new organization, so we are quite confident. These teams were volunteers and we trust their motivation to make things work, that’s why they were our first choice. Another team dedicated to internal tooling grew while partly moving to Bordeaux. Romain Lods, who also volunteered, leads the team, whose objectives are to meet the increasing demand for internal tools and improve the effectiveness of various trades (developers, product managers, brand and product marketing teams, etc.) What are the next steps? Our primary mission was to get 10 team members settled in Bordeaux by the end of July 2018 and make sure communication and collaboration run smoothly between Bordeaux and Paris. A few Parisian colleagues and local recruits joined the Bordeaux offices since. We also accommodate a start-up we have been working with (Simbals), which facilitate exchanges. Our 2019 plans for Bordeaux consist in: - welcoming interns in order to actively contribute to the practical training of future graduates; - assembling a new team dedicated to exploring new technological, business, product or design opportunities, and to developing prototype applications. Our offices can contain up to 50 people, and 40 to 80 Deezer employees expressed the will to relocate to provinces. Deezer is on the move and we will assist our teams the best we can! Besides, there are quite a few top schools and universities in Bordeaux where many talents are trained. We would like to approach these colleges about working together on innovative, challenging, value-creating projects that we wish to carry out and which can be of interest and educational to their students. Are you considering moving to Bordeaux yourself? I did consider it. At first when I was leading the project, I had observed this will to relocate and find a new life environment among employees. I wanted to move there, accompany the teams and take part in the opening of a new office. It turns out I was appointed CTO in September 2017 so it became complicated to consider moving as most of my teams are here in Paris (about 150 people). I didn’t want to abandon the project though. Luckily Romain Lods was motivated to take up the torch and carry on. I’m a bit “sad” not to be going but I’m very happy and proud that it has happened. Wonderful challenges await me in Paris anyway! If 50% of my teams were to be based in Bordeaux in the future, I would seriously think of moving there. But for now, I just want the new office to work correctly and serve a real purpose, and keep the teams motivated. We will deal with my personal situation later! www.deezerjobs.com Stories of engineering, data, product and design teams… 71 Deezer Bordeaux App Development Engineering Management Tech Culture 71 claps 71 Written by Tech Communications Coordinator @Deezer Stories of engineering, data, product and design teams building the future of music streaming Written by Tech Communications Coordinator @Deezer Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-10-20"},
{"website": "Deezer", "title": "deezer kaggledays paris", "author": ["Pierre Thary"], "link": "https://deezer.io/deezer-kaggledays-paris-b686362d00fb", "abstract": "Previous stories About Jobs I was lucky enough to attend the second edition of Kaggle Days, which took place in Paris in January. 200 Data Scientists from all over the world gathered over two days for exciting Data Science-oriented conferences, workshops, brainstorming sessions… and an offline Kaggle competition! If you haven’t heard of Kaggle, it is a famous online platform dedicated to Data Science projects. In my opinion, it is the perfect space to learn and keep yourself up to date with the latest Data Science trends, so I was expecting quite a lot from this event. I will spare you the suspense and say that I wasn’t disappointed! Below are a few takeaways from the event that particularly inspired me. The Kaggle Days have quite a unique format: along with traditional presentations, there were also some workshops and even brainstorming sessions on business problems. The event facilitated all sorts of discussions, and sharing knowledge is the organizers’ primary concern. Another noticeable particularity is that speakers were not representing a specific company, they came as passionate individuals. And I have to say I was amazed by the quality of the presentations! Their content was tailored to an audience of Data Scientists aspiring to be better at what they do, whether they do it for an industry day job or a competition. Here are some quick but insightful ideas I discovered: During his presentation, Alberto Danese explained that many companies that are not technology-focused still struggle to make Machine Learning go from a POC to production. According to him, the major obstacle in that regard is the usual lack of interpretability of the models that work best. Indeed, since we are still unable to see in N-dimensions, we need proper tools to ‘proxy’ the understanding of our model’s intrinsic mechanics. Most of us know that we can globally measure features importance of our models quite easily. But are you aware of LIME and/or Shapley concepts? They provide a framework to understand any model predictions locally ! LIME approximates the behavior of the model around the local prediction by a linear model, which we can easily interpret. A good starting point to learn more on this is this Youtube video made by the LIME paper authors. On the other hand, Shapley ’s additional explanations use game theory to explain predictions. My naive understanding is that it looks at how much each feature contributes to the final result, one after the other, in a specific order. Then, it repeats the same process for all possible orders and averages the contributions out. Have a look at the github repository to learn more about the concept. In case you have an obvious monotonicity constraint between a feature and your target variable (let’s say square meters area of a house, and it’s price) , you can constrain your model to force him to follow that constraint (here price = f(area) should be increasing only) . It seems straightforward but I’ll be honest: I just hadn’t thought about this before. Thanks Alberto Danese and Stanislas Semenov for the trick! As you can see below, it greatly improves the look of the model, even on simple problems, and probably prevents some overfitting issues. Making models more intuitive and better interpreting them are two challenging aspects of Data Science, and these few insights will definitely help me on my day-to-day job. What is knowledge without a little bit of practice though? On the second day, we took part in an 11-hour offline Kaggle competition: perfect to get a hands-on experience on challenging tasks! The objective was to predict the sales of ~2000 LVMH products (LVMH being a sponsor of the event) 3 months after their release. Quite an interesting task with a lot of different data sources to train our models on (e-commerce navigation metrics, physical sales on the first few days following the launch, descriptions and pictures of the product, etc.)! Here are the three key things I will most remember from this competition: We all know that practice makes perfect right? In addition to improving your overall knowledge and coding skills, it also helps you build your own library of code snippets that you will be able to instantly reuse in the future. Of course all competitions are different but there are always some functions you used previously that could make you good here, and very quickly! So keep practising! Competitions such as these make awesome practice material due to a wide variety of problems which you might not encounter in your day job. Your training and validation score match, however your testing score is worse! What could possibly happen here? Many things actually. For example, if your training/validation split does not match the real initial training/test split, it will prevent you from predicting your model performance accurately. An accidental shuffle might also distort your predictions. Eventually, some features’ distributions might totally differ from training to testing set, and it sometimes happens for obvious reasons if you think intuitively about the underlying business problem and the way data was split. Your Cross validation splitting technique should mimic the training testing strategy to ensure predictable performance: this may deserve a prior good thought. For example, testing a model that predicts people’s clothing choices with today’s data will likely yield very bad results if the model was trained and validated with data from the 80s… I am highlighting this because at first we blindly threw all the features in the model, expected good results and were surprised by the very low testing score. The best way to do it was to start with a small number of features and then add features incrementally! Of course, we usually have more time to test these different combinations of features in our day job. But these time constrained challenges force us to use a clean protocol in order to be efficient, which should definitely be part of our best practices if we are to improve our daily work. In the end, our team placed 21st out of 77 teams: a good score but also with room for improvement! Thank you to my great teammates Clément Dessole and Tuan-Phuc Phan, Kaggle Days’ staff for the organization and Deezer for supporting these kinds of initiatives. These two days were both informative and fun, and gave me the opportunity to get better at what I love and do for a living, while meeting inspiring people from the same community, who share my interest. That’s what I like about these events and why I encourage you to seize these opportunities whenever they present themselves. So…see you next time? Alberto Danese — KaggleDays 2019 Paris presentation: “Machine Learning Interpretability: The key to ML adoption in the enterprise” Stanislav Semenov — KaggleDays 2019 Paris presentation: “Tips and tricks for Machine Learning” www.deezerjobs.com Stories of engineering, data, product and design teams… 577 Thanks to Pauline Munier . Machine Learning Kaggle Data Science Comics Conference 577 claps 577 Written by Presenting things makes me fully understand them: if you like it, it’s a win-win ! ;) Stories of engineering, data, product and design teams building the future of music streaming Written by Presenting things makes me fully understand them: if you like it, it’s a win-win ! ;) Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-03-15"},
{"website": "Deezer", "title": "a journey towards atomic design on android at deezer", "author": ["Jean-Baptiste Vincey"], "link": "https://deezer.io/a-journey-towards-atomic-design-on-android-at-deezer-f384928bb04e", "abstract": "Previous stories About Jobs As a French music streaming company evolving in an international, fast-paced environment, Deezer has quickly adapted to new opportunities over the past 10 years. The team has continuously grown in order to improve services, address very diverse and inspiring topics, develop new features and hardware integrations, etc. This expansion has indeed brought new challenges, one of them being application design. How could we ensure consistency across platforms with more than 15 developers working on each, along with a very large number of UX/UI designers? How could we maintain a uniform design throughout our app while also allowing the design to evolve? How do we fill in the knowledge gaps between designers and developers, and ensure knowledge is shared effectively with newcomers? These were our main concerns to continue improving the app in an efficient way. When addressing these issues, we first had to change the way we understand application design. It was important to stop seeing the apps as sets of pages , but instead as a design system . As a matter of fact, this idea is not new. Brad Frost developed a methodology to create and maintain robust design systems that he called Atomic Design (really worth a read). His idea was to break down pages into design components in an organized hierarchy . What is an interface made of? What are our Lego bricks? What are our Subway sandwich pieces that we combine into millions of delicious combinations? - Brad Frost, Atomic Design (chapter 1) The main point was not to think top down (with pages coming first) but bottom up, i.e. to start with the tiniest component and see how it integrates in bigger components and so on. To explain it simply, he took his inspiration from chemistry — more precisely from the science of matter — and ended up with 5 stages: atoms , molecules , organisms , templates and pages : Atoms are the fundamental building blocks of our interfaces that cannot be broken down into smaller pieces. They consist of properties such as font or sizing of text, dimensions for a cover picture, etc. Molecules are simple combinations of elements (atoms) that, when combined, become a functional unit. Molecules should remain quite simple in order to be easily reusable. Organisms are more complex components, comprised of atoms, molecules and possibly other organisms. They generally represent entire sections of an interface. For the last stages of the design process, Brad Frost refers back to a widely recognizable metaphor as seen in the publishing industry: Templates are blueprints of an interface organization, defining positions for each complex building block (typically organisms). Pages are actual instances of templates, with real data input, that show the result as the user will see it. To sum up, molecules are made up of atoms, organisms are built from those molecules, which are then placed into templates to create pages. This hierarchal organization allows to: build a more consistent user experience throughout the application. A change on any component (e.g. an atom or molecule) is automatically applied to the entire app. speed up feature development , by leveraging the components’ reusability. Building a new page with already existing components becomes fast and easy. It frees up time for what matters, such as focusing on details at the atomic or molecular level, knowing it will be valuable for the whole application. support collaboration across disciplines . Developers and designers would work closely together to build all levels of components. But this also benefits other trades, from product to marketing, by establishing a vocabulary shared by all those contributing to the application. By making a style guide a cornerstone of your workflow […], designers and developers are forced to think about how their decisions affect the broader design system. - Brad Frost, Atomic Design (chapter 2) First, we needed a set of vocabulary that embraces these concepts and integrates them in a way that suits our organization. At Deezer, we rely on the LEGO metaphor as it conveys the idea of reusable building blocks used to construct bigger components until the final stage. This analogy is unsurprisingly quite common in the UX/UI communities, and is already put into practice at many companies. Second, we established a UIKit , i.e. a pattern library (as called by Brad Frost) defining every brick that should be present in the app. The UIKit has both a design (built on Sketch and exported on Zeplin ) and an Android implementation. On the development side, we created an external module implementing the bricks. This module also includes a playground, i.e. a small app serving as a showcase for the UIKit. This helps to quicken UI components’ development and testing as they can be checked all at once, and the playground is much faster to build than the actual application. In order to implement the UIKit, we first specified the style guide (colors, textures, typography) that provides the design atmosphere of the app. Then came the atoms , which were built from Android base widgets with specific styles or custom views. Molecules were defined by two things, first a brick — which is a model embedding all data to display in the view, then the view itself, composed of widgets, custom views or other bricks. The view relies on databinding to display the actual data from the brick. Organisms are also bricks, acting as wrappers, and taking a set of child bricks as input. For instance, the discography carousel is based on a horizontal carousel brick (using an horizontal RecyclerView ) and takes a list of album card bricks. The last part is the integration of bricks in pages (activity or fragment). To achieve this, we rely on RecyclerView . A LegoAdapter (implementation of RecyclerView.Adapter ) takes a set of bricks (any bricks combined in any way) as argument to render in a RecyclerView. The LegoAdapter takes advantage of DiffUtil in order to update required views only. Therefore, building a page simply consists in retrieving the data, creating the brick set and passing it to the LegoAdapter. This process makes new feature development easier, faster and more reliable for it is easily testable. We had (and still have) to adapt our organization in order to tackle a number of issues with this architecture, especially as android developers are split in different teams: Who approves and who implements component changes? Who handles and fixes bugs? How to release changes on components? What to do when a component does not exactly fit for a specific case? Should it be modified? Should a new component be created instead? Another thing is that setting up a design system takes time, all the more for a long existing application like Deezer. Hence we have to proceed in stages, i.e. implement the Lego Brick architecture for new or revamped features first, and progressively extend it to the entire application. This implies maintaining a legacy and a new architecture for some time, which can appear cumbersome. Anyway, these are some of the challenges we are currently taking up! Our efforts are paying off though: we can already see the results and benefits when developing new features, making changes on components that impact the entire application, and coordinating with designers on major design changes. Establishing a design system clearly helped us optimize our design and development processes with better collaboration and a shared vocabulary, and build a more consistent application, thus improving the user experience. www.deezerjobs.com Stories of engineering, data, product and design teams… 518 4 Thanks to Arthur Guibert and Syrine Trabelsi . Design Android Android App Development Uikit Deezer 518 claps 518 4 Written by Android Engineer @Deezer Stories of engineering, data, product and design teams building the future of music streaming Written by Android Engineer @Deezer Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-07-05"},
{"website": "Deezer", "title": "hack as you are story of a hackathon at deezer", "author": ["Guillaume Grillat"], "link": "https://deezer.io/hack-as-you-are-story-of-a-hackathon-at-deezer-10ae0b6199ae", "abstract": "Previous stories About Jobs My role as a Tech Community Ambassador is basically to create “learning experiences” for my engineering team, and hackathons are part of the process. Today, more and more employees decide to work for a company not only to climb the career ladder or for financial reasons, but in order to gain knowledge. At Deezer, we strongly believe that our people need to feel they are continuously learning on their professional journey, not only by doing but also by meeting experts from a variety of fields. Taking part in an internal hackathon and meeting co-workers from other departments with whom you don’t have the opportunity to work on a daily basis, is extremely valuable for anyone in our company. It is also a first step before getting in touch with external communities. For a few years now, we have been organizing internal hackathons at our Parisian headquarters. These happen over two days, twice a year. They have become tradition and if you are part of the Deezer team — whatever your department — you are given the opportunity to work on a project of your choice and assemble a multidisciplinary team around you (one idea, one technical solution, one graphic identity imagined and developed by colleagues with various skills). Our only rule has been that all participants should challenge our Deezer product and its ecosystem — in any manner. Our core values for each hackathon are the following: JOIN : Anyone working at Deezer is welcome to participate, no matter their area of competence. TRY : Just try it, dare to tackle challenges, don’t be afraid to fail. BUILD : Believe in better, work on something new that has never been seen before. ENJOY : These are 2 days outside your everyday work, so have a good time! LEARN : Learn by doing and exchange ideas with co-workers you may not have met before. SHARE : Present your work in front of colleagues or a jury — you will always gain constructive feedback. So far, we have tended to give teams as much autonomy as possible, allowing them to organize their time as they like before and during the hackathon. At the end of the two days, all of the project teams — generally about 30 teams of varying sizes (1–10 people) — would pitch their work in front of a jury of managers, and 3 projects would be awarded: “Most Business-ready”, “Most Innovative” and “Most Fun”. We have received good feedback on our past events and our co-workers have always been happy to take some time to step back, think about the Deezer product and experience, and to suggest how we could improve. For various reasons, however, some promising innovations have not been given as much attention as they deserved and others have unfortunately not seen the light of day — at least, not outside of the company. Of course it’s always fun to spend two days hacking with your co-workers, but it would be a missed opportunity if we didn’t encourage our teams to follow through with their more promising ideas. Therefore it seemed necessary to challenge and redefine our way of organizing our internal hackathons in order to better highlight our teams’ work. Also, after organizing and participating in several internal hackathons at Deezer, there are a few things I have noticed: Each hackathon needs to have its own identity : people are more likely to engage in a hackathon if your event stands out. For the last few hack sessions I tried to find a catchy tagline and create a dedicated visual that everyone could relate to. Clearly communicating the goal and organization of the hackathon ensures that people can prepare properly and that the event goes smoothly — the sooner, the better . Limiting constraints to a minimal set of rules is the key to success : people are a lot more productive and efficient when they feel free to be creative and when nothing is off limits. For our hackathon last February, we brought in 2 robots ( Nao & Pepper ). Even though it sounded pretty cool, the final result — i.e. enabling the robots to play Deezer through voice control — was not so innovative. The proposition was probably too obvious, and so not challenging enough. “We eat as we work” (French proverb from the Franche-Comté region): last but not least, it is crucial to offer participants the best hacking conditions (and quality food is compulsory!) These observations helped us revamp our concept and deliver a new format that would hopefully better suit our teams’ initiatives, foster innovation and improve our product. So we organized our latest hackathon last December with a new formula: We decided to use a grunge band — Nirvana — renowned for never accepting the music business rules as inspiration for our fifth Hack Session. We hijacked the famous track “Come as you are” and turned it into “Hack as you are”. The formulation fits particularly well with our spirit — always challenge our product , and some of our internal values — Just hack it & Believe in better . We first teased our colleagues with a few mysterious posters inviting them to save the date, then the first official message announcing the hackathon was sent in October — more than 2 months before D-Day. Expectations were high so people were very enthusiastic! For the first time we also changed the way we organized the hackathon. We tried out new rules with both positive and negative results: 1. A 3-week deadline Come up with an idea and share it on a registration form before it’s too late! Pros: By setting a time limit to register, we encouraged people to flesh out their project (with details on the idea and people to work on it) quickly. Cons: It was quite a fast turnaround and some people who were on vacation at the time missed the boat…but the most motivated people anticipated it! 2. A proper mixed and multi-skilled jury The jury pre-selected 10 projects before the hackathon. It consisted of 2 women and 4 men (Brand Manager, Head of Business Intelligence & Data Analytics, VP Design, VP Product, CTO and Chief Data & Research Officer) and for the first time ensured a reasonable balance between tech and non-tech managers. Pros: With each member of the jury bringing their own skills and experiences to the table (in product, engineering, data, design, etc.), it was the perfect composition to coach and strengthen the 10 pre-selected teams before D-Day, and also challenge their project. Cons: We encountered some resistance at first. People felt such a jury would hinder potential innovations. It was also unclear to them whether they could participate in the hackathon if they had not been pre-selected. 3. A ‘pirate’ mode for non-selected projects Teams that were not pre-selected were still able to carry out their project and organize their own work, without having to pitch their projects. Pros: Not having a deadline or having to pitch allowed people to enjoy the hack and work on their project without worrying about if they had put together a team with the right skillset. Cons: The ‘pirate’ mode can be viewed as a second-class option because teams don’t get to pitch their project in front of the jury and don’t benefit from their coaching either. 4. A new voting system In total, the jury voted 3 times: before the hack: a first vote to pre-select 10 projects; right after pitches: a second vote to rank their favorite projects (from 1 to 10); after the hackathon: a third and final vote to select the teams that will get extra time to deliver a production-ready project. Pros: Voting anonymously means that every member of the jury feels free to vote however they choose. It is also a good way to keep them involved before, during and after the hackathon. Last but not least, this system allows us to easily identify relevant trends, and by extension, to improve and refine our roadmap. Cons: Voting three times can be too repetitive and it is quite time-consuming to organize the votes well, especially the first and second times. 5. Estimation of the time-to-production When the hackathon is over and before the last voting session, each team must estimate the extra time they need to deliver a finished feature / innovation: S (Small): less than 1 sprint (15 days) M (Medium): less than 2 sprints (<1 month) L (Large): more than 2 sprints (>1 month) Pros: “Time is money” and time is key to identify strategic, innovative projects. This step also empowers teams to organize their work and pushes them to be realistic about the time that they have. Finally, it’s a very decisive metric that eases time allocation. Cons: Pragmatism is often frustrating and it is sometimes hard to accept that some really cool projects are incompatible with our strategy and roadmap. 6. No more categories We removed the three prize categories (“Most Business-ready”, “Most Innovative” and “Most Fun”) and decided instead to offer a unique possibility to earn extra-time to finalize the winning projects. Pros: All kinds of projects stand a chance and a single ranking shows a vision and trends, which has a greater impact on teams. Cons: There is no longer an awards ceremony at the end of the hackathon…but the true satisfaction will come from the public launch of hackathon projects! When we announced our new rules in November through internal communications, there were mixed reactions. This was mainly because some people thought that pre-selecting 10 projects would prevent other projects from taking part in the hackathon. Our main goal was to give 10 teams the opportunity to strengthen their team and put their concept to the test, but by advertising this and focusing on the 10 pre-selected teams, we overlooked communication on the projects that were not selected and didn’t properly mention that they would still be able to work on their project on their own. This meant that we had to clarify the situation very quickly through internal communications and direct face-to-face meetings with competing teams… Things eventually fell into place. In the end, hackathons are all about learning by meeting people and doing stuff with them. It’s a win-win situation for your co-workers and your product, and my role is to ensure that all the stakeholders feel that way and enjoy every step of the journey. In order to do this, I learnt that it is crucial to be well-organized, communicate effectively with the teams and to follow up on projects. To conclude, here is Hack-session #5 in facts and figures: 39 projects drafted 16 projects finalized, involving 83 participants 2 projects included in the roadmap for Q1 2019 www.deezerjobs.com Stories of engineering, data, product and design teams… 110 Hackathons Tech Events Engineering Culture Deezer 110 claps 110 Written by Tech Community Ambassador @leboncoinEng. My goal ? Helping my teams to flourish at work, by meeting exciting & innovative people & gaining new knowledge. Stories of engineering, data, product and design teams building the future of music streaming Written by Tech Community Ambassador @leboncoinEng. My goal ? Helping my teams to flourish at work, by meeting exciting & innovative people & gaining new knowledge. Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-03-01"},
{"website": "Deezer", "title": "hamr ismir2018 deezer hq", "author": ["Manuel Moussallam"], "link": "https://deezer.io/hamr-ismir2018-deezer-hq-598203e42982", "abstract": "Previous stories About Jobs We hosted the 2018 ISMIR edition of the HAMR Hackathon . It was awesome, people from all over the world collaborated on fun and innovative research topics during two days. A lot of very cool projects emerged. Most of them won’t stick but some will, and that -you have to admit- is the beauty of it! Music Information Retrieval (a.k.a. MIR) is a research field that overlaps various scientific disciplines such as signal processing, machine learning, computer science but also musicology, psychology, acoustics etc. Roughly speaking, MIR researchers are a community of curious people interested in dissecting music. They try to detect notes in audio files, transcribe lyrics, separate the drum part from the other instruments, estimate keys, chords, tempos, synthesize new sounds or model the styles of classical composers, among other things. Every year the community gathers for its ritual scientific conference called ISMIR. This year it was held in Paris and co-organized by Ircam and Telecom ParisTech . At Deezer we do some MIR too, especially in the R&D team. Actually this year we had three papers accepted at ISMIR. One on estimating the mood of a track using its audio content and its associated lyrics , another on disambiguating artists using their music and a third one on o vercoming musical genre ambiguities through audio analysis . Cool stuff, but let’s save that for another post. For some years now, Colin Raffel and friends have organized hackathons called HAMR for Hacking Audio and Music Research. Since 2014, special HAMRs have been organized in conjonction with ISMIR. It turns out that we are super friends with this year’s general chairs of ISMIR so when they asked us if we wanted to host a hackathon as a satellite event we said yes right away. After reaching out to Colin, we agreed to co-organize HAMR@ISMIR2018 at Deezer HQ. This is what took place on September 21–22. Unlike other hackathons such as MusicHackDays, the goal of HAMR is to trigger emulation between MIR researchers and have them work on new research ideas. During this 2-day event, participants are encouraged to identify these uncharted MIR territories, form teams and start exploring. It may seem a highly ambitious project but the purpose is more the exploration itself than the results. Popular vote actually designates three winning teams for Best Research Idea, Best Code and Best Documentation. Participants are encouraged to draft a scientific document describing their idea and how far they’ve gone exploring it. Participants are mostly ISMIR attendants, i.e. either young researchers, PhD students, engineers or even some respected professors . They came from France, Germany, South Korea, Brazil, Spain, UK, Finland, Austria, US, Japan, Netherlands, Norway and probably from a few other places. We welcomed 50 people in total in our Paris office. Well, it went well according to pretty much everyone I’ve talked to! The food was very good -as expected from a Paris held event- and I must say I was positively surprised by how easily people talked to each other, formed teams and how efficient they were in their collaboration. We invited Hash Riaz from Abbey Road as an event partner and observer and he shared our opinion. We tried to render the nice, productive atmosphere in the following video. First of all, all the code and documentation that were produced are accessible on our github page so you should definitely go there to have a look. Now in details, the award for Best Code went to Chris Trailie and his FaceJam project ! Chris combined audio analysis and image manipulation techniques to animate still pictures of faces according to the rhythm and the timbral landscape of the music. Go check the video, it’s quite.. something! Best documentation went to the Neural Bubble Beat project . The team consisted of people from Linz and Wien universities and aimed at using a physical model to visualize inner activations of neural networks fed with music. The idea is quite complex because it requires to synchronize 3 different networks respectively detecting the downbeat, a drum transcript and timbral features of the music. The result is quite convincing when you think that what you see actually comes from the true activations of the networks. The physical model they use to animate the bubbles is a great idea, giving the whole visualization a very natural feeling. Best Research Idea was awarded to the Meter Anomaly project that tackles a very annoying phenomenon arising in beat tracking systems, namely that some songs have signature changes that mess up traditional algorithm outputs. To overcome this, the team proposed to compute several self-similarity matrices with various phase shifts and use them to detect consistent segments and unusual bars: A special prize for the most interesting topic for music streaming services was awarded to a group of four just-started PhD students from all over the world. They addressed the issue of describing music consumer profiles not by using traditional music similarity schemes but by learning an embedding space of user tastes. A core idea is then to focus on learning the variance of a user profile in such space, which gives insights not only on what kind of music a user like, but also on how far from it he can wander in his musical explorations. Prior to organizing the event, we talked to Abbey Road studios in London who were very interested in attending HAMR and took the opportunity to promote their own upcoming Abbey Road Red Hackathon that will take place in the mythical Studio 1 on November 10–11. Hashim awarded one Golden Ticket to one of the participants who will be able to travel to London and attend the event! Thanks to all the participants! It was a great experience and I hope you enjoyed it as much as we did. See you next year at ISMIR and hopefully some day back in Paris, at Deezer. www.deezerjobs.com Stories of engineering, data, product and design teams… 41 Hackathons Machine Learning Mir Deezer Research And Development 41 claps 41 Written by Deezer Research. Audio Signal Processing and Machine learning for Music. Stories of engineering, data, product and design teams building the future of music streaming Written by Deezer Research. Audio Signal Processing and Machine learning for Music. Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-10-24"},
{"website": "Deezer", "title": "deezer data data works summit berlin 2018", "author": ["Pierre Thary"], "link": "https://deezer.io/deezer-data-data-works-summit-berlin-2018-fcee27432f26", "abstract": "Previous stories About Jobs co-authored by Hugo Sempere Shows and conferences about the fast-evolving data technologies follow one after the other and yet each one is so unique! No year passes by without any new challenge to be discussed within the active and passionate data community that Deezer is proud to be part of. This year’s Data Works Summit didn’t disappoint, with subjects like the incoming GDPR, the necessity to keep a clear and clean data catalog, portability across an ever-growing number of languages and frameworks and speed improvements! Some of our highlights were: The technologies empowering companies to handle data at scale have recently reached a new milestone. Indeed, the recent GDPR raises the bar in protecting and giving more control over personal data. Since our company only uses data to provide you with the best music streaming experience, and pay our distributors with a fair amount of royalties, there’s no reason we might have trouble applying GDPR’s guidelines. Apache Ranger and Apache Atlas are two projects that help in achieving these data governance requirements. Apache Ranger is a tool that plugs on your Hadoop stack and provides you with a centralised security administration. It provides fine grained authorisation control user-and location-wise over your data lake. Apache Atlas is another Hadoop framework that also works closely with Ranger to better manage metadata across the data catalog. It basically helps in classifying, organising and making sense of the company data assets. However, it still takes time when it comes to modifying legacy software to be compliant, as shows this interesting survey of the audience showing how prepared companies are for this change: Enza Iannopollo and Bernard Marr both emphasized the opportunities that GDPR can bring to businesses which would, according to them, greatly outweigh its constraints. You can watch their keynotes here . When it comes to developing Machine Learning algorithms and putting them in production at high scale, two states of mind fight against each other: A development phase requires flexibility, a good amount of trial-errors, and the ability to adapt quickly to feedbacks Algorithms put at high scale production requires a very rigorous and thorough upfront design and accepts no errors: this inevitably causes some rigidity after it is done As our models may decay over time we might have to transition between the two states multiple times. Nick introduced the Portable Format for Analytics (PFA) that aims at helping to “smooth the transition from development to production”. It provides a JSON description of any model that could be read by any language and any framework. Hence, it separates the production pipeline concerns from the model development, and removes environment constraints slowing down engineers in the process. The summit was also an opportunity for Apache committers to present the new features in Apache Spark 2.3. UDFs is a very useful feature of PySpark that enables user to have the flexibility of defining their custom operations. However, UDFs were previously computed one row by one row (for the Python interface of Spark), making it a slow process and preventing its use in the most challenging environments. Spark 2.3 introduces batch UDFs computation which greatly improves the performance of these operations! More info here . Putting simple Machine Learning models in production in our data processing workflows is quite smooth thanks to the handy Spark ML library. However, working with more complicated models such as neural networks for image classification was still complicated due to the requirement of external libraries and the lack of Image simple representation in Spark. The latter is solved in the 2.3 version. The related and interesting JIRA ticket can be found here . For more info on Apache Spark 2.3’s new features, check out Databricks’ blog . Wangda Tan and Billie Rinaldi, Apache committers, gave an insightful view of Yarn majors improvements over the past year: Default ordering policy is FIFO but now queue can have priorities among others: Since 2.8.X a new system called ReservationSystem can reserve resources ahead of time, to ensure that certains critical jobs have enough resources to be launched. Check out hadoop documentation for more info. A lot of work was made in Yarn to speed up containers allocation, and it can now allocate up to 3k containers per second. Yarn can now use specific profile and handle GPUs and FPGAs resources. More info on GPU support More info on FGPA support From Hadoop 3.1, it is possible to run docker containers on YARN! This new feature unleash a lot of new possibilities, such as deploying spark jobs with all needed libraries without installing anything on cluster or orchestrating stateless distributed applications. Now back to work to put this brilliant stuff into practice… And looking forward to the next European edition! www.deezerjobs.com Stories of engineering, data, product and design teams… 61 Hadoop Machine Learning Spark Data Gdpr 61 claps 61 Written by Presenting things makes me fully understand them: if you like it, it’s a win-win ! ;) Stories of engineering, data, product and design teams building the future of music streaming Written by Presenting things makes me fully understand them: if you like it, it’s a win-win ! ;) Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-08-31"},
{"website": "Deezer", "title": "ismir 2018 hamr deezer hq", "author": ["Romain Lods"], "link": "https://deezer.io/ismir-2018-hamr-deezer-hq-a00e9d690b0f", "abstract": "Previous stories About Jobs In 2018, the September rally is pretty exciting :) For the first time in its history, Deezer is really proud to be partnering with ISMIR , the world’s leading research forum on processing, searching, organizing and accessing music-related data. Our Engineering team based in Paris will this year host the HAMR (Hacking Audio & Music Research) hackathon on September 21 & 22. We can’t wait to meet you guys, and discover your innovative projects… See you next week ✌ Stories of engineering, data, product and design teams… 3 Hackathons Research Engineering 3 claps 3 Written by Tech, Innovation and Music Lover | Working at @Deezer @DeezerDevs @DeezerFrance. Stories of engineering, data, product and design teams building the future of music streaming Written by Tech, Innovation and Music Lover | Working at @Deezer @DeezerDevs @DeezerFrance. Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-10-18"},
{"website": "Deezer", "title": "akka as a bridge between rest and kafka", "author": ["Mario Cornejo"], "link": "https://deezer.io/akka-as-a-bridge-between-rest-and-kafka-acfe6194c202", "abstract": "Previous stories About Jobs co-authored by Huikan Xiang At Deezer, we love to play different chords, and to play with different technologies. Here we will present a solution to the problem of receiving millions of messages per day from a REST endpoint, processing them and pushing them back to Kafka. Inspired by The Reactive Manifesto , we decided to use Akka , which perfectly follows the four principles of reactive systems: Responsive, Elastic, Resilient and Message-Driven. Data-Core is the team responsible for handling the big data at Deezer and we collaborate closely with different teams to improve their processes related to data ingestion. One of the teams we work with wanted to improve the way we follow our communication and marketing campaigns. At Deezer, we use a third-party emailing platform that helps us generate reports and statistics for the different campaigns. Once an email campaign is launched, each email goes through different statuses over the period: they can be delivered , opened or clicked among others. All these events are analyzed in real-time and batched by the CRM team. The third-party emailing platform pushes the information of the campaigns to our REST API by sending a POST request with a JSON in the body. From our side, we handle, clean, process and store every message so that the CRM team can easily use them afterwards. While designing the API, we took into consideration that our application has to be always available and guarantee that no messages will be lost. Another aspect to consider is that the amount of API calls we will receive from our third-party is not constant. In fact it is highly variable per day and per second as shown in Fig. 1 and Fig. 2. Considering these bursts of requests, the application should be able to handle those peaks without losing messages while keeping response times low. Based on all those requirements, we decided to implement the application using Akka to benefit from the HTTP module to create a high-performance REST API, the Actor module to make the pipeline of transformations and validations easier and the Persistence module to keep the state of the system in case of crashes and other systems’ downtimes. Akka is a palindrome of the letters A and K as in Actor Kernel, from which it is inspired. The Actor model has been brought into Akka for its high efficiency and easy-to-duplicate characteristics. An Actor can be seen as a thread that is responsible for an atomic operation. The Akka application is presented in blue in the Fig. 3. The application is quite simple, we split the system in three actor roles. The first one is the Reception Actor that receives all HTTP POST requests and extracts the body. The second actor is the Kafka Producer Actor that receives messages from Reception Actor, constructs a valid Kafka record and sends it to Kafka. It also monitors the Kafka status. Finally, we defined a third actor that plays the role of a buffer in case Kafka is not available. The buffer is a persistent storage for disaster recovery (discussed later), which is an independent component to resume the system during failures and to achieve the zero-loss goal. Each actor is defined as a “round robin” pool of actors that can be automatically resized depending on the load. If the amount of messages is low, actors are removed, and if the amount of messages is high, more actors are added so that the application is capable of handling bursts of messages. One of the characteristics of Akka is that each actor has a very small footprint. It can handle around 2 million actors per gigabyte of heap. One of the four principles in a reactive system is resilience and it means that when there is a failure, the system should stay responsive. In our circumstances, the system depends on two external components: the emailing platform and our Kafka cluster. Here we focus on discussing a potential Kafka cluster failure since it’s in the Deezer production scope. In the system architecture diagram (Fig. 4), there is a Buffer Actor responsible for taking over the message ingestion in case Kafka is down. The dotted line between Buffer Actor and Kafka Producer Actor signifies its standby role. The flowchart above (Fig. 5) illustrates how the Buffer Actor activates when the connection with Kafka is down (red X), and how the Kafka Producer Actor resumes after Kafka came back (black arrow). In this case, Record 1 is the ideal situation, in which we get an acknowledgement (with a Kafka offset metadata inside) back from Kafka after we successfully pushed a message. In the unlikely event of a failure in our Kafka cluster, we might have a fatal error or just no reply until a timeout is reached. This is depicted with Record 2, which is forwarded to the Buffer Actor whose job is to persist the Record for a limited time using its own persistence layer. Every message that failed to be transmitted to Kafka is stored in this buffer. As soon as the downtime is over, all new messages will be transmitted in the same manner as Record 1. In regular time intervals, the Kafka Producer Actor will ask the Buffer Actor if there is any message in the buffer. If there is any, the Kafka Producer Actor will retrieve them and try to send them again to Kafka. Deploying an Akka Application is dead easy. After getting some pieces of advice from our friends from the Infra Team , we decided to work with a Docker container and deploy the application in our Kubernetes cluster . In our project, we use SBT as the build tool, which has a plugin called SBT Native Packager . This plugin offers a very simple way to build an Akka application in many formats and in particular simplifies the creation of a docker image. This comes as a very handy tool since it configures the Akka HTTP server as an entry point listening on the chosen port. The current pipeline is easy: we push our code to our repository, our Jenkins service builds the docker image and it is automatically deployed in our K8s cluster ensuring high availability. For our requirements, we allocated 1 container with 512 MB of RAM and 2 CPUs . We tested our application in a development environment, which is limited in network bandwidth but sufficient to verify the throughput and behavior of the application. To test our Application we use Apache JMeter to simulate many HTTP requests. When the application is idle (ie. no requests but waiting for connections), the CPU consumption is around 0.001–0.002 and the memory consumption is flat at 129 MB. With our simple configuration, we achieved a throughput of ~10,000 messages per second (Fig. 6) with a very low error rate of ~0.5%. An interesting point is that it is possible to see some spikes in the CPU usage in the ‘CPU Consumption over time’ graph. This is because we allowed our container to have short bursts in our Kubernetes cluster configuration. We also tried a different cluster configuration: we added two containers to test high availability and we had a linear increase in the throughput (20,000 messages per second). Based on our results, being able to handle 10,000 messages per second with one container of 512 MB of RAM and 2 CPUs is vastly enough for now . Akka is fun to code, so fun that we are already planning to replace another piece of our stack: a module that consumes logs from Kafka and ingests them directly to HDFS. Stay tuned! www.deezerjobs.com Stories of engineering, data, product and design teams… 57 1 Akka Kafka Rest Api Kubernetes Deezer 57 claps 57 1 Written by Computer Scientist and Functional Programmer. Interested in concurrency and parallelism, complexity and Turing machines. PhD in Cryptography from ENS Paris. Stories of engineering, data, product and design teams building the future of music streaming Written by Computer Scientist and Functional Programmer. Interested in concurrency and parallelism, complexity and Turing machines. PhD in Cryptography from ENS Paris. Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-09-06"},
{"website": "Deezer", "title": "musicathon lets rock it innovation", "author": ["Mickaël A"], "link": "https://deezer.io/musicathon-lets-rock-it-innovation-a29ecec44bdc", "abstract": "Previous stories About Jobs I am a research software engineer in the Deezer R&D team and happen to be in Germany for a three-month experience in our Berlin office. My work within the R&D team consists in making all the necessary data conveniently available for various experiments involving machine learning or natural language understanding. I focus on matching entities from one database to another , and I rarely miss an opportunity to have fun in hackathons, like this one , or this one , or this one . So when I saw the Eventbrite event about the Musicathon, I didn’t hesitate to register. Musicathon is a hackathon — about music! Hosted by Universal Music in Berlin, it ran on April 19th and 20th and brought together 50 happy hackers. The event took place in the Porsche Digital Lab in a pristine location just by the Spree river, in front of the Molecule Man , a 45 ton aluminum sculpture symbolizing the encounter between three neighborhoods of Berlin: Treptow, Kreuzberg and Friedrichshain. The first day started with an ice-breaker. We had to build a structure with raw spaghetti that could support a marshmallow, in less than 3 minutes. In our random team of six, we quickly brainstormed and tried our best to make the highest construction possible. It unfortunately failed… but we nonetheless knew a bit more about our teammates. After the introduction of the members of the jury, Anton (Marketing Intern at Deezer Berlin and fresh hackathon enthusiast) and myself went on stage to talk about our idea. It was not a tightly-defined idea but rather a list of thoughts around the Amazon Echo device. Although Deezer is already available on Alexa , we wanted to get our hands dirty and attach external capabilities to a new Skill. We managed to gather a group of seven people and immediately started to brainstorm and narrow our ideas to one realistic project. To this end, Michael from Amazon (and a jury member as well) gave us some input to help us decide. For instance, he confirmed there is a way to retrieve a textual version of a user talk (from speech-to-text ) and that we can retrieve a slot with no limited content, like an “open text” rather than a value in a list. In the end, our project led to recommending music based on a voice prompt, possibly without any musical clue. The user would say something about its day and the device would recommend a Deezer song accordingly. Then, we got to work. On the first day, we struggled a lot with testing our Skill in the Alexa Skill Center. I found other people experiencing the same issue in a forum post. It ended around 11pm with the bitter feeling of an unresolvable issue. Fortunately, the day after went better and we could finally make our first Skill work from a new Amazon account! We plugged the Watson API for emotion detection and the Deezer API for music search. During the whole hackathon, free drinks (including the unavoidable Club Maté) and excellent food were provided. A huge cymbal extracted us from concentrated work every six hours or so to give us general information, such as “Lunch!” Finally, the last cymbal hit resonated in the lab at 6pm on Friday. “Presentation time!” At the same moment, our project stopped working. Oh no! We ended up with the issue we encountered the day before. We applied the workaround found in the morning as fast as possible. It worked! We could safely (well… with some glitches) present our work. The ideas from other teams were very diverse, going from blockchain pre-payment to dynamic digital headphone labeling. Even with Universal’s promised awards going up to 3,000€, we did not find the atmosphere competitive at all. Participants from a various range of backgrounds showed up, from students to marketing people, from Brazil to Israel and India, all with one interest in common: music. A BBQ was served while the jury deliberated. Eating on the terrace was undoubtedly the most relaxing moment of the event, far away from the stress of the last 48 hours. It was a good time to get to know other hackers better. The jury came back one hour later, apparently exhausted. To our great surprise and satisfaction, our team got one of two third prizes! Our hack caught the jury’s interest as well as many other projects. This is the reason why they doubled the third prize. A playlist generator from two distinct profiles and an automatic web scraping tool got the second and first prize respectively. In a cheerful and enthusiastic atmosphere, we picked up our limited-edition Musicathon vinyl, along with a 500€ voucher to share with all the team members. The DJ started to play music for the last hours in the Digital Lab. We said goodbye to our ephemeral team and promised to keep in touch. www.deezerjobs.com Stories of engineering, data, product and design teams… 48 Hackathons Deezer Voice Control Alexa Skills Research And Development 48 claps 48 Written by CTO of Vialma, a classical and jazz streaming platform. Triathlon enthousiast. Classical music nerd. Stories of engineering, data, product and design teams building the future of music streaming Written by CTO of Vialma, a classical and jazz streaming platform. Triathlon enthousiast. Classical music nerd. Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-05-22"},
{"website": "Deezer", "title": "visualize your music dna with data", "author": ["Pierre Thary"], "link": "https://deezer.io/visualize-your-music-dna-with-data-b1f1b6631b98", "abstract": "Previous stories About Jobs At Deezer, we take the world of music and make it personal. That’s why we designed Flow , your personal soundtrack, and a dynamic home page to help you find your perfect tracks in no time. But does personal content only revolve around music recommendation ? Of course not! This article will show you how your listening habits can picture your unique music DNA. In other words, we will here detail a simple process that transforms your listening history into a personalized artwork . No more suspense, let’s get into it ! Let’s retrieve your listening history of the last 100 days. Nothing that the recent GDPR wouldn’t allow though! Just the precise date, genre of the songs listened to and how many times you’ve listened to the same song/artist. — Wait, this is my music DNA?! Of course! It’s still hidden in the table though, so the next steps will shape it in a more intuitive manner . How to shape this table into something more recognisably DNA-y? Well, there are millions of possibilities! But for now, this simple recipe will do: First, we need to put a body to those intangible listens! Hmmm ‘kay, keeping the same colour for all the bubbles is a bit boring, isn’t it? Let’s colour them according to the genre of the listen then: Better! Now the size of the bubbles is still not particularly clear. What if we resized them according to how much you’ve listened to the same content? Eventually, we’ll need to position the bubbles by following some simple rules. No room for randomness here, we are telling your story! Straight timelines are intuitive, yet a bit too academic, so we chose to display the bubbles on an simple clock: Voilà! I am pretty sure that at this point you are already imagining what it could look like. Let’s simply add some shiny post-prod glitter to it and output your personalized artwork! An example : A collection of generated artworks: This project was made in the context of Deezer’s 2018 internal hackathon. I was inspired by this very nice article . We used the processing library to make the script that automates the artwork’s creation, after data were extracted from our cluster with Apache Spark. processing.org (Do you know other libraries that could have worked as well? Let us know in the ‘Comments’ section!) What did we miss? Do you have any other recipe ideas? You can play with the toy code we made, which I uploaded on my personal github . Please let us know what do you think in the ‘Comments’ section! Hope you liked it! www.deezerjobs.com Stories of engineering, data, product and design teams… 671 Data Visualization Apache Spark Data Deezer Hackathons 671 claps 671 Written by Presenting things makes me fully understand them: if you like it, it’s a win-win ! ;) Stories of engineering, data, product and design teams building the future of music streaming Written by Presenting things makes me fully understand them: if you like it, it’s a win-win ! ;) Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-01-19"},
{"website": "Deezer", "title": "fosdem 2018", "author": ["Lamia Caliati"], "link": "https://deezer.io/fosdem-2018-d2edf86cf06a", "abstract": "Previous stories About Jobs FOSDEM (Free and Open-source Software Developer’s European Meeting) is one of the largest European open-source and non-commercial conference, taking place annually at the Université Libre de Bruxelles. This year it offered more than 650 lectures presented by 600 speakers on 50 different themes. It’s a real place to share and learn about open-source. In keeping with the ideals of open-source, FOSDEM is an open and free event, with no registration needed and no entry fees, which allows more than 4,000 people to attend each year. It’s mostly funded by sponsors and sales of goodies during the weekend. Most organizers are passionate volunteers, I’ll let you imagine how many there are! I had the chance to attend the FOSDEM event for the first time this year, with 8 other Deezer folks from different teams (Core Infra, Core Data and Engineering Effectiveness). The more, the merrier, right? It was exciting to attend the event, all the more when I saw all the many different tracks and lectures. When we landed on Friday, we charged around Brussels’ city center on a whistlestop tour (some of us also tested the famous Belgian beers, for research purposes). We decided to make an approximate draft schedule for our stay and bookmarked the presentations we wished to attend using the FOSDEM app. To make the most of the event we focused on several subjects we are working on or want to work on in Deezer infrastructure . I set my priorities on subjects like containers and databases so mainly on lectures about Kubernetes and MYSQL . We also wanted to see some presentations on different tools that help us manage our Deezer infrastructure such as Prometheus monitoring, debug tools like strace, dtrace and Grafana for dashboards, etc. On Saturday morning we started with the first meeting “Welcome to FOSDEM 2018” as an introduction in the biggest, fully packed amphitheater of the university. It was the beginning of a super two-day knowledge marathon surrounded by passionate people happy to share and learn. I liked that feeling to be a student again in university amphitheaters with the rush to join the “class” before the bell rings and the importance of taking notes to remember the important information among all that you hear and see. At FOSDEM everything goes fast. The format of the lectures is short, with most of them under 30 minutes so you don’t want to miss the start. It’s quite a nice way to attend many different lectures but it can also be a bit frustrating sometimes when the subject is very interesting or when you’re stuck in the middle of a crowd, trying to reach the room. Fortunately we were able to attend most of the lectures we were interested in and gleaned lots of information on the subjects that are important for us to improve our infrastructure at Deezer. We were able to learn more details about certain tools (ProxySQL, MySQL InnoDB cluster, etc.) to help us with our upcoming study on our MYSQL databases fault tolerance and performance, which are very important aspects of our infrastructure at Deezer. We are secure with our actual processes on MYSQL (failover, replications, backups) but our challenge is to always improve our infrastructure, just like we did with our work on memcache fault tolerance with mcrouter. Tools like ProxySQL could help us improve reliability by executing more graceful switchover and failover without breaking transactions as they understand the MYSQL protocol and will act accordingly, which other proxy layers like HaProxy can’t do. Another interesting feature in such tools are the dynamic query routing (for example, forwarding queries to specific Mysql backend based on the query type) and possibly the query caching part that could help better optimise databases performances. We also went to lectures on Kubernetes, which is a big topic we started to work on with the project “Infrastructure as Music” . This powerful container orchestration tool is open-source and originally developed by Google. It will ease our infrastructure management, as well as our developers’ work, and help operate more easily with working as microservices at Deezer. Kubernetes impacts different fields of our infrastructure (network, security, code deployment, dependency between dev and infra teams ) so learning about its various topics was very helpful to orientate our work. Finally we attended lectures on many different tools we already use to manage our infrastructure on a daily basis. We learned about the new features offered by Grafana ’s new version (5.0), which is now ready to be used by our different tech teams, and also on Prometheus monitoring. At Deezer, Grafana dashboards are used by almost all the tech teams. It can be simple dashboards with server resources information but also application errors (sql, php, etc.), web monitoring (network traffic, memcache, mcrouter, number of connections and requests, adn/cdn, etc.), database monitoring (replication, slow queries, threads, etc.), specific content metrics (encoding, publication, fraud, streams), Kafka bus message metrics (consumers offset/lag, brokers metrics, topics, etc.) and many more. We use different data sources such as Elasticsearch, InfluxDB but mostly Prometheus (v2.2), which is our main monitoring tool (working with consul and node-exporter). With Prometheus at Deezer, we currently have over 2.3 million time series, scraping around 60k samples per second and 300 servers are monitored. You can refer to the meetup we held on Prometheus with Antoine Leroyer to get more details ( video & slides ). These tools among others allow our tech teams to have an overview of Deezer’s infrastructure and services and to be proactive when necessary. FOSDEM 2018 was a rich weekend! I can’t talk in details about all the topics we covered and all that we shared or learned but it was a really great experience to live with my teammates (Cédric, Antoine, Luc, Alexis, André, Alexis, Benjamin and Romain) and I recommend it. A good surprise also was the meeting about the Unix architecture evolution from 1970 to 2018, which I didn’t plan to attend at first. It was an amazing reminder of all the work done on its architecture and how it all started. The organisation behind events like FOSDEM must be quite heavy and we thank all their teams for the hard work. If you wish to attend the FOSDEM, preparation is key! The place is huge and composed of many different buildings. All the lectures you are interested in will probably not take place in the same building and it can take up to 10 mins sometimes to reach a room. To avoid the disappointment of missing some lectures, try to plan your schedule accordingly. Given the huge crowd, it also happens that the room is full and you can’t attend. The official website contains a lot of information to help you and an application is also available. A nice feature in the app is the ability to bookmark each lecture you may be interested in, so that you can view your own schedule for the two days in the “Bookmarks” tab. And, most of all, be prepared to learn a lot, enjoy yourself and don’t miss the belgian fries and waffles, you can’t leave without tasting them! www.deezerjobs.com Stories of engineering, data, product and design teams… 39 Kubernetes Grafana MySQL Prometheus Infrastructure 39 claps 39 Written by Infrastructure Engineer/SRE @ Deezer Stories of engineering, data, product and design teams building the future of music streaming Written by Infrastructure Engineer/SRE @ Deezer Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-04-05"},
{"website": "Deezer", "title": "what the bug our method to detect outliers", "author": ["Paul Peseux"], "link": "https://deezer.io/what-the-bug-our-method-to-detect-outliers-3dbf623774c8", "abstract": "Previous stories About Jobs We are the Data-Analytics team. Insights, numbers, dashboards… we provide data to our company. The goal? Help business and product teams in designing music and digital experiences that will be the best they can be for our users. As a tech data team, we rely on different KPIs . We check, provide, love and trust them. Why? Because the better they are, the happier our users are. In DATA we trust! As a consequence, we want to detect any peak or drop, understand it and take actions! There are lots of ways to detect outliers on time series. Some are good, some are great! We have used our own implementation of STL ( Seasonal Decomposition of Time Series by Loess ), which performs well. It decomposes a time series into trend and seasonality. The delta between reality and the fitted value is what we call the outlier score. Tuning parameters in STL is critical. We have optimized them on a labeled dataset. We use Tableau to visualize our results. For example, the graph below represents streams in a certain country and on a certain device: One of our main problems was the cost: it is a resource and time consuming process, so it cannot be applied to a huge number of cases. But Deezer is a worldwide company available on a large number of platforms, and we want to explore all the cases , as every user counts! This is why we developed the What The Bug (WTB) project. For a single KPI, we want to check more than tens of thousands of cases, so the model has to be as simple and scalable as possible. Weekly seasonality Like a lot of businesses, one of our main seasonalities is the weekly rhythm. For example, the graph below represents streams in a certain country, on a certain device and on a certain scale: We clearly see the weekly seasonality, which implies high daily variations. So we have decided to look for the weekly ratio This transformation has another great advantage: it turns the time series stationary. Trust me, it makes it easier to detect outliers. Now we have a set of values that we center and reduce (as good data scientists would). It is tuned enough to apply one of the simplest outlier detector, i.e. a queue rule: after 2 (we could talk for hours of this threshold, this one works for us) standard deviations, it is considered suspicious . Once it is done, we have to add a step to work on what matters: if there is a huge peak on streams in France in general, we do not want to check every sub-case such as IOS in France, Android in France, WindowsPhone in France, etc. As a consequence, at this point we only return the suspicious cases that are not a sub-case of another suspicious case. STL returns Now we can apply our costly model, based on STL, to the limited subset of cases that have been pre-selected by the queue rule. Thanks to this model, we get numerical scores that rank outliers. Therefore we can prioritize and take actions on Ukrainian streams on tablet or downloads in South America! As the code is as dynamic as possible, we can easily add any metric we want! To sum up WTB workflow: Apply a very simple and scalable model to detect suspicious time series Apply a complex and precise model to quantify anomalies Visualize on Tableau Take actions That’s one way we play. Of course, we do other things at Data-Analytics: we fight churn, we support Deezer partnerships, we look for trendy artists, etc. If you are interested, stay tuned! www.deezerjobs.com Stories of engineering, data, product and design teams… 536 Data Analytics Data Analytics Tools Kpi Anomaly Detection Deezer 536 claps 536 Written by Stories of engineering, data, product and design teams building the future of music streaming Written by Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-08-17"},
{"website": "Deezer", "title": "from a few insights to a key feature", "author": ["François De Sagazan"], "link": "https://deezer.io/from-a-few-insights-to-a-key-feature-9435e022aa5a", "abstract": "Previous stories About Jobs Product conception is based on a multitude of factors: data analytics, market research, benchmark, feedback from users/stores/customer support, user testing, intuition…And it always requires a few steps (and sometimes more than expected) to go from the initial concept or hypothesis to the final product pushed to all users. During my time at Deezer I’ve had the chance to work as product manager across Search, Library and our Homepage to improve the experiences and boost Deezer’s main KPIs (streams, engagement, reconnection, etc). We’ve been iterating on those pages to optimize them with common guidelines: Reduce “time-to-stream” on all journeys (quick accesses to the content the user is most likely to listen to) Display the most relevant content on top (“the right content at the right moment” based on data & moments) Optimize page loading performances Have the same experience on all devices (cross-device consistency) Last year, part of my mission was to improve My Music , which is the “library” space where users can find all their favorited contents (playlists, albums, artists, podcasts, etc). My Music was (and still is) Deezer’s main tab on iOS & Android in terms of clicks, streams and reconnection, but it had a lot more potential: we wanted to make this tab even more efficient to encourage users to collect more Deezer contents, as well as optimize the library browsing and listening experience. Initial assessment to confirm assumptions During the analysis of My Music journeys, we identified a few insights & problems to solve: My Music was the main tab (in terms of share of streams) but you needed at least 3 clicks (4 most of the time) to play music Among all the contents users listened to every week, some of them were particularly rotating The existing ‘Listening History’ playlist that aggregates all the tracks recently listened to was not performing well (only a few percents of My Music clicks) Our users place great value on album & playlist artwork, but couldn’t find them in My Music main page Based on these insights, we decided to implement a minimum viable product for “Recently Played” content to A/B test on a portion of our user base. As a product manager, I wanted to find the best way to highlight the content that users had listened to during their latest Deezer experience while upping artwork visibility. Test & Learn or the road to the final feature We worked with the UX design team to find the perfect room for this new section while keeping the target in mind in order to have a visible, quick and easy access to all kinds of contents played recently. After a few experimentations we decided to integrate a carousel at the top of the page to facilitate both developments and user adoption for our tests. Throughout the testing period we used streams that originated from My Music as our guiding light, with across product engagement as a secondary metric. As the first results were positive, we started to iterate on the feature to gradually increment substantial improvements: One-click play: We added a direct play functionality to the carousel’s covers to stream directly from My Music. And since My Music was most users’ first tap when opening the Deezer app, we decided to launch Deezer on the last tab used. It was also a way to match different behaviors (browsing through the Homepage recommendations vs. rotating favorite contents). User experience : Thanks to a great revamp of the page, we reduced the masthead size of My Music to give more visibility to the most used sections including “Recently Played”. Technical improvements : We worked on a better performing and persistent storage of “Recently Played” to display more items from a longer period of time and make sure our users find the carousel anytime they go back to My Music . Another major technical improvement was to optimize the refresh system of the “Recently Played” items in order to update the carousel as soon as a new content is added. Multi-type : “Recently Played” was the first multi-content section we launched. We gradually included all kinds of Deezer contents in this carousel (artists, albums, playlists, mixes, podcasts, live radios, etc.) to meet all kinds of listeners’ needs and usage. Cross-device : We improved “Recently Played” to display all the latest items played on all potential devices in order to ensure continuity (given that contents can be streamed on desktop, tablet and mobile). The aggregation of the “Recently Played” creation, direct play integration on the covers and open-on-start for the last visited tab represents a huge ease of use and journey improvement: you can now launch your favorite album or playlist of the moment in one click and the “Recently Played” section now accounts for a third of the clicks in My Music . Besides, by minimizing the number of interactions needed to stream and maximizing the efficiency of this essential page, we improved both global streams per user and weekly reconnection on profile. www.deezerjobs.com Stories of engineering, data, product and design teams… 20 Product Management Product Design Product Product Development Deezer 20 claps 20 Written by Stories of engineering, data, product and design teams building the future of music streaming Written by Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-03-22"},
{"website": "Deezer", "title": "whats right with errors", "author": ["Bertrand Longevialle"], "link": "https://deezer.io/whats-right-with-errors-2fe419424f62", "abstract": "Previous stories About Jobs You’re enjoying your app when suddenly an error message pops up, reading “ Error: Something went wrong ” Or you’re developing a new feature and attempting to interact with this outdated API that just returns NO when you call [Obscure.sharedManager doYourMagic] … And you’re frustrated! We tend to focus on producing what’s supposed to happen in our product, rather than thinking of what could go wrong. It should just work ! But sometimes things don’t go to plan. It might be that the password we typed in doesn’t meet the security standards or a signup form that doesn’t tell us what we did wrong. It’s like a job we applied for and got rejected from without getting any feedback — it’s hard to know what to improve. Which is why we should give our client (end-user or fellow developer) the opportunity to learn from their errors by giving them feedback. Over the last few months at Deezer we have worked on refactoring components of our iOS app, including In-App Purchase and Authentication. The latter is one of the first interaction users have with our app, the former involves their money. So in both cases it’s very important to handle errors nicely, either internally or by clearly communicating them to the users. Good news is, as we found out, there are plenty of tools available such as language features in Swift and modelling in Foundation, which are becoming increasingly compatible. There is even a clever trick for error presentation that our team ported to iOS from their Mac OS days. Now, let’s make Errors 🎉 What is a Swift Error? In Swift, errors are represented by values of types that conform to the Error protocol. This empty protocol indicates that a type can be used for error handling. This means we’re free to use whichever implementation we want to modelize an error, be it an enum, a struct or an object. Contrary to Objective-C where error handling was opt-in (we chose whether or not we want an API to notify us of an error), this is mandatory in Swift. It is based on a throwing mechanism, similar to what’s found in other languages such as Java. A function that throws errors it generates or encounters is called in a do - try - catch statement. Swift has pattern matching, allowing for some fancy code like this: Beyond the fanciness, it allows to take precise action on specific errors. Considering we are talking about Swift, why haven’t we mentioned using a question mark or an exclamation point? Are Errors some second-class Swift citizens? Well not at all, both our favourite punctuation signs are here: try? for getting optionals out of failable functions, and try! for disabling error propagation. Some real-world examples: try? : try! : Swift provides a way to specify cleanup actions using the defer keyword. For example, here is my routine preparing a talk: You could think of defer as some Swift conterpart to Java’s finally ; but you don’t have to use it in an error-handling / do-try-catch context. It’s just a block of code that gets executed at the end of the scope it is defined in. Now that we’ve seen what’s new and shiny in Swift, let’s dust off those good old NSErrors. You’ve probably encountered some of them and passed NULL more than [NSJSONSerialization JSONObjectWithData: options: error:] deserved 🙈 But do you really know NSError ? An NSError object encapsulates information specific to an error, including the domain (subsystem) originating the error and the localized strings to present in an error alert. NSError is the model of an error in all the Foundation world, providing identification (domain and code) and informations (localized infos and context). The domain is the general identifier: it is a unique string, usually defined in a reverse-DNS fashion: @\"com.deezer.Deezer.subdomain\" . The code is more precise about the error and there are several ones per domain, thus traditionally leading to an enum declaration: typedef NS_ENUM (NSInteger, subdomainErrorCode) {subdomainErrorCode1,…n} . Then comes the localized informations, which is directly-displayable, user friendly strings. Two are for explaining the error (description, failureReason) and two are for recovering from it (recoverySuggestion, recoveryOptions). - localizedDescription : the main message to the client. It’s a complete sentence that should be sufficient to understand what went wrong. - localizedFailureReason : the cause detailed in the localized description. - localizedRecoverySuggestion : choices available for recovering from that error. - localizedRecoveryOptions : titles of actions (aforementioned choices) the client can take. Although the last three can be nil, localizedDescription never is. It’s also important to point-out that NSError does its best to provide you with something to display. Finally, some context can be available in the userInfo[NSUnderlyingErrorKey] (the NSError from an underlying layer, that has been wrapped by the layer you are calling). And also the custom domain data the error creator chose to add. When you open up NSError.h you face lots UserInfo keys and similarly named properties. But which one should we use? For setting values , you can either use the UserInfo dictionary when calling init(domain: code: userInfo:) or by calling NSError.setUserInforValueProvider(forDomain: provider:) ( available(iOS 9.0, *) ). Both mechanism rely on the various UserInfoKeys constants. For accessing information , the preferred way is via the properties. This is because NSError tries really hard to give you something to work with. For example if no value is provided for the description, it combines the failure ( @available(iOS 11.0, *) ) and failureReason strings, if provided, and fallbacks at the very minimum to a combination of the domain and code. All values are first looked up in the userInfo dictionary, fallback to the domain provider before eventually returning nil. For custom information you could provide your property doing a job similar to NSError localized properties. But let’s KISS and just access the dictionary! You’ve got it, NSError is all about giving information. So it’s best to be clear when giving it. Objective-C has a few macros to available to make your own NSError+YourDomain.[hm] self-documentingly crystal-clear. 👌 I give you NSErrorDomain , NS_ERROR_ENUM , NSErrorUserInfoKey . Before… After! Note that the macro associates the codes to the domain, hence the shortening of the name. But that’s not all about NS_ERROR_ENUM ! Because it is imported in Swift as a specific type, properly defined NSError domains and code enums let the client benefit from all the completeness of NSError while writing first-class swift code. While NSError initing remains very similar, their handling gets shortened and clarified. In the following example inspired by the revamp of our Authentication engine, we try to register a Deezer user by using Facebook authentication. This can fail if we can’t get enough information from Facebook to meet our needs. In this case our Facebook Authentication service can generate an error stating this problem and providing the obtained informations. This in turn can be used to pre-fill our registration form. Here it is with just NS_ENUM … And now with NS_ERROR_ENUM ! Don’t try to fool us, authentication occurs asynchronously! Will this shiny syntax work in the real world? Actually yes: because pattern-matching in Swift goes beyond error-handling, you can handle async errors just as easily: That’s about all we need to know about (NS)Error creation and handling. Now let see what 2 of our iOS Leads, Ashley and Guillaume, imagined for error presentation. But first some macOS background : in Cocoa, the responder is based is the NSResponder class and handles not only the user interactions but also the presentation of errors. Each responder of the chain can presentError: itself and by default only forwards to the nextResponder . All are given the opportunity to customize an error when they’re informed they willPresentError: . There is a quite simple way to mimic this behaviour and get an Error Responder Chain out of iOS’ UIResponder : Categories. Here is all the code: On iOS the UIResponders are all the views, viewControllers, application, and applicationDelegate that took part in presenting something that the user interacts with. They, in reverse, are given a chance to respond to this interaction. For example let’s imagine an app, with a main viewController, that has an embedded viewController, with a button targetting its printResponderChain() action. The outptut is something like this: The most straightforward way to take profit of the chain is to use it as a centralized way to inform the user about an error. No matter which viewController received an error from the service it represents, you can always pop an AlertController to the user. Nice and Easy. Building on this, you can take some partial / particular action at one point of the chain, and let some other link take care of the full presentation. In this example, we both shake the button, and display a banner. The architecture of the screen an AuthenticationViewController diplaying the various authentication methods, with a few children, including a LoginViewController managind the login and password fields and the “Log in” button. To properly give feedback to the user, the LoginViewController first shakes its button, then passes the error to the chain, and the AuthenticationViewController which displays the banner. Other children can focus on their part of the UI and take advantage of the banner error presentation, without implementing some other delegation. We’re reaching the end of this article and might have encountered the word “error” more than in our own codebases: it’s probably time to dismiss those errors! To do so, we can implement dismissError(_ error:) and remove any UI we displayed in presentError(_ error:) . If you associate specific (ranges of) error codes to specific UI element, it’s easy to track where to present and dismiss any errors. I decided to make this article to share what I discovered exploring the wonderland of error handling in iOS development. Like most of us out there, our little iOS Deezer app has evolved to a full-grown application that requires a whole team of devs to collaborate on. In order to carry on shipping features in a reliable product, we are continuously refactoring our code, defining a proper architecture out of the (controlled) chaos, exposing what can be done by a module and what can legitimately go wrong. We have all we need to do so: The overlooked NSError, here from the beginning, has plenty to offer. And I believe it is here to stay since Apple continues to invest in it (new APIs in the recent major iOS versions). Swift has business errors baked in at the language level and every version comes with improved syntax and compatibility with NSError. In a mixed Swift/Objective-C environment, I definitely recommend using NSError to convey data: it’s really expressive and works well with UIKit. If you have ditched (or never coded) with Objective-C, there’s an extra step to model the errors with your own implementation of the Error protocol. Maybe we’ll code long enough to see a full-swift Error object ! I encourage you to embrace coding with Errors as Swift makes it really easy to write some expressive code with that paradigm. A good place to start? Search for a guard statement in your codebase where you just return : chances are you just threw an Error under the carpet. Go! Refactor it! You won’t even have to mess too much with the caller if you treat the error as an optional. So, why not give it a try? Swift Programming Language: Error Handling Cocoa Error Handling Using Swift with Cocoa and Objective-C: Error Handling This story is adapted from the talk I gave at the CocoaHeads @ Deezer this November. Keep up to date with all the meetups we host ! www.deezerjobs.com Stories of engineering, data, product and design teams… 112 Thanks to Arthur Guibert . Swift Nserror iOS iOS App Development Deezer 112 claps 112 Written by iOS @DeezerDevs, musician, photographer Stories of engineering, data, product and design teams building the future of music streaming Written by iOS @DeezerDevs, musician, photographer Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-01-04"},
{"website": "Deezer", "title": "from 2 weeks to 15 minutes how we optimized playlist generation", "author": ["Ludovic Heyberger"], "link": "https://deezer.io/from-2-weeks-to-15-minutes-how-we-optimized-playlist-generation-20078702042", "abstract": "Previous stories About Jobs In the past few years, the music streaming industry has been very competitive and all companies are looking for a way to differentiate themselves. At Deezer, we aim at being the least standardized, most personalized and most human streaming service. Providing musical recommendations to millions of users relies heavily on algorithms but a key part of what makes Deezer different, more human, is that we also truly count on the wonderful work of our Editorial team. They are the ones behind Deezer’s curated playlists, looking to create the perfect combination of tracks to enlighten key moments of your life. “100%” playlists are a collection of playlists that our editors curate and that our users particularly love. They consist of a selection of the 50 most iconic tracks of an artist or band. To make things even better, Deezer Free users can listen to some of these playlists without restrictions depending on their tastes, listening habits and favorite content. Our Editorial team was so overwhelmed by the success of the “100%” playlists that at some point, an average of two weeks were needed to create a single playlist! That’s when the Tooling Team stepped in. The Tooling Team works in the shadows, as part of the Quality & Operations division, and tries to automate tedious tasks, mainly collaborating with teams who don’t have their own developers and can’t solve those issues in autonomy. The main clients of the Tooling Team are the Customer Care and Editorial teams. When breaking down the process of generating a “100%” playlist, it became clear where the issues lay and what needed to be done to bring joy and happiness back into our editors’ lives. There are 3 main steps required to create a “100%” playlist: Gather an initial list of tracks from the artist or band (these are provided by the Recommendation team) Check, reorder and filter these tracks to create the best listening experience (this is done by the Editorial Team) Create the playlist cover that will be displayed in the Deezer product (this is done by the Studio Team, who is responsible for all of our digital assets) The second step is where the editorial magic happens, and it can hardly be done by an algorithm, especially if you want to keep this human touch that we like so much at Deezer. However, step 1 and 3 are both ideal candidates for automation. The story of how we automated the first step would be a good read, but here I chose to focus on how we designed a tool that automatically generates covers for “100%” playlists. Because every new cover was generated by the Studio team, a bottleneck had appeared: in the best case scenario, it would take them one week to deliver the cover; in the worst case scenario, it would take them up to three weeks. The Tooling Team has a history of finding funny names for their various projects they create and this one is no exception. Named after the French riddle website ouverture-facile , the goal of couverture-facile is to automatically generate “100%” playlist covers. Starting from an artist ID, we need to: Find the artist name and the image that will be used for the cover Render the background Draw the Deezer logo Draw the “100%” asset Draw the artist name Thanks to the Partnership division at Deezer, external developers have access to a simple subset of our internal APIs. It is documented here . A few Python lines later, we have what we were looking for. Once we have an image url, creating our background for the next steps is very easy. The issue when automating image generation is that you have to deal with various visuals. So here, we are turning down brightness a little bit to accommodate very bright images and be able to draw things in white on top of them. Thanks to the rebranding we conducted a few years ago, the Deezer logo is now a bit simpler to draw programmatically. Here, we are placing it in the bottom right corner of the cover. Since all the covers have the same size, the quick and dirty way involves a lot of (otherwise undesirable) magic numbers. Nothing fancy here: The most tricky thing is to determine the top position of the asset. And THAT is where all the complexity lies. More on that next! Since we are dealing with all kinds of artists’ names (from a few letters to several words) and alphabets (mainly Latin, Japanese, Arabic, Hebrew), drawing the artist’s name is the most complicated part of the project. Different alphabets mean that we require different fonts. To select the appropriate one, we first need to detect which one is used. Fortunately for us, it can be done easily: Here, we are only dealing with the first alphabet we found in the artist’s name, but you get the idea. Once we know the one to target, a quick lookup will give us the font: The last line of this snippet is very important since some languages are read from right to left and require some text transformations. One thing I didn’t anticipate is that I would have to manually split the artist’s name on several lines when it is too long for the selected configuration. After a lot of trials and errors, this is the method i settled for: If the artist’s name doesn’t fit on one line, we try to split the name in tokens that we evenly spread on two lines. Some artists’ names are still too long to fit on two lines, and for those special edge cases, we just fill one line before adding another one. Now that we know if our text fits on one line or more, we know where we can draw the “100%” asset and consequently we know where we can position the text, wrapping up this project with a nice and clean: Notice that the anchor parameter is set to “ms”. This is the simplest way I could find to position multiline text on an image while ensuring decent results. This tool doesn’t cover every edge case but it doesn’t really matter. The idea was to speed up the process for 95% of playlists generated and let the Studio Team deal with the remaining 5% corner cases that would be a nightmare to automate. The key for a quick hack to be successful is to move fast. As soon as couverture-facile worked in a test environment, the last step was to integrate it into the “100%” playlist generator that we created for editors in our internal back office. Once done, the process was almost fully automated: editors can now pick the artist they wish to create a playlist for and the tool automatically generates the cover and initial track list using our internal recommendation API. In a few steps, editors are provided with a basic but satisfactory “100%” playlist that they can tailor to their tastes before publishing it to the users. The whole process — from creation to publication — now only takes 15 minutes instead of 2 weeks. In line with one of Deezer’s core values — “Just Hack It”, we managed to create a tool in a short time that greatly helps our Editorial team in their day-to-day work. Helping internal teams be more efficient can be achieved in a lot of surprising ways but one thing that is common to all these projects is the satisfaction of unlocking “virtual head counts” by automating repetitive tasks, so that other teams can focus their efforts on what really matters: our users. Do you want to help us build and deliver the best experience on Deezer? Take a look at our open positions and join one of our teams! Stories of engineering, data, product and design teams… 98 Python Python Pillow Programming Image Generation Deezer 98 claps 98 Written by Stories of engineering, data, product and design teams building the future of music streaming Written by Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2021-04-29"},
{"website": "Deezer", "title": "womens voices a product marketing story", "author": ["Camille Bonenfant"], "link": "https://deezer.io/womens-voices-a-product-marketing-story-15fd64384bb3", "abstract": "Previous stories About Jobs This post is part of a series about the women behind ‘Women’s Voices’, an in-app story created by Deezer for International Women’s Day: Part. 1: Women’s Voices — A Product Story Part. 2: Women’s Voices — A Brand Story Part. 3: Women’s Voices — A Data Story Part. 4: Women’s Voices — A Product Design Story Part. 5: Women’s Voices — A Product Marketing Story My name is Camille Bonenfant and I have been a Product Marketing Manager for 3 years now at Deezer. I am the link between Product, Global Marketing (Social Media, Press Relations, CRM, Customer Support, etc.) and Local teams (local marketing directors and specialists in Deezer’s key markets). I ensure everything our product offers is well communicated, from showcasing new smart playlists to highlighting podcasts, to promoting the #MyDeezerYear experience, and much more. My day-to-day work consists in building marketing plans, helping brand teams create powerful stories, briefing Social Media and Press Relations teams, etc. I was introduced to the soon-to-be ‘Women’s Voices’ project by Product Manager Aina after it was decided that Deezer would create a special experience for International Women’s Day. At that stage, we didn’t know what the in-app story would look like but we knew our objective was to raise awareness on where women stand in music, and give our users the power to change things. My role was first to accompany the Product teams in the creation of the story and provide them with marketing insights. Then, my main mission was to onboard the Brand, Local and Marketing teams in order to build a consistent marketing strategy, with a 360 approach . The idea was to speak about this project with one voice and one branding message on all our channels (e.g. emails, social media, PR, Deezer Community , etc.). I was also in charge of coordinating specific local needs and the creation of visuals, among other things. Lastly, as a Product Marketing Manager, I needed to make sure that the launch of the project was successful, to follow the various marketing metrics (e.g. the number of PR hits aka the number of articles that picked up the news, the engagement rate on social media, the stream rate after direct communication, feedback on Community, etc.), and to analyze the impact of our communication on product usage. The main challenge I faced was the number of people involved. As the idea was to have a consistent 360 approach, more than 10 different teams were involved, and we had to make sure they were all on the same page at each step of the project. It was a great opportunity to test new, innovative media of communication though. For example, thanks to the help of the Growth and Customer Success teams, we created a Google Web Story , which is a format Google developed to reach wider audiences. We used it for non-Deezer users and journalists in particular in order to give them a glimpse of the experience and encourage them to download the app to discover the full story. Among the many things I’ve learned from this experience is that we really are at an early stage in terms of women representation in the music industry . Although I suspected such a situation, I didn’t know that there were, for instance, only 3% of women in technical jobs (e.g. sound engineer, stage manager, lighting designer, etc.) in the music industry. On a brighter note, I was happy to discover that the first person to use autotune was a woman! Besides facts and figures, I have also learned a lot from the people I have been working with, may it be on SEO optimization for a blog post, or on diversity and inclusion, thanks to the internal initiatives our HR team put in place. One of the things that I love about this project is the theme. Women representation in the music industry, and also in today’s society, is a hot topic, and a very important one for me. This project was the perfect opportunity for me to learn more about it and identify the Deezer data that clearly highlights the issue. For instance, my favorite part of the ‘Women’s Voices’ story is the card that reveals the genre that has the lowest representation of female artists (only 4%!) I think that, as a music and podcast streaming platform, it’s our responsibility to emphasize the challenges that minorities are facing in these industries, and to inform our users. We want them to become aware of the situation, and we wish to give them the tools to improve these numbers. In some way, I feel like we are doing our part for this international fight, by raising awareness and provoking debates on social media. Then it’s up to all of us as music listeners to change our habits (what artists we listen to or what content we share for example) and make a difference. The power is in our hands. I also love the fact that all the teams at Deezer agreed that this shouldn’t be a one-day effort. In addition to the in-app experience, we are doing our best to make a lasting impact, for example through the launch of the ‘Women’s Voices’ collection that will stay in the product permanently (on both mobile and web). What I would love to see is a clear increase in favor of female artists in the share of streams in the long run. If we were to implement a similar experience next year, I would love to see that numbers have changed for the best. I also hope that we will achieve our goal to start the debate and create conversations about women behind music. Finally, and most importantly, I simply hope our users will learn something new thanks to our experience. To conclude my take on ‘Women’s Voices’, I’ll say a few words about my favorite woman’s voice. It’s the one we don’t hear on the radio. It’s the one we hear on the streets — the unified voice of thousands of women chanting the same slogan. It’s the voice of my favorite singer but also the voice of my neighbor. It’s the voice of all those women who don’t know each other but trust each other for a better future, and know that we need role models in the various career paths that the music industry offers to inspire millions of emerging talents in new generations. This post is the last part of a series of articles written by some of the women behind the ‘Women’s Voices’ project. You can find the links to the previous articles at the beginning of this post and on the blog’s homepage . If this type of projects motivates you and you would like to help us make an impact, apply for one of our open positions ! Stories of engineering, data, product and design teams… 24 International Womens Day Music Industry Product Marketing Iwd2021 Deezer 24 claps 24 Written by Stories of engineering, data, product and design teams building the future of music streaming Written by Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2021-04-19"},
{"website": "Deezer", "title": "womens voices a data story", "author": ["Inès"], "link": "https://deezer.io/womens-voices-a-data-story-1707432b6157", "abstract": "Previous stories About Jobs This post is part of a series about the women behind ‘Women’s Voices’, an in-app story created by Deezer for International Women’s Day: Part. 1: Women’s Voices — A Product Story Part. 2: Women’s Voices — A Brand Story Part. 3: Women’s Voices — A Data Story Part. 4: Women’s Voices — A Product Design Story Part. 5: Women’s Voices — A Product Marketing Story My name is Inès and I started my adventure at Deezer as a Data Analyst and Storyteller in January with this challenging project: Glocal (= global x local) Social Stories for International Women’s Day . What is important to me about this specific day is that it invites people from all backgrounds, genders, cultures and ages to consider what concrete actions they can take to make the world more equal. Telling people where we stand is certainly a first step because sadly, not enough people are aware of the current situation of women in the world. Once informed though, we have to act. This is the direction Deezer chose to take for this campaign — celebrate, advocate and encourage action , which is why I believe in this project and contributed to the best of my abilities. My role within the Business Analytics team, with my manager Mark Tyley, was to collect the proper data about women in music from our internal database and find insights that are worth sharing, while putting a human perspective to the data. We didn’t want people to simply read some numbers and forget about them the second they close the app. The challenge was to make sure our community understands our powerful message: female representation in music needs to keep improving , and we all have the power to take action at our own level in this fight. We brainstormed about how we would tackle the subject in a series of workshops with several teams at Deezer, using many design thinking processes. We eventually all agreed that, in line with the International Women’s Day 2021 campaign theme — #ChooseToChallenge , we would call out gender inequality by communicating statistics on women’s underrepresentation in the music industry — but not only. We also decided to highlight the improvements already made and celebrate the careers of some of the greatest female artists of all time. Paying tribute to Björk, Cher, Aretha Franklin and SOPHIE — to name a few — is a way to give voice and visibility to minorities who made history and still are powerful sources of inspiration. Another key decision the team made was to dedicate stories to local artists , which illustrates our will and commitment to be close to local, unique markets. It was actually interesting to analyze the data per country, and see how trends can sometimes be global, sometimes specific to certain areas of the world. I particularly enjoyed the French local story about Yseult , who is to me one of the most inspiring and promising artists of the French music scene because of her voice, lyrics and music choices, but also because I like the way she speaks her mind and doesn’t let herself be pushed around. Extracting and analyzing data was a challenging task technically speaking. Yet it was rewarding — and kind of reassuring — to find out, for example, that the most streamed song in 2020 came from a woman ( Tones and I ), and that even in the music genres in which women are the least represented (Electro and Rap), some female artists are still leading by example with stunning performances in charts (like Cardi B , FKA twigs , Yaeji or Suzane ). Besides gathering information about the streaming performance of top songs, I got to explore women’s impact in the whole audio industry. Global and local data on playlists, podcasts and audiobooks specifically were needed to illustrate press releases about our International Women’s Day campaign. So I was, for instance, asked to retrieve the top 200 female authors for audiobooks in Germany, and the top 5 most streamed ‘ Women of ’ playlists for PR and editorial purposes. You can find the request I made for the latter below: Statistics help people believe what is actually happening. Storytelling helps people understand these stats and feel concerned by them. Whether you are the crazy streamer who listened to Dance Monkey a thousand times, or simply the latest user who added Yseult’s newest album Brut to their favorites, be glad because all contributions , even the smallest ones, participate in women’s empowerment . I personally don’t think of my favorite “woman’s voice” as being my favorite singer or podcaster, but rather as my favorite artist as a whole. Usually, female artists are considered activists only when they sing explicit feminist lyrics. But to me, having a voice , in this context, is the power of sharing one specific message . You don’t necessarily need to use your tongue and words to convey an idea. I listen to a lot of instrumental music such as electro, jazz and ambient, and when I see female producers release amazing EPs and albums while leading a label or working under several aliases, I feel so admiring. The music industry is rude when you’re a woman . You have to work harder, put up with sexist judgments (in particular figure-related comments) and constantly suffer the comparison with other women. So I think you can make an impact in the industry just by the act of making your art , even if you only use a modular synth or a drum set. And when you are brave enough to dedicate yourself to the music genre whose top charts only include 4% of women (i.e. Electro), you deserve to be seen as someone who takes part in women’s empowerment. Finally, as I feel famous female icons had enough coverage on International Women’s Day, I would like to share the work of a less known DJ and producer that I have listened to quite a lot recently: elkka . She produces absorbing music between house, electronica and percussive beats, and is supported by many influential artists like Four Tet and Caribou. Her artistic universe is fascinating and I would recommend having a listen to her EP Everybody is Welcome , as well as her latest release I. Miss. Raving . On a side note, she also runs the record label called “ femme culture ” , which champions women, non-binary people and the LGBT+ community . This post is part of a series of articles written by some of the women behind the ‘Women’s Voices’ project. Read the fourth part of the series here . If you would like to help us put data at the service of powerful messages, our Data & Analytics team is recruiting! Check the open positions here . Stories of engineering, data, product and design teams… 136 Iwd2021 International Womens Day Deezer Data Storytelling 136 claps 136 Written by http://inespiederriere.fr/ Stories of engineering, data, product and design teams building the future of music streaming Written by http://inespiederriere.fr/ Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2021-04-07"},
{"website": "Deezer", "title": "womens voices a product design story", "author": ["Jaqueline Morais"], "link": "https://deezer.io/womens-voices-a-product-design-story-bb9891530ff2", "abstract": "Previous stories About Jobs This post is part of a series about the women behind ‘Women’s Voices’, an in-app story created by Deezer for International Women’s Day: Part. 1: Women’s Voices — A Product Story Part. 2: Women’s Voices — A Brand Story Part. 3: Women’s Voices — A Data Story Part. 4: Women’s Voices — A Product Design Story Part. 5: Women’s Voices — A Product Marketing Story My name is Jaqueline and I’m a Product Designer at Deezer. My mission is to understand our users’ needs, create visual compositions, and build relevant experiences that people will interact with, easily and fluidly. I joined the ‘Women’s Voices’ project in the early stages. We wanted to create a meaningful and impactful experience to celebrate and advocate International Women’s Day . In order to develop the whole concept and define the story we wanted to tell our users, we worked in collaboration with multiple teams, using design thinking tools, brainstorming workshops, user journeys — to illustrate how people would interact with the in-app experience — and empathy maps — to guide us on the feelings we wanted to evoke with this initiative (i.e. What would people think of the International Women’s Day campaign? What content would they see, hear and share?) After lots of online sticky notes and ideation sessions that resulted in about 45 card ideas , our concept was eventually defined. That’s when the real fun started for me: designing the in-app experience people would have in what we call ‘stories’ , i.e. thinking up the whole user journey through the content presented, as well as the visual aspect of each screen, and sharing assets for social media. In collaboration with the Product Design team, I decided on the visual direction for our 12 global content cards and 4 local artist cards , as well as artists’ pictures, and the overall look and feel. Purple is one of the colors used in the International Women’s Day movement, so we decided to use it as our ‘highlight’ (or as designers would call it, ‘accent’) color. Besides, purple is often associated with justice, power, pride and dignity. So it definitely represents the core of the whole project! Deezer is present in many countries, so designing a single experience that works for different cultures, diverse backgrounds, and numerous languages is always a challenging and exciting experience. One of the main challenges in designing for ‘Women’s Voices’ was to bring all the interesting and important facts we wanted to share, keeping the story experience format. It should be informative, but in a simple, short, easy way — in Portuguese, French, Russian, or any other language — and believe me, we work with dozens of translations! What I love about this powerful project is the subject itself. Talking about women’s voices, shedding light on their achievements, recognizing their efforts and challenges (I’m having goosebumps just by thinking about it) is an important way of empowerment and celebration. I particularly enjoyed the data we collected for this experience. For example, I learnt that it would take 11 years if I listened to ‘ Respect ’, from Queen Aretha Franklin, as many times as it was played on Deezer in 2020. That’s a lot of ‘play’ buttons pressed on that anthem! In addition to all of this, the initiative allowed me to work closely with women from different teams at Deezer. I’m really proud to have been involved in this experience and surrounded by so many talented women. When it comes to women’s voices, I think about the voice of my sister, my mom, my friend, my colleague. All the women who inspire me, make me learn and grow everyday. And also the voices of those who can’t break the silence yet. I hope ‘Women’s Voices’ by Deezer can spread the word about the impact women have made in music, despite all the challenges they face every day to conquer their places. I hope we can raise their voices, bring awareness to the lack of representation we still observe in certain genres, but also give hope for change. I hope people can get inspired by this project, learn with it, and be part of change — streaming, buying, reading, supporting women creators. This post is the penultimate part of a series of articles written by some of the women behind the ‘Women’s Voices’ project. Read the last story now or check the links to the previous articles at the beginning of this post. If you would like to help us design the best experience to millions of users on Deezer, take a look at our open positions and join one of our teams! Stories of engineering, data, product and design teams… 91 International Womens Day Deezer Product Design Womens Voices Iwd2021 91 claps 91 Written by I design things and write with my left hand. || designer @ deezer Stories of engineering, data, product and design teams building the future of music streaming Written by I design things and write with my left hand. || designer @ deezer Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2021-04-07"},
{"website": "Deezer", "title": "womens voices a product story", "author": ["Aina Demirova"], "link": "https://deezer.io/womens-voices-a-product-story-39da52ca30ba", "abstract": "Previous stories About Jobs This post is part of a series about the women behind ‘Women’s Voices’, an in-app story created by Deezer for International Women’s Day: Part. 1: Women’s Voices — A Product Story Part. 2: Women’s Voices — A Brand Story Part. 3: Women’s Voices — A Data Story Part. 4: Women’s Voices — A Product Design Story Part. 5: Women’s Voices — A Product Marketing Story On March 8th, we launched an in-app story to amplify women’s voices . There are Instagram, Snapchat and other social media stories, but what kind of story can Deezer have, you might ask? This is a story with a purpose to celebrate and advocate women behind music and podcasts, and make a difference in their lives. Any user can currently discover the story in the ‘Music’ homepage of their Deezer mobile application only (with the minimum app version 6.2.13 on Android and 8.25 on iOS), in the fourth rail after the ‘Playlists you’ll love’ section. Once you dive into the story, you will see a series of swipeable cards (as you are used to seeing on social media apps) that include engaging insights on where women stand globally, a trivia question, profiles of global and local women who made an impact in music, and of course, playlists and podcasts that have been hand-picked by our amazing Editorial team. The music that is playing in the background of each card will invite you to immerse yourself in the world of women, and set the right mood straight away! The great thing about our story is that you don’t have to keep it to yourself. Every card is easily shareable so that you can start a conversation or a debate with your friends, or just put it out there to raise awareness and pay a tribute! Creating this story has been quite a journey and has involved many teams in different divisions at Deezer — from Product & Engineering to Data, Brand, Marketing, etc., a fair share of them being women. So it seemed like a good opportunity to introduce some of the women who worked hard to make this project happen, and to listen to the voices of some of the women who are behind ‘Women’s Voices’. My name is Aina and I am a Product Manager, leading the social sharing experience at Deezer. My mission is to enable Deezer users to share their love for music and podcasts, and connect them with their friends or favorite artists. It involves a constant research of user problems and opportunities, and what I like most is working with my team and multiple stakeholders to drive change! While pivoting from the #MyDeezerMonth experience (i.e. insights on your monthly listening habits, top artists, top tracks, etc.) to our next project, we were looking for a powerful story to connect with our users, and that our users would be proud to share. I led a series of brainstorming sessions with our pilot countries (France and Brazil) to identify topics where we could make a difference. Deezer being an equal opportunity employer, the International Women’s Day appeared to us like the perfect occasion to make an impact regarding a cause we stand for. Once it was defined as a brand direction to do more than celebrating women, and to also advocate and strive for change , the second part of my job was to shape the story we wanted to tell our users, together with talented colleagues from various teams like Editors, Brand, Artist Marketing, Product Marketing, Design, Tech, Data, Social Media, PR. Last but not least, it was my responsibility to ensure that the in-app story was released properly and on time, which would have been impossible without everyone’s contribution. The biggest challenge on this amazing project was to orchestrate different teams to put the story together. Each has its own constraints and it was a great opportunity for me to learn how other teams operate. I also learned that in order to move “fast” you need to take it “slow” with people, i.e. take the time to get to know each other and the project to work effectively. Having a clear purpose with the will to make a change surely unites and motivates people though. We acted as one and stepped up to take an active position in the fight for gender equality, by challenging centuries-old stereotypes and biases. Seeing everyone’s contribution, ideas and passion was very inspiring to me! What I like most about the ‘Women’s Voices’ experience is that the real beauty of the story lies in the powerful combination of its core — a clear purpose and surprising facts, along with amplifiers that make the story truly alive — hand-picked audio excerpts, catchy visuals and dynamic transitions. I hope that, with it, we achieve what we envisioned at the start: Raise awareness on gender inequality in music Celebrate women’s diverse talents and impacts Inspire users to take an action to make a change by streaming and sharing women’s art When the time comes to rerun the data on gender equality overall and by genres, I hope we will see an increase! Finally, when I think about women’s voices, I’m thinking about all the women that surround me. They have played so many roles. The key for me is to pay attention to what women think, feel, say and do . This post is part of a series of articles written by some of the women behind the ‘Women’s Voices’ project. Continue the reading with the second part of the series! If this initiative resonates with you and you would like to help us build new experiences on Deezer, take a look at our open positions and join one of our teams! Stories of engineering, data, product and design teams… 113 International Womens Day Storytelling Deezer Product Management Iwd2021 113 claps 113 Written by Product Manager @Deezer // Always curious, always learning // Amateur painter, vedic meditator, triathlonist Stories of engineering, data, product and design teams building the future of music streaming Written by Product Manager @Deezer // Always curious, always learning // Amateur painter, vedic meditator, triathlonist Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2021-04-07"},
{"website": "Deezer", "title": "womens voices the story", "author": ["Sarah Kendous"], "link": "https://deezer.io/womens-voices-the-story-7e2876ebe3ea", "abstract": "Previous stories About Jobs This post is part of a series about the women behind ‘Women’s Voices’, an in-app story created by Deezer for International Women’s Day: Part. 1: Women’s Voices — A Product Story Part. 2: Women’s Voices — A Brand Story Part. 3: Women’s Voices — A Data Story Part. 4: Women’s Voices — A Product Design Story Part. 5: Women’s Voices — A Product Marketing Story My name is Sarah Kendous and I am Head of Brand at Deezer. Along with the Product and Product Marketing teams, we have been exploring opportunities for Deezer to create meaningful, human and impactful experiences in order to help us connect with our users on a deeper level, and express ourselves on societal issues that we deeply care about through music. As a challenger and human brand, we are not only a music streaming service but we also feel deeply rooted in what matters to our audiences. Music is both a driver and a reflection of change in the world — that’s the kind of insight we started from. Being a feminist myself, I was delighted to see how onboard and driven all the teams were about creating an experience around International Women’s Day. My role in particular was to elaborate a singular concept that revolves around 3 key pillars : 1. Celebrating our victories , 2. Raising awareness on what is yet to be done, 3. And in time, being part of change for a more equal world. We didn’t want to stop at celebrating what has been achieved. We wanted to get into the nitty gritty of what is still to be fixed. This gave way to the idea of showcasing women of the industry, and breaking the stereotype of women being only behind the mics. It was important to first establish that women are active in all professions related to music and have moved the needle in so many different ways. But more is yet to be done! It has been a massive undertaking as it has involved orchestration between tech, marketing, HR, local and global teams, who have all come together so we can have a 360 consistent approach and make sure the impact is strong within and beyond our product. I am very proud of each one of us, in particular on how strong and committed we have been. You can feel that it is a subject that we all take to heart and that perfectly combines professional and personal passions. When it comes to women’s voices, my favorite woman’s voice has to be her royal highness Nina Simone — the fierceness, the edges that she didn’t care to round, and how unapologetically herself she was (or rather, still is, because she’s immortal!) She broke down so many barriers, and her words and stands are more relevant than ever for women and people of color, but also for people experiencing mental health issues. This story obviously goes beyond one single day or month as we, at Deezer, have embarked on a bigger conversation about Diversity and Inclusion , in which gender and identity are key aspects to address. Lastly, this initiative is dedicated first and foremost to the women that we are, the ones we raise, the ones that raised us, the ones we fall in love with and those that inspire us, the ones born into it and those that become it; women of all shapes, sizes, colors and walks of life. We can do it. You can do it. Happy International Women’s Month! This post is the second part of a series of articles written by some of the women behind the ‘Women’s Voices’ project. The third part is available here . If you wish to be part of change and help us build meaningful experiences on Deezer, check our open positions and join one of our teams! Stories of engineering, data, product and design teams… 3 International Womens Day Storytelling Deezer Brand Content Diversity And Inclusion 3 claps 3 Written by Stories of engineering, data, product and design teams building the future of music streaming Written by Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2021-04-07"},
{"website": "Deezer", "title": "private libraries in android how to manage them", "author": ["Jean-Baptiste Vincey"], "link": "https://deezer.io/private-libraries-in-android-how-to-manage-them-46aa1f098ade", "abstract": "Previous stories About Jobs This post is part of a series on private libraries: Part 1: Private libraries in Android — why you should consider it Part 2: Private libraries in Android — how to manage them In the first part of the series, we talked about the benefits of private libraries. On top of modularizing our code, they help us build reusable components across different projects/applications. In the second part, I will explain how we manage them in the day-to-day project routine at Deezer. For this, we will use the Deezer Android libraries as our example. Note that the breakdown and the content of the libraries are not the point of this article. They will only serve as examples to show how we develop and integrate them and how we adapted our processes to handle frequent changes. We currently have three private libraries for Android projects at Deezer: Core : manages the player, domain models, requests, and caching Design : provides the Deezer style guide and shared UI components Server-driven UI : implements server-driven User Interface For the rest of the article, we will focus on the Design library as the main example. Like any of our private libraries, the Design library is meant to be used in several applications. Therefore, it is developed as an independent Gradle project, with its own Git repository. This helps us to define clear boundaries for each library’s scope. It enables a good separation of concern and enforces better interface and abstraction. The Design library is composed of several modules. Each module will produce a JAR or AAR; hence they can be independently loaded in an app. Notice that some modules may depend on other modules from the library, bringing them along when loaded in an app. Libraries are provided as frozen binary artifacts, so they need versioning. Our versioning convention takes its inspiration from Semantic Versioning ( semver ) [major].[minor].[patch] Major : implies important changes such as raising min supported Android API, or important migrations (e.g., AndroidX) Minor : for new features, feature enhancement, or minor bug fixes (unlike Semantic Versioning, we allow API breaking changes here) Patch : for critical bug fixes that cannot wait for the next minor version (API breaking changes NOT allowed here) Similar to our applications, the library’s release lifecycle of minor (or more rarely, major) versions follow a “release train” of two weeks. As soon as a library version is released, the library’s master branch becomes the snapshot of the next version. This snapshot will be released as the next version upon release two weeks later. Patches can be made at any time in order to fix a critical bug and do not follow the release train lifecycle. On each release, the library gets a release note describing all changes from the newer version. Release notes are important in order to keep track of changes in a library. It is quite essential when updating the library in any project/application. To help with this recurring task japicmp generates reports on the differences between two library versions (comparing java class files contained in JAR archives). In order to make our libraries available to every developer within Deezer, we use a Nexus repository . Nexus is a repository manager that allows us to store, manage, and load library binary artifacts (JAR / AAR). The usage is quite similar to JCenter or MavenCentral , except that Nexus (open source version) is installed on Deezer servers and is reachable only within Deezer. The publishing process is handled with our Continuous Integration server ( Jenkins in our case). A dedicated job takes care of building the library and uploading it to the Nexus. The job is triggered automatically every night on the snapshot of the Design library to embed changes made during the day. It is also used to build release versions of the library, being triggered manually in this case. Furthermore, we can also publish the Design library locally on our machine, on a local maven repository. This is often used to test the impact of some changes from the library on an application. To achieve this, the library’s package name is overridden with the suffix _local when publishing the library locally (e.g., com.deezer.design_local instead of com.deezer.design ). Then, the app also adds the suffix _local to the name of the Design library dependency, forcing it to switch to the locally generated library. To keep things clear and maintainable, this override is done in a separate Gradle file dependency_local_overrides.gradle : Main development branches on applications always target a released version of the Design library. It should not target the library’s current snapshot, since, by definition, the snapshot is considered unstable and can be subject to API changes. However, we sometimes need to start integrating some changes in the library that are only on the snapshot, into an application. Let’s consider an example. Our last released version of the Design library is 2.2.0 . New icons need to be added in the Design System and can be required in a feature on the Deezer application. We add these icons in the Design library, but at first, they are only available on the snapshot (that will become the release 2.3.0 some point). The issue here is that our develop branch in the app is targeting Design library 2.2.0 , meaning we don’t have access to the new icons. To address this situation, we define a specific branch in the app, targeting the snapshot version of the library, which we call design-snapshot . The whole process would be as follows: Add the new icons to the Design library (on snapshot) Generate a build of the Design library snapshot On the app, create a new branch design-snapshot from develop Update Design library dependency to use the snapshot instead of version 2.2.0 Develop new feature in-app on design-snapshot branch At the end of the usual release train of 2 weeks, release a new version 2.3.0 of the Design library from a snapshot On the app, merge design-snapshot to develop This way we can develop new features on the app based on the last change from the Design library snapshot without impacting the main development branch from the app. Documenting a library is an important aspect not to be neglected. Without any documentation, it is very difficult to know what the purpose of the library is, what features it provides, and how to integrate it. A proper README on the library (and on each module) is a good way to address these points. However, sometimes concrete examples can even be more powerful than words. For example, a Playground app within the library can be used as a showcase of the library to list and demonstrate the features it provides. Furthermore, a Sample app can be very helpful to show how to integrate the library into an application. It can complement documentation as a living code example. We currently don’t have a Sample app for our Design library but we have one for our Core library that shows how to integrate the player as well as other features. The processes described in this article allow smooth updates and integration of a library. However, it becomes more complicated when dealing with multiple libraries. Currently, each of our libraries are managed with their own repository as an independent project. Private libraries sometimes depend on other ones, although we try to make them as independent as possible. This can cause multiple issues: Keeping dependencies aligned (i.e. same versions) on all libraries and applications is quite cumbersome. For dependencies such as Kotlin, Android Gradle Plugin, AppCompat, MaterialComponents, etc. this can lead to important issues. When a private library depends on a second private library, we may encounter some conflicts when an application depends on these libraries, but with a different version for the second one. Finally, on applications integrating multiple private libraries, we may have multiple snapshot branches (each per library). This can possibly lead to major conflicts on these parallel branches. We are currently investigating the possibility of putting all of our libraries under the same Git repository. This could allow us to manage these dependencies more easily while keeping them independent. Private libraries help us develop multiple applications at a fast pace by allowing us to reuse components and features while ensuring consistency of behaviors among all Deezer applications. In this article, we have shown how we manage our private libraries at Deezer with the example of the Design libraries. Each library is an independent project with its own repository. CI jobs make the release process partially automated — handling build and publication on the Nexus repository — allowing frequent releases. Each release is documented with release notes. App’s main development branches always target a released version of libraries, while branches dedicated to snapshots allow the integration of changes from a not-yet-released library version. Finally, documentation plays a key part in library usage and integration. It is achieved with proper READMEs, but also with Playground and Sample applications. If you would like to help us build and deliver the best experience on Deezer, take a look at our open positions and join one of our teams! Stories of engineering, data, product and design teams… 143 Thanks to Arthur Guibert and Pauline Munier . Libraries Android Android App Development Deezer AndroidDev 143 claps 143 Written by Android Engineer @Deezer Stories of engineering, data, product and design teams building the future of music streaming Written by Android Engineer @Deezer Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2021-03-09"},
{"website": "Deezer", "title": "updating kubernetes ca certificates the hard way", "author": ["Denis GERMAIN"], "link": "https://deezer.io/updating-kubernetes-ca-certificates-the-hard-way-f9518108791d", "abstract": "Previous stories About Jobs In this article, we will share our experience with a tricky situation we found ourselves in a few months ago. It involves Kubernetes clusters, CA certificates, Hashicorp Vault, and transparent operations in production for millions of users. So buckle up; it might be a bumpy ride! At Deezer, Kubernetes has been deployed and used in production for more than three years. Today, there are tons of different ways to deploy Kubernetes. Each has its advantages, depending on your cluster topology and needs (cloud, baremetal, edge, etc.). You may have guessed it: this article’s title refers to the famous “Kubernetes the hard way” written by Kelsey Hightower. That’s because back in 2017, our clusters have been deployed using Ansible playbooks, written by hand from the documentation. And this looks a bit like automating “Kubernetes the hard way” with Ansible 😉. Though you would (and should) not do it like this today, you have to put yourself in the context of 3+ years ago. At that time, in terms of deployment method, the options were more limited than they are today, especially for bare metal installations (for example, kubeadm , now GA, was still displaying big red warnings of being “NOT PRODUCTION READY”). It’s the final countdown Not all Deezer applications are running inside Kubernetes, but a significant enough portion of them do. Should our Kubernetes platform be down , Deezer end users would start noticing that something is up (see what I did here? 😉). And that’s where our story begins: The Certificate Authority (CA) of Kubernetes was about to expire in a few months, and with it, the whole certificate chain. To put more graphically, instead of reassuring green indicators everywhere, we had a lot of orange warnings. Since all communications inside the Kubernetes cluster are encrypted and authenticated with this CA, letting it expire would be VERY bad. You would essentially lose all access to your cluster and the applications within 😱. If you search a little on the Internet, you may find horror stories from people who let their cluster’s CA expire (see at the end of the article). So, we had to find a way to renew the certificate, without any interruption for end-users, before the expiration date. With the CA certificate involved with all internal Kubernetes components, renewing is not a trivial task. To make things worse, the official documentation is incomplete on this matter, and external documentation on this topic is relatively thin on the Internet. There are multiple reasons for that. Many organizations use Kubernetes through a managed offer from their cloud provider, who hides the complexity of running it from them. In this case, providers worry about this for them (but do they really?). Among the remaining Kubernetes users, most of those running on-prem clusters like us use tools like kubespray or kubeadm , which contain some sort of certificate renewal procedures but were not available at that time. Then, on the very thin crowd that remains, regenerating CA by hand is not something that has to be done often. CAs are often generated with 3 to 10 years expiration dates (best practices dictate shorter periods). If you compare this to the relatively recent adoption of Kubernetes (a project initially released six years ago), you can assume that many CAs have yet to expire! How are we going to play this? We had to find out how to change Kubernetes CA on the fly with no end-user downtime. Luckily for everyone, if you plan it carefully, it should be possible for many workloads. The first thing to know is that, even if the cluster is unresponsive while the certificates are being renewed, the applications in Kubernetes ( Pods ) that are already deployed and running will try to continue to run. However, new Jobs won’t be scheduled, crashing Pods or workloads on failed nodes won’t be restarted. The second thing to know is that all Pods run with a security context given by a ServiceAccount . If you don’t specify a ServiceAccount in your application’s manifest, you’ll get the default one in the Namespace . What’s really important is that the Pod is granted access to the cluster according to the identity it was given (the ServiceAccount ), which is authenticated with a token. That token is generated using the Kubernetes Controller Manager . Once we renew the CA, all tokens are toast. You must regenerate them all and restart the Pods . So, the goal is to restart what’s required — nothing more — in the right order, then, renew all the tokens. All this quickly enough so that no application or node fails in the meantime. Easy peasy BUT! There are, of course, exceptions or else that would be too easy: Applications that need to communicate with the Kubernetes API will be disrupted until they are restarted completely (once they have their new token). This probably includes your monitoring (Prometheus), which scrapes your cluster’s API. You’ll fly blind for a few minutes. Applications maintaining long network connections open (like websockets) will probably be cut at some point because the IngressController you are using most likely requires access to the API server (see the previous point). If you’ve implemented a proper retry mechanism, that may not be a big issue. All applications will have to be restarted at some point. For apps with no replicas, a little downtime will be unavoidable. To work around this, always try to have multiple replicas for all your applications. If that’s technically impossible, restarting these applications may not be urgent, so that you may plan this later on. Now that you know everything, let’s do it 😊. Sooooo let’s renew a CA in Kubernetes with (nearly) no downtime Before doing anything, we need to be able to roll back everything if necessary. This means backups and tested procedures to restore them. Backup every certificate you currently use in /etc/kubernetes , but also in /var/lib/kubelet if there are any, and those of etcd . You might also want to backup all the Kubernetes ServiceAccount tokens. These tokens are generated by the Kubernetes Controller Manager and are used by your pods to communicate with the API server (we will talk again about this). Finally, dumping the whole etcd database might also be a good idea (if all else fails)… Using Hashicorp Vault Historically at Deezer, CA certs for the Kubernetes clusters were generated using standard openssl commands, based on the requirements described in the Kubernetes official documentation . This works perfectly (official Kubernetes documentation uses cfssl , but that’s the same thing). Unfortunately it’s not really efficient, not super safe (CA keys are stored on disk), and prone to errors. As most of our secrets are stored in Hashicorp Vault , we decided to take this opportunity to move the certs to Vault (which has its own PKI engine). I’ll not get into the details to use Vault’s PKI engine, but the idea here is to: create a new secret engine generate a CA that will be safely stored in Vault configure a role that will allow some basic configuration generate a bunch of certs with it Certs for everyone! You get a cert! And you get a cert! And you get a cert! (I hope you get the “Oprah meme” as well) In order to safely automate and deliver the certificates to the Kubernetes nodes, we used consul-template . consul-template is another tool from Hashicorp that you can use to generate configuration files based on templates, with variables (or secrets) found in Consul or Vault. Basically, the template for a single certificate file will look like this ( apiserver-cert.tpl for the API server certificate in this example): The consul-template file will look like that: And we’ll run a command like this one, which will replace all previous certificates with the new ones. Note: One issue with this approach is that each call of the consul-template executable will produce a new certificate. For individual certs like node certs, this is fine. But that’s not true for the API server certificate key, which has to be the same on every master node. So, we made an exception for all certificates on the master node: we generated them on only one node then pushed them on all the other master nodes. All the certificates are now refreshed. But not all the Kubernetes components support certificate reload on the fly. We will need to restart them in the right order! The first thing to do is to restart all etcd servers, roughly at the same time. Once you do this, the API server will be unable to query the Kubernetes cluster’s state. That means you will lose all control on your cluster 😱. At this point all future kubectl commands will fail. All Kubernetes functions (such as autoscaling, scheduling pods etc.) will stop working. So our priority is to restore this as soon as possible. This can be fixed quickly by restarting the api-server manually (or killing them with a SIGKILL if they are Kubernetes Pods and not systemd units). We can then start to properly restart the control plane components that rely on etcd or the API server. Start with the CNI (like flannel for example), delete the kubelet certificate/key pair and restart them on all nodes (which will automatically regenerate it). Note: kubelet certificates location may depend on your setup, but it usually can be found in /var/lib/kubelet/pki/ on all the nodes. Finally, we can restart all remaining Kubernetes control plane components using kubectl commands from the master node themselves. Our control plane is up again! Now that the control plane runs with the new certificates, if you look at the API server logs, you’ll see lots of cryptic error messages about wrong tokens. We have to refresh all the tokens stored in the cluster and that will be done automatically by the Kubernetes Controller Manager once we clear the Secrets . This loop will parse all the Namespaces , find all Secrets of type service-account-token and will patch the secret to remove the token from them. We can now move on to restarting applications, with priority to those that really need access to the API server. Restart coredns first (or kubedns , depending on your setup). Since the certs have been renewed, all internal Kubernetes name resolution should be failing, which will soon be problematic. But you should also restart every app that seems important to the function of the cluster, including (but not limited to): kube-proxy prometheus Ingress controllers any monitoring app Finally, restart all applications that need API access after that. All applications that don’t need access to the API server can be restarted when you have the time. Your Kubernetes CA certificate is now renewed; you can relax for a few more months 😊. Wrapping this up Renewing Kubernetes CA certificates is not a trivial task but we’ve learned that it could be done in most cases, with careful planning. We automated all the tasks described in this article in Ansible roles and played them on staging environments until we were ready to roll it out. Last December, we planned this intervention in all our clusters. They were all updated in a few minutes with no observed downtime on our services visible to end-users, and no more disruption than expected on applications that required access to the API server. I love it when a plan comes together Sources Github — Kelsey Hightower’s “Kubernetes the hard way” Postmortems on CA certs expirations Vadosware — 2019–12 K8s certificate expiration outage Building and breaking Kubernetes clusters on the fly Youtube — Continuously Deliver your Kubernetes Infrastructure — Mikkel Larsen, Zalando SE Kubernetes official documentation on certificates Kubernetes — Single Root CA best practices Kubernetes — Manual rotation of CA certificates Kubeadm — Manual certificate renewal Hashicorp Vault Hashicorp — Build Your Own Certificate Authority (CA) Hashicorp Vault — PKI Secrets Engine (API) Digital Ocean — Using Vault as a Certificate Authority for Kubernetes Youtube — Streamline Certificate Management www.deezerjobs.com Stories of engineering, data, product and design teams… 271 Kubernetes Certificates Hashicorp Vault Deezer Infrastructure 271 claps 271 Written by Senior SRE at Deezer Stories of engineering, data, product and design teams building the future of music streaming Written by Senior SRE at Deezer Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2021-02-11"},
{"website": "Deezer", "title": "using a go links service in your organization", "author": ["Cédric Delfosse"], "link": "https://deezer.io/using-a-go-links-service-in-your-organization-99666125bccc", "abstract": "Previous stories About Jobs When I joined Deezer three years ago, the onboarding process (called DeezCover) was undergoing a complete redesign, so mine was relatively brief. I struggled to find my way around the company’s organization and documentation as I stepped into my new job. Of course, there was an internal Wiki, but you know what happens to Wikis over time: every team sets up their own space and organization. Before anyone realizes it, the Wiki has become hundreds of pages long. Luckily I could count on my colleagues to help me find the correct, most up-to-date information, but I couldn’t help thinking I was bothering them unnecessarily. At the time, I remember telling myself: “I’m missing go links so much!”. I had experimented with them in my previous company and thought Deezer could benefit from them too. So one of the first items on my to-do list was to “Set up a go links system”. Go links are available via a short-link service, which allows you to quickly shorten a URL into something easier to remember and faster to type in your browser. This service first appeared at Google in 2010 and propagated to companies joined by Ex-Googlers. Here are some examples: go/wiki : your corporate Wiki home page at https://wiki.example.com go/hr : the HR homepage inside your Wiki at https://wiki.example.com/wiki/spaces/HR/overview go/vacation : Employee vacation request tool at https://deezer.example.com/vacation/leave-request go/rust : Rust programming guidelines at https://source.example.com/portal/rust-guidelines/README.md I think you get the idea now. You only have to type “go/wiki” in the URL bar of your browser, and the short-link service will redirect it automatically to your corporate Wiki home page. As seen in my examples, you can use go links to make it easier for employees to discover and reach essential company documents and resources . It’s faster to type a go link than to browse bookmarks for a given resource, and your browser autocomplete capabilities will make it even faster. Go links can also be used in internal communication (emails, slides, etc.) as they are more simple and convenient to share than long URL links no one can remember. Go links are not for everything. Focus on making shortcuts for: The apps and tools people use frequently. For example, the company internal documentation portal, the bug reporting tool, the company organization chart, etc. Any other resource someone thinks should have its own shortcut because it would be useful. Good go links are also: Logical so that they can be instinctively discovered. Short so they are easy to type and remember. If you are a large organization, you may want to define more precise rules about the wording of the shortcuts, but to improve the tool adoption, I would suggest taking a relaxed approach on this in the first few months. However, I would enforce these two rules: Use lowercase only. Use a hyphen to split words (no underscore). Coding a go link service is rather easy. At Deezer, we decided to install this simple implementation from Kelly Norton available on Github: https://github.com/kellegous/go . We made some small changes so that: The service supports URL like go/j/PROJECT-42 to easily access and share JIRA tickets. Short links can be listed and looked up in the database. Other implementations are more in-depth and allow to set up permissions on who can create/update/delete go links, but you don’t need all of that if you want to give go links a try. Go links have come a long way from their creation at Google. Thanks to the ex-Google employees who continued to improve upon short-link systems, go links have become essential to many professionals. I discovered them when working at Twitter. Back then, people were truly addicted to go links and would use them all the time. I have realized that I’m also addicted now as I can no longer work without this time-saving (or dare I say “life-saving”) tool, and set it up yet in another company, Deezer. Willing to enhance resource discovery and sharing within your organization? Give it a try! www.deezerjobs.com Stories of engineering, data, product and design teams… 21 Deezer Productivity Tools Golink Short Link Internal Communications 21 claps 21 Written by Technology Expansion Director at Deezer X Stories of engineering, data, product and design teams building the future of music streaming Written by Technology Expansion Director at Deezer X Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2021-01-13"},
{"website": "Deezer", "title": "using the multidisciplinary approach to drastically improve your performances", "author": ["Loïc Boulakras"], "link": "https://deezer.io/using-the-multidisciplinary-approach-to-drastically-improve-your-performances-4a8ca0515d90", "abstract": "Previous stories About Jobs When it comes to solving a problem, developers often tend to use the technologies they’re comfortable with — the technologies they work with on a daily basis. Most of the time this is fine — we work within a specific scope in which this technology makes sense and has already proven relevant and efficient. But have you ever encountered this case where, while it works, your solution is either too slow, consumes too many resources or is simply unsatisfying? This article aims to introduce you to the multidisciplinary approach, which consists in making different disciplines work together to reach a goal. Although you might focus on one or a few specific technologies, in a software company you’re likely to work with other people tackling other subjects with different tools. The first step will be to study the disciplines at hand to see what each of them can achieve and whether it’s worth investing in them or not. Once the list of technologies you see fit is established, you need to bridge them to work together to implement an actual solution. A recurring mission in our catalog team is to import huge files regularly. Such tasks are kind of a routine, but the scripts we create can sometimes take a lot of time to run — ranging from one to over a dozen of hours . With an ever growing quantity of data to ingest and a limited pool of back-end machines, this started to become an issue. We recently had to rework from scratch the way music lyrics are imported, that was the perfect opportunity to try something different. We receive lyrics in the form of gzip compressed files of about 10 million lines, with each line being a JSON string. Every day. Most of the content in these files is already in our databases, so on a day-to-day basis the ratio of new lines, updated lines, and deleted lines is rather small. Yet the solutions that were implemented in the past are not always efficient enough to tackle this problem in an acceptable time frame. Back in the days when these files didn’t exceed 1 million lines, the most naive solution was adopted: a loop to insert or update each line along with a reference to the file, after which we would delete all entries containing a different reference. There are at least two problems with this approach: First, we had to process every single line from the file to hydrate an entity with it. Second, we’d update every single line in the database — as long as it’s in the file — in order to change the reference. Even with transactions that’s a lot of useless queries and it’s downright slow. It’s also worth mentioning that the reference column needs to be indexed for the delete query to run efficiently. A couple years back we decided to improve our ingestion process, and implemented another concept. The idea here was to use what we called buffer tables — basically a copy of our regular table minus the indexes and other additions — that would act as a way to compute differences (additions, updates, deletions) with our real tables. The first step consists in formatting every line in the file and inserting it in the buffer table (if no formatting was necessary and our input file was a CSV , we could import it straight into MySQL , which would be sweet). Insertions and updates are then handled by a single insert on duplicate key update query and the deletion is handled by a single update left join query where the right columns are null. Auto generated query for insertion and update Auto generated query for deletion There are a couple of advantages with this solution: It allows us to calculate the differences before actually applying them which enables us to have a protection mechanism to cancel the import in case an abnormal amount of lines changed (never trust files coming from third party sources!). MySQL is rather efficient at batch update since we rely on the primary key. However some issues remain: Every single line from the file has to be formatted and inserted in a table. It forces us to have twice as many tables and entity / repository classes. It’s still slow overall (a couple hours). That’s when I started to ask myself what could be done differently and experiment with a bunch of other solutions. My goal here was to find a better way to compute a diff. Since MySQL wouldn’t make it easy, I decided to run a diff against the previous file instead. After a few experiments I came up with the idea of indexing all the lines from my file into a key-hash YAML document. Given the following file where the foo property would be our id: The following YAML would be produced: I then used an open-source Go tool to run a diff between the 2 generated YAML files and parse the output to either insert, update or delete lines. The result was much faster (down to a few minutes), but I met new problems: Very memory intensive. I would still have to loop through the whole original document to retrieve the data to insert or update since the parsed diff only gives me the line id and an actually useless hash. But wait a minute. Aren’t I basically trying to compare two datasets? Sounds like the daily routine of our data engineers. Let’s upload our files to HDFS (Hadoop Distributed File System) and open a notebook on Zeppelin (a Spark interpreter within a web interface) to try some stuff as a proof of concept. And that’s it, we’re now down to a couple of minutes only. Now as we were discussing at the beginning of this article, the multidisciplinary approach consists in making different disciplines work together to reach a goal. We still have to plug everything together though, so our back-ends can communicate with our big data clusters and vice versa. Interactions between back-end and big data could be designed by either: Using a messaging system such as Kafka. Triggering jobs on both sides using our schedulers’ APIs, and access files as well as generated diffs using a volume or an ftp. By using the different tools at our disposal we have been able to create a working architecture with different technologies effectively communicating with each other. The PHP back-end is still responsible for importing the lyrics, however, the computation of which lines should be inserted, updated, or removed has been delegated to the big data cluster. Doing so allows us to speed up the whole ingestion process a hundredfold, which reduces its runtime from multiple hours to just a couple of minutes and a very low CPU time for our PHP back-ends — a resource we strive for. We believe such an approach can be applied to solve many problems and are working more and more closely with other teams — notably by enabling bridges between our stacks. www.deezerjobs.com Stories of engineering, data, product and design teams… 326 PHP Backend Big Data Multidisciplinary Deezer 326 claps 326 Written by Stories of engineering, data, product and design teams building the future of music streaming Written by Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-10-01"},
{"website": "Deezer", "title": "lyrics in french hip hop curious iam and you should be too", "author": ["Arthur Pace"], "link": "https://deezer.io/lyrics-in-french-hip-hop-curious-iam-and-you-should-be-too-be62b2b6e7ab", "abstract": "Previous stories About Jobs In 2018, hip-hop was the most consumed genre in the United States. Since then, many analyses have dived deep in understanding the lyrics of those artists. However, as the pioneer of music streaming in France, we wanted to know more about how our local stars created their art. With the wealth of data surrounding the lyrics that we own at Deezer, and because millions of people used the “view lyrics” button on Deezer in 2019, we thought it would be interesting to conduct a study. Let’s go back in time and see just where lyrics come from. First, the word “Lyric” is ultimately derived (via Latin) from the Greek word “Lyrikós” , the adjectival form of lyre , the ancient string instrument. Greek lyric poetry was often recited accompanied by the dulcet tones of the lyre. This paved the way for millennia of musical storytelling, passing through Homer’s Odyssey all the way to Aya Nakamura’s Djadja. Although music itself has likely been around for at least 55,000 years, the oldest known (complete) composition with both lyrics and musical notation that historians have been able to dig out comes from the Seikilos epitaph, written around the 1st or 2nd century AD. Short but sweet, it is thought to be a dedication from husband to wife during the Hellenistic Greek period: “While you live, shine, have no grief at all Life exists only for a short while and time demands its toll” Since then, lyrics have evolved and the number of words per song has increased. The birth of rap and hip-hop helped accelerate this trend. Throughout this article we will talk a lot about unique words and the vocabulary diversity rate . Let’s define them! A unique word is a word used by an artist once or several times in their songs, excluding stop words (stop words usually refer to the most common words in a language: “I”, “you”, “with”…etc). For example, in Rihanna’s “work work work work”, “work” is a unique word . Vocabulary diversity is a score. The closer the score is to 1, the less the artists repeat themselves. The closer the score is to 0, the more the artists repeat themselves. For example, in Rihanna’s “work work work work,” the repetition rate is 4 because we have 4 words but just 1 unique words. Accordingly, the Vocabulary Diversity Rate is ¼ = 0.25 . www.deezer.com In 2019, the top 200 Francophone artists streamed on Deezer were composed of 142 Rap & Hip-hop artists, 39 Pop artists , 14 Rock artists , 2 Electro artists and 2 Reggae artists . This means that Rap & Hip-hop accounted for more than 70% of the artists. This highlights the importance of the genres. First, let’s try to understand what differences there are between rap and other genres in terms of lyrics. In order to have a general overview, we decided to plot a map of the artists’ vocabulary diversity on the X axis and the number of unique words on the Y axis. Like above, the colors are split by genre. Unsurprisingly, a rapper, Rohff , is the artist who has the largest vocabulary. Somewhat more surprisingly, the artist with the lowest repetition rate (the point the most to the right) is an electro artist. However, on closer look, the reason is intuitive; the majority of the artist’s songs contain few lyrics (hence the low ‘unique words’ on the left), and it is, therefore, less probable that the artist will repeat themselves. Finally, through his stories that both praise and critique French society, the rock artist with the largest vocabulary is French folk rock icon Renaud. www.deezer.com For the rest of our analysis, we will focus on the 3 main genres (Hip-hop, Pop, Rock). Digging deeper, we see below that rappers are the artists who use the most unique words: on average 2,938 unique words per artist. As opposed to pop artists, who are the ones who use the least (1,651 unique words / artist). It is interesting to note that pop artists are the ones who repeat themselves the most, while having the least unique words. Let’s take the example of two popular songs from the year 2019: La vie qu’on mène by Ninho (Rap) Pookie by Aya Nakamura (Pop) Both pieces have (approximately) the same duration and the same number of words, but don’t have the same number of unique words . The difference is huge, with the number of unique words in the rap song more than twice as many as in the pop song. www.deezer.com www.deezer.com We then ran a ‘sentiment analysis’ on the lyrics to see if our French rappers are negative (or positive) compared to other artists. A sentiment analysis is the process of identifying the emotional tone behind a series of words. For this particular analysis, we decided to look at every word used in every song by every artist. For the most curious among you, we used the TextBlob python library, which “provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more.” ( https://textblob.readthedocs.io/en/dev/ ) It seems that our rappers are not much more negative than other artists, but seem to be more neutral in their lyrics (i.e. less positive). The most positive artist is not a rapper. It’s Aldebert, an artist who writes songs for children. About 17% of the words he uses are positive. www.deezer.com In order to better understand and measure the difference between our rap artists and artists from other genres, we looked at the top 20 words they use. Let’s take a closer look at our data by checking the top 20 most used words in our 3 main genres. The first thing that we can notice is that there are certain words that appear in each of the tops, like: “Vie”, “Trop”, “Rien”, “Jamais”, “Toujours.” The word “Vie” is the most used word by pop and rock artists, and by our rap artists too (apart from the word “Ouais” ). Other words are specific to each genre. Rappers will be more likely to use words like “Yeah” , “Gros” , “Mère” , “Seul” and “Sale.” Meanwhile, pop artists will say words like “Besoin” and “Bébé” . It’s interesting to see how the words used by the artists of a genre reflect the identity of the genre and the image it projects fairly well. This article highlights facts on lyrics and is not judgmental. It certainly doesn’t mean that pop artists are any less creative than artists of other genres, as musical tastes are subjective , and each genre has its own strengths and qualities , be it the diversity of lyrics or catchy, melodic choruses. That’s what music is all about: different genres, a variety of lyrics, a vocabulary that changes between rap, rock, pop, reggae… etc. But we all listen to music. For Deezer, lyrics are more than just a feature, they are a strength, a giant source of data that can tell many stories. The objective of this analysis is to highlight the power of lyrics data , which can help us, at Deezer, improve our content and recommendations. www.deezerjobs.com Stories of engineering, data, product and design teams… 196 Thanks to Pauline Munier and Sabrina Aguirre . Lyrics NLP Rap Hip Hop Deezer 196 claps 196 Written by Computer Science & Information Systems Engineer | French winner of the AWS Deepracer League 2019 | Linkedin : https://www.linkedin.com/in/pacearthur/ Stories of engineering, data, product and design teams building the future of music streaming Written by Computer Science & Information Systems Engineer | French winner of the AWS Deepracer League 2019 | Linkedin : https://www.linkedin.com/in/pacearthur/ Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-09-08"},
{"website": "Deezer", "title": "hey siri play my flow on deezer part 2 integrating sirikit with deezer", "author": ["Jean-Christophe Paris"], "link": "https://deezer.io/hey-siri-play-my-flow-on-deezer-part-2-integrating-sirikit-with-deezer-57c20359a0c1", "abstract": "Previous stories About Jobs co-authored by Martin Lukacs Following WWDC19 announcements, Deezer quickly decided to support Media Intent with SiriKit. To know more about all the benefits of integrating a voice assistant in your app, you can read this post written by Anna Louis , Product Owner of Voice at Deezer. Adding support of SiriKit in your app is as simple as… Wait, nothing is that simple! In this post, you won’t actually find a “How to do it” tutorial. It would be too long and would only be copying what Apple already provides with this video and this sample code . Instead, you will discover what it has meant to make this feature work in Deezer’s iOS app. Throughout this article, we will take a closer look at several interesting aspects of Siri implementation in an iOS application. You will discover how to work with the Apple extension dedicated to supporting Siri intents and discuss the relative complexity of application-extension communication. At this point, you’ll be ready to dive into the specifics of working with and supporting Media Intents. Finally, we’ll finish off with a few tips on Siri’s vocabulary comprehension. A good introduction to how an app extension works can be found on Apple’s developer documentation . To interact with Siri, iOS will use an app extension to launch a lighter process of our app that will only aim to support those kinds of interactions. Once again, Apple documentation to create an Intents App Extension is well written on. So in this section, we will focus on how, at Deezer, we have created this extension while trying to reuse and share as much code as possible. What are the tools our app can provide to make the extension work flawlessly? A tool to make network requests to our API The current user session — to update their library and check their rights to access some content (to fail early and return the appropriate .subcriptionRequired response code) The user’s playlist metadata — to improve the local search of content and avoid a network request By chance, Deezer recently had created an Auth.framework module to manage authentication across our different apps. This framework supports SSO (sign in on an app and share these credentials with another app). This facilitates the work to share user session credentials to the extension and authenticates the user from the Siri extension. To support network calls to our API in Siri extension without duplicating code, we shared some files between our main app and the extension. To do so, it just required to check a box: This allowed us to share all the logic of a request to our API: setting default HTTP headers, default query parameters and configuring default processing of our API response. Lastly, to share some data between our app and our extension, we created an Interface SharedStorage that uses an instance of UserDefaults(suiteName:) to store simple data. For example, some metadata composed of only the name and the ID of each user’s playlist. One downside of working with Apple Extension is that the code we write is difficult to unit test: To test an app extension using the Xcode testing framework (that is, the XCTest APIs), write tests that exercise the extension code use your containing app as the host environment ( source ). There are 4 types of Media Intents available since iOS 13. Deezer chose to support all of them from day one: 🎶 An intent to support playing media: INPlayMediaIntent ➕ An intent to add a media to a playlist or the user library: INAddMediaIntent 👍 An intent to like or dislike a media: INUpdateMediaAffinityIntent 🔍 An intent to search a media and open it in the app: INSearchForMediaIntent Each INIntent has the same 3 following phases: In this first phase, we will check if the received parameters are valid and if we have enough information to handle the intent. For media intents, it usually means finding content in the Deezer catalog using information from the INMediaSearch object. Our extension won’t resolve the parameters of INPlayMediaIntent and INAddMediaIntent in the same way. Let’s see why. To understand the difference, we should look at what behavior the user expects from a request and what we want to support. What are you more likely to ask Siri? “Hey Siri, tell Deezer to add Makeba to my Chill playlist” or “Hey Siri, add this song to my Chill playlist” In the case of a playing intent, if we don’t play exactly what the user asked for, he can just try again and expect better results. In the case of adding the media to the user’s library or a playlist, this can be more of a problem. The operation will be done but if we matched the wrong content, then the user will have unwanted media in their collection. By supporting only .currentlyPlaying for INAddMediaIntent we know that we will have more information at hand. INMediaSearch object is indeed prefilled with information from MPNowPlayingInfoCenter . So all properties: mediaName , artistName , albumName … will exactly match what our main app player has set in the nowPlayingInfo dictionary. As INMediaSearch doesn’t contain our internal track identifier, we will have to retrieve it with the help from our search engine. According to the type of intent and the information on INMediaReference property, we can alter our search behavior: For Play and Search intent — we will use our default search as you would use our search inside the application, and we will query our search using the terms available in INMediaSearch object For AddMedia and UpdateAffinity intent — we will only resolve for .currentlyPlaying content and then use this information to search for an exact match of the song title, artist name and album name This phase is usually used to confirm with the user what the app understood before proceeding (like making sure the order is correct before making the payment). In the context of a media intent, Apple doesn’t recommend using this phase to smooth out the process during Siri interaction. To handle the intent, you can either do the action from the extension or delegate it to the main app. For INPlayMediaIntent we have to first launch the app to play the audio. This is because our extension has a short living period and will be killed after handling the intent. We will keep the app launched in the background so that we can start the audio without interrupting what the user is doing. To request a background launch, just add this line in your handle phase: completion(.init(code: .handleInApp, userActivity: nil)).To handle the intent, you can either do the action from the extension or delegate it to the main app. For INPlayMediaIntent we have to first launch the app to play the audio. This is because our extension has a short living period and will be killed after handling the intent. We will keep the app launched in the background so that we can start the audio without interrupting what the user is doing. To request a background launch, just add this line in your handle phase: completion(.init(code: .handleInApp, userActivity:nil)) . INSearchForMediaIntent expects you to open the app and show the search results to the user. This can be done using the .continueInApp response code. But when looking at INUpdateMediaAffinityIntent and INAddMediaIntent , things become more challenging. Both intents could be handled directly by the extension. This would prevent the app from being launched in the background, taking more resources and time to execute the action. But is this task easily doable inside the extension? To do so we would need to access some classes that manage the user’s library state which, in turn, will also access some persisted information (the extension will not be granted access to the app storage so we need to store our data in some shared container ). From here, it seems we don’t have much of a choice. To avoid a huge refactoring of an important piece of our code, we will have to hand over the control to the app to execute the action. Before making this decision, we have to look at one last thing, the documentation. There, you will discover that Apple doesn’t allow .handleInApp as a response to handle the INUpdateMediaAffinityIntent . Hopefully, handling the media affinity updates can be quite simple: Make the network call to update the affinity state on the server’s side Ask the app to refresh the cached affinity state (usually done at the app launch to get the changes you may have done on another device such as a desktop, but here we must take into account that the app will already be running) To handle the latter, after making the network call, we will set a flag in a shared storage (using UserDefaults(suiteName:) ). In the app, we listen to UIApplication.didBecomeActiveNotification notification to check this flag and trigger the necessary refresh. Siri already does a lot of work to automatically recognize and match artists, albums and even song titles out of the box. This process seems to rest on Siri’s knowledge of Apple’s Music catalog. But, there are still some cases where Siri falls short and will require some help to process the intent. “Hey Siri, play my lockdown playlist” For user playlists, there are cases where some entities are fully customizable and unpredictable. Siri provides an API through INVocabulary to feed it new content. It enables you to associate certain words with a specific content type. In our case, we mostly used .mediaPlaylistTitle vocabulary type attached to the user’s playlist names. This enables users to request their playlists by name through Siri. In the Deezer app, we configure a PlaylistVocabularyUpdater object that listens to any playlist update in the user’s library and keeps the user’s Siri vocabulary always up to date. Another kind of vocabulary interesting to focus on is specific company brand names. “Hey Siri, play my Flow in Deezer” In this particular case, Siri will not recognize what “Flow” is, and putting this term in the Deezer search engine will not provide any result. Flow is our app custom terminology to identify a custom playlist that contains all your favorite music, mixed with fresh recommendations and songs you forgot you loved. One solution to tackle this problem would have been using Global Vocabulary Reference , unfortunately, it doesn’t apply to Media Intent. Instead, during the resolve phase, we will look for the word “flow” to resolve against our Flow custom INMediaItem . This has some caveats. If a song / album / playlist contains the word flow, it will never be found by Siri because of us bypassing the search process, as we will always prefer matching flow to our custom experience. (hopefully, this ‘flow’ term is rare enough to not conflict in most cases and still offer a great Deezer experience). “Hey Siri, play my favorites on Deezer” “Hey Siri, play new releases on Deezer” As you can imagine, there are many ways to ask Siri to play such things. Just to give you an example it could be any phrase containing words like “play favorite music” or “fetch new releases.” Consequently, it would be very difficult to use the flow technique discussed above to match this kind of intent for all languages. The task doesn’t seem easy at first. But, while conducting tests, we discovered that Siri has built-in helpers for these cases. When interpreting these generic media intents, Siri will always return some exact string in INMediaSearch.mediaName be it \"new releases\" or \"my favorite mix\" . This makes it much easier to parse the user’s intention and respond to it. There may be more of these prebuilt catchphrases that we didn’t discover and we hope that Apple will document them in the near future. Overall our journey in the implementation of Siri was very interesting and rewarding. Seeing first hand the progress in language processing and what can be done with it is impressive. We can just hope and see what the next big step can be built on top of that. www.deezerjobs.com Stories of engineering, data, product and design teams… 130 Thanks to Pauline Munier , Martin Lukacs , Bertrand Longevialle , and Zouhair Mahieddine . Siri iOS Voice Assistant Deezer iOS App Development 130 claps 130 Written by iOS Software Engineer at Deezer Stories of engineering, data, product and design teams building the future of music streaming Written by iOS Software Engineer at Deezer Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-07-08"},
{"website": "Deezer", "title": "hey siri play my flow on deezer part 1", "author": ["Anna Louis"], "link": "https://deezer.io/hey-siri-play-my-flow-on-deezer-part-1-7cb5232c61d5", "abstract": "Previous stories About Jobs Voice interactive devices and services have significantly spread over the last four years. Consequently, voice assistants have become smarter and more sophisticated in their ability to handle new and complex demands. Moreover, voice assistance has also become an available feature in more and more devices. In 2018, Apple announced that there were 500 million devices in the world that used the Siri voice assistant. Not surprisingly, according to Voicebot.ai in 2019, requesting music features were also among the top most common uses for Smart Speakers. In June 2019 when Apple announced during WWDC19 that Siri would be opened to third-party music providers in the iOS13 version, it was great news for Deezer as it meant that we could offer our users a new way of accessing music. As a result, our users can now listen to Deezer by navigating the app from their mobile device or Smartwatch , by opening Deezer on their TV and also, if they’re using an iPhone, they can easily activate Deezer just by saying ‘Hey Siri, play my Flow’ From a Product perspective, we no longer talk about “UI” — User Interface — but “VUI” for Voice User Interface. This VUI is available on any iOS phone updated with the iOS13 version (or any later version). Once Siri has been enabled into the iOS settings, the user can unlock their phone by saying the wake-up-word ‘Hey Siri’ or just by pressing the Home button. VUI can change depending on the device. As a result, the way a user interacts with it is dependent on whether the voice assistant is integrated into a watch, Speaker, or TV. For example, to interact with Siri on your Apple Watch, you’ll need to use Siri Raise. Siri Raise means that you can talk to Siri by raising your wrist and speaking into your Apple Watch. For now, Deezer is usable through Siri with a mobile device only📱. The experience is defined by the scope of intents that the voice assistant understands. Following Siri’s scope of comprehension defined for third-party music providers, we started a POC — proof of concept — to begin our work with ‘ must-have ’ intents: songs, albums, playlists, artists, mixes . We released the POC version in December 2019 to enable users to test the new integration at once and quickly get their feedback. A few weeks later, in January 2020, we added the last two missing intents: podcasts and live radios . Therefore Deezer now offers its full content through voice assistants. Users just need to make sure that they call “on Deezer” for each of their requests: ‘Play my Flow on Deezer’ ‘Play Beyonce on Deezer’ ‘Play my Sunday mood playlist on Deezer’ ‘Play the TED talks daily on Deezer’ Users can also control the player by voice by saying ‘Pause’, ‘Resume’, ‘Volume up’, ‘Volume down’, ‘I like this song’, ‘I don’t like this song’, ‘Turn off Shuffle ’, etc. If you’re interested, all the other commands are accessible here . Siri is not the first voice assistant Deezer has been included on. Since 2018, Deezer has also been available on both Google Home and Alexa. Even though all assistants work in a similar manner, for each voice assistant that we partner with, we develop a custom integration plan that specifically aligns with that architecture of that partner’s device. Each voice assistant or Artificial Intelligence is made of an algorithm of language comprehension. As any child learning to speak , the more he reads, the more he gets used to words and the associated ideas behind those words. This is why an AI without any data is like a baby starting to speak. Exponentially, the AI will get smarter and reach the level of comprehension of a 7 years old child within a few weeks. The algorithm will scan hundreds of words that are referred to as entities in the machine learning world. In the case of Deezer, we want the algorithm to learn all song titles, artists, albums, playlists, etc. We have a basis of more than 55M entities, and a delta of new releases added every Friday . Once the algorithm has seen the full catalog, it will be able to recognize all the entities when someone asks for them. In addition, we also expose the algorithm to some actions like ‘Play’, ‘Pause’ or ‘Resume’, so that the AI also learns to detect the action to perform when the entity is found. This intention and entity recognition is called ‘Natural Language Understanding/Processing’, “NLU” or “NLP” for our Data Scientists friends. So now, if I say ‘Hey Siri, play Imagine by John Lennon on Deezer,’ what happens? The NLU engine will get the action ‘ play ’, the track’s title ‘ Imagine ’ and the artist name ‘ John Lennon ’. Then it will simply get the corresponding track with this song’s name and artist from our catalog and play it. For each entity, we also share metadata (the genre of a track, its popularity rank, its ID, etc.) so that it’s easier for the search engine to find the right entity. For voice assistants such as Google Home and Alexa, the entity recognition and search engine within the Deezer catalog are managed respectively on Google’s and Amazon’s side. Consequently, Google and Amazon trigger the entity with the highest popularity rank provided by our catalog and directly play the top one entity. For Siri, this is slightly different. The two matched entities: ‘Imagine’ and ‘John Lennon’ will be given to Deezer as ‘raw data’. Then it’s up to Deezer to find the right entity. For Siri, we are using the search engine of our iOS mobile application. Another option for us would have been to develop a new search endpoint dedicated to Siri use cases. However, after assessing this solution, we realized that time and work it would require would not outweigh the endpoint’s gain in terms of the performance and feature improvements that it would allow. In order to boost our search results, we shared some vocabulary with Siri so that the Siri NLP algorithm would learn Deezer’s specific content such as moods, genres or users’ playlist names. For example, if you ask Siri to ‘Play some music to fall asleep’, Siri will return ‘moodNames:(meditation)’ . On Deezer’s side, if you search for ‘meditation’ as a mood, we will return the Zen mix . The user who asked for music to fall asleep will, therefore, be played the Zen mix. We tested all the genres and moods available on Deezer in order to map them with genres and moods available and understood by Siri. If you wish to know more, the technical integration of Siri into Deezer is explained in another post . As voice assistants are available with more and more devices, usage is continuing to grow and users are getting used to this new way of interacting with their devices. Our success rate (i.e. the rate of requests that returned a content streamed) on Siri is high thanks to the exemplary level of comprehension of our AI and the efficiency of our search engine. We realized that the most used commands are the simplest ones like: ‘Play some music’, ‘Play my Flow’. In comparison, other phrases like ‘I like this track’ or ‘I don’t like this track’ are not often used. Yet, in particular, Deezer Flow is well suited for voice usage as it allows users to get a mix customized to their music taste with a single voice command. At Deezer, we are proud of this voice integration and we hope that you will check it out (if you haven’t already), and enjoy it as much as we do! We are currently looking forward to expanding this new system to other devices. Thank you for reading! www.deezerjobs.com Stories of engineering, data, product and design teams… 118 Voice Assistant Siri Deezer Music Streaming Artificial Intelligence 118 claps 118 Written by Product Manager 🎧 Stories of engineering, data, product and design teams building the future of music streaming Written by Product Manager 🎧 Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-07-08"},
{"website": "Deezer", "title": "private libraries in android why you should consider it", "author": ["Jean-Baptiste Vincey"], "link": "https://deezer.io/private-libraries-in-android-why-you-should-consider-it-91dcc1af3d7b", "abstract": "Previous stories About Jobs This post is part of a series on private libraries: Part 1: Private libraries in Android — why you should consider it Part 2: Private libraries in Android — how to manage them Libraries are one of the foundations of Android development. Think about all dependencies in your Android projects, either from Google (the former Support Library and now AndroidX , Material Components , etc.), or other companies (like Square with their famous OkHttp or Retrofit ), or developers who simply shared their own work. The interest of these public libraries is quite obvious: it allows anyone to reuse in his own project a component made by somebody else. What about private libraries ? In what situation should we consider a library with a scope specific to some context or organization? In this post, we will define the concept of a library and discuss its primary purposes including the context which we consider private libraries interesting at Deezer, the content we put into libraries and how we organize them. Although we will talk about Android development, many principles will also apply to other areas. The concept of a library has been around since the beginning of computer programming. A library can be defined as such: An independent collection of behaviors, accessible through a well-defined interface and available for reuse by multiple programs. In the context of Android, a library corresponds to a versioned, compiled artifact: either a JAR (Java archive) or an AAR (Android archive, that can also include Android specific resources), not to mention native libraries (C / C++ libraries with NDK or more recently Kotlin Multiplatform libraries). Libraries can also be composed of multiple artifacts under the same versioning. At Deezer, each library is an independent Android project with its own repository that can have multiple modules (with possible dependencies among them), each producing an AAR or JAR. Consequently, unique versioning can apply to the entire library (i.e. to all modules of the library). Modular architecture is based on Android modules and, in recent years, has become the standard of Android development. What could be the benefit of a library over a module in an Android application project? An Android library is structurally the same as a module. Therefore it shares most of the benefits of modularization : reduces build time splits project code into smaller (more manageable) pieces enables better separation of concern enforces better interface and abstraction allows using different architectures and technologies makes refactoring easier helps in scaling development teams Interested in diving deeper into the benefits of modularization? You might want to check out this series by Jeroen Mols. Between libraries and modules, there is one major difference: a library, as a compiled artifact, is frozen . Any change to the library requires a new artifact to be built within the newer version. On the contrary, an Android module is not frozen. Instead, it directly references the source code and is recompiled with the main project whenever a change occurs. In practice, a library requires much more work to integrate than a module, especially when it continues to evolve. As previously mentioned, developing and integrating changes in a library implies building a new version. This includes uploading it onto a repository (e.g. Maven), and possibly even setting up a specific configuration for local integration tests before its publication. On the contrary, making changes on a module does not require any specific action. Moreover, the only consequence of such changes are that the first build is slightly longer. Hence, it is often more convenient to use a module. However, there is one case where libraries might be of greater interest when compared to modules. This is when behavior is shared between multiple applications . In these situations, modules are not appropriate because any evolution of a shared module would have an immediate impact on every app using it. This would then require that the code be adapted on all apps at once in order to immediately test the potential impacts. On the other hand, since a library is provided as a frozen artifact, it can evolve independently from any app that might be relying on it. The changes will then be delivered in a newer version of the entire library. Thereof, the newer version with the changes can be independently integrated in any app at any time. At Deezer, we are working on growing a series of applications for the Deezer app that will be usable on smartphones, tablets, Android TVs and Wear OS. However, we are also expanding our brand by developing additional standalone apps such as Audiobooks, 360, Analytics, and more to come soon. All of these apps have many common behaviors. These behaviors include things like playing audio content, authentication and signup, requests and caching, some domain logic, style guide and UI components, etc. For these common behaviors, we developed libraries in order to speed up development and ensure consistency among all our apps. In practice, we have currently 3 Android libraries: Core: Manages the player, domain models, requests and caching Design System: Provides the Deezer style guide and shared UI components Server-driven UI: Implements server-driven User Interface There is no absolute rule about the type of behavior that can or cannot go into a library. In theory, it can be almost anything. However, at Deezer, we make sure that all behavior going into a library first meets a number of criteria: The behavior is effectively used in several applications Inputs and outputs are defined with a clear and documented interface The behavior is identical for all projects relying on it Trying to anticipate what behavior might or might not be used within different applications can be cumbersome, and often ends up being incorrect. Consequently, at Deezer, whenever a new behavior is required in an application, we develop a module within the application’s project. Then, we wait to move the module to a library until it is needed in other apps. Another common challenge is when behaviors are very close but not exactly identical among the apps. If a little amount of configuration can sometimes be acceptable for shared behaviors, we think that app specificities should generally remain on the app side and not go into the libraries. This is why we try to extract what is actually common among behaviors and let the apps handle the specifics. Let’s take a concrete example: at Deezer, we are developing multiple applications relying on the same authentication service. Wouldn’t it be nice to have a ready-to-use authentication feature (including the UI and all the logic), whose entry point would simply be a deeplink? This would allow the Deezer app, as well as the 360, Audiobooks, and Analytics app, to integrate authentication in just one line of code. It would be something like this: The idea might seem appealing. However, a library designed this way would have an important flaw in Deezer context: it does not allow any customization of the User Interface. Thus, projects such as the Wear OS and the Android TV apps could not benefit from it, even though it relies on the same authentication service. In fact, even our smartphone apps require a certain amount of specificities in their interface. To tackle this, we made 2 libraries. The first, the authentication library, exposes a Repository. The second, the design library, provides a style guide and shared UI components. This way, every app can easily build an authentication funnel with these ready-to-use building blocks while simultaneously keeping enough flexibility to create a specific interface. Should we have a few big libraries, each encapsulating a bunch of shared behaviors? Or should we split them into many libraries that are only responsible for a single unit of behavior (meaning possibly dozens of libraries)? On one hand, keeping all shared behaviors in a single library makes it difficult to understand and maintain. Every little change on one part requires to release new versions of the library, while other parts might not have changed at all. Also, a new version of the library might include changes for totally independent parts. This obliges clients of the library to integrate new versions very frequently making changes difficult to understand and follow. Just think about the former Android support library (before we got AndroidX with independent versioning). Then, every new version included thousands of changes. This is why when you wanted to upgrade to a new version of the support library for a specific feature, you as well had to adapt many other features. Consequently, you couldn’t be sure of all the impacts and behavior changes an upgrade would bring. On the other hand, integrating and maintaining private libraries is quite heavy in terms of process, as it goes exponentially with the number of libraries. Hence, this comes back to what Uncle Bob explains with the Component Cohesion Principles : Release Reuse Equivalency Principle (REP) : the granule of reuse is the granule of release. Only components that are released through a tracking system can be effectively reused. Common Closure Principle or (CCP) : the classes should be closed together against the same kinds of changes. Related to the definition of the Single Responsibility Principle (SRP) given by Uncle Bob: Gather together those things that change for the same reason, and separate those things that change for different reasons . Common Reuse Principle or (CRP) : the classes in a component are reused together. If you reuse one of the classes, you reuse them all (which also means that we shouldn’t force users of a component to depend on things they don’t need). These principles tend to conflict with one another. REP and CCP are inclusive and make components larger, but CRP encourages us to make them as small as possible. The challenge is to find the right balance, addressing the needs of each project reusing the component. A good architect finds a position in that tension triangle that meets the “current” concerns of the development team, but is also aware that those concerns will change over time. — Robert C. Martin At Deezer, we try to compose our libraries with these principles in mind. When a behavior or group of behaviors is totally independent, we extract it in a separate library, following the CRP principle (e.g. the server-driven UI was recently extracted from our core library to its own library). However, when a behavior is tightly coupled with other ones, we let them in the same library as CCP principle suggests (though possibly in different modules, that allows us to load only the needed behaviors in each project). Finding the best architecture that addresses all the needs is quite a challenge. At Deezer, this statement particularly rings true, especially with our constantly growing number of projects. This is why we are often challenging our modularization and library organization to better fit evolving requirements. Still, at Deezer, the benefits of having private libraries are clear. Private libraries have allowed us to develop new applications at a very fast pace, allowing us to leverage reuse of behaviors , while ensuring consistency among all our apps. Discover how we manage private libraries at Deezer in the second post of the series . www.deezerjobs.com Stories of engineering, data, product and design teams… 384 2 Thanks to Arthur Guibert and Pierre Benayoun . Libraries Android Android App Development Deezer AndroidDev 384 claps 384 2 Written by Android Engineer @Deezer Stories of engineering, data, product and design teams building the future of music streaming Written by Android Engineer @Deezer Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2021-03-04"},
{"website": "Deezer", "title": "introducing an improved deezer experience on xbox access your favorite songs directly while", "author": ["Nathan Tourreille"], "link": "https://deezer.io/introducing-an-improved-deezer-experience-on-xbox-access-your-favorite-songs-directly-while-e784af059f7b", "abstract": "Previous stories About Jobs Until now, users only had access to the Deezer mini player on the Xbox guide. With our latest update, we’ve added a new feature to help our users quickly find the best soundtrack while playing. By pressing the Xbox button on your controller, you access the Xbox Guide. This is the quickest way for you to find out what your friends are up to, check your messages and notifications and view your most recently used apps and games. Our updated application brings improved background music control support. Previously, our player allowed users to access media controls (play, pause, next, previous) and displayed the current track’s cover. Now, Deezer (since version 4.1.0.70 ) displays up to 20 additional playlists right under the media controls, making it easier for users to quickly access recently played contents, their music library and their Flow. During your next game session, open the Xbox guide and select a new playlist to start listening to it! We first need to register the EBM feature in the package manifest (which contains the app’s package identity and dependencies, required capabilities, etc). We also need to specify the name of the app service that our application uses to implement the feature. This app service enables the EBM feature to wake up our app in the background and start a direct line of communication with it. App Services can now run in the same process as the foreground application, which makes communication between apps a lot easier to handle. Let’s modify our manifest: When the user navigates through Deezer in the Xbox Guide, the EBM feature enumerates apps that are registered to support it (the ms-xbox-backgroundmusicplaylists extension in our case). Once a compatible app is found, it will look for the associated service and then start communicating through it, allowing our app to know that actions have been made by a user. Here is the list of actions to be performed: The user wants to access the list of media contents: our service will fetch the user’s data The user selects a playlist: our service will display the tracklist in a second panel The user wants to play a content: our service will start the stream When the service is spun up from the Xbox Guide, it triggers a BackgroundActivated event that we can catch into our App class: This is our entry point to communicate with our App Service. At this point, we have all the information in the BackgroundActivatedEventArgs : We have the TriggerDetails: We have the deferral (that we saved to keep the communication opened): We also have access to events to manage the state of our service: When our app receives a request, we get a ValueSet of data that we can parse to get the wanted information. Here, we look for a ServiceMethod parameter key. After we pull the value out of the ValueSet (within a try/finally block), we can add a switch statement to know what action the user has performed, and what data we need to send back to them. In order to show how the app service should respond to communicate with the Xbox Guide, we will use the GetPlaylists case. First, we build the content structure we wish to display to the user. There should be 2 different sections as of today: Today, we made for you section (Flow + Favorite Tracks playlist) Recently Played section (Albums and Playlists) We send back a ValueSet that includes a Result value containing a string of the items in the following JSON format: The first three parameters are for display purposes. Id will just be stored to be used later to tell the service what id was invoked. ShowDetailsOnInvoke is optional, and if provided and true, it will open the next page and ask for playlist track information. We use the following lines to send the result back: Regarding the GetPlaylistTracks case, when a playlist is marked with ShowDetailsOnInvoke as true, the tracklist is opened in a second panel. Two more key parameters are associated in the incoming ValueSet. PlaylistId is the clicked playlist whose tracklist we are querying. Similar to the GetPlaylists case, we will return a ValueSet containing a Result string with the following JSON format: As for the InvokePlaylist case, we need to start playing content. We get another parameter key, PlaylistId , and its value will be the string id provided by GetPlaylists (in case ShowDetailsOnInvoke equals false e.g Flow). If we are in a tracklist details page, there will be a third parameter key, TrackId , which is the selected track in the playlist. We can then complete the call with an empty ValueSet. There is a whitelist of app-ids that the EBM feature checks for a service to be able to communicate (mostly music streaming apps that are available in the Microsoft Store). Some feature limitations also exist: a maximum of 99 songs per playlist can be displayed in the second panel and up to 20 different contents are available. The user state can change while using the Deezer app (log in, log out, profile switch, etc.) and we wanted to avoid updating our app service with new information each time it does. Moreover, as the app service uses deferrals, updating the service could cause the suspension of the app in background on multiple occasions. So, to avoid those troubles, we made the decision to create an “in-process” app service that allows a direct line of communication. However, you can definitely create out-of-process background tasks instead if your application architecture allows it. To do so, you would need to add an entry point in your manifest, and implement the IBackgroundTask interface in your app service project. And I guess there is just one thing left to say: happy gaming and listening! www.deezerjobs.com Stories of engineering, data, product and design teams… 47 Deezer Music Xbox One Csharp 47 claps 47 Written by Stories of engineering, data, product and design teams building the future of music streaming Written by Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-05-29"},
{"website": "Deezer", "title": "detecting explicit content in songs", "author": ["Manuel Moussallam"], "link": "https://deezer.io/detecting-explicit-content-in-songs-274967de7fd1", "abstract": "Previous stories About Jobs We’ve done some research about how to automatically detect explicit content in songs using only the music itself, and no additional metadata. Since it’s a sensitive and subjective issue, we did not want to use a black-box model, but rather build a modular system whose decisions can be traced back to some keywords being detected in the song. Our system gives promising results but we do NOT consider it fit for tagging songs as explicit in a fully automated manner. This work was jointly conducted with Telecom Paris and the full paper has been accepted for publication at the ICASSP 2020 conference . When it comes to figuring out what explicit lyrics are, there is no general consensus. It’s obviously a cultural issue, with lots of considerations about the intended audience and the listening context. As is the case with movies, the primary objective of tagging a piece as “explicit” is to provide guidance to determine how suitable it is for an intended audience. This is often referred to as “parental advisory” because the audience in mind are mostly kids. If you’re interested, there have been scientific studies regarding the impact of explicit content on children, but that’s not what our research is about. There’s a pretty good Wikipedia article about the creation of the parental advisory label for music. Despite its loose definition, it is generally admitted that strong language (curse words and sexual terms), depictions of violence and discriminatory discourse fall under the scope of what’s not suitable for children to hear in a song and should, therefore, be marked as explicit content. Of course, this definition is open to various interpretations. As of today, only humans make decisions on whether a song should be tagged as explicit or not. The person in charge of this is typically someone who works at a music label and follows internal guidelines set forth by the company. When songs are delivered to streaming services like ours, they are sometimes accompanied by the “explicit” tag, and sometimes not. When no tag is provided, it can mean that the song is suitable for all audiences, but it can also mean that no decision was made on the label’s side regarding its explicitness . There is a substantially large part of our catalog that falls under this category. Before asking ourselves whether we could build a system that could do this automatically, by analyzing the music, we studied the problem from a human point of view. Having to decide which track should be tagged as explicit and which shouldn’t is a complex task, it requires a high-level understanding of cultural expectations and involves a lot of subjectivity. Let’s take aside the above considerations and now just assume that there exists a deterministic method to decide whether a song should be tagged as explicit or not. The purpose of our research is to try to discover this method. The problem can be framed as a binary classification one: Given a song X, can we come up with a function f that will output either 1 if the song is explicit or 0 if it isn’t? For instance, if X is “All Eyez On Me” by 2Pac, we expect f(X)=1 while if X is this song , then f(X)=0. Like most classification tasks nowadays, it may be a good idea to use supervised machine learning (ML) techniques. If you were asked to label songs as explicit or not, how would you proceed? A natural answer is to look at the words that are pronounced by the singer(s); if “explicit words” appear in the lyrics then the explicit label should be applied. That’s a simple answer, but arguably, that’s the best one, based on recent research that compared it to more complex machine learning approaches. Now, what if you don’t have access to the text of the lyrics? That’s actually pretty common when dealing with millions of songs. If you are a machine learning aficionado, you might say: let’s build a big annotated dataset and train a supervised classifier to do it from audio. This approach is be called end-to-end or sometimes black-box ML. One family of such models are deep neural networks. They have been used with great success on classification tasks of images, sentences, videos and of course, music. However, a common fallback is their lack of interpretability . Indeed, once the system is trained (e.g. by making it learn from an annotated set of examples), it’s not easy to explain how it makes its decisions on new samples that have not seen during training. While we can evaluate the performance of the model on the set of known examples, one cannot easily associate its output to tangible elements of the input, such as musical features, or sung lyrics. We did try nonetheless this black-box ML approach, but we also wanted to try out another system more reliable, whose decisions could be easily justified. Such system is usually called explainable ML . Fortunately, on our team, we have a PhD student, Andrea Vaglio (jointly supervised by Telecom Paris — Image Data and Signal Department ) who is working on extracting lyrical information from music. One way to do explicit content detection is by first getting a transcript of sung lyrics, and then just using the presence or absence of words from an “explicit language” set to decide whether to label the song as explicit or not. Getting an exact transcription from a singing voice mixed with music is a very challenging task. It’s already much easier if you can extract only the vocal part of the song. At Deezer, we developed a tool called Spleeter . It is freely available for everyone to use and does quite a good job at extracting vocals from songs. Still, detecting uttered keywords from singing voices, even when isolated, is a complex problem, and much of our contribution here is to propose a system to do just that. It is called a Keyword Spotting System and is the main contribution of this work. Once you have the probability of presence for all words in your “explicit” dictionary it becomes easy to make a decision. We use a simple, binary classifier for that, whose outputs can straightforwardly be linked to the “explicit words” presence probabilities. One of the usual pitfalls of doing machine learning is when your model doesn’t learn what you think it does, but only adapts to a bias in your sample data. In music analysis, this phenomenon is called a “horse” . In the case of explicit content detection, the most important bias to consider is on the music genres. It won’t surprise anyone that many rap songs contain explicit lyrics. More than country songs, for instance. Yet, there are non-explicit rap songs and there are explicit country songs. If you’re not careful when designing your experiment, you may end up with a system that instead of detecting explicit lyrics, will detect rap songs, just because they have, on average, a higher probability of containing explicit lyrics. To avoid that, we need to balance the data to remove this music genre bias. In practice, you need to train on as many explicit rap songs as non-explicit ones, and the same for all music genres. In the paper, we compare our modular approach with a black-box one, and with an oracle system that knows the lyrics and detects the keywords directly on the text (think of it as the upper bound of what we can achieve, if our keyword spotting system was perfect). You will find all the details on the experimental setup in the paper but here’s the takeaway findings: Although it’s not as good as the oracle one, our approach produces quite promising results. Cherry on the cake, it beats the black-box model, which is always a nice result to have and goes against the commonly found belief that there is a trade-off to make between accuracy and explainability of an ML system. As a side note, we found that actually none of the systems considered reached levels of accuracy comparable to human ones. Even the oracle system only has an F1-score of 73%. This is an important fact to consider, especially when taking into account the sensitivity of the task. At this point, it’s not realistic to fully automate a decision process for explicit content labeling. We investigated a first ever (to the best of our knowledge) approach to build an explicit content detector purely based on audio. Despite reaching some encouraging results, we’d like to emphasize that this task is not satisfactorily solved by machines as of today. At Deezer, we give our partners, music providers and labels the possibility to mark the song they deliver to us as being explicit. Then users have the possibility to filter content based on this label. In this process, we just implement a user defined rule, based on the metadata we get. We do not intend to intervene in this process or to make decisions in place of either partners or our customers. One could nonetheless use our work to build a system to assist humans in their labeling tasks. With our approach, we can not only detect the presence of explicit keywords but also know where they occur in the song. We could, therefore, highlight some parts of the audio to an annotator to facilitate his task. In a broader perspective, our goal is to gain knowledge on millions of songs and leverage that to improve the Deezer product. This research is another stone in this building. www.deezerjobs.com Stories of engineering, data, product and design teams… 260 1 Machine Learning Spleeter Explicit Deezer Music 260 claps 260 1 Written by Deezer Research. Audio Signal Processing and Machine learning for Music. Stories of engineering, data, product and design teams building the future of music streaming Written by Deezer Research. Audio Signal Processing and Machine learning for Music. Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-04-30"},
{"website": "Deezer", "title": "a better way to search for music through query suggestion", "author": ["Aurélien Saint Requier"], "link": "https://deezer.io/a-better-way-to-search-for-music-through-query-suggestion-4bca10918c27", "abstract": "Previous stories About Jobs At Deezer, we are fully aware that the number one reason why users subscribe to our service is to access the depth of our catalogue of 50 million tracks. And for that the Search is key. To provide a best in class user experience, the Search team at Deezer had been working on fine-tuning the ranking algorithm, displaying top results, integrating new types of content (users, podcasts, live-streams, etc.) and optimizing our spell-checking system. However, we soon realized that it was actually our instant Search UX (i.e. displaying of results as a user types) that was hindering our optimization efforts in two ways: Having to deal with prefix search Having no opportunity to help the user formulate their need except handling errors This resulted in an overall affected user experience. We started investigating whether users completed their queries before clicking on a search result. As an example, we wanted to know: when a user searched for Eminem, did they type the whole “eminem” query or did they stop at the character that triggered the retrieval of the right artist in the search result (the first character in this case)? The results, especially for one word queries (like “eminem”, “u2”, “rihanna”, etc.), were that a user tends to type the whole query. This was also confirmed in a Google study showing that instant search UX for mobile is not relevant. We launched Google Instant back in 2010 with the goal to provide users with the information they need as quickly as possible, even as they typed their searches on desktop devices. Since then, many more of our searches happen on mobile, with very different input and interaction and screen constraints. With this in mind, we have decided to remove Google Instant, so we can focus on ways to make Search even faster and more fluid on all devices. Based on these facts and learning from all the main actors who faced the same problem, we decided to provide a query suggestion feature. To create this new search experience, we began by developing a query suggestion service with the following elements: Suggest query based on real searches & generated from music metadata (song, album, playlist and podcast titles, artist names, etc.) Localise suggestions according to the user country Suggest spelling corrections Personalise query suggestions according to the user’s musical tastes Show previously searched suggestions (search history) Suggest related queries Again, learning from the giants, to improve the UX we followed these design patterns: Avoid Scrollbars & Keep the List Manageable: no scrollbars and max 10 items. Highlight the Differences: it makes more sense to highlight the additions Support Keyboard Navigation: having the suggestion copied to the search field (only for Web UX) Match User’s Hover Expectations (only for Web UX): it’s important that the hovered autocomplete suggestion is highlighted and invokes the “hand” cursor Show Search History: a visited state adds an entire dimension to your site search history. Reduce Visual Noise: it’s important not to go overboard with the visual separators. This new query suggestion UX allowed us to better understand what the user is actually searching for through their own query or a query that our system suggested to them. This means that if a user typed “Jim” and searched for it, we know there is a higher probability that they searched for the keyword “jim” rather than for keywords “jimi” or “jimmy”. In order to add more precision, we developed a new search ranking algorithm giving more importance to an exact match than a prefix match. We added a boost to non prefix clauses and combined the BM25 relevance score with the item’s popularity calculated from the number of listenings. At this point, we conducted usability tests with paying Deezer subscribers, asking them to complete a simulated work task : “A friend of yours convinced you to check out Julien Doré. You’ve never heard of this artist but would like to listen to some of his tracks” . Results: not one user commented on the fact that the instant search results were replaced by the query auto-completion feature . This was a good indication that no one would mind if instant search were to be replaced. Getting the green light from our users, we opened the new UX and the new ranking algorithm to 50% of our Android users. We followed three metrics to evaluate the performance of the A/B test: · Ranked Half-Life (RHL) indicator denotes the degree to which relevant items are located on the top of a ranked retrieval result. · Success indicator describes the search session (i.e. a group of queries corresponding to the same need) where the user clicks on the last query of the group. · Effort indicator includes the typing time, the considering time (i.e. the time for a user to click on an item after a query), the number of reformulations and the rank of the clicked items in a search session. For the Success and RHL indicators, we saw that the new search UX and the original were essentially the same. However, there was a significant difference regarding the Effort indicator to the benefit of the new UX. All in all, the new search UX did not have a negative impact on search satisfaction. On the contrary, users need to provide less effort to find the music they want to listen to! Our team is glad to introduce this new search experience into the Deezer product. The next steps consist in improving the query suggestion algorithm and tune the new search ranking algorithm, but also in continuing to improve the Search UX. We are currently working on a new design and user experience of the search results. Stay tuned! www.deezerjobs.com Stories of engineering, data, product and design teams… 48 Deezer Search Search Ux 48 claps 48 Written by Stories of engineering, data, product and design teams building the future of music streaming Written by Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-03-11"},
{"website": "Deezer", "title": "creating cron job vitals with prometheus pushgateway", "author": ["Laura Roudge"], "link": "https://deezer.io/creating-cron-job-vitals-with-prometheus-pushgateway-7111d0c4658e", "abstract": "Previous stories About Jobs Software Engineering is often all about automating what can be automated. That’s why we build cron jobs . cron is a utility that can be found in any Unix-like system, used to schedule tasks and commands that need to be executed repeatedly (i.e. every hour or every day). For example, we often need to make regular calls to an API to update a database with new info, and we don’t really want to have to think about doing it manually every day. A cron job is the perfect format for doing so. However, monitoring the health of a cron job can be complicated and it’s really easy to just forget we have cron jobs running (or trying to) continuously. In order to improve performances and make sure our programs are running smoothly, we need monitoring one way or another. From network, to execution time, to the number of requests, there are a billion vitals we can and want to monitor. A lot of software offers simple ways to do so, like Datadog, Prometheus or Grafana. Often, monitoring works by scraping vitals from servers or containers, and making requests to them. But cron jobs are programs that are supposed to run once for a short time, and monitoring can’t guess exactly when the cron runs in order to request metrics from it. In that case, how can we make sure that our cron ran correctly? Well, if we use Kubernetes, we can always go to the dashboard and search for that cron and look at the logs. But there is no way to make sure that someone will think about doing that, because humans tend to be forgetful and software engineers simply have too many things to think about already! Bottom line, if it can be automated, it should be. Prometheus has a service that allows to push metrics from jobs that can’t be scraped: the Pushgateway . The way it works is pretty simple: it serves as a middleman between Prometheus and the cron job. Instead of Prometheus making requests to the job to get metrics, the job can push metrics to the Pushgateway when it’s executing, and then Prometheus will make calls to the Pushgateway to get those metrics. Then, we can visualize those metrics in a Pushgateway board, and create graphs and alerts from them with another service like Grafana, for example. The Tooling team at Deezer has created Software Development Kits (SDK) that allow other teams to interact with Prometheus and the Pushgateway easily. Of course this could be done separately by each team, but the need for cron monitoring is wide enough to create a dedicated and shared SDK, and it’s much cleaner to keep that logic out of each application and centralize it. There is one SDK per language (Node.js, Python and PHP), and they all have the same functionalities. Each team can then use the one they need as a package. We recently updated the Python SDK to have those functionalities, so we’ll be talking about our process. In order to push metrics to Pushgateway, we need to register those metrics, and then dispatch them through a connection . In our SDK, we created four main classes: Client , Dispatcher , Renderer and Metric . We actually have one class per type of metric, even though they’re all very similar. We’re going to focus here on the JobStepTimestamp , that will give us the timestamp in milliseconds of a step of the cron (i.e. start or end). Here is how we could implement it in Python: In this simplified version of the JobStepTimestamp metric, we’re creating an instance of a Gauge which is a numerical value that can increase or decrease, which is perfect for a timestamp. This gauge will become an attribute of our JobStepTimestamp instances. Then, let’s create a Client class. This class will allow us to create a client that will post our metrics to the Pushgateway. Again, this is a simplified version for basic usage. In reality, we might want to add default options and merge those options with the provided options on instantiation. Then, we should create a Renderer, that will hold a registry where our metric values will live. Finally, let’s create our Dispatcher. When the dispatch() method will be called, each metric it contains will be transferred to the registry of the Renderer, that will generate Prometheus output data from it. This output data, containing the name of the job and the metric with its values, will then be sent to the Pushgateway through the Client post() method. The hardest part is done, so let’s use our previous work in an exemple cron from an example app. We’ll skip the load-balancer configuration and the docker-compose steps, but they’ll be needed in case of a containerized application. We’ll also skip the cron scheduling part, that could be done in a Helm for example, and the deployment part. Here, the cron job itself is really uncomplicated, and consists of printing a sentence to the standard output. It only took a few steps to wrap our vitals around it: We create an instance of the Dispatcher, that uses an instance of the Client, that will connect to http://localhost:9091 ( for the purpose of this article, we’ll run the app and the cron locally, but usually we would put an environment variable that will hold the “real” pushgateway URL and path linked to the app). We create an instance of the JobStepTimestamp metric, that we add to our dispatcher’s registry through the registerMetric() method. Right before the start of the cron, we set the value of the “start” step to the current time in milliseconds. The cron runs. After it has run, we set the value of the “end” step to the current time in milliseconds. Finally, we dispatch those values to the Pushgateway through the dispatcher, that calls the post() method of the Client, like we saw previously. Note that our cron is not asynchronous here, but we can produce all those steps in an asynchronous operation, just by waiting for the cron to be done before setting the end value. This can be done in Javascript by setting the value in the .then() method of the cron itself, for example. And voilà! We can now run our cron and have access to those metrics on our local Prometheus-Pushgateway. Let’s navigate to localhost:9091 . Here we can see our two steps: start and end, with their respective values. Now that our app is functional and our cron pushed metrics to Prometheus-Pushgateway, there are plenty of ways we can monitor them. A first step could be linking a Grafana board with the production image of Prometheus-Pushgateway that we send our metrics to. Here is a non-exhaustive list of ways to monitor them: Make a graph that calculates the difference between the end and the start steps to show how much time the cron takes to execute. Create an alert if the cron takes too long to execute. Make a graph that shows when the last execution of the cron job was. Knowing the cron schedule, we can create an alert if the last execution was not on time. Create an alert if the cron job starts but never properly ends (it probably crashed). Create an alert if the cron job never starts. Register more step timestamps for debugging purposes. Register other types of metrics. ( see Pushgateway documentation ) Monitoring is essential to every developer, and keeping track of commands and programs that are running in the background on our servers can be tricky. Thanks to the Pushgateway, we can control what vitals we send and when. Knowing that we have access to the health of our cron jobs, we can free ourselves from always looking at logs, and save precious time. About Laura (Roudge) de Rohan Last November, I joined the Tooling team at Deezer for an internship in Paris. Prior to this, I studied Software Engineering at Holberton School in San Francisco, after deciding to move away from my career as a dancer. Over the course of the last year I developed a passion for coding, both on back-end and front-end stacks, and I love sharing what I learn! Follow me on Twitter or connect with me on LinkedIn for more technical blog posts. www.deezerjobs.com www.ostechnix.com prometheus.io github.com github.com prometheus.io Stories of engineering, data, product and design teams… 221 Thanks to Pauline Munier . Prometheus Python Monitoring Software Development Deezer 221 claps 221 Written by Software Engineer at Deezer in Paris, former student at Holberton School in San Francisco, always striving to build a better world. Stories of engineering, data, product and design teams building the future of music streaming Written by Software Engineer at Deezer in Paris, former student at Holberton School in San Francisco, always striving to build a better world. Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-02-12"},
{"website": "Deezer", "title": "1001 ways to code your projects 1 way to meet your needs", "author": ["Angéline Perdereau"], "link": "https://deezer.io/1001-ways-to-code-your-projects-1-way-to-meet-your-needs-f0e7b7f1a1ad", "abstract": "Previous stories About Jobs Have you ever noticed that there are about 1001 ways to do something, especially in software development? As I tell every developer who joins my team, especially interns or junior devs, no one knows everything and every solution should be considered. I was recently confronted with 1001 ways of doing a task, so I thought it would be a good opportunity for me to explain the different steps I went through to write a little algorithm. The example is quite simple; you might know that in the music industry, new releases are released on Fridays. My task was to retrieve date ranges of releases weeks, therefore from Friday to Thursday — and it could start from any date. When you’re working on an algorithm, there are two possibilities: you already have the best solution because you’ve worked on something similar before, or you start off with a draft version that you will improve step by step. In this example, I went step by step to find the best way to work on it. “ The best way ” . What is ‘the best way’? What does that mean? How can you be confident that your solution will be the best? The best way means the solution that best matches your own acceptance criteria. After all, there are 1001 ways to do anything. This means that the best solution for you might not be the best one for everyone else. So it is important to define your own criteria. Your company might have their own criteria, and you may have had different criteria before. More generally, I think that everything that could impact the developer experience may be considered. You might want to focus on the number of lines, semantic, maintainability, performance or some other criterion. Choose the ones that are most relevant to you. As for me, when I need to develop something, my criteria are almost always the same: scalability, readability and efficiency. Now the criteria are well defined, let’s try to provide a first version of the algorithm. We need to retrieve an array containing all the date ranges (Fridays to Thursdays) from a start date. Even though the deadlines are not so tight, you may need to provide a first draft of the algorithm that works and meets our needs before improving it. This can help you keep a healthy balance between delivering and “pimping”. Now the job is done, we need to refer to the criteria previously defined (scalability, readability and efficiency) to check if the code above respect them. Evaluating code can be subjective, in particular with the readability criteria, this is why I suggest you to read multiple times your code before sharing it with other developers to get their feedback. The example above could be improved, here are the first issues noticed: Using a 1-day period loop whereas Fridays come every 7 days (efficiency) Too long lines (readability) Complexity using int values for both Thursdays and Fridays (readability) Complexity using if/else condition within a foreach loop (readability, scalability) … You could find many more things to improve in the code above, don’t forget to refer to your criteria. Before starting to “pimp” the code above, it would be interesting to write down unit tests. They will ensure that all the updates you make don’t break the algorithm. Some of you may be familiar with the Test-Driven Development (TDD) process, which consists in writing unit tests before starting developments. In any case, this is the right time to cover your code with unit tests. How did I improve the algorithm? What helped me? What are the steps I went through to get this solution? I first focused on the efficiency issue so I turned the 1-day loop into a 7-day one. Once done, I handled the nested if/else condition complexity as I had no idea how to fix the too long lines issue. Of course, instead of setting the $startDate in one line, I could have split it into several ones in order to reduce line length. Readability would have been improved. But there must be another way to do so. After talking about the last issues I had with other developers, they helped me find out the best solution. Exposing them the need and all the solutions I thought about made us more efficient in solving the issue. It is easier to review suggested solutions than to create a new one. Finally, digging deeper into the php doc on date helped us. It is important to be surrounded by other developers, whether they are more experienced or not, as we all have different experiences. The final solution passes the unit test and matches my criteria. The developer experience has been improved and the loop is clear and more efficient. The algorithm is now matching my criteria. As said above, my criteria are “almost always the same”. Why not always ? Web development is a sector that changes so quickly that new methods, new objects, new frameworks and even new principles come up very often. Someday, you will meet someone or read an article describing different criteria from yours and they will pique your interest. But do not forget that criteria are something subjective so don’t lose sight of what matters to you and be prepared to defend your point of view while keeping an open mind. Trends change, and one day a method used in the algorithm above will be replaced by something more efficient. Then it will be no problem to update it because maintainability was part of my criteria. The thing to keep in mind is that the solution you choose at the moment you choose is the right one because it was the one that matched the criteria you had at the time and you should be proud of it. As you can see with the issue I had, every step I went through has always returned the same result (ensuring that you are still able to deliver on time). There might be 1001 ways to do something but in the end you have to choose one. Defining your own acceptance criteria and exposing your solution to other developers are the keys to finding a suitable solution. www.deezerjobs.com Stories of engineering, data, product and design teams… 427 Thanks to Pauline Munier and Christian Jennewein . Programming Coding Software Development Web Development Deezer 427 claps 427 Written by Web Engineer @Deezer Stories of engineering, data, product and design teams building the future of music streaming Written by Web Engineer @Deezer Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-01-15"},
{"website": "Deezer", "title": "i went to a conference on tech leadership and you should too", "author": ["Zouhair Mahieddine"], "link": "https://deezer.io/i-went-to-a-conference-on-tech-leadership-and-you-should-too-e7c1866cbdef", "abstract": "Previous stories About Jobs A conference on engineering management and leadership? Sounds useless… In the past few weeks, I got a lot of those. For some reason in our community (or at least, in the subset of it I am currently involved with), a tech conference should be about learning actual technical skills (this new language, that new tool or that totally new way of doing things enabled by your favorite language’s totally unreadable new syntax) and can’t really be just about how to be a better tech leader or engineering manager. Fortunately, my company, Deezer, didn’t share this feeling and a few days ago, I had the chance to attend the Lead Dev Berlin 2019 conference to learn about and reflect on what it means to be a tech lead today. So, first of all, let me thank @DeezerDevs and our Tech Community Ambassador Guillaume Grillat for supporting this. You rock 🙇‍♂️. Curious to know how it went? Read on and I’ll try to explain why you should grab a ticket for the next edition, or any of the other Lead Dev conferences they organise around the globe. You can find the list here . The goal of the Lead Dev conferences is pretty straight-forward, and it’s the first thing you read on their website: by sharing “inspiring stories and practical advice on leading and managing technical teams based on the themes of teams, tech and tools”, they aim to help you become a better technical leader. This year in Berlin, we learned about the many challenges paving the way of the Lead Developer in a great opening talk by Pat Kua (Chief Scientist at N26 ), where each of the potential pitfalls were monsters you need to overcome in your quest to becoming a better tech lead. We also reflected on how, when moving from Individual Contributor to Manager, all your software engineering practices were not lost but could become strengths to leverage in your new role. nicky thompson ’s (Technical Manager, Future Learn) talk was full of insights and practical advice, mapping practices like DRY or KISS to engineering management. Many other topics were covered, which you’ll be able to discover in the next few days as slides and videos from the conference will be posted on https://berlin2019.theleaddeveloper.com/ . But my favorite intervention is probably the one by Kevin Goldsmith (CTO, Onfido) who dropped — to paraphrase him — “decades of knowledge on how salary works, acquired by making mistakes” on us: from his crystal clear recap of how hiring slots and salary reviews all start with putting together a company budget, to how that process works, to a very actionable list of dos and don’ts when making an offer or navigating a salary review with your team members. I could go on and on about how inspiring the speakers and their talks were, but I have to tell you a bit about the conference’s organisation. White October Events did an amazing job putting Lead Dev Berlin 2019 together. From the spacious and cosy venue to the large stage where you could always see the speakers and the big screens with their slides, to the excellent food and always available beverages, everything was aimed at making us comfortable and ready to learn, reflect and share. But what impressed me the most were the efforts in making the conference as inclusive and accessible as possible. Here is a non-exhaustive list of initiatives they put together: Live captioning powered by White Coat Captioning ( Norma Miller ) A dedicated room for parents and one for prayer An enforced Code of Conduct and incident response team Communication preference (“Say hello” or “Do not disturb”) and pronoun stickers No-photo lanyards Accessible venue and bathroom And I am probably forgetting some of them… And along with all this, a big shout out to Meri Williams (CTO, Monzo) for MC’ing the event, introducing each speaker and dropping nuggets of knowledge and funny anecdotes while doing so. Just grab a ticket for one of the future editions, if you consider it important to reflect about your craft and you want to learn more from others’ experiences, you won’t regret it. You’ll also get to meet peers at different levels of seniority (from aspiring lead to CTO) with whom to discuss practises, challenges, etc. I hope I helped convince you of how useful and fun these sorts of conferences can be, and hope to see you there next year (if I am lucky enough to grab a ticket again!). P.S.: attended or heard of other tech conferences focused on leadership and management practises? Please leave a comment and let me know what you thought about those. www.deezerjobs.com Stories of engineering, data, product and design teams… 26 Thanks to Arthur De Kimpe and Pauline Munier . Conference Leadership Lead Dev Berlin Engineering Management 26 claps 26 Written by Lead iOS Engineer @Deezer Stories of engineering, data, product and design teams building the future of music streaming Written by Lead iOS Engineer @Deezer Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-12-12"},
{"website": "Deezer", "title": "releasing spleeter deezer r d source separation engine", "author": ["Manuel Moussallam"], "link": "https://deezer.io/releasing-spleeter-deezer-r-d-source-separation-engine-2b88985e797e", "abstract": "Previous stories About Jobs We are releasing Spleeter to help the research community in Music Information Retrieval (MIR) leverage the power of a state-of-the-art source separation algorithm. It comes in the form of a Python Library based on Tensorflow , with pretrained models for 2, 4 and 5 stems separation. Spleeter will be presented and live-demoed at the 2019 ISMIR conference in Delft. While not a broadly known topic, the problem of source separation has interested a large community of music signal researchers for a couple of decades now. It starts from a simple observation: music recordings are usually a mix of several individual instrument tracks (lead vocal, drums, bass, piano etc..). The task of music source separation is: given a mix can we recover these separate tracks (sometimes called stems )? This has many potential applications: think remixes, upmixing, active listening, educational purposes, but also pre-processing for other tasks such as transcription. Interestingly, our brain is very good at isolating instruments. Just focus on one of the instrument of this track (say the lead vocal for instance) and you will be able to hear it quite distinctively from the others. Yet that’s not really separation, you still hear all the other parts. In many cases, it may not be possible to exactly recover the individual tracks that have been mixed together. The challenge is thus to approximate them the best we can, that is to say as close as possible to the originals without creating too much distortions. For years, a lot of strategies have been explored, by dozens of brilliant research teams from all over the world. If you’re interested in this fascinating journey you should go read this literature overview, or this one . The pace of progress has recently made some giant leaps, mainly due to advances in machine learning methods. To keep track, people have been comparing their algorithm in international evaluation campaigns . That’s how we know that Spleeter performances match those of the best proposed algorithms. Additionally, Spleeter is very fast. If you are running the GPU version you can expect separating 100x faster than real-time which makes it a good option to process large datasets. Quite a lot I’d say. If you’re a researcher working on Music Information Retrieval and have always considered that source separation artifacts made it unsuitable as a pre-processing step in your pipeline... Well, you should probably reconsider and try Spleeter . If you are a music hacker and want to build something awesome using Spleeter , then go ahead. Actually Spleeter is MIT-Licensed so you are really free to use it in any way you want. It goes without saying that if you plan to use Spleeter on copyrighted songs, make sure you get proper authorization from right owners beforehand. Under the hood, Spleeter is a fairly complex and crafted engine but we’ve worked hard to make it really easy to use. The actual separation can be achieved with a single command line , and it should work on your laptop regardless of your Operating System. For more advanced users, there is a python API class called Separator that you can manipulate directly into your usual pipeline. We’ve tried hard to come up with a thorough documentation . Don’t hesitate to give us feedback, point out issues or suggest improvement through the traditional github tools! Short answer: we use it for our research and think other might want too. We’ve been working on source separation for a long time (and we already had a publication at ICASSP 2019 ). We have benchmarked Spleeter against Open-Unmix -another open-source model recently released by a research team from Inria- and reported slightly better performances with increased speed (note that the training dataset is not the same). One of the hard limitations faced by MIR researchers is the lack of publicly available datasets due to copyright issues. Here at Deezer , we have access to a fairly large catalog that we’ve been leveraging to build Spleeter . Since we can not share this data, turning it into an accessible tool is a way for us to make our research reproducible by everyone. On a more ethical standpoint, we feel there should not be an unfair competition between researchers based on their access to copyrighted material or lack thereof. Last but not least, training this kind of models requires a lot of time and energy. By doing it once and sharing the result, we hope to save others some trouble and resources. Since we released Spleeter , we have received numerous feedback, most of them very positive and we’re thrilled to see all that attention given to our work. A few of these reactions may however be a little over-enthusiastic, so let’s just restate a few things. Spleeter is a neat tool, but in no way do we claim to have “ solved ” source separation. Hundreds of researchers and engineers working for decades have made the advances and built the tools on which Spleeter is based. It’s our contribution to a vivid, ever-growing and open ecosystem and hopefully something others will build upon too. Finally, it’s worth pointing out that music mixing is a fine art and that mastering sound engineers are artists in their own rights. Obviously we do not intend to harm their work in any manner or affect anyone’s credit. When you use Spleeter , please do so responsibly. That being said, happy hacking everyone! www.deezerjobs.com Stories of engineering, data, product and design teams… 2.6K 13 Machine Learning Open Source Deezer Music Source Separation 2.6K claps 2.6K 13 Written by Deezer Research. Audio Signal Processing and Machine learning for Music. Stories of engineering, data, product and design teams building the future of music streaming Written by Deezer Research. Audio Signal Processing and Machine learning for Music. Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-02-03"},
{"website": "Deezer", "title": "my first job as a web engineer from doubt to fulfillment", "author": ["Pierre-Olivier Dézard"], "link": "https://deezer.io/my-first-job-as-a-web-engineer-from-doubt-to-fulfillment-6513b7a15429", "abstract": "Previous stories About Jobs This is what you’ll typically find in the ‘requirements’ section of a tech job vacancy: “Has strong knowledge in several languages, is “full-stack”, is able to create strong architecture, scalable and flexible, has experience in understanding large and complex code bases, must be proactive and organized, has a great team spirit …” It scared me too at first. That’s why I thought it could be interesting to share my experience with you in order to demystify tech jobs for juniors. At the end of engineering school, I had to find an internship that would possibly become my future job. I was not confident in my skills, I felt as if I couldn’t do anything. The only experience I had was from the projects I did at school. How could I compare that to coding for a real app used by millions of people? From a general engineering school, not everyone was really into code. Some went on to project management jobs or to be junior managers for tech teams. I didn’t feel I was ready for that kind of job at 23. But even if I didn’t feel like a big shot in code, I liked the creative side of it. To find a job that could really motivate me, I tried to focus on things that really mattered to me in life and my big passion is music. I think this is the big thing when you are about to start your career. You will spend most of your time at work, what will all those hours count for at the end of the day? Once you work this out, job-hunting will be much easier. You will have the opportunity to be a part of a sector that you are passionate about and that means something to you. It was a friend from school who made me take the leap to send my resumé. We used to play music together and we both ran the school music club. He was older than me and when he finished school he went to Deezer. He told me it was really cool but that I needed strong technical skills. When I had to look for my final internship, an announcement on deezerjobs.com came up that matched my skills perfectly. I contacted my friend who told me that he worked with the team that were hiring. So I decided to apply. But how can you make a difference in a room full of people who are more experienced? Of course you have to know the basics but you will quickly realize that soft skills are as important as hard skills. This was actually the case for me. My manager was torn between me and another candidate who answered tech questions perfectly and was a very confident coder but who was not really into the music world and seemed less willing to learn. My honesty during the interview, in particular when it comes to admitting I don’t know something, and my genuine interest in the product convinced my manager that I would be the right fit for the job in the long run. So she chose me. A developer is vital to any team that wishes to carry out a project successfully. It’s a creative job that gives you the opportunity to design project features and transform ideas into realities. How cool is that? Ok, so you found a project you want to contribute to, you are convinced that being a developer is awesome, but how do you get a company to hire you? As I said before you will have to know the basics of the languages you will work on and some common concepts in web engineering (such as design patterns, Object-oriented programming and Inheritance, etc). But they will mostly expect you to be curious and motivated to learn. You haven’t picked up any outdated habits, you are a fresh mind and your future manager will be able to teach you how to code as he does. Your manager will show you his way of working and how to navigate the workplace. He will also encourage you to take pride in your work. My manager came down to my level and treated me as her equal to encourage me to share, discuss and challenge her ideas. These discussions helped me get an idea of my manager’s guidelines. The more you integrate your manager’s guidelines, the more you will be able to challenge them. You will then be able to see improvements in the code base, you will be more efficient during code reviews and you will be able to apply your manager’s feedback to your code efficiently. Knowledge sharing is the key to building a consistent code base over the years, even if people come and go. You will not only learn from your team, you will bond with them too. I have a lot in common with my team, some of them are the same age as me and they make everyday at the office more enjoyable. We organise a lot of events like dinners at restaurants, barbecues, or team Hackathons; we had the last event at our Bordeaux office and we spent the weekend together! They are now really good friends of mine. Having a “junior” in a team is an advantage and your manager has as much to learn from you as you from him. Your manager will explain things to you that he has been doing for years, sometimes automatically. If you ask him questions, he will have to explain his process. In some cases, you might be able to point out how a process the team has used for years could be improved or updated. The team will discuss the point and it will be an opportunity for you to ask questions and discuss technical issues. Even if your question doesn’t lead to any changes, it’s always good to question every process you don’t understand. It lets the team know that the process is still up to date and helps you understand why things are done like this. Ultimately it’s a great way for you and your team to make progress! This sums up my experience at Deezer so far. I know it’s not universal and everyone’s career path is unique but the last factor that will help you feel comfortable in your team is the people. You can be in a company doing amazing things in the sector you love, but if you don’t get on well with your team it won’t work. It’s the same with your mentor. More generally, you need to choose a company whose values match yours so that your time and energy goes towards something you deem worthwhile. I’m glad to have found a team that fulfills all the criteria and makes this first job so enjoyable. www.deezerjobs.com Stories of engineering, data, product and design teams… 373 Programming Web Development First Job Career Advice Deezer 373 claps 373 Written by Web engineer @Deezer Stories of engineering, data, product and design teams building the future of music streaming Written by Web engineer @Deezer Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-08-26"},
{"website": "Deezer", "title": "throwback to deezer contest the first hackathon for product and qa", "author": ["Guillaume Grillat"], "link": "https://deezer.io/throwback-to-deezer-contest-the-first-hackathon-for-product-and-qa-dc685e9dcbd7", "abstract": "Previous stories About Jobs co-authored by Anna Louis & Baptiste Villain Deezer has a long tradition of hosting and organizing tech events such as internal or external hackathons, meetups , and more. Our motivation is to offer our people learning experiences by connecting them to external communities. Deezer ConTest was about encouraging our QA and Product communities to collaborate with three goals in mind: to be fun, inspiring, and useful for our mobile application development. It’s one thing to measure employee engagement; it’s another one to assess the impact of an event on the quality of our platforms. We knew beforehand that results would only be measurable in the mid-term but we wanted to give you our initial feedback on the journey. We had the idea for the project during our previous internal hackathon, but it was really set into motion in February 2019. It was the first time our Product and QA teams worked together on designing and planning an event, and I believe they did a great job co-organizing it. Basically their two common goals were: learn together and from each other, and enjoy the day! Our event took place in our Paris headquarters on a weekend…because one evening would not be enough time for people to feel comfortable and properly connect with other participants. The support of two other well-known communities, Product Stories and Agile Testing Paris , made it possible to get in touch with potential participants more easily. It was a great opportunity to gather our Product and QA teams, as well as host a “big family reunion” for external Product & QA people. We received 81 subscriptions beforehand, and invited 54 participants to attend. In the end, 38 people came to our office on the day of the event. This event showed us that we could learn a lot more with the support of our communities. The feedback from our Deezer ConTESTants was great and we couldn’t be happier to read some of their posts afterwards ( here , here and here ). The roundtable was practical, without waffle or prevarication, and the proximity with our 5 speakers was precious to our participants. Mentors discussed their own experiences and offered professional advice to our attendees, who in return had the opportunity to share their difficulties and ask questions. Baptiste , Release Manager at Deezer “We would like to thank every participant in the very first Deezer ConTEST! Thanks to your dedication, we were able to significantly improve our JIRA backlogs and ultimately make our customers’ experience better. Let’s start with a statistical overview of the current status of the JIRA tickets you raised… in the form of a tasty donut: You can see that only a very small portion of the 405 JIRAs were instantly rejected. On the bugs’ side, if we don’t take into account the issues that were already known to our teams (yes, our teams also work on bugs here!), a bit more than 60 reports have been added to our backlogs, and almost two-thirds have already been addressed. Some concrete examples of this? We fixed a lot of things on our search engine, when searching for an empty string for instance: it no longer sends users to a 404 page — a miracle! We also handle apostrophes better and certain numbers in artist names are also more easily recognized. Regarding feedback and suggestions, it’s not possible to sort them statistically but rest assured the most relevant proposals and ideas that emerged from the brainstorms are being used by our Product team. A lot of surprises are in the works and I suppose people focused on Freemium in particular will find many of their suggestions in the Deezer app in the upcoming months… We will get back in touch with you later, as most topics all of you identified have been handled and developed by our teams.” Anna , Product Owner at Deezer “There is no way to improve a product without knowing it inside out, that’s why I believe Quality Management and Product Management teams complement each other and should work hand in hand. Yet these two teams often work in a linear way: PMs imagine a solution and send it through the QA process once it’s ready. Most of the time, valuable QA knowledge and experience of the product aren’t applied as QA people are not involved in the conception of new features. It’s a shame, for QA analysts are often a gold mine of information when it comes to identifying all the product use-cases. With an event like Deezer ConTEST, we hoped to be able to go beyond the usual barriers and bring QA and Product together. To keep the participants focused during the day and quickly onboard them on one specific subject, we assigned each group one of the 4 available topics that we selected for investigation on the Deezer mobile applications (both Android and iOS): 1: the freemium experience 2: usage under bad network conditions 3: offline mode usage 4: the search experience Once all the teams tested and retested all our products, hunted for bugs and other types of inconsistencies in our user journeys, they took the time to prioritize issues and choose which major pain point should be tackled first. Finally, they imagined solutions to this problem and submitted their best proposal to our jury. After several rounds of iteration, all the solutions were presented in front of everyone in a short pitch. After assessing the relevance of the chosen pain point, the feasibility of the solution and the storytelling of the presentation, our amazing judging panel chose the best pitch by topic. Our jury composed of experts of Product Management and Quality Assurance Making the most of their knowledge of the product and using effective methodology, Quality Assurance Testers and Product Managers all worked together throughout the day to design the best experience for our apps. For example, one of the teams who were working on offline mode usage decided to focus on discovering new songs in airplane mode. They came up with the idea of sharing a discovery playlist with peers around you thanks to local bluetooth connection. Clever! In the end, Deezer ConTEST seemed to be a very enriching experience for the members of both communities, during which they became better aware of their mutual dependencies and of the importance of collaborating better in their day-to-day work. Mission accomplished!” You asked for more and we’ve heard you. We are planning to host Deezer ConTEST #2, probably with a few organizational changes. Rest assured that we will keep you updated when things come together, hopefully in the next few months. Until then, here is a video sum-up of this first Deezer ConTEST edition: Many thanks to all the partners and speakers who made this fun and inspiring day possible: Victoria Bocquet , Yann Person , Florian Zilliox , Geoffroy de la Rochebrochard, Clément Falchier , Jean-Pierre Lambert , and Thomas DIDIER . 🙏 to Pauline Munier for her continuous help on this project and Luis Carlos Garelli Boada for the pictures. www.deezerjobs.com Stories of engineering, data, product and design teams… 107 Hackathons Product Management QA Testathon Deezer 107 claps 107 Written by Tech Community Ambassador @leboncoinEng. My goal ? Helping my teams to flourish at work, by meeting exciting & innovative people & gaining new knowledge. Stories of engineering, data, product and design teams building the future of music streaming Written by Tech Community Ambassador @leboncoinEng. My goal ? Helping my teams to flourish at work, by meeting exciting & innovative people & gaining new knowledge. Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-09-25"},
{"website": "Deezer", "title": "matching albums through cover art fingerprinting", "author": ["Mickaël A"], "link": "https://deezer.io/matching-albums-through-cover-art-fingerprinting-bdca82cd17dc", "abstract": "Previous stories About Jobs Matching albums from one source to another is a recurring task at Deezer. Several reasons require platforms to do this such as partnerships, when two companies merge or compare their catalog and metadata enhancement. This also occurs when a platform refers to another source like Wikipedia or Musicbrainz for additional crowd sourced information. Unique identifiers like the UPC (Universal Product Code) is the best common ID to be found in catalogs and is still widely used. However this ID comes from a brick-and-mortar realm, affixed to the barcode usually scanned by a cashier, and is not consistently found with digital-only releases. As often is the case, the title of the album and artist name can be used to match albums from different sources, with known caveats like spelling discrepancies, multi-artist releases or homonym titles. The image comparison aims to enhance this metadata matching by leveraging image fingerprinting. The immediate advantage of album covers is their coherence from one source to another. When any cover is given, no matter what the quality there are often a lot of similarities and they generally stem from one official source. The worst material provided can include photographs or scans of CD/vinyl cover with possible truncations, empty spaces and color changes. Hopefully image fingerprints are robust enough to detect such alterations. A fingerprint of a given image (like a scan or photograph) can be considered close enough to the official counterpart by a robust image fingerprint technique. During the 2002 International Conference on Image Processing, H. Chi Wong, Marshall Bern and David Goldberg published a paper named An image signature for any kind of image . Developer ascribe implemented this paper in Python in a library called image-match , including an Elasticsearch and a Mongo backend to store and retrieve documents following Wong et al. algorithm. This library is particularly helpful and has been at the core of our experimentations. In the Wong et al. approach, a signature of the image is computed from a grid where each grid point is compared to its eight neighbors according to its average gray level: “The result of a comparison can be much darker , darker , same , lighter or much lighter , represented numerically as -2, -1, 0, 1 and 2.” Thus, an image is signed as an array of values (a vector ) from -2 to 2, such as [0, 0, -2, -1, 2, 2, 1, 0]. Comparing an image to another boils down to computing a distance between their two vectors. The result is a value between 0.0 and 1.0, where 0.0 means both vectors are exactly the same. This method is sufficient to compare two images, but when it comes to looking for image similarities in a big corpus, one needs another representation. The signature is sorted into a list of words (in fact integers) that can be “speedily indexed and matched”. The commented source code here is self-explanatory . Each image can then be stored and indexed by a new list of integers, such as [3141592, 6535897, 93238462, 643383, 27950288]. Two images sharing one or several words are likely to be similar. Their similarity is then computed by their signature’s distance. “This method solves the problem of nearest-neighbor search in high dimensions by finding exact matches in a number of lower-dimensional projections.” Implementing this technique to match external albums to our Deezer catalog implies two important steps: To fingerprint our entire cover catalog To determine the similarity threshold Fingerprinting the entire Deezer catalog took us around two days, parallelizing the task 6 times. The task consists in computing the signature and the words for each image, as well as storing them in the backend — Elasticsearch in this case. Some metadata is also added, like the album ID, the album title and the artist name, that will be useful for later development. The threshold in Wong et al. is normalized: “The parameters […] are chosen so that any vector v within normalized distance of .6 is sure to match on at least one word”. Given the high volume of covers — around 8 million — .6 seems too high a limit. To refine this threshold, an annotation campaign was held within our team (R&D). Using a threshold of .4, covers from Musicbrainz were matched to Deezer. A simple page allowed us to annotate if the matches were considered legitimate or not. From this campaign, the following data was collected: When analyzing the false positive and false negative, two specific problems were blindingly obvious. We named them: The collection issue The single issue In the collection issue, several covers are the same or similar enough to be considered as the same image . However, they do not represent the same album . Look at the following: Another effect of the collection issue is the presence of a small icon on the cover, like a “Parental Advisory” icon or a “Deluxe” icon. The original cover and its iconed counterpart can be seen as belonging to the same collection of releases. On the other hand, in the single issue , the cover is exactly the same but the music content is not, usually (but not only) a single release and its album counterpart: The collection and single issues give good false positive examples. False negative results include image distortions confusing the Wong et al. approach. One of them is the rotation. Image-match implements 90, 180 and 270-degree transformation by altering and requesting each image four times (at a high cost), but other angles are not covered. The Wong et al. approach is also very sensitive to truncated image, confusing the brightness comparisons. Unfortunately, there is no easy way to avoid false positives (single and collection issues). However, there is an approach on top of sole image fingerprinting to match more efficiently two albums, by adding metadata in the comparison. Instead of just comparing the images, it is possible to compare the titles and the main artist when the metadata is available. We expect to disambiguate the collection and single issues, where the title and artist are supposedly highly discriminating. The text comparison is based on a difflib algorithm (a Python library), self-described as: “The basic algorithm predates, and is a little fancier than, an algorithm published in the late 1980’s by Ratcliff and Obershelp under the hyperbolic name “gestalt pattern matching.” The idea is to find the longest contiguous matching subsequence that contains no “junk” elements” The image-match backend implements the storage of metadata in a JSON dictionary. Thus, an image in Elasticsearch (for instance) is an aggregation of three main parts: the signature, the words and metadata. The following graph shows that considering both the image and metadata improves the matching. From an annotated dataset, we run these five matching techniques: The image fingerprint score by itself obtained the worst results (93.8) compared to metadata only, especially Title + Artist (95.6). However the two combined raised the F1 score to 96.4. This analysis helped us to determine thresholds for each matching technique. One might think that such an approach eliminates all the caveats described previously. However, it can also reveal new false positives, which are less numerous than the ones observed earlier. In particular, when the image is close enough to validate a match, the text comparison can eliminate the option because of potential discrepancies from one source to another. To conclude, the cover image comparison led us to build a better matching algorithm, and an option to match albums in situations where the metadata is not available. Image-matching proved to us in an internal hackathon earlier this year that it can be legitimately used to match photographs or scans of a personal catalog to an official catalog, opening new ideas for features. The main condition is that the target catalog must maintain all cover fingerprints. Our experience with the Deezer and Musicbrainz catalog proved that it is reasonably feasible as part of a catalog ingestion and maintenance, in the same fashion as storing the audio fingerprintings . Our work on image-match led me to suggest (what I think to be) an improvement in the way of storing and retrieving fingerprint documents. For more information on how to join Deezer, have a look at our dedicated website: www.deezerjobs.com Stories of engineering, data, product and design teams… 62 Thanks to Romain Lods . Data Science Fingerprint Album Covers Deezer Research And Development 62 claps 62 Written by CTO of Vialma, a classical and jazz streaming platform. Triathlon enthousiast. Classical music nerd. Stories of engineering, data, product and design teams building the future of music streaming Written by CTO of Vialma, a classical and jazz streaming platform. Triathlon enthousiast. Classical music nerd. Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-01-03"},
{"website": "Deezer", "title": "a distributed computation system for deep learning experiments with docker compose and rabbitmq", "author": ["Anis Khlif"], "link": "https://deezer.io/a-distributed-computation-system-for-deep-learning-experiments-with-docker-compose-and-rabbitmq-5ac4ab344406", "abstract": "Previous stories About Jobs We created this article to present a solution designed to handle the distribution of the computation tasks that arise when we run deep learning experiments that require heavy processing to generate data during the training process. It shows a practical case study for the usual broker pattern, and gives an example of how it is possible to painlessly deploy with Docker the different components of the system’s stack. In our case, we mean: a RabbitMQ server hosting the job queues a manager server to administer computation sessions swarms of workers to handle the computation tasks associated with each experiment a program that controls the health of the clusters of workers and that adapts the publish rate of the tasks in real-time. The application layer, used by the manager and worker components, has been developed in python using the framework Celery . Other technologies involved are: Docker , docker-compose, docker swarm, docker stack Flask (for the manager server), Tornado (for asynchronous networking in the health controller) Joblib to cache the results of the tasks At Deezer’s Research and Development team, a large part of our activities consists in designing deep learning experiments, and training models able to generate knowledge on our track database. Examples of such system are: Royo-Letelier, Jimena, Romain Hennequin, and Manuel Moussallam. “Mirex 2015 music/speech classification.” Music Inform. Retrieval Evaluation eXchange (MIREX) (2015). Hennequin, Romain, Jimena Royo-Letelier, and Manuel Moussallam. “Codec independent lossy audio compression detection.” Acoustics, Speech and Signal Processing (ICASSP), 2017 IEEE International Conference on . IEEE, 2017. Below is shown a typical representation of an experiment. In our case study, the data generation pipeline is run on CPU, while the training is performed on GPU. The data generation takes its source from a dataset specific to the experiment, as represented by the circle in the figure above. When in production, these systems run permanently, analyzing everything that is added to the catalog, and exporting information that will serve various purposes, like fueling up our recommendation systems. However, bringing such systems to production takes time, and tremendous research efforts are invested into the design of the experiment that will produce the best results. Training a model is a very lengthy process, especially when the number of parameters is high and the size of the dataset large. Another factor that can increase the training time is the generation of a the data that goes into the system. Let’s start by taking a look at what the features are, to make this statement clearer. In a lot of experiments, akin to those involving images, raw data is directly fed into the network, with possibly light preprocessing. In this situation, the time taken to generate new batches to train the model on is effectively negligible compared to the time taken by the training pass. This is even more so when the whole database can fit into the RAM or when the model is complex. In the first case, the access times to the data are very low, and in the latter, the time taken to update all the parameters becomes high. Raw data is seldom used in the case of audio experiments. Instead, descriptors (or features) computed on the raw data are used. There are arrays of values as well. Some of these features have the advantage of lying in a space of a much lower dimension while maintaining all the relevant information for the task (or such is assumption). In such situations, using them instead of raw data considerably reduces the number of free parameters in the model, which in turn reduces the training time and possibly the required amount of data needed to train a robust system. On the other hand, directly feeding raw data into the network does away with the need to find good features by transferring the responsibility of learning them to the first layers of the network. That being said it has been noticed that when feeding raw audio data into a network, the first layers effectively learn features that resembles a lot the usual spectral features used in audio processing. Example of such features are: Mel spectrograms Mel Frequency Cepstral Coefficient (MFCC) Constant Q Transforms Spectrograms or log-spectrograms For the reasons listed above, we always take the decision to feed our networks with features instead of raw data when it comes to audio. The direct consequence of that is a dramatic increase in the workload that goes into data generation, which now becomes significant and makes up a large part of an experiment run time. Moreover, it is common to artificially increase the size of the dataset by applying additional transformations (which are also computationally demanding) on features to simulate new samples. This is commonly known as “data augmentation”. Examples of transformations are pitch shifting, equalizing, noise addition, lossy encoding, etc… For the music/speech/noise classifier experiment, we used around 100k 10-second excerpts, multiplied by nine different transformations. Computing all the corresponding features in parallel on a single machine takes a couple of days. The features for this experiment amount to tens of gigabytes. Fortunately, the task of computing features and transformations over each entry in our dataset can be carried independently and in any order, making it an ideal candidate for parallelization. We used to do this by multiprocessing but why not using as many machines as we can? Besides, the same entry in the database is commonly used more than once during the training (at least one per training epoch). Therefore, we do not want to compute the same features over and over. To handle the job of caching features, we use a library called Joblib. It takes care of writing out computation results to disk, and retrieving the results from the disk on subsequent calls to a cached function. Ideally when using a cluster of machines, we would have a shared file system to write this cache. To better understand how this goal can be reached, let us take a closer look at how we generate data in practice. We have developed our own framework built around Keras that takes care of a lot of things, including creating a pipeline to extract those features. A pipeline is a chained list of generators (blocks) that lazily generate samples, and perform operations on it. It follows a Pipe-filter pattern. Blocks can be used to: Add transformations Shuffle samples Gather samples in batches much more… All the objects involved in the pipeline (including the pipeline itself) are serializable, meaning that they can be easily sent through the network. A sample represents one entry in the dataset against which we wish to train the model. It is an object that contains all the information that makes it unique like the path to the audio file, the start and end time, possible transformations to apply etc…. Features are computed for each sample. The output of the pipeline is directly fed into the network. Among the existing blocks, there is one that is of particular interest to us: the one responsible for computing the features. It is instantiated from a configuration that contains a description of the features it should extract (there can be many when doing multi-modal learning for example). Without remote task distribution, we are find ourselves in the situation where a single FeaturesAnnotation block is used to compute features from a sample (possibly using multiprocessing). The architecture we would like to reach is the following. For simplicity and without any substantial incurring costs, we can imagine first computing offline all the features and store them into a cache. The features computation is split across many machines and the results are written to a shared cache. In our situation, the cache is shared by being written to an NFS. This cache can then be copied locally to the machine that will run the experiment to reduce the read-times. When re-running the pipeline for training, the features computation step will have been reduced to a disk read. Now that we have an idea of what we want to achieve, let’s move on to the implementation part! By now, we’re in the situation where we want to distribute a finite list of tasks to be handled by a cohort of workers. We want to be able to publish feature computation tasks associated with an experiment, and we want these tasks to be collected by workers. In terms of interface, we want to be able to interact with the system through a simple HTTP API, where we could post the configuration describing the experiment that we already use to train our models, and the features would start being computed in a distributed fashion. Quite a lot of solutions are available to perform the actual distribution of tasks. To reduce the field of possibilities, a few more requirements are worth mentioning: We want guarantees that all the tasks in the experiment are executed (no loss) We want the task queue to be persistent, so computation can be easily resumed in case of a failure The tasks taking a while to be executed, the publish rate for the tasks will never be a bottleneck. So we do not mind trading speed for robustness, or coding time. The implementation must not depend on the number of workers, and in particular, the number of worker must be flexible (with machines going offline, or new machines joining the work pool). Put like this, it immediately calls to a centralized system, built around a broker. The broker would take care of the communication between publishers and workers, by receiving tasks, storing them to a queue, and handing them to workers that ask for it, while keeping tracks of the state the tasks are in. All of this is done extremely well by RabbitMQ . An advantage of going with RabbitMQ (over some other message brokers), is that there exists a very nice python framework called Celery, that provides an additional application layer over messaging protocols like AMQP (the one used by RabbitMQ), and which would make the development of our application considerably easier since it already implements the notions of task, publisher, worker, result back-end, removing the need to master the AMQP protocol. Publishing and executing tasks is made really simple, all we have to do is the following. From the publishers’ perspective: build a pipeline up to the FeaturesAnnotation block run it to get samples serialize these samples publish them as celery tasks From the worker’s perspective: retrieve the configuration for the experiment instantiate the FeaturesAnnotation block start receiving samples run the features extraction on them (which comprises the caching to the shared cache) Assimilating a Celery task message to a sample is an elegant solution because it eliminates “by definition” of the sample a lot of redundant information, and therefore keeps the network usage to a minimum. For example, the configuration describing the features to extract, common to all the samples, is only transmitted once to initialize the worker. Zoom on the manager We have reached a state where we have a manager server, that exposes an API to which configurations for an experiment can be posted and handled by the service manager. The service manager is responsible for launching new computation sessions and administrating them. It holds references to all the sessions that are currently running. Each session is responsible for creating the exchange and the queue associated with it, and for publishing samples (or tasks) to it. The routing logic is extremely simple since there is a one to one mapping between an experiment and a queue, all of which are identified by a name. The worker only has to subscribe to a single topic and start consuming tasks from the corresponding queue. A Celery application being thread safe, all the sessions’ publishing job are run in separate threads. Controlling the publishing rate of the tasks All of this works pretty well except on one point. The session can publish tasks at a much faster rate than the worker can consume them, which bloats up the queue. This is undesirable for many reasons, the most obvious being that we don’t want to saturate the rabbitmq server’s memory, which is a real risk especially when running many sessions at the same time. Therefore, we came up with another component in the system to tackle this issue: a program that can be run from any machine in the same network as the manager and the RabbitMQ server, and whose job is to perform control over the tasks’ publish rate. It collects health metrics retrieved either from the manager’s API, or from the RabbitMQ server, and takes a decision to increase or decrease the publish rate for a session. When the new publish rate is decided, a call is made to the manager’s API to set it in the corresponding session. This presents a lot of advantages like: Freeing the manager from having to do that itself so it can focus its resources on the API and the session management. Dynamically adapting the publish rate to the available computing power, if a machine goes offline or if we add new workers. We know that we always make the best of the available power. Prevents the message queue from getting too big. To do that we could have opted for full blown solutions like Prometheus but we felt like it was an over-engineered option to our rather simple case. Our solution is a single python script called “health” based on Tornado, here is how it works. We have different monitors that periodically collect data asynchronously from the manager, the RabbitMQ server or the workers, and store these values in doubly linked list acting as time series. Values from these time series are also retrieved periodically through Tornado’s event loop by controller objects (one per session). These controllers use the retrieved data to make a decision as to how the publish rate should evolve, and send it to the manager. At this stage we have identified the different actors of the system and implemented what needed to be. Still, some questions remain: how to deploy all those bricks in practice, and in particular the workers? It is clearly out of question to run each of them manually. Now that we have carried the ground work, our application has individual parts that seem to get the job done. It is time to bring it all together and make the ensemble shine! Enters Docker. An advantage of the solution we have laid out, is that it is made of well separated actors behaving in a well defined environment. This makes the process of containerizing them a lot easier. When they are, we will be able to easily deploy the RabbitMQ server, the manager and the health program with docker compose. Deploying workers will just amount to creating a docker service. By far the easiest part to containerize is the RabbitMQ server, because there is literally nothing to do! There is an official image for it so let us just use that. Actually, since we need the management plug-in, we deploy the image tagged “management”. Here is the hierarchy of the docker images that we have built. Manager, worker and health mainly differ by the command run on launch. Now, deploying the Celery workers for an experiment becomes as simple as creating a swarm service. This is done by the Session objects using the docker-py module. It communicates with the Docker daemon on launch to create a service with the same name as the experiment. The final strike At this point, we are able to: Run all the components of the system with docker compose Create a swarm service to spawn Celery workers There’s one glitch left to fix. Since the components need to be able to communicate with each other, the first thing we did was hard-code in the configuration files their addresses. This is clumsy since it requires us to change the address depending on the machine the components are running on. Now that we deploy with Docker Compose, all the components live in the same Docker network, and Docker takes care of resolving addresses from the component’s name! That means that instead of an explicit IP in our configurations we can use only “manager”, or “rabbitmq-server”. Because the workers are deployed through docker swarm, and since they need to contact the manager to retrieve the configuration, it would be awesome to make sure they also belong to the same network in order to use the same trick and completely remove the need for any hard-coded IP. When creating a service, it is possible to bind it to an existing overlay network. However, Docker Compose creates a bridge network by default. A simple solution to that is creating a specific network in the compose file. Another, is to deploy our components with docker stack. Docker stack will simply take the Compose file and use Swarm to deploy it. In doing so, it creates an overlay network, used by swarm, and now only remains to attach the services we create for this network. We have now completely eradicated the need for any hard-coded address, and have robust way to deploy everything anywhere without any change, and docker stack can be used to update the software in one command! We can now focus on the experiment ;). We have described an architecture we have been using for a few months in production on our machines to process as quickly as possible all the computation task necessary to start the training of a deep learning experiment based on audio. How to easily deploy such architecture is a problem in itself for which we provided an answer that relies on powerful technologies like RabbitMQ and Docker (swarm and compose). Although we have deliberately chosen to present this work under the scope of our own activities (deep learning experiments), there is nothing so specific to deep learning in this architecture, and it should be suitable for a great variety of other use cases, where we simply wish to distribute a number of individual, independent tasks, in a processing pipeline. Stories of engineering, data, product and design teams… 61 Thanks to Mickaël A and Alba Xhani . Docker Deep Learning Rabbitmq Research And Development Deezer 61 claps 61 Written by Stories of engineering, data, product and design teams building the future of music streaming Written by Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-12-18"},
{"website": "Deezer", "title": "web and surf summit afterglow a product managers review", "author": ["Elize Bombeztic"], "link": "https://deezer.io/web-and-surf-summit-afterglow-a-product-managers-review-610e31f38056", "abstract": "Previous stories About Jobs Lisbon, an up and coming tech hub in Europe, host to the Web Summit for the second year in a row. This year was also my second year attending the Web Summit and where I was overwhelmed by the amount of attendees, talks and start-ups at last year’s edition, this year I came slightly better prepared. First of all key to the Web Summit are the side events. Like last year the best conversations did not happen on the Web Summit stages. The stage talks just set the tone, the start-ups took it further by presenting themselves at their booth or through a pitch, investors shared their thoughts, founders their ideas, attendees their knowledge. To make the most out of my trip I decided to kick things off with a special affiliated side event, the Surf Summit . In the weekend before the Web Summit, a group of outdoor enthusiasts met to surf, dine and network in the beautiful surroundings of Mafra, in the town of Ericeira. Ericeira is known for its great surfing conditions and being the first World Surfing Reserve in Europe. The talented team at Na Onda coached us in the water while we surfed along side, well at least in the same waters, as pro surfer Anastasia Ashley . During the Surf Summit I met with some inspiring founders of start-ups, talked product with fellow product people and discussed the future of blockchain technology and how this can help industries become more transparent but also the risk bias brings to machine learning technologies with investors and CEOs alike. Similar talks would be held the following days on the Web Summit’s main stages in Lisbon, but not before we formed the base of new found friendships and potential future (business) connections. Monday’s opening night promised a good start, especially during the pub crawl. Tuesday’s talk by Des Traynor, Co-founder and Chief Strategy Officer of Intercom , highlighted some key issues start-ups face when creating a product strategy: the idea might be good, but does it solve a real problem, i.e. is the problem you’re solving so critical that people are willing to pay for your product? Obviously our Deezer product solves some clear user problems as our research team has been able to verify and our growing numbers support. Access any time and anywhere, a large catalog, audio quality, offline listening, all key elements that make our product work. But how does Flow fit in? I need to make sure the problem we solve is big enough to attract more users, keep them satisfied and surprise them at the right time. Something that was confirmed by one of my product mentors Dirk Bartels who was pitching with his BETA start-up Featvre : make sure your core product is on point before you add “delightful features”. Another inspiring talk was a conversation between a “flying car” maker, the leader of an aerospace accelerator and the GM of Airbus’ Urban Air Mobility efforts, who explained why the idea of urban air mobility has finally grown wings. As a fan of the Jetsons growing up, I can’t wait to see a world in which we don’t need cars anymore and can finally cruise through the urban air. Coming to us by the early 2020s, even if it won’t be a mass product just yet, the future is near. Based on the popularity at the AutoTech/TalkRobot stage, more so than last year, we can tell that self-driving cars are not the next big thing anymore, they are here to stay, even after last year’s obvious questions about safety and insurance issues. The next big thing is urban air mobility. Speaking of robotics, Sofia the AI Robot was quite an impressive appearance . What struck me most were her facial expressions. Even if her responses were sometimes still a bit unnatural, you can tell it’s only a matter of time really. One of the talks to which I’d been looking forward, but which took a segue into Facebook and Russian politics, was the conversation between VP Product FB Messenger’s Stan Chudnovsky and CNN Money’s senior technology correspondent Laurie Segall. Where Stan Chudnovsky was trying to sell the future of chatbots for businesses within Facebook from a product point of view, Laurie Segall asked him some tough questions that needed a nod to Mark Z’s previous statements and unfortunately didn’t get the conversation about the future of AI in messaging communication back on track. On Tuesday’s after hours, the Night Summit took the crowds to Pink Street. We decided to stop by the Sunset Summit first, go for a great dinner at Faz Figura to enjoy some more views of the city from Alfama and then headed towards the Funky Party by La French Tech, Duplex and MusicBox. On Wednesday I spent some time at the Content Makers, PandaConf -nobody could tell me what this actually stood for- and the Creatiff stage, hoping for some inspiration in content and creativity. The best talk on the PandaConf stage that I managed to see was held by SurveyMonkey ’s CEO Zander Lurie on Tuesday actually where he stressed the importance of asking why. “The answer to the arrogance in the business culture is curiosity: pushing the envelope to learn, in the age of AI and Machine learning, curiosity is the last defence in our trades.” It was a clear story that blended the sales pitch for their product with some interesting findings about the people that should matter the most: your employees, followed by your existing customers and thirdly your potential customers. For instance in the US 69% of C-suite thinks there are no barriers in asking questions within the company, whereas only 36% of employees feel the same way. Asking the right questions is an art, as has been said by Tony Robbins, and Zander Lurie repeated this mantra while stating “the quality of life is created by the quality of your questions”. His main advice on how to create a culture of curiosity in your business: Create a safe space for Q&A Hire a diverse team Reward great questions Connect all teams to customers Provide context about your data While getting my brain re-hydrated I stopped by the talk of DoubleNegative ’s CTO Graham Jack about “VFX then and now” used in Blade Runner 2049, being the not-so-secret geek that I am, only to stay for the most hilarious talk after: take a fake product from scratch to launch to show you the a-z of content creation by David Schneider, Gail Heimann and Zoe Henry. I was not sure if I needed SPATNAV, a personal AI for your love life, in my life but after watching the launch plan come together I think I might want to give it a try, if it ever becomes available on the app store of course. AI was clearly dominating most of the talks during the WebSummit. Another buzz word this Web Summit as mentioned before was blockchain . Cryptocurrencies are just the start, blockchain offers huge potential for accessing medical data safely and immediately, creating a transparent supply chain and sharing resources for business solutions. Blockchain will continue to disrupt and improve several industries. Finance, health care and logistics are just the start. Wednesday was also an opportunity to explore LxFactory, home to the Night Summit for this night. First we explored another side event ‘Hong Kong: Asia at the Cutting Edge’ at the most impressive venue I had witnessed during this trip: Palacio Foz. Thursday, last leg of the Web Summit and all about Music Notes. I was supposed to take part in an intimate Round Table discussion with Werner Vogel, the CTO of Amazon, to talk about the future of voice. Unfortunately the organization messed up the Round Tables and had let in too many people. Having seen some talks on the schedule about the future of voice in the work place, wondering how we will be able to use such technology without all shouting at our screens in the open space that is our office, I was left with questions unanswered. Instead I decided to join our Chief Content and Product Officer Alexander Holland at the Music Notes stage where he expressed how curated playlists are indeed serving as the new taste makers, alongside (local) radio stations. Radio is a brand on its own, it’s personable, often has great local selections but only reaches a small audience. Streaming platforms can give them global reach, whereas curated playlists blend local content and provide data to artists featured in these playlists to know exactly where and when people have been listening to their tracks. Funnily enough Laidback Luke seemed genuinely excited about algorithms as well as these provide an even more personal recommendation, if executed right, because Flow, Discover Weekly and I’m Feeling Lucky all have something in common in terms of users’ responses: “How did you find this track? I love it but this is crazy!” That obviously only works as long as the content on the platform is carefully tagged and curated, which is why tech can definitely help save the music industry. Algorithms can dig up the gold your ears need to hear, especially when supported by curated content, dedicated fans and engaging artists. Our CEO Hans Holger Albrecht highlighted this during his talk on the Centre Stage , but before his conversation alongside Martin Garrix and Wyclef Jean, Roland Lamb of Roli demonstrated with piano virtuosi Marco and Jack Parisi the future of synths. Making (electronic) music has never been this accessible. Thanks to new technology, not only for production tools, but also for marketing: “try giving out your personal phone number to increase social engagement” was suggested by Ryan Leslie on the same stage. Times have changed. Remember when we taped radio shows at night with the volume on low, to listen to the music the next day on our walkman? Or was that just me… I guess I was already trying to solve a problem at an early age: missing out on radio shows after bed time, hoping to discover new music of the previous night in the following morning. To finish up a day of music talks right, we went for music tech drinks hosted by Tiago Correia of H Bureau . In the Impact Hub co-working space around LxFactory a group of musicians, tech people and start-ups gathered to drink and mingle. We headed back to the city centre’s infamous Pink Street afterwards and ended up at Beato , Lisbon’s brand new innovation and creativity hub, which is said to become one of the biggest hubs in Europe. Factory , Station F , StartupDelta , watch your back! It was a wonderful end to an eventful week. To finish up my blog about the Web Summit 2017, I leave you with a shortlist of start-ups to read up on and a nice playlist to match the Web Summit experience, curated by yours’ truly. radiu.io (NL) — RADIU filters favourite music from radio stations around the globe and turns it into one personally adapted radio station Sounds a bit like Flow, but then more complicated… reminds me of Milk Music partyplay.cc (ES)— PartyPlay is a new way to manage music at a party by creating one playlist for multiple users, combining content from different platforms Fun idea and we all know the problem, but how to work with freemium (and premium) access to specific content from different platforms? gigworks.com (DE)— Connecting professional artists and event communities We saw a few similar start-ups last year, like https://www.livemasters.co/ jyve.io (US)— Jyve streamlines the music booking experience by connecting musicians to local venues, venues to music talent and fans to Seems to be in the same line of start-ups around booking platforms, reminds me of fyre bookings as well alissia.io (DE)— blockchain powered musical journey technology Pay per stream and a-coin, the international (crypto)currency for streaming music, not sure if labels would be convinced about this Stories of engineering, data, product and design teams… 43 Websummit Startup Deezer Product Management Conference 43 claps 43 Written by VP Product // Conference speaker // Open to (remote) opportunities // Ex @MTV + @Deezer + @SpideoTV // 💕music // https://www.linkedin.com/in/elizebosker Stories of engineering, data, product and design teams building the future of music streaming Written by VP Product // Conference speaker // Open to (remote) opportunities // Ex @MTV + @Deezer + @SpideoTV // 💕music // https://www.linkedin.com/in/elizebosker Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-12-08"},
{"website": "Deezer", "title": "deezer research recsys 2017 11th acm recommender systems conference", "author": ["Alexis Benichoux"], "link": "https://deezer.io/deezer-research-recsys-2017-11th-acm-recommender-systems-conference-9b92e1d9d0f3", "abstract": "Previous stories About Jobs Blog post coauthored by Viet Anh Tran. Special thanks to Romain Lods . A key aspect of Deezer product is providing our users with a personalized listening experience. To pick the perfect tracks for the right moment, a wide variety of signal processing and machine learning algorithms are involved in Deezer’s recommender system, and our teams are constantly working to improve them. To this end, we sent two of our best research engineers to this year’s ACM Recommender Systems Conference in Como, Italy. 2017’s hot topics include: Matrix Factorization Transfer learning Cross-Domain Recommendation for large-scale data Deep Learning Below is a selection of papers presented during the conference. E Palumbo, G Rizzo , R Troncy entity2rec is an interesting user-item embedding system demonstrated on the movie lens dataset. First they build a knowledge graph including actors, directors, genres which represent users’ preferences. Then they project the graph using random walks and word2vec , which sends all nodes (movies, users, features) to the same space. Top recommendations for each user are obtained among nearest neighbors in the resulting space. D Xin , N Mayoraz, H Pham, K Lakshmanan , J. R. Anderson Alternate Least Square algorithms for matrix completion have attracted a lot of attention in the recommendation community since Simon’s seminal blog post “ Netflix Update: Try This at Home ”. It provides a fast and scalable method to predict users’ ratings and still is the core of a wide range of recommender systems. There is very little theoretical explanation, it just works . However, the success of this approach relies on some misunderstood hypotheses and some scenarios lead to spurious recommendations. This nice visualization of what happens in a “block low-rank” scenario provides some intuition on how to reduce the folding of users’ prediction. User-item blocks are easily understandable: extreme movie genres (kids-horror) and countries induce a strict co-segmentation of the user-item matrix. Then the folding happens: a naive ALS algorithm will rate items out of user-item blocks, resulting in a few far fetched items polluting users’ predictions. Sergio Oramas , Oriol Nieto , Mohamed Sordo , Xavier Serra Content based features are very precious for long tail recommendations: rare tracks with few daily streams are often left apart by popularity based recommender systems. Building an item-item similarity measure based only on tracks and metadata, it is possible to build tracklists using the entire catalog. Here is an interesting deep learning pipeline for feature extraction that combines both audio spectral analysis and artist biography. And of course we are very excited about ACM RecSys Challenge 2018: Spotify Playlist continuation … Stories of engineering, data, product and design teams… 70 1 Machine Learning Music Recommendation System Deezer 70 claps 70 1 Written by Stories of engineering, data, product and design teams building the future of music streaming Written by Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-10-13"},
{"website": "Deezer", "title": "engines revving for my first hackathon", "author": ["Dorothée Doublet"], "link": "https://deezer.io/engines-revving-for-my-first-hackathon-578cd661a24", "abstract": "Previous stories About Jobs Laughter, stress, the curse of the demo, relief and pride: hackathons are often an emotional rollercoaster. Before my participation in my first two-day hackathon, organized by Deezer last January, I wasn’t sure what to expect. And what awaited me was definitely not a walk in the park. Since the previous day, the atmosphere of the office had changed. The usual calm of the open space had evaporated to be replaced by a fever pitch. We were full of beans and ready to stay for beers, pizza and the raspberry pi! We didn’t notice when night fell, because the project was taking shape; because it was teamwork; because the demo… was the following day. I had joined the ranks of Deezer as an iOS developer a few months earlier. It was a small team and I was a little timid at first. For the hack day we decided to spend two days creating an improved experience of the Deezer app for drivers. We presented the project, and recruited a few people who were convinced by the economic benefit underlying such a project. In the end, five of us climbed on board. Big buttons and rapid access The idea was simple. We didn’t want to reinvent the wheel, we just wanted to create a simple and fun experience for Deezer users who drive, and allow them to use the iOS app in their car safely. It’s not an easy task, because a phone screen can appear small when mounted on a car’s dashboard, but clicking on the normal version of the app requires precision and attention. As with devices like sat-navs and stereos, interaction with the Deezer app can only be done when the car is stationary in a safe place, so allowing users to do so quickly and easily would make life much easier. Big buttons providing simple and rapid access to Deezer content: and all on one screen… Over the course of two days, we saw the project take form. For fun, we decided to add a ‘Wake me up!’ button. After an intense couple of days, an interested audience came to see the project presentations. “Who has ever yawned their head off and felt their eyes closing whilst they’re on a long journey?”, asked Erwan, the originator of the idea. “If you click on the new button ‘wake me up’, you’ll get an explosion of music. We’ll select the song from your music based on its beats per minute. You want rhythm, we have it!”. The sound of AC/DC blasted out, whilst we explained that songs with a higher bpm tend to be those thumping beats. The rest of our presentation concentrated on the marketing and business opportunities that the project could offer. Buckle up! When we were selected for the second round, we couldn’t believe it, but we were determined to give it our all. Again, we suffered from the curse of the demo… we obviously had some improvement to make on that front. Happily, we kept smiling: being able to laugh at ourselves kept us from going off the road. Regardless, it was clear that people liked our idea. When the winners of the hackathon were announced, we couldn’t believe our luck. The key outcome was the opportunity to develop our idea for real. There were smiles all round. Woo! Buckle up! I hadn’t yet realised that it was the next step in this beautiful adventure that would teach me the most. Puns aside, we burned rubber organise our next meeting. The idea: to bring together all the contributors who would be tasked with releasing a productionised version of the feature presented during the hackathon. Designers, an agile coach, Android and iOS developers, and a product manager. In fact, nothing was going to happen as we expected. I feel like I’m stating the obvious, this was not a normal kind of project. It wasn’t just a matter of bringing together teams who usually work together: the way of working was new for everyone, and time was short. Thanks for now! Everyone was very motivated, but conscious of the limited time available. The die was cast, but the roles and objectives weren’t clear. Should we produce the solution from the hackathon without touching it and release it on iOS and Android? Or should we re-evaluate, based on the logic that an idea developed in two days would need a little polishing. We were divided on which road to take. The week flew by. Day one: we cleared the decks. At the end of the day, some people were still working on the definition of the minimum viable product. Second day: we picked it apart a little. Third day: the same. Were we going to follow the proposals of the designers, or the MVP established by the product team? Development started on the fourth day, when there remained only one and a half days. Mission: Impossible. The following month of development passed quickly. Despite the motivation of the team and the interest generated by the project, which was palpable, there was a lot of hesitation along the way. Today, car mode is available for Deezer premium users, assuming that they have activated the feature on the Labs page of their iPhone. It’s these users who will give the verdict on the work of the last few months. Will they welcome this simplified mode in the app? For our part, I hope that we will learn from this experience, thanks to our use of user statistics and reviews on the App Store. Regardless of the outcome, I loved working on this project. The idea of mixing up the teams and agreeing on a short timeframe to release a project went really well. Whatever happens, in my opinion, it provided an awesome chance to get out of my comfort zone and bring benefit to the product, working with new people with differing opinions and ideas, and have a go at driving on the other side of the road. This is exactly the reason that I enjoy coding at Deezer. I hope that we can replace the bonnet again soon, and take lessons from this experience. Thanks for now! It’s an incredible adventure that I’m not ready to forget! Stories of engineering, data, product and design teams… 126 Hackathons iOS Deezer Labs Features 126 claps 126 Written by ios developer, otherwise journalist Stories of engineering, data, product and design teams building the future of music streaming Written by ios developer, otherwise journalist Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-10-26"},
{"website": "Deezer", "title": "are you chilling right now contextualizing music recommendations", "author": ["Mickaël A"], "link": "https://deezer.io/are-you-chilling-right-now-contextualizing-music-recommendations-b449555889c8", "abstract": "Previous stories About Jobs Blog post coauthored by Carlotta Riesser . As you may know, Sonar Festival is one of the biggest electronic music festivals in Europe and takes place every June in Barcelona. Alongside the festival, Sonar organizes a technical challenge as part of the Sonar +D. For this event, Sonar has partnered with companies who define new challenges. Several groups get together in order to work on these subjects. Deezer is once again part of this challenge for the third year running. This year, we tried to pick an actual product issue encountered at Deezer. One of Deezer’s core values is catalog diversity. Today, we have over 43 million tracks to offer, and will expand in the near future… This is great, but also represents a big challenge for Deezer. Curation, personalization and recommendation are key to us in order to define the best possible user experience, which is to offer the perfect tailored content, in a specific context. Understanding the usage context is essential. Home context is probably one of the most complex ones, as it is influenced by a lot of different things: Activities: cooking, taking care of family, work, working out… Moods: lazy, active, sleepy, happy, sad… Connected devices and IOT present in homes: TV, speaker, lights, PC, mobile… The time of day, weather, season etc. Although the hack is defined by the team, Deezer introduced the topic to give a direction. When a user is at home, how can we detect the context in which they are listening to music and recommend music accordingly? We engaged the challengers on this topic and encouraged them to come up with cool ideas. At first, we anxiously waited for applicants. For a more interesting challenge we made a wishlist with complementary profiles: a UX designer, a mobile developer and a data scientist. We really wanted to bootstrap the project before the Sonar took place to anticipate specific needs on our side (hardware or software). Our past experiences have proved that starting on site with a little preparation beforehand is far more productive. Then it happened! Sonar sent us an email with five challengers all with impressive credentials. We talked internally about asking for more applicants but quickly realized that this was enough. We got ready for our first video call. Unfortunately it didn’t go smoothly because of technical issues: we desperately tried to find the right tool (Hangout, Slack, Skype?) to reduce lags and sound issues like sudden echos, sound drops, or background noises. It took us half of the first meeting to efficiently talk to each other. When we finally managed to communicate, we were pleased to discover that each of the challengers was already bringing ideas to the table. From this call onwards we narrowed down the subject, and took action on our side about who to include and what equipment to use. The project’s goal is to capture the listener’s environment thanks to various sensors, to identify the activity they are currently engaging in, and to recommend the adequate music. The retained activities were: relaxing, working out, cooking, sex and study. The sensors captured temperature, humidity (from a Raspberry Pi + sensors), accelerometer, proximity, light intensity (from a smartphone), heart rate, steps, activity (from a Fitbit provided by Deezer), weather and time (from external APIs). Deezer, for the first time, opened a recommendation API to the challengers, to help them leverage the power of our algorithms, and to be able to improve them thanks to fresh new data. This data helped the challengers to train models, for instance. On site, the atmosphere was studious. When we joined our team — and met them for the first time — they were already working alongside the other teams. The impressive congress hall was tall wide and pretty hot in Barcelona especially during the heatwave. We all prepared to spend a few sweaty days at Sonar! We opened our laptops and joined the hack. During these three days, a model was trained to classify audio tracks into the five chosen activities. Another was trained to detect activity from the sensor’s data (well… simulated to be more accurate!). All of the pieces were put together around an Electron’s app. The whole concept, baptised Yin, describes this enhanced listening experience. [UPDATE]: The video of the presentation is available here: The Sonar +D challenge was again a great experience. We achieved what we wanted: A project which answers to a very concrete Deezer product issue. A great collaboration with great profiles. A proof of concept which inspires Deezer’s work and strategy. With this proof of concept the first step is to create in-house awareness on context aware recommendation systems. Secondly, we hope that this challenge will be the starting point of an internal product initiative and that we will begin to focus on contextualized recommendations. Stories of engineering, data, product and design teams… 32 IoT Personalization Content Curation Recommendations Sonar 32 claps 32 Written by CTO of Vialma, a classical and jazz streaming platform. Triathlon enthousiast. Classical music nerd. Stories of engineering, data, product and design teams building the future of music streaming Written by CTO of Vialma, a classical and jazz streaming platform. Triathlon enthousiast. Classical music nerd. Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-10-31"},
{"website": "Deezer", "title": "real time music visualization on the iphone gpu", "author": ["Arthur Guibert"], "link": "https://deezer.io/real-time-music-visualization-on-the-iphone-gpu-579d631272d3", "abstract": "Previous stories About Jobs Earlier this year we organized our in-house Hackathon at Deezer . One of the themes was “ More Emotional Engagement ”. We decided to get together and work on a real-time music visualizer (think Winamp). The idea was to display an animation that would react to the music currently playing in the Deezer app. We required a very flexible framework and playground in order to make original animations, so we resolved to create a visualization directly on the GPU using shaders. We also included a bonus: they could run on Android, in web browsers with WebGL or any platform compatible with OpenGL. The first thing was to identify the information that needed to be extracted from the track. There were several options: we could obtain the current power/volume from the audio signal (think RMS ) or provoke a reaction if a high-pitched sound was emitted. The main tool used to work on frequencies using a signal is called a Fourier transform . The method is to apply a FFT ( Fast Fourier Transform ) algorithm on the audio signal. There are a few ways to do that on iOS. We chose to use AVAudioEngine, then tapped on the output buffer and finally applied the FFT with the help of the Accelerate framework . But before that we needed to initialize an instance of AVAudioEngine with an AVAudioPlayerNode . You can see the process in the following code extract. Following that step we needed to get the audio buffer (an array of float) to perform the FFT. In order to do so, we used the method installTap to observe the output of the player node. Now the fun part: performing the FFT on iOS. As I said earlier, in order to succeed we recommend you use the Accelerate framework. You can see a Swift implementation below. Now that we had the FFT output we wanted to explore what we could do achieve. The idea is to create the visualization with a shader firstly, because it’s fast and secondly because it’s fun. You can look at what people are doing on Shadertoy . It’s inspiring. We decided to go with OpenGL as it’s supported by almost every mobile platform. We could have attempted the same thing with Metal but we would have lost the cross-platform ability that OpenGL shaders offer. We needed to show GLKViewController and then display a rectangle with a GLSL shader program attached to it. So the idea is to display two triangles. Here is the data we sent to the GPU below: We then created the buffer objects to display everything we needed. You can check out the process here . It’s still a bit cumbersome to work with C methods in swift — especially when there are pointers involved, but all in all we still managed to get the desired result. The tricky part is generally to transfer the FFT output data to the GPU as a GPU is mainly used to handle graphics. The way to do it is to transfer a texture but instead of receiving an image you obtain the FFT data stored in the pixels. You can see how we store the float array into a texture below: Now that we had everything sorted on the GPU side, we needed to create the visualization. If you’re familiar with GLSL, it’s quite a simple task: you just write a fragment shader that uses the FFT output data as a texture as well as a few uniforms (elapsed time, mean value of the audio signal, etc.) For a quick introduction to GLSL shader programming you can read the very good Book of Shaders . Here is a simple shader that only displays the FFT data as a texture in grayscale. This was all great but we needed to go a step further. We could compute the RMS ( Root Mean Square ) of the audio signal to have one single value representing the current audio level. In our case we turned it into a uniform variable for the fragment shader. We call it u_rms (cf. the code snippet below). Just for fun we did a few more… We’re always exploring new ways to improve your listening experience. We believe a clever visualization is a good way to accomplish this. This experiment will be integrated into the app one way or another, whether it’s in the Deezer mobile app ‘Labs’ or in the actual player. You can follow some of the tech events we host on our dedicated DeezerTech meet up group and discover our latest career opportunities on jobs.deezer.com . We’re currently looking for several Senior and Lead iOS engineers to join our team. Stories of engineering, data, product and design teams… 181 7 Thanks to Romain Lods . Programming iOS Visualization Deezer Swift 181 claps 181 7 Written by iOS Engineer @ Deezer Stories of engineering, data, product and design teams building the future of music streaming Written by iOS Engineer @ Deezer Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-06-01"},
{"website": "Deezer", "title": "computing royalties at deezer", "author": ["Thibault Roucou"], "link": "https://deezer.io/computing-royalties-at-deezer-782ee06eb177", "abstract": "Previous stories About Jobs Deezer, as a streaming service, needs to pay royalties to music providers (record companies that make their music available on Deezer) as well as publishers and copyright collective management organisations so that the music you listen to gets payed at the right price according to the number of times it has been streamed. We also need to generate and send them financial and statistics reports to allow them to pay their own right owners (performers, song writers) accordingly. Reports are also sent to various other partners (charts companies for example). This is a more complex problem than it seems, in particular because we need to compute the data from billions of listens coming from millions of users in almost 200 countries. Therefore a whole engineering team is dedicated to this work to make sure all computations are correct and completed in time. Logging Listens from our Customers In order to pay everybody fairly, we need to collect data from all devices that can stream music from Deezer. It includes the website, Android and iOS apps but also all the other devices like connected TVs, connected Hi-Fi systems, car stereos and many more. One of our jobs is to make sure that all these devices developed by different teams at Deezer send the right information to our system. Once we have all the listens from the users, we need to extend the log with additional data: we need information on the users’ offers to know how much we should pay, the name of the provider to know who we should pay, etc. Gather Information from Many Teams in the Company In order to extend the log with more data, we need to collect details from various teams at Deezer: Payment team: they provide information about offers and currencies for each country, which is needed because each offer is paid differently and we need to convert all invoices in local currency; Catalog team: they provide us with metadata information to know to whom we need to pay (labels, publishers or collective management organisations); Financial team: they have -among other things- the turnover information, which is a key information in the payment of the royalties since we pay the right owners according to how much money we have earned in the month for a specific country and service; Acquisition team: this is the most important team for us because they provide information about the contracts we have with rights holders (the applicable terms and conditions) and all the details that will help us send the invoices and reports to the right person (report format, ftp credentials, email for notifications, etc.) We consequently work with this team on a daily basis and provide them with a user interface to ingest all this data efficiently. Once we have gathered all the data we need, a lot of computation is required to analyze it and extract something meaningful. That’s why we need to use BigData technologies. The Royalties and Reporting team has been the first team at Deezer to use BigData technologies in 2012. A Hadoop Cluster was created to support the ever-growing quantity of data collected to compute royalties and generate reports. The cluster is now used by many teams within the company and is the center of all the business. At the beginning, the cluster was only doing the pre-computation for another system (that had been working since 2009) that would compute the royalties. But we understood in 2014 that this system had reached its limits (in terms of performance and features) and that we needed at least a new version or better a fresh new start. That’s finally what we started to do in September 2014 when the Zephir project was born. In March 2015, after a few months of parallel run with the previous system, the software was ready to be put into production and February 2015 was the first royalties computation performed by Zephir using only BigData technologies to do the computations. We decreased the computation time from 24h to 40 minutes ! Daily Statistics Workflow On the following workflow you can see the operations that we perform daily. We collect all the listens (logs streams) then we add information about users (like their offer and country) and the songs they listened to. We do a bit of cleaning of these logs (removing duplicates, malformed logs, etc.), then we gather information about partners so that we know what to send and to whom and create all the daily reports requested by our partners. Monthly Financial Workflow At the end of each month, a new workflow is launched (see following diagram). We take all the daily data we generated for the month and add data on offers, subscribers and turnovers we collected for the month on a specific country and offer. The royalties computation is then executed and various reports are created from this. Since the launch of Zephir in 2015, the team has kept growing from 2 data engineers to a team of 3 data engineers, 2 web engineers and a data scientist. The project has grown very fast and allows Deezer to be more and more precise in its royalties computations, especially in what we report to our partners. The BigData technologies even allow us to imagine new ways of making payments in the music industry: the user centric payment system, a much better and fairer way of distributing revenues to artists...but it’s another story! In the meantime you can discover our latest career opportunities on Deezer Jobs . In particular, the Royalties and Reporting team is currently looking for a Data Architect . www.deezerjobs.com Stories of engineering, data, product and design teams… 184 1 Thanks to Romain Lods and Ludovic Pouilly . Music Deezer Big Data Hadoop Music Business 184 claps 184 1 Written by Stories of engineering, data, product and design teams building the future of music streaming Written by Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-04-18"},
{"website": "Deezer", "title": "towards an accessible player", "author": ["Cormac Flynn"], "link": "https://deezer.io/towards-an-accessible-player-b38431bb5721", "abstract": "Previous stories About Jobs Earlier this year, Deezer hosted one of its regular Hackathons, giving developers from different teams the chance to collaborate on ad-hoc projects or fun ideas they might have been hankering to implement. Myself and Ahmed Abdellaoui, an iOS developer, decided to work on making the web and iOS players more accessible. As we would have to present our work, it seemed like the perfect opportunity to explain to Deezer staff and management why accessibility is important and how it can make the Deezer platform better for everyone. In this final article I’ll focus on the web player, a standalone component that currently appears in the bottom-left of the Deezer.com interface, and how we made it more accessible. In many ways the issues resolved — bad labeling, keyboard navigation problems & content unoptimized for screen readers — act as a summary of the topics we’ve covered in the previous articles in this series . Our first task was to examine the content of the player — what its sub-components were, what HTML elements were used for each control and how these were labeled for the user (if at all). The web player is composed of the following elements: The album cover for the currently playing track, overlayed with the track title and artist name; Buttons that control favoriting, the creation of playlists and other advanced features; A “seek” slider that shows the current position and allows the user to seek to any point in the track; Standard back, next and play control buttons; A bottom set of controls for changing the volume, setting the repeat and shuffle states and a button to open the list of current and upcoming tracks. To improve basic support for screen-readers and keyboard navigation, we first improved all labels, made sure simple controls were correctly represented by <button> elements, making them automatically accessible, and added :focus rules where :hover appeared in our CSS, as described in the previous article. I then focused on two elements in the bottom set of controls — the repeat control and the shuffle button. Take a look at this video, created for our Hackathon presentation, that shows the unoptimized site and player as read by a screen-reader. At the 30s mark, note that the screen-reader speaks “Repeat — button” as we change the state from “Repeat none” to “Repeat all” to “Repeat one”. The shuffle button exhibits the same behavior — its current state cannot be known as the label is always set to “Shuffle”. The solution in both cases was to improve the use of the aria-label attribute. In particular, the label should describe the future state of the button, rather than its current value. For example when repeat is turned off, the label should read “Repeat all tracks in the list”, as this is the action performed by selecting the button. When “all” is selected, the label should read “Repeat only the current track”, and so on. It might seem obvious, but our instinct is often to label controls by their current value rather than their value after the button has been clicked. Staying with screen-reader accessibility, we next tried to think a little about how a visually-impaired user might interact with the player. For example, suppose the user was listening to a “mix”, a randomly selected playlist of thematically-related tracks. Such a user would only know the artist and title of the currently playing track if they manually moved the focus to the text at the top of the player component (or if they know the track already). To accommodate such a user, we added some hidden text containing the artist name and the track title; in addition, we enclosed that text in an element with the attribute aria-live=\"assertive\" . The contents of the inner <span> are updated dynamically when a new track starts. The aria-live=\"assertive\" attribute tells the screen-reader to speak every change to this element, even if it has to interrupt itself to do so. The reader acts somewhat like a radio-show host, announcing the title as the current track changes before continuing with whatever it was previously speaking. There are two sliders in the Player component — the seek bar and the volume control. The WAI accessibility guidelines describe attributes and functionality changes that work to make a slider component more accessible to keyboard and screen-reader users alike — MDN has a good summary . In addition, we used the “Able Player” , a fully accessible music player, as an example of best-practices. We began by marking up the slider using the correct ARIA role, label and slider-specific attributes. Below, a simplified example: There are a few things to note here There exists a “slider” role in the ARIA spec, so we use it here, along with a label; The ARIA valuenow attribute contains the current track position, to two decimal places, with valuemin and valuemax the expected upper and lower limits. In practice, a screen-reader will ignore these in favour of aria-valuetext . As we have only tested on VoiceOver, we include all attributes to ensure maximum compatibility; The aria-valuetext attribute contains the text spoken by the screen-reader as the user interacts with the component. In the real code, we localize and format the values; If the track has a “falsy” duration (i.e. is empty or not yet loaded), we disable the component and set its tabIndex to -1, so the slider is skipped during keyboard navigation. We treat keyboard navigation in the _seekOnKeyDown callback: If the user presses the left or down arrow, we step back about 5 seconds in the current track. Pressing “Page down” jumps back 30 seconds; For right or up, we seek forward a 5 second step; “Page up” moves us 30 seconds forward; Pressing the “Home” button returns the user to the very start of the track. Pressing “End” moves the position to just before the end of the current track. When navigating via the keyboard, a user will have access to the full range of expected functionality. While using a screen-reader, each of these position changes is spoken aloud. You can see a little of this functionality in the following video, again a part of our Hackathon presentation. Similar changes were made by Ahmed to the iOS application and, following our presentation, Deezer decided to make accessibility an official corporate goal, meaning that accessibility will be a considering for all future mobile and web developments here at Deezer. Stories of engineering, data, product and design teams… 5 Accessibility A11y Web Development Music Player Deezer 5 claps 5 Written by R&D Engineer at DataDog Stories of engineering, data, product and design teams building the future of music streaming Written by R&D Engineer at DataDog Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-05-24"},
{"website": "Deezer", "title": "content import en php chez deezer", "author": ["Romain Cottard"], "link": "https://deezer.io/content-import-en-php-chez-deezer-9fdad6968893", "abstract": "Previous stories About Jobs On the occasion of the PHP Tour 2017 -a French event on PHP programming- that takes place in Nantes on May 18–19, here is a post on Content Import that is being handled internally using PHP. Deezer is a partner of the PHP Tour 2017 and several of our developers will attend the event. They will enjoy some of the technical sessions, discuss our projects on our stand and offer attendees to take part in coding challenges. In the meantime, let’s read about content import at Deezer! Let’s talk a bit about history. Deezer was originally Blogmusik.net, a music download platform founded in 2007 by Daniel Marhely and developed entirely in PHP. In 2007, there were only few developers within Deezer’s teams. It was then even more important to share knowledge and have common modules to handle the website, as well as the processing and insertion of data into the database. Therefore managing the import of data in PHP was an undeniable advantage. Also we did not need a developer dedicated to a different stack to process the data. This way, a developer working on the website could also take care of the import part and vice versa. PHP was already very popular so it was always easy to find documentation and answers in case of a problem. One other advantage was that devops had only one technological stack to deploy. A very precious time saver. PHP was not made for pure backend processing. The language is not compiled but rather web-oriented; performance in the case of large data processing, disk access and parallelization is not as good as what we could get from a pure backend and processing oriented language. PHP was not natively multi-threaded. Now there is a “native” library but we are still far from the simple and effective integration that other languages can offer. PHP was and remains very web oriented. As a consequence, there are many libraries to facilitate web development … but far fewer for backend development. Libraries regularly appear according to the needs of their authors but some teams do not have time to undertake the development of these tools. How can we overcome these “drawbacks”? Even if PHP is not multi-threaded, it is possible to launch the same script several times with different arguments in order to process various data in parallel. The only disadvantage compared to multi-threading: memory resources are not shared but it can be solved with a cache system. A script is executed procedurally: if a task is blocked, the script waits for it to be completed then carries on. However, if the result of the script is not needed to continue, it is better to start this processing asynchronously and not block the script execution. We receive an always increasing volume of metadata from our providers so it is vital that we go always faster to process them in order to put new albums live on time. We chose to parallelize the scripts and have a master script that coordinates them all. Its role is crucial, it has to be effective but simple: Check if there are things to do Run one or more scripts (if necessary) with the appropriate parameters to handle the heavy processing In order to facilitate the launch and monitoring of these scripts, we converted them into Daemons. This way it is very easy for devops to manipulate the master scripts. To avoid overloading backend servers, the scripts also need to regulate the load. If there is nothing to do, no subscript is started If the number of elements to be processed increases, the master script launches a number of subscripts in parallel to cope with the workload Last advantages of a master / subscripts system: the memory is freed once the script is finished and we prevent memory leak and crashes. We also set up a “server” script to receive commands to execute through a socket system. The advantage here is to have a “server” on each backend machine and therefore to distribute the load on different machines. The aforementioned command server also allows asynchronous execution. The script that sends the request to the server is only waiting for the message to be accepted by the server. This prevents the blocking of the script execution while triggering a command that will restart a script for an additional processing. Finally, in order to better distribute the load on the various stages of the data processing, we have implemented a relatively simple tail system. Currently, it is used to put the elements to publish from the Import Database to the Shared Database used by other teams and the website. This makes it possible to separate the publication by type of entities to publish (albums, podcasts, radios …) on different scripts (via the system master script described above). Thus we benefit from parallelization and load adaptation. At the moment our queuing system is reserved to publication but the need to trigger tasks asynchronously becomes more and more important and frequent. Also, we are currently experiencing a limitation in the addition of asynchronous tasks. For example, we would like to add an item to the queue that would be processed at a certain date / time, especially to deal with cache desynchronization between Europe and America, which sometimes occurs and is linked to a delay in SQL replication. We will for instance need to update the cache, but only a few seconds or minutes after queuing, not immediately. Finally, we are in the process of completely refactoring our PHP architecture to make it more SOLID while keeping a certain flexibility: injection of dependencies, use of services, creation of internal API to isolate our code and our data, etc. In short, the future is full of promises and complexity to our greatest delight! Parlons un peu d’histoire. Deezer était à l’origine Blogmusik.net, une plateforme de téléchargement de musique fondée en 2007 par Daniel Marhely et développée entièrement en PHP. En 2007, le nombre de développeurs au sein des équipes de Deezer était réduit. Il était donc plus profitable de partager les connaissances et d’avoir des briques communes pour l’affichage du site et le traitement et insertion des données en base de données. Par conséquent, l’import des données en PHP était ici un avantage indéniable. Un autre avantage était de ne pas avoir besoin d’un développeur dédié à une techno différente pour traiter ces données. Ainsi, un développeur du site pouvait également s’occuper de la partie import et vice-versa. PHP étant très populaire, il a toujours été très facile de trouver de la documentation et des réponses en cas de problème. Enfin, cela ne nécessitait pas des devops d’avoir des connaissances pour déployer un environnement pour une techno différente. Un gain de temps très précieux. PHP n’était pas pensé pour du traitement backend pur. Le langage n’étant pas compilé mais plutôt orienté web, les performances pour du traitement de données volumineuses, accès disques et parallélisation sont moindres qu’un langage orienté pur backend et traitement. PHP n’était pas nativement multi-threadé. Il existe aujourd’hui une librairie “native” mais on est encore loin d’une intégration simple et efficace comme avec d’autres langages informatiques. PHP était et reste très orienté Web. Par conséquent, on trouve de nombreuses librairies pour faciliter le développement Web… mais beaucoup moins pour du développement backend. Certaines librairies apparaissent régulièrement en fonction des besoins de leurs auteurs mais certaines équipes n’ont pas de temps à allouer au développement de ces outils. Quelles sont les pistes pour répondre à ces “inconvénients” ? En effet, même si PHP n’est pas multi-threadé, il est tout à fait possible de lancer plusieurs fois un même script avec des arguments différents pour traiter en parallèle des données différentes. Seul désavantage par rapport à du multi-threading : le non-partage des ressources mémoires, que l’on peut résoudre par un système de cache. Un script est exécuté de manière procédurale : si un élément est bloquant, le script attend que cet élément soit terminé pour continuer. Cependant, si le script n’a pas un besoin impératif du résultat du traitement, autant lancer ce traitement de manière asynchrone et ne pas bloquer l’exécution du script. Confronté à ce problème de traitement d’un grand nombre de livraisons de méta-data par les maisons de disque, et afin de garantir qu’un nouvel album arrive bien en temps et en heure sur Deezer, il est vital que nous puissions traiter au plus vite ce volume en constante augmentation. La solution retenue a été de paralléliser les scripts. Nous fonctionnons donc avec un script master, qui ne fait que lancer les scripts à paralléliser. Son rôle étant crucial, il est important qu’il soit efficace, même si limité en terme de technicité : Vérifier s’il y a des choses à faire Lancer un ou plusieurs scripts (si nécessaire) avec les paramètres appropriés, chargés de faire les traitements lourds. Afin de faciliter le lancement et le monitoring de ces scripts, une surcouche Bash Linux est implémentée afin d’en faire des Daemons . De cette façon, il est très aisé pour les devops de manipuler ces scripts masters. Pour éviter une surcharge des serveurs backend, ces scripts doivent aussi réguler la charge. S’il n’y a rien à faire, aucun sous-script n’est lancé. Si le nombre d’éléments à traiter augmente, le master script lance en parallèle un certain nombre de sous-scripts pour faire face à la charge de travail. Derniers avantages d’un système master / sous-scripts : la mémoire est libérée une fois le script terminé et donc on limite la fuite mémoire et le plantage du script. Nous avons également mis en place un script “serveur” chargé de recevoir des commandes à exécuter à travers un système de sockets. L’avantage est de pouvoir avoir un “serveur” par machine backend et donc de répartir la charge sur différentes machines. Le serveur de commande mentionné précédemment permet aussi d’avoir une exécution asynchrone. Le script qui envoie la demande au serveur n’attend que l’acceptation du message par le serveur. Cela permet donc de ne pas bloquer l’exécution du script, tout en déclenchant une commande qui va relancer un script, pour un traitement annexe. Enfin, afin de mieux répartir la charge sur les différentes étapes du traitement des données importées chez Deezer, nous avons mis en place un système de queue relativement simple. Actuellement, il nous sert à mettre des éléments à “publier” de la base de données d’import vers la base de données accessible au reste des équipes & du site. Cela permet de séparer la publication par type d’entités à publier (albums, podcasts, radios…) sur des scripts différents (via le système master script décrit précédemment). Nous bénéficions ainsi de la parallélisation et de l’adaptation de charge. Pour le moment, notre système de queue est réservé à la publication mais nous avons de plus en plus besoin de déclencher certaines tâches de manière asynchrone, et non uniquement sur la publication. De plus, nous rencontrons actuellement une limitation sur l’ajout de tâches asynchrones. Nous souhaiterions par exemple ajouter un élément dans la file d’attente mais pour un traitement à partir d’une certaine date / heure; ceci notamment pour faire face à un souci de désynchronisation de cache entre l’Europe et l’Amérique qui se produit parfois et est lié à un retard de réplication SQL. Nous devrons par exemple effectuer une mise à jour du cache, mais seulement quelques secondes ou minutes après la mise en queue, pas immédiatement. Enfin nous sommes en train de revoir entièrement notre architecture PHP pour la rendre plus SOLID , tout en gardant une certaine souplesse : injection de dépendance, utilisation de services, création d’une API en interne pour isoler notre code et nos données, etc. Bref, l’avenir s’annonce plein de promesses et de complexité, pour notre plus grand bonheur ! Stories of engineering, data, product and design teams… 6 PHP Php Developers Backend Data Database 6 claps 6 Written by Stories of engineering, data, product and design teams building the future of music streaming Written by Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-12-01"},
{"website": "Deezer", "title": "some fun with deezer data and elastic graph", "author": ["Aurélien Saint Requier"], "link": "https://deezer.io/some-fun-with-deezer-data-and-elastic-graph-afbdf776d1a2", "abstract": "Previous stories About Jobs A year ago the Elastic team released the Graph product, whose promise is to offer: a new way to discover, and explore, the relationships that live in your data. Here we will describe how Graph could be used on Deezer data to rebuild our “Similar Artists” feature. What do we need ? Although most Elastic products are freely available, you need a license to use the Graph product. You can opt for the Elastic Cloud free trial. Sign up here and you are ready to start! You should now provision a single cluster with 1GB memory and 16GB storage. Next you need to configure a few settings in the ‘Configuration’ section to enable Kibana. Shield configuration is mandatory, otherwise you cannot start a cluster. To get one, go to the ‘Security Editor’ section and reset the password. Then go to Kibana to create a new user with new roles. Once you complete these steps, go to Kopf — a web admin tool for Elasticsearch, check the ‘Overview’ section and observe the green cluster (see below). Now that our tools are configured, we need some data to play with. You can find the data and script used to write this article on this GitHub repository. In order to build relations between artists, we rely on the following hypothesis : artists have a connection if the same user listens to them . Relevant connections will appear with a sufficiently large number of users. Our dataset is composed of the ‘listened to artist’ profile of our users. Here are some data examples: A small, anonymous dataset is provided in order to allow you to play with data in Elastic Graph. The dz_id field has been removed from the data to preserve anonymity. It’s now time to push our data to Elasticsearch. Even though Elasticsearch allows data indexing without setting a mapping, it is not advised. Indeed without a mapping, Elasticsearch will analyze string fields using a Standard analyzer, which could have negative effects on our project. You can set the following settings and mapping for the dz-music index by using Sense — a browser plugin to send REST requests: This mapping creates the users type in the dz-music index. Note that we do not analyze the artist_names field. You can now use the bulk API to index the data sample. Here is your new indice displayed in Kopf: With our data indexed in Elasticsearch, we can now create a visualization for them. Given that Kibana is not the subject of this post, I will just explain how to configure it and how to create a visualization. First you need to configure the index pattern. Go to the ‘Indices’ tab in the ‘Settings’ page. Untick the Index contains time-based events box and enter dz-music in the Index name or pattern text field. We want to create a visualization of the 10 most frequent artists in user profiles. So go to the ‘Visualize’ tab, choose the ‘Vertical bar’ chart and select the dz-music index pattern as a new search source. Select the X-Axis as buckets and then select a Terms aggregation. Choose the artist_names field and set the size to 10. Click on the green arrow to apply changes. You should obtain a chart similar to this one (top artists depend on the sample data though). It is finally time to play with Elastic Graph. It is a Kibana plugin that allows us to display a graph of our data. When you open the Graph application (see the screenshot above), getting started is as easy as counting to 3: 1. Select the dz-music index 2. Select the artist_names field that contains the artist names we want to graph 3. Configure the Max terms per hop (set to 15 for the example) parameter and search for an artist name And voila! Here are some graph visualizations showing links between artists in various music genres: This post is just an introduction to Elastic Graph and how to try it for free. If you are interested in going deeper into Elastic Graph, you can read the official Elastic Graph guide here . Stories of engineering, data, product and design teams… 7 2 Elasticsearch AWS Deezer Data Science 7 claps 7 2 Written by Stories of engineering, data, product and design teams building the future of music streaming Written by Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-04-26"},
{"website": "Deezer", "title": "merging like its 2099", "author": ["Xavier Gouchet"], "link": "https://deezer.io/merging-like-its-2099-7f9b13800cac", "abstract": "Previous stories About Jobs Working in a 19 developers team (and counting) has its perks, but merging isn’t one of them. Anyone who has worked with a big team on a big project has experienced merge conflicts at least once. If you have never seen a list of ‘CONFLICT’ lines when rebasing or merging commits, let me give you some insight. Conflicts happen when two people make changes in the same place in the same files. As a result, a developer must solve the conflict by hand, i.e. hand-pick which modifications should be kept. Resolving conflicts is a tedious task but conflicts are more often than not pretty easy to solve. This problem has bugged me for quite some time now and at first my idea was to write a syntax-aware merge tool. I’m not the first to have had this idea (Ian Clarke talked about it in 2008 and again in 2011 ) and some commercial, proprietary tools seem to handle this for $7 a month. But as interesting as it sounds, building a semantic parser and merger is a really huge task so I took another point of view on the problem. Instead of trying to build a complex solver that would understand and solve all kinds of conflicts, I chose to build a lot of small, simple solvers and a basic tool to chain them. This is how AutoMergeTool was born. The basic process of the AutoMergeTool can be easily described. Whenever a file reveals one or more conflicts during a merge/rebase, AutoMergeTool will launch one solver after the other until all conflicts are solved. In cases where the conflits cannot be resolved by any of the solvers, the tool will open a Visual Editor so the conflict can be solved manually: Once I successfully completed the script, all I had to do was to write the different solvers. For that I had a single rule inspired by the Unix philosophy: “write a solver that does one thing and does it well” . The first solver only dealt with Java import conflicts, which are easy to solve but take a lot of time. The tool suite now has 7 solvers, most of which can be fine tuned. Out of those, only the first one is specific to Java source files; all the others can be used regardless of your preferred programming language. AutoMergeTool is also extensible so that you can write your own solvers for the problems you often experience or that may be specific to your file types. The first time I used it at Deezer, it was for a rebase of a large refactoring, which was around 50 commits behind master . On my first try, there was around sixty conflicts in thirty different files. After building the first draft of AutoMergeTool and running it, I only had to solve 3 conflicts manually; all the others were solved automatically. Since then we have been dogfooding this tool internally within the Deezer Core Android team and have reduced the time spent on merge conflicts, also improving the mood of the developers performing merges or rebases. AutoMergeTool is open sourced (under the Apache Licence 2.0) so any issues , feature requests , pull requests or feedback are welcome. I presented the first public version of AutoMergeTool during AndroidMakers 2017. French speakers can watch the presentation below: Stories of engineering, data, product and design teams… 6 Git Github Android Mobile App Development 6 claps 6 Written by Lead Android Engineer @ Workwell.io, erstwhile teacher. Music addict, irredeemably curious and adept of pogonotomy Stories of engineering, data, product and design teams building the future of music streaming Written by Lead Android Engineer @ Workwell.io, erstwhile teacher. Music addict, irredeemably curious and adept of pogonotomy Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-05-11"},
{"website": "Deezer", "title": "focus and contrast", "author": ["Cormac Flynn"], "link": "https://deezer.io/focus-and-contrast-c84e1619cf0b", "abstract": "Previous stories About Jobs In the last article , we looked at how to manage focus when using the keyboard. By default, the browser will place an outline around the currently active element. Firefox, for example, adds a dotted border; Chrome draws a blue halo. This outline is added irrespective of how the element is given focus — whether via the keyboard or by clicking in the conventional way. Many websites (Deezer included) override this behavior, removing the focus outline entirely in order to maintain a consistent design across browsers. Moreover designers often choose colors purely for aesthetic reasons, without considering how they impact legibility and contrast. Both these design choices impact users with visual disabilities, those with reduced visual acuity (due to age, for example) or those with conditions such as color-blindness. In this article, we’ll examine how to tweak an existing design to give sufficient visual feedback to accommodate keyboard navigation and to ensure our content has enough contrast to be legible to as many users as possible. Accessibility should ideally be something we think about from the start of the design process; as here we’re working with an existing site design that predates our accessibility improvements, the changes outlined below are necessarily conservative and incomplete. However Deezer has committed to making accessibility a company policy that will be a consideration in all future design briefs. In the previous article, we used a Chrome extension called Focus Indicator to help debug the keyboard navigation functionality. Disable this extension and it quickly becomes clear that, despite our improvements, navigating effectively with the keyboard is near impossible without some sort of focus outline or other visual feedback. Ideally, we would simply remove the CSS rules that block the standard focus outline and return the browser to its default behavior. However as accessibility was not part of the original design brief, the look of the site could be aesthetically compromised if we went down this route. Instead we look to reuse, where possible, the visual feedback that occurs when we “rollover” an active element using the mouse pointer — for example, the darker black background that appears when you move your mouse pointer over an item in the left-hand menu on Deezer.com. In most cases, this is simply a matter of adding the focus pseudo class to the CSS rule in question: Other cases require the rule to be moved to the element that can take keyboard focus. For example, consider the following HTML: A CSS selector that uses li:hover would work correctly using a mouse, but adding li:focus would fail as the <li> element can never take keyboard focus. Instead, the hover selector must be switched to the anchor tag. Sometimes we want to apply a style only when navigating with the keyboard, and not when using the mouse. For example, following changes in the last article, closing a panel will return focus to the element in the left-hand menu that opened it. With a :focus CSS selector, the parent menu item will appear selected until focus is moved elsewhere. If we move the mouse pointer over another item while the menu is in this state, more than one menu item will appear selected at once. The solution, as in the last article, was to differentiate programmatically between a close action performed by a keypress versus one invoked by the mouse. So, occasionally, don’t be afraid to implement programmatic workarounds to accommodate the expectations of users. A good tool to check the legibility of your site content is the bookmarklet tota11y . Using this on the current site reveals areas of content that are likely difficult to read for those without full visual acuity. Another useful tool is the Chrome extension ChromeLens , with which you can view the site as it appears to users with a variety of visual disabilities or conditions. Using both these tools, we worked with a designer to improve the color contrast of site headings and sub-headings. In addition, we added more impactful focus styles for search input fields and items in the left-hand menu. These were fast and simple changes that should ensure the majority of text-related contrast and focus issues are resolved. However, problems remain with the player itself: Although our contrast changes improve the legibility of the player’s heading and sub-heading, its icons still suffer from low-contrast and an insufficiently visible “rollover” style. These issues, along with accessibility in general, are now being used to inform an upcoming redesign of the web player. It illustrates how making your web app accessible is an ongoing process, requiring continuous attention, rather than a single set of definitive changes. In the final article in this series, I’ll focus again on the web player, explaining a little about how we made it and related components more accessible during a recent Hackathon at Deezer. Stories of engineering, data, product and design teams… 3 Accessibility Design Deezer A11y Web Development 3 claps 3 Written by R&D Engineer at DataDog Stories of engineering, data, product and design teams building the future of music streaming Written by R&D Engineer at DataDog Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-04-20"},
{"website": "Deezer", "title": "deezer at icassp 2017", "author": ["Manuel Moussallam"], "link": "https://deezer.io/deezer-at-icassp-2017-347cd296bd45", "abstract": "Previous stories About Jobs I went to ICASSP in New Orleans to present a (small) part of the research done at Deezer in the R&D team. It was the opportunity to exchange with the community and gather much valuable knowledge and ideas. Our work was about using Convolutional Neural Networks (CNNs) to solve a lossy codec usage detection problem. I CASSP is one of the largest conference on Signal Processing in the world. Signal Processing is a broad subject; it includes sounds, images, biomedical data, sensors and radars. A key feature of this conference is the mix of theoretical work with more applied research. People from every continent gather to share about their latest research and learn from others. This year it was in New Orleans, cradle of Jazz and arguably one of the best place in the world for lovers of live music. During a full 4 days of talks, over 1300 differents research papers are being exposed, either on 20min oral presentation or on poster sessions. The conference is rather intense; you find yourself running from presentation to presentation to gather as much knowledge as you can. I focused on the topics we’re the most interested in at Deezer R&D: Music Information Retrieval , Deep Learning and dimensionality reduction techniques such as matrix factorization and embedding learning. General Impression The last time I went to ICASSP was five years ago, I was still a phD student then, so this was a great opportunity for me to witness how my field of research has advanced in this time. The two most obvious changes to me were: Hard tasks, such as musical transcription , music source separation or audio classification are being addressed in much more effective ways nowadays and the improvement in quality is very impressive. Deep learning has taken over the signal processing field quite extensively. ICASSP 2012 had zero sessions about it; five years later there were 7 sessions specifically focused on it, 2 tutorials and numerous papers using deep learning techniques — ours for example! The work we presented emerged from a problem that you could only encounter at a place like Deezer. As such, it attracted a lot of interest since the challenge was quite new: Can we detect that an audio signal has been altered by a lossy codec ? Let’s start with the distinction between Lossy and Lossless audio codecs. Deezer audio catalog is delivered to us by rights owners in the FLAC format, which is usually a compressed version of the reference CD file (16-bits encoding, 44Khz sampling rate). Although compressed, FLAC is a lossless codec, which means once decoded, the raw signal is exactly equivalent to the original one. Think of FLAC as a zipped version of a text file that can be unzipped without losing content. Lossy compression, on the other hand, will also compress the audio file but will modify the signal in a non-reversible manner, typically producing a much smaller file. Think of it more as a JPEG version of an image. Audio being a rather big media, compression is actually a huge deal, saving storage space , transmision bandwidth and ultimately energy and money. Among lossy codecs, the most popular are perceptual lossy codecs such as MP3, AAC, Vorbis, WMA, AC3, etc. All of them use the following principles: Firstly, the signal is processed by a filterbank (typically MDCT ) to deal separately with each frequency range. The second step is the quantization process, where the loss occurs. The principle is simple enough: high-precision coefficients will be approximated by numbers with lower definition that are cheaper to encode . The larger the approximation, the cheaper it gets but also the larger the loss of information and signal distortion. Codec efficiency is typically judged in terms of this Bitrate/Distortion ratio . The quantization is informed by a psychoacoustic model to make sure the approximations have a limited impact on the perceived quality. Finally the quantized coefficients are transformed into bitstream for final compression. This operation is fully reversible at the decoder side. Perceptual Lossy codecs are able to compress a signal by saving space in time-frequency areas where the impact on the perceived quality is the lowest possible. What distinguishes the various codecs is how each of the above block are implemented and parameterized. Another critical factor is the output bitrate (expressed in kilobytes per second or kbps ) which will limit the size of the compressed file and therefore the quality of the audio. When referring to a compression scheme we typically specify both the codec family and the bitrate used. For instance you can use MP3@320kpbs, AAC@256kpbs, WMA2@128kbps and so on so forth. If everything goes right (and it usually does) we get FLAC files that are truly unaltered audio. But sometimes, the FLAC itself is just a re-encoding of a signal that has been altered previously. Imagine the following pipeline: An original file A.wav is compressed in MP3 format, which outputs A.mp3 A.mp3 is decoded back to wav. This gives Abis.wav an altered version of A Finally Abis.wav is encoded to Abis.flac and delivered to Deezer Now because of the lossy MP3 compression, Abis.flac is not equivalent to what a direct A.flac would have been. The question is thus: when we receive Abis.flac can we detect that it went through the lossy process at some point before? if so can we do it irrespective of what the codec or bitrate were? T he key to solving the problem is to understand the impact of lossy compression in the Time-Frequency domain . If you are not familiar with Time-Frequency Transforms (TFT) and spectrograms , you’ll find plenty of material online to explain it. I like this video for instance; if you want to skip the math and just grab a graphical idea, jump to 4:26. If it’s still too difficult, check the wikipedia spectrogram page . What is really interesting with spectrograms is that the impact of lossy compression is clearly visible.. at least for a trained eye! See for yourself -below is a picture where we have highlighted the key artifacts typically created by lossy codecs: The picture above exposes three typical artifacts of perceptual lossy codecs: High frequency cuts : human ears are not designed to hear sounds with frequencies above 20Khz and this higher bound tend to decrease as one ages. The quickest gain for a codec is thus to remove the highest frequencies. Breaks between bands: codecs will typically process big TF chunks of audio separately, thus different quantization strategies at different frequency range can produce these discontinuities in the spectrograms. Clusters and Holes : Finer psychoacoustic effects take advantage of the way our ear is able (or more importantly not able) to process a sound when it occurs right after (or before) a powerful event such as a snare hit. This phenomenom goes by the name of pschoacoustical masking . Pretty much in the same manner our eyes have an intrinsic latency by design, our ears also experience some delays in our ability to process sounds. A smart codec will leverage this to gain space in those areas, thus creating holes and isolated clusters in the spectrogram. N ow that we know what to look for, we are going to train a classifier to discriminate between altered and unaltered files. We do so by building a convolutional neural network (CNN). Given the current hype around AI in general and Deep Learning in particular, you’ll have zero difficulty finding resources online about CNNs . The one we built takes spectrograms as input and outputs a binary label: altered or unaltered . We train it by showing it a large number from both sample classes so that it ends up making very few errors, as the table below shows: Our system achieves very good performance scores, higher than all state-of- the-art methods that we found in the scientific literature. When it fails, it is mostly on cases where the decision is difficult because the lossy codec has a very high quality output (typically AAC@320kpbs). At those bitrates, artifacts are very weakly perceptible in spectrograms. Interested readers will find more details in the paper itself , especially on how we build the dataset and additional experiments showing the robustness of this method to codec and sampling rate changes. But to sum it up, it works pretty well provided you carefully design your processing pipeline (spectrogram and neural net) and leverage the access to a sufficiently large amount of training data. I CASSP was great. The community as a whole is embracing deep learning techniques to achieved performance jumps on hard, challenging tasks. Through our participation, we saw that our work at Deezer R&D is pretty much in sync with what’s happening out there. We’re confident that we’ll be able to contribute even more in the future. Aside from that, I suspected New Orleans was great , but it’s actually awesome. I just didn’t know how much one could miss it, even so I could have guessed from what the great Louis has been singing . Stories of engineering, data, product and design teams… 7 Thanks to Romain Lods . Music Tech Audio Compression Machine Learning Deezer 7 claps 7 Written by Deezer Research. Audio Signal Processing and Machine learning for Music. Stories of engineering, data, product and design teams building the future of music streaming Written by Deezer Research. Audio Signal Processing and Machine learning for Music. Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-04-10"},
{"website": "Deezer", "title": "deezer introduces search by lyric", "author": ["Aurélien Saint Requier"], "link": "https://deezer.io/deezer-introduces-search-by-lyric-e25f7d0d4d8", "abstract": "Previous stories About Jobs Two years ago, Deezer introduced the ability to display lyrics while listening to a song. Today, almost 6 million tracks are available with lyrics. In the Deezer tech team, we thought it would be really convenient to allow tracks to be searched by lyric query. Who has never been stuck with lyrics in their head and unable to find the song they belong to? So we introduced the lyric search feature… When we talk about search, we’re talking about both the search query itself and the index of search terms. First, we build an index based on the open source search engine Elasticsearch (indeed the entire Deezer search engine relies on it). We index JSON documents representing track lyrics like this: So we have four fields: id : ID of the track text : lyric text of the track global_rank : popularity of the track product_track_id : a field that allows us to regroup the same audio track behind a unique ID Our index contains around 5,700,000 tracks for an index size inferior to 3GB. Let’s talk about the search query now. Commonly, searching for music involves the querying of short text metadata. The frequency of the search term in the metadata text, typically measured by a tf-idf score, is not particularly important; we only care if the search query is there or not. Querying lyrics text however is more akin to a traditional search on a text document where the frequency of the query must be taken into account. In order to compute a score that reflects the matching between the user query and the track lyrics, we use the ElasticSearch Function Score Query . Another thing when searching in textual document is to deal with stop words. The basic approach is to remove stop words during the indexing phase. The problem with this approach is that, while stop words normally have a small impact on relevance, they are important in the specific case of music lyrics. If we remove stop words, we are unable to distinguish between “happy” and “not happy”. Moreover, stop words are language dependent. An alternative to stop words is provided by the Elasticsearch Common Terms Query . To briefly describe the process, the common terms query divide terms into two groups; more important (i.e. low frequency terms) and less important (i.e. high frequency terms). Finally, our complete query pattern is the following: This query pattern works well in most cases. However sometimes this query pattern doesn’t retrieve the expected track in the top position. This is because we consider the query as a group of words instead of a phrase. Searching for “ Look at your children” is equivalent to search for “ Children look at your ”. So, in a future improvement, we plan to use a match phrase query to resolve this issue but we need to be careful on the query response time given the results of this study . The first option is to search for tracks by lyrics directly on Deezer. In order to do that, you need to use the hidden advanced search feature. So go to Deezer and search for a query with the following syntax: lyrics:“oh yeah yeah oh yeah yeah yeah yeah” . The second, more user-friendly option is to go to the dedicated Deezer search by lyrics website: www.deezbylyrics.fr . It provides you with a light search interface and a Deezer player that allows you to listen to the tracks you were looking for. Interested in joining our teams? Discover all our open positions on jobs.deezer.com . Stories of engineering, data, product and design teams… 15 1 Elasticsearch Search Lyrics Music Deezer 15 claps 15 1 Written by Stories of engineering, data, product and design teams building the future of music streaming Written by Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-04-03"},
{"website": "Deezer", "title": "lets meet at android makers 2017", "author": ["Romain Lods"], "link": "https://deezer.io/lets-meet-at-android-makers-2017-57403f6e809e", "abstract": "Previous stories About Jobs Next week, Deezer will be partner of Android Makers 2017. Several of our Android engineers will join the event to enjoy the technical sessions, workshops, and debates, learn from others and discuss about our mobile technical challenges and opportunities. Monday morning, Xavier Gouchet , Senior Android Developer & Architect at Deezer, will give a talk entitled “Merge like it’s 2099” . Xavier focuses on the core architecture of the apps and developer tools. His talk will be a proposal to resolve the tedious talk of resolving conflicts, merge smarter and not harder. Here are some of presentations, Deezer Engineers have given in the past during Android Conferences and Meetups. And then my phone became smarter — A journey into the Awareness API by Syrine Trabelsi and François Blavoet Android VIPER — MVVM Hybrid Architecture by Marius Constantin Let’s sprinkle some #PerfMatters on our ViewGroups by François Blavoet Aspect Oriented Programming — Add magic to your code and remove Boilerplate by Xavier Gouchet You can follow some of the technical events we host on our dedicated DeezerTech meetup group, and discover current Career Opportunities within our Engineering teams on jobs.deezer.com . Stories of engineering, data, product and design teams… 11 Android Software Development Android App Development 11 claps 11 Written by Tech, Innovation and Music Lover | Working at @Deezer @DeezerDevs @DeezerFrance. Stories of engineering, data, product and design teams building the future of music streaming Written by Tech, Innovation and Music Lover | Working at @Deezer @DeezerDevs @DeezerFrance. Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-04-07"},
{"website": "Deezer", "title": "is music personal", "author": ["Julie Knibbe"], "link": "https://deezer.io/is-music-personal-315106ec0179", "abstract": "Previous stories About Jobs Every time I interview product managers who are willing to join the Deezer team and ask about a feature they would love to build, they would invariably design a social feature. Yet, our user base is not exactly craving to scrobble their listening history to Facebook. Most users would even ask for a private mode to make sure that some sessions don’t affect their recommendations or even worse, be shared with other users. (Yep, that “Barbie Girl” moment might better not be shared with everyone). We recently polled users, using the Kano framework, to evaluate their interest in sharing music from Deezer on social networks. I expected that social media sharing wouldn’t be among the most appealing features, but the results were clear cut. Social media sharing is leaving our users indifferent. And yet, music is an important part of most of our social gatherings. “Whenever humans come together for any reason, music is there: weddings, funerals, graduation from college, men marching off to war, stadium sporting events, a night on the town, prayer, a romantic dinner, mothers rocking their infants to sleep … music is a part of the fabric of everyday life.” Daniel J. Levitin, This Is Your Brain on Music Is music that personal and private? Technology enabled music to become more private. Headphones changed the game during the 20th century . Sound and especially music could be heard by you and only you. But the real breakthrough happened in 1979, when Sony invented the walkman with portable headphones. “The Sony that broke the sound barrier”. Indeed, you could listen to music privately almost anywhere. A music library is a collection of memories. Music is among the top things we like to collect. Most of us feel the need to have a place to store all the music we love, be it a shelf with stacks of Vinyls or CDs, or piles of favorite tracks on Deezer. Why collecting music? As Daniel Levitin mentioned above, music is present in so many key moments of our lives. Collecting music is not only about the sounds we like, it’s about memories and how these songs make us feel. A quick look at my music library on Deezer reveals “Sailing to Philadelphia” by Mark Knopfler, which reminds me of the first time I’ve been in Paris, and “Hey Boy Hey Girl” by The Chemical Brothers, which reminds me of the first Microsoft event I’ve attended. No one browsing my music library would see any logic but me. (On a side note: this is why music recommendation can be so tricky.) Hits and mainstream music. “Despite the availability of entertainment specially tailored for each individual, people still crave experiences they can share with others. What they want most is what everyone else wants.” Winner takes all, Special Report on Mass Entertainment, Gady Epstein, The Economist, February 2017 Charts are among the most successful discovery features. It’s not only due to a lack of time to explore the long tail, nor the fear of missing out, that is driving people to listen to hits; it’s also because social proof and familiarity are quite reassuring when it comes to uncover something new. Making music. Producing music most often requires playing in tune with other human beings. Music is nothing more than the sound result of a team of several artists playing various instruments. DJ could be an exception, but still, even DJs often don’t work by themselves. I stopped counting the number of David Guetta or Robin Schultz’s featurings. Using music to communicate. Remember that good old mixtape? Music can be used to express emotions, sometimes way better than just words. There are even doubts regarding what emerged first in our civilizations as a way to communicate: singing or speaking? Is there even a difference? Foreign friends told me that hearing me speaking French was like me singing a song to them. (In case you wonder, I’m terrible at singing.) Experiencing live music. Nowadays, with millions of songs available online, what is the point of going to a concert apart from living the experience, watching the DJ or the band, and feeling the music with other people? Growing revenues from live concert tickets sales suggest that people crave those shared experiences now more than ever. “The concert industry had another good year as fans continue to buy increasingly expensive live event tickets while artists at every level became even more reliant on concert revenues as their primary source of income.” Pollstar’s 2015 Year End Stats & Analysis Feeling part of a community. Listening to a specific music genre can make people feel part of a virtual community, which bridges geographical boundaries. Hip Hop or Metal are good examples where music blends with many other codes like hair styling, dressing, etc. Music is a way to define an identity. We’ve witnessed many attempts to master the social experience of music online fail. Twitter Music is one example: the music app never got much traction and shut down pretty quickly 6 months after launch . That is the irony: Twitter Music failed while artists like Katy Perry and Justin Bieber are among accounts followed the most. Following these artists is following a live documentary of their lives. Youtube, Facebook, Twitter, Snapchat… remove music from these platforms and their service would end up differently. Music is part of both our shared and private sphere. What we listen to at a specific time depends very much on where we are, who we are with, how we feel… That’s why it is so difficult to get a digital experience right. To get that digital experience right, the key is to go back to product management basics and understand users’ context to create tailored experiences, personal or social, depending on their intent. Although it’s been a challenge in the past, much progress is now possible: Connectivity will facilitate retrieving more accurate contextual data, Artificial Intelligence will enable adapting experiences to context in real time, Virtual Reality, by enabling users to escape reality, will even go a step further by immersing them in a whole new context Whether a single service can be a jack of all trades and serve all needs is an entirely other question. Stories of engineering, data, product and design teams… 17 1 Music Startup Product Management Deezer Community 17 claps 17 1 Written by CPO @Soundcharts, ex Product Strategy @Deezer @Microsoft Stories of engineering, data, product and design teams building the future of music streaming Written by CPO @Soundcharts, ex Product Strategy @Deezer @Microsoft Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-03-09"},
{"website": "Deezer", "title": "transversal agile scaling", "author": ["Mehdi Haizoum"], "link": "https://deezer.io/transversal-agile-scaling-b5c60b61ba10", "abstract": "Previous stories About Jobs Deezer is an audio company. Our goal is to fill your life with sound. Pretty ambitious, right? If we want to achieve it — to quote Carl Anderson — “it’s about processing work in a way that balances the company’s competing needs and requirements in order to deliver the best final value to the customer.” In other words, understanding and adaption are key elements to moving forward towards a relevant and top-notch experience. The main question is: how do we get to it? Alignment. We’ve been working to provide an in-depth vision of Deezer’s strategy to every individual in the company. The big plus is that everyone is now contributing towards it. The purpose is to get all teams on board around a shared vision. It’s obviously easier to engage with motivated co-workers than to coerce skeptical ones. A master plan It all starts with the top management taking part in strategic planning. Executives come up with initiatives, i.e. the master plan to achieve their corporate mission. Nothing’s set in stone and this plan may change to meet the evolving market demand. This allows us to prepare each quarter and share our plans for Deezer to the whole team. We use a simple matrix to make quick decisions about what we should focus on: The team evaluates each initiative weighing implementation and development constraints against its impact and added value. The impact/effort analysis allows arbitration: multiple options are prioritized thanks to rational reasons. However, remember things can change and we may have to adapt. Even if we’re planning three months ahead, we’re constantly challenging the matrix to do the right thing at the right time. Team level Teams self-manage and self-organize to deliver. It’s all about collegial work. Product owners provide input matching both the product vision and roadmaps while developers help estimate stories, identify spikes, dependencies and any missing story. Eventually these initiatives are sliced into versions with defined release dates. This part of the work is done as early as possible to make everyone’s workflow smoother. At Deezer we want to start building each product in a tangible manner. In « Making sense of MVP » Henri Kniberg elaborates on what should be a truly iterative and incremental development in opposition to a « Big Bang Delivery », meaning this… instead of… Here the aim is to focus on users’ expectations and needs. It’s a try and learn approach: test something smaller than expected, gather feedback, then improve your product until you achieve a cool result. Remember there are three stages of maturity to every product: - Testable : grab it and play with it knowing there’s still a lot of room for improvement. - Usable : the service offered is satisfying and really matches your needs, you can consider using it daily. - Lovable : awesome, it’s a game changer. A goal oriented roadmap We mix the MVP approach with the Go Product Roadmap of Roman Pitchler. It provides a straightforward way of visualizing each increment with key details such as goals, scopes and expectations or KPIs. It has the advantage of being a meaningful and easy-to-read medium for anyone. We talked about vision, product definition and execution. But how do we get there? In 2017 we wanted to empower teams and remind them that building Deezer’s products and features concerns everyone. That’s why we booked a day and a half to gather everyone — developers, product teams, designers, executives, QA analysts — and threw a quarter planning, aka PI Planning in SAFe. The event allowed us to create a link between the three main levels of planning (we do not consider the Value Stream here) in SAFe: Portfolio, embodied by strategic intents and vision, Program, the vision is sliced into projects, Teams, they are self-organizing (Scrum, XP, Kanban…) and have a clean and prioritized backlog. ‘ We’re in this together ’ — Nine Inch Nails As mentioned before, alignment was the #1 objective of our quarter planning. We talked about vision, product and features, teams and management. It is important to hold such an event to catalyze team work, align and create movement before the quarter actually starts. The white paper Hubert Smits wrote, « Five levels of Agile », helps us understand what are the inputs and outputs to tackle a quarter planning in the best possible way. Inputs Product vision is where we want to be in the future in terms of features and as differentiators, in an elevator pitch way in order to keep it simple (and not too detailed if we need to adapt to a market change later for instance). GO Product Roadmap exposes a deeper vision with product managers & owners identifying requirements, drawing a first timeline and prioritizing features according to their complexity. Outputs Release plan: now that we know in which direction we’re heading, it’s time to define each feature release, i.e. scope and release date. We mitigate potential risks and dependencies that may block the teams concerned. Sprint plan: the product owner sets the highest priorities in terms of stories while the development team asks any questions they may have. The two main outputs consist of defining a sprint goal and building a clear sprint backlog. The goal is to drive away any possible ambiguity. Daily commitment and self-organization are key. Enhanced cooperation and communication within teams, notably through daily meetings, are necessary to reach the objectives set in sprint plannings. One last thing: it really is a no brainer to share, build products and get everyone’s curiosity, feedback and involvement. Now that you had an overview of the way we build things at portfolio, program and team level, we’ll conclude by highlighting that communication and coordination are catalysts of a great workflow. We have not reached perfection yet but we want everyone to work with a lean approach. There’s always room for improvement, the main pitfall is to be stuck and remain passive. We’re all learning, why don’t we share best practices? Stories of engineering, data, product and design teams… 10 Product Management Agile Lean MVP Organization 10 claps 10 Written by Agile Coach @ Deezer Stories of engineering, data, product and design teams building the future of music streaming Written by Agile Coach @ Deezer Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-03-01"},
{"website": "Deezer", "title": "web streams enter the warp zone", "author": ["Nicolas Le Gall"], "link": "https://deezer.io/web-streams-enter-the-warp-zone-6cfd79662d83", "abstract": "Previous stories About Jobs We recently rolled out a new HTML5 player on Chrome and Safari, with cross-browser compatibility on the way. So long Flash and thanks for the good work! For this transition, we decided to give Web Streams a try; they promised lower memory consumption, faster launch times for tracks and improved code readability. Just like Node.js’ stream API or Unix’s standard streams , Web Streams standardize the flow of data. The obvious example is fetching a big file on the network; wouldn’t it be nice if you could start working with bytes without waiting for the whole file to download? Well, say no more: Web Streams 🎉 ! Web Streams are useful if you have to handle a considerable amount of data, whether or not the data is sent over a network. I used the word “considerable” because it may depend on your context; on mobile for example, where CPU or bandwidth is limited, you may have to throttle the amount of data. “Handle” is another word whose meaning depends on your needs; to handle data is to transform it or to write it somewhere, even to the screen. The Web Streams specification is described as a “living standard” which means it is continuously updated as the working group receives feedback (just like Fetch but I’m sure you already use it). Only Chrome implements the standard for the moment, and then only a small subset — but you should start using it in your code right now and send feedback! You can also try the polyfill , based on a reference implementation. Enough of this introduction, I am sure you are craving for some code. Let’s try fetching a file and reading it incrementally. Firstly we need a ReadableStream (our data source): ReadableStream s are constructed by passing an underlying source object which will handle data retrieval, described as follows: The start method is called on construction and is used to initialize the data source. The pull method is called whenever the internal queue is empty and needs to be filled (more on that later). The cancel method is called when a stream is cancelled; you can perform actions to gracefully release access. We’ll take a look at the queuingStrategy argument later. Let’s fetch some bytes: Our console should output this: Speaking of result , it is an object described like this: (yep, that looks like an Iterator ) The fetch’s ReadableStream lets you cancel the fetch. Simply call reader.cancel() and the browser will stop downloading 🎊. Boom, another bonus. All this is great but we should do something with our data. One of the more interesting feature of Web Streams is piping. This is a simple way to describe the data flow, for example: Our data source ( ReadableStream ) emits some data which goes through three transformations ( TransformStream — decrypter , decoder and checker ) and finally to a player ( WritableStream ). Isn’t that nice? Let’s have a look at WritableStream (our data destination): WritableStream s are constructed by passing an underlying sink object which will handle data writing, described as follows: The start method is called on construction and is used to initialize the data writer. The write method is called whenever a new chunk of data is ready to be written. The close method is called when a stream is done (all data have been written). The abort method is called when a stream is closed; you can perform clean up of any held resources. Let’s create a destination for our data: We have two more things to do. Our source isn’t sending data through the pipe and it isn’t signalling the end of the stream: And voila , we’ve fetched a URL and streamed its bytes in a player’s buffer 🤘. Well… I lied a little since the WritableStream ’s code is incomplete. The buffer is locked when appending a chunk and it can’t be updated again until its lock is released. Fortunately, we should soon be able to use SourceBuffer.appendStream() . Since fetch is our best friend, I should mention that we will soon be able to use a WritableStream in fetch’s request’s body . The last part of this dream team is chunks transformation: TransformStream s are constructed by passing a transformer object which will handle data transformation, described as follows: The start method is called on construction and is used to initialize the transformer in case you have an asynchronous function to call. The transform method is called whenever a new chunk comes in the pipe. The flush method is called when a stream is closed; you can perform clean up of any held resources. As an example, let’s transform every bytes received: Pretty straightforward isn’t it? As we have seen, Web Streams are really promising. The code is more readable and it is easier to understand the flow of data, thanks to “piping”. Practically, it allowed us to do real streaming; fetching some bytes and using them as soon as possible, leading to lower launch times for tracks (usually users do not like to wait). There are still some concepts to tackle, such as back-pressure, queuing strategies, throttling, cancelling, teeing and probably other made-up “ing” words. Stay tuned! 🎧 Web Streams specification 2016 — the year of web streams , a great introduction by Jake Archibald Platform Status: Chrome, Firefox , Edge, WebKit You can follow some of the tech events we host on our dedicated DeezerTech meet up group and discover our latest career opportunities on jobs.deezer.com . Stories of engineering, data, product and design teams… 3 JavaScript Nodejs Streams Streaming 3 claps 3 Written by Web Engineer @ Deezer Stories of engineering, data, product and design teams building the future of music streaming Written by Web Engineer @ Deezer Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-06-02"},
{"website": "Deezer", "title": "how to read music", "author": ["Cormac Flynn"], "link": "https://deezer.io/how-to-read-music-b7986780ba9f", "abstract": "Previous stories About Jobs Assistive technologies are examples of the practical application of what’s often called the “semantic web” — the judicious use of HTML to describe the meaning or purpose of web content. Recall from the last article how a blind or visually-impaired person uses assistive technology to navigate the web — websites are read aloud by a screen reader from the top of the HTML to the bottom. Lacking visual cues, the user relies instead on auditory ones gleaned from the underlying HTML. In this article, I’ll explain how to write accessible HTML for screen readers using semantic markup. To explain these ideas, I’ll return to the video from last week— a VoiceOver session on the unoptimized website, read by the voice Moira —to explore the problems raised, how to resolve them and to show how VoiceOver performs on a site optimized for accessibility. When we talk about semantic markup, we tend to think immediately of new HTML5 additions like <header> or <section> . Although important, more conventional elements like buttons, links or page heading tags transmit semantic information that is arguably more useful. Screen readers like VoiceOver have a feature called the rotor , available via the shortcut Ctrl — Alt — U , that tabulates page elements like this and permits the user to jump between them. A heading — the tags <h1> to <h6> — marks the beginning of a distinct section of the content, ranked from the most to the least important. The headings on the unoptimized site are symptomatic of tags chosen with visual rather than semantic meaning in mind. The level-5 headings for example act as Thumbnail subtitles; they do not follow a level-4 heading nor give any indication of context to the user. Returning to the video, the first thing VoiceOver encounters should be a top-level <h1> heading containing the word “Deezer”. Instead, Moira tries to speak the site logo, a decorated link that happens to be the first active element in the page: Here’s how the logo should be marked up: We made the following changes: The <header> element encloses the content, providing an additional semantic cue to the screen reader; A <h1> tag surrounds the anchor which itself contains the word “Deezer” marked up with the class sr-only . This CSS class moves the content off-screen, but its order in the HTML keeps it visible to the screen reader. Finally, we add a lang=”en” attribute to the <a href> . This is an attempt to ensure the correct pronunciation of “Deezer” in languages other than English. Sadly, this is not currently used by VoiceOver on MacOS. Note also the attribute role on the <header> tag. This is our first encounter with Accessible Rich Internet Applications (ARIA) attributes, annotations that provide contextual clues and make it easier for screen readers to use interactive content. An ARIA role highlights an important landmark in the page and is available in the VoiceOver rotor landmark list. For the remaining headings in the page, we must decide what the primary content is and how it’s divided into sections, then add semantic headings and eliminate presentational ones to produce a logical flow from most to least important. With this work done, we have the following organization, presenting a cleaner summary of Deezer.com’s content. A screen reader speaks the textual content of a website until it encounters a focusable element such as a form control or link. Both form controls and links are special elements as they are automatically accessible — a <button> or an <a href> take focus and respond to mouse or keyboard interaction with no further work from the developer. If we instead choose a different HTML tag to carry an event listener — a <div> or a <span> for example — we must take special steps so the element is focusable and responds to the keyboard; the same goes for <a> tags without an href attribute. It’s for this reason that properly labeled form controls are recommended for all UI actions, and valid anchor tags to change the URL. Let’s return to the VoiceOver rotor ; here’s the list of form controls for the unoptimized site: The top two items in the list are the next two spoken by Moira in the video — the search field and its button. Here’s how they’re marked up: This looks pretty good — we’ve used the correct ARIA role and have provided a label for the screen reader. When Moira speaks she says “Search, search edit text, button” , reading the label, the input field and the unlabeled button. Let’s look at how this might be improved: Here’s what we did: Enclosed the content in a <form> tag and added a submit button to better communicate the purpose of these controls. We also moved the ARIA role to the parent; a <form> already has its own implicit landmark role and we don’t want to override it; The input field is given a type attribute of value “search” . Again, this is an additional clue for the screen reader. Older browsers will treat a search input as a normal text field; We made the <svg> icon accessible by adding a <title> tag. The ARIA attribute aria-labelledby references the title id to tell the screen reader where to find the label text. Moira will now speak this as “Search, search text field blank, search button” , which is a little better. Continuing down the site menu, VoiceOver reads aloud the following items: “Visited link, home” “Link, Hear this” “Report a bug” “Link, 24x24–000000–80–0–0.jpg My Music” “Link, Favorite Tracks” “Playlists” “One” “Albums” “Apps” #1, #2 and #5 are links and are correctly spoken by VoiceOver. #3, #6, #8 and #9 are spoken as non-interactive text. Each of these opens a panel when clicked and should thus be rendered as buttons. Here’s how the markup looks in the unoptimized site: As the link lacks an href attribute it never takes focus and its contents are treated as purely textual. If an anchor tag was absolutely required, we could force it to act like a focusable button by adding two attributes: role=”button” and tabindex=”0” . However in this case we can simply use a real button: #7 is the content of the “new & updated playlists” badge. As this is a non-critical part of the navigation menu, we add an aria-hidden attribute to make it invisible to the screen reader: #4 is a link that contains an incorrectly marked up <img> tag. An image with a semantic purpose should always have an alt attribute explaining its content; for purely presentational images, an empty alt will make it invisible to the screen reader and that’s what we add in this case. The final set of elements that Moira tries to read are the Player component and Flow. We’ll see in a later article how to make the Player and Flow fully accessible; for the moment, we can improve accessibility by using a combination of labeled <button> tags, the sr-only class and aria-hidden . With these changes done, the form control list in the rotor now looks like this: Despite these improvements, going through the menu every time you want to play a track or to launch Flow can get pretty tedious. There are a few ways we can improve this. Firstly, we can add hidden links just after the site logo, visible only to screen readers, that allow the user to jump to the Player component or to the main content area that begins with the Flow player. Secondly, we can make better use of HTML5 tags and ARIA landmark roles so, with the aid of the VoiceOver rotor, the user can easily skip to the important parts of the site. The most useful HTML5 tags & roles are: <header role=”banner”> <nav role=”navigation”> <section> and <article> . Note that <section> should describe a distinct document section; an <article> should markup a piece of standalone content; role=”region” — we add this to areas of special significance in the page, such as the Player and Flow components, along with an aria-label attribute that describes the region’s purpose; <aside role=”complementary”> — a piece of content ancillary to the page’s overall topic e.g. the comments on an Album page. This gives us the following list of landmark roles: So, how does Moira do on the optimized site? Note how VoiceOver distinguishes between those menu items that change the URL — links — and those that perform actions on the same page — buttons. Note also that the site still has problems. The Player in particular needs some work, and we’ll get to that in a later article. Finally, we talked here about how to improve the web platform in cases where a site is read aloud ; in the next article, we’ll look at functionality that helps all users, the able-bodied and those with both visual and motor disabilities — keyboard navigation. ( Part 3 — Keyboard Navigation — can be found here ) Stories of engineering, data, product and design teams… 8 Accessibility Web Development Music A11y 8 claps 8 Written by R&D Engineer at DataDog Stories of engineering, data, product and design teams building the future of music streaming Written by R&D Engineer at DataDog Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-03-31"},
{"website": "Deezer", "title": "pressing play", "author": ["Cormac Flynn"], "link": "https://deezer.io/pressing-play-2b9771842389", "abstract": "Previous stories About Jobs The keyboard is the primary input device for the visually-impaired, those with motor-disabilities and even so-called “power users” who may find keystrokes faster than using a mouse. The visual layout of a site has no bearing on how easy it is to navigate with a keyboard; what matters is the choice of HTML tags and the sequential order of the content. In this article I’ll explain how keyboard navigation works and share some of the steps we took to improve the keyboard accessibility of the Deezer website. Just like our work with screen readers , these changes improve the site for everyone , disabled or otherwise, and will be further refined as we move to testing with real users. When you click a link or place the cursor in a text field, the element is said to have focus . You can see what’s currently in focus by opening your browser console and executing the command document.activeElement . When a page is loaded for the first time, document.activeElement will be the <body> tag. A user can press the tab key to move the focus from <body> to the next tabbable element in the page. What is considered tabbable — meaning, an element that is keyboard focusable — differs between browsers . In general, form controls and links with href attributes are able to take keyboard focus, along with elements that possess a tabindex attribute with a non-negative value. If no element is in focus, document.activeElement will equal <body> again. The tab order of a website is the sequence in which elements are given focus as the user hits the tab key. This order is determined by the position of the element in the document (earlier take precedence) and the value of the document’s tabindex attributes (higher values take focus first). So the active element that follows <body> will be either the next tabbable element in the document’s HTML or the first with the highest non-zero tabindex value, if such an element exists. The browser will, by default, draw a visible outline around the currently active element —regardless of whether the focus is placed via the mouse or the keyboard. The appearance of this outline differs between browsers. Firefox, for example, adds a subtle dotted border; Chrome displays a blue halo. For this reason, designers often remove the focus outline to ensure a consistent look across all browsers. This makes it impossible for users to determine which element is currently in focus, something that is critical for anyone who relies on keyboard navigation. We’ll examine focus outline and color contrast in a future article; in the meantime, I’m using a Chrome extension called Focus Indicator to add a bright visible outline to the currently active element. One unfortunate exception to these rules is Firefox on MacOS. By default, the MacOS keyboard accessibility settings allow focus to be placed only on form controls and lists. Chrome and Safari manage their own a11y configuration, but Firefox uses the system one, meaning that the user is unable to use tab to place the focus on a link, regardless of the presence of tabindex or href attributes. The user can change these settings by going to System Preferences -> Keyboard -> Shortcuts -> Accessibility and selecting All Controls but there’s currently no way to force this behavior in the HTML. If the active element is a form control or an anchor tag with an href attribute, we can submit the form, follow the link or fire its event handler by pressing the Enter key. In the case of a <button> we also have the option of hitting the space bar. A tag can otherwise be made tabbable by giving it a positive tabindex attribute, but it will only respond to keypresses if the callback associated with its event handler is modified appropriately, as we’ll see later. Custom components such as volume sliders should respond to the directional arrows — “up” or “right” to increase the value, “left” or “down” to decrease it. In addition, WAI guidelines recommend that the “page up” and “page down” keys should increase and decrease the value more quickly, whereas “Home” and “End” should respectively jump to the minimum and maximum values allowed by the component. Modal dialogs, tooltips or contextual menus should allow themselves to be dismissed using the Escape key. This step is easy to reason about but, sometimes, tricky to apply to legacy code. Tab order, just like the order content is spoken by a screen reader, is largely determined by the sequential placement of elements in the HTML. This position is often chosen to more easily allow a CSS float to be applied in order to achieve a visual effect. For example, consider a track “card”: We want to give sufficient context to the user, so our desired tab order is: (1) track title, (2) artist name, (3) play button, (4) favorite icon, (5) context menu icon. However the underlying HTML is ordered as follows: (3) play button, (5) context menu, (4) favorite button, (1) track title, (2) artist title. One possible solution is to keep the HTML as is, but to set tabindex values on each part of the card — a tabindex of 1 on the track title, 2 on the artist name etc. This is a really bad idea. If all other elements on the page have a tabindex of zero or less then the uppermost card’s track title will become the first element that takes focus after the <body> tag. If we adjust all preceding tabindex attributes to compensate then we need to do this every time the card component or page layout changes. For this reason it’s recommended to set tabindex to -1 for something that should never take focus, and to zero otherwise. The best solution seems to be to change the structure of the HTML and to adjust the CSS accordingly. However, like everything in practical web development, a compromise must be found between the perfect solution and what is possible within the time given. We decided instead to use the aria-label attribute to provide additional context to the play button, and to keep the HTML as it is. This was because the card CSS is fundamental to a number of other components, so we felt a less invasive solution was best for the moment. Keyboard accessibility will be a consideration from the start for all future developments. Other parts of the site were less problematic to fix. But, as with accessibility in general, developing for keyboard navigation is a long-term process rather than something that can be corrected in one go. As we’ve already seen, form controls and anchor tags are, for the most part, automatically tabbable and accessible via the keyboard and should be used in preference to other HTML tags. In general we should try to use an <a href> for an action that changes the URL; a <button> should be used otherwise. If a link or form control can’t be used, we should still ensure that the component is focusable and responds to keypresses. For example, consider the following React component and its event handler callback: This component responds only to mouse clicks; moreover it lacks a tabindex and so isn’t in the document tab order. To address this, we make the following changes: The _showAlert callback is invoked by either a click or keypress on the element. If the keypress is anything other than the Enter key, signified by keyCode value 13, we return and allow the event to continue as usual. Without this keyCode check, the user would be unable to press tab to move to the next element in the document as any keypress event would invoke the callback, open the alert box and trap the keyboard focus. This is something to keep in mind if you need to disable the default action of an element using e.preventDefault — always place this after the if statement or you risk blocking the default action of all keypresses. Deezer provides a number of keyboard shortcuts that make it easier to control the Player — open the Deezer home page and hit the h key to see the full list: Some of these shortcuts interfere with the default behavior of the browser when using the keyboard. For example, the space bar plays the current track rather than invoking an action on a button. The Enter, left arrow and right arrow keys are similarly bound to Player actions. To fix this, we simply changed the keyboard shortcuts, adding shift or alt combinations where appropriate. The space bar is something we couldn’t change; users expect it to play the current track, something they’re used to from players like YouTube, Quicktime or VLC. Sometimes it’s necessary to intervene programmatically in the management of focus. For example, consider the “panel” that opens when you click the “cog” symbol in the left-hand menu on Deezer.com. Navigating with the keyboard, we want the component to work as follows: The user tabs to the “cog” icon in the left-hand navigation menu; The user presses the “Enter” key; The settings panel opens and the focus is placed on the first active element in the panel; The user presses tab to move through the setting options in the panel; Once finished, the user presses Esc to close the settings panel; Focus is returned to the “cog” icon and the user can continue to tab through the rest of the main navigation menu. This behavior is impossible to achieve using HTML sequential order and tabindex alone so instead we make the following changes to the Panel component: The event that opens the settings panel contains a reference to the “cog” icon as e.currentElement on the Event object. We pass this to the panel component; The panel container is made tabbable with a zero-value tabindex , acting as the first focusable element for keyboard navigation. A descriptive aria-label is also added, giving an audible reference point to visually-disabled users when the panel is opened; Panels can now be closed using the Esc key. When the user hits Esc, we return focus to the element e.currentElement Let’s take a look at the optimized site with all these changes in place: Note how Focus Indicator highlights the active element with a green border. In the next article, we’ll look at focus outlines, color contrast and how to find a compromise between design constraints and accessibility. Stories of engineering, data, product and design teams… 5 Accessibility Web Development A11y Music 5 claps 5 Written by R&D Engineer at DataDog Stories of engineering, data, product and design teams building the future of music streaming Written by R&D Engineer at DataDog Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-04-05"},
{"website": "Deezer", "title": "build a c application based on deezer native sdk with unity", "author": ["Cyril Picheney"], "link": "https://deezer.io/build-a-c-application-based-on-deezer-native-sdk-with-unity-3bf523dc40d2", "abstract": "Previous stories About Jobs Deezer offers developers various SDK (Software Development Kit) in order to include Deezer music in their applications. The Native SDK is one of these and mainly allows Deezer services like playback and offline synchronization. By integrating this SDK you can address the three major platforms: Linux, MacOS and Windows. Since the release of the Native SDK on March the 30th, Deezer tries to provide more and more sample applications in different languages in order to help developers make their own applications in their favorite one. In that context I was hired to develop the first sample apps and wrappers for some popular languages, so I created two wrappers and an example application, in C# and Python respectively. Here we will focus on the C# sample app and wrapper. After I did my first sample app and wrapper in Python2. The team started to wonder If we could embed the SDK in a game engine such as Unity. This was also a good occasion to build a C# wrapper and allow C# developers to make their own applications. The sample I made features a player interface and allows to load a playlist, toggle shuffle mode and repeat mode, etc. Plus it looks pretty cool: The whole program is written in C# and provides a C# wrapper that can be used for any other C# application with or without Unity. I used DLLImport to convert the SDK’s C-functions into C# code, and Unity3D to make the UI shown above. There are two major flaws in this application: First I couldn’t implement radio support as the tracklist display is based on its length, which does not exist for that kind of content. Second is the scaling to any resolution. Fell free to rearrange the UI elements to fit to your screen if you want to build the app and use it fullscreen. The sample app I provided contains only its own internal logic, so you can perfectly replace it with your own app and use only the C# wrapper. Even with a non Unity project. Note that this sample does not implement the OAuth2 authentication process (this process is required to identify yourself and your application when using the Deezer API). It uses instead an already generated access token for demonstration purpose. For more information about how to generate your access token, read this article . If you do not feel comfortable with this process or just want to test the sample, you can use the given access token. See “How to use this sample” for details. If you just want the C# wrapper tu build your own C# application, you can find it under Assets/Scripts/Wrapper as shown below: First you need the to clone or download the sdk samples repository along with the Native SDK , and of course Unity (the free version will do fine). Then in the Bin subfolder of the SDK, move the lib corresponding to your platform and architecture into the Assets folder of the Unity sample. If you are running on MacOSX , rename it “libdeezer.bundle”, and on Windows “libdeezer.dll”. Note that you can modify the OAuth2 information given the the sample to use it for your own account and not the default free user provided. Go into Assets/Scripts/ApplicationMainScript.cs and modify these data accordingly. If you don’t know OAuth2 process and just want to test the sample, leave the data as it is given. All set. Now do the following: Start Unity Go to File > Open Project Select the folder UnityPlayer After the project is loaded, go to the Project panel Got into the folder Scenes and click Interface.scene Feel free to rearrange the UI elements if they do not fit with your screen resolution Click Play to test the sample, or File > Build and Run to build it and run it like a real app. You can now play around with the app like every other player. You can change the content in the field above the track list. If it is available for the user, the track list will change accordingly. The application itself does not allow to play a radio or flow, as this type of content does not contain a fixed size track list. It is however possible to build your own app that will manage that, as the appropriate functions are available in the wrapper and the Deezer Native SDK. Also note that the sample (and more generally the SDK) should be able to work together with Unity and Hololens, as long as UWP integration will be resolved. Article written by: LEANDRI Aurélien Stories of engineering, data, product and design teams… 1 Unity Csharp 1 clap 1 Written by Stories of engineering, data, product and design teams building the future of music streaming Written by Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-01-17"},
{"website": "Deezer", "title": "accessibility at deezer", "author": ["Cormac Flynn"], "link": "https://deezer.io/accessibility-at-deezer-71462e019bf7", "abstract": "Previous stories About Jobs Like many developers, I have my own checklist I run through when finalizing an application — performance checks, cross-browsing testing, that kind of thing. Accessibility, the act of making a site functional for all users, was usually near the bottom, an optional extra that was nice to have, if I had the time. I understood its importance in an abstract way, but had no concrete knowledge about how a blind or visually-impaired person might actually use a web application. I think this is probably a common experience — if accessibility is addressed at all, it’s usually an afterthought or box-ticking exercise, particularly with a deadline looming. So, given the chance to work on pet projects during the pre-Holiday period at Deezer, I decided to finally dig into the topic to learn how a disabled person might use the web in the real world and how Deezer could be made more usable for everyone. This article, the first in a series, describes why accessibility is important and how we intend to change the Deezer web platform to ensure it’s as easy-to-use as possible for all our users. Accessibility ( a11y for short) describes the process of ensuring that people with disabilities can use your software. A common misunderstanding is that “disability” refers only to blind people. There are in fact a variety of visual, auditory and motor disabilities that make using the web hard, difficulties that may be shared with older people, those with learning problems or even users on low-bandwidth connections. An accessible website improves the user experience for all of these groups. Focusing on improving the accessibility of a web site is often a hard sell. It can be difficult to explain the benefits, especially when the perception exists that the number of impacted users is small. It can be harder again to advise developers when online resources are confusing or contradictory. So what are the arguments in favour of a11y compliance, and how difficult is it to build an accessible application? A hard-headed economic case can easily be made. In the United States, 23.7 million adults report difficulty seeing — or 10% of the population. In the EU27 50 to 75-million have some disability , of which 30-million are blind or partially-sighted . In France alone, 1.7 million report having vision problems . The number of elderly in both regions dwarfs all these figures — it’s projected that by 2025, 20% of the European population will be over 65 . There may be legal requirements, now or in the future. Section 508 in the United States, alongside directive 2016/2102 in the European Union , oblige public bodies to ensure their services are accessible to all users. There is every chance that work to complete the digital market in the EU will include moves to ensure that digital services are accessible to all its citizens. Deezer also has an ethical duty to its users and to society in general; as music distribution moves increasingly to streaming platforms we should make our content available to everybody or risk abandoning all but the able-bodied to legacy media, or to our competitors. Finally, it’s important to note that changes made for disabled users improve the website for everyone. A consistent and clear UI, correctly-labeled form fields and keyboard navigation make the site easier for everybody to use. Providing text-equivalents for media such as images not only helps those on low-bandwidth, but makes our content easier for search engines to index. Improving color contrast and readability assists those who are visually-impaired, but also some elderly people and those who suffer from color blindness. The concrete steps required to make a site accessible are fairly modest, and often things we’re doing already, but have the potential to improve the experience of using Deezer for a diverse cohort of users. There are two primary ways in which a visually-impaired person navigates the web. Blind or severely visually disabled users often use a screen reader , the most well-known of which are JAWS or VoiceOver (the screen reader installed with MacOS). A screen reader will speak aloud the content of a website, from top to bottom, using clues from the HTML to determine the language, and the difference between plain text, images, form fields and other elements that are part of a standard website. It allows blind users to jump between different sections, to and from site navigation and to interact with media and interactive elements using keyboard commands alone — but only if the site is correctly structured. Our current website is not optimized for screen reader navigation; if you’re on a Mac, open Deezer.com in Safari and press cmd — F5 to give VoiceOver a try. Those with some vision, or with motor disabilities, can often browse the web conventionally but are sometimes unable to use a mouse. Such users rely entirely on keyboard navigation. Again, this presupposes that a website has been designed to accommodate exclusive use of the keyboard; Deezer’s support is uneven, at best. There are a number of actions we intend to take to improve the website experience for all users. Some are simple — like changes to the underlying HTML — whereas others are more difficult, such as managing the user’s focus as they navigate through the site via the keyboard. I’ve divided the work into the following steps: Improving Semantic Markup for Screen Readers Simple Keyboard Navigation, Tab Order and Focus Element Outline & Color Contrast Accessible Components & Aria Attributes Making an Accessible Player Each of these will be followed by a blog article explaining the changes and challenges involved. Finally, as an able-bodied person, any testing I do will obviously be insufficient without input from the lived experiences of disabled people. Therefore we hope, ultimately, to test a refreshed Deezer web application with a group of real users to gauge the impact of the changes and to observe how a disabled person actually uses the website. In that sense, the completion of the above steps represents the minimum work required to ensure the site is accessible. In the meantime though, and throughout this series, all feedback and comments are very welcome! ( Part 2 — Semantic Markup and Screen Readers — can be found here ) Stories of engineering, data, product and design teams… 6 Accessibility Design Web Development 6 claps 6 Written by R&D Engineer at DataDog Stories of engineering, data, product and design teams building the future of music streaming Written by R&D Engineer at DataDog Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-03-31"},
{"website": "Deezer", "title": "a new way to look at an artist from lyrics to wordclouds christmas special", "author": ["Margot Lepizzera"], "link": "https://deezer.io/a-new-way-to-look-at-an-artist-from-lyrics-to-wordclouds-christmas-special-56a854cb4e77", "abstract": "Previous stories About Jobs O ne thing we like at Deezer is music. And another thing we are quite fond of is data. So when it comes to doing data stuff on music material, no need to tell you we’re all pretty excited! As you may or may not know, one of the cool things about Deezer product is that you can play some songs with lyrics on (there you go karaoke nerds!). That’s why we thought we could probably give a go to a small and quick lyrics analysis. Lyrics mining and analysis was actually something we had been willing to have fun with for quite a while. So after looking a bit around to document ourselves about the subject, we thought: word clouds! NB: If you’re only interested in the results, go straight to the end of the article! First thing first, we should tell you a bit more about how our lyrics data was structured. It was actually pretty convenient and nice to deal with as we basically had to work with two main columns: the song id one and the lyrics text one. sng_id: the column we use as a key to join our lyrics table to other tables in order to fetch data based on song titles or ids, artists names, etc. depending on what we were interested in (but more on that later). lyrics_text: this column basically contains the lyrics text of each song in a string format. Thanks to the sng_id variable and to the Deezer catalog information, we could have performed lots of different song aggregations. Creating word clouds on a song level would for sure haven’t been really interesting as the number of words would have been too small to create a substantial word cloud. We would probably have faced the same issue doing it on an album level considering the small number of songs there is per album. The music genre level, however, would definitely have been interesting. But considering our data, each music genre would have been a blend of French, German, English, etc. songs. And as most text mining packages work well with English texts, we would have had to translate all the non-English songs to English. Since we didn’t want to distort these songs lyrics by translating them to English, the most efficient and accurate solution to go with seemed to be the artist one : enough songs for most of them, songs of the same language, etc. We then needed to pick an artist we were interested in and for who we had enough songs to analyze. Lots of cool songs, lots of cool lyrics, we thought… The Beatles . So we got the corresponding lyrics texts data and stored it in a variable we called data (oh how original — I know). If you need data to run the script on, just build a small .csv file with lyrics from the artist you’re interested in. A single column containing the lyrics and pandas’ read_csv function should do the job! Pretty basic and classical imports basically! Except maybe for NLTK (Natural Language Toolkit) which, as its name suggests it, will help us dealing with text. It provides several super cool functions such as lemmatizer, tokenizer and much more that we will use during our data cleaning step. The WordCloud and ImageColorGenerator imports are the ones that will allow us to build our word cloud. Both of them were developed by Andreas Mueller . WordCloud is the function that will allow us to create the word cloud (you’ll be able to find the corresponding script right here ) and ImageColorGenerator is the one enabling us to generate the word cloud colors based on the ones of the original image (script right there ). Get the data: done. Import the right packages: done. Next: data cleaning! We first needed to operate some preliminary changes to have clean data to work with. We therefore converted lyrics text to lowercase, decoded the whole thing & removed backslashes that for some reason had appeared during the row[‘lyrics_text’] step. We then defined all_lyrics as the concatenation of all the songs lyrics texts thanks to a for loop because that’s how the input should look like before being inserted in the wordcloud package. Since the wordcloud package we are using is quite comprehensive, it actually contains a lot of functions that allowed us to quickly end up with a quite satisfying word cloud just by applying the wordcloud class on all_lyrics: no need to remove punctuation, stopwords (words such as “I”, “you”, etc.) or to stem the whole thing as the package does most of it on its own. For instance, have a look at the top 10 words of the all_lyrics Beatles data: “The”, “you”, “I”, “to”, etc.: not that interesting huh. Yet, look at the word cloud we would get without further coding (but the one behind the word cloud generation script obviously): Still, if you carefully look at the word cloud, you’ll see that the words aggregation is not optimal yet as there are still some words that do not seem to have been perfectly lemmatized: “said” for instance (located in the “o” of “love”) wasn’t recognized as belonging to the “say” root. This is no biggie, but we could actually consider that this might distort the final results as “said” for instance is not considered the same as “say”, and “waiting” isn’t aggregated along with “wait”. When they actually — well, in our opinion — should. This is precisely why we decided to go a bit further and to investigate in order to find a way to lemmatize those verbs properly and to aggregate our data in the best possible way. Therefore… On to tokenization . For those who don’t know, tokenization is a very common practice in text mining that consists in breaking up a text into words in order to proceed to various operations on tokens that we might not have been able to apply on a string. Tokenizing all_lyrics then allowed us to properly start the lemmatizing step and therefore to remove verbs terminations we talked about at the beginning of this part. As a reminder, lemmatization is the process of retrieving a word canonical form thanks to algorithmic processes. If you look at the word cloud package comments , you’ll see that the only text normalization applied there, is a plural normalization, meaning the algorithm only remove plural “s” to words. What bothered us was mainly verbs termination (and not only third person “s”), so we tried several lemmatizers with the aim of finding one that would remove “ing”, “ed” from verbs and basically retrieve the verb most basic form. Among Porter, Lancaster Stemmers and much more, WordNetLemmatizer appeared to be the most satisfying one. We basically characterized every single token as a verb and then let our WordNetLemmatizer do the rest, meaning remove every verb termination. “belongs” became “belong”, “gone” went “go”, “feeling” became “feel” (we deliberately chose to transform potential “ing” nouns in verbs), “changed” became “change”, and so on. Whether you set the normalize_plural parameter to “True” or “False” during the word cloud generation afterward does not change anything since all the plural “s” should have already been removed thanks to this lemmatizer. If you go back to the list of tokens that was previously screenshotted, you might note that tokenizing all_lyrics led to two main issues that we faced in two different ways: Words such as “i’m”, “you’re” or “can’t” were weirdly tokenized and were cut like “i” and “‘m”, “you” and “‘re” or “ca” and “n’t” thus leading these pieces of words to be inserted in the word cloud since they were not present in this shape in the stoplist. We therefore thought about removing every 1 or 2 letters words . This, however, meant we had to give up on words like “oh”, “ah”, and so on. But a choice had to made… And that’s the story of how we said goodbye to “n’t”, “‘ll”, and this other type of folks. Another problem we had following all_lyrics tokenization was related to English slang: “gonna”, “wanna”, etc. had been tokenized as “gon” and “na” and then rejoined as “gon na”. As these kind of words are not that many we decided to simply manually replace them with the right corresponding stem . Et voilà! As the word cloud package removes punctuation and stopwords on its own, we then only needed to apply the wordcloud class to lyrics_abrev to get our word cloud! Hold on, upcoming steps are the coolest. You first need to define what your word cloud mask will look like (the shape and color your word cloud will take basically). To do so, we created two files: the “music genre” one containing generic music genre related pictures and another one (the “other images” file) with more artist related pictures. If you need to, you can use the small mask repository we created on GitHub . Then, you just need to adjust your word cloud parameters : its width, its height, the margin, the font, the maximum and the minimum font sizes, the background color, and many more (if you’re interested in the comprehensive parameters list, have a look at the wordcloud.py script on A. Mueller’s GitHub ). And… it’s finally time to generate your word cloud from your last lyrics variable. We went for the colorful version and, therefore, also used the ImageColorGenerator we talked about earlier. Eventually, we used matplotlib to display the word cloud & defined a path to store all of our brand new word clouds. I finally tested the script on several artists that I globally grouped by music genre, and here are the results. Enjoy! So that’s about it for us! We’d love to get your feedbacks about the methods we used, the word clouds, and so on, so feel absolutely free to comment or to get in touch with us if you have any suggestions, questions or advice! Stories of engineering, data, product and design teams… 12 12 claps 12 Written by Data Storyteller @Deezer Stories of engineering, data, product and design teams building the future of music streaming Written by Data Storyteller @Deezer Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-12-25"},
{"website": "Deezer", "title": "build a python2 based application using deezer native sdk", "author": ["Cyril Picheney"], "link": "https://deezer.io/build-a-python2-based-application-using-deezer-native-sdk-a80e32079984", "abstract": "Previous stories About Jobs Deezer offers developers various SDK (Software Development Kit) in order to include Deezer music in their applications. The Native SDK is one of these and mainly allows Deezer services like playback and offline synchronization. By integrating this SDK you can address the three major platforms: Linux, MacOS and Windows. Since the release of the Native SDK on March the 30th, Deezer tries to provide more and more sample applications in different languages in order to help developers make their own applications in their favorite one. In that context I was hired to develop the first sample apps and wrappers for some popular languages, so I created two wrappers and an example application, in C# and Python respectively. Here we will focus on the Python2 sample app and wrapper. As the SDK is originally written in C/C++, I managed to develop a python2 wrapper in order to allow developers to make their own Python application using the NativeSDK. The Python2 sample is a console-based application . It calls the SDK functions via a wrapper that uses CTypes to translate C functions from the SDK into Python2 functions. I chose CTypes mostly because I was used to it and because I didn’t need any advanced wrapping functionalities, just to translate a bunch of C types and some simple functions. Although the structure pointers translation was pretty rough I managed to translate the major SDK functions into Python definitions. The wrapper itself can be used with any application you created. You can delete the whole sample app and replace it with your own. This sample does not implement the OAuth2 authentication process (this process is required to identify yourself and your application when using the Deezer API). It uses instead an already generated access token for demonstration purpose. For more information about how to generate your access token, read this article . If you do not feel comfortable with this process or just want to test the sample, you can use the given access token (see below). You first need to download the NativeSDK here , then clone the sample code repository on GitHub there . Place the SDK folder from the NativeSDK folder into the root of the repository as shown below: Open myDeezerApp.py with your favorite text editor. Here change the authentication info with your personal OAuth info. Leave the original values for a sample free user. Run app_start.py with Python2. For example, in order to play the album 11375984 (Zanaka by Jain): That’s it! Follow the instruction on the output in order to use the player. Here are the basic keystrokes: You can of then reuse the python wrapper for your own python application and use this sample as an inspiration. Article written by: LEANDRI Aurélien Stories of engineering, data, product and design teams… 5 1 Python Music 5 claps 5 1 Written by Stories of engineering, data, product and design teams building the future of music streaming Written by Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-01-17"},
{"website": "Deezer", "title": "do we need a vision", "author": ["Julie Knibbe"], "link": "https://deezer.io/do-we-need-a-vision-847592dad002", "abstract": "Previous stories About Jobs A few months ago, I’ve been given a new assignment: design Deezer’s product vision. After a couple of years working hands-on on features like Flow, it was exactly the right timing to step back and have a look at what we were all trying to build. The more I looked into the task, the more I realized how challenging it was: Our product has reached maturity, key expected features are all there, but our edge needs to be defined more clearly, Business models have converged toward freemium subscriptions, but we know that not everyone is willing to pay for music, Our competitors are highly talented and moving fast. Hopefully, this leaves plenty of room for innovation and it just so happens that we have lots of ideas. The thing is, we have been exploring many ideas at a time, and now was the time to choose a focus. Despite the fact that everyone fully agrees that we should all work towards the same goal, still we needed to figure out what that goal would be. If you are familiar with strategy, you would know that theoretically such work starts with fundamentals likes company values, core beliefs about users, and future goal(s). « The product vision should describe a broad and engaging goal: a goal that guides the development effort but leaves enough room for creativity; a goal that engages and inspires people, fosters creativity, and generates buy-in. » Scrum Alliance, The Product Vision (Although that was meant to describe a product vision, I think it still applies to vision in a broader sense as well.) Of course here, we’re not talking about short term goals like growing our user base or increasing revenue. Such a goal should sound more like Nike’s « To bring inspiration and innovation to every athlete » or Disney’s « Make people happy ». Although, some would argue that we don’t need a vision, maybe not even a strategy, because technology evolves so fast, future is so uncertain that it would be a waste of time to try to anticipate it . A more efficient way to proceed would be to try many things, fail fast, fail often and adapt. Totally legit. For sure, we can do that, failure is something we all experience :) Why would that ultimately fall short? Trying to compete on multiple segments at once is leading to confusion, both internally and externally. As such well equipped human beings, things have to make sense. Somehow, our users have to understand what it is that we try to achieve together. Very large companies can afford to try many things, launching several apps or businesses and keep the best out of them. Although, when you’re in a smaller company, real estate to experiment things with your user base is a lot more limited. Choices have to be made to keep the user experience consistent and simple at all times. Last year, we experimented a lot around what should be displayed on our home page. Must-have features had to be there like Flow, top charts or new releases, but users showed real interest as well in contextualized recommendations like playlists for running, going out, chilling, … There are so many ways to help people discover new music that it’s easy to add many items on a page, and have features cannibalizing each other. When that happens, deciding when an experiment is a failure or not becomes very difficult. Highlighting a particular feature will drive good results on it, and kill performance of hidden ones. Success of a new feature not only depends on user appeal, but also on exposure and level of implementation. « Try a lot of new things and see what works » is A LOT more complicated that it seems. Changing positions frequently is like moving every 3 months: boxing and unboxing all the time prevents you from cosying up and optimizing your place. Plus, your friends won’t remember where to find you anymore. Belief is that innovation/disruption is strongly tied to technology. Tech is so fast, that it will enable breakthrough we can’t even imagine right now. Having a vision today would be a waste of time since we can’t imagine what will be possible tomorrow. « At a time when digital technology is transforming one industry after another, large companies tend to view innovation and disruption as the result of breakthrough discoveries or technological wonders. (…) Transforming a relatively simple idea into a $19 billion windfall, it turns out, was more about solving problems with the tools at hand than inventing new solutions from scratch.» Laurent-Pierre Baculard , about Whatsapp, HBR July 2016 edition Innovation doesn’t necessarily require new tech. Wanting to solve problems for users is pushing tech frontiers even more. For example, Facebook invests heavily on VR, real time, cross-platform architectures, and that is serving their purpose to connect the world. “People use Facebook to stay connected with friends and family, to discover what’s going on in the world, and to share and express what matters to them.” Facebook Vision Statement At Deezer, our holy grail is to be able to play the right soundtrack at the right moment. Working on that particular issue, I found out that solving that problem was as much about technology as about user experience, understanding what users want. AI helped us make tremendous progress on music discovery, enabling us to build tailored music recommendations for any user. Although, this was not exactly the problem we wanted to solve. Playing the right music at the right moment means knowing when you’re in the mood to discover new tracks, or in the mood to listen to Calvin Harris for 56th time this week. AI can help find patterns in your listening habits, but building a user experience that enables real time feedback on suggestions can go a long way to solve the issue for a particular user. Things don’t evolve by themselves, what we’re working on today will be shaping tomorrow. I remember my first manager at Deezer, former VP of Product, saying that he was « making the world dance », when asked about his job. Despite the fact that we can argue a long time about whether people actually dance when waking up in the morning, it always reminded me of the luck we had to work in the entertainment industry. Let people be moved by music. The more a company’s goal is clear and engaging, the more it makes you willing to go up and beyond to achieve it. « Deezer is the link between you and the music that moves you. We listen to the beat of your heart and help you find the songs that match it. » Stories of engineering, data, product and design teams… 65 1 Startup Product Management Strategy Music 65 claps 65 1 Written by CPO @Soundcharts, ex Product Strategy @Deezer @Microsoft Stories of engineering, data, product and design teams building the future of music streaming Written by CPO @Soundcharts, ex Product Strategy @Deezer @Microsoft Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-12-20"},
{"website": "Deezer", "title": "forget speed start working on your flow", "author": ["Carl Anderson"], "link": "https://deezer.io/forget-speed-start-working-on-your-flow-685bc3d641b3", "abstract": "Previous stories About Jobs A word I hear often in discussions with tech managers is “ speed ”, such as in “we need to increase our delivery speed”, “we’d like to work twice as fast” or the alternative “we need to ship features faster”. What they often mean is that they would like to get twice as much done using the same resources and this is the confusion of two ideas: speed is not the same as productivity . If you want me to get from A to B fast, I’ll take my motorcycle. Now if you want me to take 10 people from A to B, I’ll rent a bus. Now I could ride my motorcycle from A to B 10 times, taking a passenger with me each time, and back again 9 times to pick another up. Not only would it be way slower overall, but also cost and risk would be too high. This may seem obvious to you in this example, but the truth is that many managers say they want to deliver at speed, when all the actions they take suggest that what they really wish to maximize is throughput, so how can we increase it? Well, from my experience, the key isn’t speed but flow which is the smooth progress of work throughout the entire system . Anything that halts or disrupts the work is a block or a delay — the enemies of flow. Whenever something is waiting, it is costing you money. At Deezer, we currently have a 3-week release cycle for our mobile apps, where every 3 weeks we’re publishing a new version of our apps on the stores. This 3-week release cycle is composed of 2 stages: a development sprint of 2 weeks and test phase of a week managed by our Quality Assurance (QA) team. Both our iOS and Android app release cycles are in sync so the QA can manage mobile tests and betas at the same time. At one point, this was running so smoothly in our Android team, that we thought we could speed-up our release cycle to 2 weeks for our Android app. After a talk with the Android, QA and iOS teams, it turns this would have an unintended negative consequence: the QA team did not have the resources to handle iOS and Android releases on different paces, therefore speeding-up the Android release cycle would have required iOS releases to slow down to a 4-week release cycle. The main takeaway I got from this experience is that you have to optimize the whole and not the parts. Speed isn’t the primary goal, value is. Speed is simply a means to deliver value. How fast we need to go must be judged by how speed affects the value. But however fast we want to go, we should look at the whole process and turn our attention to the areas that constrain the flow or where there is unnecessary delay and address those bottlenecks. It’s actually so important that creators of the “ PM Declaration of Interdependence ” placed it number one in their values: “We increase return on investment by making continuous flow of value our focus” — Declaration of Interdependence In my example, an idea supposed to speed up delivery ended up creating delay by increasing a bottleneck, slowing down delivery of an other app and, overall, decreasing value. There’s just nothing more upsetting than initiatives intended to make improvements that actually have negative consequences. On the upside, this experience brought to light that our QA team was already running at full speed and was starting to constrain our flow. As a consequence, before shortening our release cycles on both platforms, we needed to scale up our QA capacity. It’s actually still an ongoing process (a mix of hiring, tooling-up and new methodology) from which we’re only starting to see the first results so it’s still a bit early to share it with you, more on this later :) Moral of this story is that what we truly want is a smooth progress of developments, aka flow, and that flow isn’t about all out speed, it’s about processing work in a way that balances the company’s competing needs and requirements in order to deliver the best final value to the customer. If you’re interested in learning more about optimizing your flow, I personally loved the “Optimising your Flow” course available here , it’s full of great insights, case studies and tools to help you out in that task. Stories of engineering, data, product and design teams… 10 Software Development Agile Management Workflow 10 claps 10 Written by Stories of engineering, data, product and design teams building the future of music streaming Written by Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-12-08"},
{"website": "Deezer", "title": "deezer r d goes to nips 2016", "author": ["Jimena Royo"], "link": "https://deezer.io/deezer-r-d-goes-to-nips-2016-e7a895c2c7ff", "abstract": "Previous stories About Jobs This month, Jimena and Romain from the Deezer R&D team were in Barcelona to attend NIPS — Neural Information Processing Systems, one of the main conferences about artificial intelligence. Numerous scientists and companies working on machine learning were there: Google, Facebook, DeepMind, Microsoft Research, Amazon, Criteo, as well as numerous universities from all over the world. This year was a new record of participants with more than 6000 attendees, confirming the importance of this area in the industry and academy. Many aspects of Artificial Intelligence were addressed, but this year, the hottest topics were the following ones. Reinforcement Learning (RL) systems can learn to solve a complex problem without needing to be explicitly taught how to do it. Recently, notable problems has been tackled using RL architectures: automatically played ATARI games (from raw game pixels), defeating humans in the Go game, simulated animals that learn to walk and run, robots arms that learn to manipulate object In an RL framework, there is an agent (for example a player) living in an environment (a game) that must take a decision (ex.: a direction to take) in order to maximize a reward (winning points). In order to solve the task, the RL system learns a correspondence (a mapping) from a couple (states of the agent, environment observations) to an action to take. We saw the biggest machine learning companies releasing artificial intelligence platforms where is possible to train RL systems: Universe for Open AI, DeepMind Lab for Google’s DeepMind and Malmon (Minecraft) for Microsoft. Generative networks are systems that are able to generate data, such as images that look like real images. Recently, a new way of generating images, based on two neural networks, was proposed: one network is used to generate images, the other one is used to discriminate actual image from images generated by the first one. The two parts thus act one against the other, the discriminator trying to detect fake images and the generator trying to fool the discriminators: that’s why they are called adversarial . This kind of architecture was proposed two years ago, but was lacking stability and ability to generate actual images: as can be seen below, images looks quite real when looked from far away, but looked very weird when looked closely. Lot of papers were thus addressing these issues. Besides these main topics, others subjects drew our attention: The workshop on extreme classification (that is trying to classify items with an extremely large number of labels) was quite interesting, showing in particular how multi-modal approaches (for example using both texts and images) could result in more accurate classifications than mono-modal ones. A fun and interesting poster was the one presented by people from Boston University and Microsoft Research, where the researchers found that a word embedding space (a popular framework to represent text data as vectors), learned from Google News, contained a direction that encoded gender bias. For example, they found that the embedding revealed implicit sexism in the text, founding a geometric representation of the correspondence man::computer programmer and woman::homemaker . The authors also found a way to remove this biases from the embedding space. An interesting demo was performed by Youtube people. They showed how to learn a video content-based based similarity on the youtube8M dataset. The system was trained to predict ground-truth video relationships (identified by a co-watch-based system) only from visual content. In another interesting work using videos (from Flickr website), researchers from MIT trained a system to solve an acoustic scene/object classification task, using a large data set of unlabeled video. They managed to transfer discriminative visual knowledge from image classification networks into sound space, learning the acoustic representation of natural scenes sound. They used the raw audio as input to the deep network that processed the audio and were able to predict object in videos from audio only. So we learned a lot going to NIPS this year and had the chance to share with researchers from other tech companies! Jimena Royo-Letelier & Romain Hennequin Research Scientists at Deezer Stories of engineering, data, product and design teams… 2 Machine Learning Artificial Intelligence Deezer NIPS 2 claps 2 Written by Stories of engineering, data, product and design teams building the future of music streaming Written by Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-12-21"},
{"website": "Deezer", "title": "design wont save the world but it will save your business", "author": ["Vincent Elgrishi"], "link": "https://deezer.io/design-wont-save-the-world-but-it-will-save-your-business-60cd45f0a859", "abstract": "Previous stories About Jobs Designers hear so many stupid things in their life: “Please do your magic, it only takes a minute”, or “I don’t like it and my wife doesn’t like it”, or “Make it pretty; add some sparkles and confetti”. Those great moments when you feel like a rabbit suddenly trapped in the blinding headlights of vacuous crap. Even if it very often is one of the most pragmatic options, a basket full of kittens might not be the only solution. Let’s not discuss here what beauty means. If design was only about creating good looking products, companies like Apple, Airbnb, Dyson or Ikea would have failed a long time ago. Unlike art, design has a purpose which can be measured, it’s about how things work and feel. It’s about the customers. It’s about the object and its functionalities, behavior, content, shape and production. And yes designing all of that is not easy. Maybe that’s why we call it a job. The vast majority of the designers I have met so far are deeply curious folks who feel rewarded by finding solutions. People with a sixth sense of noticing details and spotting patterns. Most of them do not make the mistake of considering design as a weapon for marketing or the even bigger mistake of designing for divas or techies. Fortunately, they very often design for the end users. If your product is hampering your customer’s satisfaction and they lack commitment to your brand, provided your product has some potential, good design can help. Don’t be afraid to ask for help, just ask the right designer, the right way. If your service is annoying as hell, or if your UX sucks, or if your UI is doomed, then meet with good service, UX and UI designers. If your visual language seems like having dyslexia, then talk to good graphic and visual designers. A specific skillset has a cost, the right mindset is almost priceless. Like any specialist, talented and experienced designers are rare and expensive people. Maybe you can’t afford to hire them. In this case, bet on potential and hire young designers. Designers love to learn, their job is to get into your customer’s shoes. The combination of your internal data and qualitative insights, coupled with the designer’s problem solving skills, can be just as valuable as an impressive resume. If you lack both data and money, it doesn’t necessarily mean that you’re completely screwed. If you have time and interest in design, go for it. Auto-didacticism is nothing to be ashamed of; try the Six Thinking Hats system for instance. Show empathy and good sense, aim for simplicity, leverage users knowledge, be creative. It is not a designer specific thing, designers are just trained to it. Sure, design won’t save the world. Even if some bold designers work on creating new systems and materials for us to build eco-responsible houses or for kids in remote regions to access knowledge and education. No it will not save the world. But it will save your business. Stories of engineering, data, product and design teams… 42 UX Design UI Business Creativity 42 claps 42 Written by Head of Design at Deezer Stories of engineering, data, product and design teams building the future of music streaming Written by Head of Design at Deezer Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-12-13"},
{"website": "Deezer", "title": "spark summit europe 2016 deezer was there", "author": ["Nicolas Landier"], "link": "https://deezer.io/spark-summit-europe-2016-deezer-was-there-e8c0e0164247", "abstract": "Previous stories About Jobs As Apache Spark enthusiasts and beer lovers, Deezer folks were attending the Spark Summit 2016 in Brussels. Here is a sum up of what we learned in the different keynotes and talks and we thought might be worth sharing. Matei Zaharia — Databricks CTO — walked us through the new features which have been released with Spark 2.0; the main focus being higher abstractions notably with the DataSet API which help: to leverage classes and types mapping with the DataSet API (and avoid the field to field mappings); to rely on the framework to optimize the DAGs instead of applying user low-level transformations; to have better performance via columnar storage optimizations (Parquet). Video & Slides Ion Stoca — Executive chairman at Databricks — presented the new academic-industry lab which will pursue AMP Lab’s work: RISE Lab for Real-time Intelligent Secure Execution focused on real-time and decision-making systems. Not much for now except there are still many resources on AMP Lab Github and we wish RISE lab to produce just as much material: https://github.com/amplab Slides My favorite keynote at Spark Summit since it was totally unexpected: Miel Hostens — a veterinarian — talked about his activity to measure farm activities and elaborates models based on all kind of data (cows activity from sensors, weather…) in order to optimize milk production. No spoiler, just watch the video . Slides Nimbus Goehausen — Software Engineer at Bloomberg — gave us the overview of one of their library used to improve the Spark development cycle. Running a long process, developers are often waiting for the code’s part they are interested in to be executed. Thanks to spark-flow the user have the possibility to save intermediary results in his job using checkpoints. a signature is computed for each RDD based on hash functions; if the signature has changed the RDD is computed again, otherwise, it just uses the existing one; the user needs to specify each checkpoint himself. Github : https://github.com/bloomberg/spark-flow Slides Elena Lazovik — Research Scientist at TNO — covered the need for deployed Spark applications to be updated without downtime which may be a critical requirement especially for some streaming applications. using a parameter server to host constants, function arguments which are requested by workers extending Spark framework to allow hot change of data source … Video & Slides Zoltan Zvara — researcher at the Hungarian Academy of Sciences — presented us one of his project helping Spark jobs to be balanced. The project consists of improving the “Data-awareness” of Apache Spark about how the data is partitioned. approximate local key distribution and statistics on each worker (scalable sampling); send this information to the master; master adjusts partitions (driver responsibility). To be noted that because of the distribution computation an Apache Spark application can lower by 10% (in worst cases); however, the global stage runtime can be divided by half if partitioning is well done. Video & Slides One of the takeaways of the Spark Summit was the SparkLint tool released by Groupon which helps to analyze Spark deploy configuration efficiency in order to optimize parallelization and allocation. Maximise core usage, minimize idle time (noncalculation processing, that means driver node interaction) Should raise the CPU bound if a job is optimized (core usage time series), that means we are not over allocating resources. If some idle (grey parts) appears on the chart, the core usage value should be low too. core usage > 60% idle time < 1% CPU bound raised increased job & stage details recommendations auto-tuning Github: https://github.com/groupon/sparklint Video & Slides Unit testing is mandatory to deploy Spark jobs with confidence. Moreover it can helps you during the development phase, using tests to make experiments instead of relaunch each time the entire script. That’s why Ted Malaska -data architect at Blizzard- explained to us how to write efficient unit tests for Spark. Running tests in local with a shared context Data can come from user defined, production samples or generators Test the true distribution using dev environment or Docker cluster Video & Slides Kaarthik Sivashanmugam — Software Engineer at Microsoft — gave us a clear presentation about the new architecture of the stream processing pipeline at Bing. Kaarthik explained how they did manage to switch from a batch processing data pipeline to a near real time one. They developed an Open Source project Mobius a C# and F# binding extensions to Apache Spark. If you are a .Net developer, you can now write Apache Spark applications. Github: https://github.com/Microsoft/Mobius Video & Slides Spark monitoring was one of the main subjects of this Spark summit. Yiannis Gkoufas — Research Software Engineer at IBM — presented us SparkOscope overriding the native Spark UI in order to provide to the user more metrics. OS-level metrics Enrich the web UI to plot all available metrics + the new ones from OS Some other features were also announced as being in development: pluggable storage mechanism (HBase, MongoDB…) instead of HDFS making smart recommendations according to metrics patterns Github: https://github.com/ibm-research-ireland/sparkoscope Video & Slides It was our first participation to a Spark Summit and it confirmed our feeling that Apache Spark is going in the right direction and it’s going fast. Currently, Apache Spark is becoming the first option for ETL jobs at Deezer and resource allocation and job optimization considerations are our top priority to ensure a reliable platform. It felt good that most of Apache Spark users arefacing up the same situation. Another big topic was, of course, Apache Spark 2.0 which sounds promising; even though the adoption is not yet there we feel that it would improve code quality. We were a bit disappointed to not hear more talks about deploy and infrastructure aspects (YARN, Mesos and Kubernetes). By the way, you’re interested in using Apache Spark on real use case? We’re hiring . ;) Eloïse Gomez, Jullian Bellino & Nicolas Landier Stories of engineering, data, product and design teams… 4 Thanks to Romain Lods . Apache Spark Big Data Conferences 4 claps 4 Written by Lead at Criteo Stories of engineering, data, product and design teams building the future of music streaming Written by Lead at Criteo Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-11-08"},
{"website": "Deezer", "title": "r d internal hackathon voice control", "author": ["Manuel Moussallam"], "link": "https://deezer.io/r-d-internal-hackathon-voice-control-44cc3ffb8529", "abstract": "Previous stories About Jobs At Deezer, the R&D department role is to perform scientific research in order to help the production teams develop new cool features as fast as possible. The team consists of researchers in signal processing, natural language analysis and machine learning as well as software engineers. Every once in a while, we all stop working on specific topics and unite our forces during a few days for what we call internal hackathons. The goal is to take upon a subject of interest for Deezer, explore it in details and implement a mockup prototype. The last hack was about voice control and this is a resume of what we learned. Voice control could be the next big thing. Amazon, Google, and others seem to be convinced enough to invest heavily on speech recognition software and now, dedicated hardware too. The success of Amazon Echo and the launch of Google Home show that this market is expected to be growing fast in the next years., at least by some big players. In order to transform spoken words (user’s speech) into a Deezer command, one has to go through several operations described in the figure below: First, you need to capture the speech into a digital audio signal. This is called recording and requires a hardware device such as your smartphone, a computer microphone or an Amazon Echo for instance. The quality of the captured audio is of importance, so the recording conditions must be carefully taken care of. In a noisy environment, the captured signal may be very hard to exploit. To isolate the voice from the background noise, specialized devices such as Echo use Noise Cancellation techniques. Since Deezer is not going to start building its own recording devices anytime soon, we will just assume that the microphone used will be the one of the user devices. Once you have a digital audio signal, the next thing to do is convert it into raw text and this is the transcription part. The fruit of decades of research, it is a tremendously complicated problem that requires taking care of the huge variability of the input signal. Think of all the different ways to pronounce a single word like “Play”, at different tones, speed, loudness, with various voices and accent. Now things are even harder at a sentence level. Current technologies require massive amounts of training data, that is, several thousand of hours of speech being manually transcripted to “train” the automatic system. Only companies of the likes of Google, Apple, IBM, Microsoft or Amazon have access to such resources. We at Deezer, certainly don’t :-). It means that whatever technical solution is chosen, we are always going to be relying on external software to perform the transcription. Because they want developers to use their technologies, most of the pre-cited companies provide APIs for speech transcription. For instance, you can send an audio Wav file to the Google Speech API and get the result of their transcription as text. Of course, such services are paying ones Finally, the parsing consists in performing a structural analysis of the text. Basically, it means identifying tokens, then guessing what each token role is in the sentence. Take the following sentence : “Play Billie Jean By Mickael Jackson” This sentence will first be tokenized as : [“Play”, “Billie”, “Jean”, “By”, “Mickael”, “Jackson”] Then the syntactic parser will understand that “Play” is a verb and that “ Billie Jean By Mickael Jackson” is actually an object. This object is also composed of two elements “Billie Jean” and “Mickael Jackson” joined together by the connector “By”. Finally, the parser should understand that the group “Mickael Jackson” describe a person while “Billie Jean” is the title of a song and they are related to each other because the first created the second. This final step is called entity detection and relation detection and yes, it is also not easy at all! The full process is much better explained in the nltk project documentation from which I borrowed this summarizing picture: Finally, the execution part will be achieved thanks to the Deezer SDK and APIs. If I summarize, building a voice control system amounts to building bricks that are all rather complicated and would necessitate months of R&D in order to be developed internally. Since this hack was meant to last just three days, we adopted a different strategy: use and test existing technologies. We thus focus on comparing systems built with various bricks on a set of voice commands and see what exists, what works well and what doesn’t. We started this hackathon by performing a wide survey of existing technologies. The state of the art is complex because companies now ship multiple bricks together. For instance, the Amazon echo API perform both the transcription and the parsing in the same brick. Which means we cannot test those them separately. Microsoft Cortana goes even further to execute the command on your app. Google Cloud, on the opposite, exposes two different APIs so we can evaluate them independent of each other. Overall we listed: Google Speech API Performs the Transcription. Inputs audio file, outputs raw text Google Natural Language API Performs the parsing. Inputs text, outputs a list of objects and their types (track title, artist names, etc.) Amazon Alexa performs both the transcription and the parsing. inputs your voice directly if using the Echo device, or an audio file via api, outputs a list of objects and their types (track title, artist names, etc.). When using the Echo device, can also execute pre-defined commands. Microsoft Cortana Perfoms both the transcription, the parsing and executes command. IBM Watson Speech API Performs the Transcription. Input: Any wave file, Output Raw text IBM Watson AlchemyLanguage Performs the parsing. Input: Text, Output: list of objects and their types (track title, artist names, etc.) And this list is far from being an exhaustive one. We chose to focus on Google APIs, because they are of very good quality and available through web request but also natively on android devices and on the Amazon Echo device through the use of the Alexa API. Comparing approaches mean we have to design a set of sentences that will encompass the commands we want to support. We separated them in three types, ordered by increasing complexity: Correspond to the Deezer Player Available Controls : “Play”, “Pause”, “Stop”, “Resume”, “Next”, “Skip”, “Previous”, “Volume up”, “Increase Volume”, “lower Volume”, “Volume down”, “Mute”, “Unmute” Understanding those commands require little more than recognizing the words, which means any good transcription system should be enough to allow these interaction. For instance, play a specific track: “Play Losing My Religion”, “Play Jeremy by Pearl Jam”, “Play Uptown Funk by Mark Ronson”, etc. Play a specific Album: “Play Waste a Moment by Kings of Leon”, “Play Dark side of the Moon by Pink Floyd”, etc. Or play tracks by a specific artist: “Play some Rihanna”, “Play Beyonce hits”, “Play some Chostakovitch”, etc. Or start a specific playlist (e.g. one from the user): “Play playlist Carcolepsy”, “Play playlist Alternative classics”, etc. Those are already much harder because there is a syntax and objects roles that need to be recognized and mapped in Deezer Catalogue. For instance: “Play some Pop Rock from the 90s”, “Play some minimal techno”, “Play Pop Rock from the 90s”, etc. These commands are even harder to process because the concepts involved are not directly mapped to objects of the catalog but rather as a general semantic description of the content. We certainly discovered and learned a lot of things during this hackathon. We tested many solutions, took some up to the integrated prototype level, played a lot with the API in general and the Search endpoints in particular. At the end of the hack, a working prototype was finished. For now, it only works internally at Deezer, but it sure sparked a lot of interest within the company. In the team, Francesco is working on language topics and so naturally he took the subject as a challenge. The advantage of having a Francesco is that he does not fix boundaries a priori on what we can do. Instead, we need to define scenarios and he will build the parser to solve them. During this hack and based on a set of NLP tools that he has been developing, Francesco built a smart parser that we can use in the following manner: For those not used to reading code, what you see here is a practical example of the result of our custom parser. Given an input text, as should be returned by a transcription system, this parser is able to determine which part is the action command and identify various resources such as artists, track titles or general tags describing music. First of all, getting a clean speech recording is essential for such systems. This advocate in favor of putting some effort on the hardware part, because thus one can control the quality of the signal used as input of the transcription stage. Recognizing artist names or song titles is hard But clean speech is not enough. We found the transcription stumble upon a major difficulty: retrieving artist names or song titles is hard. Google does it quite well for famous ones, but smaller artist names are usually not correctly transcribed by the systems we tested. This is a very serious limitation because remember: if transcription fails, then parsing will fail and the command is bound to be a failure too. On the bright side, we have developped custom approaches for the parsing and resource matching that seem to be quite usable in practice. We intend to still work on them because it could enable rich interactions in the Deezer product. Most of all: we must extend our use cases and imagine new ways to interact with deezer through the voice. We have ideas listed below, but we are most of all eager to get feedbacks and the fruits of your imagination on this. So far we have : Flow Interaction: imagine flow start playing a Jazz tune, but you’re not in that mood, you could just say: “Hey Flow This tune is too quiet I’d like something more dancing” or on the opposite you are in a quiet drink with your friend, Flow starts playing one of your hard techno secretly loved tune and you could say: “No no, stick to the previous mood please”. These scenarios are very challenging but I think they are really interesting for the “lean back” experience. Search by lyrics. I can’t remember a song or an artist, but I know the lyrics were “ Oh baby, baby, I was I suppose to Know .. ” I could just ask Deezer to retrieve it from those lyrics! We actually have all necessary bricks to build this. The rest is up to you . Help us identify situations where you don’t want to or can’t reach the phone/computer but need to change something about the music. How would you express it ? Stories of engineering, data, product and design teams… 12 1 Voice Control Deezer Api Hackathon 12 claps 12 1 Written by Deezer Research. Audio Signal Processing and Machine learning for Music. Stories of engineering, data, product and design teams building the future of music streaming Written by Deezer Research. Audio Signal Processing and Machine learning for Music. Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-10-19"},
{"website": "Deezer", "title": "gesture controlled deezer player", "author": ["Mickaël A"], "link": "https://deezer.io/gesture-controlled-deezer-player-5294f5a7893f", "abstract": "Previous stories About Jobs The Waves Music Hackday is a one-day hackathon which takes place during the Waves Festival , a club and showcase festival with the motto “East meets West”. Many concerts and conferences happened at and around the WUK, a cultural center in the Wärhinger area. The Waves Music Hackday took place for the very first time in 2015 and was a full success! It gathers people around music & technology, with very diverse backgrounds: whether you are a musician, artist, hacker, developer, thinker or hobbyist creator — join this hack day and meet other creative people to spin your ideas around the future of music & technology! I had the chance to participate in the 2016 edition. Although I knew that music creation and hardware tools were common topics, I had no idea what to create during this day before it actually started. Hopefully, on Friday night, companies presented tools available to help and boost hackers towards fun, surprising and innovative creations. Several presentations gave me ideas, but Bitalino got my attention thanks to the many sensors readily available for developers, linked together to a modular board. My first idea, the most simple, was to be able to control a Deezer player from some gestures. This would be not only an original feature but also a way for disabled persons to use Deezer more easily. According to the remaining time, I had also the idea to capture more emotional feedback from my body and inject it to our recommendation algorithm. This would allow music recommendation algorithms to rely on one more source of data: the physical state of the listener. Being in your bed in the dark or running in a light day could influence recommendation although the user had no changes in his/her tastes or listening history. As Bitalino was new to me, I spent some time in the morning exploring its capabilities and decided to go first to a body controller for the Deezer player. I started from a given example on bitalino.com: MuscleBIT . This code is a good start to get values from the Electromyography (EMG) sensor. At this stage, the steps are to: Link Bitalino to some power (battery or USB) Install pybluez to communicate with the device in Bluetooth (with pip install…) Switch it on with the little switch on the board Connect it to the computer as a Bluetooth device (code is 1234) Get its mac address (written on the back of the board) to paste it into the code as it is the way to find the device from the Bitalino python library (included in the example) The code is basically an infinite loop reading the device input, normalizing it and comparing it to a threshold to avoid noise. If the normalized value is higher than the threshold then, in this example, the LED switches on as well as the buzzer. I kept this feature, but also used the given signal for my own needs. Playing with the EMG sensor, I was surprised to see that it was quite stable and pretty easy to control. Noise is very low: if I walk around or grab and carry things, it won’t send a signal at all. Noise is very problematic when it comes to neural sensors, I heard. Snapping my fingers or closing my fist would instantly send a signal, and keep it active until I released my muscle. And it’s very reactive, too! Now that I could capture my muscle activity, I wanted to send it to a web page. The reason is that I wanted my final app to be a Deezer Web App, as showcased on this page . This would demonstrate better, I thought, an alternative way to control the actual player. To communicate between my back-end and the player, I set up a Websocket. Each time a signal was read from the sensor, the python script would instantly inform the web page through the opened socket. However, as my python script would be too busy listening to the sensor activity (remember, the infinite loop), the socket had to be in another script, because it is also an infinite loop! I called this script server and used the Autobahn |python package, which is, among other things, an open-source implementation of the Websocket protocol. This example gave me what I needed. A socket is created on server side, the sensor script is connecting to this socket as well as the web page. The Websocket receives events from the sensor script, and broadcasts it to the web page. The server creates the Websocket and listens: The JavaScript clients connects the socket and controls the Deezer player accordingly: I rapidly created a primal set of commands: 1 contraction = play next track 2 contractions = reduce and restore volume 3 contractions = stop and resume player This is interpreted in the sensor script which basically mesures the number of contractions during three seconds from the first contraction. It counts them, and sends a textual message to the socket which will rebroadcast it. Of course this architecture is far from suitable, and is a result of a quick-and-dirty hackathon situation. The main caveat is that if a new client comes to the Websocket, it will receive messages broadcasted from the same Bitalino device. One person will control all the players! One neat solution that I had to drop because of a lack of time (the hackathon was 8 hours long after all), is to integrate all this code into one JS application. There is a Bluetooth capacity in web browser, which must be explicitly activated by the user (a flag in Chrome for instance). From there, the user could select the Bitalino device around from the web page, and the JS could receive directly the Bitalino signals, and perform adequate controls to the Deezer player. That could be a cool revamp of this application. When talking to people from IRCAM at this hackathon, and from the presentation about the Bitalino the day before, another sensor caught my attention : the Electro Dermal Activity (EDA) sensor. From this data, I could imagine how to use it as an input to our recommendation algorithm and have an impact on the Flow we are listening. Not that I want to use this sensor as the main element of music recommendation, but to add it to the current ones we manage today, and maybe to others from our body we could add later with the Bitalino. The more information we get from the listener, the better we can recommend him/her. Of course, I hope to make this work evolve! Stories of engineering, data, product and design teams… 4 Thanks to Christopher Maneu and Romain Lods . JavaScript Hackathons Music Gesture Recognition 4 claps 4 Written by CTO of Vialma, a classical and jazz streaming platform. Triathlon enthousiast. Classical music nerd. Stories of engineering, data, product and design teams building the future of music streaming Written by CTO of Vialma, a classical and jazz streaming platform. Triathlon enthousiast. Classical music nerd. Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-10-04"},
{"website": "Deezer", "title": "we were at big android bbq europe 2016", "author": ["Christopher Maneu"], "link": "https://deezer.io/we-were-at-big-android-bbq-europe-2016-442f9a9b2b9b", "abstract": "Previous stories About Jobs Since the beginning of this year, we’ve simplified our process to let Deezer developers speak or attend to community events. Three developers of our Android team have taken this opportunity to speak at Big Android BBQ Europe in Amsterdam two weeks ago: Syrine, François and Xavier. Syrine and François were impressed when Google announced their Awareness API. After some discussions — and many cups of coffee — they’ve decided to work on a little side project using this API: Lost In Context . This app lets you start a specific action when you meet a “context”. For example, when you arrive at home and plug your headphones in your device, you can automatically launch your Chill at home playlist. Google Awareness API exposes not only location-related “events” but also activity-related events (like Walking, Running ), time or weather info. Within Lost In Context app, you can combine these factors to tailor your action to a very specific context. But, Syrine and François didn’t stop there! They’ve released their code in a GitHub repository. Bonus point: there is a branch with the app 100% written in Kotlin , including support for databinding. github.com With all these experimentations, they were ready to share their learning during a talk at BigAndroidBBQ Amsterdam, and they did. If you weren’t there, you will find their slides below, in addition to the GitHub project. Even if Aspect Oriented Programming is in the air for decades, a lot of developers didn’t know really where it’s useful and how to start using it. It’s exactly the topic covered by Xavier’s talk. Again, Xavier didn’t leave us only some slides, but also a GitHub repository containing some aspects he’s using on a daily basis: Retry strategy, Run on Main Thread / Worker Thread, etc… github.com Thanks to a personal story and a memorable performance of A Whole New World from Disney’s Aladdin, Xavier won a ticket for the next editions of Big Android BBQ. Congratulations to Xavier! Are you an Android developer? Do you want to work on these topics on a top app? You should have a look at our open positions on Android teams. If you want to apply, you can also read our post about applying Agile methodologies in our HR department . This will help you understand what’s going on behind pushing the “Apply” button. Stories of engineering, data, product and design teams… 8 Thanks to Romain Lods and François Blavoet . Android Android App Development 8 claps 8 Written by 📱 / ☁️ Engineer @Microsoft | 🐠Scuba diving instructor | Previously @Deezer & Microsoft #MVP Stories of engineering, data, product and design teams building the future of music streaming Written by 📱 / ☁️ Engineer @Microsoft | 🐠Scuba diving instructor | Previously @Deezer & Microsoft #MVP Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-08-25"},
{"website": "Deezer", "title": "the challenges behind releasing our windows 10 universal app", "author": ["Christopher Maneu"], "link": "https://deezer.io/the-challenges-behind-releasing-our-windows-10-universal-app-5d743c5b856d", "abstract": "Previous stories About Jobs This summer, we’ve removed our “preview” tag from our Windows 10 Universal app . It’s a huge milestone for the Windows Team at Deezer. Even if the work is not done, we wanted to share a few things we’ve done between our first line of code and today. dzr.fm Deezer has a full-time Windows Engineering team for four years now. During this time, we’ve shipped several apps, and more than 100 updates for: Deezer for Windows 8, Deezer for Windows Phone 8, Deezer for Windows 8.1 (Non-Universal/Shared), We’ve skipped the “Windows 8.1 shared” wave for several reasons: the background audio architecture was changed, causing us a lot of rework, Some XAML controls also have evolved from Windows Phone 8 Silverlight to Windows Phone 8.1. As we have a lot of custom controls and styles, this was also going to cause a lot of rewriting. As Microsoft “veterans” developers, we knew that more changes were coming to Windows 10, so we postponed our next big release for the Windows 10 Universal platform. As a company, one of our main objectives is to give you access to your music anywhere you are, with any hardware that outputs sound . Windows 10 promise is to let you deploy your app on phones, laptops, PCs, Xbox, Raspberry Pi and other devices. For a “small” development team like us, that seems to be a perfect fit. We will have only one code base to handle all our apps on Windows Platform. We already had a Windows 8.1 app, and a Windows Phone 8 app. There was no feature parity between these two apps. Moreover, there was a huge feature gap between our Windows apps and their iOS and Android counterparts. However, we are still a small development team, and we were rewriting the app from scratch. Obviously, we had a choice to make. All our core experience features — like playback and offline sync — are a part of our Windows 10 app. However, some features didn’t make it. As of today, here are our top features on Deezer Music for Windows 10 . On that list, you’ll see few features in bold: these features made their first appearance on the Windows platform ever. Even if we rewrote the app, we’ve taken the time to implement great new features for our Windows users. One of the challenges behind releasing a Windows universal app is to make it working for any device size. Even if we have apps for desktop, mobile, tablet and TVs, combining most of these requirements within one app is quite challenging. At the time of designing it, there were no app in the store to take inspiration from. We’ve done a bunch of research and mockups before finalizing our choices. At the end, we’ve decided to focus only on navigation. We have a Hamburger menu — with items always accessible through their icon — for desktop and a tab menu on phone. This core principle spans across all the app. Then, each page is “responsible” to get an adaptive treatment. Our reasoning behind this choice is to make a usable page, and then see — via telemetry — what pages are used most and on which screens. Then, we will optimize each page according to where they’re used. We will only start this second step now, after our release. At Deezer Devs, we have standards for any code we produce, including: writing unit tests, build & run tests before each merge, code review any line of code, have automated builds for deployment. To be honest, this part of the story is a sad one. Even after more than a year from their releases, UWP tooling is not at a maturity level we expect to create great apps. UWP tooling is not at a maturity level we expect to create great apps. First, the unit testing landscape is a little bit empty. There are few unit test frameworks that integrate well with Visual Studio and the build pipeline. However, we lack tools around unit testing (like a mocking library/framework), and MS Test runner needs to be run in interactive mode. This come with huge drawbacks: you need a custom and on-premises build agent, and you can’t run your unit test in parallel. One year after our first line of code, there is still no way to build a package for store submission from our CI pipeline. Every package we submit has to be built on a developer machine. This is not the release process we had on our Windows Phone 8 app, and it’s sad to have a drawback on such basic development workflow step. Since the writing of this article and it’s publication, Microsoft released a Visual Studio Team Services “Windows Store” extension . We didn’t tested it, as it does not yet support flighted submissions, but we’re looking forward to resolve this issue soon :) This story is not ended yet! We’re working closely with Microsoft engineering teams on a broad range of UWP development issues — including tooling, testing, deployment, framework, etc… — and they have great features coming soon. All these issues have been discussed with them, we just hoped for a sooner resolution. We keep up the good work, as they do. For example, at the beginning of the year, we had no options to run UI Automation tests for our UWP app. Since Microsoft announced a driver for Selenium-based tests, we can now run some sanity tests automatically. Beta releases are part of our mobile workflow since years. Our new Windows Universal app was no exception. However, we’ve slightly changed things compared to our previous windows beta. We had three beta phases: An invite-only beta for our paying subscribers, An invite-only beta for our free users, A public beta for everyone. We chose this approach to let our paying subscribers be the first to test the app. When they’ve started using the app, we were able to implement mandatory features for our free users. The feedback was tremendous: we’ve multiplied by 40 the number of beta registrants compared to our previous beta. We had such a great amount of candidate that we had to work with Microsoft to overcome the Windows Store limits around this feature. When working on a mobile app, you really have unexpected issues like crash the app because of wifi in campings (true story). As we didn’t have a great telemetry in place, we’ve relied on two tools to get feedback from our users: Windows Store ratings and comments, and a custom “send feedback” feature. Everyday, we read all store comments. We also use appbot.co , a service that extract store comments and give you powerful tools to review them: grouping, auto-categorization, sentiment analysis, etc… Everyday, we read all store comments. Comments rated with one or two stars get pushed into one of our Slack channels, so we get sure that any weird issue is getting notified by our engineering and product team. For the beta version, we’ve also created a little page where users can send us a direct feedback — bug or suggestion — via email. Between April and July, we had more than 2000 feedback sent from this feature, far beyond our expectations. It has helped us shape our roadmap for the rest of the year. Even if we’ve reached a big milestone, the Windows Team at Deezer is still working on our Windows 10 Universal app. We can’t share what’s coming yet, but watch out the store for new updates and new features coming in the next weeks :). Are you a C# developer? Do you want to work on these topics? You should send your application and meet our Windows team. If you want to apply, you can also read our post about applying Agile methodologies in our HR department . This will help you understand what’s going on behind pushing the “Apply” button. Stories of engineering, data, product and design teams… 9 Thanks to Romain Lods . Microsoft Windows 10 Mobile App Development 9 claps 9 Written by 📱 / ☁️ Engineer @Microsoft | 🐠Scuba diving instructor | Previously @Deezer & Microsoft #MVP Stories of engineering, data, product and design teams building the future of music streaming Written by 📱 / ☁️ Engineer @Microsoft | 🐠Scuba diving instructor | Previously @Deezer & Microsoft #MVP Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-09-22"},
{"website": "Deezer", "title": "deezer at sónar d innovation challenge 2016", "author": ["Romain Lods"], "link": "https://deezer.io/deezer-at-sónar-d-innovation-challenge-2016-9b25133ef397", "abstract": "Previous stories About Jobs The Sónar Innovation Challenge (SIC) takes place during the Sónar and Sónar+D events in Barcelona, one of the biggest worldwide electronic music festival. This year, it was the 23rd edition of Sónar and like previous years, Deezer was partner of the music hack days which now became the Sónar+D. For the 2016 edition, each partner proposed, a few weeks prior to the event, a challenge and several external profiles were selected based on their motivation and skills to compose ephemeral teams. These international teams had the opportunity to start collaborating online through a dedicated platform to converge on their projects ideas. Partners were the Pompeu Fabra University , Absolut LABS (focus on nightlife innovative experiences and richer social connection), Rapidmix ( produce hardware and software tools for users and makers), CoSiMa (platform for collaborative and collective interaction) and Deezer . Explore a way to monitor crowd behavior in order to generate and control a live music flow. What kind a crowd? From small to large group of people with various similar or distinct musical taste. We wanted challengers to propose a system that would consider several aspects of the person using it: a system that would be interactive and give original live visual feedbacks. Crowd behavior can be estimated in many different ways Knowledge of the participant tastes and profiles Video analysis Noise analysis Sensors integration Allow playlist control from basic interactions to finer settings Control — next, like, dislike, … Rhythm — faster songs, slower songs, … Genre — more acoustic, less acoustic, electronic, … Popularity Other parameters Give some live ambient visual feedback adjusted to the flow The whole team (challengers and mentors) did a few video calls, prior to the events, to discuss and converge on the project. On June 15th 2016, everyone finally met for the first time: the external challengers and the Deezer mentors. The selected challengers were 2 product designers and 3 developers coming from Spain, Italy, UK, Ireland. From Deezer, we were four to come from our Headquarters in Paris. Our missions: mentor the challengers, coordinate the booth and present the challenge and Deezer to the public. Jimena and Manuel are part of our R&D Team , Louis is part of our Platform Team and Romain is in charge of technical challenge, events and ecosystem. Deezer R&D and Platform Teams, Technologies, Projects and Organization will be presented in separate articles, stay tuned ! Manuel presented the challenge and highlighted specific details and problematics relative to Deezer recommendation systems and mechanisms. From our Spain office, there was also Pablo, responsible of Deezer Iberia Operations, who was part of the jury for the French startups competition that was happening at Sónar+D. This competition was organized by La French Tech and Business France. The challengers decided to work on a gaming principle to cover most of the aspects of the challenge: users are players which are automatically distributed in two or more teams based on their musical tastes which are detected when they login through their Deezer profile. They mixed up several “retro” inspirations to get the name of the project, the UI and even its UX. At the end, they came with “Fight For Your Track”. As we could have hope, the 2016 Sónar+D Innovation Challenge was a very musical and technological experience, challengers and mentors will all remember. We are looking forward to be part of it next year and work on other challenges. If you’re interested by our API, take a look at developers.deezer.com You can get more informations about technical events we host, in our Headquarters in Paris, on our dedicated DeezerTech meetup group. Discover Engineering, Product and other open positions on jobs.deezer.com . Stories of engineering, data, product and design teams… 1 Music Innovation Festival 1 clap 1 Written by Tech, Innovation and Music Lover | Working at @Deezer @DeezerDevs @DeezerFrance. Stories of engineering, data, product and design teams building the future of music streaming Written by Tech, Innovation and Music Lover | Working at @Deezer @DeezerDevs @DeezerFrance. Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-08-09"},
{"website": "Deezer", "title": "making our recruitment process more agile a 10 month retrospective", "author": ["Christopher Maneu"], "link": "https://deezer.io/making-our-recruitment-process-more-agile-a-10-month-retrospective-18f9ccd4928e", "abstract": "Previous stories About Jobs For any growing startup, recruitment is challenging, time-consuming, costly and risky. At Deezer, we had the chance to experience this growth: our engineering team has doubled in the last year, and we have many positions open for this year . That’s why we work closely with our dedicated technical recruitment team to improve how we engage with the developer community, and how we handle the recruitment process for our candidates. Today, I would like to share our ongoing experiment on adopting Kanban methodology for our recruitment process. This work was originally initiated with our technical recruitment team. Over the months, it’s being expanded to all our recruitments. Our initial state was not too different from what you can see at many companies. Recruitment team was using a classic Applicant Tracking System(3)/Talent acquisition tool to track applicants, the recruitment process was not harmonized between all our business units. On top of that, there was a lot of recurring meetings, spreadsheets, chat, and emails to help synchronize all involved people. From an applicant point of view, a lot of them didn’t know what to expect when starting the process. The quality of feedback we gave to a specific candidate was too much dependent on which recruiter and the hiring manager were handling this position. Some open positions require an interview first, other starts by a technical exercise. The minimal number of interviews was inconsistent too. The candidates may wait for ages at one step without knowing when they will get an answer. Sometimes, few candidates were not called back when we know we will not pursue their application (1). Definitely not a great candidate experience. When we work in such a competitive space, these major issues hold us back in assembling the right engineering team. That’s why recruitment and engineering teamed up to change the situation. Applying Kanban for HR is a part of a series of initiatives to enhance the collaboration between our HR/Recruitment team and our engineering teams. If you want to learn more about these initiatives, tell us on the comments. Our recruitment team has identified some pain points in their daily activities. In June 2015, we set a time slot to discuss them and find something to improve the situation. We listed three top pain points: Have a real, explicit process : we needed a unique “canvas” for any applicant. That way, any Hiring Manager(2) know what to do, and when we really need his/her support. It also helps us explain the process upfront to any candidate, whenever it’s when applying for a job, or discussing with potential candidates at job fairs and tech events. It also helps report progress in a unique way for any open position, Get measurable metrics : We “knew” that some steps in the process were a source of most bottlenecks. Without measuring them, it’s hard to improve them, Better reporting : recruitment teams are one of the teams who may report to the most diverse quantity of profiles. Each job offer has a hiring manager, Business unit managers who regularly want to know the status of dozen of open positions, C-Level executives, and their direct managers. Each of them wants different info with a specific level of details and across a different number of open positions. After a quick chat, we decided to try Kanban. However, before starting to put post-it on any walls, we’ve done two things: Send a formal request to HR Director : We wanted to build this experiment, however, this can be done only with full support from our hierarchy. We were about to change the way a team is operating, with (hopefully positive) side effects on candidate experience, team efficiency (during transition phase) and office decoration :), Map our current recruitment process : we needed to have the same vision of our recruitment process. We took 2 hours to map — with post-its — our current recruitment process and decide on which parts we wanted to change. Choosing Kanban over other agile methodologies was quite a no-brainer. The concept of sprints from Scrum is not really applicable to recruitment jobs: candidates (eg. work item) can appear at any time, we can’t really evaluate time, and we can’t ensure any application can be processed within a specific time-frame or sprint. In the other hand, Kanban fits well with these constraints, and few key concepts, like limiting number of in-progress tasks, seemed like a great solution to some of our issues. To begin our experiment, we chose to get two different boards: a candidate board and a job offer board. The job offer board was here to get an overview of all opened jobs. One post-it equals to one open position. Each recruiter uses a different post-it color. This job board has 7 columns: Assigned : An open position has been assigned to a recruiter, Brief : We start the process of opening this position with a brief between the hiring manager and the recruiter, Job Description Writing : Job description is underwriting or review. This may need a lot of back-and-forth between different services, Management Approval : This step represents the final top management approval, including base package and recruitment priority. When this step is completed, the financial, administrative and logistical roadblocks are — for most of them — handled, Job Desc Approval : we’re finishing the job description writing, ensure it’s coherent with our others job desc, Hunt : The position is advertised on our deezer.com/jobs page and in some other places. Our recruiters are actively looking for candidates, Hired! : We’ve found our best candidates, we’re waiting for her/his arrival. This board was intended to be used when reporting to HR Director and Executives. It was also placed in a strategic place where any employee can see it. Our second board, named “ Candidate board ”, was here to track each candidate through the recruitment process. We were not sure about which columns were needed, so we started to map out the different steps that follow a candidate through our process. We’ve done this on glass walls with an erasable pen. That way, we could test if we had the room for each column we want. The candidate funnel can lost a huge number of candidate in the first steps, so we decided to start our board by the “Test Sent” column. Each open position comes with a technical test or challenge that must be done by any candidate. When the test is sent, the hiring process is really kicked-off. For the first iteration, we choose the following columns: Test sent : the exercise was sent to the candidate. We’re waiting for the completed project, Test received : the candidate sent us his/her project. It was sent to the hiring manager for review, Test validated : the hiring manager — with assistance of his/her team — has validated the candidate’s project, 1st Interview : this column represents the 1st in-person interview slot. It’s subdivided into two columns: planned/done, 2nd Interview : this column represents the 2nd in-person interview slot. It’s not a mandatory steps for all our positions, This column is also subdivided into planned and done, Validation : We’re waiting for the validation of all the interviewers from the 1st and 2nd interview, Decision making : We’re discussing the opportunity between the hiring manager and the recruitment team to elect this candidate or not, Management approval : we’re waiting for the management to sign-off this hiring, Prop. Sent : we sent an offer letter to the candidate and waiting for his/her answer, Hired : the candidate has accepted our offer. It will be now handled to our HR department for contracts and onboarding, Rejected : Somewhere during the process, we’ve decided that the candidate is not ready of any of our open positions, Gave up : Somewhere during the process, the candidate canceled himself/herself the hiring process. “Gave up” is one of the most important column, at several levels. When a recruiter engages in a conversation with a candidate, getting a “no” from a candidate — unexpected or not — can be emotionally challenging. This is event more disturbing when the recruiter has promoted the candidate internally. This gave up column helps the recruiter to move on this bad news, with the assurance that we will take care of this issue at a better time. Periodically -ideally every 4 to 6 weeks- we take a moment to parse all “gave up” notes. We try to find patterns that explains why candidates gave up, and how we can prevent that outcome, or make it happen as soon as possible. We also do the same job for “rejected” column. However there is no such emotional pressure compared to gave ups. We spent a bit of time thinking about the post-it itself. First, we used a color code to differentiate candidates among the recruiters. Each recruiter was then able to quickly see her candidates. The candidate name was obviously the main info. Even if this board was placed in a limited access space, it was still accessible by a lot of employees. We made the choice to display only the first family name letter, to ensure the confidentiality of our candidates. If you don’t have access to physical space, there is a lot of online apps that can do the job — like trello or some ATS. After few months of experimentation, we decided to reflect on our setup, and change a few things: it was our iteration #2. The first thing we changed was removing “Decision making” and “approval” columns. In fact, almost no Post-It stayed in these columns. The recruiter color code put a lot of pressure on them. Sometimes, a recruiter will not have any candidate in the process for few days, and it can be totally okay. He/she might be working on a recruiting event, on employer brand, or on other subjects. However, it was not clear when managers see a board without any note of your color. So, we reverted to same color notes for everyone. For this 2nd iteration, we also decided to remove the step dates on each sticky note. Maintaining them up to date was time consuming, and we didn’t have a proper way to parse them. If we needed this info, we can find it in our ATS app. With this new iteration, we’ve also adressed a physical issue with the Post-It. Some of them did not resist time. Recruiters needed to use scotch or Patafix for some of the sticky notes. After few days of research, I found the perfect replacement product: Magentic Notes from Tesla Amazing . Yes, these sticky notes neither use glue, nor paper! It’s a statically charged plastic film. Writing is as smooth and as resistant as if it was paper, and all the note surface adhere to almost any other surface (excluding screens and MacBook cases). In the middle of this iteration, a big part of our recruitment team changed. Without prior experience about Kanban, and a little knowledge of agile methodologies, we’ve decided to setup a training session. The agenda was divided in two parts: introducing Kanban, and introducing how we adapted Kanban for our technical recruitments. We made two main changes with our current iteration. First, we finally get rid of our Job board. The info was redundant with our ATS(3) system. The second update we made was on the candidate note template. In this third iteration, we’ve added the candidate origin in lieu of the steps dates we’ve removed after iteration #1. A candidate can come directly to us — via our Jobs website , on our LinkedIn page , or by meet us at various events and meetups . We can also use external resources to help us find great candidates. It’s important to know how a specific candidate was in touch to personalize the candidate experience. We didn’t changed the columns of our candidate board between iteration #2 and iteration #3. There is few core Kanban/agile concepts we decided not to follow. The first one was about Definition of Done . For so many reasons, we cannot be 100% strict about our recruitment process. Sometimes, we had opportunities that make sense to not follow our process. For example, a candidate is in the city only for few days — so we can interview him before the technical exercise step. The other one is more open to debate: the Work In Progress (WIP) limit. We don’t have any limit on our columns. It’s quite difficult to ask a candidate for waiting before moving to the next step because we’ve reached a specific limit. The approach we choose is to look at the first columns in terms of volume, to get an insights of the future work: number of technical exercises sent drives the number of exercises we need to review, that in turn drives the number of interview we need to plan. As any agile methodology, this is still a work in progress. Our latest reorganization causes us some reporting issues. Our boards clearly communicates the current progress. However it’s still difficult to get precise metrics to communicate without spending some time reporting board info into another system. In the next few weeks, we will look at new ways to compute these metrics more easily. The candidate experience still has room for improvement. We worked on this topic during our latest hackathon, and we’re evaluating how we can phase out these ideas. Christopher Maneu is our Engineering Effectiveness Lead, and part-time developer on the Core Windows Team. You should follow him on Twitter and medium @cmaneu. Thanks to Marion, Pauline, Camille, Cécile, Anne-Sophie, Corentin and Guillaume for reviewing this article. Footnotes 1. It happens at least one time with a candidate I’ve recommended. I was really confused, as this candidate was actually a great one and has demonstrated his motivation. However, we had one better candidate. One year later, I’ve called him back when a new position was opened. He is working full time with Deezer now. 2. Hiring Managers (HM) are an operational manager that ensure all recruitment work for their teams. In general, they also lead the team. 3. Applicant Tracking System (ATS) is an app dedicated to handling candidates, applications and all the steps within the hiring process. During our research, we found that Recruitee was presenting applications with a “kanbanesque” view. For several reasons, we didn’t try it, but we may not be the only ones to apply kanban principles to recruitment :) Stories of engineering, data, product and design teams… 14 HR Startup Agile 14 claps 14 Written by 📱 / ☁️ Engineer @Microsoft | 🐠Scuba diving instructor | Previously @Deezer & Microsoft #MVP Stories of engineering, data, product and design teams building the future of music streaming Written by 📱 / ☁️ Engineer @Microsoft | 🐠Scuba diving instructor | Previously @Deezer & Microsoft #MVP Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-05-27"},
{"website": "Deezer", "title": "deezer hack session", "author": ["Romain Lods"], "link": "https://deezer.io/deezer-hack-session-c88cffa510b4", "abstract": "Previous stories About Jobs On April 14th 2016, we organized a Deezer Hack Session in our Paris Office, a day long internal hackathon open to all our collaborators, whatever the team they belong to. Engineers, Product Specialists, Designers, HR and Marketing Fellows, etc. all joined to explore, try, innovate and build original prototypes. We had a lot of ideas, music and pizzas! We have been holding internal hackathons for several years, simply because we believe that they are a great way for employees to bond, brainstorm, explore new technologies, be creative and above all have fun! Flow Radio was one of the cool and original inventions the teams managed to concoct during this session: Mickaël from the R&D team and seven other colleagues managed to bring an old Schneider radio back to to life by integrating our streaming services (Flow and Discovery). How cool is that?! Our teams have many other tricks up our sleeves and it’s a matter of time before we’ll introduce some our creative (and sometimes crazy) ideas into your Deezer experience. At the end, more than 80 employees were part of the hackathon and almost 30 projects have been initiated: functional hardware prototype, new features for our mobile product, music social solutions, bots and integrations in external services, recognition and recommendation exclusive concepts. The next day, the 30 teams had to present what they created. You can get more informations about the technical events we host on our dedicated DeezerTech meetup group. Discover our Engineering, Product and other positions on jobs.deezer.com . Stories of engineering, data, product and design teams… 3 Hackathons Startup Coding Makers Innovation 3 claps 3 Written by Tech, Innovation and Music Lover | Working at @Deezer @DeezerDevs @DeezerFrance. Stories of engineering, data, product and design teams building the future of music streaming Written by Tech, Innovation and Music Lover | Working at @Deezer @DeezerDevs @DeezerFrance. Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-07-22"},
{"website": "Deezer", "title": "deezer flow radio", "author": ["Mickaël A"], "link": "https://deezer.io/deezer-flow-radio-10556d37a3c6", "abstract": "Previous stories About Jobs The Flow Radio is a physical radio that has been hacked to link it to our Deezer Flow . This project has been initiated during the Deezer Hack Session of April: we had one day to constitute a team of six and reach our goal. The idea was to fit our Deezer Audio Player in a Raspbian Linux Distribution, on a Raspberry Pi ; and then to fit the Raspberry Pi into a very old radio bought used on the Internet. Actually, the radio belonged to a woman living close to our office. Picking the radio up was quite easy. A search on the web revealed that the Schneider radio was built in 1959… This hackathon project was quite special compared to other projects as it relied not only on software but also on hardware. We had to prepare the session and gather some basic equipment and components before starting working on it, in order to save us time. The most important equipment was the radio itself, the Raspberry Pi and electronic components. To make the project more interesting, we specifically wanted that the old radio has at least two buttons and one potentiometer, so we could hack them in manners that led to (probably) interesting results. We had no clear idea about how to hack these interfaces, but as long as we could decide it during the day it was not yet a problem. The Schneider radio has three buttons and two potentiometers. It was easier to get a Raspberry Pi. Many of the developers at Deezer play with these little and fantastic machines. We had a choice between three Rasps. We took the latest one, which natively supports wifi. One of the developers happened to have basic electronic equipment: wires, soldering iron, resistors, potentiometers… We had to buy an analog/digital converter (MCP3008) as the Rasp PINs only receive digital signals, an amplifier to connect to the original speaker (if it was still working, which was not sure at that time), and an SD card to store the Raspbian OS. Although Deezer has a robust and powerful player, which decodes our audio from our servers, it has never been ported to an ARM architecture so far. Experimental work has been made by chance just before the hackathon, and we could finally use some test code. However, it was the first time the player would be adapted to a Raspberry, and it had to work! If not we had potential backups: to play only local tracks or download them first hand. These options were not really sexy, and we hoped hard to stream directly from the Raspberry. The old radio was not functioning anymore. One big question was what could be reused. The power was not in there anymore, so we used a 9V battery to power the original assembly. It was not possible to hear any sound clearly. We realized quickly that some of the components were unplugged. We were not very confident about the original pieces and decided to test them one by one with a voltmeter. Only the speaker, potentiometers, and buttons were useful for our project, and one by one we realized they were fully functional. When the speaker played music from an iPhone we connected through an open Jack cable we were very happy and very eased. One of the two potentiometers was broken and we replaced it by an electric guitar potentiometer we had from a guitar player at Deezer. Little by little we could link the original components to our own circuit. We also changed the original functioning of the three buttons to make them “pushable”. Initially, the buttons were exclusive. Because we finally decided to link the button to standard Deezer action: Repeat, Skip, and Like , we did not need the buttons to stay down. This was quite an easy mechanical trick to neutralize the hook. The speaker was quite in good shape. As it’s a mono speaker and after considering an electronic assembly to transform a stereo to a mono signal, we decided to do this on the software side. It was definitely easier and faster. And at this stage of the project, we were already after lunch, and we wanted to move on! The GPIOs are the pins as visible on the Raspberry. The Raspberry Pi 3 has the following PINs: From the list of these 40 pins, we must build our schema in order to link the three buttons, two potentiometers linked to the MCP3008 and the audio amplifier. The Python program, which uses the Rpi GPIO library declares each GPIO as an input and reads, within a loop, the current value of each of them to check it and process it. Here is a simplified view of our program: One of the great advantages concerning the team management is that this part of the project is autonomous from the schema assembly. We could easily split the team during the day and drastically improve our efficiency: hardware versus software. We had one button and one potentiometer to test our schema and to decide which GPIO to use. The final step of the day was to connect the assembly on the GPIO and theoretically it would work instantly. And it actually did! The audio player is written in C: it authenticates a valid Deezer user (we created one for the project) and handles the audio stream decryption. The ARM compiler ( armcc ) was used and we had to recompile the player several times. The radio program is written in Python and controls the C player through system calls. The user authentication is made using OAuth and the parameters are given through a hard coded file in the Raspberry. With this configuration, it is quite hard to personalize the radio with one’s account. It’s one of the limitations of this fast coded hackathon! The player is given a list of Deezer song ids and play them one after the other. The loop itself catches electrical events from the three buttons and two potentiometers. From the following events, a specific player action is taken: Repeat Button : The current song ID is added again as the first element of the list and is played instantly Skip Button : The player is given the next element in the list and plays it instantly Discovery Potentiometer : When changed, the discovery value is sent as an input to the API. The resulted song ID list is given to the player and played immediately. Like Button : The public Deezer API is called with the song ID to like (no player action is taken) Deezer has a few mechanisms to control the discovery of a given mix or flow. However, they are not exposed via an API. As the Raspberry does not hold all the Deezer code stack and data, it was quite impossible to recover a list of song IDs according to a given discovery rate. Besides, no discovery algorithms take as an entry such a thing as a discovery rate . The idea here was then to create a new private API, only in development, for our immediate needs. This API is currently only available in Deezer’s office, making the radio quite private! Come here to see it! The Raspberry recovers the discovery potentiometer value and normalizes it between 0 and 100. A step of 10 is arbitrary set to avoid inopportune inaccuracy from the received signal (in fact, the analog values are constantly moving). We send the normalized value to our new Deezer API. That API sends back a list of 10 tracks computed from the user’s profile. If the discovery value is close to 100 then many new song IDs are returned (not very popular, not listened by the user so far). On the contrary, popular tracks are returned and played one after the other. Although it looks quite trivial, it has to be programmed! Recovering the value is exactly the same process than the discovery value (on another GPIO). However, the action is not to call a web API of course. We decided to call an internal Raspberry command called amixer (from Alsa Mixer). Another choice would have been controlling the Deezer player volume. The Alsa Mixer controls the Master sound from a value from 0 to 65535, or a percentage. We decided to use the 0–65535 range as it provides a better accuracy. We observed that our Raspberry made audible sound from 22000 to 32000. The idea was then to map the potentiometer value to this range. A threshold is also set, but lower than 10 to have a better accuracy than the discovery. After a day here is our old radio fully working. The main limitations are: Working only on Deezer network (hardcoded wifi configuration, and internal Discovery API) Hard coded user. We created a specific user for the Old Radio but the idea in the future is to be able to easily configure the radio to put anyone’s ID and make it personalized As a bonus, here are more photos of the team working with wires. You’re interested by our API and SDK? Take a look at developers.deezer.com . You can get more informations about the technical events we host on our dedicated DeezerTech meetup group. Discover Engineering, R&D and other open positions on jobs.deezer.com . Stories of engineering, data, product and design teams… 57 1 Raspberry Pi Technology Music Hacking 57 claps 57 1 Written by CTO of Vialma, a classical and jazz streaming platform. Triathlon enthousiast. Classical music nerd. Stories of engineering, data, product and design teams building the future of music streaming Written by CTO of Vialma, a classical and jazz streaming platform. Triathlon enthousiast. Classical music nerd. Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-07-22"},
{"website": "Deezer", "title": "hello world", "author": ["Christopher Maneu"], "link": "https://deezer.io/hello-world-bc93e4f6881d", "abstract": "Previous stories About Jobs Hello Technology & Music fans, Everyday, our team is working hard to help you discover and enjoy great music moments. Whatever phone you’re using, whenever you’re in a workout, working in the office, or chilling-out with friends, we’re here to offer your great songs — in a pool of 40 Million — with our apps. Everyday, we try new things, we face interesting challenges and we try to find clever solutions. It’s exciting for us, and internally we have a broad range of events to share our learnings. Today, we want to share these stories with you. Our technological choices, our latest experiments, or even how we organize as a team. Our next posts are already underway, so feel free to follow our publication to be the first to read us. Meanwhile, you should follow @deezerdevs on twitter, and if you’re around Paris, you can join the DeezerTech meetup group, where we publish the meetups we host in our Headquarters. Stories of engineering, data, product and design teams… 5 Thanks to Romain Lods . Hello World Engineering Technology Music Startup 5 claps 5 Written by 📱 / ☁️ Engineer @Microsoft | 🐠Scuba diving instructor | Previously @Deezer & Microsoft #MVP Stories of engineering, data, product and design teams building the future of music streaming Written by 📱 / ☁️ Engineer @Microsoft | 🐠Scuba diving instructor | Previously @Deezer & Microsoft #MVP Stories of engineering, data, product and design teams building the future of music streaming Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-07-18"}
]