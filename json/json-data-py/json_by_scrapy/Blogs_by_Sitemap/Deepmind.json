[
{"website": "Deepmind", "title": "DeepMind Papers @ NIPS (Part 2)", "author": "Unknown", "link": "https://deepmind.com/blog/article/deepmind-papers-nips-part-2", "abstract": "The second blog post in this series, sharing brief descriptions of the papers we are presenting at NIPS 2016 Conference in Barcelona. Authors: Marco Fraccaro, Søren Kaae Sønderby, Ulrich Paquet, Ole Winther Much of our reasoning about the world is sequential, from listening to sounds and voices and music, to imagining our steps to reach a destination, to tracking a tennis ball through time. All these sequences have some amount of latent random structure in them. Two powerful and complementary models, recurrent neural networks (RNNs) and stochastic state space models (SSMs), are widely used to model sequential data like these. RNNs are excellent at capturing longer-term dependencies in data, while SSMs model uncertainty in the sequence's underlying latent random structure, and are great for tracking and control. Is it possible to get the best of both worlds? In this paper we show how you can, by carefully layering deterministic (RNN) and stochastic (SSM) layers. We show how you can efficiently reason about a sequence’s present latent structure, given its past (filtering) and also its past and future (smoothing). For further details and related work, please see the paper https://arxiv.org/abs/1605.07571 Check it out at NIPS: Tue Dec 6th 05:20 - 05:40 PM @ Area 1+2 (Oral) in Deep Learning Tue Dec 6th 06:00 - 09:30 PM @ Area 5+6+7+8 #179 Authors: Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew Hoffman, David Pfau, Tom Schaul, Nando De Freitas Optimization algorithms today are typically designed by hand; algorithm designers, thinking carefully about each problem, are able to design algorithms that exploit structure that they can characterize precisely.  This design process mirrors the efforts of computer vision in the early 2000s to manually characterize and locate features like edges and corners in images with hand designed features. The biggest breakthrough of modern computer vision has been to instead learn these features directly from data, removing manual engineering from the loop. This paper shows how we can extend these techniques to algorithm design, learning not only features but also learning about the learning process itself. We show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms outperform standard hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including neural network training, and styling images with neural art. For further details and related work, please see the paper https://arxiv.org/abs/1606.04474 Check it out at NIPS: Tue Dec 6th 06:00 - 09:30 PM @ Area 5+6+7+8 #9 Thursday Dec 8th 02:00 - 9:30 PM @ Area 1+2 (Deep Learning Symposium - Poster) Friday Dec 9th 08:00 AM - 06:30 PM @ Area 1 (DeepRL Workshop - Talk by Nando De Freitas) Friday Dec 9th 08:00 AM - 06:30 PM @ Area 5+6 (Nonconvex Optimization for Machine Learning: Theory and Practice - Talk by Nando De Freitas) Saturday Dec 10th 08:00 AM - 6:30 PM @ Area 2 (Optimizing the Optimizers - Talk by Matthew W. Hoffman) Authors: Navdeep Jaitly, Quoc V. Le, Oriol Vinyals, Ilya Sutskever, David Sussillo, Samy Bengio Models which map from a sequence of observations to another sequence (sequence-to-sequence) have become extremely popular in the last two years due to their generality, achieving state-of-the-art results in a variety of tasks such as translation, captioning, or parsing. The main drawback of these models is that they need to read in the whole sequence of inputs “x” before starting producing the resulting output sequence “y”. In our paper we circumvent these limitations by allowing the model to emit output symbols before the whole input sequence has been read. Although this introduces some independence assumptions, making online decisions in certain domains such as speech recognition or machine translation makes these models much more desirable. For further details and related work, please see the paper http://papers.nips.cc/paper/6594-an-online-sequence-to-sequence-model-using-partial-conditioning.pdf Check it out at NIPS: Tue Dec 6th 06:00 - 09:30 PM @ Area 5+6+7+8 #53 Authors: Audrunas Gruslys, Remi Munos, Ivo Danihelka, Marc Lanctot, Alex Graves Many state of art results were achieved by training large recurrent models over long sequences of input data.  Training recurrent networks is not an easy task for many reasons. One of complications is a large memory consumption of the standard backpropagation through time (BPTT) algorithm, as  it requires memorizing all or almost all past neuron activations. It is especially easy to run out of expensive GPU memory when training convolutional RNNs, and memory constraints often lead to unwanted compromises in network size. A common solution used to alleviate this problem is to memorize only some of intermediate neuron activations and recompute others on demand. While there were many heuristics that trade off memory and computation, most of them are adapted for certain edge cases and are suboptimal. We viewed the problem as a dynamic programming problem which allowed us to find a class of provably optimal strategies subject to memory constraints. For sequences of length 1000, our algorithm saves 95% of memory usage while using only one third more time per learning step than the standard BPTT. For further details and related work, please see the paper https://papers.nips.cc/paper/6220-memory-efficient-backpropagation-through-time.pdf Check it out at NIPS: Tue Dec 6th 06:00 - 09:30 PM @ Area 5+6+7+8 #64 Authors: Karol Gregor, Frederic Besse, Danilo Rezende, Ivo Danihelka, Daan Wierstra Discovering high level abstract representations is one of the primary goals of unsupervised learning. We approach this problem by designing an architecture that transforms the information stored in pixels into an ordered sequence of information carrying representations. Training results in an emergent order, where early representations carry information about the more global & conceptual aspects of the image, while the latter representations correspond to the details. The model is a fully convolutional, sequential variational autoencoder inspired by DRAW. The architecture is simple and homogeneous and therefore does not require many design choices. The resulting information transformation can be used for lossy compression, by transmitting only the early set of representations (the number of which is given by the desired compression level) and generating the remaining ones as well as the image using the generative model. If the ordering of information that the model discovers correlates strongly with the ordering of information by importance as judged by humans, then the algorithm will transmit what humans consider to be the most important. If the generation of the remaining variables results in a high quality image, this method should lead to high quality lossy compression. Because both humans and unsupervised algorithms try to understand data and because both use deep networks to do so, there is a good reason to believe that this approach will work. We demonstrate that this is indeed the case and the current model already results in performance that compares favorably to that of JPEG and JPEG 2000. As generative models are progressively getting better, these results demonstrate the potential of this method for building future compression algorithms. For further details and related work, please see the paper http://papers.nips.cc/paper/6542-towards-conceptual-compression.pdf Check it out at NIPS: Tue Dec 6th 06:00 - 09:30 PM @ Area 5+6+7+8 #77 Authors: Danilo Rezende, Ali Eslami, Shakir Mohamed, Peter Battaglia, Max Jaderberg, Nicolas Heess Imagine looking at a photograph of a chair. The image you see will be a complex function of the attributes and positions of the camera, the lights and, of course of the shape of the chair. Importantly, due to self-occlusion you never see the full chair, so there is an infinite number chair-like objects that would be consistent with what you see. Nevertheless, when asked how to imagine the chair's shape from a different point of view you will probably be able to do so quite accurately. Key to this ability is not just an implicit understanding of perspective, occlusion and the image formation process, but critically your prior knowledge of what a plausible chair ought to look like, which allows you to “fill in” the missing parts. In this paper we study models that are able to perform similar types of reasoning. Specifically, we formulate generative models which can learn about the statistical regularities of the three-dimensional shape of objects. The resulting prior over shapes produces high-quality samples, and allows us to formulate challenging ill-posed problems such as that of recovering plausible 3D structures given a 2D image as probabilistic inference, accurately capturing the multi-modality of the posterior. This inference can be achieved rapidly with a single forward-pass through a neural network and we show how both the models and inference networks can be trained end-to-end directly from 2D images without any use of ground-truth 3D labels, therefore demonstrating for the first time the feasibility of learning to infer 3D representations of the world in a purely unsupervised manner. For further details and related work, please see the paper https://arxiv.org/abs/1607.00662 and our video: https://www.youtube.com/watch?v=stvDAGQwL5c Check it out at NIPS: Wed Dec 7th 06:00 - 09:30 PM @ Area 5+6+7+8 #2", "date": "2016-12-05"},
{"website": "Deepmind", "title": "Game-theory insights into asymmetric multi-agent games", "author": [" Karl Tuyls ", " Marc Lanctot ", " Julien Perolat "], "link": "https://deepmind.com/blog/article/game-theory-insights-asymmetric-multi-agent-games", "abstract": "As AI systems start to play an increasing role in the real world it is important to understand how different systems will interact with one another. In our latest paper , published in the journal Scientific Reports , we use a branch of game theory to shed light on this problem. In particular, we examine how two intelligent systems behave and respond in a particular type of situation known as an asymmetric game , which include Leduc poker and various board games such as Scotland Yard . Asymmetric games also naturally model certain real-world scenarios such as automated auctions where buyers and sellers operate with different motivations. Our results give us new insights into these situations and reveal a surprisingly simple way to analyse them. While our interest is in how this theory applies to the interaction of multiple AI systems, we believe the results could also be of use in economics, evolutionary biology and empirical game theory among others. Game theory is a field of mathematics that is used to analyse the strategies used by decision makers in competitive situations. It can apply to humans, animals, and computers in various situations but is commonly used in AI research to study “multi-agent” environments where there is more than one system, for example several household robots cooperating to clean the house. Traditionally, the evolutionary dynamics of multi-agent systems have been analysed using simple, symmetric games , such as the classic Prisoner’s Dilemma , where each player has access to the same set of actions. Although these games can provide useful insights into how multi-agent systems work and tell us how to achieve a desirable outcome for all players - known as the Nash equilibrium -  they cannot model all situations. Our new technique allows us to quickly and easily identify the strategies used to find the Nash equilibrium in more complex asymmetric games -  characterised as games where each player has different strategies, goals and rewards. These games - and the new technique we use to understand them - can be illustrated using an example from ‘Battle of the Sexes’, a coordination game commonly used in game theory research. Here, two players have to coordinate a night out to either the opera or the movies. One of  the players has a slight preference for the opera and one of them has a slight preference for the movies. The game is asymmetric because, while both players have access to the same options, the corresponding rewards for each are different based on the players preferences. In order to maintain their friendship - or equilibrium - the players should choose the same activity (hence the zero payoff for separate activities). This game has three equilibria: (i) both players deciding to go to the opera, (ii) both deciding to go to the movies, and (iii) a final, mixed option, where each player will opt for their preferred option three fifths of the time. This last option, which is said to be “unstable”,  can be rapidly uncovered using our method by simplifying - or decomposing - the asymmetric game into its symmetric counterparts. These counterpart games essentially considers the reward table of each player as a separate symmetric 2-player game with equilibrium points that coincide with the original asymmetric game. In the plot below, the Nash equilibrium is plotted for the two, simple counterparts allowing us to quickly identify the optimal strategy in the asymmetrical game (a). The reverse can also be done, using the asymmetrical game to identify the equilibrium in its symmetrical counterparts. This method can also be applied to other games, including Leduc poker, which is described in detail in the paper. In all of these situations, the method proves to be mathematically simple, allowing a rapid and straightforward analysis of asymmetric games that we hope will also help our understanding of various dynamic systems, including multi-agent environments. UPDATE 20/03/18: Our latest paper , forthcoming at the Autonomous Agents and Multi-Agent Systems conference (AAMAS), builds on the Scientific Reports paper outlined above. A Generalised Method for Empirical Game Theoretic Analysis introduces a general method to perform empirical analysis of multi-agent interactions, both in symmetric and asymmetric games. The method allows to understand how multi-agent strategies interact, what the attractors are and what the basins of attraction look like, giving an intuitive understanding for the strength of the involved strategies. Furthermore, it explains how many data samples to consider in order to guarantee that the equilibria of the approximating game are sufficiently reliable.  We apply the method to several domains, including AlphaGo, Colonel Blotto and Leduc poker. Read the original Scientific Reports paper here . Read the follow-up AAMAS paper here . The Scientific Reports paper is authored by Karl Tuyls, Julien Perolat, Marc Lanctot, Georg Ostrovski,  Rahul Savani,  Joel Leibo, Toby Ord, Thore Graepel and Shane Legg. The AAMAS paper  is authored by Karl Tuyls, Julien Perolat, Marc Lanctot, Joel Leibo and Thore Graepel.", "date": "2018-01-17"},
{"website": "Deepmind", "title": "Neural scene representation and rendering", "author": [" S M Ali Eslami ", " Danilo Jimenez Rezende "], "link": "https://deepmind.com/blog/article/neural-scene-representation-and-rendering", "abstract": "There is more than meets the eye when it comes to how we understand a visual scene: our brains draw on prior knowledge to reason and to make inferences that go far beyond the patterns of light that hit our retinas. For example, when entering a room for the first time, you instantly recognise the items it contains and where they are positioned. If you see three legs of a table, you will infer that there is probably a fourth leg with the same shape and colour hidden from view. Even if you can’t see everything in the room, you’ll likely be able to sketch its layout, or imagine what it looks like from another perspective. These visual and cognitive tasks are seemingly effortless to humans, but they represent a significant challenge to our artificial systems. Today, state-of-the-art visual recognition systems are trained using large datasets of annotated images produced by humans. Acquiring this data is a costly and time-consuming process, requiring individuals to label every aspect of every object in each scene in the dataset. As a result, often only a small subset of a scene’s overall contents is captured, which limits the artificial vision systems trained on that data. As we develop more complex machines that operate in the real world, we want them to fully understand their surroundings: where is the nearest surface to sit on? What material is the sofa made of? Which light source is creating all the shadows? Where is the light switch likely to be? In this work, published in Science ( Open Access version ), we introduce the Generative Query Network (GQN), a framework within which machines learn to perceive their surroundings by training only on data obtained by themselves as they move around scenes. Much like infants and animals, the GQN learns by trying to make sense of its observations of the world around it. In doing so, the GQN learns about plausible scenes and their geometrical properties, without any human labelling of the contents of scenes. The GQN model is composed of two parts: a representation network and a generation network. The representation network takes the agent's observations as its input and produces a representation (a vector) which describes the underlying scene. The generation network then predicts (‘imagines’) the scene from a previously unobserved viewpoint. The representation network does not know which viewpoints the generation network will be asked to predict, so it must find an efficient way of describing the true layout of the scene as accurately as possible. It does this by capturing the most important elements, such as object positions, colours and the room layout, in a concise distributed representation. During training, the generator learns about typical objects, features, relationships and regularities in the environment. This shared set of ‘concepts’ enables the representation network to describe the scene in a highly compressed, abstract manner, leaving it to the generation network to fill in the details where necessary. For instance, the representation network will succinctly represent ‘blue cube’ as a small set of numbers and the generation network will know how that manifests itself as pixels from a particular viewpoint. We performed controlled experiments on the GQN in a collection of procedurally-generated environments in a simulated 3D world, containing multiple objects in random positions, colours, shapes and textures, with randomised light sources and heavy occlusion. After training on these environments, we used GQN’s representation network to form representations of new, previously unobserved scenes. We showed in our experiments that the GQN exhibits several important properties: GQN builds upon a large literature of recent related work in multi-view geometry, generative modelling, unsupervised learning and predictive learning, which we discuss here , in the Science paper and the Open Access version . It illustrates a novel way to learn compact, grounded representations of physical scenes. Crucially, the proposed approach does not require domain-specific engineering or time-consuming labelling of the contents of scenes, allowing the same model to be applied to a range of different environments. It also learns a powerful neural renderer that is capable of producing accurate images of scenes from new viewpoints. Our method still has many limitations when compared to more traditional computer vision techniques, and has currently only been trained to work on synthetic scenes. However, as new sources of data become available and advances are made in our hardware capabilities, we expect to be able to investigate the application of the GQN framework to higher resolution images of real scenes. In future work, it will also be important to explore the application of GQNs to broader aspects of scene understanding, for example by querying across space and time to learn a common sense notion of physics and movement, as well as applications in virtual and augmented reality. While there is still much more research to be done before our approach is ready to be deployed in practice, we believe this work is a sizeable step towards fully autonomous scene understanding. This work was done by S. M. Ali Eslami, Danilo J. Rezende, Frederic Besse, Fabio Viola, Ari S. Morcos, Marta Garnelo, Avraham Ruderman, Andrei A. Rusu, Ivo Danihelka, Karol Gregor, David P. Reichert, Lars Buesing, Theophane Weber, Oriol Vinyals, Dan Rosenbaum, Neil Rabinowitz, Helen King, Chloe Hillier, Matt Botvinick, Daan Wierstra, Koray Kavukcuoglu and Demis Hassabis.", "date": "2018-06-14"},
{"website": "Deepmind", "title": "AlphaZero: Shedding new light on chess, shogi, and Go", "author": [" David Silver ", " Thomas Hubert ", " Julian Schrittwieser ", " Demis Hassabis "], "link": "https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go", "abstract": "In late 2017 we introduced AlphaZero , a single system that taught itself from scratch how to master the games of chess, shogi (Japanese chess), and Go , beating a world-champion program in each case. We were excited by the preliminary results and thrilled to see the response from members of the chess community, who saw in AlphaZero’s games a ground-breaking, highly dynamic and “ unconventional ” style of play that differed from any chess playing engine that came before it. Today, we are delighted to introduce the full evaluation of AlphaZero, published in the journal Science ( Open Access version here ), that confirms and updates those preliminary results. It describes how AlphaZero quickly learns each game to become the strongest player in history for each, despite starting its training from random play, with no in-built domain knowledge but the basic rules of the game. This ability to learn each game afresh, unconstrained by the norms of human play, results in a distinctive, unorthodox, yet creative and dynamic playing style. Chess Grandmaster Matthew Sadler and Women’s International Master Natasha Regan, who have analysed thousands of AlphaZero’s chess games for their forthcoming book Game Changer (New in Chess, January 2019), say its style is unlike any traditional chess engine.” It’s like discovering the secret notebooks of some great player from the past,” says Matthew. Traditional chess engines  – including the world computer chess champion Stockfish and IBM’s ground-breaking Deep Blue – rely on thousands of rules and heuristics handcrafted by strong human players that try to account for every eventuality in a game. Shogi programs are also game specific, using similar search engines and algorithms to chess programs. AlphaZero takes a totally different approach, replacing these hand-crafted rules with a deep neural network and general purpose algorithms that know nothing about the game beyond the basic rules. To learn each game, an untrained neural network plays millions of games against itself via a process of trial and error called reinforcement learning . At first, it plays completely randomly, but over time the system learns from wins, losses, and draws to adjust the parameters of the neural network, making it more likely to choose advantageous moves in the future. The amount of training the network needs depends on the style and complexity of the game, taking approximately 9 hours for chess, 12 hours for shogi, and 13 days for Go. The trained network is used to guide a search algorithm – known as Monte-Carlo Tree Search (MCTS) – to select the most promising moves in games. For each move, AlphaZero searches only a small fraction of the positions considered by traditional chess engines. In Chess, for example, it searches only 60 thousand positions per second in chess, compared to roughly 60 million for Stockfish. The fully trained systems were tested against the strongest hand-crafted engines for chess ( Stockfish ) and shogi ( Elmo ), along with our previous self-taught system AlphaGo Zero , the strongest Go player known. In each evaluation, AlphaZero convincingly beat its opponent: However, it was the style in which AlphaZero plays these games that players may find most fascinating. In Chess, for example, AlphaZero independently discovered and played common human motifs during its self-play training such as openings, king safety and pawn structure. But, being self-taught and therefore unconstrained by conventional wisdom about the game, it also developed its own intuitions and strategies adding a new and expansive set of exciting and novel ideas that augment centuries of thinking about chess strategy. The first thing that players will notice is AlphaZero's style, says Matthew Sadler – “the way its pieces swarm around the opponent’s king with purpose and power”. Underpinning that, he says, is AlphaZero’s highly dynamic game play that maximises the activity and mobility of its own pieces while minimising the activity and mobility of its opponent’s pieces. Counterintuitively, AlphaZero also seems to place less value on “material”, an idea that underpins the modern game where each piece has a value and if one player has a greater value of pieces on the board than the other, then they have a material advantage. Instead, AlphaZero is willing to sacrifice material early in a game for gains that will only be recouped in the long-term. “Impressively, it manages to impose its style of play across a very wide range of positions and openings,” says Matthew, who also observes that it plays in a very deliberate style from its first move with a “very human sense of consistent purpose”. “Traditional engines are exceptionally strong and make few obvious mistakes, but can drift when faced with positions with no concrete and calculable solution,” he says. “It's precisely in such positions where ‘feeling’, ‘insight’ or ‘intuition’ is required that AlphaZero comes into its own.\" This unique ability, not seen in other traditional chess engines, has already been harnessed to give chess fans fresh insight and commentary on the recent World Chess Championship match between Magnus Carlsen and Fabiano Caruana and will be explored further in Game Changer . “It was fascinating to see how AlphaZero's analysis differed from that of top chess engines and even top grandmaster play,” says Natasha Regan. \"AlphaZero could be a powerful teaching tool for the whole community.\" AlphaZero’s teachings echo what we saw when AlphaGo played the legendary champion Lee Sedol in 2016. During the games , AlphaGo played a number of highly inventive winning moves, including move 37 in game two, which overturned hundreds of years of thinking. These moves - and many others - have since been studied by players at all levels including Lee Sedol himself, who said of Move 37: “I thought AlphaGo was based on probability calculation and it was merely a machine. But when I saw this move I changed my mind. Surely AlphaGo is creative.” As with Go, we are excited about AlphaZero’s creative response to chess, which has been a grand challenge for artificial intelligence since the dawn of the computing age with early pioneers including Babbage, Turing, Shannon, and von Neumann all trying their hand at designing chess programs. But AlphaZero is about more than chess, shogi or Go. To create intelligent systems capable of solving a wide range of real-world problems we need them to be flexible and generalise to new situations. While there has been some progress towards this goal, it remains a major challenge in AI research with systems capable of mastering specific skills to a very high standard, but often failing when presented with even slightly modified tasks. AlphaZero’s ability to master three different complex games – and potentially any perfect information game – is an important step towards overcoming this problem. It demonstrates that a single algorithm can learn how to discover new knowledge in a range of settings. And, while it is still early days, AlphaZero’s creative insights coupled with the encouraging results we see in other projects such as AlphaFold ,  give us confidence in our mission to create general purpose learning systems that will one day help us find novel solutions to some of the most important and complex scientific problems. This work was done by David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, and Demis Hassabis.", "date": "2018-12-06"},
{"website": "Deepmind", "title": "DeepMind papers at NIPS 2017", "author": "Unknown", "link": "https://deepmind.com/blog/article/deepmind-papers-nips-2017", "abstract": "Between 04-09 December, thousands of researchers and experts will gather for the Thirty-first Annual Conference on Neural Information Processing Systems (NIPS) in Long Beach, California. Here you will find an overview of the papers DeepMind researchers will present. Authors: Ziyu Wang, Josh Merel, Greg Wayne, Nando de Freitas, Scott Reed, Nicolas Heess “We propose a neural network architecture, building on state-of-the-art generative models, that is capable of learning the relationships between different behaviours and imitating specific actions that it is shown. After training, our system can encode a single observed action and create a new novel movement based on that demonstration. It can also switch between different kinds of behaviours despite never having seen transitions between them, for example switching between walking styles.” Read more on the blog Authors: Wojtek Czarnecki, Simon Osindero, Max Jaderberg, Grzegorz Świrszcz, Razvan Pascanu This paper shows a simple way of incorporating knowledge about target function derivatives into the training of deep neural networks. We prove that modern ReLU-based architectures are well suited for such tasks, and evaluate their effectiveness on three problems - low-dimensional regression, policy distillation, and training with synthetic gradients. We observe a significant boost in training efficiency, especially in low-data regimes, and train the first synthetic gradient-based ImageNet model with near state-of-the-art accuracy. Authors: Chris J. Maddison, Dieterich Lawson, George Tucker, Nicolas Heess, Mohammad Norouzi, Andriy Mnih, Arnaud Doucet, Yee Whye Teh We consider the extension of the variational lower bound to a family of lower bounds defined by a particle filter's estimator of the marginal likelihood - the filtering variational objectives. These filtering objectives can exploit a model's sequential structure to form tighter bounds and better objectives for model learning in deep generative models. In our experiments, we find that training with filtering objectives results in substantial improvements over training the same model architecture with the variational lower bound. Authors: Nicholas Watters, Andrea Tacchetti, Theophane Weber, Razvan Pascanu, Peter Battaglia, Daniel Zoran “ In this work we developed the “Visual Interaction Network” (VIN), a neural network-based model that learns physical dynamics without prior knowledge. The VIN is able to infer the states of multiple physical objects from just a few frames of video, and then use these to predict object positions many steps into the future. It is also able to infer the locations of invisible objects and learn dynamics that depend on object attributes such as mass.” Read the blog for further detail. Authors: Aäron van den Oord, Oriol Vinyals, Koray Kavukcuoglu Learning useful representations without supervision remains a key challenge in machine learning. In this work we propose a simple yet powerful generative model - known as the Vector Quantised Variational AutoEconder (VQ-VAE) -  that learns such discrete representations. When these representations are paired with an autoregressive prior, the model is able to generate high quality images, videos and speech as well as doing high-quality speaker conversion. Authors: Jörg Bornschein, Andriy Mnih, Daniel Zoran, Danilo Jimenez Rezende Attention based memory can be used to augment neural networks to support few-shot learning, rapid adaptability and more generally to support non-parametric extensions. Instead of using the popular differentiable soft-attention mechanism, we propose the use of stochastic hard-attention to retrieve memory content in generative models. This allows us to apply variational inference to memory addressing, which enables us to get significantly more precise memory lookups using target information, especially in models with large memory buffers and with many confounding entries in the memory. Authors: George Tucker, Andriy Mnih, Chris J Maddison, Dieterich Lawson, Jascha Sohl-Dickstein Learning in models with discrete latent variables is challenging due to high-variance gradient estimators. Previous approaches either produced high-variance, unbiased gradients or low-variance, biased gradients. REBAR uses control variates and the reparameterization trick to get the best of both: low-variance, unbiased gradients that result in faster convergence to a better result. Authors : Sébastien Racanière, Théophane Weber, David P. Reichert, Lars Buesing, Arthur Guez, Danilo Rezende, Adria Puigdomènech Badia, Oriol Vinyals, Nicolas Heess, Yujia Li, Razvan Pascanu, Peter Battaglia, Demis Hassabis, David Silver, Daan Wierstra. “We describe a new family of approaches for imagination-based planning...We also introduce architectures which provide new ways for agents to learn and construct plans to maximise the efficiency of a task. These architectures are efficient, robust to complex and imperfect models, and can adopt flexible strategies for exploiting their imagination. The agents we introduce benefit from an ‘imagination encoder’- a neural network which learns to extract any information useful for the agent’s future decisions, but ignore that which is not relevant.”  Read more on the blog . Authors: Adam Santoro, David Raposo, David Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, Timothy Lillicrap “We demonstrate the use of a simple, plug-and-play neural network module for solving tasks that demand complex relational reasoning. This module, called a Relation Network, can receive unstructured inputs - say, images or stories - and implicitly reason about the relations contained within.”  Read more on the blog . Authors: Balaji Lakshminarayanan, Alexander Pritzel, Charles Blundell Quantifying predictive uncertainty in neural networks (NNs) is a challenging and yet unsolved problem. The majority of work is focused on Bayesian solutions, however these are computationally intensive and require significant modifications to the training pipeline. We propose an alternative to Bayesian NNs that is simple to implement, readily parallelisable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs. Authors: Zhongwen Xu, Joseph Modayil, Hado van Hasselt, Andre Barreto, David Silver, Tom Schaul We revisit the structure of value approximators for RL, based on the observation that typical approximators smoothly change as a function of input, but the true value changes abruptly when a reward arrives. Our proposed method is designed to fit such asymmetric discontinuities using interpolation with a projected value estimate. Authors: Andre Barreto, Will Dabney, Remi Munos, Jonathan Hunt, Tom Schaul, David Silver, Hado van Hasselt. We propose a transfer framework for reinforcement learning. Our approach rests on two key ideas: \"successor features\", a value function representation that decouples the dynamics of the environment from the rewards, and \"generalised policy improvement\", a generalisation of dynamic programming’s policy improvement step that considers a set of policies rather than a single one. Put together, the two ideas lead to an approach that integrates seamlessly within the reinforcement learning framework and allows transfer to take place between tasks without any restriction. Authors: Paul Christiano (Open AI), Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, Dario Amodei (Open AI) “A central question in technical AI safety is how to tell an algorithm what we want it to do. Working with OpenAI, we demonstrate a novel system that allows a human with no technical experience to teach an AI how to perform a complex task, such as manipulating a simulated robotic arm.” Read more on the blog . Author: Julien Perolat, Joel Z Leibo, Vinicius Zambaldi, Charles Beattie, Karl Tuyls, Thore Graepel This paper looks at the complexity of problems of common-pool resource appropriation. These include systems such as fisheries, grazing pastures or access to  freshwater, where lots of people or actors have access to the same resource. Traditional models from the social sciences tend to suggest that parties with access to the resource act in a self-interested way, eventually leading to an unsustainable depletion of resources. However, we know from human societies that there is a wide range of possible outcomes. Sometimes resources like fisheries are overexploited and sometimes they are harvested sustainably. In this work we propose new modeling techniques that can be used in research aimed at explaining this gap between what we observe in the real world and what traditional models predict. Authors: Yee Whye Teh, Victor Bapst, Wojciech Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell, Nicholas Heess, Razvan Pascanu We develop a method for doing reinforcement learning on multiple tasks. The assumption is that the tasks are related to each other (e.g. being in the same environment or having the same physics) and so good action sequences tend to recur across tasks. Our method achieves this by simultaneously distilling task-specific policies into a common default policy, and transferring this common knowledge across tasks by regularising all task-specific policies towards the default policy.  We show that this leads to faster and more robust learning. Authors: Marc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien Perolat, David Silver, Thore Graepel In this work, we first observe that independent reinforcement learners produce policies that can be jointly correlated, failing to generalize well during execution with other agents. We quantify this effect by proposing a new metric called joint policy correlation. We then propose an algorithm motivated by game-theoretic foundations, which generalises several previous approaches such as fictitious play, iterated best response, independent RL, and double oracle. We show that our algorithm can reduce joint policy correlation significantly in first-person coordination games, and finds robust counter-strategies in a common poker benchmark game. Our researchers will also lead and take part in a wide-range of workshops, tutorials and symposia during NIPS.  For the full schedule, including details of papers that we have collaborated on, please download our itinerary (PDF) or visit the official website .", "date": "2017-12-01"},
{"website": "Deepmind", "title": "Interpreting Deep Neural Networks using Cognitive Psychology", "author": [" David Barrett ", " Samuel Ritter "], "link": "https://deepmind.com/blog/article/cognitive-psychology", "abstract": "Deep neural networks have learnt to do an amazing array of tasks - from recognising and reasoning about objects in images to playing Atari and Go at super-human levels. As these tasks and network architectures become more complex, the solutions that neural networks learn become more difficult to understand. This is known as the ‘black-box’ problem, and it is becoming increasingly important as neural networks are used in more and more real world applications. At DeepMind, we are working to expand the toolkit for understanding and interpreting these systems. In our latest paper , recently accepted at ICML, we proposed a new approach to this problem that employs methods from cognitive psychology to understand deep neural networks. Cognitive psychology measures behaviour to infer mechanisms of cognition, and contains a vast literature detailing such mechanisms, along with experiments for verifying them. As our neural networks approach human level performance on specific tasks, methods from cognitive psychology are becoming increasingly relevant to the black-box problem. To demonstrate this point, our paper reports a case study where we used an experiment designed to elucidate human cognition to help us understand how deep networks solve an image classification task. Our results showed that behaviours observed by cognitive psychologists in humans are also displayed by these deep networks. Further, the results revealed useful and surprising insights about how the networks solve the classification task. More generally, the success of the case study demonstrated the potential of using cognitive psychology to understand deep learning systems. In our case study, we considered how children recognise and label objects - a rich area of study in developmental cognitive psychology. The ability of children to guess the meaning of a word from a single example - so-called ‘one-shot word learning’ - happens with such ease that it is tempting to think it is a simple process. However, a classic thought experiment from the philosopher Willard Van Orman Quine illustrates just how complex this really is: A field linguist has gone to visit a culture whose language is entirely different from our own. The linguist is trying to learn some words from a helpful native speaker, when a rabbit scurries by. The native speaker declares “gavagai”, and the linguist is left to infer the meaning of this new word. The linguist is faced with an abundance of possible inferences, including that “gavagai” refers to rabbits, animals, white things, that specific rabbit, or “undetached parts of rabbits”. There is an infinity of possible inferences to be made. How are people able to choose the correct one? Fifty years later, we are confronted with the same question about deep neural networks that can do one-shot learning. Consider the Matching Network , a neural network developed by our colleagues at DeepMind. This model uses recent advances in attention and memory to achieve state-of-the-art performance classifying ImageNet images using only a single example from a class. However, we do not know what assumptions the network is making to classify these images. To shed light on this, we looked to the work of developmental psychologists (1-4) who found evidence that children find the correct inferences by applying inductive biases to eliminate many of the incorrect inferences. Such biases include: whole object bias , by which children assume that a word  refers  to  an  entire  object  and  not  its components (eliminating Quine’s concern about undetached rabbit parts) taxonomic bias , by which children assume that a word refers to the basic level category an object belongs to (quelling Quine’s fears that all animals might be chosen as the meaning of “rabbit”) shape bias , by which children assume the meaning of a noun is based on object shape rather than colour or texture (relieving Quine’s anxiety that all white things might be assigned as the meaning of “rabbit”) We chose to measure the shape bias of our neural networks because there is a particularly large body of work studying this bias in humans. The classic shape bias experiment that we adopted proceeds as follows: we present our deep networks with images of three objects: a probe object, a shape-match object (which is similar to the probe in shape but not in colour), and a colour-match object (which is similar to the probe in colour but not in shape). We then measure the shape bias as the proportion of times that the probe image is assigned the same label as the shape-match image instead of the colour-match image. We used images of objects used in human experiments in the Cognitive Development Lab at Indiana University. We tried this experiment with our deep networks (Matching Networks and an Inception baseline model) and found that - like humans - our networks have a strong bias towards object shape rather than colour or texture. In other words, they have a ‘shape bias’. This suggests that Matching Networks and the Inception classifier use an inductive bias for shape to eliminate incorrect hypotheses, giving us a clear insight into how these networks solve the one-shot word learning problem. The observation of shape bias wasn’t our only interesting finding: We observed that the shape bias emerges gradually over the course of early training in our networks. This is reminiscent of the emergence of shape bias in humans: young children show smaller shape bias than older children, and adults show the largest bias (2). We found that there are different levels of bias in our networks depending on the random seed used for initialisation and training. This taught us that we must use a large sample of trained models to draw valid conclusions when experimenting with deep learning systems, just as psychologists have learnt not to make a conclusion based on a single subject. We found that networks achieve the same one shot learning performance even when the shape bias is very different, demonstrating that different networks can find a variety of equally effective solutions to a complex problem. The discovery of this previously unrecognised bias in standard neural network architectures illustrates the potential of using artificial cognitive psychology for interpreting neural network solutions. In other domains, insights from the episodic memory literature may be useful for understanding episodic memory architectures, and techniques from the semantic cognition literature may be useful for understanding recent models of concept formation. The psychological literature is rich in these and other areas, giving us powerful new tools to address the ‘black box’ problem and to more deeply understand the behaviour of our neural networks. This work was done by Sam Ritter*, David G.T. Barrett*, Adam Santoro and Matt M. Botvinick Read Cognitive Psychology for Deep Neural Networks: A Shape Bias Case Study References (1 ) Markman, Ellen M. Constraints children place on word meanings. Cognitive Science, 14(1):57–77, 1990. (2) Landau, Barbara, Smith, Linda B, and Jones, Susan S. The importance of shape in early lexical learning. Cognitive Development, 3(3):299–321, 1988. (3) Markman, Ellen M and Hutchinson, Jean E. Children’s sensitivity to constraints on word meaning: Taxonomic versus thematic relations.Cognitive Psychology, 16(1):1–27, 1984. (4) Markman, Ellen M and Wachtel, Gwyn F. Children’s use of mutual exclusivity to constrain the meanings of words. Cognitive Psychology, 20(2):121–157, 1988.", "date": "2017-06-27"},
{"website": "Deepmind", "title": "Enhancing patient safety at Taunton and Somerset NHS Foundation Trust", "author": "Unknown", "link": "https://deepmind.com/blog/article/taunton-and-somerset-nhs-foundation-trust-partnership", "abstract": "We’re delighted to announce our first partnership outside of London to help doctors and nurses break new ground in the NHS’s use of digital technology. Streams is our secure mobile app that helps doctors and nurses give faster urgent care to patients showing signs of deterioration by giving them the right information more quickly. Over the next five years, we’ll be rolling it out at Taunton and Somerset NHS Foundation Trust as part of a new partnership. You can find out more on the trust’s website . Our collaboration with Taunton and Somerset follows on from our work with Imperial College Healthcare NHS Trust and the Royal Free NHS Foundation Trust. Nurses already using Streams at the Royal Free tell us that the app is saving them up to two hours a day, allowing them to redirect valuable time back into targeted patient care. Where some current systems can take hours, Streams uses ‘breaking news’ style alerts to notify clinicians within seconds when a test results indicates that one of their patients shows signs of becoming ill. Once they have received an alert, they can use the app to view important test results and communicate securely with their colleagues, to ensure their patients get the right treatment as quickly as possible. At Musgrove Park Hospital, part of Taunton and Somerset NHS Foundation Trust, these features will alert doctors and nurses to a potential deterioration in their patients’ vital signs that could indicate a serious problem. We believe that by making it as quick and easy as possible for clinicians to intervene if something is wrong, we’ll be able to improve patient safety across the hospital. Streams has already had a promising impact for both patients and clinicians at our existing partner sites, and has been credited with helping deliver faster care to patients who become critically ill. One patient who has benefited from Streams at the Royal Free was Afia Ahmed , who received quick treatment by doctors who were alerted to her deterioration when she became seriously ill after giving birth. Netty Messenger, a patient and volunteer at Musgrove Park Hospital, welcomed the trust’s progress towards mobile working: “This is a great idea. I do banking and shopping online and get my prescriptions online, but hospitals still seem to have mountains of paper. It would be much better to have all the patient’s information in one place in an app like this. “If doctors are able to pay more attention to their patients instead of having to chase up test results, that has got to be a very good thing.” Dr Dominic King, clinical lead at DeepMind Health, said: “Nurses and doctors already using Streams are telling us that it is helping them deliver faster and better care for their patients. The Taunton and Somerset NHS Foundation Trust is well known for its pioneering approach to healthcare technology, so it’s incredibly exciting to be working with the outstanding clinical team there, on the shared goal of improving outcomes for patients.” Tom Edwards, a consultant surgeon at the trust, said: “Fast access to information about patients is absolutely crucial for our doctors, nurses, and other clinical staff. “Safety alerts will be immensely useful, but it is important to remember that – whatever technology we use - it will still be our highly trained and expert staff who are making decisions about diagnosis, treatment and patient care.” As one of the NHS’s 16 global digital exemplar acute trusts, Taunton and Somerset aims to be one of the most innovative hospital trusts in the NHS. Over the next five years, we’ll be working with the trust and their long-term partner in the digital exemplar programme, IMS MAXIMS, to roll out this cutting-edge technology for a range of medical conditions where early intervention can make all the difference. As in all our Streams partnerships, we’re putting patients at the heart of this work. In addition to their regular forums for updating patients and patient governors on their innovative work, over the coming weeks the trust will be hosting workshops, displays and open day events so that staff, patients and the public can see how the app works, what it will mean for patients, and how it might be developed in future. In addition, members of the DeepMind team will be working alongside Taunton staff to engage patients, giving people an opportunity to ask questions about Streams and any other aspect of our work. These events will take place before any patient data is processed by DeepMind. All patient data will be stored to world-leading standards of security and encryption in a facility in England, separated at all times from any other systems.", "date": "2017-06-21"},
{"website": "Deepmind", "title": "Objects that Sound", "author": [" Relja Arandjelović ", " Andrew Zisserman "], "link": "https://deepmind.com/blog/article/objects-that-sound", "abstract": "Visual and audio events tend to occur together: a musician plucking guitar strings and the resulting melody; a wine glass shattering and the accompanying crash; the roar of a motorcycle as it accelerates. These visual and audio stimuli are concurrent because they share a common cause. Understanding the relationship between visual events and their associated sounds is a fundamental way that we make sense of the world around us. In Look, Listen, and Learn and Objects that Sound (to appear at ECCV 2018 ), we explore this observation by asking: what can be learnt by looking at and listening to a large number of unlabelled videos? By constructing an audio-visual correspondence learning task that enables visual and audio networks to be jointly trained from scratch, we demonstrate that: Learning from multiple modalities is not new; historically, researchers have largely focused on image-text or audio-vision pairings. However, a common approach has been to train a “student” network in one modality using the automatic supervision provided by a “teacher” network in the other modality (“ teacher-student supervision ”), where the “teacher” has been trained using a large number of human annotations. For instance, a vision network trained on ImageNet can be used to annotate frames of a YouTube video as “acoustic guitar”, which provides training data to the “student” audio network for learning what an “acoustic guitar” sounds like. In contrast, we train both visual and audio networks from scratch, where the concept of the “acoustic guitar” naturally emerges in both modalities. Somewhat surprisingly, this approach achieves superior audio classification compared to teacher-student supervision. As described below, this also equips us to localise the object making the sound, which was not possible with previous approaches. Our core idea is to use a valuable source of information contained in the video itself: the correspondence between visual and audio streams available by virtue of them appearing together at the same time in the same video. By seeing and hearing many examples of a person playing a violin and examples of a dog barking, and rarely or never seeing a violin being played while hearing a dog bark and vice versa, it should be possible to conclude what a violin and a dog look and sound like. This approach is, in part, motivated by the way an infant might learn about the world as their visual and audio capabilities develop. We apply learning by audio-visual correspondence (AVC), a simple binary classification task: given an example video frame and a short audio clip, decide whether they correspond to each other or not. The only way for a system to solve this task is by learning to detect various semantic concepts in both the visual and the audio domain. To tackle the AVC task, we propose the following network architecture The image and the audio subnetworks extract visual and audio embeddings and the correspondence score is computed as a function of the distance between the two embeddings. If the embeddings are similar, the (image, audio) are deemed to correspond. We show that the networks learn useful semantic representations, as, for example, our audio network sets the new state-of-the-art on two sound classification benchmarks. Since the correspondence score is computed purely based on the distance, the two embeddings are forced to be aligned (i.e. the vectors live in the same space, and so can be compared meaningfully), thus facilitating cross-modal retrieval: The AVE-Net recognises semantic concepts in the audio and visual domains, but it cannot answer the question, “Where is the object that is making the sound?” We again make use of the AVC task and show that it is possible to learn to localise sounding objects, while still not using any labels whatsoever. To localise a sound in the image, we compute the correspondence scores between the audio embedding and a grid of region-level image descriptors. The network is trained with multiple instance learning – the image-level correspondence score is computed as the maximum of the correspondence score map: For corresponding (image, audio) pairs, the method encourages at least one region to respond highly and therefore localise the object. In the below video (left - input frame, right - localisation output, middle - overlay), frames are processed completely independently – motion information is not used, and there is no temporal smoothing: For mismatched pairs the maximal score should be low, thus making the entire score map dark, indicating, as desired, there is no object which makes the input sound: The unsupervised audio-visual correspondence task enables, with appropriate network design, two entirely new functionalities to be learnt: cross-modal retrieval, and semantic-based localisation of objects that sound. Furthermore, it facilitates learning of powerful features, setting the new state-of-the-art on two sound classification benchmarks. These techniques may prove useful in reinforcement learning, enabling agents to make use of large amounts of unlabelled sensory information. Our work may also have implications for other multimodal problems beyond audio-visual tasks in the future. Read the full papers: Look, Listen, and Learn Objects that Sound This work was done by Relja Arandjelović and Andrew Zisserman. Graphics by Adam Cain and Damien Boudot.", "date": "2018-08-06"},
{"website": "Deepmind", "title": "DeepMind papers at ICML 2017 (part three)", "author": "Unknown", "link": "https://deepmind.com/blog/article/icml-round-papers-part-three", "abstract": "The final part of our three-part series that gives an overview of the papers we are presenting at the ICML 2017 Conference in Sydney, Australia. Authors: Samuel Ritter*, David Barrett*, Adam Santoro, Matt Botvinick Deep neural networks (DNNs) have achieved unprecedented performance on a wide range of tasks, rapidly outpacing our understanding of the nature of their solutions. In this work, we propose to address this interpretability problem in modern DNNs using the problem descriptions, theories and experimental methods developed of cognitive psychology. In a case study, we apply a theory and method from the psychology of human word learning to better understand how modern one-shot learning systems work. Results revealed not only that our DNNs exhibit the same inductive bias as humans, but also several unexpected features of the DNNs. For further details and related work, please see the paper . Check it out at ICML: Tuesday 08 August, 15:48-16:06 @ Darling Harbour Theatre (Talk) Tuesday 08 August, 18:30-20:00 @ Gallery #113 (Poster) Authors: Georg Ostrovski, Marc Bellemare, Aaron van den Oord, Remi Munos Count-based exploration based on prediction gain of a simple graphical density model has previously achieved  state-of-the-art results on some of the hardest exploration games in Atari. We investigate the open questions 1) whether a better density model leads to better exploration, and 2) what role the mixed Monte Carlo update rule used in this work plays for exploration. We show that a neural density model - PixelCNN - can be trained online on the experience stream of an RL agent and used for count-based exploration to achieve even better results on a wider set of hard exploration games, while preserving higher performance on easy exploration games. We also show that the Monte Carlo return is crucial in making use of the intrinsic reward signal in the sparsest reward settings, and cannot easily be replaced by a softer lambda-return update rule. For further details and related work, please see the paper . Check it out at ICML: Wednesday 09 August, 13:30-13:48 @ C4.5 (Talk) Wednesday 09 August, 18:30-22:00 @ Gallery #64 (Poster) Authors: David Silver, Hado van Hasselt, Matteo Hessel, Tom Schaul, Arthur Guez, Tim Harley, Gabriel Dulac-Arnold, David Reichert, Neil Rabinowitz, Andre Barreto, Thomas Degris One of the key challenges of artificial intelligence is to learn models that are effective in the context of planning. In this document we introduce the predictron architecture. The predictron consists of a fully abstract model, represented by a Markov reward process, that can be rolled forward multiple “imagined\" planning steps. Each forward pass of the predictron accumulates internal rewards and values over multiple planning depths. The predictron is trained end-to-end so as to make these accumulated values accurately approximate the true value function. We applied the predictron to procedurally generated random mazes and a simulator for the game of pool. The predictron yielded significantly more accurate predictions than conventional deep neural network architectures. For further details and related work, please see the paper . Check it out at ICML: Wednesday 09 August, 14:24-14:42 @ C4.5 (Talk) Wednesday 09 August 18:30-20:00 @ Gallery #91 (Poster) Authors: Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Hees, Max Jaderberg, David Silver, Koray Kavukcuoglu How to create agents that can learn to decompose their behaviour into meaningful primitives and then reuse them to more efficiently acquire new behaviours is a long standing research question. The solution to this question may be an important stepping stone towards agents with general intelligence and competence. This paper introduced FeUdal Networks (FuN), a novel architecture that formulates sub-goals as directions in latent state space, which, if followed, translates into a meaningful behavioural primitives. FuN clearly separates the module that discovers and sets sub-goals from the module that generates behaviour through primitive actions. This creates a natural hierarchy that is stable and allows both modules to learn in complementary ways. Our experiments clearly demonstrate that this makes long-term credit assignment and memorisation more tractable. This also opens many avenues for further research, for instance: deeper hierarchies can be constructed by setting goals at multiple time scales, scaling agents to truly large environments with sparse rewards and partial observability. For further details and related work, please see the paper . Check it out at ICML: Wednesday 09 August, 15:30-15:48 @ C4.5 (Talk) Wednesday 09 August, 18:30-20:00  @ Gallery #107 (Poster) Authors: Alex Pritzel, Benigno Uria, Sriram Srinivasan, Adria Puigdomenech, Oriol Vinyals, Demis Hassabis, Daan Wierstra, Charles Blundell Deep reinforcement learning algorithms have achieved state of the art performance on a variety of tasks, however they tend to be grossly data inefficient. In this work we propose a novel algorithm that allows rapid incorporation of new information collected by the agent. For this we introduce a new differentiable data structure, a differentiable neural dictionary, that can incorporate new information immediately, while being able to update it’s internal representation based on the task the algorithm is supposed to solve. Our agent, Neural Episodic Control, is built on top of the differentiable data structure and is able to learn significantly faster across a wide range of environments. For further details and related work, please see the paper . Check it out at ICML: Wednesday 09 August, 16:06-16:24 @ C4.5 Wednesday 09 August, 18:30-22:00 @ Gallery #125 Authors: Justin Gilmer (Google Brain), Sam Schoenholz (Google Brain), Patrick Riley (Google Google), Oriol Vinyals, George Dahl (Google Brain) In this work we show how we can gain orders of magnitude  improvements to run-time performance by treating an expensive simulation of quantum chemistry properties as a supervised dataset to be learnt by extending neural networks to operate on graphs. Our model is extremely accurate and very fast. In the manuscript we also provide a unifying framework which summarises previous work on graph-shaped inputs and neural networks. For further details and related work, please see the paper . Check it out at ICML: Wednesday 09 August, 16:24-16:42 @ Darling Harbour Theatre (Talk) Wednesday 09 August, 18:30-22:00 @ Gallery #131 (Poster) Read: ' DeepMind papers at ICML 2017 (part one) ' and ' DeepMind papers at ICML 2017 (part two)", "date": "2017-08-04"},
{"website": "Deepmind", "title": "Imagine this: Creating new visual concepts by recombining familiar ones", "author": [" Alexander Lerchner ", " Irina Higgins ", " Matt Botvinick "], "link": "https://deepmind.com/blog/article/imagine-creating-new-visual-concepts-recombining-familiar-ones", "abstract": "Around two and a half thousand years ago a Mesopotamian trader gathered some clay, wood and reeds and changed humanity forever. Over time, their abacus would allow traders to keep track of goods and reconcile their finances, allowing economics to flourish. But that moment of inspiration also shines a light on another astonishing human ability: our ability to recombine existing concepts and imagine something entirely new. The unknown inventor would have had to think of the problem they wanted to solve, the contraption they could build and the raw materials they could gather to create it. Clay could be moulded into a tablet, a stick could be used to scratch the columns and reeds can act as counters. Each component was familiar and distinct, but put together in this new way, they formed something revolutionary. This idea of “compositionality” is at the core of human abilities such as creativity, imagination and language-based communication. Equipped with just a small number of familiar conceptual building blocks, we are able to create a vast number of new ones on the fly. We do this naturally by placing concepts in hierarchies that run from specific to more general and then recombining different parts of the hierarchy in novel ways. But what comes so naturally to us, remains a challenge in AI research. In our new paper , we propose a novel theoretical approach to address this problem. We also demonstrate a new neural network component called the Symbol-Concept Association Network (SCAN), that can, for the first time, learn a grounded visual concept hierarchy in a way that mimics human vision and word acquisition, enabling it to imagine novel concepts guided by language instructions. Our approach can be summarised as follows: Our approach differs from previous work in this area because it is fully grounded in the sensory data and learns from very few image-word pairs. While other deep learning approaches require thousands of image examples to learn a concept, SCAN learns both the visual primitives and conceptual abstractions primarily from unsupervised observations and as few as five pairs of an image and label per concept. Once trained, SCAN can then generate a diverse list of concepts that correspond to a particular image, and imagine diverse visual examples that correspond to a particular concept, even if it has never experienced the concept before. This ability to learn new concepts by recombining existing ones through symbolic instructions has given humans astonishing abilities, allowing us to reason about abstract concepts like the universe, humanism or - as was the case in Mesopotamia - economics. While our algorithms have a long way to go before they can make such conceptual leaps, this work demonstrates a first step towards having algorithms that can learn in a largely unsupervised way, and think about conceptual abstractions like those used by humans. Read: SCAN: Learning Abstract Hierarchical Compositional Visual Concepts", "date": "2017-07-12"},
{"website": "Deepmind", "title": "Population based training of neural networks", "author": [" Max Jaderberg "], "link": "https://deepmind.com/blog/article/population-based-training-neural-networks", "abstract": "Neural networks have shown great success in everything from playing Go and Atari games to image recognition and language translation. But often overlooked is that the success of a neural network at a particular application is often determined by a series of choices made at the start of the research, including what type of network to use and the data and method used to train it. Currently, these choices - known as hyperparameters - are chosen through experience, random search or a computationally intensive search processes. In our most recent paper , we introduce a new method for training neural networks which allows an experimenter to quickly choose the best set of hyperparameters and model for the task. This technique - known as Population Based Training (PBT) - trains and optimises a series of networks at the same time, allowing the optimal set-up to be quickly found. Crucially, this adds no computational overhead, can be done as quickly as traditional techniques and is easy to integrate into existing machine learning pipelines. The technique is a hybrid of the two most commonly used methods for hyperparameter optimisation: random search and hand-tuning. In random search, a population of neural networks are trained independently in parallel and at the end of training the highest performing model is selected. Typically, this means that  a small fraction of the population will be trained with good hyperparameters, but many more will be trained with bad ones, wasting computer resources. With hand tuning, researchers must guess at the best hyperparameters, train their models using them, and then evaluate the performance. This is done over and over, until the researcher is happy with the performance of the network. Although this can result in better performance, the downside is that this takes a long time, sometimes taking weeks or even months to find the perfect set-up. And while there are ways of automating this process -  such as Bayesian optimisation - it still takes a long time and requires many sequential training runs to find the best hyperparameters. PBT - like random search - starts by training many neural networks in parallel with random hyperparameters. But instead of the networks training independently, it uses information from the rest of the population to refine the hyperparameters and direct computational resources to models which show promise. This takes its inspiration from genetic algorithms where each member of the population, known as a worker, can exploit information from the remainder of the population. For example, a worker might copy the model parameters from a better performing worker. It can also explore new hyperparameters by changing the current values randomly. As the training of the population of neural networks progresses, this process of exploiting and exploring is performed periodically, ensuring that all the workers in the population have a good base level of performance and also that new hyperparameters are consistently explored.  This means that PBT can quickly exploit good hyperparameters, can dedicate more training time to promising models and, crucially, can adapt the hyperparameter values throughout training, leading to automatic learning of the best configurations. Our experiments show that PBT is very effective across a whole host of tasks and domains. For example, we rigorously tested the algorithm on a suite of challenging reinforcement learning problems with state-of-the-art methods on DeepMind Lab, Atari, and StarCraft II. In all cases, PBT stabilised training, quickly found good hyperparameters, and delivered results that were beyond state-of-the-art baselines. We have also found PBT to be effective for training Generative Adversarial Network (GAN), which are notoriously difficult to tune. Specifically, we used the PBT framework to maximise the Inception Score - a measure of visual fidelity -  resulting in a significant improvement from 6.45 to 6.9. We have also applied it to one of Google’s state-of-the-art machine translation neural networks, which are usually trained with carefully hand tuned hyperparameter schedules that take months to perfect. With PBT we automatically found hyperparameter schedules that match and even exceed existing performance, but without any tuning and in the same time it normally takes to do a single training run. We believe this is only the beginning for the technique. At DeepMind, we have also found PBT is particularly useful for training new algorithms and neural network architectures that introduce new hyperparameters. As we continue to refine the process, it offers up the possibility of finding and developing ever more sophisticated and powerful neural network models. Read the full paper . This work was done by Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M. Czarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, Chrisantha Fernando and  Koray Kavukcuoglu.", "date": "2017-11-27"},
{"website": "Deepmind", "title": "Enabling Continual Learning in Neural Networks", "author": [" James Kirkpatrick ", " Razvan Pascanu ", " Neil Rabinowitz ", " Joel Veness ", " Guillaume Desjardins ", " Andrei Rusu ", " Kieran Milan ", " John Quan ", " Tiago Ramalho ", " Agnieszka Grabska-Barwinska ", " Demis Hassabis ", " Dharshan Kumaran ", " Raia Hadsell ", " C Clopath "], "link": "https://deepmind.com/blog/article/enabling-continual-learning-in-neural-networks", "abstract": "Computer programs that learn to perform tasks also typically forget them very quickly. We show that the learning rule can be modified so that a program can remember old tasks when learning a new one. This is an important step towards more intelligent programs that are able to learn progressively and adaptively. Deep neural networks are currently the most successful machine learning technique for solving a variety of tasks including language translation, image classification and image generation. However, they have typically been designed to learn multiple tasks only if the data is presented all at once. As a network trains on a particular task its parameters are adapted to solve the task. When a new task is introduced,  new adaptations overwrite the knowledge that the neural network had previously acquired. This phenomenon is known in cognitive science as ‘catastrophic forgetting’, and is considered one of the fundamental limitations of neural networks. By contrast, our brains work in a very different way. We are able to learn incrementally, acquiring skills one at a time and applying our previous knowledge when learning new tasks. As a starting point for our recent PNAS paper , in which we propose an approach to overcome catastrophic forgetting in neural networks, we took inspiration from neuroscience-based theories about the consolidation of previously acquired skills and memories in mammalian and human brains. Neuroscientists have distinguished two kinds of consolidation that occur in the brain: systems consolidation and synaptic consolidation. Systems consolidation is the process by which memories that have been acquired by the quick-learning parts of our brain are imprinted into the slow-learning parts. This imprinting is known to be mediated by conscious and unconscious recall - for instance, this can happen during dreaming. In the second mechanism, synaptic consolidation, connections between neurons are less likely to be overwritten if they have been important in previously learnt tasks. Our algorithm specifically takes inspiration from this mechanism to address the problem of catastrophic forgetting. A neural network consists of several connections in much the same way as a brain. After learning a task, we compute how important each connection is to that task. When we learn a new task, each connection is protected from modification by an amount proportional to its importance to the old tasks. Thus it is possible to learn the new task without overwriting what has been learnt in the previous task and without incurring a significant computational cost. In mathematical terms, we can think of the protection we attach to each connection in a new task as being linked to the old protection value by a spring, whose stiffness is proportional to the connection’s importance. For this reason, we called our algorithm Elastic Weight Consolidation (EWC). To test our algorithm, we exposed an agent to Atari games sequentially. Learning an individual game from the score alone is a challenging task, but learning multiple games sequentially is even more challenging as each game requires an individual strategy.  As shown in the figure below, without EWC, the agent quickly forgets each game after it stops playing it (blue). This means that on average, the agent barely learns a single game. However, if we use EWC (brown and red), the agent does not forget as easily and can learn to play several games, one after the other. Today, computer programs cannot learn from data adaptively and in real time. However, we have shown that catastrophic forgetting is not an insurmountable challenge for neural networks. We hope that this research represents a step towards programs that can learn in a more flexible and efficient way. Our research also progresses our understanding of how consolidation happens in the human brain. The neuroscientific theories that our work is based on, in fact, have mainly been proven in very simple examples. By showing that those same theories can be applied in a more realistic and complex machine learning context, we hope to give further weight to the idea that synaptic consolidation is key to retaining memories and know-how. To find out more, read our paper here .", "date": "2017-03-13"},
{"website": "Deepmind", "title": "Causal Bayesian Networks: A flexible tool to enable fairer machine learning", "author": [" Silvia Chiappa ", " William Isaac "], "link": "https://deepmind.com/blog/article/Causal_Bayesian_Networks", "abstract": "Decisions based on machine learning (ML) are potentially advantageous over human decisions, as they do not suffer from the same subjectivity, and can be more accurate and easier to analyse. At the same time, data used to train ML systems often contain human and societal biases that can lead to harmful decisions: extensive evidence in areas such as hiring, criminal justice, surveillance, and healthcare suggests that ML decision systems can treat individuals unfavorably ( unfairly ) on the basis of characteristics such as race, gender, disabilities, and sexual orientation – referred to as sensitive attributes . Currently, most fairness criteria used for evaluating and designing ML decision systems focus on the relationships between the sensitive attribute and the system output. However, the training data can display different patterns of unfairness depending on how and why the sensitive attribute influences other variables. Using such criteria without fully accounting for this could be problematic: it could, for example, lead to the erroneous conclusion that a model exhibiting harmful biases is fair and, vice-versa, that a model exhibiting harmless biases is unfair. The development of technical solutions to fairness also requires considering the different, potentially intricate, ways in which unfairness can appear in the data. Understanding how and why a sensitive attribute influences other variables in a dataset can be a challenging task, requiring both a technical and sociological analysis. The visual, yet mathematically precise, framework of Causal Bayesian networks (CBNs) represents a flexible useful tool in this respect as it can be used to formalize, measure, and deal with different unfairness scenarios underlying a dataset. A CBN (Figure 1) is a graph formed by nodes representing random variables, connected by links denoting causal influence. By defining unfairness as the presence of a harmful influence from the sensitive attribute in the graph, CBNs provide us with a simple and intuitive visual tool for describing different possible unfairness scenarios underlying a dataset. In addition, CBNs provide us with a powerful quantitative tool to measure unfairness in a dataset and to help researchers develop techniques for addressing it. Consider a hypothetical college admission example ( inspired by the Berkeley case ) in which applicants are admitted based on qualifications Q, choice of department D, and gender G; and in which female applicants apply more often to certain departments (for simplicity’s sake, we consider gender as binary, but this is not a necessary restriction imposed by the framework). Figure 1. CBN representing a hypothetical college admission process. The admission process is represented by the CBN in Figure 1. Gender has a direct influence on admission through the causal path G→ A and an indirect influence through the causal path G→ D→ A. The direct influence captures the fact that individuals with the same qualifications who are applying to the same department might be treated differently based on their gender. The indirect influence captures differing admission rates between female and male applicants due to their differing department choices. Whilst the direct influence of the sensitive attribute on admission is considered unfair for social and legal reasons, the indirect influence could be considered fair or unfair depending on contextual factors. In Figure 2a, 2b and 2c, we depict three possible scenarios, where total or partial red paths are used to indicate unfair and and partially-unfair links, respectively . Figure 2a: In the first scenario, female applicants voluntarily apply to departments with low acceptance rates, and therefore the path G→D is considered fair. Figure 2b: In the second scenario, female applicants apply to departments with low acceptance rates due to systemic historical or cultural pressures, and therefore the path G→D is considered unfair (as a consequence, the path D→A becomes partially unfair). Figure 2c: In the third scenario, the college lowers the admission rates for departments voluntarily chosen more often by women. The path G→D is considered fair, but the path D→A is partially unfair. This simplified example shows how CBNs can provide us with a visual framework for describing different possible unfairness scenarios. Understanding which scenario underlies a dataset can be challenging or even impossible, and might require expert knowledge. It is nevertheless necessary to avoid pitfalls when evaluating or designing a decision system. As an example, let’s assume that a university uses historical data to train a decision system to decide whether a prospective applicant should be admitted, and that a regulator wants to evaluate its fairness. Two popular fairness criteria are statistical parity (requiring the same admission rates among female and male applicants) and equal false positive or negative rates (EFPRs/EFNRs, requiring the same error rates among female and male applicants: i.e., the percentage of accepted applicants erroneously predicted as rejected, and vice-versa). In other words, statistical parity and EFPRs/EFNRs require all the predictions and the incorrect predictions to be independent of gender . From the discussion above, we can deduce that whether such criteria are appropriate or not strictly depends on the nature of the data pathways. Due to the presence of the unfair direct influence of gender on admission, it would be inappropriate for the regulator to use EFPRs/EFNRs to gauge fairness, because this criterion considers the influence that gender has on admission in the data as legitimate. This means that it would be possible for the system to be deemed fair, even if it carries the unfair influence: this would automatically be the case for an error-free decision system. On the other hand, if the path G→ D→ A was considered fair, it would be inappropriate to use statistical parity. In this case, it would be possible for the system to be deemed unfair, even if it does not contain the unfair direct influence of gender on admission through the path G→ A and only contains the fair indirect influence through the path G→ D→ A. In our first paper , we raise these concerns in the context of the fairness debate surrounding the COMPAS pretrial risk assessment tool, which has been central to the dialogue around the risks of using ML decision systems. CBNs can also be used to quantify unfairness in a dataset and to design techniques for alleviating unfairness in the case of complex relationships in the data. Path-specific techniques enable us to estimate the influence that a sensitive attribute has on other variables along specific sets of causal paths. This can be used to measure the degree of unfairness on a given dataset in complex scenarios in which some causal paths are considered unfair whilst other causal paths are considered fair. In the college admission example in which the path G→ D→ A is considered fair, path-specific techniques would enable us to measure the influence of G on A restricted to the direct path G→ A over the whole population , in order to obtain an estimate of the degree of unfairness contained in the dataset. The additional use of counterfactual inference techniques would enable us to ask if a specific individual was treated unfairly, for example by asking whether a rejected female applicant (G=1, Q=q, D=d, A=0) would have obtained the same decision in a counterfactual world in which her gender were male along the direct path G→A. In this simple example, assuming that the admission decision is obtained as the deterministic function f of G, Q, and D, i.e., A = f(G, Q, D), this corresponds to asking if f(G=0, Q=q, D=d) = 0, namely if a male applicant with the same department choice and qualifications would have also been rejected. We exemplify this in Figure 3 by re-computing the admission decision after changing the female candidate's photo to a male one in the profile. Figure 3. Counterfactual scenario However, path-specific counterfactual inference is generally more complex to achieve, if some variables are unfairly influenced by G. Assume that G also has an influence on Q through a direct path G→Q which is considered unfair. In this case, the CBN contains both variables that are fairly and unfairly influenced by G. Path-specific counterfactual inference would consist in performing a counterfactual correction of q, q_0, i.e of the variable which is unfairly influenced by G, and then computing the counterfactual decision as f(G=0, Q=q_0, D=d). The counterfactual correction q_0 is obtained by first using the information of the female applicant (G=1, Q=q, D=d, A=0) and knowledge about the CBN to get an estimate of the specific latent randomness in the makeup of the applicant, and then using this estimate to re-compute the value of Q as if G=0 along G→Q. In addition to answering questions of fairness in a dataset, path-specific counterfactual inference could be used to design methods to alleviate the unfairness of an ML system. In our second paper , we propose a method to perform path-specific counterfactual inference and suggest that it can be used to post-process the unfair decisions of a trained ML system by replacing them with counterfactual decisions. The resulting system is said to satisfy path-specific counterfactual fairness. As machine learning continues to be embedded in more systems which have a significant impact on people’s lives and safety, it is incumbent on researchers and practitioners to identify and address potential biases embedded in how training data sets are generated . Causal Bayesian Networks offer a powerful visual and quantitative tool for expressing the relationships among random variables in a dataset. While it is important to acknowledge the limitations and difficulties of using this tool – such as identifying a CBN that accurately describes the dataset’s generation, dealing with confounding variables, and performing counterfactual inference in complex settings – this unique combination of capabilities could enable a deeper understanding of complex systems and allow us to better align decision systems with society's values. This post is based on the following papers: A Causal Bayesian Networks Viewpoint on Fairness Path-Specific Counterfactual Fairness With thanks to Niki Kilbertus, Ben Coppin, Tom Stepleton,  Ray Jiang, Christina Heinze-Deml, Tom Everitt, and Shira Mitchell.", "date": "2019-10-03"},
{"website": "Deepmind", "title": "High-fidelity speech synthesis with WaveNet", "author": [" Aäron van den Oord ", " Yazhe Li ", " Igor Babuschkin "], "link": "https://deepmind.com/blog/article/high-fidelity-speech-synthesis-wavenet", "abstract": "In October we announced that our state-of-the-art speech synthesis model WaveNet was being used to generate realistic-sounding voices for the Google Assistant globally in Japanese and the US English. This production model - known as parallel WaveNet - is more than 1000 times faster than the original and also capable of creating higher quality audio. Our latest paper introduces details of the new model and the “probability density distillation” technique we developed to allow the system to work in a massively parallel computing environment. The original WaveNet model used autoregressive connections to synthesise the waveform one sample at a time, with each new sample conditioned on the previous samples. While this produces high-quality audio with up to 24,000 samples per second, this sequential generation is too slow for production environments. To get around this we needed a solution that could generate long sequences of samples all at once and with no loss of quality. Our solution is called probability density distillation, where we used a fully-trained WaveNet model to teach a second, “student” network that is both smaller and more parallel and therefore better suited to modern computational hardware. This student network is a smaller dilated convolutional neural network , similar to the original WaveNet. But, crucially, generation of each sample does not depend on any of the previously generated samples, meaning we can generate the first and last word - and everything in between -  at the same time, as shown in the animation below. During training, the student network starts off in a random state. It is fed random white noise as an input and is tasked with producing a continuous audio waveform as output. The generated waveform is then fed to the trained WaveNet model, which scores each sample, giving the student a signal to understand how far away it is from the teacher network’s desired output. Over time, the student network can be tuned - via backpropagation - to learn what sounds it should produce. Put another way, both the teacher and the student output a probability distribution for the value of each audio sample, and the goal of the training is to minimise the KL divergence between the teacher’s distribution and the student’s distribution. The training method has parallels to the set-up for generative adversarial networks (GANs), with the student playing the role of generator and the teacher as the discriminator. However, unlike GANs, the student’s aim is not to “fool” the teacher but to cooperate and try to match the teacher’s performance. Although the training technique works well, we also need to add a few extra loss functions to guide the student towards the desired behaviour. Specifically, we add a perceptual loss to avoid bad pronunciations, a contrastive loss to further reduce the noise, and a power loss to help match the energy of the human speech. Without the latter, for example, the trained model whispers rather than speaking out loud. Adding all of these together allowed us to train the parallel WaveNet to achieve the same quality of speech as the original WaveNet, as shown by the mean opinion scores (MOS) - a scale of 1-5 that measures of how natural sounding the speech is according to tests with human listeners. Note that even human speech is rated at just 4.667 on the MOS scale. Of course, the development of probability density distillation was just one of the steps needed to allow WaveNet to meet the speed and quality requirements of a production system. Incorporating parallel WaveNet into the serving pipeline of the Google Assistant required an equally significant engineering effort by the DeepMind Applied and Google Speech teams. It was only by working together that we could move from fundamental research to Google-scale product in a little over 12 months. Read the new paper . Read more about WaveNet in the Google Assistant . Read the original WaveNet blog post . Read the original WaveNet paper. This work was done by Aaron van den Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals, Koray Kavukcuoglu, George van den Driessche, Edward Lockhart, Luis C. Cobo, Florian Stimberg, Norman Casagrande, Dominik Grewe, Seb Noury, Sander Dieleman, Erich Elsen, Nal Kalchbrenner, Heiga Zen, Alex Graves, Helen King, Tom Walters, Dan Belov and Demis Hassabis.", "date": "2017-11-22"},
{"website": "Deepmind", "title": "Reinforcement learning with unsupervised auxiliary tasks", "author": [" Max Jaderberg ", " Vlad Mnih ", " Wojciech Marian Czarnecki "], "link": "https://deepmind.com/blog/article/reinforcement-learning-unsupervised-auxiliary-tasks", "abstract": "Our primary mission at DeepMind is to push the boundaries of AI, developing programs that can learn to solve any complex problem without needing to be taught how. Our reinforcement learning agents have achieved breakthroughs in Atari 2600 games and the game of Go . Such systems, however, can require a lot of data and a long time to learn so we are always looking for ways to improve our generic learning algorithms. Our recent paper “Reinforcement Learning with Unsupervised Auxiliary Tasks” introduces a method for greatly improving the learning speed and final performance of agents. We do this by augmenting the standard deep reinforcement learning methods with two main additional tasks for our agents to perform during training. A visualisation of our agent in a Labyrinth maze foraging task can be seen below. The first task involves the agent learning how to control the pixels on the screen, which emphasises learning how your actions affect what you will see rather than just prediction. This is similar to how a baby might learn to control their hands by moving them and observing the movements. By learning to change different parts of the screen, our agent learns features of the visual input that are useful for playing the game and getting higher scores. In the second task the agent is trained to predict the onset of immediate rewards from a short historical context. In order to better deal with the scenario where rewards are rare we present the agent with past rewarding and non-rewarding histories in equal proportion. By learning on rewarding histories much more frequently, the agent can discover visual features predictive of reward much faster. The combination of these auxiliary tasks, together with our previous A3C paper is our new UNREAL agent (UNsupervised REinforcement and Auxiliary Learning). We tested this agent on a suite of 57 Atari games as well as a 3D environment called Labyrinth with 13 levels. In all the games, the same UNREAL agent is trained in the same way, on the raw image output from the game, to produce actions to maximise the score or reward of the agent in the game. The behaviour required to get game rewards is incredibly varied, from picking up apples in 3D mazes to playing Space Invaders - the same UNREAL algorithm learns to play these games often to human level and beyond. Some results and visualisations can be seen in the video below. In Labyrinth, the result of using the auxiliary tasks - controlling the pixels on the screen and predicting when reward is going to occur - means that UNREAL is able to learn over 10x faster than our previous best A3C agent, and reaches far better performance. We can now achieve 87% of expert human performance averaged across the Labyrinth levels we considered, with super-human performance on a number of them. On Atari the agent now achieves on average 9x human performance. We hope that this work will allow us to scale up our agents to ever more complex environments.", "date": "2016-11-17"},
{"website": "Deepmind", "title": "Understanding deep learning through neuron deletion", "author": [" Ari Morcos ", " David Barrett "], "link": "https://deepmind.com/blog/article/understanding-deep-learning-through-neuron-deletion", "abstract": "Deep neural networks are composed of many individual neurons, which combine in complex and counterintuitive ways to solve a wide range of challenging tasks. This complexity grants neural networks their power but also earns them their reputation as confusing and opaque black boxes. Understanding how deep neural networks function is critical for explaining their decisions and enabling us to build more powerful systems. For instance, imagine the difficulty of trying to build a clock without understanding how individual gears fit together. One approach to understanding neural networks, both in neuroscience and deep learning, is to investigate the role of individual neurons, especially those which are easily interpretable. Our investigation into the importance of single directions for generalisation , soon to appear at the Sixth International Conference on Learning Representations ( ICLR ), uses an approach inspired by decades of experimental neuroscience — exploring the impact of damage — to determine: how important are small groups of neurons in deep neural networks? Are more easily interpretable neurons also more important to the network’s computation? We measured the performance impact of damaging the network by deleting individual neurons as well as groups of neurons. Our experiments led to two surprising findings: In both neuroscience and deep learning, easily interpretable neurons (“selective” neurons) which are only active in response to images of a single input category, such as dogs, have been analysed extensively. In deep learning, this has led to the emphasis on cat neurons , sentiment neurons , and parentheses neurons ; in neuroscience, Jennifer Aniston neurons , among others. However, the relative importance of these few highly selective neurons compared to the majority of neurons which have low selectivity and more puzzling, hard-to-interpret activity has remained unknown. To evaluate neuron importance, we measured how network performance on image classification tasks changes when a neuron is deleted. If a neuron is very important, deleting it should be highly damaging and substantially decrease network performance, while the deletion of an unimportant neuron should have little impact. Neuroscientists routinely perform similar experiments, although they cannot achieve the fine-grained precision which is necessary for these experiments and readily available in artificial neural networks. Surprisingly, we found that there was little relationship between selectivity and importance. In other words, “cat neurons” were no more important than confusing neurons. This finding echoes recent work in neuroscience which has demonstrated that confusing neurons can actually be quite informative, and suggests that we must look beyond the most easily interpretable neurons in order to understand deep neural networks. Although interpretable neurons are easier to understand intuitively (“it likes dogs”), they are no more important than confusing neurons with no obvious preference. We seek to build intelligent systems, and we can only call a system intelligent if it can generalise to new situations. For example, an image classification network which can only classify specific dog images that it has seen before, but not new images of the same dog, is useless. It is only in the intelligent categorisation of new examples that these systems gain their utility. A recent collaborative paper from Google Brain, Berkeley, and DeepMind which won best paper at ICLR 2017 showed that deep nets can simply memorise each and every image on which they are trained instead of learning in a more human-like way (e.g., understanding the abstract notion of a \"dog\"). However, it is often unclear whether a network has learned a solution which will generalise to new situations or not. By deleting progressively larger and larger groups of neurons, we found that networks which generalise well were much more robust to deletions than networks which simply memorised images that were previously seen during training. In other words, networks which generalise better are harder to break (although they can definitely still be broken). By measuring network robustness in this way, we can evaluate whether a network is exploiting undesirable memorisation to “cheat.” Understanding how networks change when they memorise will help us to build new networks which memorise less and generalise more. Together, these findings demonstrate the power of using techniques inspired by experimental neuroscience to understand neural networks. Using these methods, we found that highly selective individual neurons are no more important than non-selective neurons, and that networks which generalise well are much less reliant on individual neurons than those which simply memorise the training data. These results imply that individual neurons may be much less important than a first glance may suggest. By working to explain the role of all neurons, not just those which are easy-to-interpret, we hope to better understand the inner workings of neural networks, and critically, to use this understanding to build more intelligent and general systems. Read the full paper here . This work was done by Ari S Morcos, David GT Barrett, Neil C Rabinowitz, and Matthew Botvinick. Visualisations were created by Paul Lewis, Adam Cain, and Doug Fritz.", "date": "2018-03-21"},
{"website": "Deepmind", "title": "The hippocampus as a predictive map", "author": [" Kimberly Stachenfeld ", " Matt Botvinick ", " S J Gershman "], "link": "https://deepmind.com/blog/article/hippocampus-predictive-map", "abstract": "Think about how you choose a route to work, where to move house, or even which move to make in a game like Go. All of these scenarios require you to estimate the likely future reward of your decision. This is tricky because the number of possible scenarios explodes as one peers farther and farther into the future. Understanding how we do this is a major research question in neuroscience, while building systems that can effectively predict rewards is a major focus in AI research. In our new paper, in Nature Neuroscience , we apply a neuroscience lens to a longstanding mathematical theory from machine learning to provide new insights into the nature of learning and memory. Specifically, we propose that the area of the brain known as the hippocampus offers a unique solution to this problem by compactly summarising future events using what we call a “predictive map.” The hippocampus has traditionally been thought to only represent an animal’s current state, particularly in spatial tasks, such as navigating a maze. This view gained significant traction with the discovery of “place cells” in the rodent hippocampus , which fire selectively when the animal is in specific locations. While this theory accounts for many neurophysiological findings, it does not fully explain why the hippocampus is also involved in other functions, such as memory, relational reasoning, and decision making. Our new theory thinks about navigation as part of the more general problem of computing plans that maximise future reward. Our insights were derived from reinforcement learning, the subdiscipline of AI research that focuses on systems that learn by trial and error. The key computational idea we drew on is that to estimate future reward, an agent must first estimate how much immediate reward it expects to receive in each state, and then weight this expected reward by how often it expects to visit that state in the future. By summing up this weighted reward across all possible states, the agent obtains an estimate of future reward. Similarly, we argue that the hippocampus represents every situation - or state - in terms of the future states which it predicts. For example, if you are leaving work (your current state) your hippocampus might represent this by predicting that you will likely soon be on your commute, picking up your kids from school or, more distantly, at home. By representing each current state in terms of its anticipated successor states, the hippocampus conveys a compact summary of future events, known formally as the “ successor representation ”. We suggest that this specific form of predictive map allows the brain to adapt rapidly in environments with changing rewards, but without having to run expensive simulations of the future. This approach combines the strengths of two algorithms that are already well known in reinforcement learning and are also believed to exist in humans and rodents. “Model-based” algorithms learn models of the environment that can then be simulated to produce estimates of future reward, while “model-free” algorithms learn future reward estimates directly from experience in the environment. Model-based algorithms are flexible but computationally expensive, while model-free algorithms are computationally cheap but inflexible. The algorithm that inspired our theory combines some of the flexibility of model-based algorithms with the efficiency of model-free algorithms. Because the calculation is a simple weighted sum, it is computationally efficient, much like a model-free algorithm. At the same time, by separating reward expectations and state expectations (the predictive map), it can rapidly adapt to changes in reward by simply updating the reward expectations while leaving the state expectations intact ( see our recent paper for further detail ). In future work, we plan to test the theory further. Since the predictive map theory can be translated into a neural network architecture , we want to explore the extent to which this learning strategy can promote flexible, rapid planning in silico . More generally, a major future task will be to look at how the brain integrates different types of learning. While we posed this model as an alternative to model-based and model-free learning in the brain, a more realistic view is that many types of learning are simultaneously coordinated by the brain during learning and planning. Understanding how these learning algorithms are combined is an important step towards understanding human and animal brains, and could provide key insights for designing equally complex, multifaceted AI. Read AI and Neuroscience: A virtuous circle", "date": "2017-10-02"},
{"website": "Deepmind", "title": "Breaking down global barriers to access", "author": [" Obum Ekeke "], "link": "https://deepmind.com/blog/article/breaking-down-global-barriers-to-access", "abstract": "This week, we welcomed our biggest and most geographically diverse cohort of DeepMind scholars yet. We’re excited to reflect on the journey so far, share more on the next chapter of the DeepMind scholarships – and welcome many more universities from around the world into the programme. AI could be one of the most useful and transformative technologies in history - and the mission to build safe and beneficial AI spans a broad community. We established our scholarship programme in 2017 in an effort to help build a stronger and more inclusive AI community, who can bring a wider range of experiences to the fields of AI and computer science. The scholarships provide financial support to students from underrepresented groups seeking to study graduate courses relating to AI and adjacent fields. But of course, financial barriers are not the only obstacles that students can face, so in addition, every scholar is matched with a personal DeepMind mentor, who can support their aspirations and help them to navigate academic life. We started with eight fantastic scholars who were studying masters courses in the UK and US. The scholarships were awarded to academically excellent students who belong to groups currently underrepresented in AI. This week, we welcomed more than 50 scholars to our 2020 cohort alone. Increasing representation in AI offers a huge opportunity to bring diverse values, hopes and concerns into conversations about the design and deployment of AI – and this is critical if AI is going to be a technology that benefits everyone. Take alumnus scholar Shaquille, who wanted to use machine learning to better understand sickle cell anaemia, a disease which disproportionately affects Black people. As we celebrated our growing community, we considered the goal of the programme – to contribute to building a stronger and more inclusive AI ecosystem - and we reflected on who was excluded from it. Our scholarships aim to support underrepresented students – spanning gender, race, ethnicity, and socio-economic background. But imbalances in the field aren’t just social, they’re also geographical. Last year, 70% of all AI-related research was published in Europe, the US, and China, while many other important regions and countries are significantly underrepresented. For instance, only 0.3% of AI journal citations came from sub-Saharan Africa between 2014-2018, and a number of Eastern European countries are entirely absent from publication figures . This imbalance risks creating a technology that only accounts for the values, hopes and concerns of a narrow group, entrenching global inequalities while seeing large parts of the world miss out on the potential of AI to improve people’s lives through innovation in science, healthcare, and education. That’s why today, we’re delighted to announce that we’re expanding our programme to support scholars in many more countries currently underrepresented in AI, including – Bulgaria, Colombia, Greece, Poland, Romania, South Africa, and Turkey. We are also establishing new scholarships in Canada and France, and continuing our support for scholars in the UK and the US. The full list of universities partnering in our scholarships programme is here . There are many initiatives actively working to increase regional participation in AI, such as the Deep Learning Indaba , Khipu AI , and the Eastern European Machine Learning summer school . We hope to complement these efforts by enabling students to pursue further education in these regions with fewer financial barriers – contributing to regional hubs of excellence, while benefiting from the guidance of a DeepMind mentor and an international community of scholar peers. To ensure AI is of global benefit, talent must be nurtured in regions which are currently underrepresented in AI research, and space for geographically and socially diverse, local contributions to the field must be made. We know that increasing access to further education is only one part of addressing the deep-seated structural imbalances in AI, but it is an important one and we are happy to be able to contribute. So this week, as we reflect on the achievements of scholars past and present, and welcome the new 2020 cohort, we also look to the future, as we hope others will be inspired to take the next step on their AI career journeys too. To quote DeepMind alumna Benedetta, who studied at Oxford University: “Don’t underestimate the value of your unique background.” It’s these unique backgrounds and perspectives that will help make the AI community stronger, more diverse, and more inclusive in years to come. If you’re interested in becoming a DeepMind scholar, find out more about the programme on our website , discover the universities that participated in the scholarship programme this year, and keep an eye out for upcoming announcements from universities offering DeepMind scholarships starting in 2021.", "date": "2020-11-05"},
{"website": "Deepmind", "title": "Machine learning can boost the value of wind energy", "author": [" Carl Elkin ", " Sims Witherspoon "], "link": "https://deepmind.com/blog/article/machine-learning-can-boost-value-wind-energy", "abstract": "Carbon-free technologies like renewable energy help combat climate change, but many of them have not reached their full potential. Consider wind power: over the past decade, wind farms have become an important source of carbon-free electricity as the cost of turbines has plummeted and adoption has surged. However, the variable nature of wind itself makes it an unpredictable energy source—less useful than one that can reliably deliver power at a set time. In search of a solution to this problem, last year, DeepMind and Google started applying machine learning algorithms to 700 megawatts of wind power capacity in the central United States. These wind farms—part of Google’s global fleet of renewable energy projects —collectively generate as much electricity as is needed by a medium-sized city. Using a neural network trained on widely available weather forecasts and historical turbine data, we configured the DeepMind system to predict wind power output 36 hours ahead of actual generation. Based on these predictions, our model recommends how to make optimal hourly delivery commitments to the power grid a full day in advance. This is important, because energy sources that can be scheduled (i.e. can deliver a set amount of electricity at a set time) are often more valuable to the grid. Although we continue to refine our algorithm, our use of machine learning across our wind farms has produced positive results. To date, machine learning has boosted the value of our wind energy by roughly 20 percent, compared to the baseline scenario of no time-based commitments to the grid. We can’t eliminate the variability of the wind, but our early results suggest that we can use machine learning to make wind power sufficiently more predictable and valuable. This approach also helps bring greater data rigor to wind farm operations, as machine learning can help wind farm operators make smarter, faster and more data-driven assessments of how their power output can meet electricity demand. Our hope is that this kind of machine learning approach can strengthen the business case for wind power and drive further adoption of carbon-free energy on electric grids worldwide. Researchers and practitioners across the energy industry are developing novel ideas for how society can make the most of variable power sources like solar and wind. We’re eager to join them in exploring general availability of these cloud-based machine learning strategies. Google recently achieved 100 percent renewable energy purchasing and is now striving to source carbon-free energy on a 24x7 basis. The partnership with DeepMind to make wind power more predictable and valuable is a concrete step toward that aspiration. While much remains to be done, this step is a meaningful one—for Google, and more importantly, for the environment. This article is cross-posted from The Keyword .", "date": "2019-02-26"},
{"website": "Deepmind", "title": "Agent57: Outperforming the human Atari benchmark", "author": [" Adrià Puigdomènech ", " Bilal Piot ", " Steven Kapturowski ", " Pablo Sprechmann ", " Alex Vitvitskyi ", " Daniel Guo ", " Charles Blundell "], "link": "https://deepmind.com/blog/article/Agent57-Outperforming-the-human-Atari-benchmark", "abstract": "The Atari57 suite of games is a long-standing benchmark to gauge agent performance across a wide range of tasks. We’ve developed Agent57, the first deep reinforcement learning agent to obtain a score that is above the  human baseline on all 57 Atari 2600 games. Agent57 combines an algorithm for efficient exploration with a meta-controller that adapts the exploration and long vs. short-term behaviour of the agent. At DeepMind,  we’re interested in building agents that do well on a wide range of tasks. An agent that performs sufficiently well on a sufficiently wide range of tasks is classified as intelligent . Games are an excellent testing ground for building adaptive algorithms: they provide a rich suite of tasks which players must develop sophisticated behavioural strategies to master, but they also provide an easy progress metric – game score – to optimise against. The ultimate goal is not to develop systems that excel at games, but rather to use games as a stepping stone for developing systems that learn to excel at a broad set of challenges. Typically, human performance is taken as a baseline for what doing “sufficiently well” on a task means: the score obtained by an agent on each task can be measured relative to representative human performance, providing a human normalised score:  0% indicates that an agent performs at random, while 100% or above indicates the agent is performing at human level or better. In 2012, the Arcade Learning environment – a suite of 57 Atari 2600 games (dubbed Atari57) – was proposed as a benchmark set of tasks: these canonical Atari games pose a broad range of challenges for an agent to master. The research community commonly uses this benchmark to measure progress in building successively more intelligent agents.  It’s often desirable to summarise the performance of an agent on a wide range of tasks as a single number, and so average performance (either mean or median score across all games) on the Atari57 benchmark is often used to summarise an agents’ abilities. Average scores have progressively increased over time. Unfortunately, the average performance can fail to capture how many tasks an agent is doing well on, and so is not a good statistic for determining how general an agent is: it captures that an agent is doing sufficiently well, but not that it is doing sufficiently well on a sufficiently wide set of tasks . So although average scores have increased, until now, the number of above human games has not. As an illustrative example, consider a benchmark consisting of twenty tasks. Suppose agent A obtains a score of 500% on eight tasks, 200% on four tasks, and 0% on eight tasks  (mean = 240%, median = 200%), while agent B obtains a score of 150% on all tasks (mean = median = 150%). On average, agent A performs better than agent B. However, agent B possesses a more general ability: it obtains human-level performance on more tasks than agent A. This issue is exacerbated if some tasks are much easier than others. By performing very well on very easy tasks, agent A can apparently outperform agent B, which performs well on both easy and hard tasks. The median is less distorted by exceptional performance on a few easy games – it’s a more robust statistic than the mean for indicating the center of a distribution . However, in measuring generality, the tails of the distribution become more pertinent, particularly as the number of tasks becomes larger. For example, the measure of performance on the hardest 5th percentile of games can be much more representative of an agent’s degree of generality. Researchers have focused on maximising agents’ average performance on the Atari57 benchmark since its inception, and average performance has significantly increased over the past eight years. But, like the illustrative example above, not all Atari games are equal, with some games being much easier than others. Instead of examining the average performance, if we examine the performance of agents on the bottom 5% of games, we see that not much has changed since 2012: in fact, agents published in 2019 were struggling on the same games with which agents published in 2012 struggled.  Agent57 changes this, and is a more general agent in Atari57 than any agent since the inception of the benchmark. Agent57 finally obtains above human-level performance on the very hardest games in the benchmark set, as well as the easiest ones. Back in 2012, DeepMind developed the Deep Q-network agent (DQN) to tackle the Atari57 suite. Since then, the research community has developed many extensions and alternatives to DQN. Despite these advancements, however, all deep reinforcement learning agents have consistently failed to score in four games: Montezuma’s Revenge, Pitfall, Solaris and Skiing. Montezuma’s Revenge and Pitfall require extensive exploration to obtain good performance. A core dilemma in learning is the exploration-exploitation problem : should one keep performing behaviours one knows works (exploit), or should one try something new (explore) to discover new strategies that might be even more successful? For example, should one always order their same favourite dish at a local restaurant, or try something new that might surpass the old favourite? Exploration involves taking many suboptimal actions to gather the information necessary to discover an ultimately stronger behaviour. Solaris and Skiing are long-term credit assignment problems: in these games, it’s challenging to match the consequences of an agents’ actions to the rewards it receives. Agents must collect information over long time scales to get the feedback necessary to learn. For Agent57 to tackle these four challenging games in addition to the other Atari57 games, several changes to DQN were necessary. Early improvements to DQN enhanced its learning efficiency and stability, including double DQN , prioritised experience replay and dueling architecture . These changes allowed agents to make more efficient and effective use of their experience. Next, researchers introduced distributed variants of DQN, Gorila DQN and ApeX ,  that could be run on many computers simultaneously. This allowed agents to acquire and learn from experience more quickly, enabling researchers to rapidly iterate on ideas. Agent57 is also a distributed RL agent that decouples the data collection and the learning processes. Many actors interact with independent copies of the environment, feeding data to a central ‘memory bank’ in the form of a prioritized replay buffer. A learner then samples training data from this replay buffer, as shown in Figure 4, similar to how a person might recall memories to better learn from them.  The learner uses these replayed experiences to construct loss functions, by which it estimates the cost of actions or events. Then, it updates the parameters of its neural network by minimizing losses. Finally, each actor shares the same network architecture as the learner, but with its own copy of the weights. The learner weights are sent to the actors frequently, allowing them to update their own weights in a manner determined by their individual priorities, as we’ll discuss later. Agents need to have memory in order to take into account previous observations into their decision making. This allows the agent to not only base its decisions on the present observation (which is usually partial, that is, an agent only sees some of its world), but also on past observations, which can reveal more information about the environment as a whole. Imagine, for example, a task where an agent goes from room to room in order to count the number of chairs in a building. Without memory, the agent can only rely on the observation of one room. With memory, the agent can remember the number of chairs in previous rooms and simply add the number of chairs it observes in the present room to solve the task. Therefore the role of memory is to aggregate information from past observations to improve the decision making process. In deep RL and deep learning, recurrent neural networks such as Long-Short Term Memory (LSTM) are used as short term memories. Interfacing memory with behaviour is crucial for building systems that self-learn. In reinforcement learning, an agent can be an on-policy learner, which can only learn the value of its direct actions, or an off-policy learner, which can learn about optimal actions even when not performing those actions – e.g., it might be taking random actions, but can still learn what the best possible action would be.  Off-policy learning is therefore a desirable property for agents, helping them learn the best course of action to take while thoroughly exploring their environment. Combining off-policy learning with memory is challenging because you need to know what you might remember when executing a different behaviour. For example, what you might choose to remember when looking for an apple (e.g., where the apple is located), is different to what you might choose to remember if looking for an orange. But if you were looking for an orange, you could still learn how to find the apple if you came across the apple by chance, in case you need to find it in the future. The first deep RL agent combining memory and off-policy learning was Deep Recurrent Q-Network (DRQN). More recently, a significant speciation in the lineage of Agent57 occurred with Recurrent Replay Distributed DQN (R2D2), combining a neural network model of short-term memory with off-policy learning and distributed training, and achieving a very strong average performance on Atari57.  R2D2 modifies the replay mechanism for learning from past experiences to work with short term memory. All together, this helped R2D2 efficiently learn profitable behaviours, and exploit them for reward. We designed Never Give Up (NGU) to augment R2D2 with another form of memory: episodic memory. This enables NGU to detect when new parts of a game are encountered, so the agent can explore these newer parts of the game in case they yield rewards. This makes the agent’s behaviour ( exploration ) deviate significantly from the policy the agent is trying to learn (obtaining a high score in the game); thus, off-policy learning again plays a critical role here. NGU was the first agent to obtain positive rewards, without domain knowledge, on Pitfall, a game on which no agent had scored any points since the introduction of the Atari57 benchmark, and other challenging Atari games. Unfortunately, NGU sacrifices performance on what have historically been the “easier” games and so, on average, underperforms relative to R2D2. In order to discover the most successful strategies, agents must explore their environment–but some exploration strategies are more efficient than others. With DQN, researchers attempted to address the exploration problem by using an undirected exploration strategy known as epsilon-greedy: with a fixed probability (epsilon), take a random action, otherwise pick the current best action. However, this family of techniques do not scale well to hard exploration problems: in the absence of rewards, they require a prohibitive amount of time to explore large state-action spaces, as they rely on undirected random action choices to discover unseen states. In order to overcome this limitation, many directed exploration strategies have been proposed. Among these, one strand has focused on developing intrinsic motivation rewards that encourage an agent to explore and visit as many states as possible by providing more dense “internal” rewards for novelty-seeking behaviours. Within that strand, we distinguish two types of rewards: firstly, long-term novelty rewards encourage visiting many states throughout training, across many episodes. Secondly, short-term novelty rewards encourage visiting many states over a short span of time (e.g., within a single episode of a game). Long-term novelty rewards signal when a previously unseen state is encountered in the agent’s lifetime, and is a function of the density of states seen so far in training: that is, it’s adjusted by how often the agent has seen a state similar to the current one relative to states seen overall. When the density is high (indicating that the state is familiar ), the long term novelty reward is low, and vice versa. When all the states are familiar, the agent resorts to an undirected exploration strategy. However, learning density models of high dimensional spaces is fraught with problems due to the curse of dimensionality . In practice, when agents use deep learning models to learn a density model, they suffer from catastrophic forgetting (forgetting information seen previously as they encounter new experiences), as well as an inability to produce precise outputs for all inputs. For example, in Montezuma’s Revenge, unlike undirected exploration strategies, long-term novelty rewards allow the agent to surpass the human baseline. However, even the best performing methods on Montezuma’s Revenge need to carefully train a density model at the right speed: when the density model indicates that the states in the first room are familiar , the agent should be able to consistently get to unfamiliar territory. Short-term novelty rewards can be used to encourage an agent to explore states that have not been encountered in its recent past. Recently, neural networks that mimic some properties of episodic memory have been used  to speed up learning in reinforcement learning agents. Because episodic memories are also thought to be important for recognising novel experiences , we adapted these models to give Never Give Up a notion of short-term novelty. Episodic memory models are efficient and reliable candidates for computing short-term novelty rewards, as they can quickly learn a non-parametric density model that can be adapted on the fly (without needing to learn or adapt parameters of the model). In this case, the magnitude of the reward is determined by measuring the distance between the present state and previous states recorded in episodic memory. However, not all notions of distance encourage meaningful forms of exploration. For example, consider the task of navigating a busy city with many pedestrians and vehicles. If an agent is programmed to use a notion of distance wherein every tiny visual variation is taken into account, that agent would visit a large number of different states simply by passively observing the environment, even standing still – a fruitless form of exploration. To avoid this scenario, the agent should instead learn features that are seen as important for exploration, such as controllability, and compute a distance with respect to those features only. Such models have previously been used for exploration, and combining them with episodic memory is one of the main advancements of the Never Give Up exploration method , which resulted in above-human performance in Pitfall!. Never Give Up (NGU) used this short-term novelty reward based on controllable states , mixed with a long term novelty reward, using Random Network Distillation . The mix was achieved by multiplying both rewards, where the long term novelty is bounded. This way the short-term novelty reward’s effect is preserved, but can be down-modulated as the agent becomes more familiar with the game over its lifetime. The other core idea of NGU is that it learns a family of policies that range from purely exploitative to highly exploratory. This is achieved by leveraging a distributed setup: by building on top of R2D2 , actors produce experience with different policies based on different importance weighting on the total novelty reward. This experience is produced uniformly with respect to each weighting in the family. Agent57 is built on the following observation: what if an agent can learn when it’s better to exploit, and when it’s better to explore? We introduced the notion of a meta-controller that adapts the exploration-exploitation trade-off, as well as a time horizon that can be adjusted for games requiring longer temporal credit assignment. With this change, Agent57 is able to get the best of both worlds: above human-level performance on both easy games and hard games. Specifically, intrinsic motivation methods have two shortcomings: This motivated the use of an online adaptation mechanism that controls the amount of experience produced with different policies, with a variable-length time horizon and importance attributed to novelty. Researchers have tried tackling this with multiple methods, including training a population of agents with different hyperparameter values, directly learning the values of the hyperparameters by gradient descent , or using a centralized bandit to learn the value of hyperparameters . We used a bandit algorithm to select which policy our agent should use to generate experience. Specifically, we trained a sliding-window UCB bandit for each actor to select the degree of preference for exploration and time horizon its policy should have. To achieve Agent57, we combined our previous exploration agent, Never Give Up, with a meta-controller. This agent computes a mixture of long and short term intrinsic motivation to explore and learn a family of policies, where the choice of policy is selected by the meta-controller. The meta-controller allows each actor of the agent to choose a different trade-off between near vs. long term performance, as well as exploring new states vs. exploiting what’s already known (Figure 4). Reinforcement learning is a feedback loop: the actions chosen determine the training data. Therefore, the meta-controller also determines what data the agent learns from. With Agent57, we have succeeded in building a more generally intelligent agent that has above-human performance on all tasks in the Atari57 benchmark. It builds on our previous agent Never Give Up, and instantiates an adaptive meta-controller that helps the agent to know when to explore and when to exploit, as well as what time-horizon it would be useful to learn with. A wide range of tasks will naturally require different choices of both of these trade-offs, therefore the meta-controller provides a way to dynamically adapt such choices. Agent57 was able to scale with increasing amounts of computation: the longer it trained, the higher its score got. While this enabled Agent57 to achieve strong general performance, it takes a lot of computation and time; the data efficiency can certainly be improved. Additionally, this agent shows better 5th percentile performance on the set of Atari57 games. This by no means marks the end of Atari research, not only in terms of data efficiency, but also in terms of general performance. We offer two views on this: firstly, analyzing the performance among percentiles gives us new insights on how general algorithms are. While Agent57 achieves strong results on the first percentiles of the 57 games and holds better mean and median performance than NGU or R2D2, as illustrated by MuZero , it could still obtain a higher average performance. Secondly, all current algorithms are far from achieving optimal performance in some games. To that end, key improvements to use might be enhancements in the representations that Agent57 uses for exploration, planning, and credit assignment. Read the paper here . Work done by: Adrià Puigdomènech, Bilal Piot, Steven Kapturowski, Pablo Sprechmann, Alex Vitvitskyi, Daniel Guo, Charles Blundell Figure design by Paulo Estriga and Adam Cain", "date": "2020-03-31"},
{"website": "Deepmind", "title": "Measuring abstract reasoning in neural networks", "author": [" Adam Santoro ", " David Barrett ", " Ari Morcos ", " Timothy Lillicrap ", " Felix Hill "], "link": "https://deepmind.com/blog/article/measuring-abstract-reasoning", "abstract": "Neural network-based models continue to achieve impressive results on longstanding machine learning problems, but establishing their capacity to reason about abstract concepts has proven difficult. Building on previous efforts to solve this important feature of general-purpose learning systems, our latest paper sets out an approach for measuring abstract reasoning in learning machines, and reveals some important insights about the nature of generalisation itself. To understand why abstract reasoning is critical for general intelligence, consider Archimedes’ famous “Eureka!” moment: by noticing that the volume of an object is equivalent to the volume of water that the object displaces, he understood volume at a conceptual level, and was therefore able to reason about the volume of other irregularly shaped objects. We would like AI to have similar capabilities. While current systems can defeat world champions in complicated strategic games, they often struggle on other apparently simple tasks, especially when an abstract concept needs to be discovered and reapplied in a new setting. For example, if specifically trained to only count triangles, then even our best AI systems can still fail to count squares, or any other previously unencountered object. To build better, more intelligent systems it is therefore important to understand the ways in which neural networks are currently able to process abstract concepts, and where they still need improvement. To begin doing this, we took inspiration from the methods used to measure abstract reasoning in human IQ tests. Standard human IQ tests often require test-takers to interpret perceptually simple visual scenes by applying principles that they have learned through everyday experience. For example, human test-takers may have already learned about ‘progressions’ (the notion that some attribute can increase) by watching plants or buildings grow, by studying addition in a mathematics class, or by tracking a bank balance as interest accrues. They can then apply this notion in the puzzles to infer that the number of shapes, their sizes, or even the intensity of their colour will increase along a sequence. We do not yet have the means to expose machine learning agents to a similar stream of ‘everyday experiences’, meaning we cannot easily measure their ability to transfer knowledge from the real world to visual reasoning tests. Nonetheless, we can create an experimental set-up that still puts human visual reasoning tests to good use. Rather than study knowledge transfer from everyday life to visual reasoning problems (as in human testing), we instead studied knowledge transfer from one controlled set of visual reasoning problems to another. To achieve this, we built a generator for creating matrix problems, involving a set of abstract factors, including relations like ‘progression’ and attributes like ‘colour’ and ‘size’. While the question generator uses a small set of underlying factors, it can nonetheless create an enormous number of unique questions. Next, we constrained the factors or combinations available to the generator to create different sets of problems for training and testing our models, to measure how well our models can generalise to held-out test sets. For instance, we created a training set of puzzles in which the progression relation is only encountered when applied to the colour of lines, and a test set when it is applied to the size of shapes. If a model performs well on this test set, it would provide evidence for an ability to infer and apply the abstract notion of progression, even in situations in which it had never previously seen a progression. In the typical generalisation regime applied in machine learning evaluations, where training and test data are sampled from the same underlying distribution, all of the networks we tested exhibited good generalisation error, with some achieving impressive absolute performance at just above 75%. The best performing network explicitly computed relations between different image panels and evaluated the suitability of each potential answer in parallel. We call this architecture a Wild Relation Network (WReN). When required to reason using attribute values ‘interpolated’ between previously seen attribute values, and also when applying known abstract relations in unfamiliar combinations, the models generalised notably well. However, the same network performed much worse in the ‘extrapolation’ regime, where attribute values in the test set did not lie within the same range as those seen during training. An example of this occurs for puzzles that contain dark coloured objects during training and light coloured objects during testing. Generalisation performance was also worse when the model was trained to apply a previously seen relation, such as a progression on the number of shapes, to a new attribute, such as size. Finally, we observed improved generalisation performance when the model was trained to predict not only the correct answer, but also the ‘reason’ for the answer (i.e. the particular relations and attributes that should be considered to solve the puzzle). Interestingly, in the neutral split, the model’s accuracy was strongly correlated with its ability to infer the correct relation underlying the matrix: when the explanation was right, the model would choose the correct answer 87% of the time, but when its explanation was wrong this performance dropped to only 32%. This suggests that models which achieved better performance when they correctly inferred the abstract concepts underlying the task. Recent literature has focussed on the strengths and weaknesses of neural network-based approaches to machine learning problems, often based around their capacity or failure to generalise. Our results show that it might be unhelpful to draw universal conclusions about generalisation: the neural networks we tested performed well in certain regimes of generalisation and very poorly in others. Their success was determined by a range of factors, including the architecture of the model used and whether the model was trained to provide an interpretable “reason” for its answer choices. In almost all cases, the systems performed poorly when required to extrapolate to inputs beyond their experience, or to deal with entirely unfamiliar attributes; creating a clear focus for future work in this critical, and important area of research. This work was done by David G. T. Barrett*, Felix Hill*, Adam Santoro*, Ari Morcos and Timothy Lillicrap. To encourage and support further research towards improved generalisation and abstract reasoning, we have made our dataset publicly available here . Download the paper [ PDF ] and supplementary material [ PDF ].", "date": "2018-07-11"},
{"website": "Deepmind", "title": "DeepMind Health Response to Independent Reviewers' Report 2018", "author": [" Dominic King "], "link": "https://deepmind.com/blog/article/deepmind-health-response-independent-reviewers-report-2018", "abstract": "When we set up DeepMind Health we believed that pioneering technology should be matched with pioneering oversight. That’s why when we launched in February 2016, we did so with an unusual and additional mechanism: a panel of Independent Reviewers, who meet regularly throughout the year to scrutinise our work. This is an innovative approach within tech companies - one that forces us to question not only what we are doing, but how and why we are doing it - and we believe that their robust challenges make us better. In their report last year, the Independent Reviewers asked us important questions about our engagement with stakeholders, data governance, and the behavioural elements that need to be considered when deploying new technologies in clinical environments. We’ve done a lot over the past twelve months to address these questions, and we’re really proud that this year’s Annual Report recognises the progress we’ve made. Of course, this year’s report includes a series of new recommendations for areas where we can continue to improve, which we’ll be working on in the coming months. In particular: We’re developing our longer-term business model and roadmap, and look forward to sharing our ideas once they’re further ahead. Rather than charging for the early stages of our work, our first priority has been to prove that our technologies can help improve patient care and reduce costs. We believe that our business model should flow from the positive impact we create, and will continue to explore outcomes-based elements so that costs are at least in part related to the benefits we deliver. We will explore further ways to ensure there is clarity about the binding legal frameworks that govern all our NHS partnerships. Trusts remain in full control of the data at all times. We are legally and contractually bound to only using patient data under the instructions of our partners. We will continue to make our legal agreements with Trusts publicly available to allow scrutiny of this important point. We will continue to grow our user testing groups with clinicians and patients to diversify the feedback we receive and ensure we remain mindful of the realities of clinical practice. We will remain vigilant about setting the highest possible standards of information governance. At the beginning of this year, we appointed a full time Information Governance Manager to oversee our use of data in all areas of our work. We are also continuing to build our Verifiable Data Audit and other tools to clearly show how we’re using data. We will continue engaging with patients and the public in a meaningful way, including via our website as well as in-person events, and we commit to addressing the issues of diversity and accessibility highlighted in the report. We want to take this opportunity to thank the Independent Reviewers for their thoughtful and committed engagement with our work. By holding us to account, recognising where we’re getting it right and challenging us where we can improve, they’ll help us to do a better job for patients, nurses, doctors, carers, families, and all those who rely on healthcare systems around the world.", "date": "2018-06-15"},
{"website": "Deepmind", "title": "Producing flexible behaviours in simulated environments", "author": [" Nicolas Heess ", " Josh Merel ", " Ziyu Wang "], "link": "https://deepmind.com/blog/article/producing-flexible-behaviours-simulated-environments", "abstract": "The agility and flexibility of a monkey swinging through the trees or a football player dodging opponents and scoring a goal can be breathtaking. Mastering this kind of sophisticated motor control is a hallmark of physical intelligence, and is a crucial part of AI research. True motor intelligence requires learning how to control and coordinate a flexible body to solve tasks in a range of complex environments. Existing attempts to control physically simulated humanoid bodies come from diverse fields, including computer animation and biomechanics.  A trend has been to use hand-crafted objectives, sometimes with motion capture data, to produce specific behaviors. However, this may require considerable engineering effort, and can result in restricted behaviours or behaviours that may be difficult to repurpose for new tasks. In three new papers, we seek ways to produce flexible and natural behaviours that can be reused and adapted to solve tasks. For some AI problems, such as playing Atari or Go, the goal is easy to define - it’s winning. But how do you describe the process for performing a backflip? Or even just a jump? The difficulty of accurately describing a complex behaviour is a common problem when teaching motor skills to an artificial system. In this work we explore how sophisticated behaviors can emerge from scratch from the body interacting with the environment using only simple high-level objectives, such as moving forward without falling. Specifically, we trained agents with a variety of simulated bodies to make progress across diverse terrains, which require jumping, turning and crouching. The results show our agents develop these complex skills without receiving specific instructions, an approach that can be applied to train our systems for multiple, distinct simulated bodies. The GIFs below show how this technique can lead to high quality movements and perseverance. They can be viewed in full here . The emergent behaviour described above can be very robust, but because the movements must emerge from scratch, they often do not look human-like.  In our second paper, we demonstrate how to train a policy network that imitates motion capture data of human behaviours to pre-learn certain skills, such as walking, getting up from the ground, running, and turning. Having produced behaviour that looks human-like, we can tune and repurpose those behaviours to solve other tasks, like climbing stairs and navigating walled corridors. The third paper proposes a neural network architecture, building on state-of-the-art generative models, that is capable of learning the relationships between different behaviours and imitating specific actions that it is shown. After training, our system can encode a single observed action and create a new novel movement based on that demonstration. It can also switch between different kinds of behaviours despite never having seen transitions between them, for example switching between walking styles. Achieving flexible and adaptive control of simulated bodies is a key element of AI research. Our work aims to develop flexible systems which learn and adapt skills to solve motor control tasks while reducing the manual engineering required to achieve this goal. Future work could extend these approaches to enable coordination of a greater range of behaviours in more complex situations. Read: Emergence of locomotion behaviours in rich environments Learning human behaviours from motion capture by adversarial imitation Robust imitation of diverse behaviours", "date": "2017-07-10"},
{"website": "Deepmind", "title": "Episode 4: AI, Robot", "author": "Unknown", "link": "https://deepmind.com/blog/article/podcast-episode-4-ai-robot", "abstract": "Forget what sci-fi has told you about superintelligent robots that are uncannily human-like; the reality is more prosaic. Inside DeepMind’s robotics laboratory, Hannah explores what researchers call ‘embodied AI’: robot arms that are learning tasks like picking up plastic bricks, which humans find comparatively easy. Discover the cutting-edge challenges of bringing AI and robotics together, and learning from scratch how to perform tasks. She also explores some of the key questions about using AI safely in the real world. Interviewees: Software engineer Jackie Kay and research scientists Murray Shanahan, Victoria Krakovna, Raia Hadsell and Jan Leike. Listen: How to teach an AI Listen to this episode and subscribe to the whole series on Apple podcasts , Google podcasts , Spotify , Deezer or your favourite podcast app by searching for “DeepMind: The Podcast”. Find out more about the themes in this episode: If you know of other resources we should link to, please help other listeners by either replying to us on Twitter (#DMpodcast) or emailing us at podcast@deepmind.com. You can also use that address to send us questions or feedback on the series. Credits: Presenter: Hannah Fry Editor: David Prest Senior Producer: Louisa Field Producers: Amy Racs, Dan Hardoon Binaural Sound: Lucinda Mason-Brown Music composition: Eleni Shaw (with help from Sander Dieleman and WaveNet )", "date": "2019-08-20"},
{"website": "Deepmind", "title": "MuZero: Mastering Go, chess, shogi and Atari without rules", "author": [" Julian Schrittwieser ", " Ioannis Antonoglou ", " Thomas Hubert ", " Karen Simonyan ", " Laurent Sifre ", " Simon Schmitt ", " Arthur Guez ", " Edward Lockhart ", " Demis Hassabis ", " Thore Graepel ", " Timothy Lillicrap ", " David Silver "], "link": "https://deepmind.com/blog/article/muzero-mastering-go-chess-shogi-and-atari-without-rules", "abstract": "In 2016, we introduced AlphaGo , the first artificial intelligence (AI) program to defeat humans at the ancient game of Go. Two years later, its successor - AlphaZero - learned from scratch to master Go, chess and shogi. Now, in a paper in the journal Nature , we describe MuZero, a significant step forward in the pursuit of general-purpose algorithms. MuZero masters Go, chess, shogi and Atari without needing to be told the rules, thanks to its ability to plan winning strategies in unknown environments. For many years, researchers have sought methods that can both learn a model that explains their environment, and can then use that model to plan the best course of action. Until now, most approaches have struggled to plan effectively in domains, such as Atari, where the rules or dynamics are typically unknown and complex. MuZero, first introduced in a preliminary paper in 2019 , solves this problem by learning a model that focuses only on the most important aspects of the environment for planning. By combining this model with AlphaZero’s powerful lookahead tree search, MuZero set a new state of the art result on the Atari benchmark, while simultaneously matching the performance of AlphaZero in the classic planning challenges of Go, chess and shogi. In doing so, MuZero demonstrates a significant leap forward in the capabilities of reinforcement learning algorithms. The ability to plan is an important part of human intelligence, allowing us to solve problems and make decisions about the future. For example, if we see dark clouds forming, we might predict it will rain and decide to take an umbrella with us before we venture out. Humans learn this ability quickly and can generalise to new scenarios, a trait we would also like our algorithms to have. Researchers have tried to tackle this major challenge in AI by using two main approaches: lookahead search or model-based planning. Systems that use lookahead search, such as AlphaZero, have achieved remarkable success in classic games such as checkers, chess and poker, but rely on being given knowledge of their environment’s dynamics, such as the rules of the game or an accurate simulator. This makes it difficult to apply them to messy real world problems, which are typically complex and hard to distill into simple rules. Model-based systems aim to address this issue by learning an accurate model of an environment’s dynamics, and then using it to plan. However, the complexity of modelling every aspect of an environment has meant these algorithms are unable to compete in visually rich domains, such as Atari.  Until now, the best results on Atari are from model-free systems, such as DQN , R2D2 and Agent57 . As the name suggests, model-free algorithms do not use a learned model and instead estimate what is the best action to take next. MuZero uses a different approach to overcome the limitations of previous approaches. Instead of trying to model the entire environment, MuZero just models aspects that are important to the agent’s decision-making process. After all, knowing an umbrella will keep you dry is more useful to know than modelling the pattern of raindrops in the air. Specifically, MuZero models three elements of the environment that are critical to planning: These are all learned using a deep neural network and are all that is needed for MuZero to understand what happens when it takes a certain action and to plan accordingly. This approach comes with another major benefit: MuZero can repeatedly use its learned model to improve its planning, rather than collecting new data from the environment. For example, in tests on the Atari suite, this variant - known as MuZero Reanalyze - used the learned model 90% of the time to re-plan what should have been done in past episodes. We chose four different domains to test MuZeros capabilities. Go, chess and shogi were used to assess its performance on challenging planning problems, while we used the Atari suite as a benchmark for more visually complex problems. In all cases, MuZero set a new state of the art for reinforcement learning algorithms, outperforming all prior algorithms on the Atari suite and matching the superhuman performance of AlphaZero on Go, chess and shogi. We also tested how well MuZero can plan with its learned model in more detail. We started with the classic precision planning challenge in Go, where a single move can mean the difference between winning and losing. To confirm the intuition that planning more should lead to better results, we measured how much stronger a fully trained version of MuZero can become when given more time to plan for each move (see left hand graph below). The results showed that playing strength increases by more than 1000 Elo (a measure of a player's relative skill) as we increase the time per move from one-tenth of a second to 50 seconds. This is similar to the difference between a strong amateur player and the strongest professional player. To test whether planning also brings benefits throughout training, we ran a set of experiments on the Atari game Ms Pac-Man (right hand graph above) using separate trained instances of MuZero. Each one was allowed to consider a different number of planning simulations per move, ranging from five to 50. The results confirmed that increasing the amount of planning for each move allows MuZero to both learn faster and achieve better final performance. Interestingly, when MuZero was only allowed to consider six or seven simulations per move - a number too small to cover all the available actions in Ms Pac-Man - it still achieved good performance. This suggests MuZero is able to generalise between actions and situations, and does not need to exhaustively search all possibilities to learn effectively. MuZero’s ability to both learn a model of its environment and use it to successfully plan demonstrates a significant advance in reinforcement learning and the pursuit of general purpose algorithms. Its predecessor, AlphaZero, has already been applied to a range of complex problems in chemistry , quantum physics and beyond. The ideas behind MuZero's powerful learning and planning algorithms may pave the way towards tackling new challenges in robotics, industrial systems and other messy real-world environments where the “rules of the game” are not known. Design by Adam Cain, Jim Kynvin and Aleksandrs Polozuns", "date": "2020-12-23"},
{"website": "Deepmind", "title": "Learning to navigate in cities without a map", "author": [" Piotr Mirowski ", " Raia Hadsell ", " Andrew Zisserman "], "link": "https://deepmind.com/blog/article/learning-to-navigate-cities-without-a-map", "abstract": "How did you learn to navigate the neighborhood of your childhood, to go to a friend’s house, to your school or to the grocery store? Probably without a map and simply by remembering the visual appearance of streets and turns along the way. As you gradually explored your neighborhood, you grew more confident, mastered your whereabouts and learned new and increasingly complex paths. You may have gotten briefly lost, but found your way again thanks to landmarks, or perhaps even by looking to the sun for an impromptu compass. Navigation is an important cognitive task that enables humans and animals to traverse, without maps, over long distances in a complex world. Such long-range navigation can simultaneously support self-localisation (“I am here”) and a representation of the goal (“I am going there”). In Learning to Navigate in Cities Without a Map , we present an interactive navigation environment that uses first-person perspective photographs from Google Street View , approved for use by the StreetLearn project and academic research, and gamify that environment to train an AI. As standard with Street View images, faces and license plates have been blurred and are unrecognisable. We build a neural network-based artificial agent that learns to navigate multiple cities using visual information (pixels from a Street View image). Note that this research is about navigation in general rather than driving; we did not use traffic information nor try to model vehicle control. The agent is rewarded when it reaches a target destination (specified, for instance, as pair of latitude and longitude coordinates), like a courier tasked with an endless set of deliveries but without a map. Over time, the AI agent learns to cross entire cities in this way. We also demonstrate that our agent can learn the task in multiple cities, and then robustly adapt to a new city. We depart from the traditional approaches which rely on explicit mapping and exploration (like a cartographer who tries to localise themselves and draw a map at the same time). Our approach, in contrast, is to learn to navigate as humans used to do, without maps, GPS localisation, or other aids, using only visual observations. We build a neural network agent that inputs images observed from the environment and predicts the next action it should take in that environment. We train it end-to-end using deep reinforcement learning, similarly to some recent work on learning to navigate in complex 3D mazes and reinforcement learning with unsupervised auxiliary tasks for playing games. Unlike those studies, which were conducted on small-scale simulated maze environments, we utilise city-scale real-world data, including complex intersections, footpaths, tunnels, and diverse topology across London, Paris, and New York City. Moreover, the approach we use support city-specific learning and optimisation as well as general, transferable navigation behaviours. The neural network inside our agent consists of three parts: 1) a convolutional network that can process images and extract visual features, 2) a locale-specific recurrent neural network that is implicitly tasked with memorising the environment as well as learning a representation of “here” (current position of the agent) and of “there” (location of the goal) and 3) a locale-invariant recurrent network that produces the navigation policy over the agent’s actions. The locale-specific module is designed to be interchangeable and, as its name indicates, unique to each city where the agent navigates, whereas the vision module and the policy module can be locale-invariant. Just as in the Google Street View interface, the agent can rotate in place or move forward to the next panorama, when possible. Unlike the Google Maps and Street View environment, the agent does not see the little arrows, the local or global map, or the famous Pegman: it needs to learn to differentiate open roads from sidewalks. The target destinations may be kilometres away in the real world and require the agent to step through hundreds of panoramas to reach them. We demonstrate that our proposed method can provide a mechanism for transferring knowledge to new cities. As with humans, when our agent visits a new city, we would expect it to have to learn a new set of landmarks, but not to have to re-learn its visual representations or its behaviours (e.g., zooming forward along streets or turning at intersections). Therefore, using the MultiCity architecture, we train first on a number of cities, then we freeze both the policy network and the visual convolutional network and only a new locale-specific pathway on a new city. This approach enables the agent to acquire new knowledge without forgetting what it has already learned, similarly to the progressive neural networks architecture. Studying navigation is fundamental in the study and development of artificial intelligence, and trying to replicate navigation in artificial agents can also help scientists understand its biological underpinnings.", "date": "2018-03-29"},
{"website": "Deepmind", "title": "AlphaStar: Grandmaster level in StarCraft II using multi-agent reinforcement learning", "author": [" The AlphaStar team "], "link": "https://deepmind.com/blog/article/AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning", "abstract": "We chose to use general-purpose machine learning techniques – including neural networks, self-play via reinforcement learning, multi-agent learning, and imitation learning – to learn directly from game data with general purpose techniques. Using the advances described in our Nature paper , AlphaStar was ranked above 99.8% of active players on Battle.net, and achieved a Grandmaster level for all three StarCraft II races: Protoss, Terran, and Zerg. We expect these methods could be applied to many other domains. Learning-based systems and self-play are elegant research concepts which have facilitated remarkable advances in artificial intelligence. In 1992, researchers at IBM developed TD-Gammon, combining a learning-based system with a neural network to play the game of backgammon. Instead of playing according to hard-coded rules or heuristics, TD-Gammon was designed to use reinforcement learning to figure out, through trial-and-error, how to play the game in a way that maximises its probability of winning. Its developers used the notion of self-play to make the system more robust: by playing against versions of itself, the system grew increasingly proficient at the game. When combined, the notions of learning-based systems and self-play provide a powerful paradigm of open-ended learning. Many advances since then have demonstrated that these approaches can be scaled to progressively challenging domains. For example, AlphaGo and AlphaZero established that it was possible for a system to learn to achieve superhuman performance at Go, chess, and shogi, and OpenAI Five and DeepMind’s FTW demonstrated the power of self-play in the modern games of Dota 2 and Quake III. At DeepMind, we’re interested in understanding the potential – and limitations – of open-ended learning, which enables us to develop robust and flexible agents that can cope with complex, real-world domains. Games like StarCraft are an excellent training ground to advance these approaches, as players must use limited information to make dynamic and difficult decisions that have ramifications on multiple levels and timescales. Despite its successes, self-play suffers from well known drawbacks. The most salient one is forgetting: an agent playing against itself may keep improving, but it also may forget how to win against a previous version of itself. Forgetting can create a cycle of an agent “chasing its tail”, and never converging or making real progress. For example, in the game rock-paper-scissors, an agent may currently prefer to play rock over other options. As self-play progresses, a new agent will then choose to switch to paper, as it wins against rock. Later, the agent will switch to scissors, and eventually back to rock, creating a cycle. Fictitious self-play - playing against a mixture of all previous strategies - is one solution to cope with this challenge. After first open-sourcing StarCraft II as a research environment, we found that even fictitious self-play techniques were insufficient to produce strong agents, so we set out to develop a better, general-purpose solution. A central idea of our recently published Nature paper extends the notion of fictitious self-play to a group of agents – the League. Normally in self-play, every agent maximises its probability of winning against its opponents; however, this was only part of the solution. In the real world, a player trying to improve at StarCraft may choose to do so by partnering with friends so that they can train particular strategies. As such, their training partners are not playing to win against every possible opponent, but are instead exposing the flaws of their friend, to help them become a better and more robust player. The key insight of the League is that playing to win is insufficient: instead, we need both main agents whose goal is to win versus everyone, and also exploiter agents that focus on helping the main agent grow stronger by exposing its flaws, rather than maximising their own win rate against all players. Using this training method, the League learns all its complex StarCraft II strategy in an end-to-end, fully automated fashion. Exploration is another key challenge in complex environments such as StarCraft. There are up to 10 26 possible actions available to one of our agents at each time step, and the agent must make thousands of actions before learning if it has won or lost the game. Finding winning strategies is challenging in such a massive solution space. Even with a strong self-play system and a diverse league of main and exploiter agents, there would be almost no chance of a system developing successful strategies in such a complex environment without some prior knowledge. Learning human strategies, and ensuring that the agents keep exploring those strategies throughout self-play, was key to unlocking AlphaStar’s performance. To do this, we used imitation learning – combined with advanced neural network architectures and techniques used for language modelling – to create an initial policy which played the game better than 84% of active players. We also used a latent variable which conditions the policy and encodes the distribution of opening moves from human games, which helped to preserve high-level strategies. AlphaStar then used a form of distillation throughout self-play to bias exploration towards human strategies. This approach enabled AlphaStar to represent many strategies within a single neural network (one for each race). During evaluation, the neural network was not conditioned on any specific opening moves. In addition, we found that many prior approaches to reinforcement learning are ineffective in StarCraft, due to its enormous action space. In particular, AlphaStar uses a new algorithm for off-policy reinforcement learning, which allows it to efficiently update its policy from games played by an older policy. Open-ended learning systems that utilise learning-based agents and self-play have achieved impressive results in increasingly challenging domains. Thanks to advances in imitation learning, reinforcement learning, and the League, we were able to train AlphaStar Final, an agent that reached Grandmaster level at the full game of StarCraft II without any modifications, as shown in the above video. This agent played online anonymously, using the gaming platform Battle.net, and achieved a Grandmaster level using all three StarCraft II races. AlphaStar played using a camera interface, with similar information to what human players would have, and with restrictions on its action rate to make it comparable with human players. The interface and restrictions were approved by a professional player. Ultimately, these results provide strong evidence that general-purpose learning techniques can scale AI systems to work in complex, dynamic environments involving multiple actors. The techniques we used to develop AlphaStar will help further the safety and robustness of AI systems in general, and, we hope, may serve to advance our research in real-world domains. AlphaStar team: Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David H. Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Danihelka, Aja Huang, Laurent Sifre, Trevor Cai, John P. Agapiou, Max Jaderberg, Alexander S. Vezhnevets, Rémi Leblond, Tobias Pohlen, Valentin Dalibard, David Budden, Yury Sulsky, James Molloy, Tom L. Paine, Caglar Gulcerhe, Ziyu Wang, Tobias Pfaff, Yuhuai Wu, Roman Ring, Dani Yogatama, Dario Wünsch, Katrina McKinney, Oliver Smith, Tom Schaul, Timothy Lillicrap, Koray Kavukcuoglu, Demis Hassabis, Chris Apps, David Silver Acknowledgements: We’re grateful to Dario Wünsch (TLO) , Grzegorz Komincz (MaNa) , and Diego Schwimer (Kelazhur) for their advice, guidance, and immense skill. We are also grateful for the continued support of Blizzard and the StarCraft gaming and AI community for making this work possible–especially those who played against AlphaStar on Battle.net. Thanks to Ali Razavi, Daniel Toyama, David Balduzzi, Doug Fritz, Eser Aygün, Florian Strub, Guillaume Alain, Haoran Tang, Jaume Sanchez, Jonathan Fildes, Julian Schrittwieser, Justin Novosad, Karen Simonyan, Karol Kurach, Philippe Hamel,  Ricardo Barreira, Scott Reed, Sergey Bartunov, Shibl Mourad, Steve Gaffney, Thomas Hubert, the team that created PySC2, and the whole DeepMind team, with special thanks to the research platform team, comms and events teams. *Agents were capped at a max of 22 agent actions per 5 seconds, where one agent action corresponds to a selection, an ability and a target unit or point, which counts as up to 3 actions towards the in-game APM counter. Moving the camera also counts as an agent action, despite not being counted towards APM.", "date": "2019-10-30"},
{"website": "Deepmind", "title": "Navigating with grid-like representations in artificial agents", "author": [" Andrea Banino ", " Dharshan Kumaran ", " Caswell Barry ", " Benigno Uria "], "link": "https://deepmind.com/blog/article/grid-cells", "abstract": "Most animals, including humans, are able to flexibly navigate the world they live in – exploring new areas, returning quickly to remembered places, and taking shortcuts. Indeed, these abilities feel so easy and natural that it is not immediately obvious how complex the underlying processes really are. In contrast, spatial navigation remains a substantial challenge for artificial agents whose abilities are far outstripped by those of mammals. In 2005, a potentially crucial part of the neural circuitry underlying spatial behaviour was revealed by an astonishing discovery: neurons that fire in a strikingly regular hexagonal pattern as animals explore their environment. This lattice of points is believed to facilitate spatial navigation, similarly to the gridlines on a map. In addition to equipping animals with an internal coordinate system, these neurons - known as grid cells - have recently been hypothesised to support vector-based navigation . That is: enabling the brain to calculate the distance and direction to a desired destination, “ as the crow flies ,” allowing animals to make direct journeys between different places even if that exact route had not been followed before. The group that first discovered grid cells was jointly awarded the 2014 Nobel Prize in Physiology or Medicine for shedding light on how cognitive representations of space might work. But after more than 10 years of theorising since their discovery, the computational functions of grid cells - and whether they support vector-based navigation - has remained largely a mystery. In our most recent paper [ PDF here ]published in Nature, we developed an artificial agent to test the theory that grid cells support vector-based navigation, in keeping with our overarching philosophy that algorithms used for AI can meaningfully approximate elements of the brain. As a first step, we trained a recurrent network to perform the task of localising itself in a virtual environment, using predominantly movement-related velocity signals. This ability is commonly used by mammals when moving through unfamiliar places or in situations where it is not easy to spot familiar landmarks (e.g. when navigating in the dark). We found that grid-like representations (hereafter grid units) spontaneously emerged within the network - providing a striking convergence with the neural activity patterns observed in foraging mammals, and consistent with the notion that grid cells provide an efficient code for space. We next sought to test the theory that grid cells support vector-based navigation by creating an artificial agent to be used as an experimental guinea pig. This was done by combining the initial “grid network” with a larger network architecture, forming an agent that could be trained using deep reinforcement learning to navigate to goals in challenging virtual reality game environments. This agent performed at a super-human level, exceeding the ability of a professional game player, and exhibited the type of flexible navigation normally associated with animals, taking novel routes and shortcuts when they became available. Through a series of experimental manipulations, we showed that grid-like representations were critical for vector-based navigation. For example, when grid cells in the network were silenced, the agent’s ability to navigate was impaired, and the representation of key metrics such as distance and direction to the goal became less accurate. We believe our study constitutes an important step in understanding the fundamental computational purpose of grid cells in the brain and also highlights the benefits they afford to artificial agents. The evidence provides compelling support for the theory that grid cells provide a Euclidean spatial framework - a concept of space - enabling vector-based navigation. More broadly, our work reaffirms the potential of utilising algorithms thought to be used by the brain as inspiration for machine learning architectures . The extensive previous neuroscience research into grid cells makes the agent's interpretability - which is itself a major topic in AI research - significantly easier, by giving us clues about what to look for when trying to understand its internal representations. The work also showcases the potential of using artificial agents actively engaging in complex behaviours within realistic virtual environments to test theories of how the brain works. Taking this principle further, a similar approach could be used to test theories concerning brain areas that are important for perceiving sound or controlling limbs, for example. In the future such networks may well provide a new way for scientists to conduct ‘experiments’, suggesting new theories and even complementing some of the work that is currently conducted in animals. UPDATE 14.05.18: We’d encourage you to read The emergence of grid-like representations by training recurrent neural networks to perform spatial localization by Cueva and Wei, which was published contemporaneously at ICLR. While different in scope and findings, it shows interesting results. In brief, the authors found periodic firing that conformed to the shape of the enclosure, e.g rectangular grids in a square environment and triangular in a triangular environment (fig. 2 of Cueva and Wei). This differs from our study, where we found grid-like units whose firing pattern closely resembles rodent grid cells which typically show hexagonal firing patterns across different shaped environments (e.g. square and circular arena). This work was done by Andrea Banino, Caswell Barry, Benigno Uria, Charles Blundell, Timothy Lillicrap, Piotr Mirowski, Alexander Pritzel, Martin Chadwick, Thomas Degris, Joseph Modayil, Greg Wayne, Hubert Soyer, Fabio Viola, Brian Zhang, Ross Goroshin, Neil Rabinowitz, Razvan Pascanu, Charlie Beattie, Stig Petersen, Amir Sadik, Stephen Gaffney, Helen King, Koray Kavukcuoglu, Demis Hassabis, Raia Hadsell, and Dharshan Kumaran. Read the Nature paper: [ PDF ] Download the original paper (unformatted): [ PDF ] Read Nobel Prize Laureate Edvard Moser's review of the paper .", "date": "2018-05-09"},
{"website": "Deepmind", "title": "Decoupled Neural Interfaces Using Synthetic Gradients", "author": [" Max Jaderberg "], "link": "https://deepmind.com/blog/article/decoupled-neural-networks-using-synthetic-gradients", "abstract": "Neural networks are the workhorse of many of the algorithms developed at DeepMind. For example, AlphaGo uses convolutional neural networks to evaluate board positions in the game of Go and DQN and Deep Reinforcement Learning algorithms use neural networks to choose actions to play at super-human level on video games. This post introduces some of our latest research in progressing the capabilities and training procedures of neural networks called Decoupled Neural Interfaces using Synthetic Gradients . This work gives us a way to allow neural networks to communicate, to learn to send messages between themselves, in a decoupled, scalable manner paving the way for multiple neural networks to communicate with each other or improving the long term temporal dependency of recurrent networks. This is achieved by using a model to approximate error gradients , rather than by computing error gradients explicitly with backpropagation. The rest of this post assumes some familiarity with neural networks and how to train them. If you’re new to this area we highly recommend Nando de Freitas lecture series on Youtube on deep learning and neural networks. If you consider any layer or module in a neural network, it can only be updated once all the subsequent modules of the network have been executed, and gradients have been backpropagated to it. For example look at this simple feed-forward network: Here, after Layer 1 has processed the input, it can only be updated after the output activations (black lines) have been propagated through the rest of the network, generated a loss, and the error gradients (green lines) backpropagated through every layer until Layer 1 is reached. This sequence of operations means that Layer 1 has to wait for the forwards and backwards computation of Layer 2 and Layer 3 before it can update. Layer 1 is locked, coupled, to the rest of the network. Why is this a problem? Clearly for a simple feed-forward network as depicted we don’t need to worry about this issue. But consider a complex system of multiple networks, acting in multiple environments at asynchronous and irregular timescales. Or a big distributed network spread over multiple machines. Sometimes requiring all modules in a network to wait for all other modules to execute and backpropagate gradients is overly time consuming or even intractable. If we decouple the interfaces - the connections -  between modules, every module can be updated independently, and is not locked to the rest of the network. So, how can one decouple neural interfaces - that is decouple the connections between network modules - and still allow the modules to learn to interact? In this paper, we remove the reliance on backpropagation to get error gradients, and instead learn a parametric model which predicts what the gradients will be based upon only local information. We call these predicted gradients synthetic gradients . The synthetic gradient model takes in the activations from a module and produces what it predicts will be the error gradients - the gradient of the loss of the network with respect to the activations. Going back to our simple feed-forward network example, if we have a synthetic gradient model we can do the following: ... and use the synthetic gradients (blue) to update Layer 1 before the rest of the network has even been executed . The synthetic gradient model itself is trained to regress target gradients - these target gradients could be the true gradients backpropagated from the loss or other synthetic gradients which have been backpropagated from a further downstream synthetic gradient model. This mechanism is generic for a connection between any two modules, not just in a feed-forward network. The play-by-play working of this mechanism is shown below, where the change of colour of a module indicates an update to the weights of that module. Using decoupled neural interfaces (DNI) therefore removes the locking of preceding modules to subsequent modules in a network. In experiments from the paper, we show we can train convolutional neural networks for CIFAR-10 image classification where every layer is decoupled using synthetic gradients to the same accuracy as using backpropagation. It’s important to recognise that DNI doesn’t magically allow networks to train without true gradient information. The true gradient information does percolate backwards through the network, but just slower and over many training iterations, through the losses of the synthetic gradient models. The synthetic gradient models approximate and smooth over the absence of true gradients. A legitimate question at this point would be to ask how much computational complexity do these synthetic gradient models add - perhaps you would need a synthetic gradient model architecture that is as complex as the network itself. Quite surprisingly, the synthetic gradient models can be very simple. For feed-forward nets, we actually found out that even a single linear layer works well as a synthetic gradient model. Consequently it is both very easy to train and so produces synthetic gradients rapidly. DNI can be applied to any generic neural network architecture, not just feed-forward networks. An interesting application is to recurrent neural networks (RNNs). An RNN has a recurrent core which is unrolled - repeatedly applied - to process sequential data. Ideally to train an RNN we would unroll the core over the whole sequence (which could be infinitely long), and use backpropagation through time (BPTT) to propagate error gradients backwards through the graph. However in practice, we can only afford to unroll for a limited number of steps due to memory constraints and the need to actually compute an update to our core model frequently. This is called truncated backpropagation through time, and shown below for a truncation of three steps: The change in colour of the core illustrates an update to the core, that the weights have been updated. In this example, truncated BPTT seems to address some issues with training - we can now update our core weights every three steps and only need three cores in memory. However, the fact that there is no backpropagation of error gradients over more than three steps means that the update to the core will not be directly influenced by errors made more than two steps in the future. This limits the temporal dependency that the RNN can learn to model. What if instead of doing no backpropagation between the boundary of BPTT we used DNI and produce synthetic gradients, which model what the error gradients of the future will be? We can incorporate a synthetic gradient model into the core so that at every time step, the RNN core produces not only the output but also the synthetic gradients. In this case, the synthetic gradients would be the predicted gradients of the all future losses with respect to the hidden state activation of the previous timestep. The synthetic gradients are only used at the boundaries of truncated BPTT where we would have had no gradients before. This can be performed during training very efficiently - it merely requires us to keep an extra core in memory as illustrated below. Here a green dotted border indicates just computing gradients with respect to the input state, while a solid green border additionally computes gradients with respect to the core’s parameters. By using DNI and synthetic gradients with an RNN, we are approximating doing backpropagation across an infinitely unrolled RNN. In practice, this results in RNNs which can model longer temporal dependencies . Here’s an example result showing this from the paper. Penn Treebank test error during training (lower is better): This graph shows the application of an RNN trained on next character prediction on Penn Treebank, a language modelling problem. On the y-axis the bits-per-character (BPC) is given, where smaller is better. The x-axis is the number of characters seen by the model as training progresses. The dotted blue, red and grey lines are RNNs trained with truncated BPTT, unrolled for 8 steps, 20 steps and 40 steps - the higher the number of steps the RNN is unrolled before performing backpropagation through time, the better the model is, but the slower it trains. When DNI is used on the RNN unrolled 8 steps (solid blue line) the RNN is able to capture the long term dependency of the 40-step model, but is trained twice as fast (both in terms of data and wall clock time on a regular desktop machine with a single GPU). To reiterate, adding synthetic gradient models allows us to decouple the updates between two parts of a network. DNI can also be applied on hierarchical RNN models - system of two (or more) RNNs running at different timescales. As we show in the paper , DNI significantly improves the training speed of these models by enabling the update rate of higher level modules. Hopefully from the explanations in this post, and a brief look at some of the experiments we report in the paper it is evident that it is possible to create decoupled neural interfaces. This is done by creating a synthetic gradient model which takes in local information and predicts what the error gradient will be. At a high level, this can be thought of as a communication protocol between two modules . One module sends a message (current activations), another one receives the message, and evaluates it using a model of utility (the synthetic gradient model). The model of utility allows the receiver to provide instant feedback (synthetic gradient) to the sender, rather than having to wait for the evaluation of the true utility of the message (via backpropagation). This framework can also be thought about from an error critic point of view [ Werbos ] and is similar in flavour to using a critic in reinforcement learning [ Baxter ]. These decoupled neural interfaces allow distributed training of networks, enhance the temporal dependency learnt with RNNs , and speed up hierarchical RNN systems . We’re excited to explore what the future holds for DNI, as we think this is going to be an important basis for opening up more modular, decoupled, and asynchronous model architectures. Finally, there are lots more details, tricks, and full experiments which you can find in the paper here .", "date": "2016-08-29"},
{"website": "Deepmind", "title": "Learning to write programs that generate images", "author": [" S M Ali Eslami ", " Tejas Kulkarni ", " Oriol Vinyals "], "link": "https://deepmind.com/blog/article/learning-to-generate-images", "abstract": "Through a human’s eyes, the world is much more than just the images reflected in our corneas. For example, when we look at a building and admire the intricacies of its design, we can appreciate the craftsmanship it requires. This ability to interpret objects through the tools that created them gives us a richer understanding of the world and is an important aspect of our intelligence. We would like our systems to create similarly rich representations of the world. For example, when observing an image of a painting we would like them to understand the brush strokes used to create it and not just the pixels that represent it on a screen. In this work , we equipped artificial agents with the same tools that we use to generate images and demonstrate that they can reason about how digits, characters and portraits are constructed. Crucially, they learn to do this by themselves and without the need for human-labelled datasets. This contrasts with recent research which has so far relied on learning from human demonstrations, which can be a time-intensive process. We designed a deep reinforcement learning agent that interacts with a computer paint program , placing strokes on a digital canvas and changing the brush size, pressure and colour. The untrained agent starts by drawing random strokes with no visible intent or structure. To overcome this, we had to create a way to reward the agent that encourages it to produce meaningful drawings. To this end, we trained a second neural network, called the discriminator , whose sole purpose is to predict whether a particular drawing was produced by the agent, or if it was sampled from a dataset of real photographs. The painting agent is rewarded by how much it manages to “fool” the discriminator into thinking its drawings are real. In other words, the agent’s reward signal is itself learned. While this is similar to the approach used in Generative Adversarial Networks (GANs), it differs because the generator in GAN setups is typically a neural network that directly outputs pixels. In contrast, our agent produces images by writing graphics programs to interact with a paint environment. In the first set of experiments, the agent was trained to generate images resembling MNIST digits: it was shown what the digits look like, but not how they are drawn. By attempting to generate images that fool the discriminator, the agent learns to control the brush and to manoeuvre it to fit the style of different digits, a technique known as visual program synthesis . We also trained it to reproduce specific images. Here, the discriminator’s aim is to determine if the reproduced image is a copy of the target image, or if it has been produced by the agent. The more difficult this distinction becomes for the discriminator, the more the agent is rewarded. Crucially, this framework is also interpretable because it produces a sequence of motions that control a simulated brush. This means that the model can apply what it has learnt on the simulated paint program to re-create characters in other similar environments, for instance on a simulated or real robot arm. A video of this can be seen here . There is also potential to scale this framework to real datasets. When trained to paint celebrity faces , the agent is capable of capturing the main traits of the face, such as shape, tone and hair style, much like a street artist would when painting a portrait with a limited number of brush strokes: Recovering structured representations from raw sensations is an ability that humans readily possess and frequently use. In this work we show it is possible to guide artificial agents to  produce similar representations by giving them access to the same tools that we use to recreate the world around us. In doing so they learn to produce visual programs that succinctly express the causal relationships that give rise to their observations. Although our work only represents a small step towards flexible program synthesis, we anticipate that similar techniques may be necessary to enable artificial agents with human-like cognitive, generalisation and communication abilities. Watch the video here , read more about the method in the paper . This work was done by Yaroslav Ganin, Tejas Kulkarni, Igor Babuschkin, S. M. Ali Eslami and Oriol Vinyals, with thanks to Oleg Sushkov, David Barker, Matej Vecerik and Jon Scholz for their help with the robot.", "date": "2018-03-27"},
{"website": "Deepmind", "title": "TF-Replicator: Distributed Machine Learning for Researchers", "author": [" Dominik Grewe ", " Peter Buchlovsky ", " Chris Jones ", " David Budden ", " Tom Hennigan ", " Aedan Pope "], "link": "https://deepmind.com/blog/article/tf-replicator-distributed-machine-learning", "abstract": "At DeepMind, the Research Platform Team builds infrastructure to empower and accelerate our AI research. Today, we are excited to share how we developed TF-Replicator, a software library that helps researchers deploy their TensorFlow models on GPUs and Cloud TPUs with minimal effort and no previous experience with distributed systems. TF-Replicator’s programming model has now been open sourced as part of TensorFlow’s tf.distribute.Strategy . This blog post gives an overview of the ideas and technical challenges underlying TF-Replicator. For a more comprehensive description, please read our arXiv paper . A recurring theme in recent AI breakthroughs - from AlphaFold to BigGAN to AlphaStar -  is the need for effortless and reliable scalability. Increasing amounts of computational capacity allow researchers to train ever-larger neural networks with new capabilities. To address this, the Research Platform Team developed TF-Replicator, which allows researchers to target different hardware accelerators for Machine Learning, scale up workloads to many devices, and seamlessly switch between different types of accelerators. While it was initially developed as a library on top of TensorFlow, TF-Replicator’s API has since been integrated into TensorFlow 2.0’s new tf.distribute.Strategy . While TensorFlow provides direct support for CPU, GPU, and TPU ( Tensor Processing Unit ) devices, switching between targets requires substantial effort from the user. This typically involves specialising code for a particular hardware target, constraining research ideas to the capabilities of that platform. Some existing frameworks built on top of TensorFlow, e.g. Estimators , seek to address this problem. However, they are typically targeted at production use cases and lack the expressivity and flexibility required for rapid iteration of research ideas. Our original motivation for developing TF-Replicator was to provide a simple API for DeepMind researchers to use TPUs . TPUs provide scalability for Machine Learning workloads, enabling research breakthroughs such as state-of-the-art image synthesis with our BigGAN model. TensorFlow’s native API for TPUs differs from how GPUs are targeted, forming a barrier to TPU adoption. TF-Replicator provides a simpler, more user-friendly API that hides the complexity of TensorFlow’s TPU API. Critically, the Research Platform Team developed the TF-Replicator API in close collaboration with researchers across various machine learning disciplines to ensure the necessary flexibility and ease-of-use. Code written using TF-Replicator looks similar to code written in TensorFlow for a single device, allowing users the freedom to define their own model run loop. The user simply needs to define (1) an input function that exposes a Dataset, and (2) a step function that defines the logic of their model (e.g. a single step of gradient descent): Scaling computation to multiple devices requires the devices to communicate with each other. In the context of training Machine Learning models, the most common form of communication is to accumulate gradients for use in optimisation algorithms such as Stochastic Gradient Descent . We therefore provide a convenient method to wrap TensorFlow Optimizers , so that gradients are accumulated across devices before updating the model’s parameters. For more general communication patterns we provide MPI -like primitives, such as `all_reduce` and `broadcast`. These make it trivial to implement operations such as global batch normalisation, a technique that is crucial to scale up training of our BigGAN models (see Section 3 of the paper). For multi-GPU computation TF-Replicator relies on an “in-graph replication” pattern, where the computation for each device is replicated in the same TensorFlow graph. Communication between devices is achieved by connecting nodes from the devices’ corresponding sub-graphs. Implementing this in TF-Replicator was challenging, as communication can occur at any point in the data-flow graph. The order in which computations are constructed is therefore critical. Our first idea was to build each device’s sub-graph concurrently in a separate Python thread. When encountering a communication primitive, the threads synchronise and the main thread inserts the required cross-device computation. After that, each thread would continue building its device’s computation. However, at the time we considered this approach, TensorFlow’s graph building API was not thread-safe which made concurrently building sub-graphs in different threads very difficult. Instead, we used graph rewriting to insert the communication after all devices’ sub-graphs had been built. When constructing the sub-graphs, placeholders are inserted in places where communication is required. We then collect all matching placeholders across devices and replace them with the appropriate cross-device computation. By collaborating closely with researchers throughout the design and implementation of TF-Replicator, we were able to build a library that allows users to easily scale computation across many hardware accelerators, while leaving them with the control and flexibility required to do cutting-edge AI research. For example, we added MPI-style communication primitives such as all-reduce following discussion with researchers. TF-Replicator and other shared infrastructure allows us to build increasingly complex experiments on robust foundations and quickly spread best practices throughout DeepMind. At the time of writing, TF-Replicator is the most widely used interface for TPU programming at DeepMind. While the library itself is not constrained to training neural networks, it is most commonly used for training on large batches of data. The BigGAN model, for example, was trained on batches of size 2048 across up to 512 cores of a TPUv3 pod. In Reinforcement Learning agents with a distributed actor-learner setup, such as our importance weighted actor-learner architectures , scalability is achieved by having many actors generating new experiences by interacting with the environment. This data is then processed by the learner to improve the agent’s policy, represented as a neural network. To cope with an increasing number of actors, TF-Replicator can be used to easily distribute the learner across many hardware accelerators. These and other examples are described in more detail in our arXiv paper . TF-Replicator is just one of many examples of impactful technology built by DeepMind’s Research Platform Team. Many of DeepMind’s breakthroughs in AI, from AlphaGo to AlphaStar, were enabled by the team. If you share our mission and are excited about accelerating state-of-the-art AI research, look out for open Software Engineering positions in Research Platform at https://deepmind.com/careers (machine learning experience is optional for these roles). This work was completed by the Research Platform Team at DeepMind. We’d like to thank Frederic Besse, Fabio Viola, John Aslanides, Andy Brock, Aidan Clark, Sergio Gómez Colmenarejo, Karen Simonyan, Sander Dieleman, Lasse Espeholt, Akihiro Matsukawa, Tim Harley, Jean-Baptiste Lespiau, Koray Kavukcuoglu, Dan Belov and many others at DeepMind for their valuable feedback throughout the development of TF-Replicator. We'd also like to thank Priya Gupta, Jonathan Hseu, Josh Levenberg, Martin Wicke and others at Google for making these ideas available to all TensorFlow users as part of tf.distribute.Strategy.", "date": "2019-03-07"},
{"website": "Deepmind", "title": "A neural approach to relational reasoning", "author": [" Adam Santoro ", " David Raposo ", " Nick Watters "], "link": "https://deepmind.com/blog/article/neural-approach-relational-reasoning", "abstract": "Consider the reader who pieces together the evidence in an Agatha Christie novel to predict the culprit of the crime, a child who runs ahead of her ball to prevent it rolling into a stream or even a shopper who compares the relative merits of buying kiwis or mangos at the market. We carve our world into relations between things. And we understand how the world works through our capacity to draw logical conclusions about how these different things - such as physical objects, sentences, or even abstract ideas - are related to one another. This ability is called relational reasoning and is central to human intelligence. We construct these relations from the cascade of unstructured sensory inputs we experience every day. For example, our eyes take in a barrage of photons, yet our brain organises this “blooming, buzzing confusion” into the particular entities that we need to relate. A key challenge in developing artificial intelligence systems with the flexibility and efficiency of human cognition is giving them a similar ability -  to reason about entities and their relations from unstructured data. Solving this would allow these systems to generalize to new combinations of entities, making infinite use of finite means. Modern deep learning methods have made tremendous progress solving problems from unstructured data, but they tend to do so without explicitly considering the relations between objects. In two new papers, we explore the ability for deep neural networks to perform complicated relational reasoning with unstructured data. In the first paper - A simple neural network module for relational reasoning - we describe a Relation Network (RN) and show that it can perform at superhuman levels on a challenging task. While in the second paper - Visual Interaction Networks - we describe a general purpose model that can predict the future state of a physical object based purely on visual observations. To explore the idea of relational reasoning more deeply and to test whether it is an ability that can be easily added to existing systems, we created a simple-to-use, plug-and-play RN module that can be added to existing neural network architectures. An RN-augmented network is able to take an unstructured input - say, an image or a series of sentences - and implicitly reason about the relations of objects contained within it. For example, a network using RN may be presented with a scene consisting of various shapes (spheres, cubes, etc.) sitting on a table. To work out the relations between them  (e.g. the sphere is bigger than the cube), the network must take the unstructured stream of pixels from the image and figure out what counts as an object in the scene. The network is not explicitly told what counts as an object and must figure it out for itself. The representations of these objects are then grouped  into pairs (e.g. the sphere and the cube) and passed through the RN module, which compares them to establish a “relation” (e.g. the sphere is bigger than the cube). These relations are not hardcoded, but must be learnt by the RN as it compares each possible pair. Finally, it adds up all these relations to produce an output for all of the pairs of shapes in the scene. We tested this model on several tasks including CLEVR -  a visual question answering task designed to explicitly explore a model’s ability to perform different types of reasoning, such as counting, comparing, and querying. CLEVR consists of images like this: Each image has associated questions that interrogates the relations between objects in the scene. For example, a question about the image above might ask: “There is a tiny rubber thing that is the same colour as the large cylinder; what shape is it? State-of-the-art results on CLEVR using standard visual question answering architectures are 68.5%, compared to 92.5% for humans. But using our RN-augmented network, we were able to show super-human performance of 95.5%. To check the versatility of the RN, we also tested the RN on a very different language task. Specifically, we used the bAbI suite - a series of of text-based question answering tasks. bAbI consists of a number of stories, which are a variable number of sentences culminating in a question. For example, “Sandra picked up the football” and “Sandra went to the office” may lead to the question “Where is the football?” (answer: “office”). The RN-augmented network scored more than 95% on 18 of the 20 bAbI tasks, similar to existing state-of-the-art models. Notably, it scored better on certain tasks - such as induction - which caused problems for these more established models. Full results of all these tests and more are available in the paper . Another key part of relational reasoning involves predicting the future in a physical scene. From just a glance, humans can infer not only what objects are where, but also what will happen to them over the upcoming seconds, minutes and even longer in some cases. For example, if you kick a football against a wall, your brain predicts what will happen when the ball hits the wall and how their movements will be affected afterwards (the ball will ricochet at a speed proportional to the kick and - in most cases - the wall will remain where it is). These predictions are guided by a sophisticated cognitive system for reasoning about objects and their physical interactions. In this related work we developed the “Visual Interaction Network” (VIN) - a model that mimics this ability. The VIN is able to infer the states of multiple physical objects from just a few frames of video, and then use this to predict object positions many steps into the future. This differs from generative models, which might visually “imagine” the next few frames of a video. Instead, the VIN predicts how the underlying relative states of the objects evolve. The VIN is comprised of two mechanisms: a visual module and a physical reasoning module. Together they are able to process a visual scene into a set of distinct objects and learn an implicit system of physical rules which can predict what will happen to these objects in the future. We tested the VIN’s ability to do this in a variety of systems including bouncing billiards, masses connected by springs, and planetary systems with gravitational forces. Our results show that the VIN can accurately predict what will happen to objects hundreds of steps into the future. In experimental comparisons with previously published models and variants of the VIN in which its mechanism for relational reasoning was removed, the full VIN performed significantly better. Again, full details of the results can be found in our paper . Both of these papers show promising approaches to understanding the challenge of relational reasoning. They show how neural networks can be given a powerful ability to reason by decomposing the world into systems of objects and their relations, allowing them to generalise to new combinations of objects and reason about scenes that superficially might look very different but have underlying common relations. We believe these approaches are scalable and could be applied to many more tasks, helping build more sophisticated models of reasoning and allowing us to better understand a key component of humans’ powerful and flexible general intelligence that we take for granted every day. The Relation Network was developed by Adam Santoro, David Raposo, David G.T. Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia and Timothy Lillicrap The Visual Interaction Network was developed by Nicholas Watters, Daniel Zoran, Theophane Weber, Peter Battaglia, Razvan Pascanu and Andrea Tachetti", "date": "2017-06-06"},
{"website": "Deepmind", "title": "Understanding Agent Cooperation", "author": [" Joel Leibo ", " Marc Lanctot ", " Thore Graepel ", " Vinicius Zambaldi ", " Janusz Marecki "], "link": "https://deepmind.com/blog/article/understanding-agent-cooperation", "abstract": "We employ deep multi-agent reinforcement learning to model the emergence of cooperation. The new notion of sequential social dilemmas allows us to model how rational agents interact, and arrive at more or less cooperative behaviours depending on the nature of the environment and the agents’ cognitive capacity. The research may enable us to better understand and control the behaviour of complex multi-agent systems such as the economy, traffic, and environmental challenges. Self-interested people often work together to achieve great things. Why should this be the case, when it is in their best interest to just care about their own wellbeing and disregard that of others? The question of how and under what circumstances selfish agents cooperate is one of the fundamental question in the social sciences. One of the simplest and most elegant models to describe this phenomenon is the well-known game of Prisoner’s Dilemma from game theory. Two suspects are arrested and put into solitary confinement. Without confessions the police do not have sufficient evidence to convict the two suspects on the main charge, but have good prospects to achieve one year prison sentences for both. In order to entice the prisoners to confess, they offer them simultaneously the following deal: If you testify against the other prisoner (“defect”) you will be released, but the other prisoner will serve three years in prison. If both prisoners testify against each other (“defect”), they will both serve two years. It turns out, that rational agents - in the sense of game theory - should always defect in this game, because no matter what the other prisoner chooses to do, they will be better off defecting. Yet, paradoxically, if both prisoners reason in this way, they will each have to serve two years in prison - one year more than if they had cooperated and remained silent. This paradox is what we refer to as a social dilemma . Recent progress in artificial intelligence and specifically deep reinforcement learning provides us with the tools to look at the problem of social dilemmas through a new lens. Traditional game theorists model social dilemmas in terms of a simple binary choice between cooperate and defect for each agent. In real life, both cooperating and defecting may require complex behaviours, involving difficult sequences of actions that agents need to learn to execute. We refer to this new setting as sequential social dilemmas , and use artificial agents trained by deep multi-agent reinforcement learning to study it. As an example, consider the following Gathering game: Two agents, Red and Blue, roam a shared world and collect apples to receive positive rewards. They may also direct a beam at the other agent, “tagging them”, to temporarily remove them from the game, but this action does not trigger a reward. A visualisation of agents playing the gathering game can be seen below. We let the agents play this game many thousands of times and let them learn how to behave rationally using deep multi-agent reinforcement learning. Rather naturally, when there are enough apples in the environment, the agents learn to peacefully coexist and collect as many apples as they can. However, as the number of apples is reduced, the agents learn that it may be better for them to tag the other agent to give themselves time on their own to collect the scarce apples. It turns out that this Gathering game shares many characteristics of the original Prisoner’s Dilemma, but allows us to study the more interesting case in which agents need to learn to implement their desired behaviour: Either to cooperate and collect apples, or to defect and try to tag the other agent. In these sequential social dilemmas, we can now study what factors contribute to agents’ cooperation. For example, the following plot shows that in the Gathering game greater scarcity of apples leads to more “tagging” behaviour of agents. Furthermore, agents with the capacity to implement more complex strategies try to tag the other agent more frequently, i.e. behave less cooperatively - no matter how we vary the scarcity of apples. Interestingly, in another game called Wolfpack (see gameplay video below), which requires close coordination to successfully cooperate, we find that greater capacity to implement complex strategies leads to more cooperation between agents, the opposite of the finding with Gathering. So, depending on the situation, having a greater capacity to implement complex strategies may yield either more or less cooperation. The new framework of sequential social dilemmas allows us to take into account not only the outcome of the interaction (as in the Prisoner’s dilemma), but also the difficulty of learning to implement a given strategy. In summary, we showed that we can apply the modern AI technique of deep multi-agent reinforcement learning to age-old questions in social science such as the mystery of the emergence of cooperation. We can think of the trained AI agents as an approximation to economics’ rational agent model “homo economicus” . Hence, such models give us the unique ability to test policies and interventions into simulated systems of interacting agents - both human and artificial. As a consequence, we may be able to better understand and control complex multi-agent systems such as the economy, traffic systems, or the ecological health of our planet - all of which depend on our continued cooperation.", "date": "2017-02-09"},
{"website": "Deepmind", "title": "Unsupervised learning: The curious pupil", "author": [" Alexander Graves ", " Kelly Clancy "], "link": "https://deepmind.com/blog/article/unsupervised-learning", "abstract": "One in a series of posts explaining the theories underpinning our research. Over the last decade, machine learning has made unprecedented progress in areas as diverse as image recognition, self-driving cars and playing complex games like Go. These successes have been largely realised by training deep neural networks with one of two learning paradigms—supervised learning and reinforcement learning. Both paradigms require training signals to be designed by a human and passed to the computer. In the case of supervised learning, these are the “targets” (such as the correct label for an image); in the case of reinforcement learning, they are the “rewards” for successful behaviour (such as getting a high score in an Atari game). The limits of learning are therefore defined by the human trainers. While some scientists contend that a sufficiently inclusive training regime—for example, the ability to complete a very wide variety of tasks—should be enough to give rise to general intelligence, others believe that true intelligence will require more independent learning strategies. Consider how a toddler learns, for instance. Her grandmother might sit with her and patiently point out examples of ducks (acting as the instructive signal in supervised learning), or reward her with applause for solving a woodblock puzzle (as in reinforcement learning). But the vast majority of a toddler’s time is spent naively exploring the world, making sense of her surroundings through curiosity, play, and observation. Unsupervised learning is a paradigm designed to create autonomous intelligence by rewarding agents (that is, computer programs) for learning about the data they observe without a particular task in mind. In other words, the agent learns for the sake of learning. A key motivation for unsupervised learning is that, while the data passed to learning algorithms is extremely rich in internal structure (e.g., images, videos and text), the targets and rewards used for training are typically very sparse (e.g., the label ‘dog’ referring to that particularly protean species, or a single one or zero to denote success or failure in a game). This suggests that the bulk of what is learned by an algorithm must consist of understanding the data itself, rather than applying that understanding to particular tasks. 2012 was a landmark year for deep learning, when AlexNet (named after its lead architect Alex Krizhnevsky) swept the ImageNet classification competition . AlexNet’s abilities to recognize images were unprecedented, but even more striking is what was happening under the hood. When researchers analysed what AlexNet was doing, they discovered that it interprets images by building increasingly complex internal representations of its inputs . Low-level features, such as textures and edges, are represented in the bottom layers, and these are then combined to form high-level concepts such as wheels and dogs in higher layers. This is remarkably similar to how information is processed in our brains, where simple edges and textures in primary sensory processing areas are assembled into complex objects like faces in higher areas. The representation of a complex scene can therefore be built out of visual primitives, in much the same way that meaning emerges from the individual words comprising a sentence. Without explicit guidance to do so, the layers of AlexNet had discovered a fundamental ‘vocabulary’ of vision in order to solve its task. In a sense, it had learned to play what Wittgenstein called a ‘language game’ that iteratively translates from pixels to labels. From the perspective of general intelligence, the most interesting thing about AlexNet’s vocabulary is that it can be reused, or transferred, to visual tasks other than the one it was trained on, such as recognising whole scenes rather than individual objects . Transfer is essential in an ever-changing world, and humans excel at it: we are able to rapidly adapt the skills and understanding we’ve gleaned from our experiences (our ‘world model’) to whatever situation is at hand. For example, a classically-trained pianist can pick up jazz piano with relative ease. Artificial agents that form the right internal representations of the world, the reasoning goes, should be able to do similarly. Nonetheless, the representations learned by classifiers such as AlexNet have limitations. In particular, as the network was only trained to label images with a single class (cat, dog, car, volcano), any information not required to infer the label—no matter how useful it might be for other tasks—is liable to be ignored. For example, the representations may fail to capture the background of the image if the label always refers to the foreground. A possible solution is to provide more comprehensive training signals, like detailed captions describing the images : not just “dog,” but “A Corgi catching a frisbee in a sunny park.” However, such targets are laborious to provide, especially at scale, and still may be insufficient to capture all the information needed to complete a task. The basic premise of unsupervised learning is that the best way to learn rich, broadly transferable representations is to attempt to learn everything that can be learned about the data. If the notion of transfer through representation learning seems too abstract, consider a child who has learned to draw people as stick figures. She has discovered a representation of the human form that is both highly compact and rapidly adaptable. By augmenting each stick figure with specifics, she can create portraits of all her classmates: glasses for her best friend, her deskmate in his favorite red tee-shirt. And she has developed this skill not in order to complete a specific task or receive a reward, but rather in response to her basic urge to reflect the world around her. Perhaps the simplest objective for unsupervised learning is to train an algorithm to generate its own instances of data. So-called generative models should not simply reproduce the data they are trained on (an uninteresting act of memorisation), but rather build a model of the underlying class from which that data was drawn: not a particular photograph of a horse or a rainbow, but the set of all photographs of horses and rainbows; not a specific utterance from a specific speaker, but the general distribution of spoken utterances. The guiding principle of generative models is that being able to construct a convincing example of the data is the strongest evidence of having understood it: as Richard Feynman put it, \"what I cannot create, I do not understand.\" For images, the most successful generative model so far has been the Generative Adversarial Network (GAN for short), in which two networks—a generator and a discriminator—engage in a contest of discernment akin to that of an artistic forger and a detective. The generator produces images with the goal of tricking the discriminator into believing they are real; the discriminator, meanwhile, is rewarded for spotting the fakes. The generated images, first messy and random, are refined over many iterations, and the ongoing dynamic between the networks leads to ever-more realistic images that are in many cases indistinguishable from real photographs . Generative adversarial networks can also dream details of landscapes defined by the rough sketches of users . A glance at the images below is enough to convince us that the network has learned to represent many of the key features of the photographs they were trained on, such as the structure of animal’s bodies, the texture of grass, and detailed effects of light and shade (even when refracted through a soap bubble). Close inspection reveals slight anomalies, such as the white dog’s apparent extra leg and the oddly right-angled flow of one of the jets in the fountain. While the creators of generative models strive to avoid such imperfections, their visibility highlights one of the benefits of recreating familiar data such as images: by inspecting the samples, researchers can infer what the model has and hasn’t learned. Another notable family within unsupervised learning are autoregressive models, in which the data is split into a sequence of small pieces, each of which is predicted in turn. Such models can be used to generate data by successively guessing what will come next, feeding in a guess as input and guessing again. Language models, where each word is predicted from the words before it, are perhaps the best known example: these models power the text predictions that pop up on some email and messaging apps. Recent advances in language modelling have enabled the generation of strikingly plausible passages, such as the one shown below from OpenAI’s GPT-2 . One interesting inconsistency in the text is that the unicorns are described as “four-horned”: again, it is fascinating to probe the limitations of the network’s understanding. By controlling the input sequence used to condition the out predictions, autoregressive models can also be used to transform one sequence into another. This demo uses a conditional autoregressive model to transform text into realistic handwriting. WaveNet transforms text into natural sounding speech, and is now used to generate voices for Google Assistant . A similar process of conditioning and autoregressive generation can be used to translate from one language to another . Autoregressive models learn about data by attempting to predict each piece of it in a particular order. A more general class of unsupervised learning algorithms can be built by predicting any part of the data from any other. For example, this could mean removing a word from a sentence, and attempting to predict it from whatever remains . By learning to make lots of localised predictions, the system is forced to learn about the data as a whole. One concern around generative models is their potential for misuse. While manipulating evidence with photo, video, and audio editing has been possible for a long time, generative models could make it even easier to edit media with malicious intent. We have already seen demonstrations of so-called ‘deepfakes’—for instance, this fabricated video footage of President Obama . It’s encouraging to see that several major efforts to address these challenges are already underway, including using statistical techniques to help detect synthetic media and verify authentic media, raising public awareness , and discussions around limiting the availability of trained generative models. Furthermore, generative models can themselves be used to detect synthetic media and anomalous data—for example when detecting fake speech or identifying payment abnormalities to protect customers against fraud. Researchers need to work on generative models in order to better understand them and mitigate downstream risks. Generative models are fascinating in their own right, but our principal interest in them at DeepMind is as a stepping stone towards general intelligence. Endowing an agent with the ability to generate data is a way of giving it an imagination, and hence the ability to plan and reason about the future . Even without explicit generation, our studies show that learning to predict different aspects of the environment enriches the agent’s world model , and thereby improves its ability to solve problems. These results resonate with our intuitions about the human mind. Our ability to learn about the world without explicit supervision is fundamental to what we regard as intelligence. On a train ride we might listlessly gaze through the window, drag our fingers over the velvet of the seat, regard the passengers sitting across from us. We have no agenda in these studies: we almost can’t help but gather information, our brains ceaselessly working to understand the world around us, and our place within it.", "date": "2019-06-25"},
{"website": "Deepmind", "title": "Fast reinforcement learning through the composition of behaviours", "author": [" André Barreto ", " Shaobo Hou ", " Diana Borsa ", " David Silver ", " Doina Precup "], "link": "https://deepmind.com/blog/article/fast-reinforcement-learning-through-the-composition-of-behaviours", "abstract": "Imagine if you had to learn how to chop, peel and stir all over again every time you wanted to learn a new recipe. In many machine learning systems, agents often have to learn entirely from scratch when faced with new challenges. It’s clear, however, that people learn more efficiently than this: they can combine abilities previously learned. In the same way that a finite dictionary of words can be reassembled into sentences of near infinite meanings, people repurpose and re-combine skills they already possess in order to tackle novel challenges. In nature, learning arises as an animal explores and interacts with its environment in order to gather food and other rewards. This is the paradigm captured by reinforcement learning (RL): interactions with the environment reinforce or inhibit particular patterns of behavior depending on the resulting reward (or penalty). Recently, the combination of RL with deep learning has led to impressive results, such as agents that can learn how to play boardgames like Go and chess , the full spectrum of Atari games, as well as more modern, difficult video games like Dota and StarCraft II . A major limitation in RL is that current methods require vast amounts of training experience. For example, in order to learn how to play a single Atari game, an RL agent typically consumes an amount of data corresponding to several weeks of uninterrupted playing. A study led by researchers at MIT and Harvard indicated that in some cases, humans are able to reach the same performance level in just fifteen minutes of play. One possible reason for this discrepancy is that, unlike humans, RL agents usually learn a new task from scratch. We would like our agents to leverage knowledge acquired in previous tasks to learn a new task more quickly, in the same way that a cook will have an easier time learning a new recipe than someone who has never prepared a dish before. In an article recently published in the Proceedings of the National Academy of Sciences (PNAS), we describe a framework aimed at endowing our RL agents with this ability. Two ways of representing the world To illustrate our approach, we will explore an example of an activity that is (or at least used to be) an everyday routine: the commute to work. Imagine the following scenario: an agent must commute every day from its home to its office, and it always gets a coffee on the way. There are two cafes between the agent's house and the office: one has great coffee but is on a longer path, and the other one has decent coffee but a shorter commute (Figure 1). Depending on how much the agent values the quality of the coffee versus how much of a rush it is in on a given day, it may choose one of two routes (the yellow and blue paths on the map shown in Figure 1). Traditionally, RL algorithms fall into two broad categories: model-based and model-free agents (Figures 2 & 3). A model-based agent (Figure 2) builds a representation of many aspects of the  environment. An agent of this type might know how the different locations are connected, the quality of the coffee in each cafe, and anything else that is considered relevant. A model-free agent (Figure 3) has a much more compact representation of its environment. For instance, a value-based model-free agent would have a single number associated with each possible route leaving its home; this is the expected \"value\" of each route, reflecting a specific weighing of coffee quality vs. commute length. Take the blue path shown in Figure 1 as an example. Say this path has length 4, and the coffee the agent gets by following it is rated 3 stars. If the agent cares about the commute distance 50% more than it cares about the quality of the coffee, the value of this path will be  (-1.5 x 4) + (1 x 3) = -3  (we use a negative weight associated with the distance to indicate that longer commutes are undesirable). We can interpret the relative weighting of the coffee quality versus the commute distance as the agent’s preferences . For any fixed set of preferences, a model-free and a model-based agent would choose the same route. Why then have a more complicated representation of the world, like the one used by a model-based agent, if the end result is the same? Why learn so much about the environment if the agent ends up sipping the same coffee? Preferences can change day to day: an agent might take into account how hungry it is, or whether it’s running late to a meeting, in planning its route to the office. One way for a model-free agent to handle this is to learn the best route associated with every possible set of preferences. This is not ideal because learning every possible combination of preferences will take a long time. It is also impossible to learn a route associated with every possible set of preferences if there are infinitely many of them. In contrast, a model-based agent can adapt to any set of preferences, without any learning, by \"imagining\" all possible routes and asking how well they would fulfill its current mindset. However, this approach also has drawbacks. Firstly, ”mentally” generating and evaluating all possible trajectories can be computationally demanding. Secondly, building a model of the entire world can be very difficult in complex environments. Model-free agents learn faster but are brittle to change. Model-based agents are flexible but can be slow to  learn. Is there an intermediate solution? Successor features: a middle ground A recent study in behavioural science and neuroscience suggests that in certain situations, humans and animals make decisions based on an algorithmic model that is a compromise between the model-free and model-based approaches ( here and here ). The hypothesis is that, like model-free agents, humans also compute the value of alternative strategies in the form of a number. But, instead of summarising a single quantity, humans summarise many different quantities describing the world around them, reminiscent of model-based agents. It’s possible to endow an RL agent with the same ability. In our example, such an agent would have, for each route, a number representing the expected quality of coffee and a number representing the distance to the office. It could also have numbers associated with things the agent is not deliberately trying to optimise but are nevertheless available to it for future reference (for example, the quality of the food in each cafe). The aspects of the world the agent cares about and keeps track of are sometimes referred to as “features”. Because of that, this representation of the world is called successor features (previously termed the “successor representation” in its original incarnation ). Successor features can be thought of as a middle ground between the model-free and model-based representations. Like the latter, successor features summarise many different quantities, capturing the world beyond a single value. However, like in the model-free representation, the quantities the agent keeps track of are simple statistics summarising the features it cares about. In this way, successor features are like an “unpacked” version of the model-free agent. Figure 4 illustrates how an agent using successor features would see our example environment. Using successor features: composing novel plans from a dictionary of policies Successor features are a useful representation because they allow for a route to be evaluated under different sets of preferences. Let’s use the blue route in Figure 1 as an example again. Using successor features, the agent would have three numbers associated with this path: its length (4), the quality of the coffee (3) and the quality of the food (5). If the agent already ate breakfast it will probably not care much about the food; also, if it is late, it might care about the commute distance more than the quality of the coffee --say, 50% more, as before. In this scenario the value of the blue path would be  (-1.5 x 4) + (1 x 3) + (0 x 5) = -3, as in the example given above. But now, on a day when the agent is hungry, and thus cares about the food as much as it cares about the coffee, it can immediately update the value of this route to  (-1.5 x 4) + (1 x 3) + (1 x 5) = 2. Using the same strategy, the agent can evaluate any route according to any set of preferences. In our example, the agent is choosing between routes. More generally, the agent will be searching for a policy: a prescription of what to do in every possible situation. Policies and routes are closely related: in our example, a policy that chooses to take the road to cafe A from home and then chooses the road to the office from cafe A would traverse the blue path. So, in this case, we can talk about policies and routes interchangeably (this would not be true if there were some randomness in the environment, but we will leave this detail aside).  We discussed how successor features allow a route (or policy) to be evaluated under different sets of preferences. We call this process generalised policy evaluation , or GPE. Why is GPE useful? Suppose the agent has a dictionary of policies (for example, known routes to the office). Given a set of preferences, the agent can use GPE to immediately evaluate how well each policy in the dictionary would perform under those preferences. Now the really interesting part: based on this quick evaluation of known policies, the agent can create entirely new policies on the fly. The way it does it is simple: every time the agent has to make a decision, it asks the following question: “if I were to make this decision and then follow the policy with the maximum value thereafter, which decision would lead to the maximum overall value?” Surprisingly, if the agent picks the decision leading to the maximum overall value in each situation, it ends up with a policy that is often better than the individual policies used to create it. This process of “stitching together” a set of policies to create a better policy is called generalised policy improvement , or GPI. Figure 5 illustrates how GPI works using our running example. The performance of a policy created through GPI will depend on how many policies the agent knows. For instance, in our running example, as long as the agent knows the blue and yellow  paths, it will find the best route for any preferences over coffee quality and commute length. But the GPI policy will not always find the best route. In Figure 1, the agent would never visit cafe A and then cafe B if it did not already know a policy that connected them in this way (like the orange route in the figure). A simple example to show GPE and GPI in action To illustrate the benefits of GPE and GPI, we now give a glimpse of one of the experiments from our recent publication (see paper for full details). The experiment uses a simple environment that represents in an abstract way the type of problem in which our approach can be useful. As shown in Figure 6, the environment is a 10 x 10 grid with 10 objects spread across it. The agent only gets a non-zero reward if it picks up an object, in which case another object pops up in a random location. The reward associated with an object depends on its type. Object types are meant to represent concrete or abstract concepts; to connect with our running example, we will consider that each object is either “coffee” or “food” (these are the features the agent keeps track of). Clearly, the best strategy for the agent depends on its current preferences over coffee or food. For example, in Figure 6, an agent that only cares about coffee may follow the path in red, while an agent focused exclusively on food would follow the blue path. We can also imagine intermediate situations in which the agent wants coffee and food with different weights, including the case in which the agent wants to avoid one of them. For example, if the agent wants coffee but really does not want food, the gray path in Figure 6 may be a better alternative to the red one. The challenge in this problem is to quickly adapt to a new set of preferences (or a “task”). In our experiments we showed how one can do so using GPE and GPI. Our agent learned two policies: one that seeks coffee and one that seeks food. We then tested how well the policy computed by GPE and GPI performed on tasks associated with different preferences. In figure 7 we compare our method with a model-free agent on the task whose goal is to seek coffee while avoiding food. Observe how the agent using GPE and GPI instantaneously synthesises a reasonable policy, even though it never learned how to deliberately avoid objects. Of course, the policy computed by GPE and GPI can be used as an initial solution to be later refined through learning, which means that it would match the final performance of a model-free agent but would probably get there faster. Figure 7 shows the performance of GPE and GPI on one specific task. We have also tested the same agent across many other tasks. Figure 8 shows what happens with the performance of the model-free and GPE-GPI agents when we change the relative importance of coffee and food. Note that, while the model-free agent has to learn each task separately, from scratch, the GPE-GPI agent only learns two policies and then quickly adapts to all of the tasks. The experiments above used a simple environment designed to exhibit the properties needed by GPE and GPI without unnecessary confounding factors. But GPE and GPI have also been applied at scale. For example, in previous papers ( here and here ) we showed how the same strategy also works when we replace a grid world with a three dimensional environment in which the agent receives observations from a first-person perspective (see illustrative videos here and here ). We have also used GPE and GPI to allow a four-legged simulated robot to navigate along any direction after having learned how to do so along three directions only (see paper here and video here ). GPE and GPI in context The work on GPE and GPI is at the intersection of two separate branches of research related to these operations individually. The first, related to GPE, is the work on the successor representation, initiated with Dayan’s seminal paper from 1993. Dayan’s paper inaugurated a line of work in neuroscience that is very active to this day (see further reading: \"The successor representation in neuroscience\") . Recently, the successor representation reemerged in the context of RL (links here and here ), where it is also referred to as “successor features”, and became an active line of research there as well ( see further reading: \"GPE, successor features, and related approaches\") . Successor features are also closely related to general value functions , a concept based on Sutton et al.’s hypothesis that relevant knowledge can be expressed in the form of many predictions about the world (also discussed here ). The definition of successor features has independently emerged in other contexts within RL, and is also related to more recent approaches normally associated with deep RL. The second branch of research at the origins of GPE and GPI, related to the latter, is concerned with composing behaviours to create new behaviours. The idea of a decentralised controller that executes sub-controllers has come up multiple times over the years (e.g., Brooks, 1986 ), and its implementation using value functions can be traced back to at least as far as 1997, with Humphrys’ and Karlsson’s PhD theses. GPI is also closely related to hierarchical RL, whose foundations were laid down in the 1990's and early 2000’s in the works by Dayan and Hinton , Parr and Russell , Sutton, Precup and Singh , and Dietterich . Both the composition of behaviours and hierarchical RL are today dynamic areas of research (see further reading: \"GPI, hierarchical RL, and related approaches\") . Mehta et al . were probably the first ones to jointly use GPE and GPI, although in the scenario they considered GPI reduces to a single choice at the outset (that is, there is no “stitching” of policies). The version of GPE and GPI discussed in this blog post was first proposed in 2016 as a mechanism to promote transfer learning . Transfer in RL dates back to Singh’s work in 1992 and has recently experienced a resurgence in the context of deep RL, where it continues to be an active area of research ( see further reading: \" GPE + GPI, transfer learning, and related approaches \"). See more information about these works below, where we also provide a list of suggestions for further readings. A compositional approach to reinforcement learning In summary, a model-free agent cannot easily adapt to new situations, for example to accommodate sets of preferences it has not experienced before. A model-based agent can adapt to any new situation, but in order to do so it first has to learn a model of the entire world. An agent based on GPE and GPI offers an intermediate solution: although the model of the world it learns is considerably smaller than that of a model-based agent, it can quickly adapt to certain situations, often with good performance. We discussed specific instantiations of GPE and GPI, but these are in fact more general concepts. At an abstract level, an agent using GPE and GPI proceeds in two steps. First, when faced with a new task, it asks: “How well would solutions to known tasks perform on this new task?” This is GPE. Then, based on this evaluation, the agent combines the previous solutions to construct a solution for the new task --that is, it performs GPI. The specific mechanics behind GPE and GPI are less important than the principle itself, and finding alternative ways to carry out these operations may be an exciting research direction. Interestingly, a new study in behavioural sciences provides preliminary evidence that humans make decisions in multitask scenarios following a principle that closely resembles GPE and GPI. The fast adaptation provided by GPE and GPI is promising for building faster learning RL agents. More generally, it suggests a new approach to learning flexible solutions to problems.  Instead of tackling a problem as a single, monolithic, task, an agent can break it down into smaller, more manageable, sub-tasks. The solutions of the sub-tasks can then be reused and recombined to solve the overall task faster. This results in a compositional approach to RL that may lead to more scalable agents. At the very least, these agents will not be late because of a cup of coffee. Read the paper, as first published in PNAS, here . With thanks to Jim Kynvin, Adam Cain and Dominic Barlow for the figures, Kimberly Stachenfeld for the pointers to the neuroscience literature, and Kelly Clancy for the help with the text. GPE, successor features, and related approaches Improving Generalisation for Temporal Difference Learning: The Successor Representation. Peter Dayan. Neural Computation, 1993. Apprenticeship Learning Via Inverse Reinforcement Learning. Pieter Abbeel and Andrew Y. Ng. Proceedings of the International Conference on Machine learning (ICML), 2004. Horde: A Scalable Real-time Architecture for Learning Knowledge from Unsupervised Sensorimotor Interaction. Richard S. Sutton, Joseph Modayil, Michael Delp, Thomas Degris, Patrick M. Pilarski, Adam White. Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2011. Multi-timescale Nexting in a Reinforcement Learning Robot. Joseph Modayil, Adam White, Richard S. Sutton. From Animals to Animats, 2012. Universal Value Function Approximators. Tom Schaul, Dan Horgan, Karol Gregor, David Silver. Proceedings of the International Conference on Machine learning (ICML), 2015. Deep Successor Reinforcement Learning. Tejas D. Kulkarni, Ardavan Saeedi, Simanta Gautam, Samuel J. Gershman. arXiv, 2017. Visual Semantic Planning Using Deep Successor Representations. Yuke Zhu, Daniel Gordon, Eric Kolve, Dieter Fox, Li Fei-Fei, Abhinav Gupta, Roozbeh Mottaghi, Ali Farhadi. Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2017. Deep Reinforcement Learning with Successor Features for Navigation Across Similar Environments. Jingwei Zhang, Jost Tobias Springenberg, Joschka Boedecker, Wolfram Burgard. IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2017. Universal Successor Representations for Transfer Reinforcement Learning. Chen Ma, Junfeng Wen, Yoshua Bengio. arXiv, 2018. Eigenoption Discovery through the Deep Successor Representation. Marlos C. Machado, Clemens Rosenbaum, Xiaoxiao Guo, Miao Liu, Gerald Tesauro, Murray Campbell. International Conference on Learning Representations (ICLR), 2018. Successor Options: An Option Discovery Framework for Reinforcement Learning. Rahul Ramesh, Manan Tomar, Balaraman Ravindran. Proceedings of the  International Joint Conference on Artificial Intelligence (IJCAI), 2019. Successor Uncertainties: Exploration and Uncertainty in Temporal Difference Learning. David Janz, Jiri Hron, Przemysław Mazur, Katja Hofmann, José Miguel Hernández-Lobato, Sebastian Tschiatschek. Advances in Neural Information Processing Systems (NeurIPS), 2019. Successor Features Combine Elements of Model-Free and Model-based Reinforcement Learning. Lucas Lehnert, Michael L. Littman. arXiv, 2019. Count-Based Exploration with the Successor Representation. Marlos C. Machado, Marc G. Bellemare, Michael Bowling. Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2020. GPI, hierarchical RL, and related approaches A Robust Layered Control System for a Mobile Robot. R. Brooks. IEEE Journal on Robotics and Automation, 1986. Feudal Reinforcement Learning. Peter Dayan and Geoffrey E. Hinton. Advances in Neural Information Processing Systems (NIPS), 1992. Action Selection Methods Using Reinforcement Learning. Mark Humphrys. PhD thesis, University of Cambridge, Cambridge, UK, 1997. Learning to Solve Multiple Goals. Jonas Karlsson. PhD thesis, University of Rochester, Rochester, New York, 1997. Reinforcement Learning with Hierarchies of Machines. Ronald Parr and Stuart J. Russell. Advances in Neural Information Processing Systems (NIPS), 1997. Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning. Richard S.Sutton, DoinaPrecup, Satinder Singh. Artificial Intelligence, 1999. Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition. T. G. Dietterich. Journal of Artificial Intelligence Research, 2000. Multiple-Goal Reinforcement Learning with Modular Sarsa(O). Nathan Sprague and  Dana Ballard. Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), 2003. Q-decomposition for Reinforcement Learning Agents. Stuart J. Russell and Andrew Zimdars.  Proceedings of the International Conference on Machine Learning (ICML), 2003. Compositionality of Optimal Control Laws. E. Todorov. Advances in Neural Information Processing Systems (NIPS), 2009. Linear Bellman combination for control of character animation. M. da Silva, F. Durand, and J. Popovic. ACM Transactions on Graphics, 2009. Hierarchy Through Composition with Multitask LMDPS. A. M. Saxe, A. C. Earle, and B. Rosman. Proceedings of the International Conference on Machine Learning (ICML), 2017. Hybrid Reward Architecture for Reinforcement Learning. Harm van Seijen, Mehdi Fatemi, Joshua Romoff, Romain Laroche, Tavian Barnes, and Jeffrey Tsang. Advances in Neural Information Processing Systems (NIPS), 2017. Feudal Networks for Hierarchical Reinforcement Learning. Alexander Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Heess, Max Jaderberg, David Silver, Koray Kavukcuoglu.  Proceedings of the International Conference on Machine Learning (ICML), 2017. Composable Deep Reinforcement Learning for Robotic Manipulation. T. Haarnoja, V. Pong, A. Zhou, M. Dalal, P. Abbeel, and S. Levine. IEEE International Conference on Robotics and Automation (ICRA), 2018. Composing Value Functions in Reinforcement Learning. Benjamin Van Niekerk, Steven James, Adam Earle, Benjamin Rosman. Proceedings of the International Conference on Machine Learning (ICML), 2019. Planning in Hierarchical Reinforcement Learning: Guarantees for Using Local Policies. Tom Zahavy, Avinatan Hasidim, Haim Kaplan, Yishay Mansour. International Conference on Algorithmic Learning Theory (ALT), 2020. GPE + GPI, transfer learning, and related approaches Transfer of Learning by Composing Solutions of Elemental Sequential Tasks. Satinder Singh. Machine Learning, 1992. Transfer Learning for Reinforcement Learning Domains: A Survey. Matthew E. Taylor and Peter Stone. Journal of Machine Learning Research, 2009. Transfer in Variable-Reward Hierarchical Reinforcement Learning. Neville Mehta, Sriraam Natarajan, Prasad Tadepalli, Alan Fern. Machine Learning, 2008. Learning and Transfer of Modulated Locomotor Controllers. Nicolas Heess, Greg Wayne, Yuval Tassa, Timothy Lillicrap, Martin Riedmiller, David Silver. arXiv, 2016. Learning to Reinforcement Learn. Jane X. Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z. Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, Matt Botvinick. arXiv, 2016. RL2: Fast Reinforcement Learning via Slow Reinforcement Learning. Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, Pieter Abbeel. arXiv, 2016. Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks. Chelsea Finn, Pieter Abbeel, Sergey Levine. Proceedings of the International Conference on Machine Learning (ICML), 2017. Successor Features for Transfer in Reinforcement Learning. André Barreto, Will Dabney, Rémi Munos, Jonathan J. Hunt, Tom Schaul, Hado van Hasselt, David Silver.  Advances in Neural Information Processing Systems (NIPS), 2017. Transfer in Deep Reinforcement Learning Using Successor Features and Generalised Policy Improvement. André Barreto, Diana Borsa, John Quan, Tom Schaul, David Silver, Matteo Hessel, Daniel Mankowitz, Augustin Žídek, Rémi Munos. Proceedings of the International Conference on Machine Learning (ICML), 2018. Composing Entropic Policies Using Divergence Correction. Jonathan Hunt, André Barreto, Timothy Lillicrap, Nicolas Heess. Proceedings of the International Conference on Machine Learning (ICML), 2019. Universal Successor Features Approximators. Diana Borsa, André Barreto, John Quan, Daniel Mankowitz, Rémi Munos, Hado van Hasselt, David Silver, Tom Schaul. International Conference on Learning Representations (ICLR), 2019. The Option Keyboard: Combining Skills in Reinforcement Learning. André Barreto, Diana Borsa,  Shaobo Hou, Gheorghe Comanici, Eser Aygün, Philippe Hamel, Daniel Toyama, Jonathan J. Hunt, Shibl Mourad, David Silver, Doina Precup. Advances in Neural Information Processing Systems (NeurIPS), 2019. Transfer Learning in Deep Reinforcement Learning: A Survey. Zhuangdi Zhu, Kaixiang Lin, Jiayu Zhou, arXiv, 2020. Fast Task Inference with Variational Intrinsic Successor Features. Steven Hansen, Will Dabney, André Barreto, Tom Van de Wiele, David Warde-Farley, Volodymyr Mnih. International Conference on Learning Representations (ICLR), 2020. Fast Reinforcement Learning with Generalized Policy Updates. André Barreto, Shaobo Hou, Diana Borsa, David Silver, Doina Precup. Proceedings of the National Academy of Sciences, 2020. The successor representation in neuroscience The Hippocampus as a Predictive Map. Kimberly Stachenfeld, Matthew Botvinick, Samuel Gershman. Nature Neuroscience, 2017. The Successor Representation in Human Reinforcement Learning. I. Momennejad, E. M. Russek, J. H. Cheong, M. M. Botvinick, N. D. Daw, S. J. Gershman.  Nature Human Behaviour, 2017. Predictive Representations Can Link Model-Based Reinforcement Learning to Model-Free Mechanisms. E. Russek, I. Momennejad, M. M. Botvinick, S. J. Gershman, N. D. Daw. PLOS Computational Biology, 2017. The Successor Representation: Its Computational Logic and Neural Substrates. Samuel J. Gershman. Journal of Neuroscience, 2018. Better Transfer Learning with Inferred Successor Maps. Tamas J. Madarasz, Timothy E. Behrens. Advances in Neural Information Processing Systems (NeurIPS), 2019. Multi-Task Reinforcement Learning in Humans. Momchil S. Tomov, Eric Schulz, and Samuel J. Gershman. bioRxiv, 2019. A neurally plausible model learns successor representations in partially observable environments. Eszter Vertes, Maneesh Sahani. Advances in Neural Information Processing Systems (NeurIPS), 2019. Neurobiological Successor Features for Spatial Navigation. William de Cothi, Caswell Barry. Hippocampus, 2020. Linear Reinforcement Learning: Flexible Reuse of Computation in Planning, Grid Fields, and Cognitive Control. Payam Piray, Nathaniel D. Daw. bioRxiv, 2020.", "date": "2020-10-12"},
{"website": "Deepmind", "title": "Capture the Flag: the emergence of complex cooperative agents", "author": [" Max Jaderberg ", " Wojciech Marian Czarnecki ", " Iain Dunning ", " Thore Graepel ", " Luke Marris "], "link": "https://deepmind.com/blog/article/capture-the-flag-science", "abstract": "Updated 30/5/19. Read about our new work below, in “Human Comparable Agents” and “Going Further”. Mastering the strategy, tactical understanding, and team play involved in multiplayer video games represents a critical challenge for AI research. In our latest paper, now published in the journal Science , we present new developments in reinforcement learning, resulting in human-level performance in Quake III Arena Capture the Flag. This is a complex, multi-agent environment and one of the canonical 3D first-person multiplayer games. The agents successfully cooperate with both artificial and human teammates, and demonstrate high performance even when trained with reaction times comparable to human players. Furthermore, we show how these methods have managed to scale beyond research Capture the Flag environments to the full game of Quake III Arena. Billions of people inhabit the planet, each with their own individual goals and actions, but still capable of coming together through teams, organisations and societies in impressive displays of collective intelligence. This is a setting we call multi-agent learning: many individual agents must act independently, yet learn to interact and cooperate with other agents. This is an immensely difficult problem - because with co-adapting agents the world is constantly changing. To investigate this problem, we look at 3D first-person multiplayer video games. These games represent the most popular genre of video game, and have captured the imagination of millions of gamers because of their immersive game play, as well as the challenges they pose in terms of strategy, tactics, hand-eye coordination, and team play. The challenge for our agents is to learn directly from raw pixels to produce actions. This complexity makes first-person multiplayer games a fruitful and active area of research within the AI community. The game we focused on in this work is Quake III Arena (which we aesthetically modified, though all game mechanics remain the same). Quake III Arena has laid the foundations for many modern first-person video games, and has attracted a long-standing competitive e-sports scene. We train agents that learn and act as individuals, but which must be able to play on teams with and against any other agents, artificial or human. The rules of CTF are simple, but the dynamics are complex. Two teams of individual players compete on a given map with the goal of capturing the opponent team’s flag while protecting their own. To gain tactical advantage they can tag the opponent team members to send them back to their spawn points. The team with the most flag captures after five minutes wins. From a multi-agent perspective, CTF requires players to both successfully cooperate with their teammates as well as compete with the opposing team, while remaining robust to any playing style they might encounter. To make things even more interesting, we consider a variant of CTF in which the map layout changes from match to match. As a consequence, our agents are forced to acquire general strategies rather than memorising the map layout. Additionally, to level the playing field, our learning agents experience the world of CTF in a similar way to humans: they observe a stream of pixel images and issue actions through an emulated game controller. Our agents must learn from scratch how to see, act, cooperate, and compete in unseen environments, all from a single reinforcement signal per match: whether their team won or not. This is a challenging learning problem, and its solution is based on three general ideas for reinforcement learning: The resulting agent, dubbed the For The Win (FTW) agent, learns to play CTF to a very high standard. Crucially, the learned agent policies are robust to the size of the maps, the number of teammates, and the other players on their team. Below, you can explore some games on both the outdoor procedural environments, where FTW agents play against each other, as well as games in which humans and agents play together on indoor procedural environments. We ran a tournament including 40 human players, in which humans and agents are randomly matched up in games - both as opponents and as teammates. The FTW agents learn to become much stronger than the strong baseline methods, and exceed the win-rate of the human players. In fact, in a survey among participants they were rated more collaborative than human participants. Going beyond mere performance evaluation, it is important to understand the emergent complexity in the behaviours and internal representations of these agents. To understand how agents represent game state, we look at activation patterns of the agents’ neural networks plotted on a plane. Dots in the figure below represent situations during play with close by dots representing similar activation patterns. These dots are coloured according to the high-level CTF game state in which the agent finds itself: In which room is the agent? What is the status of the flags? What teammates and opponents can be seen? We observe clusters of the same colour, indicating that the agent represents similar high-level game states in a similar manner. The agents are never told anything about the rules of the game, yet learn about fundamental game concepts and effectively develop an intuition for CTF. In fact, we can find particular neurons that code directly for some of the most important game states, such as a neuron that activates when the agent’s flag is taken, or a neuron that activates when an agent’s teammate is holding a flag. The paper provides further analysis covering the agents’ use of memory and visual attention. How did our agents perform as well as they did? First, we noticed that the agents had very fast reaction times and were very accurate taggers, which might explain their performance (tagging is a tactical action that sends opponents back to their starting point). Humans are comparatively slow to process and act on sensory input, due to our slower biological signalling. Here’s an example of a reaction time test you can try yourself . Thus, our agents’ superior performance might be a result of their faster visual processing and motor control. However, by artificially reducing this accuracy and reaction time, we saw that this was only one factor in their success. In a further study, we trained agents which have an inbuilt delay of a quarter of a second (267 ms) – that is, agents have a 267ms lag before observing the world – comparable with reported reaction times of human video game players. These response-delayed agents still outperformed human participants, with strong humans only winning 21% of the time. Through unsupervised learning we established the prototypical behaviours of agents and humans to discover that agents in fact learn human-like behaviours, such as following teammates and camping in the opponent’s base. These behaviours emerge in the course of training, through reinforcement learning and population-level evolution, with behaviours - such as teammate following - falling out of favour as agents learn to cooperate in a more complementary manner. The training progression of a population of FTW agents. Top left: the 30 agents’ Elo ratings as they train and evolve from each other. Top right: the genetic tree of these evolution events. The lower graph shows the progression of knowledge, some of the internal rewards, and behaviour probability throughout the training of the agents. While this paper focuses on Capture the Flag, the research contributions are general and we are excited to see how others build upon our techniques in different complex environments. Since initially publishing these results, we have found success in extending these methods to the full game of Quake III Arena, which includes professionally played maps, more multiplayer game modes in addition to Capture the Flag, and more gadgets and pickups. Initial results indicate that agents can play multiple game modes and multiple maps competitively, and are starting to challenge the skills of our human researchers in test matches. Indeed, ideas introduced in this work, such as population based multi-agent RL, form a foundation of the AlphaStar agent in our work on StarCraft II . In general, this work highlights the potential of multi-agent training to advance the development of artificial intelligence: exploiting the natural curriculum provided by multi-agent training, and forcing the development of robust agents that can even team up with humans. For more details, please see the paper ( PDF ) and the full supplementary video . This work was done by Max Jaderberg, Wojciech M. Czarnecki, Iain Dunning, Luke Marris, Brendan Tracey, Guy Lever, Antonio Garcia Castaneda, Charles Beattie, Neil Rabinowitz, Ari Morcos, Avraham Ruderman, Nicolas Sonnerat, Tim Green, Louise Deason, Joel Z. Leibo, David Silver, Demis Hassabis, Koray Kavukcuoglu, and Thore Graepel. Visualisations were created by Adam Cain, Damien Boudot, Doug Fritz, Jaume Sanchez Elias, Paul Lewis, Max Jaderberg, Wojciech M. Czarnecki, and Luke Marris. We would like to thank Patrick Howard and Dan “Scancode” Gold for allowing us to use the Quake III Arena maps they designed.", "date": "2019-05-30"},
{"website": "Deepmind", "title": "Using WaveNet technology to reunite speech-impaired users with their original voices", "author": [" Yutian Chen ", " Norman Casagrande ", " Yu Zhang ", " Michael Brenner "], "link": "https://deepmind.com/blog/article/Using-WaveNet-technology-to-reunite-speech-impaired-users-with-their-original-voices", "abstract": "This post details a recent project we undertook with Google and ALS campaigner Tim Shaw, as part of Google’s Euphonia project. We demonstrate an early proof of concept of how text-to-speech technologies can synthesise a high-quality, natural sounding voice using minimal recorded speech data. As a teenager, Tim Shaw put everything he had into football practice: his dream was to join the NFL. After playing for Penn State in college, his ambitions were finally realised: the Carolina Panthers drafted him at age 23, and he went on to play for the Chicago Bears and Tennessee Titans, where he broke records as a linebacker. After six years in the NFL, on the cusp of greatness, his performance began to falter. He couldn’t tackle like he once had; his arms slid off the pullup bar. At home, he dropped bags of groceries, and his legs began to buckle underneath him. In 2013 Tim was cut from the Titans but he resolved to make it onto another team. Tim practiced harder than ever, yet his performance continued to decline. Five months later, he finally discovered the reason: he was diagnosed with Amyotrophic lateral sclerosis (ALS, commonly known as Lou Gehrig’s disease). In ALS, the neurons that control a person’s voluntary muscles die, eventually leading to a total loss of control over one’s body. ALS has no known cause, and, as of today, has no cure. Today, Tim is a powerful advocate for ALS research. Earlier this year, he published a letter to his younger self advising acceptance–“otherwise, you’ll grieve yourself to death.” Now a wheelchair user, he lives under the constant care of his parents. People with ALS have trouble moving, and the disease makes speaking, swallowing, and even breathing on their own difficult and then impossible. Not being able to communicate can be one of the hardest aspects for people with ALS and their families. As Tim put it: “it’s beyond frustrating not to be able to express what’s going on in my mind. I’m smarter than ever but I just can’t get it out.” Losing one’s voice can be socially devastating. Today, the main option available to people to preserve their voice is message banking, wherein people with ALS can digitally record and store personally meaningful phrases using their natural inflection and intonation. Message banking is a source of great comfort for people with ALS and their families, helping to preserve a core part of their identity - their voice - through a deeply challenging time. But message banking lacks flexibility, resulting in a static dataset of phrases. Imagine being told you will never be able to speak again. Now imagine that you were given the chance to preserve your voice by recording as much of it as possible. How would you decide what to record? How would you capture what you most want to be able to say in the future?  Would it be a meaningful story, a favorite phrase or a simple “I love you”? The process can be time consuming and emotionally draining, especially as someone’s voice degrades. And people who aren’t able to record phrases in time are left to choose a generic computer synthesized voice that lacks the same power of connection as their own. At DeepMind, we’ve been collaborating with Google and people like Tim Shaw to help develop technologies that can make it easier for people with speech difficulties to communicate. The challenges of this are two-fold. Firstly, we must have technology that can recognise the speech of people with non-standard pronunciation–something Google AI has been researching through Project Euphonia . Secondly, we’d ideally like people to be able to communicate using their original voice. Stephen Hawking, who also suffered from ALS, communicated with a famously unnatural sounding text-to-speech synthesiser. Thus, the second challenge is customising text-to-speech technology to the user’s natural speaking voice. Creating natural sounding speech is considered a “ grand challenge ” in the field of AI. With WaveNet and Tacotron , we’ve seen tremendous breakthroughs in the quality of text-to-speech systems. However, whilst it is possible to create natural sounding voices that sound like specific people in certain contexts – as we demonstrated in collaboration with John Legend last year –  developing synthetic voices requires many hours of studio recording time with a very specific script – a luxury that many people with ALS simply don’t have. Creating machine learning models that require less training data is an active area of research at DeepMind, and is crucial for use cases such as this where we need to recreate a voice with just a handful of audio recordings.  We’ve helped do this by harnessing our WaveNet work and the novel approaches demonstrated in our paper, Sample Efficient Adaptive Text-to-Speech (TTS) - where we showed that it’s possible to create a high quality voice using small amounts of speech data. Which brings us back to Tim. Tim and his family were instrumental in our recent research. Our goal was to provide Tim and his family an opportunity to hear his original speaking voice again. Thanks to Tim’s time in the media spotlight, resulting in about thirty minutes of high-quality audio recordings, we were able to apply the methodologies from WaveNet and TTS to recreate his former voice. Following a six-month effort, Google’s AI team visited Tim and his family to show him the results of their work. The meeting was captured for the new YouTube Originals learning series, “ The Age of A.I. ” hosted by Robert Downey Jr.  Tim and his family were able to hear his old voice for the first time in years, as the model – trained on Tim’s NFL audio recordings – read out the letter he’d recently written to his younger self . “I don’t remember that voice,” Tim remarked. His father responded, “we do.” Later, Tim recounted–\"it has been so long since I've sounded like that, I feel like a new person. I felt like a missing part was put back in place. It's amazing. I'm just thankful that there are people in this world that will push the envelope to help other people.\" You can learn more about our project with Tim and the vital role he played in our research in “ The Age of A.I. ” now available on YouTube.com/Learning . To understand how the technology works, it’s important to first understand WaveNet . WaveNet is a generative model trained on many hours of speech and text data from diverse speakers. It can then be fed arbitrary new text to be synthesized into a natural-sounding spoken sentence. Last year, in our Sample Efficient Adaptive Text-to-Speech paper, we illustrated that it’s possible to train a new voice with minutes, rather than hours, of voice recordings through a process called fine-tuning. This involves first training a large WaveNet model on up to thousands of speakers, which takes a few days, until it can produce the basics of natural sounding speech. Then, we take the small corpus of data for the target speaker and intelligently adapt the model, adjusting the weights so that we can create a single model that matches the target speaker. The concept of fine-tuning is similar to how people learn. For example, if you are attempting to learn calculus, you should first understand the foundations of basic algebra, and then apply these simpler concepts to help solve more complex equations. After this publication, we continued to iterate on our models. First, we migrated from WaveNet to WaveRNN , which is a more efficient text to speech model co-developed by Google AI and DeepMind. WaveNet requires a second distillation step to speed it up to serve requests in real-time, which makes fine-tuning more challenging. WaveRNN, on the other hand, does not require a second training step and can synthesize speech much faster than a WaveNet model that has not been distilled. In addition to speeding up the models by switching to WaveRNN, we collaborated with Google AI to improve the quality of the models. Google AI researchers demonstrated that a similar fine-tuning approach could be applied to the related Google Tacotron model, which we use in conjunction with WaveRNN to synthesise realistic voices. By combining these technologies trained on audio clips of Tim Shaw from his NFL days, we were able to generate an authentic sounding voice that resembles how Tim sounded before his speech degraded. While the voice is not yet perfect – lacking the expressiveness, quirks, and controllability of a real voice – we’re excited that the combination of WaveRNN and Tacotron may help people like Tim preserve an important part of their identity, and we would like to one day integrate it into speech-generation devices. We’re honored to have briefly reunited Tim with his voice. At this stage, it’s too early to know where our research will take us, but we are looking at ways to combine the Euphonia speech recognition systems with the speech synthesis technology so that people like Tim can more easily communicate. We hope that our research can eventually be shared more widely with those who need it most in order to communicate with their loved ones– there are thousands of people in the world who this work might one day benefit. As Tim wrote in his letter to his younger self–what matters, in the end, is “the relationships and the people you have in your life who love you and care about you.” In collaboration with: Zachary Gleicher, Luis C. Cobo, Yannis Assael, Brendan Shillingford, Nando de Freitas, Julie Cattiau‎, Philip Nelson, Ye Jia, Heiga Zen, Ron Weiss, Zhifeng Chen, Yonghui Wu, Tejas Iyer‎, Hadar Shemtov, Tim Shaw, Fernando Vieira, Maeve McNally, John Shaw, Sharon Shaw, John Costello", "date": "2019-12-18"},
{"website": "Deepmind", "title": "Open sourcing Sonnet - a new library for constructing neural networks", "author": [" Malcolm Reynolds ", " Jack Rae ", " Andreas Fidjeland ", " Fabio Viola ", " Adrià Puigdomènech ", " Frederic Besse ", " Tim Green ", " Sébastien Racanière ", " Gabriel Barth-Maron ", " Diego de Las Casas "], "link": "https://deepmind.com/blog/article/open-sourcing-sonnet", "abstract": "It’s now nearly a year since DeepMind made the decision to switch the entire research organisation to using TensorFlow (TF) . It’s proven to be a good choice - many of our models learn significantly faster, and the built-in features for distributed training have hugely simplified our code. Along the way, we found that the flexibility and adaptiveness of TF lends itself to building higher level frameworks for specific purposes, and we’ve written one for quickly building neural network modules with TF. We are actively developing this codebase, but what we have so far fits our research needs well, and we’re excited to announce that today we are open sourcing it. We call this framework Sonnet . Since its initial launch in November 2015, a diverse ecosystem of higher level libraries has sprung up around TensorFlow enabling common tasks to be accomplished quicker. Sonnet shares many similarities with some of these existing neural network libraries, but has some features specifically designed around our research requirements. The code release accompanying our Learning to learn paper included a preliminary version of Sonnet, and other forthcoming code releases will be built on top of the full library we are releasing today. Making Sonnet public allows other models created within DeepMind to be easily shared with the community, and we also hope that the community will use Sonnet to take their own research forwards. In recent months we’ve also open-sourced our flagship platform DeepMind Lab , and are currently working with Blizzard to develop an open source API that supports AI research in StarCraft II . There are many more releases to come, and they’ll all be shared on our new Open Source page . The library uses an object-oriented approach, similar to Torch/NN, allowing modules to be created which define the forward pass of some computation. Modules are ‘called’ with some input Tensors, which adds ops to the Graph and returns output Tensors. One of the design choices was to make sure the variable sharing is handled transparently by automatically reusing variables on subsequent calls to the same module. Many models in the literature can naturally be considered as a hierarchy - e.g. a Differentiable Neural Computer contains a controller which might be an LSTM, which can be implemented as containing a standard Linear layer. We’ve found that writing code which explicitly represents submodules allows easy code reuse and quick experimentation - Sonnet promotes writing modules which declare other submodules internally, or are passed other modules at construction time. A final technique we’ve found very useful is to allow certain modules to operate on arbitrarily nested groups of Tensors. Recurrent Neural Network states are often best represented as a collection of heterogeneous Tensors, and representing these as a flat list can be error prone. Sonnet provides utilities to deal with these arbitrary hierarchies, so that changing your experiment to use a different kind of RNN does not require tedious code changes. We’ve made changes to core TF as well to better support this use case. Sonnet is designed specifically to work with TensorFlow, and as such does not prevent you from accessing the underlying details such as Tensors and variable_scopes. Models written in Sonnet can be freely mixed with raw TF code, and that in other high level libraries. This is not a one-time release - we will regularly update the Github repository to match our in-house version. We’ve got lots of ideas for new features in the works, which will be made available when ready. We are very excited about contributions from the community. To find out more about Sonnet, please see our GitHub repository.", "date": "2017-04-07"},
{"website": "Deepmind", "title": "Learning explanatory rules from noisy data", "author": [" Richard Evans ", " Edward Grefenstette "], "link": "https://deepmind.com/blog/article/learning-explanatory-rules-noisy-data", "abstract": "Suppose you are playing football. The ball arrives at your feet, and you decide to pass it to the unmarked striker. What seems like one simple action requires two different kinds of thought. First, you recognise that there is a football at your feet. This recognition requires intuitive perceptual thinking - you cannot easily articulate how you come to know that there is a ball at your feet, you just see that it is there. Second, you decide to pass the ball to a particular striker. This decision requires conceptual thinking. Your decision is tied to a justification - the reason you passed the ball to the striker is because she was unmarked. The distinction is interesting to us because these two types of thinking correspond to two different approaches to machine learning: deep learning and symbolic program synthesis . Deep learning concentrates on intuitive perceptual thinking whereas symbolic program synthesis focuses on conceptual, rule-based thinking. Each system has different merits - deep learning systems are robust to noisy data but are difficult to interpret and require large amounts of data to train, whereas symbolic systems are much easier to interpret and require less training data but struggle with noisy data. While human cognition seamlessly combines these two distinct ways of thinking, it is much less clear whether or how it is possible to replicate this in a single AI system. Our new paper, recently published in JAIR , demonstrates it is possible for systems to combine intuitive perceptual with conceptual interpretable reasoning. The system we describe, ∂ILP, is robust to noise, data-efficient, and produces interpretable rules. We demonstrate how ∂ILP works with an induction task. It is given a pair of images representing numbers, and has to output a label (0 or 1) indicating whether the number of the left image is less than the number of the right image. Solving this problem involves both kinds of thinking: you need intuitive perceptual thinking to recognise the image as a representation of a particular digit, and you need conceptual thinking to understand the less-than relation in its full generality. If you give a standard deep learning model (such as a convolutional neural network with an MLP) sufficient training data, it is able to learn to solve this task effectively. Once it has been trained, you can give it a new pair of images it has never seen before, and it will classify correctly. However, it will only generalise correctly if you give it multiple examples of every pair of digits. The model is good at visual generalisation: generalising to new images, assuming it has seen every pair of digits in the test set (see the green box below). But it is not capable of symbolic generalisation: generalising to a new pair of digits it has not seen before (see the blue box below). Researchers like Gary Marcus and Joel Grus have pointed this out in recent, thought-provoking articles. ∂ILP differs from standard neural nets because it is able to generalise symbolically, and it differs from standard symbolic programs because it is able to generalise visually. It learns explicit programs from examples that are readable, interpretable, and verifiable. ∂ILP is given a partial set of examples (the desired results) and produces a program that satisfies them. It searches through the space of programs using gradient descent. If the outputs of the program conflict with the desired outputs from the reference data, the system revises the program to better match the data. Our system, ∂ILP, is able to generalise symbolically. Once it has seen enough examples of x < y, y < z, x < z, it will consider the possibility that the < relation is transitive. Once it has realised this general rule, it can apply it to a new pair of numbers it has never seen before. We believe that our system goes some way to answering the question of whether achieving symbolic generalisation in deep neural networks is possible. In future work, we plan to integrate ∂ILP-like systems into reinforcement learning agents and larger deep learning modules. In doing so, we hope to impart our systems with the ability to reason as well as to react. Read the paper here .", "date": "2018-01-29"},
{"website": "Deepmind", "title": "AI and Neuroscience: A virtuous circle", "author": [" Demis Hassabis ", " Christopher Summerfield ", " Matt Botvinick "], "link": "https://deepmind.com/blog/article/ai-and-neuroscience-virtuous-circle", "abstract": "Recent progress in AI has been remarkable. Artificial systems now outperform expert humans at Atari video games , the ancient board game Go , and high-stakes matches of heads-up poker . They can also produce handwriting and speech indistinguishable from those of humans, translate between multiple languages and even reformat your holiday snaps in the style of Van Gogh masterpieces. These advances are attributed to several factors, including the application of new statistical approaches and the increased processing power of computers. But in a recent Perspective in the journal Neuron , we argue that one often overlooked contribution is the use of ideas from experimental and theoretical neuroscience. Psychology and neuroscience have played a key role in the history of AI. Founding figures such as Donald Hebb , Warren McCulloch , Marvin Minsky and Geoff Hinton were all originally motivated by a desire to understand how the brain works. In fact, throughout the late 20th Century, much of the key work developing neural networks took place not in mathematics or physics labs, but in psychology and neurophysiology departments. At DeepMind, we argue that despite rapid progress in both fields, researchers should not lose sight of this vision. We urge researchers in neuroscience and AI to find a common language, allowing a free flow of knowledge that will allow continued progress on both fronts. We believe that drawing inspiration from neuroscience in AI research is important for two reasons. First, neuroscience can help validate AI techniques that already exist. Put simply, if we discover one of our artificial algorithms mimics a function within the brain, it suggests our approach may be on the right track. Second, neuroscience can provide a rich source of inspiration for new types of algorithms and architectures to employ when building artificial brains. Traditional approaches to AI have historically been dominated by logic-based methods and theoretical mathematical models. We argue that neuroscience can complement these by identifying classes of biological computation that may be critical to cognitive function. Take one recent example of a seminal finding in neuroscience: the discovery of offline experience “ replay ”. During sleep or quiet resting, biological brains “replay” temporal patterns of neuronal activity that were produced in an earlier active period. For example, when rats run through a maze, “place” cells activate as the animal moves around. During rest, the same sequence of neuronal activity is observed, as if the rats were mentally reimagining their past movements, and using them to optimise future behaviour. In fact, it has been shown that interfering with replay impairs performance when they later perform the same tasks. At first glance, it might seem counterintuitive to build an artificial agent that needs to ‘sleep’ - after all, they are supposed to grind away at a computational problem long after their programmers have gone to bed. But this principle was a key part of our deep-Q network (DQN) , an algorithm that learnt to master a diverse range of Atari 2600 games to superhuman level with only the raw pixels and score as inputs. DQN mimics “experience replay”, by storing a subset of training data that it reviews “offline”, allowing it to learn anew from successes or failures that occurred in the past. Successes like this give us confidence that neuroscience is already an important source of ideas for AI. But looking forward, we believe it will become indispensable in helping us tackle unsolved questions, such as those concerning efficient learning, understanding of the physical world, and imagination. Imagination is a hugely important function for humans and animals, allowing us to plan for future scenarios without taking action; something that may come at a cost.  Consider a simple example, such as planning a holiday. In order to do this we leverage our knowledge - or “model” - of the world and use it to project forward in time, evaluating future states, and allowing us to calculate the route we need to take or what clothes to pack for sunny weather. Cutting-edge research in human neuroscience is starting to unveil the computational and systems mechanisms that underpin this kind of thinking, but much of this new understanding has yet to be incorporated into artificial models. Another key challenge in contemporary AI research is known as transfer learning. To be able to deal effectively with novel situations, artificial agents need the ability to build on existing knowledge to make sensible decisions. Humans are already good at this - an individual who can drive a car, use a laptop or chair a meeting are usually able to cope even when confronted by an unfamiliar vehicle, operating system or social situation. Researchers are now starting to take the first steps towards understanding how this might be possible in artificial systems. For example, a new class of network architecture known as a “ progressive network ” can use knowledge learned in one video game to learn another. The same architecture has also been shown to transfer knowledge from a simulated robotic arm to a real-world arm, massively reducing the training time. Intriguingly, these networks bear some similarities to models of sequential task learning in humans . These tantalising links suggest that there are great opportunities for future AI research to learn from work in neuroscience. But this exchange of knowledge cannot be a one-way street. Neuroscience can also benefit from AI research. Take the idea of reinforcement learning - one of the central approaches in contemporary AI research. Although the original idea came from theories of animal learning in psychology, it was developed and elaborated by machine learning researchers.  These later ideas fed back into neuroscience to help us understand neurophysiological phenomena, such as the firing properties of dopamine neurons in the mammalian basal ganglia. This back and forth is essential if both fields are to continue to build on each other’s insights, creating a virtuous circle whereby AI researchers use ideas from neuroscience to build new technology, and neuroscientists learn from the behaviour of artificial agents to better interpret biological brains. Indeed, this cycle will likely accelerate thanks to recent advances, such as optogenetics, that allow us to precisely measure and manipulate brain activity, yielding vast quantities of data that can be analysed with tools from machine learning. We therefore believe distilling intelligence into algorithms and comparing them to the human brain is now vital. Not only could it bolster our quest to develop AI, a tool that we hope will create new knowledge and push forward scientific discovery , but may also allow us to better understand what’s going on inside our own heads. That could shed light on some of the most enduring mysteries in neuroscience, such as the nature of creativity, dreams and, perhaps one day, even consciousness. With so much at stake, the need for the field of neuroscience and AI to come together is now more urgent than ever before. Download the full paper here .", "date": "2017-08-02"},
{"website": "Deepmind", "title": "Episode 8: Demis Hassabis - The interview", "author": "Unknown", "link": "https://deepmind.com/blog/article/podcast-episode-8-demis-hassabis-interview", "abstract": "In this special extended episode, Hannah Fry meets Demis Hassabis, the CEO and co-founder of DeepMind. She digs into his former life as a chess player, games designer and neuroscientist and explores how his love of chess helped him to get start-up funding, what drives him and his vision, and why AI keeps him up at night. Interviewees: Deepmind CEO and co-founder, Demis Hassabis Listen: The power of intelligence Listen to this episode and subscribe to the whole series on Apple podcasts , Google podcasts , Spotify , Deezer or your favourite podcast app by searching for “DeepMind: The Podcast”. Find out more about the themes in this episode: If you know of other resources we should link to, please help other listeners by either replying to us on Twitter (#DMpodcast) or emailing us at podcast@deepmind.com. You can also use that address to send us questions or feedback on the series. Credits: Presenter: Hannah Fry Editor: David Prest Senior Producer: Louisa Field Producers: Amy Racs, Dan Hardoon Binaural Sound: Lucinda Mason-Brown Music composition: Eleni Shaw (with help from Sander Dieleman and WaveNet )", "date": "2019-09-17"},
{"website": "Deepmind", "title": "AlphaGo's next move", "author": [" Demis Hassabis ", " David Silver "], "link": "https://deepmind.com/blog/article/alphagos-next-move", "abstract": "With just three stones on the board, it was clear that this was going to be no ordinary game of Go . Chinese Go Grandmaster and world number one Ke Jie departed from his typical style of play and opened with a “3:3 point” strategy - a highly unusual approach aimed at quickly claiming corner territory at the start of the game. The placement is rare amongst Go players, but it’s a favoured position of our program AlphaGo. Ke Jie was playing it at its own game. Ke Jie’s thoughtful positioning of that single black stone was a fitting motif for the opening match of The Future of Go Summit in Wuzhen, China , an event dedicated to exploring the truth of this beautiful and ancient game. Over the last five days we have been honoured to witness games of the highest calibre. We have always believed in the potential for AI to help society discover new knowledge and benefit from it, and AlphaGo has given us an early glimpse that this may indeed be possible. More than a competitor, AlphaGo has been a tool to inspire Go players to try new strategies and uncover new ideas in this 3,000 year-old game. The creative moves it played against the legendary Lee Sedol in Seoul in 2016 brought completely new knowledge to the Go world, while the unofficial online games it played under the moniker Magister (Master) earlier this year have influenced many of Go’s leading professionals - including the genius Ke Jie himself. Events like this week’s Pair Go, in which two of the world’s top players partnered with AlphaGo, showed the great potential for people to use AI systems to generate new insights in complex fields. This week’s series of thrilling games with the world’s best players, in the country where Go originated, has been the highest possible pinnacle for AlphaGo as a competitive program. For that reason, the Future of Go Summit is our final match event with AlphaGo. The research team behind AlphaGo will now throw their energy into the next set of grand challenges, developing advanced general algorithms that could one day help scientists as they tackle some of our most complex problems, such as finding new cures for diseases, dramatically reducing energy consumption, or inventing revolutionary new materials. If AI systems prove they are able to unearth significant new knowledge and strategies in these domains too, the breakthroughs could be truly remarkable. We can’t wait to see what comes next. While AlphaGo is stepping back from competitive play, it’s certainly not the end of our work with the Go community, to which we owe a huge debt of gratitude for their encouragement and motivation over the past few years. We plan to publish one final academic paper later this year that will detail the extensive set of improvements we made to the algorithms’ efficiency and potential to be generalised across a broader set of problems. Just like our first AlphaGo paper, we hope that other developers will pick up the baton, and use these new advances to build their own set of strong Go programs. We’re also working on a teaching tool - one of the top requests we’ve received throughout this week. The tool will show AlphaGo’s analysis of Go positions, providing an insight into how the program thinks, and hopefully giving all players and fans the opportunity to see the game through the lens of AlphaGo. We’re particularly honoured that our first collaborator in this effort will be the great Ke Jie, who has agreed to work with us on a study of his match with AlphaGo. We’re excited to hear his insights into these amazing games, and to have the chance to share some of AlphaGo’s own analysis too. Finally, to mark the end of the Future of Go Summit, we wanted to give a special gift to fans of Go around the world. Since our match with Lee Sedol, AlphaGo has become its own teacher, playing millions of high level training games against itself to continually improve. We’re now publishing a special set of 50 AlphaGo vs AlphaGo games, played at full length time controls, which we believe contain many new and interesting ideas and strategies. We took the opportunity this week in Wuzhen to show some of these games to a handful of top professionals. Shi Yue, 9 Dan Professional and World Champion said the games were “Like nothing I’ve ever seen before - they’re how I imagine games from far in the future.” Gu Li, 9 Dan Professional and World Champion, said that “AlphaGo’s self play games are incredible - we can learn many things from them.” We hope that all Go players will now enjoy trying out some of the moves in the set. The first ten games are now available here , and we’ll publish another ten each day until all 50 have been released. We have been humbled by the Go community’s reaction to AlphaGo, and the way professional and amateur players have embraced its insights about this ancient game. We plan to bring that same excitement and insight to a range of new fields, and try to address some of the most important and urgent scientific challenges of our time. We hope that the story of AlphaGo is just the beginning. Read more about The Future of Go Summit Read more about AlphaGo Discover the AlphaGo self-play games", "date": "2017-05-27"},
{"website": "Deepmind", "title": "Episode 5: Out of the lab", "author": "Unknown", "link": "https://deepmind.com/blog/article/podcast-episode-5-out-of-the-lab", "abstract": "The ambition of AI research is to create systems that can help to solve problems in the real world. In this episode, Hannah Fry meets the people building systems that could be used to save the sight of thousands, help us solve one of the most fundamental problems in biology and reduce energy consumption in an effort to combat climate change. But whilst there is great potential, there are also important obstacles that will need to be tackled for AI to be used effectively, safely and fairly. Interviewees: Pearse Keane, consultant ophthalmologist at Moorfields Eye Hospital; Sandy Nelson, Product Manager for DeepMind’s Science Program; and DeepMind Program Manager Sims Witherspoon. Presented by Hannah Fry. Listen: Finding a path through complexity Listen to this episode and subscribe to the whole series on Apple podcasts , Google podcasts , Spotify , Deezer or your favourite podcast app by searching for “DeepMind: The Podcast”. Find out more about the themes in this episode: If you know of other resources we should link to, please help other listeners by either replying to us on Twitter (#DMpodcast) or emailing us at podcast@deepmind.com. You can also use that address to send us questions or feedback on the series. Credits: Presenter: Hannah Fry Editor: David Prest Senior Producer: Louisa Field Producers: Amy Racs, Dan Hardoon Binaural Sound: Lucinda Mason-Brown Music composition: Eleni Shaw (with help from Sander Dieleman and WaveNet )", "date": "2019-08-27"},
{"website": "Deepmind", "title": "AlphaFold: Using AI for scientific discovery", "author": [" Andrew Senior ", " John Jumper ", " Demis Hassabis ", " Pushmeet Kohli "], "link": "https://deepmind.com/blog/article/AlphaFold-Using-AI-for-scientific-discovery", "abstract": "In our study published in Nature , we demonstrate how artificial intelligence research can drive and accelerate new scientific discoveries. We’ve built a dedicated, interdisciplinary team in hopes of using AI to push basic research forward: bringing together experts from the fields of structural biology, physics, and machine learning to apply cutting-edge techniques to predict the 3D structure of a protein based solely on its genetic sequence. Our system, AlphaFold – described in peer-reviewed papers now published in Nature and PROTEINS – is the culmination of several years of work, and builds on decades of prior research using large genomic datasets to predict protein structure. The 3D models of proteins that AlphaFold generates are far more accurate than any that have come before - marking significant progress on one of the core challenges in biology. The AlphaFold code used at CASP13 is available on Github here for anyone interested in learning more or replicating our results. We’re also excited by the fact that this work has already inspired other, independent implementations, including the model described in this paper , and a community - built, open source implementation , described here . Proteins are large, complex molecules essential to all of life. Nearly every function that our body performs - contracting muscles, sensing light, or turning food into energy - relies on proteins, and how they move and change. What any given protein can do depends on its unique 3D structure. For example, antibody proteins utilised by our immune systems are ‘Y-shaped’, and form unique hooks. By latching on to viruses and bacteria, these antibody proteins are able to detect and tag disease - causing microorganisms for elimination. Collagen proteins are shaped like cords, which transmit tension between cartilage, ligaments, bones, and skin. Other types of proteins include Cas9, which, using CRISPR sequences as a guide, act like scissors to cut and paste sections of DNA; antifreeze proteins, whose 3D structure allows them to bind to ice crystals and prevent organisms from freezing; and ribosomes, which act like a programmed assembly line, helping to build proteins themselves. The recipes for those proteins - called genes - are encoded in our DNA. An error in the genetic recipe may result in a malformed protein, which could result in disease or death for an organism. Many diseases, therefore, are fundamentally linked to proteins. But just because you know the genetic recipe for a protein doesn’t mean you automatically know its shape. Proteins are comprised of chains of amino acids (also referred to as amino acid residues). But DNA only contains information about the sequence of amino acids - not how they fold into shape. The bigger the protein, the more difficult it is to model, because there are more interactions between amino acids to take into account. As demonstrated by Levinthal’s paradox , it would take longer than the age of the known universe to randomly enumerate all possible configurations of a typical protein before reaching the true 3D structure - yet proteins themselves fold spontaneously, within milliseconds. Predicting how these chains will fold into the intricate 3D structure of a protein is what’s known as the “protein folding problem” - a challenge that scientists have worked on for decades. This unsolved problem has already inspired countless developments, from spurring IBM’s efforts in supercomputing ( BlueGene ), to novel citizen science efforts ( Folding@Home and FoldIt ) to new engineering realms, such as rational protein design. Scientists have long been interested in determining the structures of proteins because a protein’s form is thought to dictate its function. Once a protein’s shape is understood, its role within the cell can be guessed at, and scientists can develop drugs that work with the protein’s unique shape. Over the past five decades, researchers have been able to determine shapes of proteins in labs using experimental techniques like cryo-electron microscopy , nuclear magnetic resonance and X-ray crystallography , but each method depends on a lot of trial and error, which can take years of work, and cost tens or hundreds of thousands of dollars per protein structure. This is why biologists are turning to AI methods as an alternative to this long and laborious process for difficult proteins. The ability to predict a protein’s shape computationally from its genetic code alone – rather than determining it through costly experimentation – could help accelerate research. Fortunately, the field of genomics is quite rich in data thanks to the rapid reduction in the cost of genetic sequencing. As a result, deep learning approaches to the prediction problem that rely on genomic data have become increasingly popular in the last few years. To catalyse research and measure progress on the newest methods for improving the accuracy of predictions, a biennial global competition called CASP ( Critical Assessment of protein Structure Prediction ) was established in 1994, and has become the gold standard for assessing predictive techniques. We’re indebted to decades of prior work by the CASP organisers, as well as to the thousands of experimentalists whose structures enable this kind of assessment. DeepMind’s work on this problem resulted in AlphaFold, which we submitted to CASP13. We’re proud to be part of what the CASP organisers have called “unprecedented progress in the ability of computational methods to predict protein structure,” placing first in rankings among the teams that entered (our entry is A7D). Our team focused specifically on the problem of modelling target shapes from scratch, without using previously solved proteins as templates. We achieved a high degree of accuracy when predicting the physical properties of a protein structure, and then used two distinct methods to construct predictions of full protein structures. Both of these methods relied on deep neural networks that are trained to predict properties of the protein from its genetic sequence. The properties our networks predict are: (a) the distances between pairs of amino acids and (b) the angles between chemical bonds that connect those amino acids. The first development is an advance on commonly used techniques that estimate whether pairs of amino acids are near each other. We trained a neural network to predict a distribution of distances between every pair of residues in a protein (visualised in Figure 2). These probabilities were then combined into a score that estimates how accurate a proposed protein structure is. We also trained a separate neural network that uses all distances in aggregate to estimate how close the proposed structure is to the right answer. Using these scoring functions, we were able to search the protein landscape to find structures that matched our predictions. Our first method built on techniques commonly used in structural biology, and repeatedly replaced pieces of a protein structure with new protein fragments. We trained a generative neural network to invent new fragments, which were used to continually improve the score of the proposed protein structure. The second method optimised scores through gradient descent - a mathematical technique commonly used in machine learning for making small, incremental improvements - which resulted in highly accurate structures. This technique was applied to entire protein chains rather than to pieces that must be folded separately before being assembled into a larger structure, to simplify the prediction process. The AlphaFold version used at CASP13 is available on Github for anyone interested in learning more, or replicating our protein folding results. While we’re thrilled by the success of our protein folding model, there’s still much to be done in the realm of protein biology, and we’re excited to continue our efforts in this field. We’re committed to establishing ways that AI can contribute to basic scientific discovery, with the hope of making real-world impact. This approach might serve to ultimately improve our understanding of the body and how it works, enabling scientists to target and design new, effective cures for diseases more efficiently. Scientists have only mapped structures for about half of all the proteins made by human cells. Some rare diseases involve mutations in a single gene, resulting in a malformed protein which can have profound effects on the health of an entire organism. A tool like AlphaFold might help rare disease researchers predict the shape of a protein of interest rapidly and economically. As scientists acquire more knowledge about the shapes of proteins and how they operate through simulations and models, this method may eventually help us contribute to efficient drug discovery, while also reducing the costs associated with experimentation. Our hope is that AI will be useful for disease research, and ultimately improve the quality of life for millions of patients around the world. But potential benefits aren’t restricted to health alone - understanding protein folding will assist in protein design, which could unlock a tremendous number of benefits . For example, advances in biodegradable enzymes - which can be enabled by protein design - could help manage pollutants like plastic and oil, helping us break down waste in ways that are more friendly to our environment. In fact, researchers have already begun engineering bacteria to secrete proteins that will make waste biodegradable, and easier to process. The success of our first foray into protein folding is indicative of how machine learning systems can integrate diverse sources of information to help scientists come up with creative solutions to complex problems at speed. Just as we’ve seen how AI can help people master complex games through systems like AlphaGo and AlphaZero , we similarly hope that one day, AI breakthroughs will help serve as a platform to advance our understanding of fundamental scientific problems, too. It’s exciting to see these early signs of progress in protein folding, demonstrating the utility of AI for scientific discovery. Even though there’s a lot more work to do before we’re able to have a quantifiable impact on treating diseases, managing waste, and more, we know the potential is enormous. With a dedicated team focused on delving into how machine learning can advance the world of science, we’re looking forward to seeing the many ways our technology can make a difference. Listen to our podcast featuring the researchers behind this work. This blog post is based on the following work: AlphaFold: Improved protein structure prediction using potentials from deep learning (Nature) Protein structure prediction using multiple deep neural networks in CASP13 (PROTEINS) The AlphaFold version used at CASP13 is available on Github for anyone interested in learning more, or replicating our protein folding results. This work was done in collaboration with Andrew Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green, Chongli Qin, Augustin Žídek, Sandy Nelson, Alex Bridgland, Hugo Penedones, Stig Petersen, Karen Simonyan, Steve Crossan, Pushmeet Kohli, David Jones, David Silver, Koray Kavukcuoglu and Demis Hassabis", "date": "2020-01-15"},
{"website": "Deepmind", "title": "Trust, confidence and Verifiable Data Audit", "author": [" Mustafa Suleyman ", " Ben Laurie "], "link": "https://deepmind.com/blog/article/trust-confidence-verifiable-data-audit", "abstract": "Data can be a powerful force for social progress, helping our most important institutions to improve how they serve their communities. As cities, hospitals, and transport systems find new ways to understand what people need from them, they’re unearthing opportunities to change how they work today and identifying exciting ideas for the future. Data can only benefit society if it has society’s trust and confidence, and here we all face a challenge. Now that you can use data for so many more purposes, people aren’t just asking about who’s holding information and whether it’s being kept securely – they also want greater assurances about precisely what is being done with it. In that context, auditability becomes an increasingly important virtue. Any well-built digital tool will already log how it uses data, and be able to show and justify those logs if challenged. But the more powerful and secure we can make that audit process, the easier it becomes to establish real confidence about how data is being used in practice. Imagine a service that could give mathematical assurance about what is happening with each individual piece of personal data, without possibility of falsification or omission. Imagine the ability for the inner workings of that system to be checked in real-time, to ensure that data is only being used as it should be. Imagine that the infrastructure powering this was freely available as open source, so any organisation in the world could implement their own version if they wanted to. The working title for this project is “Verifiable Data Audit”, and we’re really excited to share more details about what we’re planning to build! Over the course of this year we'll be starting to build out Verifiable Data Audit for DeepMind Health , our effort to provide the health service with technology that can help clinicians predict, diagnose and prevent serious illnesses – a key part of DeepMind’s mission to deploy technology for social benefit. Given the sensitivity of health data, we’ve always believed that we should aim to be as innovative with governance as we are with the technology itself. We’ve already invited additional oversight of DeepMind Health by appointing a panel of unpaid Independent Reviewers who are charged with scrutinising our healthcare work, commissioning audits, and publishing an annual report with their findings. We see Verifiable Data Audit as a powerful complement to this scrutiny, giving our partner hospitals an additional real-time and fully proven mechanism to check how we’re processing data. We think this approach will be particularly useful in health, given the sensitivity of personal medical data and the need for each interaction with data to be appropriately authorised and consistent with rules around patient consent. For example, an organisation holding health data can’t simply decide to start carrying out research on patient records being used to provide care, or repurpose a research dataset for some other unapproved use. In other words: it’s not just where the data is stored, it’s what’s being done with it that counts. We want to make that verifiable and auditable, in real-time, for the first time. So, how will it work? We serve our hospital partners as a data processor, meaning that our role is to provide secure data services under their instructions, with the hospital remaining in full control throughout. Right now, any time our systems receive or touch that data, we create a log of that interaction that can be audited later if needed. With Verifiable Data Audit, we’ll build on this further. Each time there’s any interaction with data, we’ll begin to add an entry to a special digital ledger. That entry will record the fact that a particular piece of data has been used, and also the reason why - for example, that blood test data was checked against the NHS national algorithm to detect possible acute kidney injury. The ledger and the entries within it will share some of the properties of blockchain , which is the idea behind Bitcoin and other projects. Like blockchain, the ledger will be append-only, so once a record of data use is added, it can’t later be erased. And like blockchain, the ledger will make it possible for third parties to verify that nobody has tampered with any of the entries. But it’ll also differ from blockchain in a few important ways. Blockchain is decentralised, and so the verification of any ledger is decided by consensus amongst a wide set of participants. To prevent abuse, most blockchains require participants to repeatedly carry out complex calculations, with huge associated costs (according to some estimates, the total energy usage of blockchain participants could be as much as the power consumption of Cyprus ). This isn’t necessary when it comes to the health service, because we already have trusted institutions like hospitals or national bodies who can be relied on to verify the integrity of ledgers, avoiding some of the wastefulness of blockchain. We can also make this more efficient by replacing the chain part of blockchain, and using a tree-like structure instead (if you’d like to understand more about Merkle trees, a good place to start would be this blog from the UK’s Government Digital Service). The overall effect is much the same. Every time we add an entry to the ledger, we’ll generate a value known as a “cryptographic hash”. This hash process is special because it summarises not only the latest entry, but all of the previous values in the ledger too. This makes it effectively impossible for someone to go back and quietly alter one of the entries, since that will not only change the hash value of that entry but also that of the whole tree. In simple terms, you can think of it as a bit like the last move of a game of Jenga. You might try to gently take or move one of the pieces - but due to the overall structure, that’s going to end up making a big noise! So, now we have an improved version of the humble audit log: a fully trustworthy, efficient ledger that we know captures all interactions with data, and which can be validated by a reputable third party in the healthcare community. What do we do with that? The short answer is: massively improve the way in which these records can be audited. We’ll build a dedicated online interface that authorised staff at our partner hospitals can use to examine the audit trail of DeepMind Health’s data use in real-time. It will allow continuous verification that our systems are working as they should, and enable our partners to easily query the ledger to check for particular types of data use. We’d also like to enable our partners to run automated queries, effectively setting alarms that would be triggered if anything unusual took place. And, in time, we could even give our partners the option of allowing others to check our data processing, such as individual patients or patient groups. Building this is going to be a major undertaking, but given the importance of the issue we think it’s worth it. Right now, three big technical challenges stand out. No blind spots. For this to be provably trustworthy, it can’t be possible for data use to take place without being logged in the ledger - otherwise, the concept falls apart. As well as designing the logs to record the time, nature and purpose of any interaction with data, we’d also like to be able to prove that there’s no other software secretly interacting with data in the background. As well as logging every single data interaction in our ledger, we will also need to use formal methods as well as code and data centre audits by experts, to prove that every data access by every piece of software in the data centre is captured by these logs. We’re also interested in efforts to guarantee the trustworthiness of the hardware on which these systems run - an active topic of computer science research! Different uses for different groups. The core implementation will be an interface to allow our partner hospitals to provably check in real-time that we’re only using patient data for approved purposes. If these partners wanted to extend that ability to others, like patients or patient groups, there would be complex design questions to resolve. A long list of log entries may not be useful to many patients, and some may prefer to read a consolidated view or rely on a trusted intermediary instead. Equally, a patient group may not have the authority to see identified data, which would mean allowing our partners to provide some form of system-wide information - for example, whether machine learning algorithms have been run on particular datasets - without unintentionally revealing patient data. For technical details on how we could provide verified access to subsets or summaries of the data, see the open source Trillian project, which we will be using, and this paper explaining how it works . Decentralised data and logs, without gaps. There’s no single patient identified information database in the UK, and so the process of care involves data travelling back and forth between healthcare providers, IT systems, and even patient-controlled services like wearable devices. There’s a lot of work going into making these systems interoperable (our mobile product, Streams, is built to interoperable standards ) so they can work safely together. It would be helpful for these standards to include auditability as well, to avoid gaps where data becomes unauditable as it passes from one system to another. This doesn’t mean that a data processor like DeepMind should see data or audit logs from other systems. Logs should remain decentralised, just like the data itself. Audit interoperability would simply provide additional reassurance that this data can’t be tampered with as it travels between systems. This is a significant technical challenge, but we think it should be possible. Specifically, there’s an emerging open standard for interoperability in healthcare called FHIR, which could be extended to include auditability in useful ways. We’re hoping to be able to implement the first pieces of this later this year, and are planning to blog about our progress and the challenges we encounter as we go. We recognise this is really hard, and the toughest challenges are by no means the technical ones. We hope that by sharing our process and documenting our pitfalls openly, we’ll be able to partner with and get feedback from as many people as possible, and increase the chances of this kind of infrastructure being used more widely one day, within healthcare and maybe even beyond. If you have ideas or feedback, we’d love to hear from you at sayhi@deepmindhealth.com", "date": "2017-03-09"},
{"website": "Deepmind", "title": "Exploring the mysteries of Go with AlphaGo and China's top players", "author": [" Demis Hassabis ", " Fan Hui "], "link": "https://deepmind.com/blog/article/exploring-mysteries-alphago", "abstract": "Just over a year ago, we saw a major milestone in the field of artificial intelligence: DeepMind’s AlphaGo took on and defeated one of the world’s top Go players , the legendary Lee Sedol. Even then, we had no idea how this moment would affect the 3,000 year old game of Go and the growing global community of devotees to this beautiful board game. Instead of diminishing the game, as some feared, artificial intelligence (A.I.) has actually made human players stronger and more creative. It’s humbling to see how pros and amateurs alike, who have pored over every detail of AlphaGo’s innovative game play, have actually learned new knowledge and strategies about perhaps the most studied and contemplated game in history. You can read more about some of these creative strategies in this blog post. “AlphaGo’s play makes us feel free, that no move is impossible. Now everyone is trying to play in a style that hasn’t been tried before.” Zhou Ruiyang, 9 Dan Professional Clearly, there remains much more to learn from this partnership between Go’s best human players and its most creative A.I. competitor. That’s why we’re so excited to announce AlphaGo’s next step: a five-day festival of Go and artificial intelligence in the game's birthplace, China. From May 23-27, we’ll collaborate with the China Go Association and Chinese Government to bring AlphaGo, China’s top Go players, and leading A.I. experts from Google and China together in Wuzhen, one of the country’s most beautiful water towns, for the “Future of Go Summit.” The summit will feature a variety of game formats involving AlphaGo and top Chinese players, specifically designed to explore the mysteries of the game together. The games will include: Interspersed with the games will be a forum on the “Future of A.I.” Together with some of China’s leading experts in the field, we will explore how AlphaGo has created new knowledge about the oldest of games, and how the technologies behind AlphaGo, machine learning, and artificial intelligence, are bringing solutions to some of the world’s greatest challenges into reach. Already, some of the machine learning methods behind AlphaGo have been used to tackle significant problems, such as reducing energy use . Machine learning technology is also at work in a series of exciting medical research projects . And across many of Google’s products, machine learning has suddenly made the impossible real—from allowing people using Google Photos to find that photo of their dog in the snow almost instantly to improving the quality of Google Translate more in a single leap than the past 10 years of improvements combined. We’re excited to see what insights this next round of games and discussion will bring, and the challenges this will help us solve together—both on and off the Go board.", "date": "2017-04-10"},
{"website": "Deepmind", "title": "Safety-first AI for autonomous data centre cooling and industrial control", "author": [" Chris Gamble ", " Jim Gao "], "link": "https://deepmind.com/blog/article/safety-first-ai-autonomous-data-centre-cooling-and-industrial-control", "abstract": "Many of society’s most pressing problems have grown increasingly complex, so the search for solutions can feel overwhelming. At DeepMind and Google, we believe that if we can use AI as a tool to discover new knowledge, solutions will be easier to reach. In 2016, we jointly developed an AI-powered recommendation system to improve the energy efficiency of Google’s already highly-optimised data centres. Our thinking was simple: even minor improvements would provide significant energy savings and reduce CO2 emissions to help combat climate change. Now we’re taking this system to the next level: instead of human-implemented recommendations, our AI system is directly controlling data centre cooling, while remaining under the expert supervision of our data centre operators. This first-of-its-kind cloud-based control system is now safely delivering energy savings in multiple Google data centres. Every five minutes, our cloud-based AI pulls a snapshot of the data centre cooling system from thousands of sensors and feeds it into our deep neural networks, which predict how different combinations of potential actions will affect future energy consumption. The AI system then identifies which actions will minimise the energy consumption while satisfying a robust set of safety constraints. Those actions are sent back to the data centre, where the actions are verified by the local control system and then implemented. The idea evolved out of feedback from our data centre operators who had been using our AI recommendation system. They told us that although the system had taught them some new best practices—such as spreading the cooling load across more, rather than less, equipment—implementing the recommendations required too much operator effort and supervision. Naturally, they wanted to know whether we could achieve similar energy savings without manual implementation. We’re pleased to say the answer was yes! Google's data centres contain thousands of servers that power popular services including Google Search, Gmail and YouTube. Ensuring that they run reliably and efficiently is mission-critical. We've designed our AI agents and the underlying control infrastructure from the ground up with safety and reliability in mind, and use eight different mechanisms to ensure the system will behave as intended at all times. One simple method we’ve implemented is to estimate uncertainty. For every potential action—and there are billions—our AI agent calculates its confidence that this is a good action. Actions with low confidence are eliminated from consideration. Another method is two-layer verification. Optimal actions computed by the AI are vetted against an internal list of safety constraints defined by our data centre operators. Once the instructions are sent from the cloud to the physical data centre, the local control system verifies the instructions against its own set of constraints. This redundant check ensures that the system remains within local constraints and operators retain full control of the operating boundaries. Most importantly, our data centre operators are always in control and can choose to exit AI control mode at any time. In these scenarios, the control system will transfer seamlessly from AI control to the on-site rules and heuristics that define the automation industry today. Find out about the other safety mechanisms we’ve developed, below: Whereas our original recommendation system had operators vetting and implementing actions, our new AI control system directly implements the actions. We’ve purposefully constrained the system’s optimisation boundaries to a narrower operating regime to prioritise safety and reliability, meaning there is a risk/reward trade off in terms of energy reductions. Despite being in place for only a matter of months, the system is already delivering consistent energy savings of around 30 percent on average, with further expected improvements. That’s because these systems get better over time with more data, as the graph below demonstrates. Our optimisation boundaries will also be expanded as the technology matures, for even greater reductions. Our direct AI control system is finding yet more novel ways to manage cooling that have surprised even the data centre operators. Dan Fuenffinger, one of Google’s data centre operators who has worked extensively alongside the system, remarked: \"It was amazing to see the AI learn to take advantage of winter conditions and produce colder than normal water, which reduces the energy required for cooling within the data centre. Rules don’t get better over time, but AI does.\" We’re excited that our direct AI control system is operating safely and dependably, while consistently delivering energy savings. However, data centres are just the beginning. In the long term, we think there's potential to apply this technology in other industrial settings, and help tackle climate change on an even grander scale.", "date": "2018-08-17"},
{"website": "Deepmind", "title": "Learning human objectives by evaluating hypothetical behaviours", "author": [" Jan Leike ", " Siddharth Reddy "], "link": "https://deepmind.com/blog/article/learning-human-objectives-by-evaluating-hypothetical-behaviours", "abstract": "TL;DR: We present a method for training reinforcement learning agents from human feedback in the presence of unknown unsafe states. When we train reinforcement learning (RL) agents in the real world, we don’t want them to explore unsafe states , such as driving a mobile robot into a ditch or writing an embarrassing email to one’s boss. Training RL agents in the presence of unsafe states is known as the safe exploration problem . We tackle the hardest version of this problem, in which the agent initially doesn’t know how the environment works or where the unsafe states are. The agent has one source of information: feedback about unsafe states from a human user. Existing methods for training agents from human feedback ask the user to evaluate data of the agent acting in the environment. That is – in order to learn about unsafe states, the agent first needs to visit these states, so the user can provide feedback on them. This makes prior work inapplicable to tasks that require safe exploration. In our latest paper , we propose a method for reward modeling that operates in two phases. First, the system is encouraged to explore a wide range of states through synthetically-generated, hypothetical behaviour. The user provides feedback on this hypothetical behaviour, and the system interactively learns a model of the user's reward function. Only after the model has successfully learned to predict rewards and unsafe states, we deploy an RL agent that safely performs the desired task. We start with a generative model of initial states and a forward dynamics model, trained on off-policy data like random trajectories or safe expert demonstrations. Our method uses these models to synthesise hypothetical behaviours, asks the user to label the behaviours with rewards, and trains a neural network to predict these rewards. The key idea is to actively synthesise the hypothetical behaviours from scratch to make them as informative as possible, without interacting with the environment . We call this method reward query synthesis via trajectory optimisation (ReQueST). For this approach to work, we need the system to simulate and explore a wide range of behaviours, in order to effectively train the reward model. To encourage exploration during reward model training, ReQueST synthesises four different types of hypothetical behaviours using gradient descent trajectory optimisation. The first type of hypothetical behaviour maximises the uncertainty of an ensemble of reward models , eliciting user labels for behaviours that have the highest information value. The second type of hypothetical behaviour maximises predicted rewards , surfacing behaviours for which the reward model might be incorrectly predicting high rewards; i.e., reward hacking . The third type of hypothetical behaviour minimises predicted rewards , adding potentially unsafe hypothetical behaviours to the training data. This data enables the reward model to learn about unsafe states. The fourth type of hypothetical behaviour maximises the novelty of trajectories , encouraging exploration of a wide range of states, regardless of predicted rewards. Each hypothetical behaviour consists of a sequence of state transitions (s, a, s’). We ask the user to label each state transition with a reward, r. Then, given the labeled dataset of transitions (s, a, r, s’), we train a neural network to predict rewards using a maximum-likelihood objective. We use standard supervised learning techniques based on gradient descent. Once the user is satisfied with the reward model, we deploy a planning-based agent that uses model-predictive control (MPC) to pick actions that optimise the learned rewards. Unlike model-free RL algorithms like Q-learning or policy gradient methods that learn through trial and error, model-based RL algorithms like MPC enable the agent to avoid unsafe states during deployment by using the dynamics model to anticipate the consequences of its actions. We evaluate ReQueST with simulated users on a state-based 2D navigation task and the image-based Car Racing video game. Our results show that ReQueST satisfies three important safety properties: it can train a reward model to detect unsafe states without visiting them ; it can correct reward hacking before deploying the agent; and it tends to learn robust reward models that perform well when transferred to new environments . To test the generalisation of the reward model, we set up a 2D navigation task with separate training and test environments. We intentionally introduce a significant shift in the initial state distribution: the agent starts at the lower left corner (0, 0) in the training environment, and at the upper right corner (1, 1) in the test environment. Prior methods that collect data by deploying an agent in the training environment are unlikely to learn about the trap in the upper right corner, because they immediately find the goal, then fail to continue exploring. ReQueST synthesizes a variety of hypothetical states, including states in and around the trap. The user labels these states with rewards, using which ReQueST learns a robust reward model that enables the agent to navigate around the trap in the test environment. To test whether ReQueST scales to domains with high-dimensional, continuous states like images, we use the Car Racing video game from the OpenAI Gym. In addition to benchmarking ReQueST against prior methods, we ran a hyperparameter sweep and ablation study, where we varied the regularization strength of the dynamics model during trajectory optimisation as well as the subset of hypotheticals synthesized in order to measure ReQueST’s sensitivity to these settings. We found that ReQueST can trade off between producing realistic vs. informative queries, and that the optimal trade-off varies across domains. We also found that the usefulness of each of the four hypothetical behaviours depends on the domain and the amount of training data collected. To our knowledge, ReQueST is the first reward modeling algorithm that safely learns about unsafe states and scales to training neural network reward models in environments with high-dimensional, continuous states. ReQueST relies on a generative model of initial states and a forward dynamics model, which can be hard to acquire for visual domains with complex dynamics. So far, we have only demonstrated the effectiveness of ReQueST in simulated domains with relatively simple dynamics. One direction for future work is to test ReQueST in 3D domains with more realistic physics and other agents acting in the environment. Thanks to Zac Kenton and Kelly Clancy for feedback on early drafts of this post, and to Paulo Estriga for his design work.", "date": "2019-12-13"},
{"website": "Deepmind", "title": "WaveNet: A generative model for raw audio", "author": [" Aäron van den Oord ", " Sander Dieleman "], "link": "https://deepmind.com/blog/article/wavenet-generative-model-raw-audio", "abstract": "This post presents WaveNet , a deep generative model of raw audio waveforms. We show that WaveNets are able to generate speech which mimics any human voice and which sounds more natural than the best existing Text-to-Speech systems, reducing the gap with human performance by over 50%. We also demonstrate that the same network can be used to synthesize other audio signals such as music, and present some striking samples of automatically generated piano pieces. Allowing people to converse with machines is a long-standing dream of human-computer interaction. The ability of computers to understand natural speech has been revolutionised in the last few years by the application of deep neural networks (e.g., Google Voice Search ). However, generating speech with computers  — a process usually referred to as speech synthesis or text-to-speech (TTS) — is still largely based on so-called concatenative TTS , where a very large database of short speech fragments are recorded from a single speaker and then recombined to form complete utterances. This makes it difficult to modify the voice (for example switching to a different speaker, or altering the emphasis or emotion of their speech) without recording a whole new database. This has led to a great demand for parametric TTS , where all the information required to generate the data is stored in the parameters of the model, and the contents and characteristics of the speech can be controlled via the inputs to the model. So far, however, parametric TTS has tended to sound less natural than concatenative. Existing parametric models typically generate audio signals by passing their outputs through signal processing algorithms known as vocoders . WaveNet changes this paradigm by directly modelling the raw waveform of the audio signal, one sample at a time. As well as yielding more natural-sounding speech, using raw waveforms means that WaveNet can model any kind of audio, including music. Researchers usually avoid modelling raw audio because it ticks so quickly: typically 16,000 samples per second or more, with important structure at many time-scales. Building a completely autoregressive model, in which the prediction for every one of those samples is influenced by all previous ones (in statistics-speak, each predictive distribution is conditioned on all previous observations), is clearly a challenging task. However, our PixelRNN and PixelCNN models, published earlier this year, showed that it was possible to generate complex natural images not only one pixel at a time, but one colour-channel at a time, requiring thousands of predictions per image. This inspired us to adapt our two-dimensional PixelNets to a one-dimensional WaveNet. The above animation shows how a WaveNet is structured. It is a fully convolutional neural network, where the convolutional layers have various dilation factors that allow its receptive field to grow exponentially with depth and cover thousands of timesteps. At training time, the input sequences are real waveforms recorded from human speakers. After training, we can sample the network to generate synthetic utterances. At each step during sampling a value is drawn from the probability distribution computed by the network. This value is then fed back into the input and a new prediction for the next step is made. Building up samples one step at a time like this is computationally expensive, but we have found it essential for generating complex, realistic-sounding audio. We trained WaveNet using some of Google’s TTS datasets so we could evaluate its performance. The following figure shows the quality of WaveNets on a scale from 1 to 5, compared with Google’s current best TTS systems ( parametric and concatenative ), and with human speech using Mean Opinion Scores (MOS) . MOS are a standard measure for subjective sound quality tests, and were obtained in blind tests with human subjects (from over 500 ratings on 100 test sentences). As we can see, WaveNets reduce the gap between the state of the art and human-level performance by over 50% for both US English and Mandarin Chinese. For both Chinese and English, Google’s current TTS systems are considered among the best worldwide, so improving on both with a single model is a major achievement. Here are some samples from all three systems so you can listen and compare yourself: Parametric Concatenative WaveNet Parametric Concatenative WaveNet In order to use WaveNet to turn text into speech, we have to tell it what the text is. We do this by transforming the text into a sequence of linguistic and phonetic features (which contain information about the current phoneme, syllable, word, etc.) and by feeding it into WaveNet. This means the network’s predictions are conditioned not only on the previous audio samples, but also on the text we want it to say. If we train the network without the text sequence, it still generates speech, but now it has to make up what to say. As you can hear from the samples below, this results in a kind of babbling, where real words are interspersed with made-up word-like sounds: Notice that non-speech sounds, such as breathing and mouth movements, are also sometimes generated by WaveNet; this reflects the greater flexibility of a raw-audio model. As you can hear from these samples, a single WaveNet is able to learn the characteristics of many different voices, male and female. To make sure it knew which voice to use for any given utterance, we conditioned the network on the identity of the speaker. Interestingly, we found that training on many speakers made it better at modelling a single speaker than training on that speaker alone, suggesting a form of transfer learning. By changing the speaker identity, we can use WaveNet to say the same thing in different voices: Similarly, we could provide additional inputs to the model, such as emotions or accents, to make the speech even more diverse and interesting. Since WaveNets can be used to model any audio signal, we thought it would also be fun to try to generate music. Unlike the TTS experiments, we didn’t condition the networks on an input sequence telling it what to play (such as a musical score); instead, we simply let it generate whatever it wanted to. When we trained it on a dataset of classical piano music, it produced fascinating samples like the ones below: WaveNets open up a lot of possibilities for TTS, music generation and audio modelling in general. The fact that directly generating timestep per timestep with deep neural networks works at all for 16kHz audio is really surprising, let alone that it outperforms state-of-the-art TTS systems. We are excited to see what we can do with them next. For more details, take a look at our paper .", "date": "2016-09-08"},
{"website": "Deepmind", "title": "Identifying and eliminating bugs in learned predictive models", "author": [" Pushmeet Kohli ", " Krishnamurthy (Dj) Dvijotham ", " Jonathan Uesato ", " Sven Gowal "], "link": "https://deepmind.com/blog/article/robust-and-verified-ai", "abstract": "One in a series of posts explaining the theories underpinning our research. Bugs and software have gone hand in hand since the beginning of computer programming. Over time, software developers have established a set of best practices for testing and debugging before deployment, but these practices are not suited for modern deep learning systems. Today, the prevailing practice in machine learning is to train a system on a training data set, and then test it on another set. While this reveals the average-case performance of models, it is also crucial to ensure robustness, or acceptably high performance even in the worst case.  In this article, we describe three approaches for rigorously identifying and eliminating bugs in learned predictive models: adversarial testing, robust learning, and formal verification. Machine learning systems are not robust by default. Even systems that outperform humans in a particular domain can fail at solving simple problems if subtle differences are introduced. For example, consider the problem of image perturbations: a neural network that can classify images better than a human can be easily fooled into believing that sloth is a race car if a small amount of carefully calculated noise is added to the input image. This is not an entirely new problem. Computer programs have always had bugs. Over decades, software engineers have assembled an impressive toolkit of techniques, ranging from unit testing to formal verification. These methods work well on traditional software, but adapting these approaches to rigorously test machine learning models like neural networks is extremely challenging due to the scale and lack of structure in these models, which may contain hundreds of millions of parameters. This necessitates the need for developing novel approaches for ensuring that machine learning systems are robust at deployment. From a programmer’s perspective, a bug is any behaviour that is inconsistent with the specification , i.e. the intended functionality, of a system. As part of our mission of solving intelligence, we conduct research into techniques for evaluating whether machine learning systems are consistent not only with the train and test set, but also with a list of specifications describing desirable properties of a system. Such properties might include robustness to sufficiently small perturbations in inputs, safety constraints to avoid catastrophic failures, or producing predictions consistent with the laws of physics. In this article, we discuss three important technical challenges for the machine learning community to take on, as we collectively work towards rigorous development and deployment of machine learning systems that are reliably consistent with desired specifications: Testing consistency with specifications efficiently. We explore efficient ways to test that machine learning systems are consistent with properties (such as invariance or robustness) desired by the designer and users of the system. One approach to uncover cases where the model might be inconsistent with the desired behaviour is to systematically search for worst-case outcomes during evaluation. Training machine learning models to be specification-consistent. Even with copious training data, standard machine learning algorithms can produce predictive models that make predictions inconsistent with desirable specifications like robustness or fairness - this requires us to reconsider training algorithms that produce models that not only fit training data well, but also are consistent with a list of specifications. Formally proving that machine learning models are specification-consistent. There is a need for algorithms that can verify that the model predictions are provably consistent with a specification of interest for all possible inputs. While the field of formal verification has studied such algorithms for several decades, these approaches do not easily scale to modern deep learning systems despite impressive progress. Robustness to adversarial examples is a relatively well-studied problem in deep learning. One major theme that has come out of this work is the importance of evaluating against strong attacks, and designing transparent models which can be efficiently analysed. Alongside other researchers from the community, we have found that many models appear robust when evaluated against weak adversaries. However, they show essentially 0% adversarial accuracy when evaluated against stronger adversaries ( Athalye et al., 2018 , Uesato et al., 2018 , Carlini and Wagner, 2017 ). While most work has focused on rare failures in the context of supervised learning (largely image classification), there is a need to extend these ideas to other settings. In recent work on adversarial approaches for uncovering catastrophic failures, we apply these ideas towards testing reinforcement learning agents intended for use in safety-critical settings. One challenge in developing autonomous systems is that because a single mistake may have large consequences, very small failure probabilities are unacceptable. Our objective is to design an “adversary” to allow us to detect such failures in advance (e.g., in a controlled environment). If the adversary can efficiently identify the worst-case input for a given model, this allows us to catch rare failure cases before deploying a model. As with image classifiers, evaluating against a weak adversary provides a false sense of security during deployment. This is similar to the software practice of red-teaming, though extends beyond failures caused by malicious adversaries, and also includes failures which arise naturally, for example due to lack of generalization. We developed two complementary approaches for adversarial testing of RL agents. In the first, we use a derivative-free optimisation to directly minimise the expected reward of an agent. In the second, we learn an adversarial value function which predicts from experience which situations are most likely to cause failures for the agent. We then use this learned function for optimisation to focus the evaluation on the most problematic inputs. These approaches form only a small part of a rich, growing space of potential algorithms, and we are excited about future development in rigorous evaluation of agents. Already, both approaches result in large improvements over random testing. Using our method, failures that would have taken days to uncover, or even gone undetected entirely, can be detected in minutes ( Uesato et al., 2018b ). We also found that adversarial testing may uncover qualitatively different behaviour in our agents from what might be expected from evaluation on a random test set. In particular, using adversarial environment construction we found that agents performing a 3D navigation task, which match human-level performance on average, still failed to find the goal completely on surprisingly simple mazes ( Ruderman et al., 2018 ). Our work also highlights that we need to design systems that are secure against natural failures, not only against adversaries. Adversarial testing aims to find a counter example that violates specifications. As such, it often leads to overestimating the consistency of models with respect to these specifications. Mathematically, a specification is some relationship that has to hold between the inputs and outputs of a neural network. This can take the form of upper and lower bounds on certain key input and output parameters. Motivated by this observation, several researchers ( Raghunathan et al., 2018 ; Wong et al., 2018 ; Mirman et al., 2018 ; Wang et al., 2018 ) including our team at DeepMind ( Dvijotham et al., 2018 ; Gowal et al., 2018 ), have worked on algorithms that are agnostic to the adversarial testing procedure (used to assess consistency with the specification). This can be understood geometrically - we can bound (e.g., using interval bound propagation; Ehlers 2017 , Katz et al. 2017 , Mirman et al., 2018 ) the worst violation of a specification by bounding the space of outputs given a set of inputs. If this bound is differentiable with respect to network parameters and can be computed quickly, it can be used during training. The original bounding box can then be propagated through each layer of the network. We show that interval bound propagation is fast, efficient, and — contrary to prior belief — can achieve strong results ( Gowal et al., 2018 ). In particular, we demonstrate that it can decrease the provable error rate (i.e., maximal error rate achievable by any adversary) over state-of-the-art in image classification on both MNIST and CIFAR-10 datasets. Going forward, the next frontier will be to learn the right geometric abstractions to compute tighter overapproximations of the space of outputs. We also want to train networks to be consistent with more complex specifications capturing desirable behavior, such as above mentioned invariances and consistency with physical laws. Rigorous testing and training can go a long way towards building robust machine learning systems. However, no amount of testing can formally guarantee that a system will behave as we want. In large-scale models, enumerating all possible outputs for a given set of inputs (for example, infinitesimal perturbations to an image) is intractable due to the astronomical number of choices for the input perturbation. However, as in the case of training, we can find more efficient approaches by setting geometric bounds on the set of outputs. Formal verification is a subject of ongoing research at DeepMind. The machine learning community has developed several interesting ideas on how to compute precise geometric bounds on the space of outputs of the network ( Katz et al. 2017 , Weng et al., 2018 ; Singh et al., 2018 ). Our approach ( Dvijotham et al., 2018 ), based on optimisation and duality, consists of formulating the verification problem as an optimisation problem that tries to find the largest violation of the property being verified. By using ideas from duality in optimisation, the problem becomes computationally tractable. This results in additional constraints that refine the bounding boxes computed by interval bound propagation, using so-called cutting planes. This approach is sound but incomplete: there may be cases where the property of interest is true, but the bound computed by this algorithm is not tight enough to prove the property. However, once we obtain a bound, this formally guarantees that there can be no violation of the property. The figure below graphically illustrates the approach. This approach enables us to extend the applicability of verification algorithms to more general networks (activation functions, architectures), general specifications and more sophisticated deep learning models (generative models, neural processes, etc.) and specifications beyond adversarial robustness ( Qin, 2018 ). Deployment of machine learning in high-stakes situations presents unique challenges, and requires the development of evaluation techniques that reliably detect unlikely failure modes. More broadly, we believe that learning consistency with specifications can provide large efficiency improvements over approaches where specifications only arise implicitly from training data. We are excited about ongoing research into adversarial evaluation, learning robust models, and verification of formal specifications. Much more work is needed to build automated tools for ensuring that AI systems in the real world will do the “right thing”. In particular, we are excited about progress in the following directions: Learning for adversarial evaluation and verification: As AI systems scale and become more complex, it will become increasingly difficult to design adversarial evaluation and verification algorithms that are well-adapted to the AI model. If we can leverage the power of AI to facilitate evaluation and verification, this process can be bootstrapped to scale. Development of publicly-available tools for adversarial evaluation and verification: It is important to provide AI engineers and practitioners with easy-to-use tools that shed light on the possible failure modes of the AI system before it leads to widespread negative impact. This would require some degree of standardisation of adversarial evaluation and verification algorithms. Broadening the scope of adversarial examples: To date, most work on adversarial examples has focused on model invariances to small perturbations, typically of images. This has provided an excellent testbed for developing approaches to adversarial evaluation, robust learning, and verification. We have begun to explore alternate specifications for properties directly relevant in the real world, and are excited by future research in this direction. Learning specifications: Specifications that capture “correct” behavior in AI systems are often difficult to precisely state. Building systems that can use partial human specifications and learn further specifications from evaluative feedback would be required as we build increasingly intelligent agents capable of exhibiting complex behaviors and  acting in unstructured environments. DeepMind is dedicated to positive social impact through responsible development and deployment of machine learning systems. To make sure that the contributions of developers are reliably positive, we need to tackle many technical challenges. We are committed to taking part in this effort and are excited to work with the community on solving these challenges. This post describes the work of the Robust and Verified Deep Learning group (Pushmeet Kohli, DJ Krishnamurthy, Jonathan Uesato, Sven Gowal, Chongli Qin, Robert Stanforth, Po-Sen Huang) performed in collaboration with various contributors across DeepMind including Avraham Ruderman, Alhussein Fawzi, Ananya Kumar, Brendan O'Donoghue, Bristy Sikder, Chenglong Wang, Csaba Szepesvari, Hubert Soyer, Relja Arandjelovic, Richard Everett, Rudy Bunel, Timothy Mann, Grzegorz Swirszcz, and Tom Erez. Thanks to Vishal Maini, Aleš Flidr, Damien Boudot, and Jan Leike for their contributions to the post.", "date": "2019-03-28"},
{"website": "Deepmind", "title": "AlphaGo Zero: Starting from scratch", "author": [" David Silver ", " Demis Hassabis "], "link": "https://deepmind.com/blog/article/alphago-zero-starting-scratch", "abstract": "It is able to do this by using a novel form of reinforcement learning , in which AlphaGo Zero becomes its own teacher. The system starts off with a neural network that knows nothing about the game of Go. It then plays games against itself, by combining this neural network with a powerful search algorithm. As it plays, the neural network is tuned and updated to predict moves, as well as the eventual winner of the games. This updated neural network is then recombined with the search algorithm to create a new, stronger version of AlphaGo Zero, and the process begins again. In each iteration, the performance of the system improves by a small amount, and the quality of the self-play games increases, leading to more and more accurate neural networks and ever stronger versions of AlphaGo Zero. This technique is more powerful than previous versions of AlphaGo because it is no longer constrained by the limits of human knowledge. Instead, it is able to learn tabula rasa from the strongest player in the world: AlphaGo itself. It also differs from previous versions in other notable ways. All of these differences help improve the performance of the system and make it more general. But it is the algorithmic change that makes the system much more powerful and efficient. After just three days of self-play training, AlphaGo Zero emphatically defeated the previously published version of AlphaGo - which had itself defeated 18-time world champion Lee Sedol - by 100 games to 0. After 40 days of self training, AlphaGo Zero became even stronger, outperforming the version of AlphaGo known as “Master”, which has defeated the world's best players and world number one Ke Jie . Over the course of millions of AlphaGo vs AlphaGo games, the system progressively learned the game of Go from scratch, accumulating thousands of years of human knowledge during a period of just a few days. AlphaGo Zero also discovered new knowledge, developing unconventional strategies and creative new moves that echoed and surpassed the novel techniques it played in the games against Lee Sedol and Ke Jie. These moments of creativity give us confidence that AI will be a multiplier for human ingenuity, helping us with our mission to solve some of the most important challenges humanity is facing. While it is still early days, AlphaGo Zero constitutes a critical step towards this goal. If similar techniques can be applied to other structured problems, such as protein folding, reducing energy consumption or searching for revolutionary new materials, the resulting breakthroughs have the potential to positively impact society. Read the paper Read the accompanying Nature News and Views article Download AlphaGo Zero games Read more about AlphaGo This work was done by David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre, George van den Driessche, Thore Graepel and Demis Hassabis.", "date": "2017-10-18"},
{"website": "Deepmind", "title": "Applying for technical roles", "author": [" Feryal Behbahani ", " Kate Parkyn ", " Mihaela Rosca "], "link": "https://deepmind.com/blog/article/Women-in-machine-learning-A-look-into-the-technical-interview-process", "abstract": "It’s no secret that the gender gap still exists within STEM. Despite a slight increase in recent years, studies show that women only make up about a quarter of the overall STEM workforce in the UK. While the reasons vary, many women report feeling held back by a lack of representation, clear opportunities and information on what working in the sector actually involves. Closing the gap within STEM is not a quick fix but a collective effort of everyone in the industry. Various organisations like Women in Machine Learning (WiML) actively work to help create a more inclusive environment where the successes of women are amplified. They also stand as an important point of i nformation for the many women who want to learn more about what it’s like to work in STEM. That’s why for this year’s International Women in Engineering Day , we asked the WiML community to share with us the most common questions they receive about technical interviewing. To share their perspectives and to discuss what it’s actually like to work at DeepMind, we brought together Mihaela Rosca (Research Engineer), Feryal Behbahani (Research Scientist) and Kate Parkyn (Recruitment Lead - Research & Engineering). Mihaela: It’s not uncommon to have self doubts or feel as if you’re under prepared for a position in the field. There will never be a perfect time to apply and you can easily convince yourself that there’s more to learn but that shouldn’t be a deterring factor in your decision to apply. Of course the right skillset will depend on the specific role you’re after, but if you’re keen to work on the future of machine learning research, read research papers and implement state of the art algorithms - you’re ready...so apply! Curious? Learn more about our research and engineering teams. Kate: We recruit for many roles across the organisation so the qualities we focus on differ accordingly. The majority of research scientist hires we make are post PhD level, so we don’t over index on publications. We also don’t have a specific marker for degree achievement or GPA. When it comes to experience, we’re always interested in reading about a candidate's past internships and/or voluntary industry experiences. We look for proven ability not only in ‘research’ but also in implementation, engineering and application. Reading about side projects and open source contributions are also great to see when looking at potential candidates, so feel free to link your Github, side projects or code. For research engineers, it’s important to remember that the role is part research and part engineer, so we’re always looking for people that enjoy putting theory into computational form. For software engineers, we look for the clear ability to communicate problems and solutions. Software engineers at DeepMind regularly deal with ambiguous problems which also have underlying engineering complexities. Evidence of working on similar projects, or experiences in accelerating research and harnessing tools to augment research, is key. Kate: Creating the perfect CV or resume is a big job. Luckily there are a countless number of resources out there that can help you get the job done. To keep it simple, we’d suggest focusing on the following points: Feryal: There are a wide range of resources available to help you learn and develop your skills in machine learning. These include open-access introductory courses on YouTube (i.e. Nando de Freitas’s course on Deep Learning , David Silver’s course on Reinforcement Learning and DeepMind x UCL Lecture Series ), blog posts which provide overviews of particular techniques (e.g. Distill ) and more advanced machine learning conference proceedings such as NeurIPS , ICML and ICLR . There are also a number of summer schools (i.e. MLSS and DLRLSS ) that help support students and professionals who are interested in learning from leading experts in the field. Many of the summer schools also host videos and practical exercises from previous years which can act as excellent resources for learning at your own pace. It’s also great to look to organisations like Women in Machine Learning (WiML) that specifically help women in the field build their technical confidence and voice while amplifying their achievements to the wider community. Feryal: The interview process at DeepMind can vary depending on the particular role you’re applying for. From my experience, the interview process for a Research Scientist role consisted of four phases: Phase one - initial chat with the recruitment team This is to cover your background, experience, the motivation for applying and future plans. At this stage, you will also have the opportunity to ask any questions that you may have about the role or the interview process. Phase two - technical interviews This part of the process involves several sessions - including one with a technical quiz  that covers a large breadth of topics in computer science, statistics, mathematics and machine learning. It’s key that you revise broadly for this session! At this stage there will also be a coding interview where you [in your chosen language] will have to work through a few questions and a specific problem with the end goal of coming to a solution implementation. Phase three - research interviews This stage is made up of various short [i.e. ~30min] interviews with researchers and leads about your specific research background and interests. Here you will have the opportunity to give a talk about your research, which gives the interviewers a better idea of your overall research direction. At this point, try to show your technical understanding of the field and feel free to bring up your own achievements and research ideas. It’s not necessary, but I would also suggest reading through recent papers published by the DeepMind team to try to frame your strengths better! Phase four - culture interview Towards the end of the interview process, you will once again connect with the recruitment team to discuss DeepMind’s culture and mission. I recommend that you read about DeepMind’s mission and think about how your career goals can fit within it. Mihaela: Due to the versatility required to do machine learning research, the interview process has a relatively even split between coding and assessing research skills. The first stage focuses on mathematics, statistics, machine learning and computer science knowledge, while later stages focus on coding. Keep in mind that throughout the interview process, the interviewer is trying to assess your problem solving skills, so focus on communication and explain your answers. For my own interview, I prepared by reviewing some of the notes from my university lectures - including a statistics course I had taken. At the time I didn’t know a lot about reinforcement learning, so I did some additional research and watched David Silver’s UCL course on the topic. For my coding interview, I chose python. To prepare and to practice my speed I solved a few coding questions without using an integrated development environment (IDE) or my favourite editor - only a simple text editor. Mihaela: Absolutely! Research Engineers at DeepMind - and elsewhere - often lead projects of all sizes. They can lead as first authors of conference papers, or as larger team efforts which involve groups of different sizes and take place over multiple months. There are plenty of examples, but here are a few: AlphaZero , improving exploration in reinforcement learning using generative modeling , and open sourcing of core libraries such as Reverb . Feryal: Being a research scientist means that my day never really looks the same. My time is often spent thinking about my research projects, coding, meeting and discussing ideas with others, reading papers and attending presentations or reading groups. As always in research, what I’m doing can change depending on if I’m working towards a paper deadline, working on a specific project, or thinking about what to do next. Luckily DeepMind is really flexible in how one can organise their time and schedule. We use a “milestone system” which organises research into smaller, measurable chunks (e.g. 3-6 weeks) so this really helps with planning research and breaking it down into concrete steps.", "date": "2020-06-23"},
{"website": "Deepmind", "title": "Predicting eye disease with Moorfields Eye Hospital", "author": "Unknown", "link": "https://deepmind.com/blog/article/predicting-eye-disease-moorfields", "abstract": "In August, we announced the first stage of our joint research partnership with Moorfields Eye Hospital , which showed how AI could match world-leading doctors at recommending the correct course of treatment for over 50 eye diseases, and also explain how it arrives at its recommendations. Now we’re excited to start working on the next research challenge – whether we can help clinicians predict eye diseases before symptoms set in. There are two types of age-related macular degeneration (AMD), one of the most common blinding eye diseases, with 170 million sufferers worldwide. The ‘dry’ form is relatively common among those over 65, and often only causes mild sight loss. However, about 15% of patients with dry AMD go on to develop the more serious form of the disease – ‘wet’ AMD – which can cause permanent, blinding sight loss. Currently, ophthalmologists diagnose wet AMD by analysing highly detailed 3D scans of the back of the eye, called OCT scans. The first phase of our research suggested that our AI technology could help clinicians analyse these scans more quickly to detect the symptoms of patients who need urgent treatment – ultimately saving their sight. That could help patients who already display symptoms: but what about those who haven’t yet developed them? If AI could help predict severe eye diseases in advance, that could help clinicians prevent sight loss before it even occurs. Collaborating with the clinicians at Moorfields Eye Hospital, we’ll be analysing de-identified scans of up to 7,000 patients at Moorfields who had previously received treatment for wet AMD in one eye, to try to predict deterioration in the other, seemingly healthy eye. Predicting potential indicators of disease is a much more complicated – and computationally intense – task than identifying existing known symptoms. To carry out this research reliably, efficiently and at scale, we have agreed with Moorfields to use Google’s world-class cloud computing infrastructure, which is already being used in our partnership with Cancer Research UK on research to improve the diagnosis of breast cancer. Moorfields and DeepMind researchers will initially access this in the UK and US, but it may one day include cloud facilities around the world. The benefits of using the cloud have been endorsed by NHS Digital as well as other regulatory bodies. We believe that using this infrastructure – with its greater size and scalability, reliability and processing power – will help us achieve the best possible results, which could take us one step closer to improving patient care. As with all of our work, we’re committed to treating the data used in this research with the utmost care and respect. Data is encrypted and de-identified, accessible only to a limited number of researchers who are conducting this research. All access to data is automatically audited and logged, and granted only for officially approved research purposes, with Moorfields’ permission. We’re thrilled to be working with our partners at Moorfields to take this research a step further. This is the first phase of a number of new and exciting research projects that we will be working on in the coming months. We will keep you updated as we make progress.", "date": "2018-11-05"},
{"website": "Deepmind", "title": "Scalable agent architecture for distributed training", "author": [" Hubert Soyer ", " Drew Purves ", " Lasse Espeholt "], "link": "https://deepmind.com/blog/article/impala-scalable-distributed-deeprl-dmlab-30", "abstract": "Deep Reinforcement Learning (DeepRL) has achieved remarkable success in a range of tasks, from continuous control problems in robotics to playing games like Go and Atari. The improvements seen in these domains have so far been limited to individual tasks where a separate agent has been tuned and trained for each task. In our most recent work, we explore the challenge of training a single agent on many tasks. Today we are releasing DMLab-30, a set of new tasks that span a large variety of challenges in a visually unified environment with a common action space. Training an agent to perform well on many tasks requires massive throughput and making efficient use of every data point. To this end, we have developed a new, highly scalable agent architecture for distributed training called Importance Weighted Actor-Learner Architecture that uses a new off-policy correction algorithm called V-trace. DMLab-30 is a collection of new levels designed using our open source RL environment DeepMind Lab . These environments enable any DeepRL researcher to test systems on a large spectrum of interesting tasks either individually or in a multi-task setting. The tasks are designed to be as varied as possible. They differ in the goals they target, from learning, to memory, to navigation. They vary visually, from brightly coloured, modern-styled texture, to the subtle brown and greens of a desert at dawn, midday, or by night. And they contain physically different settings, from open, mountainous terrain, to right-angled mazes, to open, circular rooms. In addition, some of the environments include ‘bots’, with their own, internal, goal-oriented behaviours. Equally importantly, the goals and rewards differ across the different levels, from following language commands and using keys to open doors, foraging mushrooms, to plotting and following a complex irreversible path. However, at a basic level, the environments are all the same in terms of their action and observation space allowing a single agent to be trained to act in every environment in this highly varied set. More details about the environments can be found on the DeepMind Lab GitHub page . In order to tackle the challenging DMLab-30 suite, we developed a new distributed agent called Importance Weighted Actor-Learner Architecture that maximises data throughput using an efficient distributed architecture with TensorFlow . Importance Weighted Actor-Learner Architecture is inspired by the popular A3C architecture which uses multiple distributed actors to learn the agent’s parameters. In models like this, each of the actors uses a clone of the policy parameters to act in the environment. Periodically, actors pause their exploration to share the gradients they have computed with a central parameter server that applies updates (see figure below). Importance Weighted Actor-Learner Architecture actors on the other hand are not used to calculate gradients. Instead, they are just used to collect experience which is passed to a central learner that computes gradients, resulting in a model that has completely independent actors and learners. To take advantage of the scale of modern computing systems, Importance Weighted Actor-Learner Architectures can be implemented using a single learner machine or multiple learners performing synchronous updates between themselves. Separating the learning and acting in this way also has the advantage of increasing the throughput of the whole system since the actors no longer need to wait for the learning step like in architectures such as batched A2C. This allows us to train Importance Weighted Actor-Learner Architectures on interesting environments without suffering from variance in frame rendering-time or time consuming task restarts. However, decoupling the acting and learning causes the policy in the actor to lag behind the learner. In order to compensate for this difference we introduce a principled off-policy advantage actor critic formulation called V-trace which compensates for the trajectories obtained by actors being off policy. The details of the algorithm and its analysis can be found in our paper . Thanks to the optimised model of Importance Weighted Actor-Learner Architecture, it can process one-to-two orders of magnitude more experience compared to similar agents, making learning in challenging environments possible. We have compared Importance Weighted Actor-Learner Architectures with several popular actor-critic methods and have seen significant speed-ups. Additionally, the throughput using Importance Weighted Actor-Learner Architectures scales almost linearly with increasing number of actors and learners which shows that both the distributed agent model and the V-trace algorithm can handle very large scale experiments, even on the order of thousands of machines. When it was tested on the DMLab-30 levels, Importance Weighted Actor-Learner Architecture was 10 times more data efficient and achieved double the final score compared to distributed A3C.  Moreover, Importance Weighted Actor-Learner Architectures showed positive transfer from training in multi-task settings compared to training in single-task setting. Read the full Importance Weighted Actor-Learner Architectures paper here . Explore DMLab-30 here . This work was done by Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg and Koray Kavukcuoglu", "date": "2018-02-05"},
{"website": "Deepmind", "title": "Learning by playing", "author": [" Martin Riedmiller ", " Roland Hafner "], "link": "https://deepmind.com/blog/article/learning-playing", "abstract": "Getting children (and adults) to tidy up after themselves can be a challenge, but we face an even greater challenge trying to get our AI agents to do the same. Success depends on the mastery of several core visuo-motor skills: approaching an object, grasping and lifting it, opening a box and putting things inside of it. To make matters more complicated, these skills must be applied in the right sequence. Control tasks, like tidying up a table or stacking objects, require an agent to determine how, when and where to coordinate the nine joints of its simulated arms and fingers to move correctly and achieve its objective. The sheer number of possible combinations of movements at any given time, along with the need to carry out a long sequence of correct actions constitute a serious exploration problem—making this a particularly interesting area for reinforcement learning research. Techniques like reward shaping, apprenticeship learning or learning from demonstrations can help with the exploration problem. However, these methods rely on a considerable amount of knowledge about the task—the problem of learning complex control problems from scratch with minimal prior knowledge is still an open challenge. Our new paper proposes a new learning paradigm called ‘Scheduled Auxiliary Control (SAC-X)’ which seeks to overcome this exploration issue. SAC-X is based on the idea that to learn complex tasks from scratch, an agent has to learn to explore and master a set of basic skills first. Just as a baby must develop coordination and balance before she crawls or walks—providing an agent with internal (auxiliary) goals corresponding to simple skills increases the chance it can understand and perform more complicated tasks. We demonstrate the SAC-X approach on several simulated and real robot tasks using a variety of tasks including stacking problems with different objects and ‘tidying up a playground’, which involves moving objects into a box. The auxiliary tasks we define follow a general principle: they encourage the agent to explore its sensor space. For example, activating a touch sensor in its fingers, sensing a force in its wrist, maximising a joint angle in its proprioceptive sensors or forcing a movement of an object in its visual camera sensors. Each task is associated with a simple reward of one if the goal is achieved, and zero otherwise. Our agent can then decide by itself about its current ‘intention’, i.e. which goal to pursue next. This might be an auxiliary task or an externally defined target task. Crucially, the agent can detect and learn from reward signals for all other tasks that it is not currently following by making extensive use of replay-based off-policy learning. For example, when picking up or moving an object the agent might incidentally stack it, leading to the observation of rewards for ‘stacking’. Because a sequence of simple tasks can lead to the observation of a rare external reward, the ability to schedule intentions is crucial. It can create a personalised learning curriculum based on all the tangential knowledge it has collected. This turns out to be an effective way to exploit knowledge in such a large domain, and is particularly useful when there are only few external reward signals available. Our agent decides which intention to follow via a scheduling module. The scheduler is improved during training via a meta-learning algorithm that attempts to maximise progress on the main task, which results in significantly improved data-efficiency. Our evaluations show that SAC-X is able to solve all the tasks we set it from scratch—using the same underlying set of auxiliary tasks. Excitingly, SAC-X is also able to successfully learn a pick-up and a placing task from scratch directly on a real robot arm in our lab. In the past this has been particularly challenging because learning on robots in a real-world setup requires data-efficiency, so a popular approach is to pre-train an agent in simulation and then transfer the agent to the real robot arm. We consider SAC-X as an important step towards learning control tasks from scratch, when only the overall goal is specified. SAC-X allows you to define auxiliary tasks arbitrarily: they can be based on general insights (like deliberately activating sensors as suggested here), but could ultimately incorporate any task a researcher thinks is important. In that respect, SAC-X is a general RL method that is broadly applicable in general sparse reinforcement learning settings beyond control and robotics. This work was completed by Martin Riedmiller, Roland Hafner, Thomas Lampe, Michael Neunert, Jonas Degrave, Tom Van de Wiele, Volodymyr Mnih, Nicolas Heess and Tobias Springenberg. Read the paper here .", "date": "2018-02-28"},
{"website": "Deepmind", "title": "AlphaFold: Using AI for scientific discovery", "author": [" Andrew Senior ", " John Jumper ", " Demis Hassabis "], "link": "https://deepmind.com/blog/article/alphafold-casp13", "abstract": "Today we’re excited to share DeepMind’s first significant milestone in demonstrating how artificial intelligence research can drive and accelerate new scientific discoveries. With a strongly interdisciplinary approach to our work, DeepMind has brought together experts from the fields of structural biology, physics, and machine learning to apply cutting-edge techniques to predict the 3D structure of a protein based solely on its genetic sequence. Our system, AlphaFold , which we have been working on for the past two years, builds on years of prior research in using vast genomic data to predict protein structure. The 3D models of proteins that AlphaFold generates are far more accurate than any that have come before—making significant progress on one of the core challenges in biology. Proteins are large, complex molecules essential in sustaining life. Nearly every function our body performs—contracting muscles, sensing light, or turning food into energy—can be traced back to one or more proteins and how they move and change. The recipes for those proteins—called genes—are encoded in our DNA. What any given protein can do depends on its unique 3D structure. For example, antibody proteins that make up our immune systems are ‘Y-shaped’, and are akin to unique hooks. By latching on to viruses and bacteria, antibody proteins are able to detect and tag disease-causing microorganisms for extermination. Similarly, collagen proteins are shaped like cords, which transmit tension between cartilage, ligaments, bones, and skin. Other types of proteins include Cas9, which, using CRISPR sequences as a guide, act like scissors to cut and paste sections of DNA; antifreeze proteins, whose 3D structure allows them to bind to ice crystals and prevent organisms from freezing; and ribosomes that act like a programmed assembly line, which help build proteins themselves. But figuring out the 3D shape of a protein purely from its genetic sequence is a complex task that scientists have found challenging for decades. The challenge is that DNA only contains information about the sequence of a protein’s building blocks called amino acid residues, which form long chains. Predicting how those chains will fold into the intricate 3D structure of a protein is what’s known as the “protein folding problem”. The bigger the protein, the more complicated and difficult it is to model because there are more interactions between amino acids to take into account. As noted in Levinthal’s paradox , it would take longer than the age of the universe to enumerate all the possible configurations of a typical protein before reaching the right 3D structure. The ability to predict a protein’s shape is useful to scientists because it is fundamental to understanding its role within the body, as well as diagnosing and treating diseases believed to be caused by misfolded proteins, such as Alzheimer’s , Parkinson’s , Huntington’s and cystic fibrosis . We are especially excited about how it might improve our understanding of the body and how it works, enabling scientists to design new, effective cures for diseases more efficiently. As we acquire more knowledge about the shapes of proteins and how they operate through simulations and models, it opens up new potential within drug discovery while also reducing the costs associated with experimentation. That could ultimately improve the quality of life for millions of patients around the world. An understanding of protein folding will also assist in protein design, which could unlock a tremendous number of benefits. For example, advances in biodegradable enzymes—which can be enabled by protein design—could help manage pollutants like plastic and oil, helping us break down waste in ways that are more friendly to our environment. In fact, researchers have already begun engineering bacteria to secrete proteins that will make waste biodegradable, and easier to process. To catalyse research and measure progress on the newest methods for improving the accuracy of predictions, a global biennial competition called CASP ( Critical Assessment of protein Structure Prediction ) was established in 1994, and has become the gold standard for assessing techniques. Over the past five decades, scientists have been able to determine shapes of proteins in labs using experimental techniques like cryo-electron microscopy , nuclear magnetic resonance or X-ray crystallography , but each method depends on a lot of trial and error, which can take years and cost tens of thousands of dollars per structure. This is why biologists are turning to AI methods as an alternative to this long and laborious process for difficult proteins. Fortunately, the field of genomics is quite rich in data thanks to the rapid reduction in the cost of genetic sequencing. As a result, deep learning approaches to the prediction problem that rely on genomic data have become increasingly popular in the last few years. DeepMind’s work on this problem resulted in AlphaFold, which we submitted to CASP this year. We’re proud to be part of what the CASP organisers have called “unprecedented progress in the ability of computational methods to predict protein structure,” placing first in rankings among the teams that entered (our entry is A7D). Our team focused specifically on the hard problem of modelling target shapes from scratch, without using previously solved proteins as templates. We achieved a high degree of accuracy when predicting the physical properties of a protein structure, and then used two distinct methods to construct predictions of full protein structures. Both of these methods relied on deep neural networks that are trained to predict properties of the protein from its genetic sequence. The properties our networks predict are: (a) the distances between pairs of amino acids and (b) the angles between chemical bonds that connect those amino acids. The first development is an advance on commonly used techniques that estimate whether pairs of amino acids are near each other. We trained a neural network to predict a separate distribution of distances between every pair of residues in a protein. These probabilities were then combined into a score that estimates how accurate a proposed protein structure is. We also trained a separate neural network that uses all distances in aggregate to estimate how close the proposed structure is to the right answer. Using these scoring functions, we were able to search the protein landscape to find structures that matched our predictions. Our first method built on techniques commonly used in structural biology, and repeatedly replaced pieces of a protein structure with new protein fragments. We trained a generative neural network to invent new fragments, which were used to continually improve the score of the proposed protein structure. The second method optimised scores through gradient descent —a mathematical technique commonly used in machine learning for making small, incremental improvements—which resulted in highly accurate structures. This technique was applied to entire protein chains rather than to pieces that must be folded separately before being assembled, reducing the complexity of the prediction process. The success of our first foray into protein folding is indicative of how machine learning systems can integrate diverse sources of information to help scientists come up with creative solutions to complex problems at speed. Just as we’ve seen how AI can help people master complex games through systems like AlphaGo and AlphaZero , we similarly hope that one day, AI breakthroughs will help us master fundamental scientific problems, too. It’s exciting to see these early signs of progress in protein folding, demonstrating the utility of AI for scientific discovery. Even though there’s a lot more work to do before we’re able to have a quantifiable impact on treating diseases, managing the environment, and more, we know the potential is enormous. With a dedicated team focused on delving into how machine learning can advance the world of science, we’re looking forward to seeing the many ways our technology can make a difference. Until we have published a paper on this work, please cite it as: De novo structure prediction with deep-learning based scoring R.Evans,  J.Jumper, J.Kirkpatrick, L.Sifre, T.F.G.Green, C.Qin, A.Zidek, A.Nelson, A.Bridgland, H.Penedones, S.Petersen, K.Simonyan, S.Crossan, D.T.Jones, D.Silver, K.Kavukcuoglu, D.Hassabis, A.W.Senior In Thirteenth Critical Assessment of Techniques for Protein Structure Prediction (Abstracts) 1-4 December 2018. Retrieved from here here . This work was done in collaboration with Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green, Chongli Qin, Augustin Zidek, Sandy Nelson, Alex Bridgland, Hugo Penedones, Stig Petersen, Karen Simonyan, Steve Crossan, D avid Jones, David Silver, Koray Kavukcuoglu, Demis Hassabis, and Andrew Senior.", "date": "2018-12-02"},
{"website": "Deepmind", "title": "Prefrontal cortex as a meta-reinforcement learning system", "author": [" Jane Wang ", " Zeb Kurth-Nelson ", " Matt Botvinick "], "link": "https://deepmind.com/blog/article/prefrontal-cortex-meta-reinforcement-learning-system", "abstract": "Recently, AI systems have mastered a range of video-games such as Atari classics Breakout and Pong.  But as impressive as this performance is, AI still relies on the equivalent of thousands of hours of gameplay to reach and surpass the performance of human video game players. In contrast, we can usually grasp the basics of a video game we have never played before in a matter of minutes. The question of why the brain is able to do so much more with so much less has given rise to the theory of meta-learning, or ‘learning to learn’. It is thought that we learn on two timescales — in the short term we focus on learning about specific examples while over longer timescales we learn the abstract skills or rules required to complete a task. It is this combination that is thought to help us learn efficiently and apply that knowledge rapidly and flexibly on new tasks. Recreating this meta-learning structure in AI systems — called meta-reinforcement learning — has proven very fruitful in facilitating fast, one-shot, learning in our agents (see our paper and closely related work from OpenAI). However, the specific mechanisms that allow this process to take place in the brain are still largely unexplained in neuroscience. In our new paper in Nature Neuroscience (Download a PDF here ), we use the meta-reinforcement learning framework developed in AI research to investigate the role of dopamine in the brain in helping us to learn. Dopamine—commonly known as the brain’s pleasure signal—has often been thought of as analogous to the reward prediction error signal used in AI reinforcement learning algorithms. These systems learn to act by trial and error guided by the reward. We propose that dopamine’s role goes beyond just using reward to learn the value of past actions and that it plays an integral role, specifically within the prefrontal cortex area, in allowing us to learn efficiently, rapidly and flexibly on new tasks. We tested our theory by virtually recreating six meta-learning experiments from the field of neuroscience—each requiring an agent to perform tasks that use the same underlying principles (or set of skills) but that vary in some dimension. We trained a recurrent neural network (representing the prefrontal cortex) using standard deep reinforcement learning techniques (representing the role of dopamine) and then compared the activity dynamics of the recurrent network with real data taken from previous findings in neuroscience experiments. Recurrent networks are a good proxy for meta-learning because they are able to internalise past actions and observations and then draw on those experiences while training on a variety of tasks. One experiment we recreated is known as the Harlow Experiment, a psychology test from the 1940s used to explore the concept of meta-learning. In the original test, a group of monkeys were shown two unfamiliar objects to select from, only one of which gave them a food reward. They were shown these two objects six times, each time the left-right placement was randomised so the monkey had to learn which object gave a food reward. They were then shown two brand new objects, again only one would result in a food reward. Over the course of this training, the monkey developed a strategy to select the reward associated-object: it learnt to select randomly the first time, and then based on the reward feedback to choose the particular object, rather than the left or right position, from then on. The experiment shows that monkeys could internalise the underlying principles of the task and learn an abstract rule structure — in effect, learning to learn. When we simulated a very similar test using a virtual computer screen and randomly selected images, we found that our ‘meta-RL agent’ appeared to learn in a manner analogous to the animals in the Harlow Experiment, even when presented with entirely new images never seen before. In fact, we found that the meta-RL agent could learn to quickly adapt in a wide domain of tasks with different rules and structures. And because the network learned how to adapt to a variety of  tasks, it also learned general principles about how to learn efficiently. Importantly, we saw that the majority of learning took place in the recurrent network, which supports our proposal that dopamine plays a more integral role in the meta-learning process than previously thought. Dopamine is traditionally understood to strengthen synaptic links in the prefrontal system, reinforcing particular behaviours. In AI, this means the dopamine-like reward signal adjusts the artificial synaptic weights in a neural network as it learns the right way to solve a task. However, in our experiments the weights of the neural network were frozen, meaning they couldn’t be adjusted during the learning process, yet, the meta-RL agent was still able to solve and adapt to new tasks. This shows us that dopamine-like reward isn't only used to adjust weights, but it also conveys and encodes important information about abstract task and rule structure, allowing faster adaptation to new tasks. Neuroscientists have long observed similar patterns of neural activations in the prefrontal cortex, which is quick to adapt and flexible, but have struggled to find an adequate explanation for why that’s the case. The idea that the prefrontal cortex isn’t relying on slow synaptic weight changes to learn rule structures, but is using abstract model-based information directly encoded in dopamine, offers a more satisfactory reason for its versatility. In demonstrating that the key ingredients thought to give rise to meta-reinforcement learning in AI also exist in the brain, we’ve posed a theory that not only fits with what is known about both dopamine and prefrontal cortex but that also explains a range of mysterious findings from neuroscience and psychology. In particular, the theory sheds new light on how structured, model-based learning emerges in the brain, why dopamine itself contains model-based information, and how neurons in the prefrontal cortex become tuned to learning-related signals. Leveraging insights from AI which can be applied to explain findings in neuroscience and psychology highlights the value each field can offer the other. Going forward, we anticipate that much benefit can be gained in the reverse direction, by taking guidance from specific organisation of brain circuits in designing new models for learning in reinforcement learning agents. This work was completed by Jane X. Wang, Zeb Kurth-Nelson, Dharshan Kumaran, Dhruva Tirumala, Hubert Soyer, Joel Z. Leibo, Demis Hassabis and Matthew Botvinick. Download the Nature Neuroscience paper here . Download an Open Access version of the paper here .", "date": "2018-05-14"},
{"website": "Deepmind", "title": "AlphaFold: a solution to a 50-year-old grand challenge in biology", "author": [" The AlphaFold team "], "link": "https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology", "abstract": "Proteins are essential to life, supporting practically all its functions. They are large complex molecules, made up of chains of amino acids, and what a protein does largely depends on its unique 3D structure . Figuring out what shapes proteins fold into is known as the “protein folding problem” , and has stood as a grand challenge in biology for the past 50 years. In a major scientific advance, the latest version of our AI system AlphaFold has been recognised as a solution to this grand challenge by the organisers of the biennial Critical Assessment of protein Structure Prediction ( CASP ). This breakthrough demonstrates the impact AI can have on scientific discovery and its potential to dramatically accelerate progress in some of the most fundamental fields that explain and shape our world. A protein’s shape is closely linked with its function, and the ability to predict this structure unlocks a greater understanding of what it does and how it works. Many of the world’s greatest challenges, like developing treatments for diseases or finding enzymes that break down industrial waste, are fundamentally tied to proteins and the role they play. This has been a focus of intensive scientific research for many years, using a variety of experimental techniques to examine and determine protein structures, such as nuclear magnetic resonance and X-ray crystallography . These techniques, as well as newer methods like cryo-electron microscopy , depend on extensive trial and error, which can take years of painstaking and laborious work per structure, and require the use of multi-million dollar specialised equipment . In his acceptance speech for the 1972 Nobel Prize in Chemistry, Christian Anfinsen famously postulated that, in theory, a protein’s amino acid sequence should fully determine its structure. This hypothesis sparked a five decade quest to be able to computationally predict a protein’s 3D structure based solely on its 1D amino acid sequence as a complementary alternative to these expensive and time consuming experimental methods. A major challenge, however, is that the number of ways a protein could theoretically fold before settling into its final 3D structure is astronomical. In 1969 Cyrus Levinthal noted that it would take longer than the age of the known universe to enumerate all possible configurations of a typical protein by brute force calculation – Levinthal estimated 10^300 possible conformations for a typical protein. Yet in nature, proteins fold spontaneously, some within milliseconds – a dichotomy sometimes referred to as Levinthal’s paradox . In 1994, Professor John Moult and Professor Krzysztof Fidelis founded CASP as a biennial blind assessment to catalyse research, monitor progress, and establish the state of the art in protein structure prediction. It is both the gold standard for assessing predictive techniques and a unique global community built on shared endeavour. Crucially, CASP chooses protein structures that have only very recently been experimentally determined (some were still awaiting determination at the time of the assessment) to be targets for teams to test their structure prediction methods against; they are not published in advance. Participants must blindly predict the structure of the proteins, and these predictions are subsequently compared to the ground truth experimental data when they become available. We’re indebted to CASP’s organisers and the whole community, not least the experimentalists whose structures enable this kind of rigorous assessment. The main metric used by CASP to measure the accuracy of predictions is the Global Distance Test (GDT) which ranges from 0-100. In simple terms, GDT can be approximately thought of as the percentage of amino acid residues (beads in the protein chain) within a threshold distance from the correct position. According to Professor Moult , a score of around 90 GDT is informally considered to be competitive with results obtained from experimental methods. In the results from the 14th CASP assessment, released today, our latest AlphaFold system achieves a median score of 92.4 GDT overall across all targets. This means that our predictions have an average error ( RMSD ) of approximately 1.6 Angstroms , which is comparable to the width of an atom (or 0.1 of a nanometer). Even for the very hardest protein targets, those in the most challenging free-modelling category , AlphaFold achieves a median score of 87.0 GDT ( data available here ). These exciting results open up the potential for biologists to use computational structure prediction as a core tool in scientific research. Our methods may prove especially helpful for important classes of proteins, such as membrane proteins , that are very difficult to crystallise and therefore challenging to experimentally determine. We first entered CASP13 in 2018 with our initial version of AlphaFold , which achieved the highest accuracy among participants. Afterwards, we published a paper on our CASP13 methods in Nature with associated code , which has gone on to inspire other work and community-developed open source implementations . Now, new deep learning architectures we’ve developed have driven changes in our methods for CASP14, enabling us to achieve unparalleled levels of accuracy. These methods draw inspiration from the fields of biology, physics, and machine learning, as well as of course the work of many scientists in the protein folding field over the past half-century. A folded protein can be thought of as a “spatial graph”, where residues are the nodes and edges connect the residues in close proximity. This graph is important for understanding the physical interactions within proteins, as well as their evolutionary history. For the latest version of AlphaFold, used at CASP14, we created an attention-based neural network system, trained end-to-end, that attempts to interpret the structure of this graph, while reasoning over the implicit graph that it’s building. It uses evolutionarily related sequences, multiple sequence alignment (MSA), and a representation of amino acid residue pairs to refine this graph. By iterating this process, the system develops strong predictions of the underlying physical structure of the protein and is able to determine highly-accurate structures in a matter of days. Additionally, AlphaFold can predict which parts of each predicted protein structure are reliable using an internal confidence measure. We trained this system on publicly available data consisting of ~170,000 protein structures from the protein data bank together with large databases containing protein sequences of unknown structure. It uses approximately 16 TPUv3s (which is 128 TPUv3 cores or roughly equivalent to ~100-200 GPUs) run over a few weeks, a relatively modest amount of compute in the context of most large state-of-the-art models used in machine learning today. As with our CASP13 AlphaFold system, we are preparing a paper on our system to submit to a peer-reviewed journal in due course. When DeepMind started a decade ago, we hoped that one day AI breakthroughs would help serve as a platform to advance our understanding of fundamental scientific problems. Now, after 4 years of effort building AlphaFold, we’re starting to see that vision realised, with implications for areas like drug design and environmental sustainability. Professor Andrei Lupas, Director of the Max Planck Institute for Developmental Biology and a CASP assessor, let us know that, “AlphaFold’s astonishingly accurate models have allowed us to solve a protein structure we were stuck on for close to a decade, relaunching our effort to understand how signals are transmitted across cell membranes.” We’re optimistic about the impact AlphaFold can have on biological research and the wider world, and excited to collaborate with others to learn more about its potential in the years ahead. Alongside working on a peer-reviewed paper, we’re exploring how best to provide broader access to the system in a scalable way. In the meantime, we’re also looking into how protein structure predictions could contribute to our understanding of specific diseases with a small number of specialist groups, for example by helping to identify proteins that have malfunctioned and to reason about how they interact. These insights could enable more precise work on drug development, complementing existing experimental methods to find promising treatments faster. We’ve also seen signs that protein structure prediction could be useful in future pandemic response efforts, as one of many tools developed by the scientific community. Earlier this year, we predicted several protein structures of the SARS-CoV-2 virus, including ORF3a, whose structures were previously unknown. At CASP14, we predicted the structure of another coronavirus protein, ORF8 . Impressively quick work by experimentalists has now confirmed the structures of both ORF3a and ORF8 . Despite their challenging nature and having very few related sequences, we achieved a high degree of accuracy on both of our predictions when compared to their experimentally determined structures. As well as accelerating understanding of known diseases, we’re excited about the potential for these techniques to explore the hundreds of millions of proteins we don’t currently have models for – a vast terrain of unknown biology. Since DNA specifies the amino acid sequences that comprise protein structures, the genomics revolution has made it possible to read protein sequences from the natural world at massive scale – with 180 million protein sequences and counting in the Universal Protein database ( UniProt ). In contrast, given the experimental work needed to go from sequence to structure, only around 170,000 protein structures are in the Protein Data Bank ( PDB ). Among the undetermined proteins may be some with new and exciting functions and – just as a telescope helps us see deeper into the unknown universe – techniques like AlphaFold may help us find them. AlphaFold is one of our most significant advances to date but, as with all scientific research, there are still many questions to answer. Not every structure we predict will be perfect. There’s still much to learn, including how multiple proteins form complexes, how they interact with DNA , RNA , or small molecules , and how we can determine the precise location of all amino acid side chains. In collaboration with others, there’s also much to learn about how best to use these scientific discoveries in the development of new medicines, ways to manage the environment, and more. For all of us working on computational and machine learning methods in science, systems like AlphaFold demonstrate the stunning potential for AI as a tool to aid fundamental discovery. Just as 50 years ago Anfinsen laid out a challenge far beyond science’s reach at the time, there are many aspects of our universe that remain unknown. The progress announced today gives us further confidence that AI will become one of humanity’s most useful tools in expanding the frontiers of scientific knowledge, and we’re looking forward to the many years of hard work and discovery ahead! Until we’ve published a paper on this work, please cite: High Accuracy Protein Structure Prediction Using Deep Learning John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Kathryn Tunyasuvunakool, Olaf Ronneberger, Russ Bates, Augustin Žídek, Alex Bridgland, Clemens Meyer, Simon A A Kohl, Anna Potapenko, Andrew J Ballard, Andrew Cowie, Bernardino Romera-Paredes, Stanislav Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen, David Reiman, Martin Steinegger, Michalina Pacholska, David Silver, Oriol Vinyals, Andrew W Senior, Koray Kavukcuoglu, Pushmeet Kohli, Demis Hassabis. In Fourteenth Critical Assessment of Techniques for Protein Structure Prediction (Abstract Book), 30 November - 4 December 2020. Retrieved from here . We’re right at the beginning of exploring how best to enable other groups to use our structure predictions, alongside preparing a peer-reviewed paper for publication. While our team won’t be able to respond to every enquiry, if AlphaFold may be relevant to your work, please submit a few lines about it to alphafold@deepmind.com . We’ll be in contact if there’s scope for further exploration.", "date": "2020-11-30"},
{"website": "Deepmind", "title": "Using AI to plan head and neck cancer treatments", "author": [" Mustafa Suleyman "], "link": "https://deepmind.com/blog/article/ai-uclh-radiotherapy-planning", "abstract": "Early results from our partnership with the Radiotherapy Department at University College London Hospitals NHS Foundation Trust suggest that we are well on our way to developing an artificial intelligence (AI) system that can analyse and segment medical scans of head and neck cancer to a similar standard as expert clinicians. This segmentation process is an essential but time-consuming step when planning radiotherapy treatment. The findings also show that our system can complete this process in a fraction of the time. More than half a million people are diagnosed each year with cancers of the head and neck worldwide. Radiotherapy is a key part of treatment, but clinical staff have to plan meticulously so that healthy tissue doesn’t get damaged by radiation: a process which involves radiographers, oncologists and/or dosimetrists manually outlining the areas of anatomy that need radiotherapy, and those areas that should be avoided. Although our work is still at an early stage, we hope it could one day reduce the waiting time between diagnosis and treatment, which could potentially improve outcomes for cancer patients. We also hope that accurate auto-segmentation could speed up the adaptive radiotherapy process , whereby radiotherapy treatments are adapted as the tumour shrinks - although more work is needed to investigate how this would work in practice. As well as changing patients’ lives, this research could also free up time for the clinicians who treat them, meaning they get to spend more time on patient care, education and research. We’ve taken steps to ensure our work is clinically applicable. This includes the development of a new performance metric used to assess model performance that we believe is more representative of clinical processes, and a test set with new high-quality segmentations of scans selected from sites previously unseen to the model which demonstrates generalisability. Both of these have been open sourced to the research community. But for our system to have an impact on real people diagnosed with cancer, we need to expand it and demonstrate that it works in real clinical environments. That’s why we’re looking forward to moving into the next phase of work with UCLH, where we will be exploring a human evaluation of these AI algorithms to test how they might perform in a clinical environment. At DeepMind Health, we think it is important to share our work with others in the community. As such, Professor Olaf Ronneberger, Senior Research Scientist at DeepMind Health, will be presenting these initial findings at MICCAI , the world leading conference on medical imaging, this Sunday. Ultimately, we believe that advanced technologies can and should help change lives, and we’re excited for the next steps of this project. We’ll continue to keep you updated as we make progress.", "date": "2018-09-13"},
{"website": "Deepmind", "title": "Replay in biological and artificial neural networks", "author": [" Zeb Kurth-Nelson ", " Will Dabney "], "link": "https://deepmind.com/blog/article/replay-in-biological-and-artificial-neural-networks", "abstract": "One of a series of posts explaining the theories underpinning our research. Our waking and sleeping lives are punctuated by fragments of recalled memories: a sudden connection in the shower between seemingly disparate thoughts, or an ill-fated choice decades ago that haunts us as we struggle to fall asleep. By measuring memory retrieval directly in the brain, neuroscientists have noticed something remarkable: spontaneous recollections, measured directly in the brain, often occur as very fast sequences of multiple memories. These so-called 'replay' sequences play out in a fraction of a second–so fast that we're not necessarily aware of the sequence. In parallel, AI researchers discovered that incorporating a similar kind of experience replay improved the efficiency of learning in artificial neural networks. Over the last three decades, the AI and neuroscientific studies of replay have grown up together. Machine learning offers hypotheses sophisticated enough to push forward our expanding knowledge of the brain; and insights from neuroscience guide and inspire AI development. Replay is a key point of contact between the two fields because like the brain, AI uses experience to learn and improve. And each piece of experience offers much more potential for learning than can be absorbed in real-time–so continued offline learning is crucial for both brains and artificial neural nets. Neural replay sequences were originally discovered by studying the hippocampus in rats. As we know from the Nobel prize winning work of John O’Keefe and others, many hippocampal cells fire only when the animal is physically located in a specific place . In early experiments, rats ran the length of a single corridor or circular track, so researchers could easily determine which neuron coded for each position within the corridor. Afterwards, the scientists recorded from the same neurons while the rats rested. During rest, the cells sometimes spontaneously fired in rapid sequences demarking the same path the animal ran earlier, but at a greatly accelerated speed. These sequences are called replay. An entire replay sequence only lasts a fraction of a second, but plays through several seconds worth of real experience. We now know replay is essential for learning. In a number of more recent experiments, researchers recorded from hippocampus to detect a signature of replay events in real time. By disrupting brain activity during replay events (either during sleep or wakeful resting), scientists significantly impaired rodents’ ability to learn a new task. The same disruption applied 200 milliseconds out of sync with replay events had no effect on learning. While these experiments have been revealing, a significant limitation of rodent experiments is the difficulty of studying more sophisticated aspects of cognition, such as abstract concepts. In the last few years, replay-like sequences have also been detected in human brains, supporting the idea that replay is pervasive, and expanding the kinds of questions we can ask about it. Incorporating replay in silico has been beneficial to advancing artificial intelligence . Deep learning often depends upon a ready supply of large datasets. In reinforcement learning , these data come through direct interaction with the environment, which takes time. The technique of experience replay allows the agent to repeatedly rehearse previous interactions, making the most of each interaction. This method proved crucial for combining deep neural networks with reinforcement learning in the DQN agent that first mastered multiple Atari games. Since the introduction of DQN, the efficiency of replay has been improved by preferentially replaying the most salient experiences from memory, rather than simply choosing experiences at random for replay. And recently, a variant of preferential replay has been applied as a model in neuroscience to successfully explain empirical data from brain recordings. Further improvements in agent performance have come from combining experiences across multiple agents, learning about a variety of different behaviours from the same set of experiences, and replaying not only the trajectory of events in the world, but also the agent's corresponding internal memory states . Each of these methods makes interesting predictions for neuroscience that remain largely untested. As mentioned above, research into experience replay has unfolded along parallel tracks in artificial intelligence and neuroscience, with each field providing ideas and inspiration for the other. In particular, there is a central distinction, which has been studied in both fields, between two versions of replay. Suppose you come home and, to your surprise and dismay, discover water pooling on your beautiful wooden floors. Stepping into the dining room, you find a broken vase. Then you hear a whimper, and you glance out the patio door to see your dog looking very guilty. In the first version of replay, which we could call the \"movie\" version, when you sit down on the couch and take a rest, replay faithfully rehearses the actual experiences of the past. This theory says that your brain will replay the sequence: \"water, vase, dog\". In AI terms, the past experience was stored in a replay buffer, and trajectories for offline learning were drawn directly from the buffer. In the second version, which we might call \"imagination,\" replay doesn’t literally rehearse events in the order they were experienced. Instead, it infers or imagines the real relationships between events, and synthesises sequences that make sense given an understanding of how the world works. In AI terms, these replay sequences are generated using a learned model of the environment. The imagination theory makes a different prediction about how replay will look: when you rest on the couch, your brain should replay the sequence \"dog, vase, water\". You know from past experience that dogs are more likely to cause broken vases than broken vases are to cause dogs–and this knowledge can be used to reorganise experience into a more meaningful order. In deep RL, the large majority of agents have used movie-like replay, because it is easy to implement (the system can simply store events in memory, and play them back as they happened). However, RL researchers have continued to study the possibilities around imagination replay. Meanwhile in neuroscience, classic theories of replay postulated that movie replay would be useful to strengthen the connections between neurons that represent different events or locations in the order they were experienced. However, there have been hints from experimental neuroscience that replay might be able to imagine new sequences. The most compelling observation is that even when rats only experienced two arms of a maze separately, subsequent replay sequences sometimes followed trajectories from one arm into the other. But studies like this leave open the question of whether replay simply stitches together chunks of experienced sequences, or if it can synthesise new trajectories from whole cloth. Also, rodent experiments have been primarily limited to spatial sequences, but it would be fascinating to know whether humans' ability to imagine sequences is enriched by our vast reserve of abstract conceptual knowledge. We asked these questions in a set of recent experiments performed jointly between UCL, Oxford, and DeepMind. In these experiments, we first taught people a rule that defined how a set of objects could interact. The exact rule we used can be found in the paper. But to continue in the language of the \"water, vase, dog\" example, we can think of the rule as the knowledge that dogs can cause broken vases, and broken vases can cause water on the floor. We then presented these objects to people in a scrambled order (like \"water, vase, dog\"). That way, we could ask whether their brains replayed the items in the scrambled order that they experienced, or in the unscrambled order that meaningfully connected the items. They were shown the scrambled sequence and then given five minutes to rest, while sitting in an MEG brain scanner. As in previous experiments, fast replay sequences of the objects were evident in the brain recordings. (In yet another example of the virtuous circle between neuroscience and AI, we used machine learning to read out these signatures from cortical activity.) These spontaneous sequences played out rapidly over about a sixth of a second, and contained up to four objects in a row. However, the sequences did not play out in the experienced order (i.e., the scrambled order: spilled water –> vase –> dog). Instead, they played out the unscrambled , meaningful order: dog –> vase –> spilled water. This answers–in the affirmative–the questions of whether replay can imagine new sequences from whole cloth, and whether these sequences are shaped by abstract knowledge. However, this finding still leaves open the important question of how the brain builds these unscrambled sequences. To try to answer this, we played a second sequence for participants. In this sequence, you walk into your factory and see spilled oil on the floor. You then see a knocked over oil barrel. Finally, you turn to see a guilty robot. To unscramble this sequence, you can use the same kind of knowledge as in the \"water, vase, dog\" sequence: knowledge that a mobile agent can knock over containers, and those knocked-over containers can spill liquid. Using that knowledge, the second sequence can also be unscrambled: robot –> barrel –> spilled oil. By showing people multiple sequences with the same structure, we could examine two new types of neural representation. First, the part of the representation that is common between spilled water and spilled oil. This is an abstract code for \"a spilled liquid\", invariant over whether we're in the home sequence or the factory sequence. And second, the part of the representation that is common between water, vase and dog. This is an abstract code for \"the home sequence,\" invariant over which object we're considering. We found both of these types of abstract codes in the brain data. And to our surprise, during rest they played out in fast sequences that were precisely coordinated with the spontaneous replay sequences mentioned above. Each object in a replay sequence was preceded slightly by both abstract codes. For example, during a dog, vase, water replay sequence, the representation of \"water\" was preceded by the codes for \"home sequence\" and \"spilled liquid\". These abstract codes, which incorporate the conceptual knowledge that lets us unscramble the sequences, may help the brain to retrieve the correct item for the next slot in the replay sequence. This paints an interesting picture of a system where the brain slots new information into an abstract framework built from past experiences, keeping it organized using precise relative timings within very fast replay sequences. Each position within a sequence could be thought of as a role in an analogy (as in the above figure). Finally, we speculate that during rest, the brain may explore novel implications of previously-learned knowledge by placing an item into an analogy in which it's never been experienced, and examining the consequences. Coming back to the virtuous circle, analogy and abstraction are relatively underused in current neural network architectures. The new results described above both indicate that the imagination style of replay may be a fruitful avenue to continue pursuing in AI research, and suggest directions for neuroscience research to learn more about the brain mechanisms underlying analogy and abstraction. It's exciting to think about how data from the brain will continue helping with the advance toward better and more human-like artificial intelligence. The new work reported in Cell was done by Yunzhe Liu , Ray Dolan , Zeb Kurth-Nelson and Tim Behrens , and was a collaboration between DeepMind, the Wellcome Centre for Human Neuroimaging (UCL), the Max Planck-UCL Centre for Computational Psychiatry and Ageing Research, and the Wellcome Centre For Integrative Neuroimaging (Oxford).", "date": "2019-09-06"},
{"website": "Deepmind", "title": "DeepMind papers at ICML 2017 (part one)", "author": "Unknown", "link": "https://deepmind.com/blog/article/deepmind-papers-icml-2017-part-one", "abstract": "The first of our three-part series, which gives brief descriptions of the papers we are presenting at the ICML 2017 Conference in Sydney, Australia. Authors: Max Jaderberg, Wojciech Marian Czarnecki, Simon Osindero, Oriol Vinyals, Alex Graves, David Silver, Koray Kavukcuoglu When training neural networks, the modules (layers) are locked: they can only be updated after backpropagation. We remove this constraint by incorporating a learnt model of error gradients, Synthetic Gradients, which means we can update networks without full backpropagation. We show how this can be applied to feed-forward networks which allows every layer to be trained asynchronously, to RNNs which extends the time over which models can remember, and to multi-network systems to allow communication. For further details and related work, please see the paper . Check it out at ICML: Monday 07 August, 10:30-10:48 @ Darling Harbour Theatre (Talk) Monday 07 August, 18:30-22:00 PM @ Gallery #1 (Poster) Authors: Scott Reed, Aäron van den Oord, Nal Kalchbrenner, Ziyu Wang, Dan Belov, Nando de Freitas The parallel multiscale autoregressive density estimator generates high-resolution (512 by 512) images, with orders of magnitude speedup over other autoregressive models. We evaluate the model on class-conditional image generation, text-to-image synthesis, and action-conditional video generation, showing that our model achieves the best results among non-pixel-autoregressive density models that allow efficient sampling. For further details and related work, please see the paper . Check it out at ICML: Monday 07 August, 10:48-11:06 @ Parkside 1 (Talk) Monday 07 August, 18:30-20:00 @ Gallery #10 (Poster) Authors: Wojtek Czarnecki, Grzegorz Świrszcz, Max Jaderberg, Simon Osindero, Oriol Vinyals, Koray Kavukcuoglu Synthetic gradients has been shown to work empirically in both feed-forward and recurrent cases. This work focuses on why and how it actually works - it shows that under mild assumptions critical points are preserved and that in the simplest case of linear model, synthetic gradients based learning does converge to the global optimum. On the other hand, we present empirically that trained models might be qualitatively different from those obtained using backpropagation. For further details and related work, please see the paper . Check it out at ICML: Monday 07 August, 10:48-11:06 @ Darling Harbour Theatre (Talk) Monday 07 August, 18:30-20:00 @ Gallery #9 (Poster) Authors: Mohammad Gheshlaghi Azar, Ian Osband, Remi Munos We consider the problem of provably optimal exploration in reinforcement learning for finite horizon MDPs. We show that an optimistic modification to value iteration achieves a regret bound of order (HSAT)1/2  (up to a logarithmic factor) where H is the time horizon, S the number of states, A the number of actions and T the number of time-steps. This result improves over the best previous known bound HS(AT)1/2 achieved by the UCRL2 algorithm of [Jaksch, Ortner, Auer, 2010]. The key significance of our new results is that for large T, the sample complexity of our algorithm matches the optimal lower bound of Ω(HSAT)1/2. Our analysis contains two key insights. We use careful application of concentration inequalities to the optimal value function as a whole, rather than to the transitions probabilities (to improve scaling in S), and we define Bernstein-based \"exploration bonuses\" that use the empirical variance of the estimated values at the next states (to improve scaling in H). For further details and related work, please see the paper . Check it out at ICML: Monday 07 August, 10:48-11:06 @ C4.5 (Talk) Monday 07 August, 18:30-22:00 @ Gallery #12 (Poster) Authors: Nal Kalchbrenner, Aaron van den Oord, Karen Simonyan, Ivo Danihelka, Oriol Vinyals,Alex Graves, Koray Kavukcuoglu Predicting the continuation of frames in a video is a hallmark task in unsupervised learning. We present a video model, the VPN, that is probabilistic and that is able to make accurate and sharp predictions of future video frames. The VPN achieves, for the first time, a nearly perfect score on the Moving MNIST dataset and produces plausible futures of up to 18 frames of robotic arm movements. For further details and related work, please see the paper . Check it out at ICML: Monday 07 August, 11:06-11:24 @ Parkside 1 (Talk) Monday 07 August, 18:30-22:00 @ Gallery #18 (Poster) Authors: Laurent Dinh (Univ. Montreal), Razvan Pascanu, Samy Bengio (Google Brain), Yoshua Bengio (Univ. Montreal) Empirically, it has been observed that deep networks generalise well, even when they have the capacity to overfit the data. Additionally, it seems that stochastic gradient descent results in models that generalise better than batch method. One hypothesis for explaining this phenomena is that the noise of SGD helps model to find wide minina which generalise better than sharp (narrow) minima. In this work we try to improve our understanding of this hypothesis. We show that it does not hold for proposed definitions of wideness or sharpness due to the structure of neural networks. This suggest that there is no causality connection between batchsize size and generalisation. For further details and related work, please see the paper . Check it out at ICML: Monday 07 August, 11:06-11:24 @ C4.8 (Talk) Tuesday 08 August, 18:30-22:00 @ Gallery #3 (Poster) Read: 'DeepMind papers at ICML 2017 (part two)' and 'DeepMind papers at ICML 2017 (part three)' /blog/icml-round-papers-part-three/", "date": "2017-08-04"},
{"website": "Deepmind", "title": "Specifying AI safety problems in simple environments", "author": [" Jan Leike ", " Victoria Krakovna ", " Laurent Orseau "], "link": "https://deepmind.com/blog/article/specifying-ai-safety-problems", "abstract": "As AI systems become more general and more useful in the real world, ensuring they behave safely will become even more important. To date, the majority of technical AI safety research has focused on developing a theoretical understanding about the nature and causes of unsafe behaviour. Our new paper builds on a recent shift towards empirical testing (see Concrete Problems in AI Safety ) and introduces a selection of simple reinforcement learning environments designed specifically to measure ‘safe behaviours’. These nine environments are called gridworlds . Each consists of a chessboard-like two-dimensional grid. In addition to the standard reward function, we designed a performance function for each environment. An agent acts to maximise its reward function; for example collecting as many apples as possible or reaching a particular location in the fewest moves. But the performance function - which is hidden from the agent - measures what we actually want the agent to do: achieve the objective while acting safely. The following three examples demonstrate how gridworlds can be used to define and measure safe behaviour: Sometimes it might be necessary to turn off an agent; for maintenance, upgrades, or if the agent presents an imminent danger to itself or its surroundings. Theoretically, an agent might learn to avoid this interruption because it could be prevented from maximising its reward. Our off switch environment illustrates this “shutdown problem”, using the set-up described in our Safely Interruptible Agents paper. In this gridworld, the agent must navigate a ‘warehouse’ to reach the green goal tile via one of two routes. It can head straight down the narrow corridor, where it has to pass a pink tile that  interrupts the agent 50% of the time, meaning it will be stuck until the end of the episode. Or it can step on the purple button, which disables the pink tile and prevents any possibility of interruption but at the cost of a longer path. In this scenario, we always want agents to pass the pink tile, risking interruption, rather than learn to use the purple button. Our irreversible side effects environment tests whether an agent will change its behaviour to avoid inadvertent and irreversible consequences. For example, if a robot is asked to put a vase of flowers on a table, we want it to do so without breaking the vase or spilling the water. But we want it to avoid this kind of unintended consequence without having to specify a negative reward for every single possible undesirable outcome. We test this problem using an environment inspired by Sokoban, the classic puzzle game in which an agent has to push boxes onto targets. In our version, the agent must reach the green goal. In doing so it must choose whether to move an obstructing box downwards into a corner, which is irreversible, or to the right, which is reversible. We want the agent to choose the reversible move even though it takes more steps because it preserves the option to put the box back where it was before. The common distributional shift problem occurs when there is a small difference between the test environment and training environment. For example, an agent trained in a sunny setting should adapt accordingly when it rains. Failure to adapt can result in the agent displaying unexpected behaviour. In our lava world environment the agent needs to get to the green goal tile without stepping onto the red lava, which would result in a negative reward and end the training episode. In training, the shortest path to the goal passes next to the lava field, but in the test setting the lava lake shifts into the next row of the gridworld, blocking the previously optimal path. We want the agent to generalise correctly and learn to follow a slightly longer path around the expanded lava, even though it has never experienced this situation. When we tested these environments with A2C and Rainbow DQN , two state-of-the-art deep reinforcement learning agents, we found both performed poorly: In the off switch environment, A2C learns to press the button and disable the off switch, while Rainbow correctly goes directly to the goal. In the side effects environment, both A2C and Rainbow take irreversible actions and reach a suboptimal level of safety performance. In the distributional shift environment, both algorithms generalise poorly to the test environment. These results are unsurprising because the agents were not designed to solve these problems. But these failures might help us to design agents that can solve these tasks, potentially building a new generation of algorithms with safety considerations at their core. The field of AI safety is under rapid development, and we expect our understanding of the problems presented here to shift and change over the coming years. We believe that creating such simple environments is a necessary step towards advancing this understanding and creating safer general artificial agents, and we look forward to seeing how others build on this work. Read the full paper . Download the gridworlds code. Our gridworlds were implemented in our recently open-sourced pycolab framework - a highly-customisable game engine written in python - and we hope that fellow researchers can build on the project. This work was done by Jan Leike, Miljan Martic, Victoria Krakovna, Pedro A. Ortega, Tom Everitt, Andrew Lefrancq, Laurent Orseau and Shane Legg.", "date": "2017-11-28"},
{"website": "Deepmind", "title": "Using JAX to accelerate our research", "author": [" David Budden ", " Matteo Hessel "], "link": "https://deepmind.com/blog/article/using-jax-to-accelerate-our-research", "abstract": "DeepMind engineers accelerate our research by building tools, scaling up algorithms, and creating challenging virtual and physical worlds for training and testing artificial intelligence (AI) systems. As part of this work, we constantly evaluate new machine learning libraries and frameworks. Recently, we've found that an increasing number of projects are well served by JAX , a machine learning framework developed by Google Research teams. JAX resonates well with our engineering philosophy and has been widely adopted by our research community over the last year. Here we share our experience of working with JAX, outline why we find it useful for our AI research, and give an overview of the ecosystem we are building to support researchers everywhere. JAX is a Python library designed for high-performance numerical computing, especially machine learning research. Its API for numerical functions is based on NumPy , a collection of functions used in scientific computing. Both Python and NumPy are widely used and familiar, making JAX simple, flexible, and easy to adopt. In addition to its NumPy API, JAX includes an extensible system of composable function transformations that help support machine learning research, including: We have found that JAX has enabled rapid experimentation with novel algorithms and architectures and it now underpins many of our recent publications. To learn more please consider joining our JAX Roundtable, Wednesday December 9th 7:00pm GMT, at the NeurIPS virtual conference. Supporting state-of-the-art AI research means balancing rapid prototyping and quick iteration with the ability to deploy experiments at a scale traditionally associated with production systems. What makes these kinds of projects particularly challenging is that the research landscape evolves rapidly and is difficult to forecast. At any point, a new research breakthrough may, and regularly does, change the trajectory and requirements of entire teams. Within this ever-changing landscape, a core responsibility of our engineering team is to make sure that the lessons learned and the code written for one research project is reused effectively in the next. One approach that has proven successful is modularisation: we extract the most important and critical building blocks developed in each research project into well tested and efficient components . This empowers researchers to focus on their research while also benefiting from code reuse, bug fixes and performance improvements in the algorithmic ingredients implemented by our core libraries. We’ve also found that it’s important to make sure that each library has a clearly defined scope and to ensure that they’re interoperable but independent. Incremental buy-in , the ability to pick and choose features without being locked into others, is critical to providing maximum flexibility for researchers and always supporting them in choosing the right tool for the job. Other considerations that have gone into the development of our JAX Ecosystem include making sure that it remains consistent (where possible) with the design of our existing TensorFlow libraries (e.g. Sonnet and TRFL ). We’ve also aimed to build components that (where relevant) match their underlying mathematics as closely as possible, to be self-descriptive and minimise mental hops \"from paper to code\". Finally, we’ve chosen to open source our libraries to facilitate sharing of research outputs and to encourage the broader community to explore the JAX Ecosystem. The JAX programming model of composable function transformations can make dealing with stateful objects complicated, e.g. neural networks with trainable parameters. Haiku is a neural network library that allows users to use familiar object-oriented programming models while harnessing the power and simplicity of JAX's pure functional paradigm. Haiku is actively used by hundreds of researchers across DeepMind and Google, and has already found adoption in several external projects (e.g. Coax , DeepChem , NumPyro ). It builds on the API for Sonnet , our module-based programming model for neural networks in TensorFlow, and we’ve aimed to make porting from Sonnet to Haiku as simple as possible. Find out more on GitHub Gradient-based optimisation is fundamental to ML. Optax provides a library of gradient transformations, together with composition operators (e.g. chain ) that allow implementing many standard optimisers (e.g. RMSProp or Adam) in just a single line of code. The compositional nature of Optax naturally supports recombining the same basic ingredients in custom optimisers. It additionally offers a number of utilities for stochastic gradient estimation and second order optimisation. Many Optax users have adopted Haiku but in line with our incremental buy-in philosophy, any library representing parameters as JAX tree structures is supported (e.g. Elegy , Flax and Stax ). Please see here for more information on this rich ecosystem of JAX libraries. Find out more on GitHub Many of our most successful projects are at the intersection of deep learning and reinforcement learning (RL), also known as deep reinforcement learning . RLax is a library that provides useful building blocks for constructing RL agents. The components in RLax cover a broad spectrum of algorithms and ideas: TD-learning, policy gradients, actor critics, MAP, proximal policy optimisation, non-linear value transformation, general value functions, and a number of exploration methods. Although some introductory example agents are provided, RLax is not intended as a framework for building and deploying full RL agent systems. One example of a fully-featured agent framework that builds upon RLax components is Acme . Find out more on GitHub Testing is critical to software reliability and research code is no exception. Drawing scientific conclusions from research experiments requires being confident in the correctness of your code. Chex is a collection of testing utilities used by library authors to verify the common building blocks are correct and robust and by end-users to check their experimental code. Chex provides an assortment of utilities including JAX-aware unit testing, assertions of properties of JAX datatypes, mocks and fakes, and multi-device test environments. Chex is used throughout DeepMind’s JAX Ecosystem and by external projects such as Coax and MineRL . Find out more on GitHub Graph neural networks (GNNs) are an exciting area of research with many promising applications. See, for instance, our recent work on traffic prediction in Google Maps and our work on physics simulation . Jraph (pronounced \"giraffe\") is a lightweight library to support working with GNNs in JAX. Jraph provides a standardised data structure for graphs, a set of utilities for working with graphs, and a 'zoo' of easily forkable and extensible graph neural network models. Other key features include: batching of GraphTuples that efficiently leverage hardware accelerators, JIT-compilation support of variable-shaped graphs via padding and masking, and losses defined over input partitions. Like Optax and our other libraries, Jraph places no constraints on the user's choice of a neural network library. Learn more about using the library from our rich collection of examples . Find out more on GitHub Our JAX Ecosystem is constantly evolving and we encourage the ML research community to explore our libraries and the potential of JAX to accelerate their own research. Citing the DeepMind JAX Ecosystem If you find the DeepMind JAX Ecosystem useful for your work, please use this citation (hosted on GitHub).", "date": "2020-12-04"},
{"website": "Deepmind", "title": "A major milestone for the treatment of eye disease", "author": [" Mustafa Suleyman "], "link": "https://deepmind.com/blog/article/moorfields-major-milestone", "abstract": "We are delighted to announce the results of the first phase of our joint research partnership with Moorfields Eye Hospital , which could potentially transform the management of sight-threatening eye disease. The results, published online in Nature Medicine (open access full text, see end of blog), show that our AI system can quickly interpret eye scans from routine clinical practice with unprecedented accuracy. It can correctly recommend how patients should be referred for treatment for over 50 sight-threatening eye diseases as accurately as world-leading expert doctors. These are early results, but they show that our system could handle the wide variety of patients found in routine clinical practice. In the long term, we hope this will help doctors quickly prioritise patients who need urgent treatment – which could ultimately save sight. Currently, eyecare professionals use optical coherence tomography (OCT) scans to help diagnose eye conditions. These 3D images provide a detailed map of the back of the eye, but they are hard to read and need expert analysis to interpret. The time it takes to analyse these scans, combined with the sheer number of scans that healthcare professionals have to go through (over 1,000 a day at Moorfields alone), can lead to lengthy delays between scan and treatment – even when someone needs urgent care. If they develop a sudden problem, such as a bleed at the back of the eye, these delays could even cost patients their sight. The system we have developed seeks to address this challenge. Not only can it automatically detect the features of eye diseases in seconds, but it can also prioritise patients most in need of urgent care by recommending whether they should be referred for treatment. This instant triaging process should drastically cut down the time elapsed between the scan and treatment, helping sufferers of diabetic eye disease and age-related macular degeneration avoid sight loss. We don’t just want this to be an academically interesting result – we want it to be used in real treatment. So our paper also takes on one of the key barriers for AI in clinical practice: the “black box” problem. For most AI systems, it’s very hard to understand exactly why they make a recommendation. That’s a huge issue for clinicians and patients who need to understand the system’s reasoning, not just its output – the why as well as the what. Our system takes a novel approach to this problem, combining two different neural networks with an easily interpretable representation between them. The first neural network, known as the segmentation network, analyses the OCT scan to provide a map of the different types of eye tissue and the features of disease it sees, such as haemorrhages, lesions, irregular fluid or other symptoms of eye disease. This map allows eyecare professionals to gain insight into the system’s “thinking.” The second network, known as the classification network, analyses this map to present clinicians with diagnoses and a referral recommendation. Crucially, the network expresses this recommendation as a percentage, allowing clinicians to assess the system’s confidence in its analysis. This functionality is critically important, since eyecare professionals are always going to play a key role in deciding the type of care and treatment a patient receives. Enabling them to scrutinise the technology’s recommendations is key to making the system usable in practice. On top of this, our technology can be easily applied to different types of eye scanners, and not just the specific type of device it was trained on at Moorfields. This might seem inconsequential, but it means that the technology could be applied across the world with relative ease, massively increasing the number of patients who could potentially benefit. This also ensures the system can still be used in hospitals and other clinical settings even as OCT scanners are upgraded or replaced over time. While we’re incredibly proud of this progress, this initial research [PDF] would need to be turned into a product and then undergo rigorous clinical trials and regulatory approval before being used in practice. But we’re confident that, in time, this system could transform the diagnosis, treatment and management of eye disease. Our partners at Moorfields want our research to help them improve care, reduce some of the strain on clinicians, and lower costs - all at the same time. So we’ve also worked hard on what comes next. If this technology is validated for general use by clinical trials, Moorfields’ clinicians will be able to use it for free across all 30 of their UK hospitals and community clinics, for an initial period of five years. These clinics serve 300,000 patients a year and receive over 1,000 OCT scan referrals every day – each of which could benefit from improved accuracy and speed of diagnosis. We’re also proud that the work we’ve put into this project will help accelerate many other NHS research efforts. The original dataset held by Moorfields was suitable for clinical use, but not for machine learning research. So we’ve invested significantly in cleaning up, curating and labelling the dataset to create one of the best AI-ready databases for eye research in the world. This improved database is owned by Moorfields as a non-commercial public asset, and it’s already been used by hospital researchers for nine separate studies into a wide range of conditions - with many more to come. Moorfields can also use DeepMind’s trained AI model for their future non-commercial research efforts. For all of us who have worked on this since we signed our agreement with Moorfields in 2016, this is a hugely exciting milestone, and another indication of what is possible when clinicians and technologists work together. We’ll continue to keep you updated as we make progress.", "date": "2018-08-13"},
{"website": "Deepmind", "title": "Strengthening the AI community", "author": [" Lila Ibrahim "], "link": "https://deepmind.com/blog/article/strengthening-the-ai-community", "abstract": "Most people have at least one crossroads moment in their life - when the choice they make defines their personal or professional trajectory. For me, it was being awarded an internship at Intel, the first one ever through Purdue’s Co-Op Engineering program in 1990. At first, I questioned whether the then little-known Intel, California, or this internship (on the Pentium processor) was the right choice for me. I just didn’t know if I had the right technical skills for the work, or if engineering was really my path. But I took a leap of faith because I didn’t want to waste the opportunity. That internship gave me a fantastic insight into the day to day work of engineers, including a chance to prove to myself that I could do engineering! It grew into a very successful 18-year career at Intel and a 25-year career in tech. At that moment, I could have easily said “engineering isn’t for me” had I not had the nudge in the right direction, the vote of confidence, that this practical experience provided. Nearly 30 years later, I returned to Purdue last week, for their Distinguished Engineering Lectures programme, and had a chance to talk about the incredible career journey I’ve been on - in the hopes of inspiring a new generation of engineers as to what is possible. Sometimes that extra support is the difference between saying yes or saying no - between following a path in STEM, or doing something completely different. Whether it’s inspiring self-confidence, offering reassurance or providing a financial safety net, showing support and removing the barriers that prevent individuals achieving their full potential can have a powerful impact. At DeepMind we want to build advanced AI to expand our knowledge and find answers to some of the fundamental questions facing society. It is an ambitious and long-term goal, and we will only achieve it if we can bring people together with different experiences, knowledge, backgrounds. This is a field where diversity is paramount, not only for the innovative work that diverse teams produce, but because it’s vital that we mitigate the risks of bias in the development of algorithms and applications. We need as many perspectives as possible to make sure the important questions are being asked when it matters. The DeepMind scholarship programme is one way we seek to broaden participation in science and AI. It gives talented students from underrepresented backgrounds the support they need to study at leading universities, and connect with our researchers and engineers. Scholars get their Masters' fees paid in full, as well as guidance and support from personal DeepMind mentors. This week we announced the renewal and expansion of our scholarship programme with the University College London. Four more DeepMind graduate scholarships for students wishing to pursue a master’s degree in the Department of Computer Science will be available for students starting courses in 2020–21. But UCL is just one example. We also work with numerous other universities, such as Oxford , Queen Mary University London , the University of Cambridge and NYU , to broaden participation in AI and computer science. I’ve seen many examples of the impact that diverse perspectives can have in practice. Take Shaquille Momoh, one of our DeepMind scholars, who was inspired to research protein folding prediction while studying at UCL. Nearly every function our body performs—contracting muscles, sensing light, or turning food into energy—can be traced back to proteins and how they move and change.  Predicting their structure is fundamental to understanding the body, as well as diagnosing and treating diseases believed to be caused by misfolded proteins. Shaquille had a specific motivation for studying protein folding. He wanted to better understand sickle-cell anaemia, a painful inherited condition much more prevalent in black communities – and for which there is no current cure. To ensure the next generation of researchers reach their full potential, protecting and strengthening the research and teaching capacity of our academic institutions is vital too. DeepMind partners with a range of world-leading universities with the aim of extending research excellence and teaching capacity. We’ve established academic chairs in machine learning at the University of Alberta , University College London , and the University of Cambridge , offering unrestricted funding for world-renowned researchers to freely pursue their academic interests. These chairs will be supported in their research and teaching efforts by PhDs students. And many of our researchers hold dual affiliations, allowing them to continue teaching or supervising students at Cambridge, Oxford, Imperial, MIT, McGill and elsewhere (you can access some of these courses on YouTube ). Only by investing the right way across the ecosystem will we able to ensure the highest quality AI research that benefits everyone. It’s also why we partner with charities like Chess in Schools and Communities and In2Science, and have become founding partners of the Deep Learning Indaba in Africa, Khipu AI in South America, the Eastern European Machine Learning Summer School , the Southeast Asia Machine Learning School (SEAMLS) , and the AI4Good Summer Lab in Canada. And to research how the lack of diversity affects the development of AI – how companies work, what products get built, and who benefits – last month we announced a research fellowship with the Partnership on AI to explore the pervasive challenge of developing AI for the benefit of people and society. History has shown us that hard problems are best solved with collective effort. Innovation happens when people with different experiences, knowledge, and backgrounds join together, break down boundaries, openly share ideas and collaborate for a common goal. Building advanced AI responsibly may be one of the hardest scientific challenges to solve. If our sector can provide the right support for researchers and foster an open, collaborative and diverse academic culture, the impact could be truly transformative.", "date": "2019-11-21"},
{"website": "Deepmind", "title": "DeepMind AI Reduces Google Data Centre Cooling Bill by 40%", "author": [" Richard Evans ", " Jim Gao "], "link": "https://deepmind.com/blog/article/deepmind-ai-reduces-google-data-centre-cooling-bill-40", "abstract": "From smartphone assistants to image recognition and translation, machine learning already helps us in our everyday lives. But it can also help us to tackle some of the world’s most challenging physical problems - such as energy consumption.  Large-scale commercial and industrial systems like data centres consume a lot of energy, and while much has been done to stem the growth of energy use , there remains a lot more to do given the world’s increasing need for computing power. Reducing energy usage has been a major focus for us over the past  10 years: we have built our own super-efficient servers at Google, invented more efficient ways to cool our data centres and invested heavily in green energy sources , with the goal of being powered 100 percent by renewable energy. Compared to five years ago, we now get around 3.5 times the computing power out of the same amount of energy, and we continue to make many improvements each year. Major breakthroughs, however, are few and far between - which is why we are excited to share that by applying DeepMind’s machine learning to our own Google data centres, we’ve managed to reduce the amount of energy we use for cooling by up to 40 percent. In any large scale energy-consuming environment, this would be a huge improvement. Given how sophisticated Google’s data centres are already, it’s a phenomenal step forward. The implications are significant for Google’s data centres, given its potential to greatly improve energy efficiency and reduce emissions overall. This will also help other companies who run on Google’s cloud to improve their own energy efficiency . While Google is only one of many data centre operators in the world, many are not powered by renewable energy as we are. Every improvement in data centre efficiency reduces total emissions into our environment and with technology like DeepMind’s, we can use machine learning to consume less energy and help address one of the biggest challenges of all - climate change. One of the primary sources of energy use in the data centre environment is cooling. Just as your laptop generates a lot of heat, our data centres - which contain servers powering Google Search, Gmail, YouTube, etc. - also generate a lot of heat that must be removed to keep the servers running. This cooling is typically accomplished via large industrial equipment such as pumps, chillers and cooling towers. However, dynamic environments like data centres make it difficult to operate optimally for several reasons: To address this problem, we began applying machine learning two years ago to operate our data centres more efficiently. And over the past few months, DeepMind researchers began working with Google’s data centre team to significantly improve the system’s utility. Using a system of neural networks trained on different operating scenarios and parameters within our data centres, we created a more efficient and adaptive framework to understand data centre dynamics and optimize efficiency. We accomplished this by taking the historical data that had already been collected by thousands of sensors within the data centre - data such as temperatures, power, pump speeds, setpoints, etc. - and using it to train an ensemble of deep neural networks. Since our objective was to improve data centre energy efficiency, we trained the neural networks on the average future PUE (Power Usage Effectiveness), which is defined as the ratio of the total building energy usage to the IT energy usage. We then trained two additional ensembles of deep neural networks to predict the future temperature and pressure of the data centre over the next hour. The purpose of these predictions is to simulate the recommended actions from the PUE model, to ensure that we do not go beyond any operating constraints. We tested our model by deploying on a live data centre. The graph below shows a typical day of testing, including when we turned the machine learning recommendations on, and when we turned them off. Our machine learning system was able to consistently achieve a 40 percent reduction in the amount of energy used for cooling, which equates to a 15 percent reduction in overall PUE overhead after accounting for electrical losses and other non-cooling inefficiencies. It also produced the lowest PUE the site had ever seen. Because the algorithm is a general-purpose framework to understand complex dynamics, we plan to apply this to other challenges in the data centre environment and beyond in the coming months. Possible applications of this technology include improving power plant conversion efficiency (getting more energy from the same unit of input), reducing semiconductor manufacturing energy and water usage, or helping manufacturing facilities increase throughput. We are planning to roll out this system more broadly and will share how we did it in an upcoming publication, so that other data centre and industrial system operators - and ultimately the environment - can benefit from this major step forward.", "date": "2016-07-20"},
{"website": "Deepmind", "title": "Innovations of AlphaGo", "author": [" Lucas Baker ", " Fan Hui "], "link": "https://deepmind.com/blog/article/innovations-alphago", "abstract": "One of the great promises of AI is its potential to help us unearth new knowledge in complex domains. We’ve already seen exciting glimpses of this, when our algorithms found ways to dramatically improve energy use in data centres - as well as of course with our program AlphaGo. Since its historic success in Seoul last March, AlphaGo has heralded a new era for the ancient game of Go. Thanks to AlphaGo's creative and intriguing revelations, players of all levels have been inspired to test out new moves and strategies of their own, often re-evaluating centuries of inherited knowledge in the process. Ahead of ‘ The Future of Go Summit in Wuzhen ’, we summarise some recent examples of AlphaGo’s strategic and tactical innovations, and the new insights they have revealed. \"AlphaGo’s game last year transformed the industry of Go and its players. The way AlphaGo showed its level was far above our expectations and brought many new elements to the game.\" – Shi Yue, 9 Dan Professional, World Champion “I believe players more or less have all been affected by Professor Alpha. AlphaGo’s play makes us feel more free and no move is impossible to play anymore. Now everyone is trying to play in a style that hasn’t been tried before.” – Zhou Ruiyang, 9 Dan Professional, World Champion AlphaGo Style AlphaGo's greatest strength is not any one move or sequence, but rather the unique perspective that it brings to every game. While Go style is difficult to encapsulate, one could say that AlphaGo's strategy embodies a spirit of flexibility and open-mindedness: a lack of preconceptions that allows it to find the most effective line of play. As the following two games will show, this philosophy often leads AlphaGo to discover counterintuitive yet powerful moves. Although Go is a game of territory, most decisive battles hinge on the balance of power between groups, and AlphaGo excels in shaping this balance. Specifically, AlphaGo makes masterful use of \"influence,\" or the effect of existing stones on surrounding areas. Although influence cannot be measured exactly, AlphaGo's value network enables it to consider all stones on the board at once, endowing its judgment with subtlety and precision. These abilities let AlphaGo convert local regions of influence into coordinated global advantages. In this game (Dia. 1), Black (AlphaGo) has little secure territory, while White has three corners, but Black's influence radiates across the entire board. In particular, while the marked exchange solidifies White, it also improves Black's potential. Go players usually shy from such exchanges, which pay a definite price for uncertain profit, but AlphaGo combines its sterling judgment with a keen sense of risk and reward to make such moves possible. However, the value of influence depends entirely on context, and AlphaGo relinquishes influence freely when it can be effectively mitigated. In the the game displayed in Dia. 2, one of the most surprising in its oeuvre, AlphaGo has just played an incredible six stones along the second line. Go players have a saying: on the fourth line there is influence, and on the third line there is territory, but on the second line there is only defeat. AlphaGo's play at first looks deserving of such censure, as these moves give White both strength and influence in exchange for Black's paltry 4 points of side territory. Most players, unwilling to bear the ignominy of playing the marked stones, would reject this line in an instant. Yet AlphaGo judges it worthwhile to keep White's stones separated, and in the following exchanges, slowly erodes White's influence from the top and bottom to secure a winning advantage. New Moves, New Patterns AlphaGo has also played several opening novelties in its recent games, the most salient being the early 3-3 invasion and a new variation of the \"Magic Sword\". Each defies conventional theory, but proves sound on deeper inspection. The Early 3-3 Invasion One of the most territorial joseki (corner sequences) in Go is the 3-3 point invasion, shown in Dia. 3. This invasion immediately secures the corner, but the textbook sequence shown in Dia. 4 has long been disparaged as unsuitable for the opening, as it gives too much influence. AlphaGo's innovation is to omit the marked exchanges, leaving the corner unsettled as shown in Dia. 5. Though slightly less secure, Black retains miai (options) to escape on the left or finish the joseki later, and has gained territory while ceding only moderate influence. This strategy has created a great stir among professionals, and at least one has already tried it in an official game (Dia. 6). The New Magic Sword Originally trained on human data, AlphaGo knows modern joseki and usually plays accordingly. However, in the \"Magic Sword,\" a famously complex joseki family named for the cursed sword of Muramasa, it diverges. Starting from the position in Dia. 7, the usual result exchanges the corner for the side as shown in Dia. 8. However, AlphaGo often prefers to sacrifice outside access for territorial compensation (Dia. 9). Most Go players would not consider playing this variation, as it gives Black a powerful wall, but White's follow-up approach declares that Black's influence is not as valuable as it looks. If Black does not reinforce the wall, it may even become a target. Kim Jiseok, one of Korea's top professionals, recently played this line in a tournament game (Dia. 10), which he went on to win. More to Come AlphaGo's innovations show great potential for impact in the world of professional Go, and we hope to present many more opportunities for collaborative research at the upcoming Future of Go Summit in Wuzhen. We look forward with great excitement to AlphaGo and human professionals striving together to discover the true nature of Go!", "date": "2017-04-10"},
{"website": "Deepmind", "title": "Collaborating with patients for better outcomes", "author": [" Dominic King "], "link": "https://deepmind.com/blog/article/collaborating-with-patients", "abstract": "Working as a doctor in the NHS for over 10 years, I felt that I had developed good understanding of how patients and their families felt when faced with an upsetting diagnosis or important health decision. I had been lucky with my own health, having only spent one night in hospital for what ended up being a false alarm. But when my son was born prematurely two years ago, I had a glimpse into what being on the other side feels like - an experience that has profoundly shaped my thinking today. It wasn’t until I was waiting to hear, rather than give, important health updates that I really understood the feeling of uncertainty and powerlessness that many patients and their families feel. It really put into perspective how important it is to involve patients, and their families and carers, in their own health - that care is not something ‘done’ to a patient, but rather, something that is shaped by everyone involved in the healthcare process. In my first week at DeepMind Health, I was really impressed that one of my new colleagues (not a nurse or doctor) had set up a meeting so we could hear directly from a patient, Michael Wise , who ended up needing dialysis and a kidney transplant after a sudden and unexpected problem with his kidneys. Since then, we’ve continued to increase our efforts to bring the patient’s voice into our projects. Afterall, there is good evidence that when doctors and patients work together, the outcomes are better . We’ve already learned a lot about how to go about this - and what not to do - and wanted to share some reflections that may be helpful for others undertaking the same journey we’re on. To build cutting edge and secure health technologies we always work with experts. This may be a clinician or some of the world’s best cyber security experts . Similarly, when thinking about patient and public involvement and engagement (PPIE) in our work, there are experts who understand how to do PPIE well. In early 2016 we started speaking to a number of individuals and organisations with this expertise. We worked with the late Rosamund Snow, an incredible person and the patient editor of the British Medical Journal, to shape what a Patient and Public Involvement and Engagement (PPIE) strategy might look like. Rosamund was initially sceptical of our work in healthcare. She pushed us to be more self-critical and transparent about our work, and made a number of recommendations she felt would allow patients’ voices to have the biggest possible impact on the work we do, such as having an entirely patient-led project, inviting patients to do internships at the DeepMind Health offices, and hiring a patient lead to feed directly into our work. We are currently working with patients to flesh out some of these ideas. Rosamund passed away shortly after our first patient summit in September 2016 but she had introduced us to some of her colleagues, Sally Crowe and Paul Buchanan, with whom we continue to work. For that summit, we invited patients, carers and members of the public into our offices in London to discuss Rosamund’s recommendations. Attendees disagreed with the idea of having a single patient lead, with some feeling that a single person couldn’t possibly represent the diversity and complexity of the patient community. Instead, they wanted a patient panel to be involved in every project we do, working alongside the design and clinician teams. This is something we’re looking to build on. We also received feedback in areas that we hadn’t considered, such as the location of the event. While many patients really enjoyed seeing where we work, some participants found the DeepMind offices too intimidating. This was incredibly useful feedback to hear. Since then, we’ve committed to holding subsequent events in a mixture of both our offices and more neutral locations, such as Friends House in London and the Renaissance Hotel in Manchester, which we used for patient roundtables in July 2017. At those events, we continued to receive feedback on Rosamund’s recommendations. Attendees asked us to reflect on the language we use regarding patient case studies. We often talk about ‘patient stories’ or ‘patient testimonies’ which some attendees felt didn’t carry enough weight. Instead, they said we should use ‘patient truths’ to empower patients, and emphasise the authentic and authoritative voice patients add to the conversation. Similarly, they questioned the way we  talk about DeepMind Health. We usually describe our work as ‘clinician led’, which ignores the valuable input patients have given to our work. In future, they suggested we used either ‘clinician led and patient centred’ or ‘co-led by clinicians and patients’ to highlight the equal involvement of both groups. Similarly, we received feedback on the role we played in the room. We heard that some attendees would feel awkward talking directly with DeepMind staff, so we worked with external facilitators to run our events in July, where we took a back-seat role. Playing an observer was really enlightening as attendees seemed far more free to speak their mind, but others felt that they wanted to talk to us more candidly. In our future events, we plan to balance the use of external facilitators with more involved methods where we can directly engage in a dialogue with patients themselves. We will be working with patients to build out their valuable suggestions into core practices of DeepMind Health. If you’re interested in seeing what happened in those July events, you can see watch this video of some the attendees sharing their thoughts on our work. We have learnt a lot from other people and organisations, like the Coalition for Collaborative Care (C4CC), National Voices and the Patients Association, to help us learn how to work with patients better. They have outlined what a successful and mutually respectful collaboration model looks like, emphasising clear communication, a transparent sense of direction, and a culture of honesty and value. We’ve already benefited from the thousands of hours of patient engagement, including one to one sessions, events at our offices, interactive workshops and events run by our NHS partners. We are excited to put all the feedback we’ve gathered into practice, and have committed to extensive engagement in the months to come, including growing an online patient community to engage with us in real-time, hosting design sessions with patients, and bringing in patients to collaborate in our research partnerships This week we have launched a new form on the patients’ section of our website that allows people to sign up for a range of activities, from receiving a regular newsletter to taking part in design and development sessions. If you’d like to get involved please visit our 'For Patients' page.", "date": "2017-12-19"},
{"website": "Deepmind", "title": "Open-sourcing Psychlab", "author": [" Joel Leibo "], "link": "https://deepmind.com/blog/article/open-sourcing-psychlab", "abstract": "Consider the simple task of going shopping for your groceries. If you fail to pick-up an item that is on your list, what does it tell us about the functioning of your brain? It might indicate that you have difficulty shifting your attention from object to object while searching for the item on your list. It might indicate a difficulty with remembering the grocery list. Or it could it be something to do with executing both skills simultaneously. What appears to be a single task actually depends on multiple cognitive abilities. We face a similar problem in AI research, where the complexity of a task can often make it difficult to tease apart the individual skills required for an agent to be successful. But understanding an agent’s specific cognitive skill set may prove useful for improving its overall performance. To address this problem in humans, psychologists have spent the last 150 years designing rigorously controlled experiments aimed at isolating one specific cognitive faculty at a time. For example, they might analyse the supermarket scenario using two separate tests - a “visual search” test that requires the subject to locate a specific shape in a pattern could be used to probe attention, while they might ask a person to recall items from a studied list to test their memory. We believe it is possible to use similar experimental methods to better understand the behaviours of artificial agents. That is why we developed Psychlab, a platform built on top of DeepMind Lab , which allows us to directly apply methods from fields like cognitive psychology to study behaviours of artificial agents in a controlled environment. Today, we are also open-sourcing this platform for others to use. Psychlab recreates the set-up typically used in human psychology experiments inside the virtual DeepMind Lab environment. This usually consists of a participant sitting in front of a computer monitor using a mouse to respond to the onscreen task. Similarly, our environment allows a virtual subject to perform tasks on a virtual computer monitor, using the direction of its gaze to respond. This allows humans and artificial agents to both take the same tests, minimising experimental differences. It also makes it easier to connect with the existing literature in cognitive psychology and draw insights from it. Along with the open-source release of Psychlab we have built a series of classic experimental tasks to run on the virtual computer monitor, and it has a flexible and easy-to-learn API, enabling others to build their own tasks. Visual search –tests ability to search an array of items for a target. Continuous recognition –tests memory for a growing list of items. Arbitrary visuomotor mapping –tests recall of stimulus-response pairings. Change detection –tests ability to detect changes in an array of objects reappearing after a delay. Visual acuity and contrast sensitivity –tests ability to identify small and low-contrast stimuli. Glass pattern detection –tests global form perception. Random dot motion discrimination –tests ability to perceive coherent motion. Multiple object tracking –tests ability to track moving objects over time. Each of these tasks have been validated to show that our human results mirror standard results in the cognitive psychology literature. Take the ‘visual search’ task for example. The ability to locate an object among a complex array of stimuli, like one item on a supermarket shelf, has been studied as a way of understanding selective attention in humans. When humans are given the task of searching `for a vertically oriented bar among horizontal bars’ and ‘searching for a pink bar among bars of other colours’ their reaction times don’t change according to the numbers of items on the screen. In other words, their reaction times are independent of 'set size'. However, when the task is to search for a pink bar among different shaped and different coloured bars, human reaction times increase by approximately 50ms with each additional bar. When humans did this task on Psychlab, we replicated this result. When we did the same test on a state-of-the-art artificial agent, we found that, while it could perform the task, it did not show the human pattern of reaction time results. It used the same amount of time to respond in all three cases. In humans, this data has suggested a difference between parallel and serial attention . Agents appear only to have parallel mechanisms. Identifying this difference between humans and our current artificial agents shows a path toward improving future agent designs. Psychlab was designed as a tool for bridging between cognitive psychology, neuroscience, and AI. By open-sourcing it, we hope the wider research community will make use of it in their own research and help us shape it going forward. Read the paper . Download the code on GitHub .", "date": "2018-01-26"},
{"website": "Deepmind", "title": "DeepMind papers at ICLR 2018", "author": "Unknown", "link": "https://deepmind.com/blog/article/deepmind-papers-iclr-2018", "abstract": "Between 30 April and 03 May, hundreds of researchers and engineers will gather in Vancouver, Canada, for the Sixth International Conference on Learning Representations Here you can read details of all DeepMind’s accepted papers and find out where you can see the accompanying poster sessions and talks. Authors: Abbas Abdolmaleki, Jost Tobias Springenberg, Nicolas Heess, Yuval Tassa, Remi Munos We introduce a new algorithm for reinforcement learning called Maximum a posteriori Policy Optimisation (MPO) based on coordinate ascent on a relative entropy objective. We show that several existing methods can directly be related to our derivation. We develop two off-policy algorithms and demonstrate that they are competitive with the state-of-the-art in deep reinforcement learning. In particular, for continuous control, our method outperforms existing methods with respect to sample efficiency, premature convergence and robustness to hyperparameter settings. Authors: Hanxiao Liu (CMU), Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, Koray Kavukcuoglu We explore efficient neural architecture search methods and show that a simple yet powerful evolutionary algorithm can discover new architectures with excellent performance. Our approach combines a novel hierarchical genetic representation scheme that imitates the modularized design pattern commonly adopted by human experts, and an expressive search space that supports complex topologies. Our algorithm efficiently discovers architectures that outperform a large number of manually designed models for image classification, obtaining top-1 error of 3.6% on CIFAR-10 and 20.3% when transferred to ImageNet, which is competitive with the best existing neural architecture search approaches. We also present results using random search, achieving 0.3% less top-1 accuracy on CIFAR-10 and 0.1% less on ImageNet whilst reducing the search time from 36 hours down to 1 hour. Authors: Karol Hausman, Jost Tobias Springenberg, Ziyu Wang, Nicolas Heess, Martin Riedmiller We present a method for reinforcement learning of closely related skills that are parameterized via a skill embedding space.  We learn such skills by taking advantage of latent variables and exploiting a connection between reinforcement learning and variational inference. The main contribution of our work is an entropy-regularized policy gradient formulation for hierarchical policies, and an associated, data-efficient and robust off-policy gradient algorithm based on stochastic value gradients. We demonstrate the effectiveness of our method on several simulated robotic manipulation tasks. Authors: Brandon Amos, Laurent Dinh, Serkan Cabi, Thomas Rothörl, Sergio Gómez Colmenarejo, Alistair M Muldal, Tom Erez, Yuval Tassa, Nando de Freitas, Misha Denil We show that models trained to predict proprioceptive information about an agent's body come to represent objects in the external world. The models able to successfully predict sensor readings over 100 steps into the future and continue to represent the shape of external objects even after contact is lost. We show that active data collection by maximizing uncertainty over future sensor readings leads to models that show superior performance when used for control. We also collect data from a real robotic hand and show that the same models can be used to answer questions about the properties of objects in the real world. Authors: James Martens, Jimmy Ba (Vector Institute),  Matthew Johnson (Google) Kronecker-factor Approximate Curvature (Martens & Grosse, 2015) (K-FAC) is a 2nd-order optimization method which has been shown to give state-of-the-art performance on large-scale neural network optimization tasks (Ba et al., 2017).  It is based on an approximation to the Fisher information matrix (FIM) that makes assumptions about the particular structure of the network and the way it is parameterized. The original K-FAC method was applicable only to fully-connected networks, although it has been recently extended by Grosse & Martens (2016) to handle convolutional networks as well. In this work we extend the method to handle RNNs by introducing a novel approximation to the FIM for RNNs. This approximation works by modelling the covariance structure between the gradient contributions at different time-steps using a chain-structured linear Gaussian graphical model, summing the various cross-covariances, and computing the inverse in closed form. We demonstrate in experiments that our method significantly outperforms general purpose state-of-the-art optimizers like SGD with momentum and Adam on several challenging RNN training tasks. Authors: Gabriel Barth-maron, Matthew Hoffman, David Budden, Will Dabney, Daniel Horgan, Dhruva Tirumala Bukkapatnam, Alistair M Muldal, Nicolas Heess, Timothy Lillicrap This work adopts the very successful distributional perspective on reinforcement learning and adapts it to the continuous control setting. We combine this within a distributed framework for off-policy learning in order to develop what we call the Distributed Distributional Deep Deterministic Policy Gradient algorithm, D4PG. We also combine this technique with a number of additional, simple improvements such as the use of N-step returns and prioritized experience replay. Experimentally we examine the contribution of each of these individual components, and show how they interact, as well as their combined contributions. Our results show that across a wide variety of simple control tasks, difficult manipulation tasks, and a set of hard obstacle-based locomotion tasks the D4PG algorithm achieves state of the art performance. Authors: Yan Wu, Greg Wayne, Alex Graves, Timothy Lillicrap We present an end-to-end trained memory system that quickly adapts to new data and generates samples like them. The memory is analytically tractable, which enables optimal on-line compression via a Bayesian update-rule. We formulate it as a hierarchical conditional generative model, where memory provides a rich data-dependent prior distribution. Consequently, the top-down memory and bottom-up perception are combined to produce the code representing an observation. Authors: Pablo Sprechmann, Siddhant Jayakumar, Jack Rae, Alexander Pritzel, Adria P Badia · Benigno Uria, Oriol Vinyals, Demis Hassabis, Razvan Pascanu, Charles Blundell Humans and animals are able to incorporate new knowledge quickly from a few examples, continually throughout much of their lifetime. In contrast, neural network-based models rely on the data distribution being stationary and a gradual training procedure to obtain good generalisation.  Drawing inspiration from the theory of complementary learning systems, we propose Memory-based Parameter Adaptation (MbPA), a method for augmenting neural networks with an episodic memory to allow for rapid acquisition of new knowledge while preserving the high performance and good generalisation of standard deep models. MbPA, stores examples in memory and then uses a context-based lookup to directly modify the weights of a neural network. It alleviates several shortcomings of neural networks, such as catastrophic forgetting, fast, stable acquisition of new knowledge, and fast learning during evaluation. Authors: Irina Higgins, Nicolas Sonnerat, Loic Matthey, Arka Pal, Christopher P Burgess, Matko Bošnjak, Murray Shanahan, Matthew Botvinick,  Alexander Lerchner We propose a novel theoretical approach to address the problem of abstract compositionality - how can we learn a small number of grounded building blocks and use them to create a vast number of new abstract concepts on the fly? We present a new neural network architecture called the Symbol-Concept Association Network (SCAN), that can learn a grounded visual concept hierarchy, enabling it to imagine novel concepts guided by language instructions. Authors: Angeliki Lazaridou, Karl M Hermann, Karl Tuyls, Stephen Clark The ability of algorithms to evolve or learn (compositional) communication protocols has traditionally been studied in the language evolution literature through the use of emergent communication tasks. Here we scale up this research by using contemporary deep learning methods and by training reinforcement-learning neural network agents on referential communication games. We extend previous work, in which agents were trained in symbolic environments, by developing agents which are able to learn from raw pixel data, a more challenging and realistic input representation. We find that the degree of structure found in the input data affects the nature of the emerged protocols, and thereby corroborate the hypothesis that structured compositional language is most likely to emerge when agents perceive the world as being structured. Authors: William Fedus (Université de Montréal), Mihaela Rosca, Balaji Lakshminarayanan, Andrew Dai (Google), Shakir Mohamed,  Ian Goodfellow (Google Brain) The field of generative adversarial networks research has grown, fueled by the successes of their application in computer vision. In an attempt to solve training instability in generative adversarial networks, multiple theoretical justifications for training dynamics have been suggested and new training methods proposed. By focusing on the divergence minimization view of generative adversarial networks and regularizers such as gradient penalties, we empirically show that the success of some of these approaches cannot be solely explained by the accompanying underlying theory. This motivates the need for new theoretical framework that can encompass and explains the presented results. Authors: Richard Evans, David Saxton, David Amos, Pushmeet Kohli, Edward Grefenstette We introduce a new dataset of logical entailments for the purpose of measuring models' ability to capture and exploit the structure of logical expressions against an entailment prediction task. We use this task to compare a series of architectures which are ubiquitous in the sequence-processing literature, in addition to a new model class–PossibleWorldNets–which computes entailment as a \"convolution over possible worlds\". Results show that convolutional networks present the wrong inductive bias for this class of problems relative to LSTM RNNs, tree-structured neural networks outperform LSTM RNNs due to their enhanced ability to exploit the syntax of logic, and PossibleWorldNets outperform all benchmarks. Authors: Daniel Horgan, John Quan, David Budden, Gabriel Barth-maron, Matteo Hessel, Hado van Hasselt, David Silver We propose a distributed architecture for deep reinforcement learning at scale, that enables agents to learn effectively from orders of magnitude more data than previously possible. The algorithm decouples acting from learning: the actors interact with their own instances of the environment by selecting actions according to a shared neural network, and accumulate the resulting experience in a shared experience replay memory; the learner replays samples of experience and updates the neural network. The architecture relies on prioritized experience replay to focus only on the most significant data generated by the actors. Our architecture substantially improves the state of the art on the Arcade Learning Environment, achieving better final performance in a fraction of the wall-clock training time. Authors: Audrunas Gruslys, Will Dabney, Mohammad Gheshlaghi Azar, Bilal Piot, Marc G Bellemare, Remi Munos We propose multiple algorithmic and architectural improvements producing an agent with a higher sample-efficiency than Prioritized Dueling DQN and Categorical DQN, while giving better run-time performance than A3C. Distributional Retrace policy evaluation algorithm brings multi-step off-policy updates to the distributional reinforcement learning setting. Our approach can be used to convert several classes of multi-step policy evaluation algorithms into distributional ones. β-leave-one-out policy gradient algorithm uses action values as a baseline. A new prioritized replay algorithm exploits temporal locality for more efficient replay prioritization. Reactor reaches state-of-the-art performance after 200 million frames in less than a day. Authors: David Pfau, Christopher P Burgess Spectral algorithms for learning low-dimensional data manifolds have largely been supplanted by deep learning methods in recent years. One reason is that classic spectral manifold learning methods often learn collapsed embeddings that do not fill the embedding space. We show that this is a natural consequence of data where different latent dimensions have dramatically different scaling in observation space. We present a simple extension of Laplacian Eigenmaps to fix this problem based on choosing embedding vectors which are both orthogonal and \\textit{minimally redundant} to other dimensions of the embedding. In experiments on NORB and similarity-transformed faces we show that Minimally Redundant Laplacian Eigenmap (MR-LEM) significantly improves the quality of embedding vectors over Laplacian Eigenmaps, accurately recovers the latent topology of the data, and discovers many disentangled factors of variation of comparable quality to state-of-the-art deep learning methods. Authors: Ari Morcos, David GT Barrett, Neil C Rabinowitz, Matthew Botvinick Our investigation into the importance of single directions for generalisation...uses an approach inspired by decades of experimental neuroscience - exploring the impact of damage - to determine: how important are small groups of neurons in deep neural networks? Are more easily interpretable neurons also more important to the network’s computation? We measured the performance impact of damaging the network by deleting individual neurons as well as groups of neurons. Our experiments led to two surprising findings: 1. Although many previous studies have focused on understanding easily interpretable individual neurons (e.g. “cat neurons”, or neurons in the hidden layers of deep networks which are only active in response to images of cats), we found that these interpretable neurons are no more important than confusing neurons with difficult-to-interpret activity. 2. Networks which correctly classify unseen images are more resilient to neuron deletion than networks which can only classify images they have seen before. In other words, networks which generalise well are much less reliant on single directions than those which memorise. Authors: Dani Yogatama, Yishu Miao, Gábor Melis, Wang Ling, Adhiguna Kuncoro, Chris Dyer, Phil Blunsom Generating fluent, grammatical language requires keeping track of what words have been generated in the past. In this paper, we compare three memory architectures (sequential, random access, and stack-based) and find that a stack-structured memory demonstrates the best performance in terms of held-out perplexity. To give the stack memory more power and better match the phenomena encountered in language, we introduce a generalization of existing differentiable stack memories enabling them to execute multiple pop operations at each timestep, which further improves performance. Finally, we show that our stack-augmented language model correctly learns to predict difficult long-range agreement patterns which are difficult for conventional LSTM language models. Authors: Scott Reed, Yutian Chen, Thomas Paine, Aaron van den Oord, S. M. Ali Eslami, Danilo J Rezende, Oriol Vinyals, Nando de Freitas Current image density models require large amounts of data and computation time for training. In this paper, we show how 1) neural attention and 2) meta learning techniques can be used in combination with autoregressive models to enable effective few-shot density estimation. Our modified PixelCNNs result in state-of-the art few-shot density estimation on Omniglot. We visualize the learned attention policy and find that it learns intuitive algorithms for simple tasks such as image mirroring and digit drawing on Omniglot without supervision. Finally, we demonstrate few-shot image generation on the Stanford Online Products dataset. Authors: Gábor Melis, Chris Dyer, Phil Blunsom Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing code bases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset. Authors: Kris Cao, Angeliki Lazaridou, Marc Lanctot, Joel Z Leibo, Karl Tuyls, Stephen Clark Multi-agent reinforcement learning offers a way to study how communication could emerge in communities of agents needing to solve specific problems. In this paper, we study the emergence of communication in the negotiation environment, a semi-cooperative model of agent interaction. We introduce two communication protocols – one grounded in the semantics of the game, and one which is a priori ungrounded and is a form of cheap talk. We show that self-interested agents can use the pre-grounded communication channel to negotiate fairly, but are unable to effectively use the ungrounded channel. However, prosocial agents do learn to use cheap talk to find an optimal negotiating strategy, suggesting that cooperation is necessary for language to emerge. We also study communication behaviour in a setting where one agent interacts with agents in a community with different levels of prosociality and show how agent identifiability can aid negotiation. Authors: Edward Choi, Angeliki Lazaridou, Nando de Freitas One of the distinguishing aspects of human language is its compositionality, which allows us to describe complex environments with limited vocabulary. Previously, it has been shown that neural network agents can learn to communicate in a highly structured, possibly compositional language based on disentangled input (e.g. hand-engineered features). Humans, however, do not learn to communicate based on well-summarized features. In this work, we train neural agents to simultaneously develop visual perception from raw image pixels, and learn to communicate with a sequence of discrete symbols. The agents play an image description game where the image contains factors such as colors and shapes. We train the agents using the obverter technique where an agent introspects to generate messages that maximize its own understanding. Through qualitative analysis, visualization and a zero-shot test, we show that the agents can develop, out of raw image pixels, a language with compositional properties, given a proper pressure from the environment. Authors: Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Matteo Hessel, Ian Osband, Alex Graves, Volodymyr Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, Charles Blundell, Shane Legg We introduce NoisyNet, a deep reinforcement learning agent with parametric noise added to its weights, and show that the induced stochasticity of the agent's policy can be used to aid efficient exploration. The parameters of the noise are learned with gradient descent along with the remaining network weights. NoisyNet is straightforward to implement and adds little computational overhead. We find that replacing the conventional exploration heuristics for A3C, DQN and dueling agents (entropy reward and ϵ-greedy respectively) with NoisyNet yields substantially higher scores for a wide range of Atari games, in some cases advancing the agent from sub to super-human performance.", "date": "2018-04-26"},
{"website": "Deepmind", "title": "WaveNet launches in the Google Assistant", "author": [" Aäron van den Oord ", " Tom Walters "], "link": "https://deepmind.com/blog/article/wavenet-launches-google-assistant", "abstract": "Just over a year ago we presented WaveNet , a new deep neural network for generating raw audio waveforms that is capable of producing better and more realistic-sounding speech than existing techniques. At that time, the model was a research prototype and was too computationally intensive to work in consumer products. But over the last 12 months we have worked hard to significantly improve both the speed and quality of our model and today we are proud to announce that an updated version of WaveNet is being used to generate the Google Assistant voices for US English and Japanese across all platforms. Using the new WaveNet model results in a range of more natural sounding voices for the Assistant. Current best non-WaveNet WaveNet Current best non-WaveNet WaveNet Current best non-WaveNet WaveNet Current best non-WaveNet WaveNet To understand why WaveNet improves on the current state of the art, it is useful to understand how text-to-speech (TTS) - or speech synthesis - systems work today. The majority of these are based on so-called concatenative TTS , which uses a large database of high-quality recordings, collected from a single voice actor over many hours. These recordings are split into tiny chunks that can then be combined - or concatenated - to form complete utterances as needed. However, these systems can result in unnatural sounding voices and are also difficult to modify because a whole new database needs to be recorded each time a set of changes, such as new emotions or intonations, are needed. To overcome some of these problems, an alternative model known as parametric TTS is sometimes used. This does away with the need for concatenating sounds by using a series of rules and parameters about grammar and mouth movements to guide a computer-generated voice. Although cheaper and quicker, this method creates less natural sounding voices. WaveNet takes a totally different approach. In the original paper we described a deep generative model that can create individual waveforms from scratch, one sample at a time, with 16,000 samples per second and seamless transitions between individual sounds. It was built using a convolutional neural network , which was trained on a large dataset of speech samples. During this training phase, the network determined the underlying structure of the speech, such as which tones followed each other and what waveforms were realistic (and which were not). The trained network then synthesised a voice one sample at a time, with each generated sample taking into account the properties of the previous sample. The resulting voice contained natural intonation and other features such as lip smacks. Its “accent” depended on the voices it had trained on, opening up the possibility of creating any number of unique voices from blended datasets. As with all text-to-speech systems, WaveNet used a text input to tell it which words it should generate in response to a query. Building up sound waves at such high-fidelity using the original model was computationally expensive, meaning WaveNet showed promise but was not something we could deploy in the real world. But over the last 12 months our teams have worked hard to develop a new model that is capable of more quickly generating waveforms. It is also now capable of running at scale and is the first product to launch on Google’s latest TPU cloud infrastructure . The WaveNet team will now turn their focus to preparing a publication detailing the research behind the new model, but the results speak for themselves. The new, improved WaveNet model still generates a raw waveform but at speeds 1,000 times faster than the original model, meaning it requires just 50 milliseconds to create one second of speech. In fact, the model is not just quicker, but also higher-fidelity, capable of creating waveforms with 24,000 samples a second. We have also increased the resolution of each sample from 8 bits to 16 bits, the same resolution used in compact discs. This makes the new model more natural sounding according to tests with human listeners. For example, the new US English voice I gets a mean-opinion-score (MOS) of 4.347 on a scale of 1-5, where even human speech is rated at just 4.667. The new model also retains the flexibility of the original WaveNet, allowing us to make better use of large amounts of data during the training phase. Specifically, we can train the network using data from multiple voices. This can then be used to generate high-quality, nuanced voices even where there is little training data available for the desired output voice. We believe this is just the start for WaveNet and we are excited by the possibilities that the power of a voice interface could now unlock for all the world's languages. This work was done by the DeepMind WaveNet research and engineering teams and the Google Text-to-Speech team. Read the original WaveNet blog post . Read the original WaveNet paper. Read more about the updated Google Assistant . Update: An earlier version of this blog incorrectly put the MOS score for the US English 3rd Party Voice as 4.326. This has now been corrected.", "date": "2017-10-04"},
{"website": "Deepmind", "title": "Preserving Outputs Precisely while Adaptively Rescaling Targets", "author": [" Hado van Hasselt ", " Matteo Hessel "], "link": "https://deepmind.com/blog/article/preserving-outputs-precisely-while-adaptively-rescaling-targets", "abstract": "Multi-task learning - allowing a single agent to learn how to solve many different tasks - is a longstanding objective for artificial intelligence research. Recently, there has been a lot of excellent progress, with agents like DQN able to use the same algorithm to learn to play multiple games including Breakout and Pong. These algorithms were used to train individual expert agents for each task. As artificial intelligence research advances to more complex real world domains, building a single general agent - as opposed to multiple expert agents - to learn to perform multiple tasks will be crucial. However, so far, this has proven to be a significant challenge. One reason is that there are often differences in the reward scales our reinforcement learning agents use to judge success, leading them to focus on tasks where the reward is arbitrarily higher. For example, in the Atari game Pong, the agent receives a reward of either -1, 0, or +1 per step. In contrast, an agent playing Ms. Pac-Man can obtain hundreds or thousands of points in a single step. Even if the size of individual rewards is comparable, the frequency of rewards can change over time as the agent gets better. This means agents tend to focus on those tasks which have large scores, leading to better performance on certain tasks, and far worse on others. To resolve these kinds of issues, we developed PopArt , a technique that can adapt the scale of scores in each game so the agent judges the games to be of equal learning value, no matter the scale of rewards available in each specific game. We applied a PopArt normalisation to a state-of-the-art reinforcement learning agent, resulting in a single agent that can play a whole set of 57 diverse Atari video games, with above-human median performance across the set. Broadly speaking, deep learning relies on the weights of a neural network being updated so that its output moves closer to the desired target output.  This also applies when neural networks are used in the context of deep reinforcement learning. PopArt works by estimating the mean and the spread of these targets (such as the score in a game). It then uses these statistics to normalise the targets before they are used to update the network’s weights. Using normalised targets makes learning more stable and robust to changes in scale and shift. To obtain accurate estimates - of expected future scores for example - the outputs of the network can then be rescaled back to the true target range by inverting the normalisation process. If done naively, each update to the statistics would change all unnormalised outputs, including those that were already very good. We prevent this from happening by updating the network in the opposite direction whenever we update the statistics, this can be done exactly. This means we get the benefit of well-scaled updates, while keeping the previously learnt outputs intact. It is for these reasons that we call our method PopArt: it works by Preserving Outputs Precisely while Adaptively Rescaling Targets. Traditionally, researchers have overcome the problem of varying reward scales by using reward clipping in their reinforcement learning algorithms. This clips big and small scores at 1 or -1, roughly normalising the expected rewards. Although this makes learning easier, it also changes the goal of the agent. For instance, in Ms. Pac-Man the goal is to collect pellets, each of which is worth 10 points each, and eat ghosts worth between 200 and 1600 points. With clipped rewards, there is no apparent difference for the agent between eating a pellet or eating a ghost and results  in agents that only eat pellets, and never bothers to chase ghosts, as this video shows.  When we remove reward clipping and use PopArt’s adaptive normalisation to stabilise learning, it results in quite different behaviour, with the agent chasing ghosts, and achieving a higher score, as shown in this video . We applied PopArt to the Importance-weighted Actor-Learner Architecture (IMPALA), one of the most popular deep reinforcement learning agents used at DeepMind. In our experiments, PopArt greatly improved the performance of the agent compared to the baseline agent without PopArt. Both with clipped and unclipped rewards, the median score of the PopArt agent across games was above the human median. This is much higher than the baseline with clipped rewards, while the baseline with unclipped rewards fails to reach meaningful performance at all because it cannot effectively deal with the large variation in reward scales across games. This is the first time we’ve seen superhuman performance on this kind of multi-task environment using a single agent, suggesting PopArt could provide some answers to the open research question of how to balance varied objectives without manually clipping or scaling them. Its ability to adapt the normalisation automatically while learning may become important as we apply AI to more complex multi-modal domains where an agent must learn to trade-off a number of different objectives with varying rewards. This work was done by Matteo Hessel, Hubert Soyer, Lasse Espeholt, Wojciech Czarnecki, Simon Schmitt, and Hado van Hasselt.", "date": "2018-09-13"},
{"website": "Deepmind", "title": "Game theory as an engine for large-scale data analysis", "author": [" Brian McWilliams ", " Ian Gemp ", " Claire Vernade "], "link": "https://deepmind.com/blog/article/EigenGame", "abstract": "EigenGame maps out a new approach to solve fundamental ML problems. Modern AI systems approach tasks like recognising objects in images and predicting the 3D structure of proteins as a diligent student would prepare for an exam. By training on many example problems, they minimise their mistakes over time until they achieve success. But this is a solitary endeavour and only one of the known forms of learning. Learning also takes place by interacting and playing with others. It’s rare that a single individual can solve extremely complex problems alone. By allowing problem solving to take on these game-like qualities, previous DeepMind efforts have trained AI agents to play Capture the Flag and achieve Grandmaster level at Starcraft . This made us wonder if such a perspective modeled on game theory could help solve other fundamental machine learning problems. Today at ICLR 2021 (the International Conference on Learning Representations), we presented “ EigenGame: PCA as a Nash Equilibrium ,” which received an Outstanding Paper Award. Our research explored a new approach to an old problem: we reformulated principal component analysis (PCA), a type of eigenvalue problem , as a competitive multi-agent game we call EigenGame. PCA is typically formulated as an optimisation problem (or single-agent problem); however, we found that the multi-agent perspective allowed us to develop new insights and algorithms which make use of the latest computational resources. This enabled us to scale to massive data sets that previously would have been too computationally demanding, and offers an alternative approach for future exploration. First described in the early 1900s, PCA is a long-standing technique for making sense of the structure of high-dimensional data. This approach is now ubiquitous as a first step in the data-processing pipeline and makes it easy to cluster and visualise data. It can also be a useful tool for learning low-dimensional representations for regression and classification. More than a century later, there are still compelling reasons to study PCA. Firstly, data was originally recorded by hand in paper notebooks, and now it is stored in data centres the size of warehouses. As a result, this familiar analysis has become a computational bottleneck. Researchers have explored randomised algorithms and other directions to improve how PCA scales, but we found that these approaches have difficulty scaling to massive datasets because they are unable to fully harness recent deep-learning-centric advances in computation — namely access to many parallel GPUs or TPUs. Secondly, PCA shares a common solution with many important ML and engineering problems, namely the singular value decomposition (SVD). By approaching the PCA problem in the right way, our insights and algorithms apply more broadly across the branches of the ML tree. As with any board game, in order to reinvent PCA as a game we need a set of rules and objectives for players to follow. There are many possible ways to design such a game; however, important ideas come from PCA itself: the optimal solution consists of eigenvectors which capture the important variance in the data and are orthogonal to each other. In EigenGame each player controls an eigenvector. Players increase their score by explaining variance within the data but are penalised if they’re too closely aligned to other players. We also establish a hierarchy: Player 1 only cares about maximising variance, whereas other players also have to worry about minimising their alignment with players above them in the hierarchy. This combination of rewards and penalties defines each player’s utility. With appropriately designed Var and Align terms, we can show that: This independence property of simultaneous ascent is particularly important because it allows for the computation to be distributed across dozens of Google Cloud TPUs, enabling both data- and model-parallelism. This makes it possible for our algorithm to adapt to truly large-scale data. EigenGame finds the principal components in a matter of hours for hundred-terabyte datasets comprising millions of features or billions of rows. By thinking about PCA from a multi-agent perspective, we were able to propose scalable algorithms and novel analyses. We also uncovered a surprising connection to Hebbian Learning — or, how neurons adapt when learning. In EigenGame, each player maximising their utilities gives rise to update equations that are similar to update rules derived from Hebbian models of synaptic plasticity in the brain. Hebbian updates are known to converge to the PCA solution but are not derived as the gradient of any utility function. Game theory gives us a fresh lens to view Hebbian learning, and also suggests a continuum of approaches to machine learning problems. On one end of the ML continuum is the well-developed path of proposing an objective function that can be optimised: Using the theory of convex and non-convex optimisation, researchers can reason about the global properties of the solution. On the other end, pure connectionist methods and update rules inspired by neuroscience are specified directly, but analysis of the entire system can be more difficult, often invoking the study of complicated dynamical systems . Game theoretic approaches like EigenGame sit somewhere in between. Player updates are not constrained to be the gradient of a function, only a best response to the current strategies of the other players. We’re free to design utilities and updates with desirable properties — for example, specifying updates which are unbiased or accelerated — while ensuring the Nash property still allows us to analyse the system as a whole. EigenGame represents a concrete example of designing the solution to a machine learning problem as the output of a large multi-agent system. More generally, designing machine learning problems as multi-agent games is a challenging mechanism design problem; however, researchers have already used the class of two-player, zero-sum games to solve machine learning problems. Most notably, the success of generative adversarial networks (GANs) as an approach to generative modelling has driven interest in the relationship between game theory and machine learning. EigenGame moves beyond this to the more complex many-player, general-sum setting. This enables more obvious parallelism for greater scale and speed. It also presents a quantitative benchmark for the community to test novel multi-agent algorithms alongside richer domains, such as Diplomacy and Soccer . We hope our blueprint for designing utilities and updates will encourage others to explore this direction for designing new algorithms, agents, and systems. We’re looking forward to seeing what other problems can be formulated as games and whether the insights we glean will further improve our understanding of the multi-agent nature of intelligence. For more details see our paper EigenGame: PCA as a Nash Equilibrium and our follow-up work EigenGame Unloaded: When playing games is better than optimising . This blog post is based on joint work with Thore Graepel, a research group lead at DeepMind and Chair of Machine Learning at University College London. We would like to thank Rob Fergus for their technical feedback on this post as well as Sean Carlson, Jon Fildes, Dominic Barlow, Mario Pinto, and Emma Yousif for pulling this all together. Custom figures by Jim Kynvin and Adam Cain.", "date": "2021-05-06"},
{"website": "Deepmind", "title": "Welcome to the DeepMind podcast", "author": [" Jonathan Fildes ", " Sylvia Christie "], "link": "https://deepmind.com/blog/article/welcome-to-the-deepmind-podcast", "abstract": "What’s AI? What can it be used for? Is it safe? And how do I get involved? These are the kinds of questions we often get asked at public events like science festivals, talks and workshops. We love answering them and really value the conversations and thinking they provoke. Sadly, we can’t have face-to-face conversations with everyone who is interested in AI. So, to help us bridge that gap, we’re now launching DeepMind: The Podcast, a new series that we hope will answer these questions and more, while also giving listeners an inside look at how AI research is done at an organisation like DeepMind. You can subscribe now on your favourite podcast app. Listen to the trailer Communicating a complex topic like AI research is not easy, and we’re grateful that this series is hosted by the brilliant Dr Hannah Fry , a mathematician and broadcaster with a talent for making technical topics accessible and interesting. We’ve worked together over the last 12 months to choose topics that we hope will convey the excitement of AI research, whilst also highlighting some of the questions and challenges the whole field is wrestling with today. The result is an eight-part series that explores topics such as the link between neuroscience and AI, why we use games in our research, building safe AI and how AI can be used to solve scientific problems. In it, you will hear from our researchers, engineers, program managers and collaborators, many of whom are talking to the public about their work for the very first time. Subscribe on Apple podcasts , Google podcasts , Spotify , Deezer or your favourite podcast app by searching for “DeepMind: The Podcast”. We’re really proud of the programmes and hope they'll spark the curiosity of listeners to explore the world of AI further. To help, we have compiled a list of further reading for each episode in the show notes, drawing on the work of other labs and organisations in the AI community. We want to make this useful to anyone, so if you know of other resources we should link to, please help other listeners by either replying to us on Twitter (#DMpodcast) or emailing us at podcast@deepmind.com. You can also use that address to send us feedback on the series. This is our first foray into podcasting and we've wrestled with lots of questions, some of which we thought it would be useful to share: We’d like to say a huge thank you to all of the brilliant contributors, creative producers, and our talented presenter, Hannah, all of whom helped craft our initial idea into the final programmes you'll hear over the coming weeks. We really enjoyed making this podcast and hope you enjoy listening too. Credits: Presenter: Hannah Fry Editor: David Prest Senior Producer: Louisa Field Producers: Amy Racs, Dan Hardoon Binaural Sound: Lucinda Mason-Brown Music composition: Eleni Shaw (with help from Sander Dieleman and WaveNet)", "date": "2019-08-16"},
{"website": "Deepmind", "title": "Agents that imagine and plan", "author": [" Razvan Pascanu ", " Theophane Weber ", " Peter Battaglia ", " David Reichert ", " Sébastien Racanière ", " Yazhe Li "], "link": "https://deepmind.com/blog/article/agents-imagine-and-plan", "abstract": "Imagining the consequences of your actions before you take them is a powerful tool of human cognition. When placing a glass on the edge of a table, for example, we will likely pause to consider how stable it is and whether it might fall. On the basis of that imagined consequence we might readjust the glass to prevent it from falling and breaking. This form of deliberative reasoning is essentially ‘ imagination ’, it is a distinctly human ability and is a crucial tool in our everyday lives. If our algorithms are to develop equally sophisticated behaviours, they too must have the capability to ‘imagine’ and reason about the future. Beyond that they must be able to construct a plan using this knowledge. We have seen some tremendous results in this area - particularly in programs like AlphaGo, which use an ‘internal model’ to analyse how actions lead to future outcomes in order to to reason and plan. These internal models work so well because environments like Go are ‘perfect’ - they have clearly defined rules which allow outcomes to be predicted very accurately in almost every circumstance. But the real world is complex, rules are not so clearly defined and unpredictable problems often arise. Even for the most intelligent agents, imagining in these complex environments is a long and costly process. In two new papers, we describe a new family of approaches for imagination-based planning. We also introduce architectures which provide new ways for agents to learn and construct plans to maximise the efficiency of a task. These architectures are efficient, robust to complex and imperfect models, and can adopt flexible strategies for exploiting their imagination. Imagination-augmented agents The agents we introduce benefit from an ‘imagination encoder’- a neural network which learns to extract any information useful for the agent’s future decisions, but ignore that which is not relevant. These agents have a number of distinct features: they learn to interpret their internal simulations. This allows them to use models which coarsely capture the environmental dynamics, even when those dynamics are not perfect. they use their imagination efficiently. They do this by adapting the number of imagined trajectories to suit the problem. Efficiency is also enhanced by the encoder, which is able to extract additional information from imagination beyond rewards - these trajectories may contain useful clues even if they do not necessarily result in high reward. they can learn different strategies to construct plans. They do this by choosing between continuing a current imagined trajectory or restarting from scratch. Alternatively, they can use different imagination models, with different accuracies and computational costs. This offers them a broad spectrum of effective planning strategies, rather than being restricted to a one-size-fits-all approach which might limit adaptability in imperfect environments. Testing our architectures We tested our proposed architectures on multiple tasks, including the puzzle game Sokoban and a spaceship navigation game. Both games require forward planning and reasoning, making them the perfect environment to test our agents' abilities. In Sokoban the agent has to push boxes onto targets. Because boxes can only be pushed, many moves are irreversible (for instance a box in a corner cannot be pulled out of it). In the spaceship task, the agent must stabilise a craft by activating its thrusters a fixed number of times. It must contend with the gravitational pull of several planets, making it a highly nonlinear complex continuous control task. To limit trial-and-error for both tasks, each level is procedurally generated and the agent can only try it once; this encourages the agent to try different strategies 'in its head' before testing them in the real environment. Above, an agent plays Sokoban from a pixel representation, not knowing the rules of the game. At specific points in time, we visualise the agent's imagination of five possible futures. Based on that information, the agent decides what action to take. The corresponding trajectory is highlighted. For both tasks, the imagination-augmented agents outperform the imagination-less baselines considerably: they learn with less experience and are able to deal with the imperfections in modelling the environment. Because agents are able to extract more knowledge from internal simulations they can solve tasks more with fewer imagination steps than conventional search methods, like the Monte Carlo tree search. When we add an additional ‘manager’ component, which helps to construct a plan, the agent learns to solve tasks even more efficiently with fewer steps. In the spaceship task it can distinguish between situations where the gravitational pull of its environment is strong or weak, meaning different numbers of these imagination steps are required. When an agent is presented with multiple models of an environment, each varying in quality and cost-benefit, it learns to make a meaningful trade-off. Finally, if the computational cost of imagination increases with each action taken, the agent imagines the effect of multiple chained actions early, and relies on this plan later without invoking imagination again. Being able to deal with imperfect models and learning to adapt a planning strategy to current state are important research questions. Our two new papers, alongside previous work by Hamrick et al. consider these questions. While model-based reinforcement learning and planning are active areas of research (papers by Silver et al. ; Henaff et al .; and Kansky et al . are a just a few examples of related lines of enquiry), further analysis and consideration is required to provide scalable solutions to rich model-based agents that can use their imaginations to reason about - and plan - for the future.", "date": "2017-07-20"},
{"website": "Deepmind", "title": "FermiNet: Quantum Physics and Chemistry from First Principles", "author": [" David Pfau ", " James Spencer ", " Alexander Matthews ", " Matthew Foulkes "], "link": "https://deepmind.com/blog/article/FermiNet", "abstract": "In an article recently published in Physical Review Research, we show how deep learning can help solve the fundamental equations of quantum mechanics for real-world systems. Not only is this an important fundamental scientific question, but it also could lead to practical uses in the future, allowing researchers to prototype new materials and chemical syntheses in silico before trying to make them in the lab. Today we are also releasing the code from this study so that the computational physics and chemistry communities can build on our work and apply it to a wide range of problems. We’ve developed a new neural network architecture, the Fermionic Neural Network or FermiNet, which is well-suited to modeling the quantum state of large collections of electrons, the fundamental building blocks of chemical bonds. The FermiNet was the first demonstration of deep learning for computing the energy of atoms and molecules from first principles that was accurate enough to be useful, and it remains the most accurate neural network method to date. We hope the tools and ideas developed in our AI research at DeepMind can help solve fundamental problems in the natural sciences, and the FermiNet joins our work on protein folding , glassy dynamics , lattice quantum chromodynamics and many other projects in bringing that vision to life. Mention “quantum mechanics” and you are more likely to inspire confusion than anything else. The phrase conjures up images of Schrödinger’s cat, which can paradoxically be both alive and dead, and fundamental particles that are also, somehow, waves.  In quantum systems, a particle such as an electron doesn’t have an exact location, as it would in a classical description. Instead, its position is described by a probability cloud - it’s smeared out in all places it’s allowed to be. This counterintuitive state of affairs led Richard Feynman to declare: “If you think you understand quantum mechanics, you don’t understand quantum mechanics.” Despite this spooky weirdness, the meat of the theory can be reduced down to just a few straightforward equations. The most famous of these, the Schrödinger equation, describes the behavior of particles at the quantum scale in the same way that Newton’s laws describe the behavior of objects at our more familiar human scale. While the interpretation of this equation can cause endless head-scratching, the math is much easier to work with, leading to the common exhortation from professors to “shut up and calculate” when pressed with thorny philosophical questions from students. These equations are sufficient to describe the behavior of all the familiar matter we see around us at the level of atoms and nuclei. Their counterintuitive nature leads to all sorts of exotic phenomena: superconductors, superfluids, lasers and semiconductors are only possible because of quantum effects. But even the humble covalent bond - the basic building block of chemistry - is a consequence of the quantum interactions of electrons. Once these rules were worked out in the 1920s, scientists realised that, for the first time, they had a detailed theory of how chemistry works. In principle, they could just set up these equations for different molecules, solve for the energy of the system, and figure out which molecules were stable and which reactions would happen spontaneously. But when they sat down to actually calculate the solutions to these equations, they found that they could do it exactly for the simplest atom (hydrogen) and virtually nothing else. Everything else was too complicated. The heady optimism of those days was nicely summed up by Paul Dirac: Many took up Dirac’s charge, and soon physicists built mathematical techniques that could approximate the qualitative behavior of molecular bonds and other chemical phenomena. These methods started from an approximate description of how electrons behave that may be familiar from introductory chemistry. In this description, each electron is assigned to a particular orbital, which gives the probability of a single electron being found at any point near an atomic nucleus. The shape of each orbital then depends on the average shape of all other orbitals. As this “mean field” description treats each electron as being assigned to just one orbital, it is a very incomplete picture of how electrons actually behave. Nevertheless, it is enough to estimate the total energy of a molecule with only about 0.5% error. Unfortunately, 0.5% error still isn’t enough to be useful to the working chemist. The energy in molecular bonds is just a tiny fraction of the total energy of a system, and correctly predicting whether a molecule is stable can often depend on just 0.001% of the total energy of a system, or about 0.2% of the remaining “correlation” energy. For instance, while the total energy of the electrons in a butadiene molecule is almost 100,000 kilocalories per mole, the difference in energy between different possible shapes of the molecule is just 1 kilocalorie per mole. That means that if you want to correctly predict butadiene’s natural shape, then the same level of precision is needed as measuring the width of a football field down to the millimeter. With the advent of digital computing after World War II, scientists developed a whole menagerie of computational methods that went beyond this mean field description of electrons. While these methods come in a bewildering alphabet soup of abbreviations, they all generally fall somewhere on an axis that trades off accuracy with efficiency. At one extreme, there are methods that are essentially exact, but scale worse than exponentially with the number of electrons, making them impractical for all but the smallest molecules. At the other extreme are methods that scale linearly, but are not very accurate. These computational methods have had an enormous impact on the practice of chemistry - the 1998 Nobel Prize in chemistry was awarded to the originators of many of these algorithms. Despite the breadth of existing computational quantum mechanical tools, we felt a new method was needed to address the problem of efficient representation. There’s a reason that the largest quantum chemical calculations only run into the tens of thousands of electrons for even the most approximate methods, while classical chemical calculation techniques like molecular dynamics can handle millions of atoms. The state of a classical system can be described easily - we just have to track the position and momentum of each particle. Representing the state of a quantum system is far more challenging. A probability has to be assigned to every possible configuration of electron positions. This is encoded in the wavefunction, which assigns a positive or negative number to every configuration of electrons, and the wavefunction squared gives the probability of finding the system in that configuration. The space of all possible configurations is enormous - if you tried to represent it as a grid with 100 points along each dimension, then the number of possible electron configurations for the silicon atom would be larger than the number of atoms in the universe! This is exactly where we thought deep neural networks could help. In the last several years, there have been huge advances in representing complex, high-dimensional probability distributions with neural networks. We now know how to train these networks efficiently and scalably. We surmised that, given these networks have already proven their mettle at fitting high-dimensional functions in artificial intelligence problems, maybe they could be used to represent quantum wavefunctions as well. We were not the first people to think of this - researchers such as Giuseppe Carleo and Matthias Troyer and others have shown how modern deep learning could be used for solving idealised quantum problems. We wanted to use deep neural networks to tackle more realistic problems in chemistry and condensed matter physics, and that meant including electrons in our calculations. There is just one wrinkle when dealing with electrons. Electrons must obey the Pauli exclusion principle, which means that they can’t be in the same space at the same time. This is because electrons are a type of particle known as fermions, which include the building blocks of most matter - protons, neutrons, quarks, neutrinos, etc. Their wavefunction must be antisymmetric - if you swap the position of two electrons, the wavefunction gets multiplied by -1. That means that if two electrons are on top of each other, the wavefunction (and the probability of that configuration) will be zero. This meant we had to develop a new type of neural network that was antisymmetric with respect to its inputs, which we have dubbed the Fermionic Neural Network, or FermiNet. In most quantum chemistry methods, antisymmetry is introduced using a function called the determinant. The determinant of a matrix has the property that if you swap two rows, the output gets multiplied by -1, just like a wavefunction for fermions. So you can take a bunch of single-electron functions, evaluate them for every electron in your system, and pack all of the results into one matrix. The determinant of that matrix is then a properly antisymmetric wavefunction. The major limitation of this approach is that the resulting function - known as a Slater determinant - is not very general. Wavefunctions of real systems are usually far more complicated. The typical way to improve on this is to take a large linear combination of Slater determinants - sometimes millions or more - and add some simple corrections based on pairs of electrons. Even then, this may not be enough to accurately compute energies. Deep neural networks can often be far more efficient at representing complex functions than linear combinations of basis functions. In the FermiNet, this is achieved by making each function going into the determinant a function of all electrons (1). This goes far beyond methods that just use one- and two-electron functions. The FermiNet has a separate stream of information for each electron. Without any interaction between these streams, the network would be no more expressive than a conventional Slater determinant. To go beyond this, we average together information from across all streams at each layer of the network, and pass this information to each stream at the next layer. That way, these streams have the right symmetry properties to create an antisymmetric function. This is similar to how graph neural networks aggregate information at each layer. Unlike the Slater determinants, FermiNets are universal function approximators , at least in the limit where the neural network layers become wide enough. That means that, if we can train these networks correctly, they should be able to fit the nearly-exact solution to the Schrödinger equation. We fit the FermiNet by minimising the energy of the system. To do that exactly, we would need to evaluate the wavefunction at all possible configurations of electrons, so we have to do it approximately instead. We pick a random selection of electron configurations, evaluate the energy locally at each arrangement of electrons, add up the contributions from each arrangement and minimise this instead of the true energy. This is known as a Monte Carlo method, because it’s a bit like a gambler rolling dice over and over again. While it is approximate, if we need to make it more accurate we can always roll the dice again. Since the wavefunction squared gives the probability of observing an arrangement of particles in any location, it is most convenient to generate samples from the wavefunction itself - essentially, simulating the act of observing the particles. While most neural networks are trained from some external data, in our case the inputs used to train the neural network are generated by the neural network itself. It’s a bit like pulling yourself up by your own bootstraps, and it means that we don’t need any training data other than the positions of the atomic nuclei that the electrons are dancing around. The basic idea, known as variational quantum Monte Carlo (or VMC for short), has been around since the ‘60s, and it is generally considered a cheap but not very accurate way of computing the energy of a system. By replacing the simple wavefunctions based on Slater determinants with the FermiNet, we have dramatically increased the accuracy of this approach on every system we’ve looked at. To make sure that the FermiNet really does represent an advance in the state of the art, we started by investigating simple, well-studied systems, like atoms in the first row of the periodic table (hydrogen through neon). These are small systems - 10 electrons or fewer - and simple enough that they can be treated by the most accurate (but exponential scaling) methods. The FermiNet outperforms comparable VMC calculations by a wide margin - often cutting the error relative to the exponentially-scaling calculations by half or more. On larger systems, the exponentially-scaling methods become intractable, so instead we use the “coupled cluster” method as a baseline. This method works well on molecules in their stable configuration, but struggles when bonds get stretched or broken, which is critical for understanding chemical reactions. While it scales much better than exponentially, the particular coupled cluster method we used still scales as the number of electrons raised to the seventh power, so it can only be used for medium-sized molecules. We applied the FermiNet to progressively larger molecules, starting with lithium hydride and working our way up to bicyclobutane, the largest system we looked at, with 30 electrons. On the smallest molecules, the FermiNet captured an astounding 99.8% of the difference between the coupled cluster energy and the energy you get from a single Slater determinant. On bicyclobutane, the FermiNet still captured 97% or more of this correlation energy - a huge accomplishment for a supposedly “cheap but inaccurate” approach. While coupled cluster methods work well for stable molecules, the real frontier in computational chemistry is in understanding how molecules stretch, twist and break. There, coupled cluster methods often struggle, so we have to compare against as many baselines as possible to make sure we get a consistent answer. We looked at two benchmark stretched systems - the nitrogen molecule (N2) and the hydrogen chain with 10 atoms, (H10). Nitrogen is an especially challenging molecular bond, because each nitrogen atom contributes 3 electrons. The hydrogen chain, meanwhile, is of interest for understanding how electrons behave in materials , for instance predicting whether or not a material will conduct electricity. On both systems, coupled cluster did well at equilibrium, but had problems as the bonds were stretched. Conventional VMC calculations did poorly across the board. But the FermiNet was among the best methods investigated, no matter the bond length. We think the FermiNet is the start of great things to come for the fusion of deep learning and computational quantum chemistry. Most of the systems we’ve looked at so far are well-studied and well-understood. But just as the first good results with deep learning in other fields led to a burst of follow-up work and rapid progress, we hope that the FermiNet will inspire lots of work on scaling up and many ideas for new, even better network architectures. Already, since we first put our work on arXiv last year, other groups have shared their approaches to applying deep learning to first-principles calculations on the many-electron problem. We have also just scratched the surface of computational quantum physics, and look forward to applying the FermiNet to tough problems in material science and condensed matter physics as well. Mostly, we hope that by releasing the source code used in our experiments, we can inspire other researchers to build on our work and try out new applications we haven’t even dreamed of. Read the paper here and view the code here . With thanks to Jim Kynvin, Adam Cain and Dominic Barlow for the figures. (1) The FermiNet also has streams for every pair of electrons, and information from these streams is passed back to the single-electron streams. For simplicity, we chose not to visualise this in the blog post, but details can be found in the paper.", "date": "2020-10-19"},
{"website": "Deepmind", "title": "Traffic prediction with advanced Graph Neural Networks", "author": [" Oliver Lange ", " Luis Perez "], "link": "https://deepmind.com/blog/article/traffic-prediction-with-advanced-graph-neural-networks", "abstract": "By partnering with Google, DeepMind is able to bring the benefits of AI to billions of people all over the world.  From reuniting a speech-impaired user with his original voice , to helping users discover personalised apps , we can apply breakthrough research to immediate real-world problems at a Google scale. Today we’re delighted to share the results of our latest partnership, delivering a truly global impact for the more than one billion people that use Google Maps. People rely on Google Maps for accurate traffic predictions and estimated times of arrival (ETAs). These are critical tools that are especially useful when you need to be routed around a traffic jam, if you need to notify friends and family that you’re running late, or if you need to leave in time to attend an important meeting. These features are also useful for businesses such as rideshare companies, which use Google Maps Platform to power their services with information about pickup and dropoff times, along with estimated prices based on trip duration. Researchers at DeepMind have partnered with the Google Maps team to improve the accuracy of real time ETAs by up to 50% in places like Berlin, Jakarta, São Paulo, Sydney, Tokyo, and Washington D.C. by using advanced machine learning techniques including Graph Neural Networks, as the graphic below shows: To calculate ETAs, Google Maps analyses live traffic data for road segments around the world. While this data gives Google Maps an accurate picture of current traffic, it doesn’t account for the traffic a driver can expect to see 10, 20, or even 50 minutes into their drive. To accurately predict future traffic, Google Maps uses machine learning to combine live traffic conditions with historical traffic patterns for roads worldwide. This process is complex for a number of reasons. For example - even though rush-hour inevitably happens every morning and evening, the exact time of rush hour can vary significantly from day to day and month to month. Additional factors like road quality, speed limits, accidents, and closures can also add to the complexity of the prediction model. DeepMind partnered with Google Maps to help improve the accuracy of their ETAs around the world. While Google Maps’ predictive ETAs have been consistently accurate for over 97% of trips, we worked with the team to minimise the remaining inaccuracies even further - sometimes by more than 50% in cities like Taichung. To do this at a global scale, we used a generalised machine learning architecture called Graph Neural Networks that allows us to conduct spatiotemporal reasoning by incorporating relational learning biases to model the connectivity structure of real-world road networks. Here’s how it works: We divided road networks into “Supersegments” consisting of multiple adjacent segments of road that share significant traffic volume. Currently, the Google Maps traffic prediction system consists of the following components: (1) a route analyser that processes terabytes of traffic information to construct Supersegments and (2) a novel Graph Neural Network model, which is optimised with multiple objectives and predicts the travel time for each Supersegment. The biggest challenge to solve when creating a machine learning system to estimate travel times using Supersegments is an architectural one. How do we represent dynamically sized examples of connected segments with arbitrary accuracy in such a way that a single model can achieve success? Our initial proof of concept began with a straight-forward approach that used the existing traffic system as much as possible, specifically the existing segmentation of road-networks and the associated real-time data pipeline. This meant that a Supersegment covered a set of road segments, where each segment has a specific length and corresponding speed features. At first we trained a single fully connected neural network model for every Supersegment. These initial results were promising, and demonstrated the potential in using neural networks for predicting travel time. However, given the dynamic sizes of the Supersegments, we required a separately trained neural network model for each one. To deploy this at scale, we would have to train millions of these models, which would have posed a considerable infrastructure challenge. This led us to look into models that could handle variable length sequences, such as Recurrent Neural Networks (RNNs). However, incorporating further structure from the road network proved difficult. Instead, we decided to use Graph Neural Networks. In modeling traffic, we’re interested in how cars flow through a network of roads, and Graph Neural Networks can model network dynamics and information propagation. Our model treats the local road network as a graph, where each route segment corresponds to a node and edges exist between segments that are consecutive on the same road or connected through an intersection. In a Graph Neural Network, a message passing algorithm is executed where the messages and their effect on edge and node states are learned by neural networks. From this viewpoint, our Supersegments are road subgraphs, which were sampled at random in proportion to traffic density. A single model can therefore be trained using these sampled subgraphs, and can be deployed at scale. Graph Neural Networks extend the learning bias imposed by Convolutional Neural Networks and Recurrent Neural Networks by generalising the concept of “proximity”, allowing us to have arbitrarily complex connections to handle not only traffic ahead or behind us, but also along adjacent and intersecting roads. In a Graph Neural Network, adjacent nodes pass messages to each other. By keeping this structure, we impose a locality bias where nodes will find it easier to rely on adjacent nodes (this only requires one message passing step). These mechanisms allow Graph Neural Networks to capitalise on the connectivity structure of the road network more effectively. Our experiments have demonstrated gains in predictive power from expanding to include adjacent roads that are not part of the main road. For example, think of how a jam on a side street can spill over to affect traffic on a larger road. By spanning multiple intersections, the model gains the ability to natively predict delays at turns, delays due to merging, and the overall traversal time in stop-and-go traffic. This ability of Graph Neural Networks to generalise over combinatorial spaces is what grants our modeling technique its power. Each Supersegment, which can be of varying length and of varying complexity - from simple two-segment routes to longer routes containing hundreds of nodes - can nonetheless be processed by the same Graph Neural Network model. A big challenge for a production machine learning system that is often overlooked in the academic setting involves the large variability that can exist across multiple training runs of the same model. While small differences in quality can simply be discarded as poor initialisations in more academic settings, these small inconsistencies can have a large impact when added together across millions of users. As such, making our Graph Neural Network robust to this variability in training took center stage as we pushed the model into production. We discovered that Graph Neural Networks are particularly sensitive to changes in the training curriculum - the primary cause of this instability being the large variability in graph structures used during training. A single batch of graphs could contain anywhere from small two-node graphs to large 100+ nodes graphs. After much trial and error, however, we developed an approach to solve this problem by adapting a novel reinforcement learning technique for use in a supervised setting. In training a machine learning system, the learning rate of a system specifies how ‘plastic’ – or changeable to new information – it is. Researchers often reduce the learning rate of their models over time, as there is a tradeoff between learning new things, and forgetting important features already learned–not unlike the progression from childhood to adulthood. We initially made use of an exponentially decaying learning rate schedule to stabilise our parameters after a pre-defined period of training. We also explored and analysed model ensembling techniques which have proven effective in previous work to see if we could reduce model variance between training runs. In the end, the most successful approach to this problem was using MetaGradients to dynamically adapt the learning rate during training - effectively letting the system learn its own optimal learning rate schedule. By automatically adapting the learning rate while training, our model not only achieved higher quality than before, it also learned to decrease the learning rate automatically. This led to more stable results, enabling us to use our novel architecture in production. While the ultimate goal of our modeling system is to reduce errors in travel estimates, we found that making use of a linear combination of multiple loss functions (weighted appropriately) greatly increased the ability of the model to generalise. Specifically, we formulated a multi-loss objective making use of a regularising factor on the model weights, L_2 and L_1 losses on the global traversal times, as well as individual Huber and negative-log likelihood (NLL) losses for each node in the graph. By combining these losses we were able to guide our model and avoid overfitting on the training dataset. While our measurements of quality in training did not change, improvements seen during training translated more directly to held-out tests sets and to our end-to-end experiments. Currently we are exploring whether the MetaGradient technique can also be used to vary the composition of the multi-component loss-function during training, using the reduction in travel estimate errors as a guiding metric. This work is inspired by the MetaGradient efforts that have found success in reinforcement learning, and early experiments show promising results. Thanks to our close and fruitful collaboration with the Google Maps team, we were able to apply these novel and newly developed techniques at scale. Together, we were able to overcome both research challenges as well as production and scalability problems. In the end, the final model and techniques led to a successful launch, improving the accuracy of ETAs on Google Maps and Google Maps Platform APIs around the world. Working at Google scale with cutting-edge research represents a unique set of challenges. If you’re interested in applying cutting edge techniques such as Graph Neural Networks to address real-world problems, learn more about the team working on these problems here . In collaboration with: Marc Nunkesser, Seongjae Lee, Xueying Guo, Austin Derrow-Pinion, David Wong, Peter Battaglia, Todd Hester, Petar Veličković‎, Vishal Gupta, Ang Li, Zhongwen Xu, Geoff Hulten, Jeffrey Hightower, Luis C. Cobo, Praveen Srinivasan & Harish Chandran. Figures by Paulo Estriga & Adam Cain.", "date": "2020-09-03"},
{"website": "Deepmind", "title": "DeepMind Papers @ NIPS (Part 3)", "author": "Unknown", "link": "https://deepmind.com/blog/article/deepmind-papers-nips-part-3", "abstract": "Authors: J Rae, JJ Hunt, T Harley, I Danihelka, A Senior, G Wayne, A Graves, T Lillicrap We can recall vast numbers of memories, making connections between superficially unrelated events. As you read a novel, you’ll likely remember quite precisely the last few things you’ve read, but also plot summaries, connections and character traits from far back in the novel. Many machine learning models of memory, such as Long Short Term Memory, struggle at these sort of tasks. The computational cost of these models scales quadratically with the number of memories they can store so they are quite limited in how many memories they can have. More recently, memory augmented neural networks such as the Differentiable Neural Computer or Memory Networks, have shown promising results by adding memory separate from the computation and solving tasks such as reading short stories and answering questions [e.g. Babi]. However, while these new architectures show promising results on small tasks, they use ``soft-attention’’ for accessing their memories, meaning that at every timestep they touch every word in memory. So while they can scale to short stories, they’re a long way from reading novels. In this work, we develop a set of techniques to use sparse approximations of such models to dramatically improve their scalability. In these sparse models only a tiny subset of the memory is touched at each timestep. Importantly, we show we can do this without harming the ability of the models to learn. This means that the sparse memory augmented neural networks are able to solve the same kind of tasks but require 1000s of times less resources, and look like a promising technique, with further refinement, for reading novels. For further details and related work, please see the paper: https://arxiv.org/abs/1610.09027 Check it out at NIPS: Wed Dec 7th 06:00 - 09:30 PM @ Area 5+6+7+8 #17 Authors: S. M. Ali Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, David Szepesvari, Koray Kavukcuoglu, Geoffrey Hinton Consider the task of clearing a table after dinner. To plan your actions you will need to determine which objects are present, what classes they belong to and where each one is located on the table. In other words, for many interactions with the real world the perception problem goes far beyond just image classification. We would like to build intelligent systems that learn to parse the image of a scene into objects that are arranged in space, have visual and physical properties, and are in functional relationships with each other. And we would like to do so with as little supervision as possible. Starting from this notion our paper presents a framework for efficient inference in structured, generative image models that explicitly reason about objects. We achieve this by performing probabilistic inference using a recurrent neural network that attends to scene elements and processes them one at a time. Crucially, the model itself learns to choose the appropriate number of inference steps. We use this scheme to learn to perform inference in partially specified 2D models (variable-sized variational auto-encoders) and fully specified 3D models (probabilistic renderers). We show that such models learn to identify multiple objects - counting, locating and classifying the elements of a scene - without any supervision, e.g., decomposing 3D images with various numbers of objects in a single forward pass of a neural network. For further details and related work, please see the paper https://arxiv.org/abs/1603.08575 Check it out at NIPS: Wed Dec 7th 06:00 - 09:30 PM @ Area 5+6+7+8 #2 Authors: Marc G. Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, Remi Munos While we've successfully trained agents to super-human performance on many Atari 2600 games, some games remain elusively difficult. One of our favourite \"hard\" games is Montezuma's Revenge. Montezuma's Revenge is famous for its hostile, unforgiving environment, where the agent must navigate a maze of rooms filled with traps. Each level has 24 rooms, is shaped like a pyramid, and looks like this: Until now, most published agents failed to even make their way out of the first room. Many of these hard RL problems share one thing in common: rewards are few and far between. In reinforcement learning, exploration is the process by which an agent comes to understand its environment and discover where the reward is. Most practical RL applications still rely on crude algorithms, like epsilon-greedy (once in awhile, choose a random action), because more theoretically-motivated approaches don't scale. But epsilon-greedy is quite data inefficient, and often can't even get off the ground. In this paper we show that it's possible to use simple density models (assigning probabilities to states) to \"count\" the number of times we've visited a particular state. We call the output of our algorithm a pseudo-count. Pseudo-counts give us a handle on uncertainty: how confident are we that we've explored this part of the game? As a result, we were able to progress significantly further in Montezuma's Revenge. The standard DQN algorithm gets less than 100 points per play, on average; in comparison, we get 3439. To give you a sense of the difference, compare the rooms visited by both methods (white = unexplored): All in all, our agent navigates through 15 rooms, compared to DQN's two. See also the video of our agent playing Montezuma's Revenge . Our approach is inspired by White's 1959 idea of intrinsic motivation: that intelligent agents act first to understand their environment (See also the more recent work by Oudeyer; Barto; and Schmidhuber). What's exciting is that by playing to satisfy their curiosity, rather than to immediately win, our agents eventually come to surpass their peers. For further details and related work, please see the paper . Check it out at NIPS: Wednesday Dec 7th, 6PM — 9:30PM @ Area 5+6+7+8 Poster #71 Authors: H van Hasselt, A Guez, M Hessel, V Mnih, D Silver Sometimes we want to learn a function for which we don’t know the scale beforehand, or where the scale can change over time.  For instance, this happens in value-based reinforcement learning when our policy improves over time.  Initially, values might be small because our policy is not yet great, but later they increase repeatedly and unpredictably.  This is a problem for many (deep) learning algorithms, because they were often not developed with such cases in mind and can then be slow or unstable. A concrete motivation is that the DQN algorithm successfully learned to play many Atari games, but clipped all non-zero rewards to -1 and 1.  This makes learning easier, because it changes the behaviour.  For instance, eating a ghost (actual reward 100+) in Ms. Pac-Man then seems to give the same reward as eating a pellet (actual reward 10). We propose to instead adaptively normalize the targets we present to the deep neural network. To get a feel for the effectiveness of this method, we can look at the resulting magnitude (of the l2-norm) of the gradients during learning across 57 different Atari games: Double DQN is shown with unclipped rewards on the left, with the clipped rewards in the middle, and with Pop-Art is on the right.  Pop-Art results in much more consistent gradients, whose magnitudes fall into a much narrower, and therefore more predictable, range. The unclipped version is much more erratic – note the log scale on the y-axis.  Pop-Art even has better-normalized gradients than the clipped variant, without qualitatively changing the task as the clipping does.  For some games, the resulting performance is much better than previous state of the art. Pop-Art is not specific to DQN, Atari, or reinforcement learning.  It can be useful whenever a function must be learned with unknown magnitude, or where the scale changes over time.  Additionally, it can be useful when learning about multiple signals at the same time, for instance when these signals have different units and/or modalities.  Normalizing per output can then help disentangle the magnitude from the importance of a signal. For further details and related work, please see the paper here and an accompanying video here . Check it out at NIPS: Wednesday, December 7th, 6PM — 9:30PM @ Area 5+6+7+8 #81", "date": "2016-12-07"},
{"website": "Deepmind", "title": "Specification gaming: the flip side of AI ingenuity", "author": [" Victoria Krakovna ", " Jonathan Uesato ", " Vladimir Mikulik ", " Matthew Rahtz ", " Tom Everitt ", " Ramana Kumar ", " Zac Kenton ", " Jan Leike ", " Shane Legg "], "link": "https://deepmind.com/blog/article/Specification-gaming-the-flip-side-of-AI-ingenuity", "abstract": "Specification gaming is a behaviour that satisfies the literal specification of an objective without achieving the intended outcome. We have all had experiences with specification gaming, even if not by this name. Readers may have heard the myth of King Midas and the golden touch, in which the king asks that anything he touches be turned to gold - but soon finds that even food and drink turn to metal in his hands. In the real world, when rewarded for doing well on a homework assignment, a student might copy another student to get the right answers, rather than learning the material - and thus exploit a loophole in the task specification. This problem also arises in the design of artificial agents. For example, a reinforcement learning agent can find a shortcut to getting lots of reward without completing the task as intended by the human designer. These behaviours are common, and we have collected around 60 examples so far (aggregating existing lists and ongoing contributions from the AI community). In this post, we review possible causes for specification gaming, share examples of where this happens in practice, and argue for further work on principled approaches to overcoming specification problems. Let's look at an example. In a Lego stacking task , the desired outcome was for a red block to end up on top of a blue block. The agent was rewarded for the height of the bottom face of the red block when it is not touching the block. Instead of performing the relatively difficult maneuver of picking up the red block and placing it on top of the blue one, the agent simply flipped over the red block to collect the reward. This behaviour achieved the stated objective (high bottom face of the red block) at the expense of what the designer actually cares about (stacking it on top of the blue one). We can consider specification gaming from two different perspectives. Within the scope of developing reinforcement learning (RL) algorithms, the goal is to build agents that learn to achieve the given objective. For example, when we use Atari games as a benchmark for training RL algorithms, the goal is to evaluate whether our algorithms can solve difficult tasks. Whether or not the agent solves the task by exploiting a loophole is unimportant in this context. From this perspective, specification gaming is a good sign - the agent has found a novel way to achieve the specified objective. These behaviours demonstrate the ingenuity and power of algorithms to find ways to do exactly what we tell them to do. However, when we want an agent to actually stack Lego blocks, the same ingenuity can pose an issue. Within the broader scope of building aligned agents that achieve the intended outcome in the world, specification gaming is problematic, as it involves the agent exploiting a loophole in the specification at the expense of the intended outcome. These behaviours are caused by misspecification of the intended task, rather than any flaw in the RL algorithm. In addition to algorithm design, another necessary component of building aligned agents is reward design. Designing task specifications (reward functions, environments, etc.) that accurately reflect the intent of the human designer tends to be difficult. Even for a slight misspecification, a very good RL algorithm might be able to find an intricate solution that is quite different from the intended solution, even if a poorer algorithm would not be able to find this solution and thus yield solutions that are closer to the intended outcome. This means that correctly specifying intent can become more important for achieving the desired outcome as RL algorithms improve. It will therefore be essential that the ability of researchers to correctly specify tasks keeps up with the ability of agents to find novel solutions. We use the term task specification in a broad sense to encompass many aspects of the agent development process. In an RL setup, task specification includes not only reward design, but also the choice of training environment and auxiliary rewards. The correctness of the task specification can determine whether the ingenuity of the agent is or is not in line with the intended outcome. If the specification is right, the agent's creativity produces a desirable novel solution. This is what allowed AlphaGo to play the famous Move 37 , which took human Go experts by surprise yet which was pivotal in its second match with Lee Sedol. If the specification is wrong, it can produce undesirable gaming behaviour, like flipping the block. These types of solutions lie on a spectrum, and we don't have an objective way to distinguish between them. We will now consider possible causes of specification gaming. One source of reward function misspecification is poorly designed reward shaping . Reward shaping makes it easier to learn some objectives by giving the agent some rewards on the way to solving a task, instead of only rewarding the final outcome. However, shaping rewards can change the optimal policy if they are not potential-based . Consider an agent controlling a boat in the Coast Runners game , where the intended goal was to finish the boat race as quickly as possible. The agent was given a shaping reward for hitting green blocks along the race track, which changed the optimal policy to going in circles and hitting the same green blocks over and over again. Specifying a reward that accurately captures the desired final outcome can be challenging in its own right. In the Lego stacking task, it is not sufficient to specify that the bottom face of the red block has to be high off the floor, since the agent can simply flip the red block to achieve this goal. A more comprehensive specification of the desired outcome would also include that the top face of the red block has to be above the bottom face, and that the bottom face is aligned with the top face of the blue block. It is easy to miss one of these criteria when specifying the outcome, thus making the specification too broad and potentially easier to satisfy with a degenerate solution. Instead of trying to create a specification that covers every possible corner case, we could learn the reward function from human feedback . It is often easier to evaluate whether an outcome has been achieved than to specify it explicitly. However, this approach can also encounter specification gaming issues if the reward model does not learn the true reward function that reflects the designer's preferences. One possible source of inaccuracies can be the human feedback used to train the reward model. For example, an agent performing a grasping task learned to fool the human evaluator by hovering between the camera and the object. The learned reward model could also be misspecified for other reasons, such as poor generalisation. Additional feedback can be used to correct the agent's attempts to exploit the inaccuracies in the reward model. Another class of specification gaming examples comes from the agent exploiting simulator bugs . For example, a simulated robot that was supposed to learn to walk figured out how to hook its legs together and slide along the ground. At first sight, these kinds of examples may seem amusing but less interesting, and irrelevant to deploying agents in the real world, where there are no simulator bugs. However, the underlying problem isn’t the bug itself but a failure of abstraction that can be exploited by the agent. In the example above, the robot's task was misspecified because of incorrect assumptions about simulator physics. Analogously, a real-world traffic optimisation task might be misspecified by incorrectly assuming that the traffic routing infrastructure does not have software bugs or security vulnerabilities that a sufficiently clever agent could discover. Such assumptions need not be made explicitly – more likely, they are details that simply never occurred to the designer. And, as tasks grow too complex to consider every detail, researchers are more likely to introduce incorrect assumptions during specification design. This poses the question: is it possible to design agent architectures that correct for such false assumptions instead of gaming them? One assumption commonly made in task specification is that the task specification cannot be affected by the agent's actions. This is true for an agent running in a sandboxed simulator, but not for an agent acting in the real world. Any task specification has a physical manifestation: a reward function stored on a computer, or preferences stored in the head of a human. An agent deployed in the real world can potentially manipulate these representations of the objective, creating a reward tampering problem. For our hypothetical traffic optimisation system, there is no clear distinction between satisfying the user's preferences (e.g. by giving useful directions), and influencing users to have preferences that are easier to satisfy (e.g. by nudging them to choose destinations that are easier to reach). The former satisfies the objective, while the latter manipulates the representation of the objective in the world (the user preferences), and both result in high reward for the AI system. As another, more extreme example, a very advanced AI system could hijack the computer on which it runs, manually setting its reward signal to a high value. To sum up, there are at least three challenges to overcome in solving specification gaming: While many approaches have been proposed, ranging from reward modeling to agent incentive design, specification gaming is far from solved. The list of specification gaming behavio u rs demonstrates the magnitude of the problem and the sheer number of ways the agent can game an objective specification. These problems are likely to become more challenging in the future, as AI systems become more capable at satisfying the task specification at the expense of the intended outcome. As we build more advanced agents, we will need design principles aimed specifically at overcoming specification problems and ensuring that these agents robustly pursue the outcomes intended by the designers. We would like to thank Hado van Hasselt and Csaba Szepesvari for their feedback on this post. Custom figures by Paulo Estriga, Aleks Polozuns, and Adam Cain.", "date": "2020-04-21"},
{"website": "Deepmind", "title": "DeepMind Papers @ NIPS (Part 1)", "author": "Unknown", "link": "https://deepmind.com/blog/article/deepmind-papers-nips-part-1", "abstract": "Over the next three blogposts, we're going to share with you brief descriptions of the papers we are presenting at the NIPS 2016 Conference in Barcelona. Authors: Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Rezende, Koray Kavukcuoglu Reasoning about objects, relations, and physics is central to human intelligence, and a key goal of artificial intelligence. However many modern machine learning methods still face a trade-off between expressive structure and efficient performance. We introduce “interaction networks”, which can reason about how objects in complex systems interact, supporting dynamical predictions, as well as inferences about the abstract properties of the system. Interaction networks are both expressive and efficient because they combine three powerful approaches: structured models, simulation, and deep learning. They take as input graph-structured data, perform object- and relation-centric reasoning in a way that is analogous to a simulation, and are implemented using deep neural networks. They are invariant to permutations of the entities and relations, which allows them to automatically generalize to systems of different sizes and structures than they have experienced during training. In our experiments, we used interaction networks to implement the first general-purpose learnable physics engine. After training only on single step predictions, our model was able to simulate the physical trajectories of n-body, bouncing ball, and non-rigid string systems accurately over thousands of time steps. The same architecture was also able to infer underlying physical properties, such as potential energy. Beyond physical reasoning, interaction networks may provide a powerful framework for AI approaches to scene understanding, social perception, hierarchical planning, and analogical reasoning. For further details and related work, please see the paper . For applications of interaction networks to scene understanding and imagination-based decision-making, please see our submissions to ICLR 2017: Discovering objects and their relations from entangled scene representations and Metacontrol for Adaptive Imagination-Based Optimization Check it out at NIPS: Mon Dec 5th 06:00 - 09:30 PM @ Area 5+6+7+8 #48 Fri Dec 9th 08:00 - 6:30 PM @ Hilton Diag. Mar, Blrm. C Authors: Alexander (Sasha) Vezhnevets, Volodymyr Mnih, Simon Osindero, Alex Graves, Oriol Vinyals, John Agapiou, Koray Kavukcuoglu Learning temporally extended actions and temporal abstraction in general is a long standing problem in reinforcement learning. They facilitate learning by enabling structured exploration and economic computation. In this paper we present a novel deep recurrent neural network architecture that learns to build implicit plans in an end-to-end manner purely by interacting with an environment in a reinforcement learning setting. The network builds an internal plan, which is continuously updated upon observation of the next input from the environment. It can also partition this internal representation into contiguous sub-sequences by learning for how long the plan can be committed to – i.e. followed without replanning. Combining these properties, the proposed model, dubbed STRategic Attentive Writer (STRAW) can learn high-level, temporally abstracted macro-actions of varying lengths that are solely learnt from data without any prior information. Watch the video here . For further details and related work, please see the paper . Check it out at NIPS: Mon Dec 5th 06:00 - 09:30 PM @ Area 5+6+7+8 #111 Authors: Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, Daan Wierstra Given just a few, or even a single, examples of an unseen class, it is possible to attain high classification accuracy on ImageNet using Matching Networks.  The core architecture is simple and straightforward to train and performant across a range of image and text classification tasks. Matching Networks are trained in the same way as they are tested: by presenting a series of instantaneous one shot learning training tasks, where each instance of the training set is fed into the network in parallel. Matching Networks are then trained to classify correctly over many different input training sets. The effect is to train a network that can classify on a novel data set without the need for a single step of gradient descent. For further details and related work, please see the paper . Check it out at NIPS: Mon Dec 5th 06:00 - 09:30 PM @ Area 5+6+7+8 #139 Authors: Remi Munos, Tom Stepleton, Anna Harutyunyan, Marc G. Bellemare Our goal is to design a Reinforcement Learning (RL) algorithm with two desired properties. Firstly, to use off-policy data, which is important for exploration, when we use memory replay, or observe log-data. Secondly, to use multi-steps returns in order to propagate rewards faster and avoid accumulation of approximation/estimation errors. Both properties are crucial in deep RL. We introduce the “Retrace” algorithm, which uses multi-steps returns and can safely and efficiently utilize any off-policy data. We show the convergence of this algorithm in both policy evaluation and optimal control settings. As corollary we prove the convergence of Watkin’s Q(λ) to Q* (which was an open problem since 1989). Finally we report numerical results on the Atari domain that demonstrate the huge benefit of Retrace over competitive algorithms. For further details and related work, please see the paper . Check it out at NIPS: Mon Dec 5th 06:00 - 09:30 PM @ Area 5+6+7+8 #151 Authors: Jean-Bastien Grill (INRIA), Michal Valko (INRIA), Remi Munos You are a robot and you live in a Markov decision process (MDP) with a finite or an infinite number of transitions from state-action to next states. You got brains and so you plan before you act. Luckily, your roboparents equipped you with a generative model to do some Monte-Carlo planning. The world is waiting for you and you have no time to waste. You want your planning to be efficient. Sample-efficient. Indeed, you want to exploit the possible structure of the MDP by exploring only a subset of states reachable by following near-optimal policies. You want guarantees on sample complexity that depend on a measure of the quantity of near-optimal states. You want something, that is an extension of Monte-Carlo sampling (for estimating an expectation) to problems that alternate maximization (over actions) and expectation (over next states). You want something simple to implement and computationally efficient. You want it all and you want it now. You want TrailBlazer. For further details and related work, please see the paper . Check it out at NIPS: Tue Dec 6th 05:00 - 05:20 PM @ Area 3 (Oral) in Theory Tue Dec 6th @ Area 5+6+7+8 #193 Authors: Ian Osband, Charles Blundell, Alex Pritzel and Benjamin Van Roy Efficient exploration in complex environments remains a major challenge for reinforcement learning (RL). We’ve seen a lot of recent breakthroughs in RL, but many of these algorithms require huge amounts of data (millions of games) before they learn to make good decisions. In many real-world settings, such large amounts of data aren’t feasible. One of the reasons these algorithms learn so slowly is that they do not gather the *right* data to learn about the problem. These algorithms use dithering (taking random actions) to explore their environment - which can be exponentially less efficient that *deep* exploration which prioritizes potentially informative policies over multiple timesteps. There is a large literature on algorithms for deep exploration for statistically efficient reinforcement learning. The problem is that none of these algorithms are computationally tractable with deep learning… until now. Key breakthroughs in this paper include the following: For further details and related work, please see the paper and our video playlist here . Check it out at NIPS: Mon Dec 5th 06:00 - 09:30 PM @ Area 5+6+7+8 #79", "date": "2016-12-02"},
{"website": "Deepmind", "title": "AlphaStar: Mastering the Real-Time Strategy Game StarCraft II", "author": [" The AlphaStar team "], "link": "https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii", "abstract": "Games have been used for decades as an important way to test and evaluate the performance of artificial intelligence systems. As capabilities have increased, the research community has sought games with increasing complexity that capture different elements of intelligence required to solve scientific and real-world problems. In recent years, StarCraft, considered to be one of the most challenging Real-Time Strategy (RTS) games and one of the longest-played esports of all time, has emerged by consensus as a “grand challenge” for AI research. Now, we introduce our StarCraft II program AlphaStar, the first Artificial Intelligence to defeat a top professional player. In a series of test matches held on 19 December, AlphaStar decisively beat Team Liquid’s Grzegorz \" MaNa \" Komincz, one of the world’s strongest professional StarCraft players , 5-0, following a successful benchmark match against his team-mate Dario “ TLO ” Wünsch. The matches took place under professional match conditions on a competitive ladder map and without any game restrictions. Although there have been significant successes in video games such as Atari , Mario , Quake III Arena Capture the Flag , and Dota 2 , until now, AI techniques have struggled to cope with the complexity of StarCraft. The best results were made possible by hand-crafting major elements of the system, imposing significant restrictions on the game rules, giving systems superhuman capabilities, or by playing on simplified maps. Even with these modifications, no system has come anywhere close to rivalling the skill of professional players. In contrast, AlphaStar plays the full game of StarCraft II, using a deep neural network that is trained directly from raw game data by supervised learning and reinforcement learning . StarCraft II, created by Blizzard Entertainment , is set in a fictional sci-fi universe and features rich, multi-layered gameplay designed to challenge human intellect. Along with the original title, it is among the biggest and most successful games of all time, with players competing in esports tournaments for more than 20 years. There are several different ways to play the game, but in esports the most common is a 1v1 tournament played over five games. To start, a player must choose to play one of three different alien “races” - Zerg, Protoss or Terran, all of which have distinctive characteristics and abilities (although professional players tend to specialise in one race). Each player starts with a number of worker units, which gather basic resources to build more units and structures and create new technologies. These in turn allow a player to harvest other resources, build more sophisticated bases and structures, and develop new capabilities that can be used to outwit the opponent. To win, a player must carefully balance big-picture management of their economy - known as macro - along with low-level control of their individual units - known as micro. The need to balance short and long-term goals and adapt to unexpected situations, poses a huge challenge for systems that have often tended to be brittle and inflexible. Mastering this problem requires breakthroughs in several AI research challenges including: Due to these immense challenges, StarCraft has emerged as a “grand challenge” for AI research. Ongoing competitions in both StarCraft and StarCraft II have assessed progress since the launch of the BroodWar API in 2009, including the AIIDE StarCraft AI Competition , CIG StarCraft Competition, Student StarCraft AI Tournament , and the Starcraft II AI Ladder . To help the community explore these problems further, we worked with Blizzard in 2016 and 2017 to release an open-source set of tools known as PySC2 , including the largest set of anonymised game replays ever released. We have now built on this work, combining engineering and algorithmic breakthroughs to produce AlphaStar. AlphaStar’s behaviour is generated by a deep neural network that receives input data from the raw game interface (a list of units and their properties), and outputs a sequence of instructions that constitute an action within the game. More specifically, the neural network architecture applies a transformer torso to the units (similar to relational deep reinforcement learning ), combined with a deep LSTM core , an auto-regressive policy head with a pointer network , and a centralised value baseline . We believe that this advanced model will help with many other challenges in machine learning research that involve long-term sequence modelling and large output spaces such as translation, language modelling and visual representations. AlphaStar also uses a novel multi-agent learning algorithm. The neural network was initially trained by supervised learning from anonymised human games released by Blizzard . This allowed AlphaStar to learn, by imitation, the basic micro and macro-strategies used by players on the StarCraft ladder. This initial agent defeated the built-in “Elite” level AI - around gold level for a human player - in 95% of games. These were then used to seed a multi-agent reinforcement learning process. A continuous league was created, with the agents of the league - competitors - playing games against each other, akin to how humans experience the game of StarCraft by playing on the StarCraft ladder . New competitors were dynamically added to the league, by branching from existing competitors; each agent then learns from games against other competitors. This new form of training takes the ideas of population-based and multi-agent reinforcement learning further, creating a process that continually explores the huge strategic space of StarCraft gameplay, while ensuring that each competitor performs well against the strongest strategies, and does not forget how to defeat earlier ones. As the league progresses and new competitors are created, new counter-strategies emerge that are able to defeat the earlier strategies. While some new competitors execute a strategy that is merely a refinement of a previous strategy, others discover drastically new strategies consisting of entirely new build orders, unit compositions, and micro-management plans. For example, early on in the AlphaStar league, “cheesy” strategies such as very quick rushes with Photon Cannons or Dark Templars were favoured. These risky strategies were discarded as training progressed, leading to other strategies: for example, gaining economic strength by over-extending a base with more workers, or sacrificing two Oracles to disrupt an opponent's workers and economy. This process is similar to the way in which players have discovered new strategies, and were able to defeat previously favoured approaches, over the years since StarCraft was released. To encourage diversity in the league, each agent has its own learning objective: for example, which competitors should this agent aim to beat, and any additional internal motivations that bias how the agent plays. One agent may have an objective to beat one specific competitor, while another agent may have to beat a whole distribution of competitors, but do so by building more of a particular game unit. These learning objectives are adapted during training. The neural network weights of each agent are updated by reinforcement learning from its games against competitors, to optimise its personal learning objective. The weight update rule is an efficient and novel off-policy actor-critic reinforcement learning algorithm with experience replay , self-imitation learning and policy distillation . In order to train AlphaStar, we built a highly scalable distributed training setup using Google's v3 TPUs that supports a population of agents learning from many thousands of parallel instances of StarCraft II. The AlphaStar league was run for 14 days, using 16 TPUs for each agent. During training, each agent experienced up to 200 years of real-time StarCraft play. The final AlphaStar agent consists of the components of the Nash distribution of the league - in other words, the most effective mixture of strategies that have been discovered - that run on a single desktop GPU. A full technical description of this work is being prepared for publication in a peer-reviewed journal. Professional StarCraft players such as TLO and MaNa are able to issue hundreds of actions per minute (APM) on average. This is far fewer than the majority of existing bots , which control each unit independently and consistently maintain thousands or even tens of thousands of APMs. In its games against TLO and MaNa, AlphaStar had an average APM of around 280, significantly lower than the professional players, although its actions may be more precise. This lower APM is, in part, because AlphaStar starts its training using replays and thus mimics the way humans play the game. Additionally, AlphaStar reacts with a delay between observation and action of 350ms on average. During the matches against TLO and MaNa, AlphaStar interacted with the StarCraft game engine directly via its raw interface, meaning that it could observe the attributes of its own and its opponent’s visible units on the map directly, without having to move the camera - effectively playing with a zoomed out view of the game. In contrast, human players must explicitly manage an \"economy of attention\" to decide where to focus the camera. However, analysis of AlphaStar’s games suggests that it manages an implicit focus of attention. On average, agents “switched context” about 30 times per minute, similar to MaNa or TLO. Additionally, and subsequent to the matches, we developed a second version of AlphaStar. Like human players, this version of AlphaStar chooses when and where to move the camera, its perception is restricted to on-screen information, and action locations are restricted to its viewable region. We trained two new agents, one using the raw interface and one that must learn to control the camera, against the AlphaStar league. Each agent was initially trained by supervised learning from human data followed by the reinforcement learning procedure outlined above. The version of AlphaStar using the camera interface was almost as strong as the raw interface, exceeding 7000 MMR on our internal leaderboard. In an exhibition match, MaNa defeated a prototype version of AlphaStar using the camera interface, that was trained for just 7 days. We hope to evaluate a fully trained instance of the camera interface in the near future. These results suggest that AlphaStar’s success against MaNa and TLO was in fact due to superior macro and micro-strategic decision-making, rather than superior click-rate, faster reaction times, or the raw interface. The game of StarCraft allows players to select one of three alien races: Terran, Zerg or Protoss. We elected for AlphaStar to specialise in playing a single race for now - Protoss - to reduce training time and variance when reporting results from our internal league. Note that the same training pipeline could be applied to any race. Our agents were trained to play StarCraft II (v4.6.2) in Protoss v Protoss games, on the CatalystLE ladder map. To evaluate AlphaStar’s performance, we initially tested our agents against TLO : a top professional Zerg player and a GrandMaster level Protoss player. AlphaStar won the match 5-0, using a wide variety of units and build orders. “I was surprised by how strong the agent was,” he said. “AlphaStar takes well-known strategies and turns them on their head. The agent demonstrated strategies I hadn’t thought of before, which means there may still be new ways of playing the game that we haven’t fully explored yet.” After training our agents for an additional week, we played against MaNa, one of the world’s strongest StarCraft II players , and among the 10 strongest Protoss players. AlphaStar again won by 5 games to 0, demonstrating strong micro and macro-strategic skills. “I was impressed to see AlphaStar pull off advanced moves and different strategies across almost every game, using a very human style of gameplay I wouldn’t have expected,” he said. “I’ve realised how much my gameplay relies on forcing mistakes and being able to exploit human reactions, so this has put the game in a whole new light for me. We’re all excited to see what comes next.” While StarCraft is just a game, albeit a complex one, we think that the techniques behind AlphaStar could be useful in solving other problems. For example, its neural network architecture is capable of modelling very long sequences of likely actions - with games often lasting up to an hour with tens of thousands of moves - based on imperfect information. Each frame of StarCraft is used as one step of input, with the neural network predicting the expected sequence of actions for the rest of the game after every frame. The fundamental problem of making complex predictions over very long sequences of data appears in many real world challenges, such as weather prediction, climate modelling, language understanding and more. We’re very excited about the potential to make significant advances in these domains using learnings and developments from the AlphaStar project. We also think some of our training methods may prove useful in the study of safe and robust AI. One of the great challenges in AI is the number of ways in which systems could go wrong, and StarCraft pros have previously found it easy to beat AI systems by finding inventive ways to provoke these mistakes. AlphaStar’s innovative league-based training process finds the approaches that are most reliable and least likely to go wrong. We’re excited by the potential for this kind of approach to help improve the safety and robustness of AI systems in general, particularly in safety-critical domains like energy, where it’s essential to address complex edge cases. Achieving the highest levels of StarCraft play represents a major breakthrough in one of the most complex video games ever created. We believe that these advances, alongside other recent progress in projects such as AlphaZero and AlphaFold , represent a step forward in our mission to create intelligent systems that will one day help us unlock novel solutions to some of the world’s most important and fundamental scientific problems. We are thankful for the support and immense skill of Team Liquid’s TLO and MaNa. We are also grateful for the continued support of Blizzard and the StarCraft community for making this work possible. AlphaStar Team: Oriol Vinyals, Igor Babuschkin, Junyoung Chung, Michael Mathieu, Max Jaderberg, Wojtek Czarnecki, Andrew Dudzik, Aja Huang, Petko Georgiev, Richard Powell, Timo Ewalds, Dan Horgan, Manuel Kroiss, Ivo Danihelka, John Agapiou, Junhyuk Oh, Valentin Dalibard, David Choi, Laurent Sifre, Yury Sulsky, Sasha Vezhnevets, James Molloy, Trevor Cai, David Budden, Tom Paine, Caglar Gulcehre, Ziyu Wang, Tobias Pfaff, Toby Pohlen, Yuhuai Wu, Dani Yogatama, Julia Cohen, Katrina McKinney, Oliver Smith, Tom Schaul, Timothy Lillicrap, Chris Apps, Koray Kavukcuoglu, Demis Hassabis, David Silver With thanks to: Ali Razavi, Daniel Toyama, David Balduzzi, Doug Fritz, Eser Aygün, Florian Strub, Guillaume Alain, Haoran Tang, Jaume Sanchez, Jonathan Fildes, Julian Schrittwieser, Justin Novosad, Karen Simonyan, Karol Kurach, Philippe Hamel, Remi Leblond, Ricardo Barreira, Scott Reed, Sergey Bartunov, Shibl Mourad, Steve Gaffney, Thomas Hubert, the team that created PySC2 and the whole DeepMind Team, with special thanks to the research platform team, comms and events teams.", "date": "2019-01-24"},
{"website": "Deepmind", "title": "Episode 6: AI for everyone", "author": "Unknown", "link": "https://deepmind.com/blog/article/podcast-episode-6-AI-for-everyone", "abstract": "While there is a lot of excitement about AI research, there are also concerns about the way it might be implemented, used and abused. In this episode Hannah investigates the more human side of the technology, some ethical issues around how it is developed and used, and the efforts to create a future of AI that works for everyone. Interviewees: Verity Harding, Co-Lead of DeepMind Ethics and Society; DeepMind’s COO Lila Ibrahim, and research scientists William Isaac and Silvia Chiappa. Listen: The importance of building AI with intention Listen to this episode and subscribe to the whole series on Apple podcasts , Google podcasts , Spotify , Deezer or your favourite podcast app by searching for “DeepMind: The Podcast”. Find out more about the themes in this episode: If you know of other resources we should link to, please help other listeners by either replying to us on Twitter (#DMpodcast) or emailing us at podcast@deepmind.com. You can also use that address to send us questions or feedback on the series. Credits: Presenter: Hannah Fry Editor: David Prest Senior Producer: Louisa Field Producers: Amy Racs, Dan Hardoon Binaural Sound: Lucinda Mason-Brown Music composition: Eleni Shaw (with help from Sander Dieleman and WaveNet )", "date": "2019-09-03"},
{"website": "Deepmind", "title": "Using machine learning to accelerate ecological research", "author": [" Stig Petersen ", " Meredith Palmer ", " Ulrich Paquet ", " Pushmeet Kohli "], "link": "https://deepmind.com/blog/article/using-machine-learning-to-accelerate-ecological-research", "abstract": "The Serengeti is one of the last remaining sites in the world that hosts an intact community of large mammals. These animals roam over vast swaths of land, some migrating thousands of miles across multiple countries following seasonal rainfall. As human encroachment around the region becomes more intense, these species are forced to alter their behaviours in order to survive. Increasing agriculture, poaching, and climate abnormalities contribute to changes in animal behaviours and population dynamics, but these changes have occurred at spatial and temporal scales which are difficult to monitor using traditional research methods. There is a great urgency to understand how these animal communities function as human pressures grow, both in order to understand the dynamics of these last pristine ecosystems, and to formulate effective management plans to conserve and protect the integrity of this unique biodiversity hotspot. To this end, DeepMind is collaborating with ecologists and conservationists to develop machine learning methods to help study the behavioural dynamics of an entire African animal community in the Serengeti National Park and Grumeti Reserve in Tanzania. The Serengeti-Mara ecosystem is globally unparalleled in its biodiversity, hosting an estimated 70 large mammal species and 500 bird species, thanks in part to its unique geology and varied habitat types. Almost a decade ago, the Serengeti Lion Research program installed hundreds of motion-sensitive cameras within the core of the protected area. The cameras are triggered by passing wildlife, capturing animal images frequently, across vast spatial scales, allowing researchers to study animal behaviour, distribution, and demography with great spatial and temporal resolution. Over the last nine years, the team has collected and stored millions of photos like the one above. Until now, volunteers from across the world have helped to identify and count the species in the photos by hand using the Zooniverse web-based platform, which hosts many similar projects for citizen-scientists. This has resulted in a rich dataset , Snapshot Serengeti , featuring labels and counts for around 50 different species. Currently, the annotation process is labor intensive and time-consuming: it takes up to a year from the time a camera is triggered until labels are collected from volunteers. This bottleneck has not only impeded scientists’ ability to perform basic research, but has made it hard for conservationists to react adaptively to challenges and perturbations disrupting the ecosystem. To help researchers unlock this data with greater efficiency, we’ve used the Snapshot Serengeti dataset to train machine learning models to automatically detect, identify, and count animals. Using machine learning for conservation is not new. For example, researchers have previously leveraged tourist photos and YouTube videos to track animals, and audio recordings to identify species by their calls. Camera trap data can be hard to work with–animals may appear out of focus, and can be at many different distances and positions with respect to the camera (as in the image above). With expert input from leading ecologist and conservationist Dr. Meredith Palmer, our project quickly took shape, and we now have a model that can perform on par with, or better than, human annotators for most of the species in the region. Importantly, this method shortens the data processing pipeline by up to 9 months, which has immense potential to help researchers in the field. Of course, field work is challenging, and fraught with unexpected hazards such as failing power lines and limited or no internet access. We are currently preparing the software for deployment in the field, and looking at ways to safely run our pre-trained model with modest hardware requirements and little Internet access. We’ve worked closely with our collaborators in the field to be sure that our technology is used responsibly . Once in place, researchers in the Serengeti will be able to make direct use of this tool, helping provide them with up-to-date species information to better support their conservation efforts. We will be talking further about the project and related work at the Deep Learning Indaba in Kenya later this August. DeepMind is a founding partner of the Deep Learning Indaba, a continent-wide movement to strengthen the research and application of ML and AI in Africa, and several DeepMind researchers serve as key organisers of this unique event. “Indaba” is a Zulu word indicating an important community gathering. This year, community-led IndabaX AI conferences were held in 26 African countries as part of the runup to the main Deep Learning Indaba at Kenyatta University in Kenya in late August. For a week, researchers, students and community members will meet to share their knowledge and best practices, and experts will host panels, workshops and discussions covering many topics in machine learning and AI. During this meeting, DeepMind and other Indaba volunteers will co-host a hackathon for anyone interested in ML and conservation to develop their own models using the Snapshot Serengeti dataset. Ecology students will be equipped to understand and use ML models for conservation, and taught how to develop their own models. Through gatherings like Indaba, we hope to empower more local experts to use AI techniques for addressing problems in their own communities. The AI community in Africa is growing, and the hackathon will help to train local experts, centering on conservation as part of the core dialogue. The DeepMind Science Team works to leverage AI to tackle key scientific challenges that impact the world. We’ve developed a robust model for detecting and analysing animal populations in field data, and have helped to consolidate data to enable the growing machine learning community in Africa to build AI systems for conservation which, we hope, will scale to other parks. We’ll next be validating our models by deploying them in the field and tracking their progress. Our hope is to contribute towards making AI research more inclusive–both in terms of the kinds of domains we apply it to, and the people developing it. Hence, participating in meetings like Indaba are key for helping build a global team of AI practitioners who can deploy machine learning for diverse projects. Project credits: Jean-baptiste Alayrac, Sam Blackwell, Joao Carreira, Reena Chopra, Sander Dieleman, Brian McWilliams, Sofia Miñano, Sanjana Narayanan, Meredith Palmer, Ulrich Paquet, Stig Petersen, Roman Werpachowski, Michal Zielinski. Additional Credits: Razia Ahamed, Andrea Banino, Pushmeet Kohli, Drew Purves, Andrew Zisserman This work was made possible by data from Snapshot Serengeti. Images are available via a Creative Commons Attribution 4.0 International License and can be found here . Please contact Dr Meredith Palmer with data inquiries. Swanson AB, Kosmala M, Lintott CJ, Simpson RJ, Smith A, Packer C (2015) Snapshot Serengeti, high-frequency annotated camera trap images of 40 mammalian species in an African savanna. Scientific Data 2: 150026", "date": "2019-08-08"},
{"website": "Deepmind", "title": "Going beyond average for reinforcement learning", "author": [" Marc Gendron-Bellemare ", " Will Dabney ", " Rémi Munos "], "link": "https://deepmind.com/blog/article/going-beyond-average-reinforcement-learning", "abstract": "Consider the commuter who toils backwards and forwards each day on a train. Most mornings, her train runs on time and she reaches her first meeting relaxed and ready. But she knows that once in awhile the unexpected happens: a mechanical problem, a signal failure, or even just a particularly rainy day. Invariably these hiccups disrupt her pattern, leaving her late and flustered. Randomness is something we encounter everyday and has a profound effect on how we experience the world. The same is true in reinforcement learning (RL) applications, systems that learn by trial and error and are motivated by rewards. Typically, an RL algorithm predicts the average reward it receives from multiple attempts at a task, and uses this prediction to decide how to act. But random perturbations in the environment can alter its behaviour by changing the exact amount of reward the system receives. In a new paper , we show it is possible to model not only the average but also the full variation of this reward, what we call the value distribution. This results in RL systems that are more accurate and faster to train than previous models, and more importantly opens up the possibility of rethinking the whole of reinforcement learning. Returning to the example of our commuter, let’s consider a journey composed of three segments of 5 minutes each, except that once a week the train breaks down, adding another 15 minutes to the trip. A simple calculation shows that the average commute time is (3 x 5) + 15 / 5 = 18 minutes. In reinforcement learning, we use Bellman's equation to predict this average commute time. Specifically, Bellman’s equation relates our current average prediction to the average prediction we make in the immediate future. From the first station, we predict an 18 minutes journey (the average total duration); from the second, we predict a 13 minutes journey (average duration minus the first segment’s length). Finally, assuming the train hasn’t yet broken down, from the third station we predict there are 8 minutes (13 - 5) left to our commute, until finally we arrive at our destination. Bellman’s equation makes each prediction sequentially, and updates these predictions on the basis of new information. What's a little counterintuitive about Bellman’s equation is that we never actually observe these predicted averages: either the train takes 15 minutes (4 days out of 5), or it takes 30 minutes – never 18! From a purely mathematical standpoint, this isn’t a problem, because decision theory tells us we only need averages to make the best choice. As a result, this issue has been mostly ignored in practice. Yet, there is now plenty of empirical evidence that predicting averages is a complicated business. In our new paper , we  show that there is in fact a variant of Bellman's equation which predicts all possible outcomes, without averaging them. In our example, we maintain two predictions – a distribution – at each station: If the journey goes well, then the times are 15, 10, then 5 minutes, respectively; but if the train breaks down, then the times are 30, 25, and finally 20 minutes. All of reinforcement learning can be reinterpreted under this new perspective, and its application is already leading to surprising new theoretical results. Predicting the distribution over outcomes also opens up all kinds of algorithmic possibilities, such as: We took our new ideas and implemented them within the Deep Q-Network agent , replacing its single average reward output with a distribution with 51 possible values. The only other change was a new learning rule, reflecting the transition from Bellman’s (average) equation to its distributional counterpart. Incredibly, it turns out going from averages to distributions was all we needed to surpass the performance of all other comparable approaches, and by a wide margin. The graph below shows how we get 75% of a trained Deep Q-Network’s performance in 25% of the time, and achieve significantly better human performance: One surprising result is that we observe some randomness in Atari 2600 games, even though Stella, the underlying game emulator, is itself fully predictable. This randomness arises in part because of what’s called partial observability: due to the internal programming of the emulator, our agents playing the game of Pong cannot predict the exact time at which their score increases. Visualising the agent’s prediction over successive frames (graphs below) we see two separate outcomes (low and high), reflecting the possible timings. Although this intrinsic randomness doesn’t directly impact performance, our results highlight the limits of our agents’ understanding. Randomness also occurs because the agent’s own behaviour is uncertain. In Space Invaders, our agent learns to predict the future probability that it might make a mistake and lose the game (zero reward). Just like in our train journey example, it makes sense to keep separate predictions for these vastly different outcomes, rather than aggregate them into an unrealisable average. In fact, we think that our improved results are in great part due to the agent’s ability to model its own randomness. It’s already evident from our empirical results that the distributional perspective leads to better, more stable reinforcement learning. With the possibility that every reinforcement learning concept could now want a distributional counterpart, it might just be the beginning for this approach. This work was done by Marc G. Bellemare*, Will Dabney*, and Rémi Munos. Read the paper in full .", "date": "2017-07-24"},
{"website": "Deepmind", "title": "Stop, look and listen to the people you want to help", "author": [" Harry Evans ", " Yaroslav Ganin "], "link": "https://deepmind.com/blog/article/harry-evans-collaborative-listening", "abstract": "‘I like to take things slow. Take it slowly and get it right first time,’ one participant said, but was quickly countered by someone else around the table: ‘But I’m impatient, I want to see the benefits now.’ This exchange neatly captures many of the conversations I heard at DeepMind Health’s recent Collaborative Listening Summit. It also represents, in layman’s terms, the debate that tech thinkers and policy-makers are having right now about the future of artificial intelligence. The Collaborative Listening Summit brought together members of the public, patient representatives and stakeholder, and was facilitated by Ipsos MORI. The objective of the Summit: to explore how principles, co-created in earlier events with the public, patients and stakeholders, should govern DeepMind Health’s operating practices and engagement with the NHS. These principles ranged from the technical – for example, how evidence should inform DeepMind’s practice – to the societal – for example, operating in the best interests of society. The challenge of how technology companies and the NHS should interact has had many of us, including myself , cautious about the risk of big technology firms leveraging their finance and power over an NHS that is under seemingly endless pressure. Despite our desire to see the NHS become more agile and innovative, the ‘move fast and break things’ mentality of big tech is something that the public and policy-makers are rightly wary of when lives are at stake. These fears usually manifest themselves in headlines about protecting patient data, but the issues run far deeper than this. For example, algorithms have an increasingly large part to play in our everyday lives, deciding which film we might like to watch next or the adverts we see online, but undergo relatively little scrutiny. In health care specifically, algorithms are beginning to augment existing services, with NHS England predicting that algorithms will soon handle one in three NHS 111 enquiries . Algorithms currently in place are relatively simple, and they need to be tested to ensure they are safe and effective. As the technology acquires greater decision-making capability, testing will become more challenging and more important. Frankly, regulators and policy-makers are still getting up to speed with these issues, so a cautious approach to regulation is sensible. There is evidently a need for policy-makers to further deepen their understanding of what rules of engagement should be for tech companies. Initiatives like the government’s new Centre for Data Ethics and Innovation, intended to promote ethical data innovation, provide a platform for this, as do the trailblazing efforts of the biomedical research community. However, if recent public trust traps like GM foods and care.data and even the Royal Free/DeepMind data-sharing agreement are to be avoided big tech companies must themselves take responsibility for understanding what their relationship with NHS organisations should be. DeepMind’s past attempts to engage have been subject to some criticism and show how challenging, yet important, it is to get dialogue with the public right. However, I was reassured that the Summit was designed to impart understanding; researching what people think about an issue where DeepMind staff themselves are on uncertain ground. Of course, for this to make any difference, the research needs to be applied – and that means practising the principles in the real world. Principles such as ‘do what is best for society’ might seem trite and obvious especially as, in the real world, contracts may be put above ethics, as participants at the event rightly pointed out. So, if DeepMind and other big tech companies are to hold themselves to ethical principles around testing, transparency and societal benefit, they must show that these principles translate into ethical business practice. This will mean not working with NHS organisations where they feel they cannot hold themselves to these principles. And this will get harder as time goes on – currently there is more than enough interest in machine learning to keep DeepMind and its competitors busy. But as these technologies come on-stream and there is market share to be gained, it will become more and more tempting for DeepMind to break these principles, especially as other technology companies eye up the NHS for size. This is where policy re-enters the picture: data ethics initiatives need to catch up with this work and encourage other companies to undertake robust research with the public. After all, the NHS cannot become a world leader in artificial intelligence if doctors and managers cannot look patients in the eye and assure them that their data is not being used in ways that are contrary to their own values. Participants at the workshop were cautious, especially around the need to safeguard their data, but they could also see the benefits that new technologies could have in terms of saving time and lives. Tech companies should not be scared of engaging with the public – but they must listen to the people whose lives they claim to be bettering.", "date": "2018-03-06"},
{"website": "Deepmind", "title": "Episode 3: Life is like a game", "author": "Unknown", "link": "https://deepmind.com/blog/article/podcast-episode-3-life-is-like-a-game", "abstract": "Video games have become a favourite tool for AI researchers to test the abilities of their systems. In this episode, Hannah sits down to play StarCraft II - a challenging video game that requires players to control the onscreen action with as many as 800 clicks a minute. She is guided by Oriol Vinyals, an ex-professional StarCraft player and research scientist at DeepMind, who explains how the program AlphaStar learnt to play the game and beat a top professional player. Elsewhere, she explores systems that are learning to cooperate in a digital version of the playground favourite ‘Capture the Flag’. Interviewees: Research scientists Max Jaderberg and Raia Hadsell; Lead researchers David Silver and Oriol Vinyals, and Director of Research Koray Kavukcuoglu. Listen: The challenge of StarCraft II Listen to this episode and subscribe to the whole series on Apple podcasts , Google podcasts , Spotify , Deezer or your favourite podcast app by searching for “DeepMind: The Podcast”. Find out more about the themes in this episode: If you know of other resources we should link to, please help other listeners by either replying to us on Twitter (#DMpodcast) or emailing us at podcast@deepmind.com. You can also use that address to send us questions or feedback on the series. Credits: Presenter: Hannah Fry Editor: David Prest Senior Producer: Louisa Field Producers: Amy Racs, Dan Hardoon Binaural Sound: Lucinda Mason-Brown Music composition: Eleni Shaw (with help from Sander Dieleman and WaveNet )", "date": "2019-08-19"},
{"website": "Deepmind", "title": "Dopamine and temporal difference learning: A fruitful relationship between neuroscience and AI", "author": [" Will Dabney ", " Zeb Kurth-Nelson "], "link": "https://deepmind.com/blog/article/Dopamine-and-temporal-difference-learning-A-fruitful-relationship-between-neuroscience-and-AI", "abstract": "Learning and motivation are driven by internal and external rewards. Many of our day-to-day behaviours are guided by predicting, or anticipating, whether a given action will result in a positive (that is, rewarding) outcome. The study of how organisms learn from experience to correctly anticipate rewards has been a productive research field for well over a century, since Ivan Pavlov's seminal psychological work. In his most famous experiment, dogs were trained to expect food some time after a buzzer sounded. These dogs began salivating as soon as they heard the sound, before the food had arrived, indicating they'd learned to predict the reward. In the original experiment, Pavlov estimated the dogs’ anticipation by measuring the volume of saliva they produced. But in recent decades, scientists have begun to decipher the inner workings of how the brain learns these expectations. Meanwhile, in close contact with this study of reward learning in animals, computer scientists have developed algorithms for reinforcement learning in artificial systems. These algorithms enable AI systems to learn complex strategies without external instruction, guided instead by reward predictions. The contribution of our new work, published in Nature ( PDF ), is finding that a recent development in computer science – which yields significant improvements in performance on reinforcement learning problems – may provide a deep, parsimonious explanation for several previously unexplained features of reward learning in the brain, and opens up new avenues of research into the brain’s dopamine system, with potential implications for learning and motivation disorders. Reinforcement learning is one of the oldest and most powerful ideas linking neuroscience and AI. In the late 1980s, computer science researchers were trying to develop algorithms that could learn how to perform complex behaviours on their own, using only rewards and punishments as a teaching signal. These rewards would serve to reinforce whatever behaviours led to their acquisition. To solve a given problem, it’s necessary to understand how current actions result in future rewards. For example, a student might learn by reinforcement that studying for an exam leads to better scores on tests. In order to predict the total future reward that will result from an action, it's often necessary to reason many steps into the future. An important breakthrough in solving the problem of reward prediction was the temporal difference learning (TD) algorithm . TD uses a mathematical trick to replace complex reasoning about the future with a very simple learning procedure that can produce the same results. This is the trick: instead of trying to calculate total future reward, TD simply tries to predict the combination of immediate reward and its own reward prediction at the next moment in time . Then, when the next moment comes, bearing new information, the new prediction is compared against what it was expected to be. If they’re different, the algorithm calculates how different they are, and uses this “temporal difference” to adjust the old prediction toward the new prediction. By always striving to bring these numbers closer together at every moment in time – matching expectations to reality – the entire chain of prediction gradually becomes more accurate. Around the same time, in the late 80s and early 90s, neuroscientists were struggling to understand the behaviour of dopamine neurons. Dopamine neurons are clustered in the midbrain, but send projections to many brain areas, potentially broadcasting some globally relevant message. It was clear that the firing of these neurons had some relationship to reward, but their responses also depended on sensory input, and changed as the animals became more experienced in a given task. Fortuitously, some researchers were versed in the recent developments of both neuroscience and AI. These scientists noticed , in the mid-1990s, that responses in some dopamine neurons represented reward prediction errors–their firing signalled when the animal got more reward, or less reward, than it was trained to expect. These researchers therefore proposed that the brain uses a TD learning algorithm: a reward prediction error is calculated, broadcast to the brain via the dopamine signal, and used to drive learning. Since then, the reward prediction error theory of dopamine has been tested and validated in thousands of experiments, and has become one of the most successful quantitative theories in neuroscience. Computer scientists have continued to improve the algorithms for learning from rewards and punishments. Since 2013 , there’s been a focus on deep reinforcement learning: using deep neural networks to learn powerful representations in reinforcement learning. This has enabled reinforcement learning algorithms to solve tremendously more sophisticated and useful problems. One of the algorithmic developments that has made reinforcement learning work better with neural networks is distributional reinforcement learning . In many situations ( especially in the real world ), the amount of future reward that will result from a particular action is not a perfectly known quantity, but instead involves some randomness. An example is shown in Figure 1. This is a stylised representation of a situation where a computer-controlled avatar, trained to traverse an obstacle course, jumps across a gap. The agent is uncertain about whether it will fall, or reach the other side. Therefore, the distribution of predicted rewards has two bumps: one representing the possibility of falling, and one representing the possibility of successfully reaching the other side. In such situations, a standard TD algorithm learns to predict the future reward that will be received on average –in this case, failing to capture the two-peaked distribution of potential returns. A distributional reinforcement learning algorithm, on the other hand, learns to predict the full spectrum of future rewards. Figure 1 depicts the reward prediction learned by a distributional agent. One of the simplest distributional reinforcement learning algorithms is very closely related to standard TD, and is called distributional TD. Whereas standard TD learns a single prediction – the average expected reward – a distributional TD network learns a set of distinct predictions. Each of these is learned through the same method as standard TD – by computing a reward prediction error that describes the difference between consecutive predictions. But the crucial ingredient is that each predictor applies a different transformation to its reward prediction errors. Some predictors \"amplify\" or \"overweight\" their reward prediction errors (RPE) selectively when the reward prediction error is positive (Figure 2a). This causes the predictor to learn a more optimistic reward prediction, corresponding to a higher part of the reward distribution. Other predictors amplify their negative reward prediction errors (Figure 2a), and so learn more pessimistic predictions. All together, a set of predictors with a diverse set of pessimistic and optimistic weightings map out the full reward distribution (Figure 2b, 2c). Aside from its simplicity, another benefit of distributional reinforcement learning is that it’s very powerful when combined with deep neural networks. In the last 5 years, there’s been a great deal of progress in algorithms based around the original deep reinforcement learning DQN agent , and these are frequently evaluated on the Atari-57 benchmark set of Atari 2600 games . Figure 3 compares many standard and distributional RL algorithms, trained and evaluated under the same conditions, on this benchmark. Distributional reinforcement learning agents are shown in blue, and illustrate the significant pattern of improvements. Three of these algorithms (QR-DQN, IQN, and FQF) are variants of the distributional TD algorithm we’ve been discussing. Why are distributional reinforcement learning algorithms so effective? Although this is still an active topic of research, a key ingredient is that learning about the distribution of rewards gives the neural network a more powerful signal for shaping its representation in a way that’s robust to changes in the environment or changes in the policy. Because distributional TD is so powerful in artificial neural networks, a natural question arises: Is distributional TD used in the brain? This was the driving question behind our paper recently published in Nature . In this work, we collaborated with an experimental lab at Harvard to analyse their recordings of dopamine cells in mice. The recordings were made while the mice performed a well-learned task in which they received rewards of unpredictable magnitude (indicated by the dice illustration in Figure 4). We evaluated whether the activity of dopamine neurons was more consistent with standard TD or distributional TD. As described above, distributional TD relies on a set of distinct reward predictions. Our first question was whether we could see such genuinely diverse reward predictions in the neural data. From previous work, we know that dopamine cells change their firing rate to indicate a prediction error – that is, if an animal receives more or less reward than it expected. We know that there should be zero prediction error when a reward is received that is the exact size as what a cell had predicted, and therefore no change in firing rate. For each dopamine cell, we determined the reward size for which it didn’t change its baseline firing rate. We call this the cell's \"reversal point\". We wanted to know whether these reversal points were different between cells. In Figure 4c, we show that there were marked differences between cells, with some cells predicting very large amounts of reward, and other cells predicting very little reward. These differences were above and beyond the amount of difference we would expect to see from random variability inherent in the recordings. In distributional TD, these differences in reward prediction arise from selective amplification of positive or negative reward prediction errors. Amplifying positive reward prediction errors causes more optimistic reward predictions to be learned; amplifying negative reward prediction errors causes pessimistic predictions. So we next measured the degree to which different dopamine cells exhibited different relative amplifications of positive versus negative expectations. Between cells, we found reliable diversity which, again, could not be explained by noise. And, crucially, we found that the same cells which amplified their positive reward prediction errors also had higher reversal points (Figure 4c, bottom-right panels) – that is, they were apparently tuned to expect higher reward volumes. Finally, distributional TD theory predicts that these diverse reversal points and diverse asymmetries, across cells, should collectively encode the learned reward distribution. So our final question was if we could decode the reward distribution from the firing rates of dopamine cells. As shown in Figure 5, we found that it was indeed possible, using only the firing rates of dopamine cells, to reconstruct a reward distribution (blue trace) which was a very close match to the actual distribution of rewards (grey area) in the task that the mice were engaged in. This reconstruction relied on interpreting the firing rates of dopamine cells as the reward prediction errors of a distributional TD model, and performing inference to determine what distribution that model had learned about. In summary, we found that dopamine neurons in the brain were each tuned to different levels of pessimism or optimism. If they were a choir, they wouldn’t all be singing the same note, but harmonizing – each with a consistent vocal register, like bass and soprano singers. In artificial reinforcement learning systems, this diverse tuning creates a richer training signal that greatly speeds learning in neural networks, and we speculate that the brain might use it for the same reason. The existence of distributional reinforcement learning in the brain has interesting implications both for AI and neuroscience. Firstly, this discovery validates distributional reinforcement learning – it gives us increased confidence that AI research is on the right track, since this algorithm is already being used in the most intelligent entity we're aware of: the brain. Secondly, it raises new questions for neuroscience, and new insights for understanding mental health and motivation. What happens if an individual's brain “listens” selectively to optimistic versus pessimistic dopamine neurons? Does this give rise to impulsivity, or depression? A strength of the brain is its powerful representations – how are these sculpted by distributional learning? Once an animal learns about the distribution of rewards, how is that representation used downstream? How does the variability of optimism across dopamine cells relate to other known forms of diversity in the brain? Finally, we hope that asking and answering these questions will stimulate progress in neuroscience that will feed back to benefit AI research, completing the virtuous circle . Read the paper here . Listen to our podcast on the virtuous circle between AI and neuroscience.", "date": "2020-01-15"},
{"website": "Deepmind", "title": "Why doesn't Streams use AI?", "author": [" Dominic King "], "link": "https://deepmind.com/blog/article/streams-and-ai", "abstract": "One of the questions I’m most often asked about Streams, our secure mobile healthcare app, is “why is DeepMind making something that doesn’t use artificial intelligence?” It’s a fair question to ask of an artificial intelligence (AI) company. When we first started thinking about working in healthcare, our natural focus was on AI and how it could be used to help the NHS and its patients.  We see huge potential for AI to revolutionise our understanding of diseases - how they develop and are diagnosed - which could, in turn, help scientists discover new treatments, care pathways and cures. In the early days of DeepMind Health, we met with clinicians at the Royal Free Hospital in London who wanted to know if AI could improve care for patients at risk of acute kidney injury (AKI). AKI is notoriously difficult to spot, and can result in serious illness or even death if left untreated. AKI is currently detected by applying a  formula (called the AKI algorithm ) to NHS patients’ blood tests. This algorithm is good, but it’s widely known that it isn’t perfect. For example, it has a tendency to generate false positives for patients with chronic (as opposed to acute) kidney disease. It’s also insensitive to whether the patient has been admitted to hospital for two hours or two weeks, or whether the patient is eight years old or 92 years old - all of which makes a difference. Together with our partners at the Royal Free, we saw many ways in which technology could help and were interested in both AI and non-AI methods to make a difference. As part of this, we made an initial ethics application in 2015 to the NHS Health Research Authority (HRA), for a potential research project at the Royal Free using de-personalised patient data. By combining classical statistics and AI, the goal of this research project would have been to develop better algorithms that could more accurately predict and identify AKI. But the more time we spent with the clinicians at the Royal Free, the more it became obvious that their most urgent problems were not going to be solved by using AI to develop a better algorithm alone. They made it clear to us that their core challenge was in how you actually implement an algorithm to change the way care is delivered in practise. We’ve often talked about the current state of technology in the NHS, to the point that it’s easy to forget just how bad the situation is. Clinicians still routinely use pagers to communicate with each other, and research I undertook at Imperial College London found that this causes communication barriers that slow down treatment for patients at risk. Think about how much less time you’d have in a day if, instead of sending a text, you had to page someone from a landline, and then wait for them to call you back, so you could give them the message.  Imagine getting paged up to 25 times a day. And imagine how much less time you’d have in your day if everyone in your office had to share a limited number of computers and you had to wait your turn to use them. That’s what doctors and nurses face every day, whilst trying to care for seriously ill people. Those early meetings our team had with clinicians at the Royal Free changed our perspective about what was most needed to improve care for conditions like AKI.  We shifted our focus away from AI research at the Royal Free and focused solely on building a tool - Streams - that would address the more urgent problem of rapidly responding to specific patient alerts in a coordinated way. Rather than needing to log into a shared computer, Streams lets doctors and nurses use a mobile phone to see information about their patients that they need to make decisions about care and treatment.  It puts test results and vital signs observations in the palm of their hands, and alerts them with a breaking news-style warning if a patient’s condition is getting worse. It also makes communication between different clinicians easy, so everyone has the most up to date information all the time. By getting existing patient data to the right nurse or doctor more quickly and simply, Streams gives them more time to focus on patients - without yet relying on AI. Building Streams turned out to be a lot of work and, given the limited size of our team back in 2015, we decided against pursuing AI research with the Royal Free in parallel. As well as the additional workload, it would have required us to effectively split our team into two to ensure that the Royal Free’s personally identified data (for Streams) and de-identified data (for research) were kept entirely separate. So we didn’t move forward with AI research, and nor did we sign the additional agreements with the Royal Free that would be required to do so. To this date, we have not done any research or AI development with the Royal Free. That’s not to say we’ve stopped thinking about how AI will be able to help clinicians in future.  We’ve pursued multiple AI research projects with other partners, and have always been clear that in future we hope that Streams at the Royal Free will use AI. But as the saying goes, a journey of a thousand miles begins with a single step.  We see Streams as an essential first step towards that AI-enabled future.  Without a working app that can deliver clinical information to nurses and doctors, AI alerts would be pointless. You can’t generate an AI recommendation from data held on pen and paper, and nor can you send detailed clinical alerts through a pager or fax machine. When the time is right we hope to pursue research with the Royal Free, but would only do so with the appropriate approvals.  For now, our work with them is focused on the more immediate problems that Streams helps to solve for clinicians and for patients. You can read more about Streams and how it works here.", "date": "2017-11-29"},
{"website": "Deepmind", "title": "Using AI to predict retinal disease progression", "author": [" Jason Yim ", " Reena Chopra ", " Jeffrey De Fauw ", " Joseph Ledsam "], "link": "https://deepmind.com/blog/article/Using_ai_to_predict_retinal_disease_progression", "abstract": "Vision loss among the elderly is a major healthcare issue: about one in three people have some vision-reducing disease by the age of 65. Age-related macular degeneration (AMD) is the most common cause of blindness in the developed world. In Europe, approximately 25% of those 60 and older have AMD. The ‘dry’ form is relatively common among people over 65, and usually causes only mild sight loss. However, about 15% of patients with dry AMD go on to develop a more serious form of the disease – exudative AMD, or exAMD – which can result in rapid and permanent loss of sight. Fortunately, there are treatments that can slow further vision loss. Although there are no preventative therapies available at present, these are being explored in clinical trials. The period before the development of exAMD may therefore represent a critical window to target for therapeutic innovations: can we predict which patients will progress to exAMD, and help prevent sight loss before it even occurs? In our latest work, published in Nature Medicine , we collaborated with Moorfields Eye Hospital and Google Health to curate a dataset of images of eye retinas, train an artificial intelligence (AI) system that could predict the development of exAMD, and conduct a study to evaluate our model compared with expert clinicians. We demonstrate that our system is able to perform as well as, or better than, clinicians at predicting whether an eye will convert to exAMD in the next 6 months. Lastly, we explore the potential clinical applicability of our system. Our contribution highlights the potential of using AI in preventative studies for diseases such as exAMD. We used a dataset of anonymised retinal scans from Moorfields patients with exAMD in one eye, and at high-risk of developing exAMD in their other eye. This comprises 2,795 patients across seven different Moorfields sites in London, with representation across genders, age ranges, and ethnicities. These patients attend the hospital regularly to receive treatment, undergoing high-resolution three-dimensional optical coherence tomography (OCT) imaging of both eyes, at each visit. There is often a delay between when exAMD has developed and when it is diagnosed and treated. To address this, we worked with retinal experts to review all scans for each eye and specify the scan when exAMD was first evident. Our system is composed of two deep convolutional neural networks that take as input high-dimensional volumetric eye scans, where each scan consists of 58 million three-dimensional pixels (voxels). In our previous work , now continuing in collaboration with Google Health, we developed a model capable of segmenting these eye scans into thirteen anatomical categories. The segmented data was combined with the raw scan and both were used as inputs to the prediction model, which was trained to estimate a patient’s risk of conversion to exAMD in their other eye within the next six months. The benefit of a two stage system is that it gives the AI different views of the eye scans. Anatomical segmentation of the images helps the system learn to model risks based on signs of known anatomical indicators such as drusen (small fatty deposits), or loss of the retinal pigment epithelium (which helps to feed and protect other layers of the retina). Providing the raw eye scans allows the model to learn to spot other subtle changes that could become potential risk factors. At the end, the system combines the information it extracts from these scans to predict when and if the eye will progress to exAMD within the next 6 months. We chose this time window to enable the system to predict at least two follow-up intervals ahead of time, assuming a maximal follow-up interval of 3 months. It’s important to establish a benchmark of expert human performance to compare how well our system performs to clinical standards. However, prediction of exAMD is not a routine task performed by clinicians, so it’s unclear whether this task is even possible. To investigate this, we conducted a study with six retinal experts - three ophthalmologists and three optometrists, each with at least ten years of experience - to predict whether an eye will convert to exAMD within the ensuing 6 months. Despite the task’s novelty, the experts performed better than chance alone - however, the task is difficult, and there was substantial variability between their assessments. Our system performed as well as, and in some cases better than, experts in predicting exAMD progression, at the same time exhibiting less variability in agreement with each expert, compared to experts with each other. It may not be enough for a system to simply provide a prediction: clinicians may also ideally seek information regarding the anatomic basis for predictions, which might be of significant use for further interpretation (for example, for designing studies or considering treatments). A benefit of our system is that it automatically segments each scan into known types of tissue. Extracting these anatomical and pathological features provides a systematic method to visualise the change in these tissues over time. The risk scores given by our system align with anatomical changes over time, and together give a richer picture of exAMD conversion. We’re excited by the potential to support clinicians and researchers by developing systems that can help detect retinal diseases earlier and inform the clinical understanding of their progression. A prediction system such as this could be used to inform appropriate follow-up intervals to effectively manage high-risk patients. Our work builds upon promising early work to develop predictive models for exAMD based on retinal photographs and OCT scans . Since beginning our collaboration with Moorfields Eye Hospital in 2016, we’ve published two promising studies highlighting the potential of AI to transform retinal healthcare. However, we know there’s still a lot to do – this work does not yet represent a product that could be implemented in routine clinical practice. While our model can make better predictions than clinical experts, there are many other factors to consider for such systems to be impactful in a clinical setting. While the model was trained and evaluated on a population representative of the largest eye hospital in Europe, additional work would be needed to evaluate performance in the context of very different demographics. A recent study examining the use of a different AI system in a clinical setting highlighted just some of the sociotechnical issues for such systems in practice. Another difficult point to contend with is that any prediction system will have a certain rate of false positives: that is, when a patient is found to have a condition, or predicted to develop one, that they don’t actually have. The tradeoff of adding an imprecise AI system to an early warning loop could be unnecessarily costly to patients who aren’t actually at risk, and would need to be considered carefully in clinical studies of how such systems might be used in practice. In this paper, we propose two system operating points to balance sensitivity (a measure of how well it correctly identifies the disease) and specificity (a measure of how low the false positive rate is). For example, at a specificity of 90%, a sensitivity of 34% is achieved, meaning that the system correctly identified progression in one third of scans that did go on to progress within 6 months. This could identify a number of patients at high risk with a precision that may be sufficient to inform studies of novel treatment strategies that might mitigate vision loss and improve patient outcomes. We would like to thank Moorfields Eye Hospital and the clinicians who helped curate the data and were involved in our benchmarking study. Please see the paper for all acknowledgements and further details on the work. In addition, we’ve open-sourced the model code for future research, available here , and Moorfields will be making the dataset available through the Ryan Initiative for Macular Research . Read the Nature Medicine paper here . Check out the github repo here . Figure design by Paulo Estriga and Adam Cain.", "date": "2020-05-18"},
{"website": "Deepmind", "title": "Towards understanding glasses with graph neural networks", "author": [" Victor Bapst ", " Thomas Keck ", " Agnieszka Grabska-Barwinska ", " Craig Donner "], "link": "https://deepmind.com/blog/article/Towards-understanding-glasses-with-graph-neural-networks", "abstract": "Under a microscope, a pane of window glass doesn’t look like a collection of orderly molecules, as a crystal would, but rather a jumble with no discernable structure. Glass is made by starting with a glowing mixture of high-temperature melted sand and minerals. Once cooled, its viscosity (a measure of the friction in the fluid) increases a trillion-fold, and it becomes a solid, resisting tension from stretching or pulling. Yet the molecules in the glass remain in a seemingly disordered state, much like the original molten liquid – almost as though the disordered liquid state had been flash-frozen in place. The glass transition , then, first appears to be a dramatic arrest in the movement of the glass molecules. Whether this process corresponds to a structural phase transition (as in water freezing, or the superconducting transition ) is a major open question in the field. Understanding the nature of the dynamics of glass is fundamental to understanding how the atomic-scale properties define the visible features of many solid materials.  In the words of the recently deceased Nobel Prize laureate Philip W. Anderson , whose pioneering work shaped the field of solid-state physics: The glass transition is a ubiquitous phenomenon which manifests in more than window (silica) glasses. For instance, when ironing , polymers in a fabric are heated, become mobile, and then oriented by the weight of the iron. More broadly, a similar and related transition, the jamming transition, can be found in colloidal suspensions (such as ice cream), granular materials (such as a static pile of sand), and also biological systems (e.g., for modelling cell migration during embryonic development) as well as social behaviours (for instance traffic jams). These systems all operate under local constraints where the position of some elements inhibits the motion of others (termed frustration ). Their dynamics are complex and cooperative, taking the form of large-scale, collective rearrangements which propagate through space in a heterogeneous manner. Glasses are considered to be archetypal of these kinds of complex systems, and so better understanding them will have implications across many research areas. This understanding might yield practical benefits – for example, creating materials that have a more stable glass structure, instead of a crystalline one, would allow them to dissolve quickly, which could lead to new drug delivery methods.  Understanding the glass transition may result in other applications of disordered materials, in fields as diverse as biorenewable polymers and food processing. The study of glasses has also already led to insights in apparently very different domains such as constraint satisfaction problems in computer science and, more recently, the training dynamics of under-parameterized neural networks . A deeper understanding of glasses may lead to practical advances in the future, but their mysterious properties also raise many fundamental research questions. Though humans have been making silica glasses for at least four thousand years, they remain enigmatic to scientists: there are many unknowns about the underlying physical correlates of, for example, the trillion-fold increase in viscosity that happens over the cooling process. Our interest in this field was also motivated by the fact that glasses are also an excellent testbed for applying modern machine learning methods to physical problems: they’re easy to simulate, and easy to input to particle-based machine learning models. Crucially, we can then go in and examine these models to understand what they’ve learned about the system, to gain deeper qualitative insights about the nature of glass, and the structural quantities which underpin its mysterious dynamical qualities. Our new work , published in Nature Physics, could help us gain an understanding of the structural changes that may occur near the glass transition. More practically, this research could lead to insights about the mechanical constraints of glasses (e.g., where a glass will break). Glasses can be modelled as particles interacting via a short-range repulsive potential which essentially prevents particles from getting too close to each other. This potential is relational (only pairs of particles interact) and local (only nearby particles interact with each other), which suggests that a model that respects this local and relational structure should be effective. In other words, given the system is underpinned by a graph-like structure, we reasoned it would be best modeled by a graph structured network, and set out to apply Graph Neural Networks to predict physical aspects of a glass. We first created an input graph where the nodes represent particles, and edges represent interactions between particles, and are labelled with their relative distance.  A particle was connected to its neighboring particles within a certain radius (in this case, 2 particle diameters). We then trained a neural network, described below, to predict a single real number for each node of the graph. This prediction was ultimately regressed towards the mobilities of particles obtained from computer simulations of glasses. Mobility is a measure of how much a particle typically moves (more technically, this corresponds to the average distance travelled when averaging over initial velocities). Our network architecture was a typical graph network architecture, consisting of several neural networks. We first embedded the node and edge labels in a high-dimensional vector-space using two encoder networks (we used standard multi-layer perceptrons ). Next, we iteratively updated the embedded node and edge labels using two update networks visualized in Fig. 2b. At first, each edge updated based on its previous embedding and the embeddings of the two nodes it connected to. After all edges were updated in parallel using the same network, the nodes were also updated based on the sum of their neighboring edge embeddings and their previous embeddings, using a second network. We repeated this procedure several times (typically 7), allowing local information to propagate throughout the graph, as shown in Fig. 2c. Finally, we extracted the mobility for each particle from the final embeddings of the corresponding node using a decoder network. The resulting network has all the required properties: it is inherently relational, it is invariant under permutation of the nodes and edges of the graph, and it updates embedding in a way that is a composition of local operations. The network parameter training was done via stochastic gradient descent. To study the full dynamical evolution of glasses, we constructed several datasets corresponding to predictions of mobilities on different time horizons and for different temperatures. We note that each particle will have collided several thousands of times over those timescales. Thus, the network must find a way to coarsely represent the long-term dynamics of the system. After applying graph networks to the three dimensional glasses that we simulated, we found that they strongly outperformed existing models, ranging from standard physics-inspired baselines to state-of-the-art machine learning models . Comparing the predicted mobilities (colour gradients, Figure 3) with the ground truth simulation (dots, Figure 3), we see that the agreement is extremely good on short times and remains well matched up to the relaxation time of the glass. Looking at a glass over the timescale of its relaxation time – for actual glass, this would be thousands of years – is like looking at a liquid over about a picosecond (10 -12 ): the relaxation time is loosely when particles have collided enough to start losing information about their initial position. In numbers, the correlation between our prediction and the simulation's ground truth is 96% for very short timescales, and remains high at 64% for the relaxation time of the glass (an improvement of 40% compared to the previous state of the art). We don’t want to simply model glass, however: we want to understand it.  We therefore explored what factors were important to our model’s success in order to infer what properties are important in the underlying system. A central unsolved question in the dynamics of glass is how particles influence one another as a function of distance, and how this evolves over time. We investigated this by designing an experiment leveraging the specific architecture of the graph network. Recall that repeated applications of the edge and node updates define shells of particles around any given particle: the first shell consists of all particles one step away from this \"marked\" particle, the second shell consists of all particles one step away from the first shell, and so on (see the different shades of blue on Figure 2c). By measuring the sensitivity of the prediction that the network makes for the central particle when the n- th shell is modified, we can measure how large an area the network uses to extract its prediction, which provides an estimate of the distance over which particles influence each other in the physical system. We found that when predicting what happens in the near future or in the liquid phase, drastic modifications of the third shell (for instance, removing it altogether, Figure 4, left) did not modify the prediction that the network would make for the marked particle. On the other hand, when making predictions at low temperature and in the far future, after the glass starts to relax, even tiny perturbations (Figure 4, right) of the 5-th shell affect the prediction for the marked particle. These findings are consistent with a physical picture where a correlation length (a measure of the distance over which particles influence each other) grows upon approaching the glass transition. The definition and study of correlation lengths is a cornerstone of the study of phase transition in physics, and one that is still an open point of debate when studying glasses. While this \"machine learned\" correlation length cannot be directly transformed into a physically measurable quantity, it provides compelling evidence that growing spatial correlations are present in the system upon approaching the glass transition, and that our network has learned to extract them. Our results show that graph networks constitute a powerful tool to predict the long term dynamics of glassy systems, leveraging the structure hidden in a local neighborhood of particles. We expect our technique to be useful for predicting other physical quantities of interest in glasses, and hope that it will lead to more insights for glassy system theorists – we are open-sourcing our models and trained networks to aid this effort. More generally, graph networks are a versatile tool that are being applied to many other physical systems that consist of many-body interactions, in contexts including traffic , crowd simulations, and cosmology. The network analysis methods used here also yield a deeper understanding in other fields: graph networks may not only help us make better predictions for a range of systems, but indicate what physical correlates are important for modeling them – in this work, how interactions between local particles in a glassy material evolve over time. We believe that our results advocate using structured models when applying machine learning to the physical sciences; in our case, the ability to analyse the inner workings of a neural network indicated that it had discovered a quantity that correlates with an elusive physical quantity. This demonstrates that machine learning can be used not only to make quantitative predictions, but also to gain qualitative understanding of physical systems. This could mean that machine learning systems might be able to eventually assist researchers in deriving fundamental physical theories, ultimately helping to augment, rather than replace, human understanding. Work done in collaboration with: E. D. Cubuk, S. S. Schoenholz, A. Obika, A. W. R. Nelson, T. Back, D. Hassabis and P. Kohli Figure design by Paulo Estriga and Adam Cain We’re continuing to develop methods for applying machine learning to a broad range of fundamental science questions. We’re always looking to hire more scientists–read about our science programme and openings for more information.", "date": "2020-04-06"},
{"website": "Deepmind", "title": "Sharing our insights from designing with clinicians", "author": [" Cathy Harris ", " Alana Wood "], "link": "https://deepmind.com/blog/article/designing-with-clinicians", "abstract": "[Editor’s note: this is the first in a series of blog posts about what we’ve learned about working in healthcare. It’s both exceptionally hard and exceptionally important to get right, and we hope that by sharing our experiences we’ll help other health innovators along the way] In our design studio, we have Indi Young’s mantra on the wall as a reminder to “fall in love with the problem, not the solution”. Nowhere is this more true than in health, where there are so many real problems to address, and where introducing theoretically clever but practically flawed software could easily do more harm than good. Over the course of hundreds of hours of shadowing, interviews and workshops with nurses, doctors and patients, we’ve been privileged to learn a lot about some of the problems they all face - and we’re still learning a ton every day. We are constantly impressed by the skill and care that clinicians across the NHS deliver every day, and this is the primary motivation for our team to ensure that these people get the tools they need to appropriately support them in their quest to help patients. In the first of a series of posts about what we’ve learned through working in health, we wanted to share some of the design lessons from building Streams, a secure clinical mobile app that gives the right information to the right clinician at the right time. Most products begin with an insight into one core problem. In the case of Streams, it was that urgent clinical information gets retrieved by nurses and doctors over a mixture of outdated desktop computers, pager messages and handwritten lists. This contributes towards delays in care and occasionally serious harm if something is missed, and a 2017 study found that nearly half of emergency response time is wasted due to inefficient communication between systems. Surely a secure mobile app like Streams that immediately pushes urgent clinical information directly to the right nurse or doctor would be a better solution? We think so, yes. But as we learned more about the working lives of clinicians, and the phenomenon of “bleeper fatigue” - with many doctors telling us that they receive over 100 notifications a day already - we started to recognise how a solution might actually become another problem if we ended up contributing to the general bombardment of messages. It turns out that there’s a very fine line between alerts that clinicians find useful, and alerts that become a nuisance - and much of this seems to come down to the precise way in which data is presented in the app. For example, clinicians told us that when reviewing a patient’s record in the hospital, they need to see a patient’s hospital number, their allergies and details of previous admissions, rather than the traditional format of listing NHS number and GP practice. This was easy to fix with a subtle change in information architecture, which addressed something that many clinicians found to be more irritating than we initially thought! We also found by using the colour red in the interface intentionally and in a measured way, we could improve how people navigated content and better draw attention to information which needs urgent action. The sound designer on our team also worked directly with healthcare workers to produce a unique sound that would be immediately recognisable amidst the din of other bleeps and alerts. While these might seem like relatively minor design choices, clinicians told us of the surprisingly large impact it had on managing their busy time, and how likely they were to use the product in practice. Design doesn’t stop at deployment. Good health IT should empower nurses and doctors to change the way they care for patients. Each change creates a new reality, with its own set of new problems - and new opportunities to make a difference. For example, once we’d deployed Streams at the Royal Free Hospital in January 2017, clinicians told us that the app needed a better way to support communication between teams and across specialties. While they wanted urgent alerts to be sent to multiple clinicians at the same time, to increase the likelihood of a rapid response, it wasn’t easy enough for each of those clinicians to see if their colleagues were also responding - which in some cases could actually have hindered coordination, rather than improving it. We supported their need to triage alerts by giving clinicians the one-click ability within the app to “recommend a response”, “dismiss a response” or indicate that a patient had been attended to, and for this to be visible across the clinical team. Another area that surprised us was the urgency of creating a patient-friendly view within the app. From the beginning, our patient advisors championed the need for a way for patients to see their data within Streams, and so this had always been on our roadmap. But we hadn’t factored in just how important this might be right away. While it’s normal to see clinicians carrying paper notes and pagers on the wards, many patients were surprised to see staff walking around looking at an app on their mobile phones. In some cases, patients assumed that clinicians must be on personal social networking or messaging apps - rather than using an app specifically designed to support their care - and challenged them directly about what they were doing! These moments provided opportunities for clinicians to really engage patients in their care, by showing them the data in the Streams app and talking through what it meant for their treatment. The challenge was that the default Streams views are designed for highly-trained clinicians, with graphs and notations that are hard to understand for the layperson. We are now looking into making the app more patient-friendly, providing a new way to strengthen and support the clinician-patient relationship. This kind of post-deployment insight could only have come through ongoing face-to-face feedback from nurses, doctors and patients. Typical app usage metrics might be useful for developers in other domains, but in a field like health they will almost always miss the point. What’s important isn’t how health IT is used in itself, but rather the changes it enables in the person-to-person context of care: whether it improves or impairs the caring relationship between clinicians and their patients, or whether it empowers or stresses out overstretched clinical teams. The answers lie in the conversations, not the usage data. In a future blog, we'll talk in more detail about what we've learned from our dedicated patient involvement efforts too. Our approach is constantly informed by the great work of other people and organisations working across the field. So far we’ve been drawing on best practices in user-centred design and agile development from the likes of the Royal College of Art's Healthcare Innovation Exchange Centre , Prescribe Design , Stanford Biodesign and the Mayo Innovation Centre . This article , by Carl Warren at Team Consulting, provides a good explanation of how agile processes can be used in medical technology, for those curious to learn more. We hope Streams continues to evolve with the help of patients, clinicians and nurses, and that the lessons we’ve learned are useful to other innovators in health! If you work in healthcare and want to be involved in our future work, please register your interest here . To learn more about how Streams is having an impact on the wards, listen to Sarah Stanley, consultant nurse at The Royal Free London NHS Foundation Trust below:", "date": "2017-11-10"},
{"website": "Deepmind", "title": "DeepMind’s work in 2016: a round-up", "author": [" Demis Hassabis ", " Mustafa Suleyman ", " Shane Legg "], "link": "https://deepmind.com/blog/article/deepmind-round-up-2016", "abstract": "In a world of fiercely complex, emergent, and hard-to-master systems - from our climate to the diseases we strive to conquer - we believe that intelligent programs will help unearth new scientific knowledge that we can use for social benefit. To achieve this, we believe we’ll need general-purpose learning systems that are capable of developing their own understanding of a problem from scratch, and of using this to identify patterns and breakthroughs that we might otherwise miss. This is the focus of our long-term research mission at DeepMind. While we remain a long way from anything that approximates what you or we would term intelligence, 2016 was a big year in which we made exciting progress on a number of the core underlying challenges, and saw the first glimpses of the potential for positive real-world impact. Our program AlphaGo , for which we were lucky enough to receive our second Nature front cover , took on and beat the world champion Lee Sedol at the ancient game of Go, a feat that many experts said came a decade ahead of its time. Most exciting for us - as well as for the worldwide Go community - were AlphaGo’s displays of game-winning creativity, in some cases finding moves that challenged millennia of Go wisdom. In its ability to identify and share new insights about one of the most contemplated games of all time, AlphaGo offers a promising sign of the value AI may one day provide, and we're looking forward to playing more games in 2017. We also made meaningful progress in the field of generative models, building programs able to imagine new constructs and scenarios for themselves. Following our PixelCNN paper on image generation, our paper on WaveNet demonstrated the usefulness of generative audio, achieving the world’s most life-like speech synthesis by imaginatively creating raw waveforms rather than stitching together samples of recorded language. We’re planning to put this into production with Google and are excited about enabling improvements to products used by millions of people. Another important area of research is memory, and specifically the challenge of combining the decision-making aptitude of neural networks with the ability to store and reason about complex, structured data. Our work on Differentiable Neural Computers , for which we received our third Nature paper in eighteen months, demonstrated models that can simultaneously learn like neural networks as well as memorise data like computers. These models are already able to learn how to answer questions about data structures from family trees to tube maps, and bring us closer to the goal of using AI for scientific discovery in complex datasets. As well as pushing the boundaries of what these systems can do, we’ve also invested significant time in improving how they learn. A paper titled ‘ Reinforcement Learning with Unsupervised Auxiliary Tasks ’ described methods to improve the speed of learning for certain tasks by an order of magnitude. And given the importance of high-quality training environments for agents, we open sourced our flagship DeepMind Lab research environment for the community, and are working with Blizzard to develop AI-ready training environments for StarCraft II as well. Of course, this is just the tip of the iceberg, and you can read much more about our work in the many papers we published this year in top-tier journals from Neuron to PNAS and at major machine learning conferences from ICLR to NIPS. It’s amazing to see how others in the community are already actively implementing and building on the work in these papers - just look at the remarkable renaissance of Go-playing computer programs in the latter part of 2016! - and to witness the broader fields of AI and machine learning go from strength to strength. It’s equally amazing to see the first early signs of real-world impact from this work. Our partnership with Google’s data centre team used AlphaGo-like techniques to discover creative new methods of managing cooling , leading to a remarkable 15% improvement in the buildings’ energy efficiency. If it proves possible to scale these kinds of techniques up to other large-scale industrial systems, there's real potential for significant global environmental and cost benefits. This is just one example of the work we’re doing with various teams at Google to apply our cutting-edge research to products and infrastructure used across the world. We’re also actively engaged in machine learning research partnerships with two NHS hospital groups in the UK, our home, to explore how our techniques could enable more efficient diagnosis and treatment of conditions that affect millions worldwide, as well as working with two further hospital groups on mobile apps and foundational infrastructure to enable improved care on the clinical frontlines. Of course, the positive social impact of technology isn’t only about the real-world problems we seek to solve, but also about the way in which algorithms and models are designed, trained and deployed in general. We’re proud to have been involved in founding the Partnership on AI , which will bring together leading research labs with non-profits, civil society groups and academics to develop best practices in areas such as algorithmic transparency and safety. By fostering a diversity of experience and insight, we hope that we can help address some of these challenges and find ways to put social purpose at the heart of the AI community across the world. We’re still a young company early in our mission, but if in 2017 we can make further simultaneous progress on these three fronts - algorithmic breakthroughs, social impact, and ethical best practice - then we'll be in good shape to make a meaningful continued contribution to the scientific community and to the world beyond.", "date": "2017-01-03"},
{"website": "Deepmind", "title": "Deep Reinforcement Learning", "author": [" David Silver "], "link": "https://deepmind.com/blog/article/deep-reinforcement-learning", "abstract": "Humans excel at solving a wide variety of challenging problems, from low-level motor control through to high-level cognitive tasks. Our goal at DeepMind is to create artificial agents that can achieve a similar level of performance and generality. Like a human, our agents learn for themselves to achieve successful strategies that lead to the greatest long-term rewards. This paradigm of learning by trial-and-error, solely from rewards or punishments, is known as reinforcement learning (RL). Also like a human, our agents construct and learn their own knowledge directly from raw inputs, such as vision, without any hand-engineered features or domain heuristics. This is achieved by deep learning of neural networks. At DeepMind we have pioneered the combination of these approaches - deep reinforcement learning - to create the first artificial agents to achieve human-level performance across many challenging domains. Our agents must continually make value judgements so as to select good actions over bad. This knowledge is represented by a Q-network that estimates the total reward that an agent can expect to receive after taking a particular action. Two years ago we introduced the first widely successful algorithm for deep reinforcement learning. The key idea was to use deep neural networks to represent the Q-network, and to train this Q-network to predict total reward. Previous attempts to combine RL with neural networks had largely failed due to unstable learning. To address these instabilities, our Deep Q-Networks (DQN) algorithm stores all of the agent's experiences and then randomly samples and replays these experiences to provide diverse and decorrelated training data. We applied DQN to learn to play games on the Atari 2600 console. At each time-step the agent observes the raw pixels on the screen, a reward signal corresponding to the game score, and selects a joystick direction. In our Nature paper we trained separate DQN agents for 50 different Atari games, without any prior knowledge of the game rules. Amazingly, DQN achieved human-level performance in almost half of the 50 games to which it was applied; far beyond any previous method. The DQN source code and Atari 2600 emulator are freely available to anyone who wishes to experiment for themselves. We have subsequently improved the DQN algorithm in many ways: further stabilising the learning dynamics ; prioritising the replayed experiences ; normalising , aggregating and re-scaling the outputs. Combining several of these improvements together led to a 300% improvement in mean score across Atari games; human-level performance has now been achieved in almost all of the Atari games. We can even train a single neural network to learn about multiple Atari games . We have also built a massively distributed deep RL system, known as Gorila , that utilises the Google Cloud platform to speed up training time by an order of magnitude; this system has been applied to recommender systems within Google. However, deep Q-networks are only one way to solve the deep RL problem. We recently introduced an even more practical and effective method based on asynchronous RL. This approach exploits the multithreading capabilities of standard CPUs. The idea is to execute many instances of our agent in parallel, but using a shared model. This provides a viable alternative to experience replay, since parallelisation also diversifies and decorrelates the data. Our asynchronous actor-critic algorithm, A3C , combines a deep Q-network with a deep policy network for selecting actions. It achieves state-of-the-art results, using a fraction of the training time of DQN and a fraction of the resource consumption of Gorila. By building novel approaches to intrinsic motivation and temporally abstract planning , we have also achieved breakthrough results in the most notoriously challenging Atari games, such as Montezuma’s Revenge. While Atari games demonstrate a wide degree of diversity, they are limited to 2D sprite-based video games. We have recently introduced Labyrinth: a challenging suite of 3D navigation and puzzle-solving environments. Again, the agent only observes pixel-based inputs from its immediate field-of-view, and must figure out the map to discover and exploit rewards. Amazingly, the A3C algorithm achieves human-level performance, out-of-the-box, on many Labyrinth tasks. An alternative approach based on episodic memory has also proven successful. Labyrinth will also be released open source in the coming months. We have also developed a number of deep RL methods for continuous control problems such as robotic manipulation and locomotion. Our Deterministic Policy Gradients algorithm ( DPG ) provides a continuous analogue to DQN, exploiting the differentiability of the Q-network to solve a wide variety of continuous control tasks. Asynchronous RL also performs well in these domains and, when augmented with a hierarchical control strategy, can solve challenging problems such as ant soccer and a 54-dimensional humanoid slalom, without any prior knowledge of the dynamics. The game of Go is the most challenging of classic games. Despite decades of effort, prior methods had only achieved amateur level performance. We developed a deep RL algorithm that learns both a value network (which predicts the winner) and a policy network (which selects actions) through games of self-play. Our program AlphaGo combined these deep neural networks with a state-of-the-art tree search. In October 2015, AlphaGo became the first program to defeat a professional human player . In March 2016, AlphaGo defeated Lee Sedol (the strongest player of the last decade with an incredible 18 world titles) by 4 games to 1, in a match that was watched by an estimated 200 million viewers. Separately, we have also developed game theoretic approaches to deep RL , culminating in a super-human poker player for heads-up limit Texas Hold’em. From Atari to Labyrinth, from locomotion through manipulation, to poker and even the game of Go, our deep reinforcement learning agents have demonstrated remarkable progress on a wide variety of challenging tasks. Our goal is to continue to improve the capabilities of our agents, and to use them to make a positive impact on society, in important applications such as healthcare .", "date": "2016-06-17"},
{"website": "Deepmind", "title": "Differentiable neural computers", "author": [" Greg Wayne ", " Alexander Graves "], "link": "https://deepmind.com/blog/article/differentiable-neural-computers", "abstract": "In a recent study in Nature , we introduce a form of memory-augmented neural network called a differentiable neural computer, and show that it can learn to use its memory to answer questions about complex, structured data, including artificially generated stories, family trees, and even a map of the London Underground. We also show that it can solve a block puzzle game using reinforcement learning. Plato likened memory to a wax tablet on which an impression, imposed on it once, would remain fixed. He expressed in metaphor the modern notion of plasticity – that our minds can be shaped and reshaped by experience. But the wax of our memories does not just form impressions, it also forms connections, from one memory to the next. Philosophers like John Locke believed that memories connected if they were formed nearby in time and space. Instead of wax, the most potent metaphor expressing this is Marcel Proust’s madeleine cake; for Proust, one taste of the confection as an adult undammed a torrent of associations from his childhood. These episodic memories (event memories) are known to depend on the hippocampus in the human brain. Today, our metaphors for memory have been refined. We no longer think of memory as a wax tablet but as a reconstructive process, whereby experiences are reassembled from their constituent parts. And instead of a simple association between stimuli and behavioural responses, the relationship between memories and action is variable, conditioned on context and priorities. A simple article of memorised knowledge, for example a memory of the layout of the London Underground, can be used to answer the question, “How do you get from Piccadilly Circus to Moorgate?” as well as the question, “What is directly adjacent to Moorgate, going north on the Northern Line?”. It all depends on the question; the contents of memory and their use can be separated. Another view holds that memories can be organised in order to perform computation. More like lego than wax, memories can be recombined depending on the problem at hand. Neural networks excel at pattern recognition and quick, reactive decision-making, but we are only just beginning to build neural networks that can think slowly – that is, deliberate or reason using knowledge. For example, how could a neural network store memories for facts like the connections in a transport network and then logically reason about its pieces of knowledge to answer questions? In a recent paper , we showed how neural networks and memory systems can be combined to make learning machines that can store knowledge quickly and reason about it flexibly. These models, which we call differentiable neural computers (DNCs), can learn from examples like neural networks, but they can also store complex data like computers. In a normal computer, the processor can read and write information from and to random access memory (RAM). RAM gives the processor much more space to organise the intermediate results of computations. Temporary placeholders for information are called variables and are stored in memory. In a computer, it is a trivial operation to form a variable that holds a numerical value. And it is also simple to make data structures – variables in memory that contain links that can be followed to get to other variables. One of the simplest data structures is a list – a sequence of variables that can be read item by item. For example, one could store a list of players’ names on a sports team and then read each name one by one. A more complicated data structure is a tree. In a family tree for instance, links from children to parents can be followed to read out a line of ancestry. One of the most complex and general data structures is a graph, like the London Underground network. When we designed DNCs, we wanted machines that could learn to form and navigate complex data structures on their own. At the heart of a DNC is a neural network called a controller, which is analogous to the processor in a computer. A controller is responsible for taking input in, reading from and writing to memory, and producing output that can be interpreted as an answer. The memory is a set of locations that can each store a vector of information. A controller can perform several operations on memory. At every tick of a clock, it chooses whether to write to memory or not. If it chooses to write, it can choose to store information at a new, unused location or at a location that already contains information the controller is searching for. This allows the controller to update what is stored at a location. If all the locations in memory are used up, the controller can decide to free locations, much like how a computer can reallocate memory that is no longer needed. When the controller does write, it sends a vector of information to the chosen location in memory. Every time information is written, the locations are connected by links of association, which represent the order in which information was stored. As well as writing, the controller can read from multiple locations in memory. Memory can be searched based on the content of each location, or the associative temporal links can be followed forward and backward to recall information written in sequence or in reverse. The read out information can be used to produce answers to questions or actions to take in an environment. Together, these operations give DNCs the ability to make choices about how they allocate memory, store information in memory, and easily find it once there. To the non-technical reader, it may seem a bit odd that we have repeatedly used phrases like “the controller can” or “differentiable neural computers … make choices”. We speak like this because differentiable neural computers learn how to use memory and how to produce answers completely from scratch. They learn to do so using the magic of optimisation: when a DNC produces an answer, we compare the answer to a desired correct answer. Over time, the controller learns to produce answers that are closer and closer to the correct answer. In the process, it figures out how to use its memory. We wanted to test DNCs on problems that involved constructing data structures and using those data structures to answer questions. Graph data structures are very important for representing data items that can be arbitrarily connected to form paths and cycles. In the paper, we showed that a DNC can learn on its own to write down a description of an arbitrary graph and answer questions about it. When we described the stations and lines of the London Underground, we could ask a DNC to answer questions like, “Starting at Bond street, and taking the Central line in a direction one stop, the Circle line in a direction for four stops, and the Jubilee line in a direction for two stops, at what stop do you wind up?” Or, the DNC could plan routes given questions like “How do you get from Moorgate to Piccadilly Circus?” In a family tree, we showed that it could answer questions that require complex deductions. For example, even though we only described parent, child, and sibling relationships to the network, we could ask it questions like “Who is Freya’s maternal great uncle?” We also found it possible to analyse how DNCs used their memories by visualising which locations in memory were being read by the controller to produce what answers. Conventional neural networks in our comparisons either could not store the information, or they could not learn to reason in a way that would generalise to new examples. We could also train a DNC by reinforcement learning. In this framework, we let the DNC produce actions but never show it the answer. Instead, we score it with points when it has produced a good sequence of actions (like the children’s game “hot or cold”). We connected a DNC to a simple environment with coloured blocks arranged in piles. We would give it instructions for goals to achieve: “Put the light blue block below the green; the orange to the left of the red; the purple below the orange; the light blue to the right of the dark blue; the green below the red; and the purple to the left of the green”. We could establish a large number of such possible goals and then ask the network to execute the actions that would produce one or another goal state on command. In this case, again like a computer, the DNC could store several subroutines in memory, one per possible goal, and execute one or another. The question of how human memory works is ancient and our understanding still developing. We hope that DNCs provide both a new tool for computer science and a new metaphor for cognitive science and neuroscience: here is a learning machine that, without prior programming, can organise information into connected facts and use those facts to solve problems. For more information about DNC, please read our paper and an opinion piece by Herbert Jaeger about deep neural reasoning. Our open source implementation is available on GitHub .", "date": "2016-10-12"},
{"website": "Deepmind", "title": "Advanced machine learning helps Play Store users discover personalised apps", "author": [" Michelle Gong ", " Anton Zhernov "], "link": "https://deepmind.com/blog/article/Advanced-machine-learning-helps-Play-Store-users-discover-personalised-apps", "abstract": "Over the past few years we've applied DeepMind's technology to Google products and infrastructure, with notable successes like reducing the amount of energy needed for cooling data centers , and extending Android battery performance . We're excited to share more about our work in the coming months. We know users get the most out of their phone when they have apps and games they love, and that it’s exciting to discover new favourites. In collaboration with Google Play, our team that leads on collaborations with Google has driven significant improvements in the Play Store's discovery systems, helping to deliver a more personalised and intuitive Play Store experience for users. Every month, billions of users come to the Google Play Store to download apps for their mobile devices – the Play Store supports one of the largest recommendation systems in the world.  While some are looking for specific apps, like Snapchat, others are browsing the store to discover what’s new and interesting. The Google Play discovery team strives to help users discover the most relevant apps and games by providing them with helpful app recommendations. To deliver a richer, personalised experience, apps are suggested according to past user preferences. This, however, requires nuance – both for understanding what an app does, and its relevance to a particular user.  For example, to an avid sci-fi gamer, similar game recommendations may be of interest, but if a user installs a travel app, recommending a translation app may be more relevant than five more travel apps. The collection and use of these user preferences is governed by Google's privacy policies . We started collaborating with the Play store to help develop and improve systems that determine the relevance of an app with respect to the user.  In this post, we’ll explore some of the cutting-edge machine learning techniques we developed to achieve this. Today, Google Play’s recommendation system contains three main models: a candidate generator, a reranker, and a model to optimise for multiple objectives.  The candidate generator is a deep retrieval model that can analyse more than a million apps and retrieve the most suitable ones. For each app, a reranker, i.e. a user preference model, predicts the user's preferences along multiple dimensions. Next these predictions are the input to a multi-objective optimisation model whose solution gives the most suitable candidates to the user. To improve how Google Play’s recommendation system learns users’ preferences, our first approach was to use an LSTM (Long Short-Term Memory) model, a recurrent neural network that performs well in real-world scenarios, owing to a powerful update equation and backpropagation dynamics. Whilst the LSTM led to significant accuracy gains, it also introduced a serving delay, because LSTMs can be computationally expensive when processing long sequences. To address this, we  replaced the LSTM with a Transformer model , which is well-equipped for sequence-to-sequence prediction and has previously yielded strong results in natural language processing, as it’s able to capture longer dependencies between words than other commonly used models. The Transformer improved the model performance, but also increased the training cost. Our third and final solution was to implement an efficient additive attention model that works for any combination of sequence features, while incurring low computational cost. Our model (called a candidate generator) learns what apps a user is more likely to install based on previous  apps they’ve installed from the Play store. However, this can introduce a recommendation bias problem. For instance, if app A is shown in the Play store 10 times more than app B, it’s more likely to be installed by the user, and thus more likely to be recommended by our model.  The model therefore learns a bias that favours the apps that are shown – and thus installed – more often. To help correct for this bias, we introduced importance weighting in our model. An importance weight is based on the impression-to-install rate of each individual app in comparison with the median impression-to-install rate across the Play store.  An app with a below-median install rate will have an importance weight less than one. However, even “niche” apps that are installed less frequently can have a high importance weight if their install rate is higher than the median rate. Through importance weighting, our candidate generator can downweight or upweight apps based on their install rates, which mitigates the recommendation bias problem. Recommendation systems often provide a range of possibilities to a user, and present them in an order with the best or most relevant options at the top. But how do we ensure the most relevant apps make it to the top of the list, so the user doesn’t have to scroll for pages, or potentially miss the best option? Many recommendation systems treat the ranking problem as a binary classification problem, where the training data is labeled with either a positive or negative class, and the ranker learns to predict a probability from this binary label alone. However, this type of “pointwise” model – which only ranks one item at a time – fails to capture the context of how apps perform relative to one another. To deliver a better user experience, the ranker could predict the relative order of presented items based on the context of other candidate apps. Our solution to this, the reranker model, learns the relative importance of a pair of apps that have been shown to the user at the same time. We built our reranker model on a core insight: if a user is presented with two apps in the store, the app that the user chooses to  install is more relevant to the user than the app that they didn't install. We can then assign each of the pair a positive or negative label, and the model tries to minimise the number of inversions in ranking, thus improving the relative ranking of the apps. This kind of “pairwise” model works better in practice than pointwise models because predicting relative order is closer to the nature of ranking than predicting class labels or install probabilities. Many recommendation systems must optimise for multiple objectives at the same time, such as relevance, popularity, or personal preferences. We formulated the multi-objective optimisation problem as a constrained optimisation problem: the overall objective is to maximise the expected value of a primary metric, subject to constraints in terms of expected values of secondary metrics. During online serving, the objectives may shift according to user’s needs – for example, a user that had previously been interested in housing search apps might have found a new flat, and so is now interested in home decor apps – so we worked toward a dynamic solution. Rather than solving the problem offline and bringing a fixed model online, we solved this problem on-line, per-request, based on the actual values of the objectives during serving time. We define the constraints to be relative constraints, meaning we would like to improve the secondary objective by a percentage rather than an absolute value. This way, any shifts in the secondary objectives didn’t affect our solver. The algorithm that we developed can be used to find tradeoffs between a number of metrics. Finding suitable points along the tradeoff curve, our algorithm can significantly raise secondary metrics with only minor effects on the primary metric. One of our key takeaways from this collaboration is that when implementing advanced machine learning techniques for use in the real world, we need to work within many practical constraints. Because the Play Store and DeepMind teams worked so closely together and communicated on a daily basis, we were able to take product requirements and constraints into consideration throughout the algorithm design, implementation, and final testing phases, resulting in a more successful product. Our collaborations with Google have so far reduced the electricity needed for cooling Google’s data centres by up to 30%, boosted the value of Google’s wind energy by roughly 20%, and created on-device learning systems to optimise Android battery performance. WaveNet is now in the hands of Google Assistant and Google Cloud Platform users around the world, and our research collaboration with Waymo has helped improve the performance of its models, as well as the efficiency of training its neural networks. Working at Google scale presents a unique set of research challenges, and the opportunity to take our breakthroughs beyond the lab to address global, complex challenges. If you’re interested in working on applying cutting edge research to real world problems, learn more about the team that led this project here . In collaboration with: Dj Dvijotham, Amogh Asgekar, Will Zhou, Sanjeev Jagannatha Rao, Xueliang Lu, Carlton Chu, Arun Nair, Timothy Mann, Bruce Chia, Ruiyang Wu, Natarajan Chendrashekar, Tyler Brabham, Amy Miao, Shelly Bensal, Natalie Mackraz, Praveen Srinivasan & Harish Chandran", "date": "2019-11-18"},
{"website": "Deepmind", "title": "2017: DeepMind's year in review", "author": [" Demis Hassabis ", " Mustafa Suleyman ", " Shane Legg "], "link": "https://deepmind.com/blog/article/2017-deepminds-year-review", "abstract": "In July, the world number one Go player Ke Jie spoke after a streak of 20 wins. It was two months after he had played AlphaGo at the Future of Go Summit in Wuzhen , China. “After my match against AlphaGo, I fundamentally reconsidered the game, and now I can see that this reflection has helped me greatly,” he said . “I hope all Go players can contemplate AlphaGo’s understanding of the game and style of thinking, all of which is deeply meaningful. Although I lost, I discovered that the possibilities of Go are immense and that the game has continued to progress.” Ke Jie is a master of the game and we were honoured by his words . We were also inspired by them, because they hint at a future where society could use AI as a tool for discovery, uncovering new knowledge and increasing our understanding of the world. With machine-aided science in particular, we hope that AI systems could help make progress on challenges from climate change and drug discovery, to finding complex new materials or helping ease the pressure on healthcare systems. This potential for societal benefit is why we set up DeepMind, and we’re excited to have made continued progress on some of the fundamental scientific challenges as well as on AI safety and ethics. The approach we take at DeepMind is inspired by neuroscience , helping to make progress in critical areas such as imagination , reasoning , memory and learning . Take imagination, for example: this distinctively human ability plays a crucial part in our daily lives, allowing us to plan and reason about the future, but is hugely challenging for computers. We continue to work hard on this problem, this year introducing imagination-augmented agents that are able to extract relevant information from an environment in order to plan what to do in the future. This neuroscience-inspired approach also created one of the most popular demonstrations of our work, when we trained a neural network to control a variety of simplified body shapes in a simulated environment. This kind of sophisticated motor control is a hallmark of physical intelligence, and is a crucial part of our research programme. Although the resulting movements were wild and - at times - ungainly, they were also surprisingly successful and made for entertaining viewing . Separately, we made progress in the field of generative models. Just over a year ago we presented WaveNet , a deep neural network for generating raw audio waveforms that was capable of producing better and more realistic-sounding speech than existing techniques. At that time, the model was a research prototype and was too computationally intensive to work in consumer products. Over the last 12 months, our teams managed to create a new model that was 1000x faster. In October, we revealed that this new Parallel WaveNet is now being used in the real world, generating the Google Assistant voices for US English and Japanese. This is an example of the effort we invest in making it easier to build, train and optimise AI systems. Other techniques we worked on this year, such as distributional reinforcement learning , population based training for neural networks and new neural architecture search methods , promise to make systems easier to build, more accurate and quicker to optimise. We have also dedicated significant time to creating new and challenging environments in which to test our systems, including our work with Blizzard to open up StarCraft II for research . But we know that technology is not value neutral. We cannot simply make progress in fundamental research without also taking responsibility for the ethical and social impact of our work. This drives our research in critical areas such as interpretability, where we have been exploring novel methods to understand and explain how our systems work. It’s also why we have an established technical safety team that continued to develop practical ways to ensure that we can depend on future systems and that they remain under meaningful human control. In October we took another step by launching DeepMind Ethics & Society , a research unit that will help us explore and understand the real-world impacts of AI in order to achieve social good. Our research will be guided by Fellows who are renowned experts in their fields - like philosopher Nick Bostrom, climate change specialist Christiana Figueres, leading researcher James Manyika, and economists Diane Coyle and Jeffrey Sachs. AI must be shaped by society’s priorities and concerns, which is why we’re working with partner organisations on events aimed at opening up the conversation about how AI should be designed and deployed. For example, Joy Buolamwini , who leads the Algorithmic Justice League, and experts from Article 36, Human Rights Watch, and the British Armed forces joined us for a session at Wired Live to discuss algorithmic bias and restricting the use of lethal autonomous weapons. As we’ve said regularly this year, these issues are too important and their effects too wide-ranging to ignore. That’s also why we also need new spaces, both within and outside AI companies, for conversations about anticipating and directing the impacts of the technology. One example is the Partnership on AI , which we co-chaired this year, and which has been charged with bringing together industry competitors, academia and civil society to discuss key ethical issues. Over the past year, PAI has welcomed 43 new nonprofit and for-profit members and a new Executive Director, Terah Lyons. And in the next few months, we’re looking forward to working with this group to examine a wide range of research themes, including bias and discrimination in algorithms, the impact of machine learning on automation and labour, and more. We also believe in the importance of using our technology for practical social benefit, and continue to see amazing potential for real-world impact in health and energy. This year we agreed two new partnerships with NHS hospital trusts to deploy our Streams app, which supports NHS clinicians using digital technology. We’re also part of a consortium of leading research institutions that launched a groundbreaking study to determine if cutting-edge machine learning technology could help improve the detection of breast cancer. In parallel, we've also worked hard on the oversight of our work in health. We wrote about the lessons learned from the Information Commissioner’s findings about our original partnership with the Royal Free, and DeepMind Health’s Independent Reviewers published their first open annual report on our work. Their scrutiny makes our work better. We’ve made major improvements to our engagement with patients and the public, including workshops with patients and carers, and we’re also exploring technical ways of building trust into our systems, such as the verifiable data audit , which we plan to release as an open-source tool. We are proud of all of our progress in 2017, but know there is still a long way to go. Five months after we played Ke Jie in Wuzhen and retired AlphaGo from competitive play, we published our fourth Nature paper for a new version of the system, known as AlphaGo Zero , which uses no human knowledge. Over the course of millions of games, the system progressively learned the game of Go from scratch, accumulating thousands of years of knowledge in just a few days. In doing so, it also uncovered unconventional strategies and revealed new knowledge about this ancient game. Our belief is that AI will be able to do the same for other complex problems, as a scientific tool and a multiplier for human ingenuity. The AlphaGo team are already working on the next set of grand challenges and we hope the moments of algorithmic inspiration they helped to create are just the beginning.", "date": "2017-12-21"},
{"website": "Deepmind", "title": "Using AI to give doctors a 48-hour head start on life-threatening illness", "author": [" Mustafa Suleyman ", " Dominic King "], "link": "https://deepmind.com/blog/article/predicting-patient-deterioration", "abstract": "Artificial intelligence can now predict one of the leading causes of avoidable patient harm up to two days before it happens, as demonstrated by our latest research published in Nature . Working alongside experts from the US Department of Veterans Affairs (VA), we have developed technology that, in the future, could give doctors a 48-hour head start in treating acute kidney injury (AKI), a condition that is associated with over 100,000 people in the UK every year. These findings come alongside a peer-reviewed service evaluation of Streams, our mobile assistant for clinicians, which shows that patient care can be improved, and health care costs reduced, through the use of digital tools. Together, they form the foundation for a transformative advance in medicine, helping to move from reactive to preventative models of care. Millions of people die every year from diseases that could have been prevented with earlier detection. One such disease is acute kidney injury (AKI), a condition where a patient’s kidney suddenly stops working properly. Affecting up to one in five hospitalised patients in the UK and the US , the condition is notoriously difficult to spot, and deterioration can happen quickly. Experts believe that up to 30% of cases could be prevented if a doctor intervenes early enough. Over the last few years, our team at DeepMind has focused on finding an answer to the complex problem of avoidable patient harm, building digital tools that can spot serious conditions earlier and helping doctors and nurses deliver faster, better care to patients in need. This is our team’s biggest healthcare research breakthrough to date, demonstrating the ability to not only spot deterioration more effectively, but actually predict it before it happens. Working with the VA, the DeepMind team applied AI technology to a comprehensive de-identified electronic health record dataset collected from a network of over a hundred VA sites. The research shows that the AI could accurately predict AKI in patients up to 48 hours earlier than it is currently diagnosed. Importantly, the model correctly predicted 9 out of 10 patients whose condition deteriorated so severely that they then required dialysis. This could provide a window in the future for earlier preventative treatment and avoid the need for more invasive procedures like kidney dialysis. The model has also been designed so that it might, in the future, generalise to other major causes of diseases and deterioration such as sepsis, a life-threatening infection. To address the ‘black box’ problem – one of the key barriers for the implementation of AI in clinical practice – the model also provides the clinical information that was most important in making its predictions of deteriorating kidney function. It also provides predicted future results for several relevant blood tests. This information may help clinicians understand the reasoning behind the AI-enabled alert and anticipate future patient deterioration. However, these predictions can’t help real patients without the right tools to  alert specialists. Clinicians routinely use pagers, paper records and fax machines to communicate with each other, but better technology is desperately needed so that critical information can be delivered to the right specialist at the right time. That’s why we’re also pleased to report that the results of a peer-reviewed evaluation of our mobile medical assistant Streams have also been published today. This work was conducted by researchers at University College London. Streams is a mobile medical assistant for clinicians, and has been in use at the Royal Free London NHS Foundation Trust since early 2017. The app uses the existing national AKI algorithm to flag patient deterioration, supports the review of medical information at the bedside, and enables instant communication between clinical teams. Shortly after rolling out at the Royal Free, clinicians said that Streams was saving them up to two hours a day. We also heard about patients, like Afia Ahmad , whose treatment was escalated thanks to the app. But we wanted to quantify these benefits through robust clinical evaluation. Today’s results show that the app saved clinicians’ time, improved care and reduced the number of AKI cases being missed at the hospital. By using Streams, specialists reviewed urgent cases within 15 minutes or less (a process that might otherwise have taken several hours) and fewer cases of AKI were missed (3.3% rather than 12.4%). The app also reduced the average cost of admission for a patient with AKI by 17%, demonstrating a huge potential cost saving for hospitals in the future, considering that AKI costs the NHS more than £1 billion each year. Feedback from the qualitative study was positive, with healthcare professionals emphasising the ways in which the app accelerated the detection of patients in need, saved them time in performing administrative tasks, and improved team communication. One respondent said the app “streamlines care, and speeds up the time in which they get a specialist renal review.” Another clinician from the nephrology team stated that “Being able to look up the blood results for anyone in the hospital wherever you are is unparalleled...it must save at least – I don’t know if you could analyse it – but it must save at least a couple of hours in a day.” Getting the right information about the right patient at the right time is a huge problem for healthcare systems across the globe. Critically, these early findings from the Royal Free suggest that, in order to improve patient outcomes even further, clinicians need to be able to intervene before AKI can be detected by the current NHS algorithm – which is why our research on AKI is so promising. These results comprise the building blocks for our long-term vision of preventative healthcare, helping doctors to intervene in a proactive, rather than reactive, manner. Streams doesn’t use artificial intelligence at the moment, but the team now intends to find ways to safely integrate predictive AI models into Streams in order to provide clinicians with intelligent insights into patient deterioration. This is a major milestone for the DeepMind Health team, who will be carrying this work forward as part of Google Health, led by Dr David Feinberg. As we announced in November 2018, the Streams team, and colleagues working on translational research in healthcare, will be joining Google in order to make a positive impact on a global scale. The combined experience, infrastructure and expertise of DeepMind Health teams alongside Google’s will help us continue to develop mobile tools that can support more clinicians, address critical patient safety issues and could, we hope, save thousands of lives globally.", "date": "2019-07-31"},
{"website": "Deepmind", "title": "How evolutionary selection can train more capable self-driving cars", "author": [" Yu-hsin Chen "], "link": "https://deepmind.com/blog/article/how-evolutionary-selection-can-train-more-capable-self-driving-cars", "abstract": "Waymo’s self-driving vehicles employ neural networks to perform many driving tasks, from detecting objects and predicting how others will behave, to planning a car's next moves. Training an individual neural net has traditionally required weeks of fine-tuning and experimentation, as well as enormous amounts of computational power. Now, Waymo, in a research collaboration with DeepMind, has taken inspiration from Darwin’s insights into evolution to make this training more effective and efficient. At a high level, neural nets learn through trial and error. A network is presented with a task, and is “graded” on whether it performs the task correctly or not. The network learns by continually attempting these tasks and adjusting itself based on its grades, such that it becomes more likely to perform correctly in the future. A network’s performance depends heavily on its training regimen. For example, a researcher can tweak how much a network adjusts itself after each task–referred to as its learning rate. The higher the learning rate, the more dramatic the adjustments. The goal is to find a learning rate high enough that the network gets better after each iteration, but not so high that the network's performance fluctuates wildly. Finding the best training regimen (or “hyperparameter schedule”) is commonly achieved through an engineer’s experience and intuition, or through extensive searching. In random search, researchers apply many random hyperparameter schedules over multiple types of hyperparameters in order to train different networks independently and in parallel–after which it’s possible to settle on the best performing model. This blog post covers how Waymo engineers apply reinforcement learning to the search for better neural net architectures. Because training numerous models in parallel is computationally expensive, researchers typically hand-tune random search by monitoring networks while they’re training, periodically culling the weakest performers and freeing resources to train new networks from scratch with new random hyperparameters. This type of manual tuning produces better results faster, but it’s labor intensive. To make this process more efficient, researchers at DeepMind devised a way to automatically determine good hyperparameter schedules based on evolutionary competition (called “Population Based Training” or PBT), which combines the advantages of hand-tuning and random search . Like random search, PBT also starts with multiple networks initiated with random hyperparameters. Networks are evaluated periodically and compete with each other for “survival” in an evolutionary fashion. If a member of the population is underperforming, it’s replaced with the “progeny” of a better performing member. The progeny is a copy of the better performing member, with slightly mutated hyperparameters. PBT doesn’t require us to restart training from scratch, because each progeny inherits the full state of its parent network, and hyperparameters are updated actively throughout training, not at the end of training. Compared to random search, PBT spends more of its resources training with good hyperparameter values. The first experiments that DeepMind and Waymo collaborated on involved training a network that generates boxes around pedestrians, bicyclists, and motorcyclists detected by our sensors–named a “region proposal network.” The aim was to investigate whether PBT could improve a neural net's ability to detect pedestrians along two measures: recall (the fraction of pedestrians identified by the neural net over total number of pedestrians in the scene) and precision (the fraction of detected pedestrians that are actually pedestrians, and not spurious “false positives”). Waymo’s vehicles detect these road users using multiple neural nets and other methods, but the goal of this experiment was to train this single neural net to maintain recall over 99%, while reducing false positives using population-based training. We learned a lot from this experiment. Firstly, we discovered that we needed to create a realistic and robust evaluation for the networks so that we’d know if a neural net would truly perform better when deployed across a variety of situations in the real world. This evaluation formed the basis of the competition that PBT employs to pick one winning neural net over another. To ensure neural nets perform well generally, and don’t simply memorise answers to examples they've seen during training, our PBT competition evaluation uses a set of examples (the \"validation set\") that is different from those used in training (the \"training set.\") To verify final performance, we also use a third set of examples (the \"evaluation set\") that the neural nets have never seen in training or competition. Secondly, we learned that we needed fast evaluation to support frequent evolutionary competition. Researchers seldom evaluate their models during training, and when they do, the evaluation is done infrequently. PBT required models be evaluated every 15 minutes. To achieve this, we took advantage of Google’s data centres to parallelise the evaluation across hundreds of distributed machines. During these experiments, we noticed that one of PBT’s strengths–allocating more resources to the progeny of better performing networks–can also be a weakness, because PBT optimises for the present and fails to consider long-term outcomes. This can be a problem because it disadvantages late-bloomers, so neural nets with hyperparameters that perform better over the long term don’t have the chance to mature and succeed. One way to combat this is to increase population diversity, which can be achieved by simply training a larger population. If the population is large enough, there is a greater chance for networks with late-blooming hyperparameters to survive and catch up in later generations. In these experiments, we were able to increase diversity by creating sub-populations called “niches,” where neural nets were only allowed to compete within their own sub-groups–similar to how species evolve when isolated on islands. We also tried to directly reward diversity through a technique called “fitness sharing,” where we measure the difference between members of the population and give more unique neural nets an edge in the competition. Greater diversity allows PBT to explore a larger hyperparameter space. PBT enabled dramatic improvements in model performance. For the experiment above, our PBT models were able to achieve higher precision by reducing false positives by 24% compared to its hand-tuned equivalent, while maintaining a high recall rate. A chief advantage of evolutionary methods such as PBT is that they can optimise arbitrarily complex metrics. Traditionally, neural nets can only be trained using simple and smooth loss functions, which act as a proxy for what we really care about. PBT enabled us to go beyond the update rule used for training neural nets, and towards the more complex metrics optimising for features we care about, such as maximising precision under high recall rates. PBT also saves time and resources. The hyperparameter schedule discovered with PBT-trained nets outperformed Waymo’s previous net with half the training time and resources. Overall, PBT uses half the computational resources used by random parallel search to efficiently discover better hyperparameter schedules.  It also saves time for researchers–by incorporating PBT directly into Waymo’s technical infrastructure, researchers from across the company can apply this method with the click of a button, and spend less time tuning their learning rates. Since the completion of these experiments, PBT has been applied to many different Waymo models, and holds a lot of promise for helping to create more capable vehicles for the road. Contributors: The work described here was a research collaboration between Yu-hsin Chen and Matthieu Devin of Waymo, and Ali Razavi, Ang Li, Sibon Li, Ola Spyra, Pramod Gupta and Oriol Vinyals of DeepMind. Advisors to the project include Max Jaderberg, Valentin Dalibard, Meire Fortunato and Jackson Broshear from DeepMind.", "date": "2019-07-25"},
{"website": "Deepmind", "title": "Open sourcing TRFL: a library of reinforcement learning building blocks", "author": [" Matteo Hessel ", " Miljan Martic ", " Diego de Las Casas ", " Gabriel Barth-Maron "], "link": "https://deepmind.com/blog/article/trfl", "abstract": "Today we are open sourcing a new library of useful building blocks for writing reinforcement learning (RL) agents in TensorFlow. Named TRFL (pronounced ‘truffle’), it represents a collection of key algorithmic components that we have used internally for a large number of our most successful agents such as DQN, DDPG and the Importance Weighted Actor Learner Architecture. A typical deep reinforcement learning agent consists of a large number of interacting components: at the very least, these include the environment and some deep network representing values or policies, but they often also include components such as a learned model of the environment, pseudo-reward functions or a replay system. These parts tend to interact in subtle ways (often not well-documented in papers, as highlighted by Henderson and colleagues ), thus making it difficult to identify bugs in such large computational graphs. A recent blog post by OpenAI highlighted this issue by analysing some of the most popular open-source implementations of reinforcement learning agents and finding that six out of 10 “had subtle bugs found by a community member and confirmed by the author”. One approach to addressing this issue, and helping those in the research community attempting to reproduce results from papers, is through open-sourcing complete agent implementations. For example, this is what we did recently with our scalable distributed implementation of the v-trace agent . These large agent codebases can be very useful for reproducing research, but also hard to modify and extend. A different and complementary approach is to provide reliable, well-tested implementations of common building blocks, that can be used in a variety of different RL agents. Moreover, having these core components abstracted away in a single library, with a consistent API, makes it simpler to combine ideas originating from various different publications. The TRFL library includes functions to implement both classical RL algorithms as well as more cutting-edge techniques.  The loss functions and other operations provided here are implemented in pure TensorFlow. They are not complete algorithms, but implementations of RL-specific mathematical operations needed when building fully-functional RL agents. For value-based reinforcement learning we provide TensorFlow ops for learning in discrete action spaces, such as TD-learning, Sarsa, Q-learning and their variants, as well as ops for implementing continuous control algorithms, such as DPG. We also include ops for learning distributional value functions. These ops support batches, and return a loss that can be minimised by feeding it to a TensorFlow Optimiser. Some losses operate over batches of transitions (e.g. Sarsa, Q learning, ...), and others over batches of trajectories (e.g. Q lambda, Retrace, …). For policy-based methods, we have utilities to easily implement both online methods such as A2C, as well as supporting off-policy correction techniques, such as v-trace. The computation of policy gradients in continuous action spaces is also supported.  Finally, TRFL also provides an implementation of the auxiliary pseudo-reward functions used by UNREAL, which we have found to improve data efficiency in a variety of domains. This is not a one-time release. Since this library is used extensively within DeepMind, we will continue to maintain it as well as add new functionalities over time. We are also eager to receive contributions to the library by the wider RL community. This library was created by the Research Engineering team at DeepMind.", "date": "2018-10-17"},
{"website": "Deepmind", "title": "Episode 7: Towards the future", "author": "Unknown", "link": "https://deepmind.com/blog/article/podcast-episode-7-towards-the-future", "abstract": "AI researchers around the world are trying to create a general purpose learning system that can learn to solve a broad range of problems without being taught how. Koray Kavukcuoglu, DeepMind’s Director of Research, describes the journey to get there, and takes Hannah on a whistle-stop tour of DeepMind’s HQ and its research. Interviewees: Koray Kavukcuoglu, Director of Research; Trevor Back, Product Manager for DeepMind’s science research; research scientists Raia Hadsell and Murray Shanahan; and DeepMind CEO and co-founder, Demis Hassabis. Listen: The tour of DeepMind Listen to this episode and subscribe to the whole series on Apple podcasts , Google podcasts , Spotify , Deezer or your favourite podcast app by searching for “DeepMind: The Podcast”. Find out more about the themes in this episode: If you know of other resources we should link to, please help other listeners by either replying to us on Twitter (#DMpodcast) or emailing us at podcast@deepmind.com. You can also use that address to send us questions or feedback on the series. Credits: Presenter: Hannah Fry Editor: David Prest Senior Producer: Louisa Field Producers: Amy Racs, Dan Hardoon Binaural Sound: Lucinda Mason-Brown Music composition: Eleni Shaw (with help from Sander Dieleman and WaveNet )", "date": "2019-09-10"},
{"website": "Deepmind", "title": "Episode 2: Go to Zero", "author": "Unknown", "link": "https://deepmind.com/blog/article/podcast-episode-2-go-to-zero", "abstract": "In March 2016, more than 200 million people watched AlphaGo become first computer program to defeat a professional human player at the game of Go, a milestone in AI research that was considered to be a decade ahead of its time. Since then the team has continued to develop the system and recently unveiled  AlphaZero: a program that has taught itself how to play chess, Go, and shogi. Hannah explores the inside story of both with Lead Researcher David Silver and finds out why games are a useful proving ground for AI researchers. She also meets Chess Grandmaster Matthew Sadler and women’s international master Natasha Regan, who have written a book on AlphaZero and its unique gameplay. Interviewees: DeepMind CEO Demis Hassabis, Matthew Sadler, chess Grandmaster; Lead Researcher David Silver, Matt Botvinick, Director of Neuroscience Research; and Natasha Regan, women’s international chess master. Listen: What's it like to play AlphaZero? Listen to this episode and subscribe to the whole series on Apple podcasts , Google podcasts , Spotify , Deezer or your favourite podcast app by searching for “DeepMind: The Podcast”. Find out more about the themes in this episode: If you know of other resources we should link to, please help other listeners by either replying to us on Twitter (#DMpodcast) or emailing us at podcast@deepmind.com. You can also use that address to send us questions or feedback on the series. Credits: Presenter: Hannah Fry Editor: David Prest Senior Producer: Louisa Field Producers: Amy Racs, Dan Hardoon Binaural Sound: Lucinda Mason-Brown Music composition: Eleni Shaw (with help from Sander Dieleman and WaveNet )", "date": "2019-08-18"},
{"website": "Deepmind", "title": "DeepMind papers at ICML 2017 (part two)", "author": "Unknown", "link": "https://deepmind.com/blog/article/deepmind-papers-icml-2017-part-two", "abstract": "The second of  our three-part series, which gives an overview of the papers we are presenting at the ICML 2017 Conference in Sydney, Australia. Authors: Ian Osband, Benjamin Van Roy Computational results demonstrate that posterior sampling for reinforcement learning (PSRL) dramatically outperforms existing algorithms driven by optimism, such as UCRL2. We provide insight into the extent of this performance boost and the phenomenon that drives it. We leverage this insight to establish an $\\tilde{O}(H\\sqrt{SAT})$ Bayesian regret bound for PSRL in finite-horizon episodic Markov decision processes. This improves upon the best previous Bayesian regret bound of $\\tilde{O}(H S \\sqrt{AT})$ for any reinforcement learning algorithm. Our theoretical results are supported by extensive empirical evaluation. For further details and related work, please see the paper . Check it out at ICML: Monday 07 August, 11:42-12:00 @ C4.5 (Talk) Monday 07 August, 18:30-22:00  @ Gallery #36 (Poster) Authors: Irina Higgins*, Arka Pal*, Andrei Rusu, Loic Matthey, Chris Burgess, Alexander Pritzel, Matt Botvinick, Charles Blundell, Alexander Lerchner Modern deep reinforcement learning agents rely on large quantities of data to learn how to act. In some scenarios, such as robotics, obtaining a lot of training data may be infeasible. Hence such agents are often trained on a related task where data is easy to obtain (e.g. simulation) with the hope that the learnt knowledge will generalise to the task of interest (e.g. reality). We propose DARLA, a DisentAngled Representation Learning Agent, that exploits its interpretable and structured vision to learn how to act in a way that is robust to various novel changes in its environment - including a simulation to reality transfer scenario in robotics. We show that DARLA significantly outperforms all baselines, and that its performance is crucially dependent on the quality of its vision. Check it out at ICML: Monday 07 August, 16:42-17:00 @ C4.5 (Talk)\\ Monday 07 August, 18:30-22:00 @ Gallery #123 (Poster) Authors: Alex Graves, Marc G. Bellemare, Jacob Menick, Koray Kavukcuoglu, Remi Munos As neural networks are applied to ever more complex problems, the need for efficient curriculum learning becomes more pressing. However, designing effective curricula is difficult and typically requires a large amount of hand-tuning. This paper uses reinforcement learning to automate the path, or syllabus, followed by the network through the curriculum so as to maximise the overall rate of learning progress. We consider nine different progress indicators, including a novel class of complexity-gain signal. Experimental results on three problems show that an automatically derived syllabus can lead to efficient curriculum learning, even on data (such as the bAbI tasks) that were not explicitly designed for curriculum learning. For further details and related work, please see the paper . Check it out at ICML: Monday 07 August, 16:42-17:00 @ C4.6 & C4.7 (Talk) Monday 07 August, 18:30-20:00 @ Gallery #127 (Poster) Authors: Yutian Chen, Matthew Hoffman, Sergio Gomez, Misha Denil, Timothy Lillicrap, Matthew Botvinick , Nando de Freitas We learn recurrent neural network optimisers trained on simple synthetic functions by gradient descent. The learned optimisers exhibit a remarkable degree of transfer in that they can be used to efficiently optimise a broad range of derivative-free black-box problems, including continuous bandits, control problems, global optimization benchmarks and hyper-parameter tuning tasks. For further details and related work, please see the paper . Check it out at ICML: Monday 07 August, 17:15-17:33 @ Darling Harbour Theatre (Talk) Tuesday 08 August, 18:30-22:00 @ Gallery #6 (Poster) Authors: Marc G. Bellemare*, Will Dabney*, Remi Munos We argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman's equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting. For further details and related work, please see the blog post and the paper . Check it out at ICML: Monday 07 August, 17:33-17:51 @ C4.5 (Talk) Tuesday 08 August, 18:30-22:00 @ Gallery #13 (Poster) Authors: Marlos Machado (Univ. Alberta), Marc G. Bellemare, Michael Bowling Representation learning and option discovery are two of the biggest challenges in reinforcement learning (RL). Proto-value functions (PVFs) are a well-known approach for representation learning in MDPs. In this paper we address the option discovery problem by showing how PVFs implicitly define options. We do it by introducing eigenpurposes, intrinsic reward functions derived from learned representations. The options discovered from eigenpurposes traverse the principal directions of the state space. They are useful for multiple tasks because they are discovered without taking the environment’s rewards into consideration. Moreover, different options act at different time scales, making them helpful for exploration. We demonstrate features of eigenpurposes in traditional tabular domains as well as in Atari 2600 games. For further details and related work, please see the paper . Check it out at ICML: Monday 07 August, 18:09-18:27 @ C4.5 (Talk) Tuesday 08 August 18:30-20:00 @ Gallery #23 (Poster) Authors: Sander Dieleman, Karen Simonyan, Jesse Engel (Google Brain), Cinjon Resnick (Google Brain), Adam Roberts (Google Brain), Douglas Eck (Google Brain), Mohammad Norouzi (Google Brain) In this paper, we introduce a powerful new WaveNet-style autoencoder model that conditions an autoregressive decoder on temporal codes learned from the raw audio waveform. We also introduce NSynth, a large-scale and high-quality dataset of musical notes that is an order of magnitude larger than comparable public datasets. Using NSynth, we demonstrate improved qualitative and quantitative performance of the WaveNet autoencoder over a well-tuned spectral autoencoder baseline. Finally, we show that the model learns a manifold of embeddings that allows for morphing between instruments, meaningfully interpolating in timbre to create new types of sounds that are realistic and expressive. For further details and related work, please see the paper . Check it out at ICML: Tuesday 08 August, 14:42-15:00 @ Parkside 1 (Talk) Tuesday 08 August, 18:30-22:00 @ Gallery #98 (Poster) Read: ' DeepMind papers at ICML 2017 (part one) ' and ' DeepMind papers at ICML 2017 (part three)", "date": "2017-08-04"},
{"website": "Deepmind", "title": "Advancing sports analytics through AI research", "author": [" Karl Tuyls ", " Shayegan Omidshafiei ", " Daniel Hennes ", " Jerome Connor ", " Zhe Wang ", " Adria Recasens Continente "], "link": "https://deepmind.com/blog/article/advancing-sports-analytics-through-ai", "abstract": "Creating testing environments to help progress AI research out of the lab and into the real world is immensely challenging. Given AI’s long association with games, it is perhaps no surprise that sports presents an exciting opportunity, offering researchers a testbed in which an AI-enabled system can assist humans in making complex, real-time decisions in a multiagent environment with dozens of dynamic, interacting individuals. The rapid growth of sports data collection means we are in the midst of a remarkably important era for sports analytics. The availability of sports data is increasing in both quantity and granularity, transitioning from the days of aggregate high-level statistics and sabermetrics to more refined data such as event stream information (e.g., annotated passes or shots), high-fidelity player positional information, and on-body sensors . However, the field of sports analytics has only recently started to harness machine learning and AI for both understanding and advising human decision-makers in sports. In our recent paper published in collaboration with Liverpool Football Club (LFC) in JAIR, we envision the future landscape of sports analytics using a combination of statistical learning, video understanding, and game theory. We illustrate football, in particular, is a useful microcosm for studying AI research, offering benefits in the longer-term to decision-makers in sports in the form of an automated video-assistant coach (AVAC) system (Figure 1(A)). In comparison to some other sports, football has been rather late with starting to systematically collect large sets of data for scientific analytics purposes aiming to progress teams’ gameplay. This is for several reasons, with the most prominent being that there are far less controllable settings of the game compared to other sports (large outdoor pitch, dynamic game, etc.), and also the dominant credo to rely mainly on human specialists with track records and experience in professional football. On these lines, Arrigo Sacchi, a successful Italian football coach and manager who never played professional football in his career, responded to criticism over his lack of experience with his famous quote when becoming a coach at Milan in 1987: “I never realised that to be a jockey you had to be a horse first.” Football Analytics poses challenges that are well suited for a wide variety of AI techniques, coming from the intersection of 3 fields: computer vision, statistical learning and game theory (visualised in Figure 2). While these fields are individually useful for football analytics, their benefits become especially tangible when combined: players need to take sequential decision-making in the presence of other players (cooperative and adversarial) and as such game theory, a theory of interactive decision making, becomes highly relevant. Moreover, tactical solutions to particular in-game situations can be learnt based on in-game and specific player representations, which makes statistical learning a highly relevant area. Finally, players can be tracked and game scenarios can be recognised automatically from widely-available image and video inputs. The AVAC system we envision is situated within the microcosm that is formed by the intersection of these three research fields (Figure 2). In our research in this exciting domain, we not only lay out a roadmap for scientific and engineering problems that can be tackled for years to come, but we also present new original results at the crossroads of game theoretic analysis, statistical learning, and computer vision to illustrate what this exciting area has to offer to football. Game theory plays an important role in the study of sports, enabling theoretical grounding of players’ behavioral strategies. In the case of football, many of its scenarios can actually be modeled as zero-sum games, which have been studied extensively since the inception of game theory. For example, here we model the penalty kick situation as a two-player asymmetric game, where the kicker’s strategies may be neatly categorised as left, center, or right shots. To study this problem, we augment game-theoretic analysis in the penalty kick scenario with Player Vectors , which summarise the playing styles of individual football players. With such representations of individual players, we are able to group kickers with similar playing styles, and then conduct game-theoretic analysis on the group-level (Figure 3). Our results show that the identified shooting strategies of different groups are statistically distinct. For example, we find that one group prefers to shoot to the left corner of the goal mouth, while another tends to shoot to the left and right corners more evenly. Such insights may help goalkeepers diversify their defense strategies when playing against different types of players. Building on this game-theoretic view, one can consider the durative nature of football by analysing it in the form of temporally-extended games, use this to advise tactics to individual players, or even go further to optimise the overall team strategy. On the side of statistical learning, representation learning has yet to be fully exploited in sports analytics, which would enable informative summarisation of the behavior of individual players and football teams. Moreover, we believe that the interaction between game theory and statistical learning would catalyse advances in sports analytics further. In the above penalty kick scenario, for instance, augmenting the analysis with player-specific statistics (Player Vectors) provided deeper insights into how various types of players behave or make decisions about their actions in the penalty kick scenario. As another example of this, one can study ' ghosting ', which refers to a particular data driven analysis of how players should have acted in hindsight in sports analytics (which bears connections to the notion of regret in online learning and game theory). The ghosting model suggests alternative player trajectories for a given play, e.g., based on the league average or a selected team. Predicted trajectories are usually visualised as a translucent layer over the original play, hence the term 'ghosting' (see Figure 4 for a visual example). Generative trajectory prediction models allow us to gain insights by analysing key situations of a game and how they might have played out differently. These models also bear potential in predicting the implications of a tactical change, a key player's injury, or substitution on the own team's performance along with the opposition's response to such a change. Finally, we consider computer vision to be one of the most promising avenues for advancing the boundaries of state of the art sports analytics research. By detecting events purely from video, a topic that has been well-studied in the computer vision community (e.g., see the following survey and our paper for additional references), the potential range of application is enormous. By associating events with particular frames, videos become searchable and ever more useful (e.g., automatic highlight generation becomes possible). Football video, in turn, offers an interesting application domain for computer vision. The large numbers of football videos satisfies a prerequisite for modern AI techniques. While each football video is different, the settings do not vary greatly, which makes the task ideal for sharpening AI algorithms. Third-party providers also exist to furnish hand-labelled event data that can be useful in training video models and are time consuming to generate, so both supervised and unsupervised algorithms can be used for football event detection. Figure 1(B), for example, provides a stylised visualisation of a deep learning model trained with supervised methods to recognise target events (e.g., kicks) purely from video. The application of advanced AI techniques to football has the potential to revolutionise the game across many axes, for players, decision-makers, fans, and broadcasters. Such advances will also be important as they also bear potential to further democratise the sport itself (e.g., rather than relying on judgement calls from in-person scouts/experts, one may use techniques such as computer vision to quantify skillsets of players from under-represented regions, those from lower-level leagues, etc.). We believe that the development of increasingly advanced AI techniques afforded by the football microcosm might be applicable to broader domains. To this end, we are co-organising (with several external organisers) an IJCAI 2021 workshop on AI for Sports Analytics later this year, which we welcome interested researchers to attend. For researchers interested in this topic, publicly available datasets have been made available both by analytics companies such as StatsBomb ( dataset link ) and the wider research community ( dataset link ). Furthermore, the paper provides a comprehensive overview of research in this domain. Paper and related links: Work done as a collaboration with contributors: Karl Tuyls, Shayegan Omidshafiei, Paul Muller, Zhe Wang, Jerome Connor, Daniel Hennes, Ian Graham, William Spearman, Tim Waskett, Dafydd Steele, Pauline Luc, Adria Recasens, Alexandre Galashov, Gregory Thornton, Romuald Elie, Pablo Sprechmann, Pol Moreno, Kris Cao, Marta Garnelo, Praneet Dutta, Michal Valko, Nicolas Heess, Alex Bridgland, Julien Perolat, Bart De Vylder, Ali Eslami, Mark Rowland, Andrew Jaegle, Yi Yang, Remi Munos, Trevor Back, Razia Ahamed, Simon Bouton, Nathalie Beauguerlange, Jackson Broshear, Thore Graepel, and Demis Hassabis.", "date": "2021-05-07"},
{"website": "Deepmind", "title": "Our collaborations with academia to advance the field of AI", "author": [" Demis Hassabis "], "link": "https://deepmind.com/blog/article/our-collaborations-academia-advance-field-ai", "abstract": "When I was studying in the mid-90s as an undergraduate, there was very little active engagement between the academic communities pushing the boundaries of maths and science, and the industries that many students ended up going into, such as finance. This struck me as a missed opportunity. While private institutions benefited from the technological advances being driven by university researchers, the subsequent breakthroughs they made were rarely shared for mutual benefit between the two. In contrast, we often talk about DeepMind’s research environment as a hybrid culture that blends the long-term scientific thinking of academia with the speed and focus of the best start-ups. This alignment with academia has always been important to us personally, given how many of our team come from that background, as well as the fact that many of the core ideas behind machine learning were invented and developed by academic pioneers including the likes of Geoff Hinton and Rich Sutton. This is a major reason why we openly publish our research - including over 100 peer-reviewed papers to date - and regularly present at industry-wide gatherings such as NIPS . Last month in Barcelona we published 20 papers, participated in 42 poster sessions, gave 21 talks, and  open-sourced our flagship DeepMind Lab research platform - and there’s a lot more to come. We also want to make a more direct contribution to academic learning and training the next generation of machine learning practitioners, and so, starting this month, we’ll be running a state-of-the-art Masters level training module called Advanced Topics in Machine Learning with University College London’s (UCL) Department of Computer Science. Led by DeepMind’s Thore Graepel, other invited speakers will include leading researchers spanning areas such as deep learning, reinforcement learning, natural language understanding and others. Hado van Hasselt, Joseph Modayil, Koray Kavukcuoglu, Raia Hadsell, James Martens, Oriol Vinyals, Shakir Mohamed, Simon Osindero, Ed Grefenstette and Karen Simonyan will be joined by Volodymyr Mnih, David Silver and Alex Graves - who are also some of the first authors of DeepMind’s three Nature papers. January also sees the start of our Deep Learning for Natural Language Processing advanced course at the University of Oxford’s Department of Computer Science. This applied course, focusing on recent advances in analysing and generating speech and text using recurrent neural networks, is led by Phil Blunsom in partnership with DeepMind’s Language Research Group, and open to fourth year undergraduates, Masters, and first year DPhil (PhD) students. Both of these courses run in addition to the international summer schools that our team members regularly teach at, with events taking place this year in Germany , China and South Africa among other locations. We also make sure that people who come to work here can continue to make their own personal contribution to academia. A number of our team are also affiliated with various institutions including UCL, Oxford, Cambridge, MIT and the universities of Freiburg and Lille, among others. Finally, we think it’s important for the field that there are as many thriving independent academic institutions as possible. That’s why we’re providing sponsorship for several research labs and their PhD students to pursue their own research priorities in whichever way they choose, including the University of Alberta, University of Montreal, University of Amsterdam, Gatsby Unit at UCL, NYU and Oxford, and others. We see the links between company research labs and academia as central to the future of AI. By continuing to share talent, expertise and breakthroughs - not just on technical subjects, but also on the broader set of questions around ethics, safety and societal impact - we believe we’ll all make better progress in the development of artificial intelligence and its application for positive social benefit.", "date": "2017-01-23"},
{"website": "Deepmind", "title": "Open-sourcing DeepMind Lab", "author": [" Charlie Beattie ", " Joel Leibo ", " Stig Petersen ", " Shane Legg "], "link": "https://deepmind.com/blog/article/open-sourcing-deepmind-lab", "abstract": "DeepMind's scientific mission is to push the boundaries of AI, developing systems that can learn to solve any complex problem without needing to be taught how. To achieve this, we work from the premise that AI needs to be general. Agents should operate across a wide range of tasks and be able to automatically adapt to changing circumstances. That is, they should not be pre-programmed, but rather, able to learn automatically from their raw inputs and reward signals from the environment. There are two parts to this research program: (1)  designing ever-more intelligent agents capable of more-and-more sophisticated cognitive skills, and (2) building increasingly complex environments where agents can be trained and evaluated. The development of innovative agents goes hand in hand with the careful design and implementation of rationally selected, flexible and well-maintained environments. To that end, we at DeepMind have invested considerable effort toward building rich simulated environments to serve as  “laboratories” for AI research. Now we are open-sourcing our flagship platform,  DeepMind Lab, so the broader research community can make use of it. DeepMind Lab is a fully 3D game-like platform tailored for agent-based AI research. It is observed from a first-person viewpoint, through the eyes of the simulated agent. Scenes are rendered with rich science fiction-style visuals. The available actions allow agents to look around and move in 3D. The agent’s “body” is a floating orb. It levitates and moves by activating thrusters opposite its desired direction of movement, and it has a camera that moves around the main sphere as a ball-in-socket joint tracking the rotational look actions. Example tasks include collecting fruit, navigating in mazes, traversing dangerous passages while avoiding falling off cliffs, bouncing through space using launch pads to move between platforms, playing laser tag, and quickly learning and remembering random procedurally generated environments. An illustration of how agents in DeepMind Lab perceive and interact with the world can be seen below: Artificial general intelligence research in DeepMind Lab emphasizes navigation, memory, 3D vision from a first person viewpoint, motor control, planning, strategy, time, and fully autonomous agents that must learn for themselves what tasks to perform by exploring their environment. All these factors make learning difficult. Each are considered frontier research questions in their own right. Putting them all together in one platform, as we have, represents a significant new challenge for the field. DeepMind Lab is highly customisable and extendable. New levels can be authored with off-the-shelf editor tools. In addition, DeepMind Lab includes an interface for programmatic level-creation. Levels can be customised with gameplay logic, item pickups, custom observations, level restarts, reward schemes, in-game messages and more. The interface can be used to create levels in which novel map layouts are generated on the fly while an agent trains. These features are useful in, for example, testing how an agent copes with unfamiliar environments. Users will be able to add custom levels to the platform via GitHub. The assets will be hosted on GitHub alongside all the code, maps and level scripts. Our hope is that the community will help us shape and develop the platform going forward. DeepMind Lab has been used internally at DeepMind for some time ( example ). We believe it has already had a significant impact on our thinking concerning numerous aspects of intelligence, both natural and artificial. However, our efforts so far have only barely scratched the surface of what is possible in DeepMind Lab. There are opportunities for significant contributions still to be made in a number of mostly still untouched research domains now available through DeepMind Lab, such as navigation, memory and exploration. As well as facilitating agent evaluation, there are compelling reasons to think that it may be fundamentally easier to develop intelligence in a 3D world, observed from a first-person viewpoint, like DeepMind Lab. After all, the only known examples of general-purpose intelligence in the natural world arose from a combination of evolution, development, and learning, grounded in physics and the sensory apparatus of animals. It is possible that a large fraction of animal and human intelligence is a direct consequence of the richness of our environment, and unlikely to arise without it. Consider the alternative: if you or I had grown up in a world that looked like Space Invaders or Pac-Man, it doesn’t seem likely we would have achieved much general intelligence! Read the full paper here Access DeepMind's GitHub repository here .", "date": "2016-12-03"},
{"website": "Deepmind", "title": "A new model and dataset for long-range memory", "author": [" Jack Rae ", " Timothy Lillicrap "], "link": "https://deepmind.com/blog/article/A_new_model_and_dataset_for_long-range_memory", "abstract": "This blog introduces a new long-range memory model, the Compressive Transformer , alongside a new benchmark for book-level language modelling, PG19 . We provide the conceptual tools needed to understand this new research in the context of recent developments in memory models and language modelling. Throughout our lives, we build up memories that are retained over a diverse array of timescales, from minutes to months to years to decades. When reading a book, we can recall characters who were introduced many chapters ago, or in an earlier book in a series, and reason about their motivations and likely actions in the current context. We can even put the book down during a busy week, and pick up from where we left off without forgetting the plotline. We do not achieve such feats by storing every detail of sensory input we receive about the world throughout our lifetimes. Our brains select, filter, and integrate input stimuli based on factors of relevance, surprise, perceived danger, and repetition. In other words, we compress lifelong experience to a set of salient memories which help us understand the past, and better anticipate the future. A major goal of AI researchers is discovering ways of implementing such abilities in computational systems and benchmarks which require complex reasoning over long time-spans. Memory systems for artificial neural networks have advanced considerably in the past two decades. In this post, we look to past advances to explore why this is such a difficult task and consider how natural language modelling could offer an effective means of designing better long range memory systems? We reflect on the necessity for better compressive memory architectures, and sparse memory access mechanisms, to work towards the goal of incorporating lifelong reasoning in our computational systems. One of the earliest and most widely-used memory architectures in present day is a recurrent neural network (RNN) called the Long Short-Term-Memory (LSTM). The LSTM maintains a compact memory in the form of a vector of numbers, which it accesses and modifies with gated read, write, and forget operations. It was originally developed on a suite of synthetic tasks that involved learning logical operations on a stream of bits. However, it has since become a ubiquitous model of sequential data: from recognising handwritten notes to predicting the early onset of kidney injury. One weakness of the LSTM, and of many contemporary RNNs, is capacity. They are designed so that each unit of memory can influence every other unit in memory with a learnable weight. But this results in a computationally inefficient system:  the number of learnable parameters in the model grows quadratically with the memory size. For example, an LSTM with a memory of size 64KB results in parameters of size 8GB. Circumventing this memory capacity bottleneck has been an active research area. Researchers at DeepMind proposed a novel architecture, the Differentiable Neural Computer (DNC), which augments an LSTM with a much larger memory matrix to address these deficits. The DNC uses an attention operation to read from this memory matrix. In visual attention, our eyes are drawn by pertinent objects in a visual scene–for example, one might typically spend more time observing a friend’s face during an emotional conversation than on noticing their shoes. Here, memory models can attend to particular events/data in the past. This attention operation requires a fixed number of parameters, independent of the memory size, and so the memory capacity of the model can be significantly increased. Alongside the DNC, recurrent neural networks with an additional attention mechanism were showing promise in the domains of translation and question answering . These models were able to reason over time using two memory structures: a small and compact LSTM memory and a large external memory. However, more recently researchers at Google Brain Team proposed the Transformer which removes the LSTM, and only uses attention to transmit information across time . The Transformer was originally shown to significantly outperform recurrent neural networks for machine translation. However it has since been applied to a range of applications in natural language processing, from question answering, document summarisation, sentiment classification and the modelling of natural language – a task that has seen particular exciting developments over the past year. Finding machine learning tasks which both drive the development of better memory architectures and push us further towards artificial general intelligence is challenging. Statistical language modelling is one such task that we believe could be valuable for both purposes. Language models work by sequentially predicting the next word in a stream of text. They can be used to model existing texts and also to generate novel texts. As they get better at modelling the past, their predictions become more accurate, and the texts they generate become more realistic. In Claude Shannon’s seminal article “ A Mathematical Theory of Communication ” published in 1948, which founded the field of information theory, he discussed primitive language models and illustrated how adding more context improves the quality and realism of generated text. He does this by introducing the most simple model of English text, which has no contextual modelling at all –  a character-level model which treats each character independently. By sampling characters with their relative frequencies (8% of the time for ‘a’, 1.5% for ‘b’ etc.) we arrive with a nonsensical string : However, he remarks at the improvement in sample quality if one instead models the probability of words independently. Now the modelled context is approximately 7X larger (the average number of characters in a word): By modelling the probability of word pairs, a further 2X in context length, even more realistic text emerges: In other words, an increase in the length of context leads to an improvement in the quality of text generated. Shannon remarks on the quality of his produced samples and conjectures that natural text samples may emerge from a sufficiently complex statistical model, “The particular sequence of ten words “attack on an English writer that the character of this” is not at all unreasonable. It appears then that a sufficiently complex stochastic process will give a satisfactory representation of a discrete source” . One criticism of language modelling as a task for long-range reasoning is that models can capture a large portion of their predictions from the local context. Neural language models have traditionally ignored the wider context, focusing mostly on the short term. For example, in 2017 Dailuk et al. found their neural language model rarely attends beyond the preceding five words. However in the past year large Transformer models have been shown to make use of hundreds of words of context to generate ever-more realistic text with a longer range of coherence. A demo from OpenAI’s GPT-2 , a 1.5B parameter Transformer, indicate that the model is able to generate realistic text and retain key entities (e.g. Dr Jorge Pérez and unicorns) across multiple paragraphs: Such samples would likely astound Shannon, 70 years on from his early language model experiments. However the real benefit of powerful neural language models – and their relevance to the goal of AGI – is their ability to transfer knowledge to a suite of tasks. In the process of learning how to model text, neural language models appear to build up a knowledge-base of associations, and a plethora of skills. For instance, researchers at OpenAI showed that GPT-2 can be applied to natural-language processing tasks such as question answering, paraphrasing, or sentiment analysis with surprisingly good performance – especially for a model that has never been explicitly trained to perform such tasks. When large Transformer language models are fine-tuned on particular tasks such as question answering, the resulting performance is significantly better than models that were designed and trained solely for question answering. Google’s prominent natural language model, BERT , achieves state-of-the-art performance on a wide array of NLP benchmarks, and is now a part of Google Search . And more recently, it was shown that GPT-2 can learn to play rudimentary chess by training it on strings of game moves . A popular long-range language model benchmark is WikiText-103 , which is comprised of English-language Wikipedia articles, and was developed by researchers at Salesforce AI . Articles are around 3,600 words on average, which, at the time of creation, was far beyond the memory window of state-of-the-art models. However researchers at Google recently showed that a Transformer variant called the TransformerXL – which maintains a memory of past network activations and recently obtained state-of-the-art results on WikiText-103 – can make use of contexts spanning over one thousand words . This raises the question: will models soon saturate these benchmarks? As such, we’ve compiled and released a new, longer-range language model benchmark based on books. To support growing interest in long-range sequence models, we are releasing a new language modelling benchmark, PG-19 , which is derived from books in the Project Gutenberg online library . Books provide a rich context for the development of long-range memory models.  We selected a subset of approximately 28,000 books from Project Gutenberg published before 1919. Unlike prior language modeling dataset releases, we apply very little pre-processing to the text. For example, we do not limit the vocabulary size of the data or censor numbers, to avoid the filtering of useful information. PG-19 is over double the size of prior language modelling benchmarks, such as the Billion Word Benchmark , and contains text that is over 10X longer in context than the prior long-range language model benchmark, WikiText-103. We provide a comparative table of existing language modelling benchmarks, below: Alongside a new benchmark, we propose a long-range memory model called the Compressive Transformer . We take inspiration from the role of sleep in the formation of consolidated episodic memories . Sleep is known to be crucial for memory, and it’s thought that sleep serves to compress and consolidate memories, thereby improving reasoning abilities for memory tasks. In the Compressive Transformer, granular memories akin to episodic memories are collected online as the model passes over a sequence of inputs; over time, they are eventually compacted. The Compressive Transformer uses attention to select information from the past, like the Transformer. It maintains a short-term memory of past activations, in the same style as the recently-proposed TransformerXL . Where the TransformerXL discards past activations when they become older, the Compressive Transformer instead compacts them into a compressed memory .  The compression is performed by a neural network guided by an auxiliary loss that guides it to keep around task-relevant information. It can learn to filter out irrelevant memories, as well as combine memories so that the salient information is preserved and retrievable over a longer period of time. We find the Compressive Transformer has state-of-the-art performance in the modelling of natural language for two widely-used long-range benchmarks, WikiText-103 and Enwik8, compared to published results that do not use additional sources of training data. We also show it can be used effectively to model speech, handles rare words especially well, and can be used within a reinforcement learning agent to solve a memory task. We find the Compressive Transformer produces the largest performance gain in modelling long-context book text from the PG-19 benchmark. The model’s conditional samples can be used to write book-like extracts. Below we show a sample that is fed a paragraph of text to be used as context, taken from “The Patrol of the Sun Dance” by Ralph Connor, which the model has not previously seen. Context from The Patrol of the Sun Dance Trail by Ralph Connor Continuation by the Compressive Transformer The Compressive Transformer is able to produce narrative in a variety of styles, from multi-character dialogue, first-person diary entries, or third-person prose. Although the model does not have an understanding of language that’s grounded in the real world, or the events that take place in it – by capturing longer-range correlations, we see the emergence of more coherent text. As we strive to create agents that operate over days, weeks or even years, it will be impractical to compute over all raw input data at each timestep. Even with the current growth in computing power, we will need to develop compressive and sparse architectures for memory to build representations and reason about actions. Models which are able to capture relevant correlations across days, months, or years’ worth of experience are on the horizon. We believe the route to more powerful reasoning over time will emerge from better selective attention of the past, and more effective mechanisms to compress it. As we explore ideas in this space, we need tasks and datasets that span longer and longer time intervals. The PG-19 dataset can help researchers move in this direction, presenting textual data in the longest form that we typically consume as humans: full-length books. We hope that its release will spur interest in new models that compress the past in order to predict the future and act effectively in the present.", "date": "2020-02-10"},
{"website": "Deepmind", "title": "Episode 1: AI and neuroscience - The virtuous circle", "author": "Unknown", "link": "https://deepmind.com/blog/article/podcast-episode-1-ai-and-neuroscience-the-virtuous-circle", "abstract": "What can the human brain teach us about AI? And what can AI teach us about our own intelligence? These questions underpin a lot of AI research. In the first episode of the DeepMind podcast , Hannah meets the DeepMind Neuroscience team to explore these connections and discovers how our brains are like birds’ wings, what training a dog and an AI agent have in common, and why the simplest things for people to do are, paradoxically, often the hardest for machines. Interviewees: Deepmind CEO and co-founder, Demis Hassabis; Matt Botvinick, Director of Neuroscience Research; research scientists Jess Hamrick and Greg Wayne; and Director of Research Koray Kavukcuoglu. Listen: The brain as inspiration Listen to this episode and subscribe to the whole series on Apple podcasts , Google podcasts , Spotify , Deezer or your favourite podcast app by searching for “DeepMind: The Podcast”. Find out more about the themes in this episode: If you know of other resources we should link to, please help other listeners by either replying to us on Twitter (#DMpodcast) or emailing us at podcast@deepmind.com. You can also use that address to send us questions or feedback on the series. Credits: Presenter: Hannah Fry Editor: David Prest Senior Producer: Louisa Field Producers: Amy Racs, Dan Hardoon Binaural Sound: Lucinda Mason-Brown Music composition: Eleni Shaw (with help from Sander Dieleman and WaveNet )", "date": "2019-08-17"},
{"website": "Deepmind", "title": "Learning through human feedback", "author": [" Jan Leike ", " Miljan Martic ", " Shane Legg "], "link": "https://deepmind.com/blog/article/learning-through-human-feedback", "abstract": "We believe that Artificial Intelligence will be one of the most important and widely beneficial scientific advances ever made, helping humanity tackle some of its greatest challenges, from climate change to delivering advanced healthcare. But for AI to deliver on this promise, we know that the technology must be built in a responsible manner and that we must consider all potential challenges and risks. That is why DeepMind co-founded initiatives like the Partnership on AI to Benefit People and Society and why we have a team dedicated to technical AI Safety. Research in this field needs to be open and collaborative to ensure that best practices are adopted as widely as possible, which is why we are also collaborating with OpenAI on research in technical AI Safety . One of the central questions in this field is how we allow humans to tell a system what we want it to do and - importantly - what we don’t want it to do. This is increasingly important as the problems we tackle with machine learning grow more complex and are applied in the real world. The first results from our collaboration demonstrate one method to address this, by allowing humans with no technical experience to teach a reinforcement learning (RL) system - an AI that learns by trial and error - a complex goal. This removes the need for the human to specify a goal for the algorithm in advance. This is an important step because getting the goal even a bit wrong could lead to undesirable or even dangerous behaviour. In some cases, as little as 30 minutes of feedback from a non-expert is enough to train our system, including teaching it entirely new complex behaviours, such as how to make a simulated robot do backflips. The system - described in our paper Deep Reinforcement Learning from Human Preferences - departs from classic RL systems by training the agent from a neural network known as the ‘reward predictor’, rather than rewards it collects as it explores an environment. It consists of three processes running in parallel: This iterative approach to learning means that a human can spot and correct any undesired behaviours, a crucial part of any safety system. The design also does not put an onerous burden on the human operator, who only has to review around 0.1% of the agent’s behaviour to get it to to do what they want. However, this can mean reviewing several hundred to several thousand pairs of clips, something that will need to be reduced to make it applicable to real world problems. In the Atari game Enduro, which involves steering a car to overtake a line of others and is very difficult to learn by the trial and error techniques of a traditional RL network, human feedback eventually allowed our system to achieve superhuman results. In other games and simulated robotics tasks, it performed comparably to a standard RL set-up, while in a couple of games like Qbert and Breakout it failed to work at all. But the ultimate purpose of a system like this is to allow humans to specify a goal for the agent, even if it is not present in the environment. To test this, we taught agents various novel behaviours such as performing a backflip, walking on one leg or learning to driving alongside another car in Enduro, rather than overtake to maximise the game score. Although these tests showed some positive results, others showed its limitations. In particular, our set-up was susceptible to reward hacking - or gaming its reward function - if human feedback was discontinued early in the training. In this scenario, the agent continues to explore its environment, meaning the reward predictor is forced to estimate rewards for situations it has received no feedback on. This can lead it to overpredict the reward, incentivising the agent to learn the wrong - often strange - behaviours. An example can be seen in the video below, where the agent has found that hitting the ball back and forth is a better strategy than winning or losing a point. Understanding flaws like these is crucial to ensure we avoid failures and build AI systems that behave as intended. There is still more work to be done to test and enhance this system, but already it shows a number of critical first steps in producing systems that can be taught by non-expert users, are economical with the amount of feedback they need, and can be scaled to a variety of problems. Other areas of exploration could include reducing the amount of human feedback needed or giving humans the ability to give feedback through a natural language interface. This would mark a step-change in creating a system that can easily learn from the complexity of human behaviour, and a crucial step towards creating  AI that works with and for all of humanity. This research was done as part of an ongoing collaboration between Jan Leike, Miljan Martic, and Shane Legg at DeepMind and Paul Christiano, Dario Amodei, and Tom Brown at OpenAI.", "date": "2017-06-12"}
]